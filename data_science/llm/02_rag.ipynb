{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb65410",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**RAG** is a technique that enhances Large Language Models (LLMs) by retrieving relevant information from external knowledge sources before generating responses. This approach addresses key LLM limitations: hallucinations, outdated knowledge, and lack of domain-specific information.\n",
    "\n",
    "## Table of Contents\n",
    "1. [RAG Architecture](#1-rag-architecture)\n",
    "2. [Document Loading](#2-document-loading)\n",
    "3. [Chunking Strategies](#3-chunking-strategies)\n",
    "4. [Embedding Models](#4-embedding-models)\n",
    "5. [Vector Stores](#5-vector-stores)\n",
    "6. [Retrieval Methods](#6-retrieval-methods)\n",
    "7. [Reranking](#7-reranking)\n",
    "8. [Evaluation Metrics](#8-evaluation-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8627af97",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. RAG Architecture\n",
    "\n",
    "RAG consists of two main phases: **Indexing** (offline) and **Retrieval & Generation** (online).\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           RAG PIPELINE OVERVIEW                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                        INDEXING PHASE (Offline)                                ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║   ┌──────────┐     ┌──────────┐     ┌──────────┐     ┌──────────────────┐    ║\n",
    "║   │ Documents│────▶│  Loader  │────▶│ Chunking │────▶│ Embedding Model  │    ║\n",
    "║   │ (PDF,    │     │          │     │          │     │                  │    ║\n",
    "║   │  TXT,    │     │ Extract  │     │ Split    │     │ Text → Vectors   │    ║\n",
    "║   │  HTML)   │     │ Text     │     │ Text     │     │ [0.1, 0.3, ...]  │    ║\n",
    "║   └──────────┘     └──────────┘     └──────────┘     └────────┬─────────┘    ║\n",
    "║                                                                │              ║\n",
    "║                                                                ▼              ║\n",
    "║                                                       ┌──────────────────┐    ║\n",
    "║                                                       │   Vector Store   │    ║\n",
    "║                                                       │ (FAISS, Chroma,  │    ║\n",
    "║                                                       │  Pinecone, etc.) │    ║\n",
    "║                                                       └──────────────────┘    ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    RETRIEVAL & GENERATION PHASE (Online)                       ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║   ┌──────────┐     ┌──────────┐     ┌──────────────────┐                     ║\n",
    "║   │  User    │────▶│ Embedding│────▶│   Vector Store   │                     ║\n",
    "║   │  Query   │     │  Model   │     │                  │                     ║\n",
    "║   └──────────┘     └──────────┘     │  Similarity      │                     ║\n",
    "║                                      │  Search          │                     ║\n",
    "║                                      └────────┬─────────┘                     ║\n",
    "║                                               │                               ║\n",
    "║                                               ▼                               ║\n",
    "║   ┌──────────────────────────────────────────────────────────────────────┐   ║\n",
    "║   │                    Retrieved Documents (Top-K)                        │   ║\n",
    "║   │  ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐                      │   ║\n",
    "║   │  │ Doc 1  │  │ Doc 2  │  │ Doc 3  │  │ Doc K  │                      │   ║\n",
    "║   │  │Score:  │  │Score:  │  │Score:  │  │Score:  │                      │   ║\n",
    "║   │  │ 0.95   │  │ 0.89   │  │ 0.85   │  │ 0.72   │                      │   ║\n",
    "║   │  └────────┘  └────────┘  └────────┘  └────────┘                      │   ║\n",
    "║   └──────────────────────────────────────────────────────────────────────┘   ║\n",
    "║                                               │                               ║\n",
    "║                                               ▼                               ║\n",
    "║                                      ┌──────────────────┐                     ║\n",
    "║                                      │    Reranker      │ (Optional)          ║\n",
    "║                                      │ Cross-Encoder    │                     ║\n",
    "║                                      └────────┬─────────┘                     ║\n",
    "║                                               │                               ║\n",
    "║                                               ▼                               ║\n",
    "║   ┌──────────────────────────────────────────────────────────────────────┐   ║\n",
    "║   │                         PROMPT TEMPLATE                               │   ║\n",
    "║   │  ┌────────────────────────────────────────────────────────────────┐  │   ║\n",
    "║   │  │ Context: {retrieved_documents}                                  │  │   ║\n",
    "║   │  │                                                                  │  │   ║\n",
    "║   │  │ Question: {user_query}                                          │  │   ║\n",
    "║   │  │                                                                  │  │   ║\n",
    "║   │  │ Answer based on the context above:                              │  │   ║\n",
    "║   │  └────────────────────────────────────────────────────────────────┘  │   ║\n",
    "║   └──────────────────────────────────────────────────────────────────────┘   ║\n",
    "║                                               │                               ║\n",
    "║                                               ▼                               ║\n",
    "║                                      ┌──────────────────┐                     ║\n",
    "║                                      │       LLM        │                     ║\n",
    "║                                      │  (GPT-4, Claude, │                     ║\n",
    "║                                      │   Llama, etc.)   │                     ║\n",
    "║                                      └────────┬─────────┘                     ║\n",
    "║                                               │                               ║\n",
    "║                                               ▼                               ║\n",
    "║                                      ┌──────────────────┐                     ║\n",
    "║                                      │    Response      │                     ║\n",
    "║                                      │ (Grounded in     │                     ║\n",
    "║                                      │  retrieved docs) │                     ║\n",
    "║                                      └──────────────────┘                     ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c963439",
   "metadata": {},
   "source": [
    "### Why RAG?\n",
    "\n",
    "| Challenge | Without RAG | With RAG |\n",
    "|-----------|-------------|----------|\n",
    "| **Hallucinations** | LLM may generate plausible but incorrect facts | Responses grounded in retrieved documents |\n",
    "| **Outdated Knowledge** | Training data has a cutoff date | Access to current/updated documents |\n",
    "| **Domain-Specific** | Generic knowledge only | Access to proprietary/specialized data |\n",
    "| **Transparency** | Black-box responses | Can cite sources and provide provenance |\n",
    "| **Cost** | Fine-tuning expensive for updates | Update knowledge base without retraining |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c353246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langchain langchain-openai langchain-community chromadb faiss-cpu\n",
    "# !pip install llama-index llama-index-embeddings-openai llama-index-vector-stores-chroma\n",
    "# !pip install sentence-transformers unstructured pypdf tiktoken\n",
    "# !pip install ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Set up API keys (use environment variables in production)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf6129",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Document Loading\n",
    "\n",
    "The first step in RAG is loading documents from various sources. Both LangChain and LlamaIndex provide extensive document loaders.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                      DOCUMENT LOADERS                                │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │\n",
    "│  │    PDF      │  │    HTML     │  │  Markdown   │  │    CSV      │ │\n",
    "│  │  (.pdf)     │  │  (.html)    │  │   (.md)     │  │   (.csv)    │ │\n",
    "│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘ │\n",
    "│         │                │                │                │        │\n",
    "│         ▼                ▼                ▼                ▼        │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐│\n",
    "│  │                     Document Loader                              ││\n",
    "│  │            (Extracts text + metadata)                           ││\n",
    "│  └─────────────────────────────────────────────────────────────────┘│\n",
    "│         │                                                           │\n",
    "│         ▼                                                           │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐│\n",
    "│  │  Document(page_content=\"...\", metadata={\"source\": \"...\", ...}) ││\n",
    "│  └─────────────────────────────────────────────────────────────────┘│\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LangChain Document Loaders\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    DirectoryLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    CSVLoader,\n",
    "    WebBaseLoader,\n",
    ")\n",
    "\n",
    "# Load a PDF file\n",
    "def load_pdf(file_path: str):\n",
    "    \"\"\"Load a PDF file and return documents.\"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} pages from PDF\")\n",
    "    return documents\n",
    "\n",
    "# Load text files from a directory\n",
    "def load_directory(dir_path: str, glob_pattern: str = \"**/*.txt\"):\n",
    "    \"\"\"Load all text files from a directory recursively.\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        dir_path,\n",
    "        glob=glob_pattern,\n",
    "        loader_cls=TextLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents from directory\")\n",
    "    return documents\n",
    "\n",
    "# Load from web URLs\n",
    "def load_web_pages(urls: List[str]):\n",
    "    \"\"\"Load content from web pages.\"\"\"\n",
    "    loader = WebBaseLoader(urls)\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} web pages\")\n",
    "    return documents\n",
    "\n",
    "# Example usage (commented out - requires actual files)\n",
    "# pdf_docs = load_pdf(\"./data/document.pdf\")\n",
    "# web_docs = load_web_pages([\"https://example.com/page1\", \"https://example.com/page2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d272e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LlamaIndex Document Loaders (SimpleDirectoryReader)\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, Document\n",
    "\n",
    "def load_documents_llamaindex(input_dir: str = None, input_files: List[str] = None):\n",
    "    \"\"\"\n",
    "    Load documents using LlamaIndex's SimpleDirectoryReader.\n",
    "    Supports: .pdf, .docx, .pptx, .jpg, .png, .mp3, .mp4, etc.\n",
    "    \"\"\"\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_dir=input_dir,\n",
    "        input_files=input_files,\n",
    "        recursive=True,\n",
    "        required_exts=[\".pdf\", \".txt\", \".md\", \".docx\"],  # Optional filter\n",
    "    )\n",
    "    documents = reader.load_data()\n",
    "    print(f\"Loaded {len(documents)} documents with LlamaIndex\")\n",
    "    return documents\n",
    "\n",
    "# Create documents from text (useful for testing)\n",
    "def create_sample_documents():\n",
    "    \"\"\"Create sample documents for demonstration.\"\"\"\n",
    "    texts = [\n",
    "        \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "        \"Deep learning uses neural networks with multiple layers to model complex patterns in data.\",\n",
    "        \"Natural language processing (NLP) focuses on the interaction between computers and human language.\",\n",
    "        \"Transformers are a type of neural network architecture that uses self-attention mechanisms.\",\n",
    "        \"RAG combines retrieval systems with generative models to produce more accurate responses.\",\n",
    "    ]\n",
    "    \n",
    "    documents = [Document(text=text, metadata={\"source\": f\"doc_{i}\"}) for i, text in enumerate(texts)]\n",
    "    return documents\n",
    "\n",
    "# Create sample documents for later use\n",
    "sample_docs = create_sample_documents()\n",
    "print(f\"Created {len(sample_docs)} sample documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4883141",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Chunking Strategies\n",
    "\n",
    "Documents must be split into smaller chunks for effective retrieval. The chunking strategy significantly impacts RAG performance.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         CHUNKING STRATEGIES                                  │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ 1. FIXED-SIZE CHUNKING                                                       │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  Original:  [████████████████████████████████████████████████████████████]  │\n",
    "│                                                                              │\n",
    "│  Chunks:    [████████] [████████] [████████] [████████] [████████]          │\n",
    "│              chunk_1    chunk_2    chunk_3    chunk_4    chunk_5            │\n",
    "│                                                                              │\n",
    "│  ⚠️ May split mid-sentence or mid-concept                                   │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ 2. RECURSIVE CHARACTER SPLITTING                                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  Separators (in order): [\"\\n\\n\", \"\\n\", \" \", \"\"]                             │\n",
    "│                                                                              │\n",
    "│  Original:                                                                   │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────┐    │\n",
    "│  │ Paragraph 1: Lorem ipsum dolor sit amet...                          │    │\n",
    "│  │                                                                     │    │\n",
    "│  │ Paragraph 2: Consectetur adipiscing elit...                         │    │\n",
    "│  │                                                                     │    │\n",
    "│  │ Paragraph 3: Sed do eiusmod tempor...                               │    │\n",
    "│  └─────────────────────────────────────────────────────────────────────┘    │\n",
    "│                          │                                                   │\n",
    "│                          ▼                                                   │\n",
    "│  ┌────────────────────┐ ┌────────────────────┐ ┌────────────────────┐       │\n",
    "│  │ Chunk 1            │ │ Chunk 2            │ │ Chunk 3            │       │\n",
    "│  │ (Paragraph 1)      │ │ (Paragraph 2)      │ │ (Paragraph 3)      │       │\n",
    "│  └────────────────────┘ └────────────────────┘ └────────────────────┘       │\n",
    "│                                                                              │\n",
    "│  ✅ Respects document structure                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ 3. SEMANTIC CHUNKING                                                         │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  Sentences:  S1   S2   S3   S4   S5   S6   S7   S8   S9   S10               │\n",
    "│              │    │    │    │    │    │    │    │    │    │                 │\n",
    "│              ▼    ▼    ▼    ▼    ▼    ▼    ▼    ▼    ▼    ▼                 │\n",
    "│  Embeddings: E1   E2   E3   E4   E5   E6   E7   E8   E9   E10               │\n",
    "│                                                                              │\n",
    "│  Similarity: ───0.9──0.85─┼─0.3─┼─0.88──0.92──0.87─┼─0.25─┼─0.9──          │\n",
    "│                           │     │                  │      │                 │\n",
    "│                        break  break             break  break                │\n",
    "│                                                                              │\n",
    "│  Chunks:    [S1, S2, S3] [S4] [S5, S6, S7, S8] [S9] [S10]                   │\n",
    "│               Topic A    Topic B    Topic C    Topic D  Topic E             │\n",
    "│                                                                              │\n",
    "│  ✅ Maintains semantic coherence                                            │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│ 4. OVERLAP (Applied to any strategy)                                         │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  Without overlap:   [████████] [████████] [████████]                        │\n",
    "│                                                                              │\n",
    "│  With overlap:      [████████████]                                          │\n",
    "│                          [████████████]                                      │\n",
    "│                              [████████████]                                  │\n",
    "│                     └──overlap──┘                                            │\n",
    "│                                                                              │\n",
    "│  ✅ Preserves context at chunk boundaries                                   │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LangChain Text Splitters\n",
    "# ============================================================================\n",
    "\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "\n",
    "# Sample document for demonstration\n",
    "sample_text = \"\"\"\n",
    "Machine Learning Fundamentals\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning\n",
    "\n",
    "Supervised Learning: The algorithm learns from labeled training data and makes predictions. Examples include classification and regression tasks.\n",
    "\n",
    "Unsupervised Learning: The algorithm finds patterns in unlabeled data. Common techniques include clustering and dimensionality reduction.\n",
    "\n",
    "Reinforcement Learning: The algorithm learns by interacting with an environment and receiving rewards or penalties for its actions.\n",
    "\n",
    "Deep Learning\n",
    "\n",
    "Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks) to model complex patterns. It has achieved remarkable success in image recognition, natural language processing, and game playing.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Original text length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e26c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Character Text Splitter (Simple fixed-size)\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(sample_text)\n",
    "print(f\"Character splitter produced {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk ({len(char_chunks[0])} chars):\\n{char_chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Recursive Character Text Splitter (Recommended for most use cases)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Try to split on paragraphs first\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(sample_text)\n",
    "print(f\"Recursive splitter produced {len(recursive_chunks)} chunks\\n\")\n",
    "\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"{chunk[:100]}...\" if len(chunk) > 100 else chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd193ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Token-based Splitter (Useful for LLM context windows)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,  # 100 tokens\n",
    "    chunk_overlap=20,  # 20 token overlap\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(sample_text)\n",
    "print(f\"Token splitter produced {len(token_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6083f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Semantic Chunking (LangChain Experimental)\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Note: Requires OpenAI API key\n",
    "def semantic_chunking(text: str):\n",
    "    \"\"\"\n",
    "    Split text based on semantic similarity between sentences.\n",
    "    Groups semantically similar sentences together.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Breakpoint types:\n",
    "    # - \"percentile\": Split at sentences where similarity drops below a percentile\n",
    "    # - \"standard_deviation\": Split based on standard deviation from mean\n",
    "    # - \"interquartile\": Split based on interquartile range\n",
    "    \n",
    "    semantic_splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=95,  # Lower = more chunks\n",
    "    )\n",
    "    \n",
    "    chunks = semantic_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Example usage (requires API key)\n",
    "# semantic_chunks = semantic_chunking(sample_text)\n",
    "# print(f\"Semantic chunking produced {len(semantic_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LlamaIndex Chunking (Node Parsers)\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    "    HierarchicalNodeParser,\n",
    ")\n",
    "from llama_index.core import Document as LlamaDocument\n",
    "\n",
    "# Create a LlamaIndex document\n",
    "llama_doc = LlamaDocument(text=sample_text)\n",
    "\n",
    "# 1. Sentence Splitter (Similar to RecursiveCharacterTextSplitter)\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=32,\n",
    ")\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents([llama_doc])\n",
    "print(f\"SentenceSplitter produced {len(nodes)} nodes\\n\")\n",
    "\n",
    "for i, node in enumerate(nodes[:3]):  # Show first 3\n",
    "    print(f\"Node {i+1}: {node.text[:100]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Hierarchical Node Parser (Creates parent-child relationships)\n",
    "# Useful for \"small-to-big\" retrieval strategies\n",
    "\n",
    "hierarchical_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[512, 256, 128],  # Parent → Child → Grandchild\n",
    ")\n",
    "\n",
    "hierarchical_nodes = hierarchical_parser.get_nodes_from_documents([llama_doc])\n",
    "print(f\"Hierarchical parser produced {len(hierarchical_nodes)} nodes\")\n",
    "\n",
    "# Show node hierarchy\n",
    "for node in hierarchical_nodes[:5]:\n",
    "    parent_id = node.parent_node.node_id if node.parent_node else \"None\"\n",
    "    print(f\"Node: {node.node_id[:8]}... | Parent: {parent_id[:8] if parent_id != 'None' else 'None'}... | Size: {len(node.text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc4454",
   "metadata": {},
   "source": [
    "### Chunking Best Practices\n",
    "\n",
    "| Factor | Recommendation |\n",
    "|--------|----------------|\n",
    "| **Chunk Size** | 256-512 tokens for most use cases; smaller (128-256) for precise retrieval |\n",
    "| **Overlap** | 10-20% of chunk size to preserve context at boundaries |\n",
    "| **Strategy** | Start with Recursive/Sentence splitting; use Semantic for topic-heavy docs |\n",
    "| **Metadata** | Preserve source, page number, section headers for filtering |\n",
    "| **Evaluation** | Test retrieval quality with different settings |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b244f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Embedding Models\n",
    "\n",
    "Embedding models convert text into dense vector representations that capture semantic meaning.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         EMBEDDING PROCESS                                    │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "  Text Input                    Embedding Model                  Vector Output\n",
    "  ───────────                   ───────────────                  ─────────────\n",
    "                                                                              \n",
    "  \"Machine learning           ┌─────────────────┐              [0.023, -0.156,\n",
    "   is a subset of      ────▶  │ Transformer     │  ────▶        0.892, 0.034,\n",
    "   artificial                 │ Encoder         │               -0.445, 0.721,\n",
    "   intelligence\"              │ (BERT, etc.)    │               ..., 0.089]\n",
    "                              └─────────────────┘               (d dimensions)\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                     SIMILARITY IN VECTOR SPACE                               │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "                    ▲ Dimension 2\n",
    "                    │\n",
    "                    │     • \"deep learning\"\n",
    "                    │    ╱\n",
    "                    │   ╱  (high similarity)\n",
    "                    │  ╱\n",
    "                    │ • \"machine learning\"\n",
    "                    │\n",
    "                    │\n",
    "                    │                    • \"cooking recipes\"\n",
    "                    │                   (low similarity)\n",
    "                    │\n",
    "                    └──────────────────────────────────────▶ Dimension 1\n",
    "\n",
    "  Similarity Measures:\n",
    "  • Cosine Similarity: cos(θ) = (A · B) / (||A|| × ||B||)\n",
    "  • Euclidean Distance: √(Σ(aᵢ - bᵢ)²)\n",
    "  • Dot Product: A · B = Σ(aᵢ × bᵢ)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aebcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Embedding Models Comparison\n",
    "# ============================================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingModelInfo:\n",
    "    name: str\n",
    "    provider: str\n",
    "    dimensions: int\n",
    "    max_tokens: int\n",
    "    use_case: str\n",
    "    cost: str\n",
    "\n",
    "embedding_models = [\n",
    "    EmbeddingModelInfo(\"text-embedding-3-large\", \"OpenAI\", 3072, 8191, \"High accuracy, production\", \"$$\"),\n",
    "    EmbeddingModelInfo(\"text-embedding-3-small\", \"OpenAI\", 1536, 8191, \"Cost-effective, good quality\", \"$\"),\n",
    "    EmbeddingModelInfo(\"text-embedding-ada-002\", \"OpenAI\", 1536, 8191, \"Legacy, widely used\", \"$\"),\n",
    "    EmbeddingModelInfo(\"all-MiniLM-L6-v2\", \"Sentence-Transformers\", 384, 256, \"Fast, lightweight, local\", \"Free\"),\n",
    "    EmbeddingModelInfo(\"all-mpnet-base-v2\", \"Sentence-Transformers\", 768, 384, \"High quality, local\", \"Free\"),\n",
    "    EmbeddingModelInfo(\"bge-large-en-v1.5\", \"BAAI\", 1024, 512, \"SOTA open-source\", \"Free\"),\n",
    "    EmbeddingModelInfo(\"e5-large-v2\", \"Microsoft\", 1024, 512, \"Excellent retrieval\", \"Free\"),\n",
    "    EmbeddingModelInfo(\"voyage-large-2\", \"Voyage AI\", 1536, 16000, \"Long context, high quality\", \"$$\"),\n",
    "    EmbeddingModelInfo(\"embed-english-v3.0\", \"Cohere\", 1024, 512, \"Multilingual support\", \"$$\"),\n",
    "]\n",
    "\n",
    "print(\"Popular Embedding Models:\\n\")\n",
    "print(f\"{'Model':<25} {'Provider':<20} {'Dims':<8} {'Max Tokens':<12} {'Cost'}\")\n",
    "print(\"=\" * 80)\n",
    "for model in embedding_models:\n",
    "    print(f\"{model.name:<25} {model.provider:<20} {model.dimensions:<8} {model.max_tokens:<12} {model.cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LangChain Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# OpenAI Embeddings (requires API key)\n",
    "def get_openai_embeddings():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        # dimensions=512,  # Can reduce dimensions for efficiency\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "# Local HuggingFace Embeddings (free, no API key needed)\n",
    "def get_local_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},  # or 'cuda' for GPU\n",
    "        encode_kwargs={'normalize_embeddings': True},  # For cosine similarity\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "# Example: Generate embeddings\n",
    "# embeddings_model = get_local_embeddings()\n",
    "# query_embedding = embeddings_model.embed_query(\"What is machine learning?\")\n",
    "# doc_embeddings = embeddings_model.embed_documents([\"Doc 1 text\", \"Doc 2 text\"])\n",
    "# print(f\"Query embedding dimension: {len(query_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LlamaIndex Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# OpenAI Embedding\n",
    "def get_llamaindex_openai_embedding():\n",
    "    embed_model = OpenAIEmbedding(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        # dimensions=512,\n",
    "    )\n",
    "    return embed_model\n",
    "\n",
    "# Local HuggingFace Embedding\n",
    "def get_llamaindex_local_embedding():\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    )\n",
    "    return embed_model\n",
    "\n",
    "# Example usage\n",
    "# embed_model = get_llamaindex_local_embedding()\n",
    "# embedding = embed_model.get_text_embedding(\"What is deep learning?\")\n",
    "# print(f\"Embedding dimension: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbdad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Compute Similarity Between Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    v1, v2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def euclidean_distance(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Compute Euclidean distance between two vectors.\"\"\"\n",
    "    v1, v2 = np.array(vec1), np.array(vec2)\n",
    "    return np.linalg.norm(v1 - v2)\n",
    "\n",
    "# Demo with random vectors (replace with actual embeddings)\n",
    "vec_a = np.random.randn(384).tolist()\n",
    "vec_b = np.random.randn(384).tolist()\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_similarity(vec_a, vec_b):.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distance(vec_a, vec_b):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e858bb5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Vector Stores\n",
    "\n",
    "Vector stores are specialized databases optimized for storing and querying high-dimensional vectors.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         VECTOR STORE ARCHITECTURE                            │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "  Documents                    Vector Store                     Query\n",
    "  ─────────                    ────────────                     ─────\n",
    "\n",
    "  ┌─────────┐                 ┌─────────────────────────┐      ┌─────────┐\n",
    "  │ Doc 1   │──▶ Embed ──▶   │  ┌─────────────────────┐│      │ Query   │\n",
    "  └─────────┘                 │  │ Vector Index        ││      └────┬────┘\n",
    "  ┌─────────┐                 │  │ ┌───┐ ┌───┐ ┌───┐  ││           │\n",
    "  │ Doc 2   │──▶ Embed ──▶   │  │ │ V1│ │ V2│ │ V3│  ││           ▼\n",
    "  └─────────┘                 │  │ └───┘ └───┘ └───┘  ││       Embed\n",
    "  ┌─────────┐                 │  │ ┌───┐ ┌───┐ ┌───┐  ││           │\n",
    "  │ Doc 3   │──▶ Embed ──▶   │  │ │ V4│ │ V5│ │ V6│  ││           ▼\n",
    "  └─────────┘                 │  │ └───┘ └───┘ └───┘  ││      ┌─────────┐\n",
    "       ⋮                      │  └─────────────────────┘│      │ Query   │\n",
    "  ┌─────────┐                 │                          │      │ Vector  │\n",
    "  │ Doc N   │──▶ Embed ──▶   │  ┌─────────────────────┐│      └────┬────┘\n",
    "  └─────────┘                 │  │ Metadata Storage    ││           │\n",
    "                              │  │ • source: file.pdf  ││           ▼\n",
    "                              │  │ • page: 5           ││   ┌─────────────┐\n",
    "                              │  │ • date: 2024-01-15  ││   │ ANN Search  │\n",
    "                              │  └─────────────────────┘│   └──────┬──────┘\n",
    "                              └─────────────────────────┘          │\n",
    "                                                                   ▼\n",
    "                                                          ┌─────────────┐\n",
    "                                                          │ Top-K Docs  │\n",
    "                                                          │ + Scores    │\n",
    "                                                          └─────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                    APPROXIMATE NEAREST NEIGHBOR (ANN)                        │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  Algorithm            Description                      Trade-off             │\n",
    "│  ─────────            ───────────                      ─────────             │\n",
    "│  HNSW                 Hierarchical graph navigation    Memory ↑, Speed ↑    │\n",
    "│  IVF                  Inverted file with clusters      Balanced             │\n",
    "│  PQ                   Product quantization             Memory ↓, Accuracy ↓ │\n",
    "│  Flat                 Brute force (exact)              Slow, Accurate       │\n",
    "│                                                                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820be22",
   "metadata": {},
   "source": [
    "### Vector Store Comparison\n",
    "\n",
    "| Vector Store | Type | Hosting | Best For | Key Features |\n",
    "|--------------|------|---------|----------|-------------|\n",
    "| **FAISS** | Library | Local | Development, high performance | Fast, CPU/GPU support |\n",
    "| **ChromaDB** | Database | Local/Cloud | Prototyping, simplicity | Easy setup, metadata filtering |\n",
    "| **Pinecone** | Managed | Cloud | Production, scale | Fully managed, hybrid search |\n",
    "| **Weaviate** | Database | Self-hosted/Cloud | Semantic search | GraphQL, modules |\n",
    "| **Qdrant** | Database | Self-hosted/Cloud | Filtering, precision | Rust-based, fast |\n",
    "| **Milvus** | Database | Self-hosted/Cloud | Large-scale | Distributed, GPU support |\n",
    "| **pgvector** | Extension | Self-hosted | Postgres users | SQL integration |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LangChain Vector Stores\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\n",
    "    Document(page_content=\"Machine learning enables computers to learn from data.\", metadata={\"source\": \"ml_intro\", \"page\": 1}),\n",
    "    Document(page_content=\"Deep learning uses neural networks with many layers.\", metadata={\"source\": \"dl_intro\", \"page\": 1}),\n",
    "    Document(page_content=\"Natural language processing helps computers understand text.\", metadata={\"source\": \"nlp_intro\", \"page\": 1}),\n",
    "    Document(page_content=\"Computer vision allows machines to interpret images.\", metadata={\"source\": \"cv_intro\", \"page\": 1}),\n",
    "    Document(page_content=\"Reinforcement learning trains agents through rewards.\", metadata={\"source\": \"rl_intro\", \"page\": 1}),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e67633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS Vector Store (requires: pip install faiss-cpu)\n",
    "# Fast, in-memory, great for development\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def create_faiss_store(documents: List[Document]):\n",
    "    \"\"\"Create a FAISS vector store from documents.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    \n",
    "    # Save to disk (optional)\n",
    "    # vectorstore.save_local(\"./faiss_index\")\n",
    "    \n",
    "    # Load from disk\n",
    "    # vectorstore = FAISS.load_local(\"./faiss_index\", embeddings)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "# faiss_store = create_faiss_store(documents)\n",
    "# print(\"FAISS store created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cec592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB Vector Store (requires: pip install chromadb)\n",
    "# Easy to use, supports persistence, metadata filtering\n",
    "\n",
    "def create_chroma_store(documents: List[Document], persist_directory: str = None):\n",
    "    \"\"\"Create a ChromaDB vector store from documents.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    if persist_directory:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "            collection_name=\"my_collection\",\n",
    "        )\n",
    "    else:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            collection_name=\"my_collection\",\n",
    "        )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "# chroma_store = create_chroma_store(documents, \"./chroma_db\")\n",
    "# print(\"ChromaDB store created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LlamaIndex Vector Stores\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core import Document as LlamaDocument\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "def create_llamaindex_index(documents: List[LlamaDocument]):\n",
    "    \"\"\"Create a LlamaIndex vector store index.\"\"\"\n",
    "    # Simple in-memory index\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    return index\n",
    "\n",
    "def create_llamaindex_chroma_index(documents: List[LlamaDocument], persist_dir: str):\n",
    "    \"\"\"Create a LlamaIndex index with ChromaDB backend.\"\"\"\n",
    "    # Initialize ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\"llama_collection\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # Create index\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    return index\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "# llama_docs = [LlamaDocument(text=doc.page_content) for doc in documents]\n",
    "# index = create_llamaindex_index(llama_docs)\n",
    "# print(\"LlamaIndex created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5356d0",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Retrieval Methods\n",
    "\n",
    "Different retrieval strategies offer trade-offs between relevance, diversity, and efficiency.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         RETRIEVAL METHODS                                    │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║ 1. SIMILARITY SEARCH                                                           ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║   Query Vector ──────────────────────────────┐                               ║\n",
    "║                                              │                               ║\n",
    "║                                              ▼                               ║\n",
    "║   Document Vectors:   D1 ─────── Score: 0.95  ◀── Most Similar              ║\n",
    "║                       D2 ─────── Score: 0.89                                 ║\n",
    "║                       D3 ─────── Score: 0.85                                 ║\n",
    "║                       D4 ─────── Score: 0.72                                 ║\n",
    "║                       D5 ─────── Score: 0.65                                 ║\n",
    "║                                                                               ║\n",
    "║   Return: Top-K by similarity score                                          ║\n",
    "║   ⚠️ Issue: May return redundant/similar documents                          ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║ 2. MAXIMUM MARGINAL RELEVANCE (MMR)                                           ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║   MMR = argmax [ λ · Sim(dᵢ, q) - (1-λ) · max Sim(dᵢ, dⱼ) ]                 ║\n",
    "║           dᵢ∈R\\S                        dⱼ∈S                                 ║\n",
    "║                                                                               ║\n",
    "║   λ = 1.0: Pure similarity (no diversity)                                    ║\n",
    "║   λ = 0.0: Pure diversity (ignore relevance)                                 ║\n",
    "║   λ = 0.5: Balanced (recommended starting point)                             ║\n",
    "║                                                                               ║\n",
    "║                    High Relevance                                             ║\n",
    "║                         ▲                                                     ║\n",
    "║                         │    • D1 (selected)                                  ║\n",
    "║                         │                                                     ║\n",
    "║                         │  • D2 (similar to D1, skipped)                      ║\n",
    "║                         │                                                     ║\n",
    "║                         │           • D3 (selected - diverse)                 ║\n",
    "║                         │                                                     ║\n",
    "║                         │       • D4 (selected - diverse)                     ║\n",
    "║                         │                                                     ║\n",
    "║   Low Relevance ────────┴──────────────────────────▶ High Diversity          ║\n",
    "║                                                                               ║\n",
    "║   ✅ Balances relevance and diversity                                        ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║ 3. HYBRID SEARCH (Sparse + Dense)                                             ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║   Query: \"python machine learning tutorial\"                                  ║\n",
    "║                    │                                                          ║\n",
    "║          ┌─────────┴─────────┐                                               ║\n",
    "║          ▼                   ▼                                               ║\n",
    "║   ┌─────────────┐     ┌─────────────┐                                        ║\n",
    "║   │ BM25/TF-IDF │     │   Dense     │                                        ║\n",
    "║   │  (Sparse)   │     │ Embeddings  │                                        ║\n",
    "║   │             │     │             │                                        ║\n",
    "║   │ Keyword     │     │ Semantic    │                                        ║\n",
    "║   │ Matching    │     │ Similarity  │                                        ║\n",
    "║   └──────┬──────┘     └──────┬──────┘                                        ║\n",
    "║          │                   │                                               ║\n",
    "║          └─────────┬─────────┘                                               ║\n",
    "║                    ▼                                                          ║\n",
    "║             ┌─────────────┐                                                   ║\n",
    "║             │   Fusion    │  (RRF, weighted sum, etc.)                       ║\n",
    "║             └──────┬──────┘                                                   ║\n",
    "║                    ▼                                                          ║\n",
    "║             Combined Results                                                  ║\n",
    "║                                                                               ║\n",
    "║   ✅ Best of both: exact keywords + semantic understanding                   ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LangChain Retrieval Methods\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a sample vector store for demonstration\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Python is a high-level programming language known for its simplicity.\", metadata={\"topic\": \"python\"}),\n",
    "    Document(page_content=\"Python is widely used in machine learning and data science.\", metadata={\"topic\": \"python\"}),\n",
    "    Document(page_content=\"Machine learning algorithms learn patterns from data.\", metadata={\"topic\": \"ml\"}),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning using neural networks.\", metadata={\"topic\": \"dl\"}),\n",
    "    Document(page_content=\"Natural language processing enables computers to understand human language.\", metadata={\"topic\": \"nlp\"}),\n",
    "    Document(page_content=\"Computer vision helps machines interpret visual information from the world.\", metadata={\"topic\": \"cv\"}),\n",
    "    Document(page_content=\"TensorFlow and PyTorch are popular deep learning frameworks.\", metadata={\"topic\": \"dl\"}),\n",
    "    Document(page_content=\"Scikit-learn provides simple machine learning tools in Python.\", metadata={\"topic\": \"ml\"}),\n",
    "]\n",
    "\n",
    "# Note: Uncomment to create actual vector store\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# vectorstore = FAISS.from_documents(sample_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Similarity Search\n",
    "def similarity_search(vectorstore, query: str, k: int = 4):\n",
    "    \"\"\"Retrieve documents by cosine similarity.\"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    return results\n",
    "\n",
    "# 2. Similarity Search with Scores\n",
    "def similarity_search_with_scores(vectorstore, query: str, k: int = 4):\n",
    "    \"\"\"Retrieve documents with similarity scores.\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    # Returns list of (document, score) tuples\n",
    "    return results\n",
    "\n",
    "# 3. Maximum Marginal Relevance (MMR)\n",
    "def mmr_search(vectorstore, query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5):\n",
    "    \"\"\"\n",
    "    Retrieve documents using MMR for diversity.\n",
    "    \n",
    "    Args:\n",
    "        k: Number of documents to return\n",
    "        fetch_k: Number of documents to initially retrieve\n",
    "        lambda_mult: 0 = max diversity, 1 = max relevance\n",
    "    \"\"\"\n",
    "    results = vectorstore.max_marginal_relevance_search(\n",
    "        query,\n",
    "        k=k,\n",
    "        fetch_k=fetch_k,\n",
    "        lambda_mult=lambda_mult,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage (requires actual vectorstore)\n",
    "# query = \"What is machine learning in Python?\"\n",
    "# similar_docs = similarity_search(vectorstore, query)\n",
    "# mmr_docs = mmr_search(vectorstore, query, lambda_mult=0.5)\n",
    "# print(f\"Similarity search returned {len(similar_docs)} docs\")\n",
    "# print(f\"MMR search returned {len(mmr_docs)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metadata Filtering\n",
    "def filtered_search(vectorstore, query: str, filter_dict: Dict[str, Any], k: int = 4):\n",
    "    \"\"\"Retrieve documents with metadata filtering.\"\"\"\n",
    "    results = vectorstore.similarity_search(\n",
    "        query,\n",
    "        k=k,\n",
    "        filter=filter_dict,  # e.g., {\"topic\": \"ml\"}\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example: Filter by topic\n",
    "# filtered_docs = filtered_search(vectorstore, \"neural networks\", {\"topic\": \"dl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Hybrid Search (BM25 + Dense)\n",
    "# ============================================================================\n",
    "\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "def create_hybrid_retriever(documents: List[Document], vectorstore):\n",
    "    \"\"\"\n",
    "    Create a hybrid retriever combining BM25 (sparse) and vector (dense) search.\n",
    "    \"\"\"\n",
    "    # BM25 Retriever (keyword-based)\n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = 4\n",
    "    \n",
    "    # Vector Retriever (semantic)\n",
    "    vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # Ensemble Retriever (combines both)\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[0.5, 0.5],  # Equal weights; adjust based on use case\n",
    "    )\n",
    "    \n",
    "    return ensemble_retriever\n",
    "\n",
    "# Example usage\n",
    "# hybrid_retriever = create_hybrid_retriever(sample_docs, vectorstore)\n",
    "# results = hybrid_retriever.invoke(\"Python machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d216433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LlamaIndex Retrieval\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "def llamaindex_retrieval_example(index: VectorStoreIndex, query: str):\n",
    "    \"\"\"\n",
    "    Demonstrate LlamaIndex retrieval options.\n",
    "    \"\"\"\n",
    "    # Create retriever with similarity_top_k\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=5,\n",
    "    )\n",
    "    \n",
    "    # Retrieve nodes\n",
    "    nodes = retriever.retrieve(query)\n",
    "    \n",
    "    # Apply post-processing (filter by similarity threshold)\n",
    "    processor = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    filtered_nodes = processor.postprocess_nodes(nodes)\n",
    "    \n",
    "    return filtered_nodes\n",
    "\n",
    "# Example usage\n",
    "# nodes = llamaindex_retrieval_example(index, \"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36d99a",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Reranking\n",
    "\n",
    "Reranking improves retrieval quality by using a more powerful model to re-score initially retrieved documents.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           RERANKING PIPELINE                                 │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "  Query: \"How does attention mechanism work in transformers?\"\n",
    "         │\n",
    "         ▼\n",
    "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "  │                    STAGE 1: Initial Retrieval                           │\n",
    "  │                    (Bi-Encoder / Embedding Model)                       │\n",
    "  │                                                                         │\n",
    "  │   Query ──▶ [Embed] ──▶ Vector Search ──▶ Top-K (e.g., 20 docs)        │\n",
    "  │                                                                         │\n",
    "  │   ✓ Fast (single embedding per query)                                  │\n",
    "  │   ✗ May miss nuanced relevance                                         │\n",
    "  └─────────────────────────────────────────────────────────────────────────┘\n",
    "         │\n",
    "         ▼ Top-20 documents\n",
    "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "  │                    STAGE 2: Reranking                                    │\n",
    "  │                    (Cross-Encoder Model)                                │\n",
    "  │                                                                         │\n",
    "  │   For each document:                                                    │\n",
    "  │   ┌─────────────────────────────────────────────────────────────────┐  │\n",
    "  │   │  [CLS] Query [SEP] Document [SEP]  ──▶  Relevance Score         │  │\n",
    "  │   └─────────────────────────────────────────────────────────────────┘  │\n",
    "  │                                                                         │\n",
    "  │   Doc 1:  0.95  ──▶ Rank 1                                             │\n",
    "  │   Doc 7:  0.91  ──▶ Rank 2 (moved up!)                                 │\n",
    "  │   Doc 3:  0.87  ──▶ Rank 3                                             │\n",
    "  │   Doc 2:  0.82  ──▶ Rank 4 (moved down)                                │\n",
    "  │   ...                                                                   │\n",
    "  │                                                                         │\n",
    "  │   ✓ More accurate relevance scoring                                    │\n",
    "  │   ✗ Slower (processes query+doc pairs)                                 │\n",
    "  └─────────────────────────────────────────────────────────────────────────┘\n",
    "         │\n",
    "         ▼ Top-K (e.g., 5 docs)\n",
    "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "  │                    Final Retrieved Documents                            │\n",
    "  │                    (Higher quality, fewer docs)                         │\n",
    "  └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│              BI-ENCODER vs CROSS-ENCODER                                     │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  BI-ENCODER (Retrieval)              CROSS-ENCODER (Reranking)              │\n",
    "│  ─────────────────────               ────────────────────────               │\n",
    "│                                                                              │\n",
    "│  Query ──▶ [Encoder] ──▶ Eq          Query + Doc ──▶ [Encoder] ──▶ Score   │\n",
    "│  Doc   ──▶ [Encoder] ──▶ Ed                                                 │\n",
    "│                                                                              │\n",
    "│  Similarity = cos(Eq, Ed)            Direct relevance prediction            │\n",
    "│                                                                              │\n",
    "│  ✓ Pre-compute doc embeddings        ✗ Cannot pre-compute                  │\n",
    "│  ✓ Very fast at query time           ✗ Slower (O(n) per query)             │\n",
    "│  ✗ Less accurate                     ✓ More accurate                       │\n",
    "│                                                                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Reranking with Cross-Encoders\n",
    "# ============================================================================\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Reranker:\n",
    "    \"\"\"Rerank documents using a cross-encoder model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the reranker.\n",
    "        \n",
    "        Popular models:\n",
    "        - cross-encoder/ms-marco-MiniLM-L-6-v2 (fast, good quality)\n",
    "        - cross-encoder/ms-marco-MiniLM-L-12-v2 (better quality)\n",
    "        - BAAI/bge-reranker-base (excellent quality)\n",
    "        - BAAI/bge-reranker-large (best quality, slower)\n",
    "        \"\"\"\n",
    "        self.model = CrossEncoder(model_name)\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Rerank documents based on relevance to the query.\n",
    "        \n",
    "        Returns:\n",
    "            List of (document, score) tuples, sorted by relevance.\n",
    "        \"\"\"\n",
    "        # Create query-document pairs\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "        \n",
    "        # Get relevance scores\n",
    "        scores = self.model.predict(pairs)\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        doc_scores = list(zip(documents, scores))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return doc_scores[:top_k]\n",
    "\n",
    "# Example usage (uncomment to run - requires sentence-transformers)\n",
    "# reranker = Reranker()\n",
    "# query = \"What is the attention mechanism?\"\n",
    "# docs = [\n",
    "#     \"Transformers use self-attention to process sequences.\",\n",
    "#     \"Python is a programming language.\",\n",
    "#     \"Attention allows models to focus on relevant parts of the input.\",\n",
    "#     \"Machine learning is a field of AI.\",\n",
    "# ]\n",
    "# reranked = reranker.rerank(query, docs, top_k=2)\n",
    "# for doc, score in reranked:\n",
    "#     print(f\"Score: {score:.4f} | {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LangChain Reranking with Cohere\n",
    "# ============================================================================\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "def create_cohere_reranker(base_retriever, top_n: int = 3):\n",
    "    \"\"\"\n",
    "    Create a retriever with Cohere reranking.\n",
    "    Requires: COHERE_API_KEY environment variable.\n",
    "    \"\"\"\n",
    "    compressor = CohereRerank(top_n=top_n)\n",
    "    \n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever,\n",
    "    )\n",
    "    \n",
    "    return compression_retriever\n",
    "\n",
    "# Example usage\n",
    "# base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "# reranking_retriever = create_cohere_reranker(base_retriever, top_n=5)\n",
    "# results = reranking_retriever.invoke(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabf2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LlamaIndex Reranking\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "def llamaindex_rerank_example(nodes, query: str):\n",
    "    \"\"\"\n",
    "    Rerank nodes using a cross-encoder in LlamaIndex.\n",
    "    \"\"\"\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        top_n=3,\n",
    "    )\n",
    "    \n",
    "    query_bundle = QueryBundle(query_str=query)\n",
    "    reranked_nodes = reranker.postprocess_nodes(nodes, query_bundle)\n",
    "    \n",
    "    return reranked_nodes\n",
    "\n",
    "# Example: Using reranker in query engine\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "# query_engine = index.as_query_engine(\n",
    "#     similarity_top_k=10,\n",
    "#     node_postprocessors=[reranker],\n",
    "# )\n",
    "# response = query_engine.query(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be9733",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation Metrics\n",
    "\n",
    "Evaluating RAG systems requires metrics for both retrieval quality and generation quality.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                      RAG EVALUATION FRAMEWORK                                │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                         RETRIEVAL METRICS                                      ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  1. Precision@K = (Relevant docs in top-K) / K                               ║\n",
    "║     └── Of the K retrieved docs, how many are relevant?                      ║\n",
    "║                                                                               ║\n",
    "║  2. Recall@K = (Relevant docs in top-K) / (Total relevant docs)              ║\n",
    "║     └── Of all relevant docs, how many did we retrieve?                      ║\n",
    "║                                                                               ║\n",
    "║  3. Mean Reciprocal Rank (MRR) = (1/|Q|) Σ (1/rank_i)                        ║\n",
    "║     └── Where does the first relevant doc appear?                            ║\n",
    "║                                                                               ║\n",
    "║  4. Normalized DCG (nDCG) = DCG / IDCG                                       ║\n",
    "║     └── Considers position and graded relevance                              ║\n",
    "║                                                                               ║\n",
    "║  5. Hit Rate = (Queries with ≥1 relevant doc in top-K) / |Q|                 ║\n",
    "║     └── Simple binary: did we find anything relevant?                        ║\n",
    "║                                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                       GENERATION METRICS (RAGAS)                               ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  ┌─────────────────────────────────────────────────────────────────────────┐ ║\n",
    "║  │                         CONTEXT METRICS                                  │ ║\n",
    "║  ├─────────────────────────────────────────────────────────────────────────┤ ║\n",
    "║  │                                                                         │ ║\n",
    "║  │  Context Precision: Are retrieved chunks relevant to the question?     │ ║\n",
    "║  │  ┌─────────────────────────────────────────────────────────────────┐   │ ║\n",
    "║  │  │ Question: \"What is the capital of France?\"                      │   │ ║\n",
    "║  │  │ Retrieved: [\"Paris is the capital of France\", \"France wines\"]   │   │ ║\n",
    "║  │  │ Score: 0.5 (1 relevant / 2 total)                               │   │ ║\n",
    "║  │  └─────────────────────────────────────────────────────────────────┘   │ ║\n",
    "║  │                                                                         │ ║\n",
    "║  │  Context Recall: Does retrieved context cover the ground truth?        │ ║\n",
    "║  │  ┌─────────────────────────────────────────────────────────────────┐   │ ║\n",
    "║  │  │ Ground Truth: \"Paris is the capital and largest city of France\" │   │ ║\n",
    "║  │  │ Retrieved: Contains \"Paris is the capital of France\"            │   │ ║\n",
    "║  │  │ Score: High (covers main fact)                                   │   │ ║\n",
    "║  │  └─────────────────────────────────────────────────────────────────┘   │ ║\n",
    "║  └─────────────────────────────────────────────────────────────────────────┘ ║\n",
    "║                                                                               ║\n",
    "║  ┌─────────────────────────────────────────────────────────────────────────┐ ║\n",
    "║  │                        ANSWER METRICS                                    │ ║\n",
    "║  ├─────────────────────────────────────────────────────────────────────────┤ ║\n",
    "║  │                                                                         │ ║\n",
    "║  │  Faithfulness: Is the answer grounded in the retrieved context?        │ ║\n",
    "║  │  ┌─────────────────────────────────────────────────────────────────┐   │ ║\n",
    "║  │  │ Context: \"Paris is the capital of France\"                       │   │ ║\n",
    "║  │  │ Answer: \"The capital of France is Paris\"                        │   │ ║\n",
    "║  │  │ Score: 1.0 (fully grounded)                                      │   │ ║\n",
    "║  │  └─────────────────────────────────────────────────────────────────┘   │ ║\n",
    "║  │                                                                         │ ║\n",
    "║  │  Answer Relevancy: Does the answer address the question?               │ ║\n",
    "║  │  ┌─────────────────────────────────────────────────────────────────┐   │ ║\n",
    "║  │  │ Question: \"What is the capital of France?\"                      │   │ ║\n",
    "║  │  │ Answer: \"Paris is a beautiful city with the Eiffel Tower\"       │   │ ║\n",
    "║  │  │ Score: 0.6 (mentions Paris but doesn't directly answer)         │   │ ║\n",
    "║  │  └─────────────────────────────────────────────────────────────────┘   │ ║\n",
    "║  │                                                                         │ ║\n",
    "║  │  Answer Correctness: Is the answer factually correct?                  │ ║\n",
    "║  │  └── Compares answer against ground truth                              │ ║\n",
    "║  │                                                                         │ ║\n",
    "║  └─────────────────────────────────────────────────────────────────────────┘ ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Retrieval Metrics Implementation\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Set\n",
    "\n",
    "def precision_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision@K.\n",
    "    \n",
    "    Args:\n",
    "        retrieved: List of retrieved document IDs\n",
    "        relevant: Set of relevant document IDs\n",
    "        k: Number of top results to consider\n",
    "    \"\"\"\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)\n",
    "    return relevant_in_k / k\n",
    "\n",
    "def recall_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall@K.\n",
    "    \"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)\n",
    "    return relevant_in_k / len(relevant)\n",
    "\n",
    "def mean_reciprocal_rank(retrieved: List[str], relevant: Set[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR).\n",
    "    Returns the reciprocal of the rank of the first relevant document.\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(retrieved, 1):\n",
    "        if doc in relevant:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(retrieved: List[str], relevance_scores: dict, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (nDCG@K).\n",
    "    \n",
    "    Args:\n",
    "        retrieved: List of retrieved document IDs\n",
    "        relevance_scores: Dict mapping doc_id to relevance score (0-3 scale)\n",
    "        k: Number of top results to consider\n",
    "    \"\"\"\n",
    "    def dcg(scores: List[float]) -> float:\n",
    "        return sum(score / np.log2(i + 2) for i, score in enumerate(scores))\n",
    "    \n",
    "    # DCG for retrieved documents\n",
    "    retrieved_scores = [relevance_scores.get(doc, 0) for doc in retrieved[:k]]\n",
    "    dcg_score = dcg(retrieved_scores)\n",
    "    \n",
    "    # Ideal DCG (best possible ranking)\n",
    "    ideal_scores = sorted(relevance_scores.values(), reverse=True)[:k]\n",
    "    idcg_score = dcg(ideal_scores)\n",
    "    \n",
    "    if idcg_score == 0:\n",
    "        return 0.0\n",
    "    return dcg_score / idcg_score\n",
    "\n",
    "# Example\n",
    "retrieved_docs = [\"doc1\", \"doc3\", \"doc2\", \"doc5\", \"doc4\"]\n",
    "relevant_docs = {\"doc1\", \"doc2\", \"doc4\"}\n",
    "\n",
    "print(f\"Precision@3: {precision_at_k(retrieved_docs, relevant_docs, 3):.3f}\")\n",
    "print(f\"Recall@3: {recall_at_k(retrieved_docs, relevant_docs, 3):.3f}\")\n",
    "print(f\"MRR: {mean_reciprocal_rank(retrieved_docs, relevant_docs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAGAS Evaluation Framework\n",
    "# ============================================================================\n",
    "\n",
    "# RAGAS provides comprehensive RAG evaluation metrics\n",
    "# Install: pip install ragas\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class RAGASMetrics:\n",
    "    \"\"\"RAGAS evaluation metrics explanation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_metrics_info():\n",
    "        metrics = {\n",
    "            \"context_precision\": {\n",
    "                \"description\": \"Measures if retrieved context is relevant to the question\",\n",
    "                \"range\": \"0-1 (higher is better)\",\n",
    "                \"needs\": \"question, contexts\",\n",
    "            },\n",
    "            \"context_recall\": {\n",
    "                \"description\": \"Measures if retrieved context covers the ground truth\",\n",
    "                \"range\": \"0-1 (higher is better)\",\n",
    "                \"needs\": \"contexts, ground_truth\",\n",
    "            },\n",
    "            \"faithfulness\": {\n",
    "                \"description\": \"Measures if answer is grounded in retrieved context\",\n",
    "                \"range\": \"0-1 (higher is better)\",\n",
    "                \"needs\": \"question, contexts, answer\",\n",
    "            },\n",
    "            \"answer_relevancy\": {\n",
    "                \"description\": \"Measures if answer addresses the question\",\n",
    "                \"range\": \"0-1 (higher is better)\",\n",
    "                \"needs\": \"question, answer\",\n",
    "            },\n",
    "            \"answer_correctness\": {\n",
    "                \"description\": \"Measures factual correctness against ground truth\",\n",
    "                \"range\": \"0-1 (higher is better)\",\n",
    "                \"needs\": \"answer, ground_truth\",\n",
    "            },\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "# Display metrics info\n",
    "for name, info in RAGASMetrics.get_metrics_info().items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    print(f\"  Range: {info['range']}\")\n",
    "    print(f\"  Requires: {info['needs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAGAS Evaluation Example\n",
    "# ============================================================================\n",
    "\n",
    "# Note: This requires RAGAS and an LLM API key (OpenAI recommended)\n",
    "\n",
    "def run_ragas_evaluation():\n",
    "    \"\"\"\n",
    "    Example of running RAGAS evaluation on a RAG system.\n",
    "    \"\"\"\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import (\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        answer_correctness,\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Prepare evaluation data\n",
    "    eval_data = {\n",
    "        \"question\": [\n",
    "            \"What is machine learning?\",\n",
    "            \"How does deep learning work?\",\n",
    "        ],\n",
    "        \"answer\": [\n",
    "            \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "            \"Deep learning uses neural networks with multiple layers to learn patterns.\",\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            [\"Machine learning is a field of AI where algorithms learn from data.\"],\n",
    "            [\"Deep learning uses deep neural networks with many hidden layers.\"],\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience.\",\n",
    "            \"Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers.\",\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(eval_data)\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_correctness,\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage (uncomment to run - requires API key)\n",
    "# results = run_ragas_evaluation()\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LlamaIndex Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "\n",
    "def llamaindex_evaluation_example(query_engine, llm):\n",
    "    \"\"\"\n",
    "    Example of LlamaIndex built-in evaluators.\n",
    "    \"\"\"\n",
    "    # Initialize evaluators\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "    relevancy_evaluator = RelevancyEvaluator(llm=llm)\n",
    "    \n",
    "    # Query the system\n",
    "    query = \"What is deep learning?\"\n",
    "    response = query_engine.query(query)\n",
    "    \n",
    "    # Evaluate faithfulness (is response grounded in retrieved context?)\n",
    "    faithfulness_result = faithfulness_evaluator.evaluate_response(\n",
    "        query=query,\n",
    "        response=response,\n",
    "    )\n",
    "    \n",
    "    # Evaluate relevancy (does response answer the question?)\n",
    "    relevancy_result = relevancy_evaluator.evaluate_response(\n",
    "        query=query,\n",
    "        response=response,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"faithfulness\": faithfulness_result.passing,\n",
    "        \"relevancy\": relevancy_result.passing,\n",
    "    }\n",
    "\n",
    "# Example usage (requires actual query_engine and llm)\n",
    "# results = llamaindex_evaluation_example(query_engine, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb52e21",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete RAG Pipeline Example\n",
    "\n",
    "Putting it all together: a complete RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Complete RAG Pipeline with LangChain\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def create_rag_chain(documents: List[Document]):\n",
    "    \"\"\"\n",
    "    Create a complete RAG chain.\n",
    "    \"\"\"\n",
    "    # 1. Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # 2. Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    \n",
    "    # 3. Create retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5, \"fetch_k\": 20},\n",
    "    )\n",
    "    \n",
    "    # 4. Create prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context. \n",
    "If you cannot answer the question based on the context, say \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # 5. Create LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    # 6. Create RAG chain\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# Example usage (requires API key)\n",
    "# chain = create_rag_chain(documents)\n",
    "# response = chain.invoke(\"What is machine learning?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Complete RAG Pipeline with LlamaIndex\n",
    "# ============================================================================\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "def create_llamaindex_rag(documents):\n",
    "    \"\"\"\n",
    "    Create a complete RAG system with LlamaIndex.\n",
    "    \"\"\"\n",
    "    # 1. Configure settings\n",
    "    Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "    Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    \n",
    "    # 2. Create index\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    \n",
    "    # 3. Create query engine with MMR\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=5,\n",
    "        response_mode=\"compact\",  # Options: refine, compact, tree_summarize\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "# Example usage (requires API key)\n",
    "# from llama_index.core import Document\n",
    "# docs = [Document(text=\"Machine learning is...\")]\n",
    "# engine = create_llamaindex_rag(docs)\n",
    "# response = engine.query(\"What is machine learning?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137d7cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Component | Best Practice |\n",
    "|-----------|---------------|\n",
    "| **Document Loading** | Use appropriate loaders for file types; preserve metadata |\n",
    "| **Chunking** | Start with recursive splitter; 256-512 tokens; 10-20% overlap |\n",
    "| **Embeddings** | Match embedding model to use case; consider cost vs. quality |\n",
    "| **Vector Store** | FAISS/Chroma for dev; Pinecone/Qdrant for production |\n",
    "| **Retrieval** | Use MMR for diversity; hybrid search for keyword + semantic |\n",
    "| **Reranking** | Add cross-encoder for quality boost; worth the latency cost |\n",
    "| **Evaluation** | Use RAGAS metrics; test on representative queries |\n",
    "\n",
    "### Advanced Topics (Not Covered)\n",
    "\n",
    "- **Query Transformation**: Query rewriting, HyDE, multi-query\n",
    "- **Advanced Retrieval**: Parent-child, sentence-window, auto-merging\n",
    "- **Agentic RAG**: Tool-using agents for complex queries\n",
    "- **Knowledge Graphs**: Combining structured + unstructured retrieval\n",
    "- **Fine-tuning**: Embedding and reranker fine-tuning for domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b04bbf",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "1. Lewis, P. et al. (2020). [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "2. [LangChain Documentation](https://python.langchain.com/docs/)\n",
    "3. [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
    "4. [RAGAS: Evaluation Framework for RAG](https://docs.ragas.io/)\n",
    "5. [Sentence Transformers](https://www.sbert.net/)\n",
    "6. Gao, Y. et al. (2023). [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
