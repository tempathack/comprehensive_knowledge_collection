{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecast Metrics\n",
    "\n",
    "Forecast evaluation is not one-size-fits-all. A good metric reflects the business cost of errors and\n",
    "lets you compare models fairly across series and horizons. This notebook focuses on **point-forecast**\n",
    "metrics and their tradeoffs.\n",
    "\n",
    "**Metric families**\n",
    "- Scale-dependent: MAE, RMSE\n",
    "- Percentage-based: MAPE, sMAPE, WAPE\n",
    "- Scaled vs naive baseline: MASE\n",
    "\n",
    "Lower is better for all metrics shown here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Build a toy series\n",
    "We use a synthetic monthly series with trend, seasonality, and noise so the differences between\n",
    "metrics are easy to see.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def mae(y, yhat):\n",
    "    y = np.asarray(y)\n",
    "    yhat = np.asarray(yhat)\n",
    "    return np.mean(np.abs(y - yhat))\n",
    "\n",
    "def rmse(y, yhat):\n",
    "    y = np.asarray(y)\n",
    "    yhat = np.asarray(yhat)\n",
    "    return np.sqrt(np.mean((y - yhat) ** 2))\n",
    "\n",
    "def mape(y, yhat):\n",
    "    y = np.asarray(y)\n",
    "    yhat = np.asarray(yhat)\n",
    "    mask = y != 0\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y[mask] - yhat[mask]) / y[mask])) * 100\n",
    "\n",
    "def smape(y, yhat):\n",
    "    y = np.asarray(y)\n",
    "    yhat = np.asarray(yhat)\n",
    "    denom = np.abs(y) + np.abs(yhat)\n",
    "    mask = denom != 0\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "    return np.mean(2 * np.abs(y[mask] - yhat[mask]) / denom[mask]) * 100\n",
    "\n",
    "def wape(y, yhat):\n",
    "    y = np.asarray(y)\n",
    "    yhat = np.asarray(yhat)\n",
    "    denom = np.sum(np.abs(y))\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return np.sum(np.abs(y - yhat)) / denom * 100\n",
    "\n",
    "def mase(y, yhat, y_train):\n",
    "    y = np.asarray(y)\n",
    "    yhat = np.asarray(yhat)\n",
    "    y_train = np.asarray(y_train)\n",
    "    naive_denom = np.mean(np.abs(np.diff(y_train)))\n",
    "    if naive_denom == 0:\n",
    "        return np.nan\n",
    "    return mae(y, yhat) / naive_denom\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 120\n",
    "t = np.arange(n)\n",
    "trend = 0.2 * t\n",
    "season = 10 * np.sin(2 * np.pi * t / 12)\n",
    "noise = rng.normal(0, 3, size=n)\n",
    "y = 50 + trend + season + noise\n",
    "\n",
    "index = pd.period_range('2015-01', periods=n, freq='M').to_timestamp()\n",
    "series = pd.Series(y, index=index, name='y')\n",
    "\n",
    "train = series.iloc[:-24]\n",
    "test = series.iloc[-24:]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train.index, y=train, mode='lines', name='Train'))\n",
    "fig.add_trace(go.Scatter(x=test.index, y=test, mode='lines', name='Test', line=dict(color='black')))\n",
    "fig.add_vline(x=test.index[0], line_dash='dash', line_color='gray')\n",
    "fig.update_layout(title='Synthetic series with train/test split', xaxis_title='Date', yaxis_title='y')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Baseline forecasts\n",
    "Simple baselines are strong benchmarks. If your model cannot beat these, it is not useful yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forecast_naive(train, horizon_index):\n",
    "    return pd.Series([train.iloc[-1]] * len(horizon_index), index=horizon_index, name='Naive')\n",
    "\n",
    "def forecast_seasonal_naive(train, horizon_index, season_length=12):\n",
    "    last_season = train.iloc[-season_length:]\n",
    "    reps = int(np.ceil(len(horizon_index) / season_length))\n",
    "    vals = np.tile(last_season.values, reps)[: len(horizon_index)]\n",
    "    return pd.Series(vals, index=horizon_index, name=f'Seasonal naive (s={season_length})')\n",
    "\n",
    "def forecast_moving_average(train, horizon_index, window=6):\n",
    "    mean_val = train.iloc[-window:].mean()\n",
    "    return pd.Series([mean_val] * len(horizon_index), index=horizon_index, name=f'Moving average (w={window})')\n",
    "\n",
    "pred_naive = forecast_naive(train, test.index)\n",
    "pred_seasonal = forecast_seasonal_naive(train, test.index, season_length=12)\n",
    "pred_ma = forecast_moving_average(train, test.index, window=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train.index, y=train, mode='lines', name='Train', line=dict(color='gray')))\n",
    "fig.add_trace(go.Scatter(x=test.index, y=test, mode='lines', name='Test', line=dict(color='black')))\n",
    "fig.add_trace(go.Scatter(x=test.index, y=pred_naive, mode='lines', name=pred_naive.name))\n",
    "fig.add_trace(go.Scatter(x=test.index, y=pred_seasonal, mode='lines', name=pred_seasonal.name))\n",
    "fig.add_trace(go.Scatter(x=test.index, y=pred_ma, mode='lines', name=pred_ma.name))\n",
    "fig.add_vline(x=test.index[0], line_dash='dash', line_color='gray')\n",
    "fig.update_layout(title='Baseline forecasts on the test horizon', xaxis_title='Date', yaxis_title='y')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Metric definitions and intuition\n",
    "Let $y_t$ be the actual value and $\\hat{y}_t$ the forecast.\n",
    "\n",
    "**Scale-dependent**\n",
    "- MAE: $\\text{MAE} = \\frac{1}{n} \\sum_{t=1}^n |y_t - \\hat{y}_t|$\n",
    "- RMSE: $\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{t=1}^n (y_t - \\hat{y}_t)^2}$\n",
    "\n",
    "**Percentage-based**\n",
    "- MAPE: $\\text{MAPE} = \\frac{100}{n} \\sum_{t=1}^n \\left|\\frac{y_t - \\hat{y}_t}{y_t}\\right|$\n",
    "- sMAPE: $\\text{sMAPE} = \\frac{100}{n} \\sum_{t=1}^n \\frac{2|y_t - \\hat{y}_t|}{|y_t| + |\\hat{y}_t|}$\n",
    "- WAPE: $\\text{WAPE} = 100 \\cdot \\frac{\\sum |y_t - \\hat{y}_t|}{\\sum |y_t|}$\n",
    "\n",
    "**Scaled vs naive**\n",
    "- MASE: $\\text{MASE} = \\frac{\\text{MAE}}{\\frac{1}{n-1} \\sum_{t=2}^n |y_t - y_{t-1}|}$\n",
    "\n",
    "**Intuition**\n",
    "- MAE is the typical absolute miss.\n",
    "- RMSE punishes large errors (outliers) more than MAE.\n",
    "- MAPE is easy to explain but breaks when $y_t = 0$.\n",
    "- sMAPE reduces zero issues and is symmetric.\n",
    "- WAPE is a stable percentage alternative for many series.\n",
    "- MASE < 1 means you beat a naive one-step baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_train):\n",
    "    return {\n",
    "        'MAE': mae(y_true, y_pred),\n",
    "        'RMSE': rmse(y_true, y_pred),\n",
    "        'MAPE (%)': mape(y_true, y_pred),\n",
    "        'sMAPE (%)': smape(y_true, y_pred),\n",
    "        'WAPE (%)': wape(y_true, y_pred),\n",
    "        'MASE': mase(y_true, y_pred, y_train),\n",
    "    }\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    pred_naive.name: compute_metrics(test.values, pred_naive.values, train.values),\n",
    "    pred_seasonal.name: compute_metrics(test.values, pred_seasonal.values, train.values),\n",
    "    pred_ma.name: compute_metrics(test.values, pred_ma.values, train.values),\n",
    "}).T\n",
    "\n",
    "metrics.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "scale_dependent = ['MAE', 'RMSE']\n",
    "scale_free = ['MAPE (%)', 'sMAPE (%)', 'WAPE (%)', 'MASE']\n",
    "\n",
    "metrics_long = metrics.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "fig1 = px.bar(\n",
    "    metrics_long.melt(id_vars='model', value_vars=scale_dependent),\n",
    "    x='model',\n",
    "    y='value',\n",
    "    color='variable',\n",
    "    barmode='group',\n",
    "    title='Scale-dependent metrics (lower is better)',\n",
    ")\n",
    "fig1.update_layout(xaxis_title='Model', yaxis_title='Metric value')\n",
    "fig1.show()\n",
    "\n",
    "fig2 = px.bar(\n",
    "    metrics_long.melt(id_vars='model', value_vars=scale_free),\n",
    "    x='model',\n",
    "    y='value',\n",
    "    color='variable',\n",
    "    barmode='group',\n",
    "    title='Scale-free metrics (lower is better)',\n",
    ")\n",
    "fig2.update_layout(xaxis_title='Model', yaxis_title='Metric value')\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Sensitivity checks\n",
    "Metrics behave differently under scaling and outliers. These small experiments make the tradeoffs\n",
    "obvious.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "scale = 10\n",
    "scaled = pd.DataFrame({\n",
    "    'Original': compute_metrics(test.values, pred_seasonal.values, train.values),\n",
    "    'Scaled x10': compute_metrics(test.values * scale, pred_seasonal.values * scale, train.values * scale),\n",
    "}).T\n",
    "\n",
    "scaled[['MAE', 'RMSE', 'MAPE (%)', 'sMAPE (%)', 'WAPE (%)', 'MASE']].round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_outlier = test.copy()\n",
    "test_outlier.iloc[-1] = test_outlier.iloc[-1] + 30\n",
    "\n",
    "outlier_effect = pd.DataFrame({\n",
    "    'Baseline': compute_metrics(test.values, pred_seasonal.values, train.values),\n",
    "    'With outlier': compute_metrics(test_outlier.values, pred_seasonal.values, train.values),\n",
    "}).T\n",
    "\n",
    "outlier_effect[['MAE', 'RMSE', 'MAPE (%)', 'sMAPE (%)', 'WAPE (%)', 'MASE']].round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Metric selection cheat sheet\n",
    "- Use **MAE** when errors have roughly linear business cost.\n",
    "- Use **RMSE** when large misses are disproportionately expensive.\n",
    "- Use **MASE** to compare across series with different scales.\n",
    "- Use **sMAPE/WAPE** for percentage-style reporting without MAPE zero problems.\n",
    "- Avoid **MAPE** if zeros or tiny values are common.\n",
    "\n",
    "A good practice is to report one scale-dependent metric (MAE or RMSE) plus one scale-free metric\n",
    "(MASE, sMAPE, or WAPE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Extensions and missing pieces\n",
    "Point metrics are only part of the story. The following topics are natural next notebooks:\n",
    "- Probabilistic forecasts: pinball loss, CRPS, log score\n",
    "- Prediction intervals: coverage, Winkler score\n",
    "- Multi-horizon aggregation and weighting\n",
    "- Hierarchical and grouped series metrics\n",
    "- Business-weighted loss functions (cost matrices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}