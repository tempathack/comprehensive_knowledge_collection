{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Time Series Clustering â€” Overview\n\n## Why this matters\n- Group sequences by **shape**, **phase**, or **dynamics** to discover regimes.\n- Build cohorts for monitoring, forecasting, or anomaly detection.\n- Create prototypes (medoids/centroids) that summarize behavior.\n\n## Key decisions\n1. **Representation**: raw series, aligned series, or engineered features.\n2. **Similarity**: Euclidean, correlation, DTW, shape-based distances.\n3. **Algorithm**: partitional (k-means/k-medoids), hierarchical, density.\n4. **Validation**: silhouette, stability, and domain interpretation.\n\n## Notation\nA univariate series is $x = (x_1, \\dots, x_T)$. We often standardize per-series using\n\n$$z_t = \\frac{x_t - \\mu_x}{\\sigma_x}.$$\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\n\ncolors = [\"#1B9E77\", \"#D95F02\", \"#7570B3\"]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## A simple clustering pipeline\nThis pipeline frames the common choices you make in most projects.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "steps = [\"Ingest\", \"Preprocess\", \"Representation\", \"Similarity\", \"Clustering\", \"Validate\"]\n\nfig = go.Figure()\nfor i, step in enumerate(steps):\n    fig.add_shape(\n        type=\"rect\",\n        x0=i - 0.45,\n        x1=i + 0.45,\n        y0=-0.25,\n        y1=0.25,\n        line=dict(color=\"#2C7FB8\", width=2),\n        fillcolor=\"#D0ECF4\",\n    )\n    fig.add_annotation(x=i, y=0, text=step, showarrow=False, font=dict(size=12))\n    if i < len(steps) - 1:\n        fig.add_annotation(\n            x=i + 0.45,\n            y=0,\n            ax=i + 0.55,\n            ay=0,\n            showarrow=True,\n            arrowhead=3,\n            arrowsize=1,\n        )\n\nfig.update_xaxes(visible=False)\nfig.update_yaxes(visible=False)\nfig.update_layout(\n    title=\"Time series clustering pipeline\",\n    height=240,\n    margin=dict(l=10, r=10, t=40, b=10),\n)\nfig.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Synthetic dataset\nWe will create three families of series to make the effects of representation clear.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "rng = np.random.default_rng(7)\n\nn_series = 90\nlength = 120\nt = np.linspace(0, 2 * np.pi, length)\n\nseries = []\nlabels_true = []\n\nfor _ in range(n_series // 3):\n    phase = rng.normal(0, 0.2)\n    series.append(np.sin(t + phase) + 0.1 * rng.normal(size=length))\n    labels_true.append(\"sine\")\n\nfor _ in range(n_series // 3):\n    phase = rng.normal(1.0, 0.2)\n    series.append(np.sin(t + phase) + 0.1 * rng.normal(size=length))\n    labels_true.append(\"phase_shifted\")\n\nfor _ in range(n_series // 3):\n    trend = np.linspace(-0.6, 0.8, length)\n    series.append(trend + 0.6 * np.sin(2 * t) + 0.1 * rng.normal(size=length))\n    labels_true.append(\"trend_seasonal\")\n\nseries = np.array(series)\nlabels_true = np.array(labels_true)\n\nsample_ids = rng.choice(len(series), size=12, replace=False)\n\ndf_plot = (\n    pd.DataFrame(series[sample_ids])\n    .assign(series_id=[f\"s{i}\" for i in sample_ids])\n    .assign(true_label=labels_true[sample_ids])\n    .melt(id_vars=[\"series_id\", \"true_label\"], var_name=\"time\", value_name=\"value\")\n)\n\nfig = px.line(\n    df_plot,\n    x=\"time\",\n    y=\"value\",\n    color=\"series_id\",\n    facet_col=\"true_label\",\n    facet_col_spacing=0.05,\n    title=\"Sample series by generating process\",\n)\nfig.update_layout(showlegend=False)\nfig.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Representation choices\n\n### 1) Raw series\n- Requires equal length and aligned time indexes.\n- Sensitive to **phase shifts** and **time warping**.\n\n### 2) Feature-based vectors\n- Works with standard clustering algorithms.\n- Useful for scale, trend, seasonality, or volatility patterns.\n- Examples: mean, slope, dominant frequency, autocorrelation.\n\n### 3) Learned or shape-based embeddings\n- ROCKET, shapelets, autoencoders, wavelets.\n- Often cluster in latent space.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature-based clustering (baseline)\nWe first map each series to a feature vector and cluster with $k$-means.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef extract_features(x):\n    idx = np.arange(len(x))\n    slope = np.polyfit(idx, x, 1)[0]\n    mean = x.mean()\n    std = x.std()\n    autocorr = np.corrcoef(x[:-1], x[1:])[0, 1]\n    fft = np.fft.rfft(x - mean)\n    dominant_freq = np.argmax(np.abs(fft[1:])) + 1\n    amplitude = x.max() - x.min()\n    return [mean, std, slope, autocorr, dominant_freq, amplitude]\n\n\nX = np.array([extract_features(s) for s in series])\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, n_init=20, random_state=7)\nlabels_km = kmeans.fit_predict(X_scaled)\n\npca = PCA(n_components=2, random_state=7)\nX_2d = pca.fit_transform(X_scaled)\n\ndf_feat = pd.DataFrame(X_2d, columns=[\"PC1\", \"PC2\"])\ndf_feat[\"cluster\"] = labels_km.astype(str)\ndf_feat[\"true_label\"] = labels_true\n\nfig = px.scatter(\n    df_feat,\n    x=\"PC1\",\n    y=\"PC2\",\n    color=\"cluster\",\n    symbol=\"true_label\",\n    title=\"Feature-based clustering (PCA view)\",\n)\nfig.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "cluster_means = []\nfor k in range(3):\n    cluster_series = series[labels_km == k]\n    cluster_means.append(cluster_series.mean(axis=0))\n\nfig = go.Figure()\nfor k, mean_series in enumerate(cluster_means):\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(length),\n            y=mean_series,\n            mode=\"lines\",\n            line=dict(color=colors[k], width=3),\n            name=f\"Cluster {k} mean\",\n        )\n    )\n\nfig.update_layout(title=\"Average series per feature-based cluster\")\nfig.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Distance-based clustering with DTW\nDynamic Time Warping aligns series by allowing flexible time shifts. The classic recurrence is\n\n$$D_{i,j} = (x_i - y_j)^2 + \\min\\{D_{i-1,j}, D_{i,j-1}, D_{i-1,j-1}\\},$$\n\nand the DTW distance is $\\sqrt{D_{T,T'}}$.\n\nThis helps when two series share the same **shape** but are phase-shifted.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.cluster import AgglomerativeClustering\n\n\ndef dtw_distance(x, y):\n    n, m = len(x), len(y)\n    D = np.full((n + 1, m + 1), np.inf)\n    D[0, 0] = 0.0\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            cost = (x[i - 1] - y[j - 1]) ** 2\n            D[i, j] = cost + min(D[i - 1, j], D[i, j - 1], D[i - 1, j - 1])\n    return np.sqrt(D[n, m])\n\n\ndef pairwise_dtw(X):\n    n = len(X)\n    dist = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            d = dtw_distance(X[i], X[j])\n            dist[i, j] = d\n            dist[j, i] = d\n    return dist\n\n\nsubset = 36\nseries_small = series[:subset]\nlabels_small = labels_true[:subset]\n\nD = pairwise_dtw(series_small)\n\ntry:\n    model = AgglomerativeClustering(\n        n_clusters=3, metric=\"precomputed\", linkage=\"average\"\n    )\nexcept TypeError:\n    model = AgglomerativeClustering(\n        n_clusters=3, affinity=\"precomputed\", linkage=\"average\"\n    )\n\nlabels_dtw = model.fit_predict(D)\n\nmedoids = []\nfor k in range(3):\n    idx = np.where(labels_dtw == k)[0]\n    if len(idx) == 0:\n        medoids.append(None)\n        continue\n    sub = D[np.ix_(idx, idx)]\n    medoid = idx[np.argmin(sub.sum(axis=1))]\n    medoids.append(medoid)\n\nfig = go.Figure()\nfor k, medoid_idx in enumerate(medoids):\n    idx = np.where(labels_dtw == k)[0]\n    for j in idx:\n        fig.add_trace(\n            go.Scatter(\n                x=np.arange(length),\n                y=series_small[j],\n                mode=\"lines\",\n                line=dict(color=colors[k], width=1),\n                opacity=0.25,\n                showlegend=False,\n            )\n        )\n    if medoid_idx is not None:\n        fig.add_trace(\n            go.Scatter(\n                x=np.arange(length),\n                y=series_small[medoid_idx],\n                mode=\"lines\",\n                line=dict(color=colors[k], width=3),\n                name=f\"DTW cluster {k} medoid\",\n            )\n        )\n\nfig.update_layout(title=\"DTW-based clusters with medoids\")\nfig.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Quick sanity check: Euclidean vs DTW\nA phase-shifted pair may look far apart in Euclidean space but close under DTW.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "x = series[0]\ny = series[n_series // 3]  # shifted sine\n\neuclid = np.linalg.norm(x - y)\ndtw = dtw_distance(x, y)\n\npd.DataFrame(\n    {\n        \"metric\": [\"Euclidean\", \"DTW\"],\n        \"distance\": [euclid, dtw],\n    }\n)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Validation and selection\n- **Silhouette** scores work well for feature-based clustering.\n- For DTW or custom distances, use **stability** (resampling) and **domain coherence**.\n- Choose $k$ with elbow/Gap statistics or by inspecting medoids.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.metrics import silhouette_score\n\nsilhouette = silhouette_score(X_scaled, labels_km)\nfloat(silhouette)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Practical tips\n- Always normalize or standardize per-series when shape matters more than scale.\n- Consider **seasonal differencing** before clustering if trend dominates.\n- For multivariate series, concatenate features per channel or use multivariate DTW.\n- Use **medoids** instead of means when using non-Euclidean distances.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercises\n1. Replace DTW with **correlation distance** and compare cluster assignments.\n2. Add a new feature (e.g., spectral entropy) and observe changes in PCA space.\n3. Try **k-medoids** on the DTW distance matrix and compare medoids.\n\n## Further reading\n- tslearn documentation for DTW and k-medoids.\n- aeon/sktime for time-series transformations and pipelines.\n- Shapelets and ROCKET for fast, discriminative representations.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}