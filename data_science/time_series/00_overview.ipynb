{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series — Core Overview\n",
    "\n",
    "Time series data are **ordered observations** collected over time. The ordering is not just a detail—it is the structure. We cannot shuffle time. This notebook builds the foundations you need for forecasting, anomaly detection, and time‑series modeling in general.\n",
    "\n",
    "We will cover: structure & indexing, stationarity, trend/seasonality, autocorrelation, train/test splitting, baselines, and rolling backtests.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sktime.datasets import load_airline\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "y = load_airline()\n",
    "y.name = \"Passengers\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) What makes time series special?\n",
    "\n",
    "A time series \\(y_t\\) is indexed by time \\(t\\). Unlike i.i.d. data, consecutive observations are **dependent**. This dependence appears as:\n",
    "- **Trend**: long‑term direction\n",
    "- **Seasonality**: periodic patterns\n",
    "- **Autocorrelation**: values correlate with lagged versions of themselves\n",
    "\n",
    "The core modeling question is: **how does the future depend on the past?**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.line(y, title=\"Airline passengers over time\")\n",
    "fig.update_layout(xaxis_title=\"Time\", yaxis_title=\"Passengers\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Indexing & frequency\n",
    "\n",
    "Time series are meaningful only with a well‑defined index. The index encodes frequency (monthly, daily, hourly), which drives seasonality and modeling decisions.\n",
    "\n",
    "Common index types:\n",
    "- `DatetimeIndex`: timestamps\n",
    "- `PeriodIndex`: fixed frequency periods (often preferred for forecasts)\n",
    "\n",
    "If frequency is missing or irregular, you must **resample** or **impute**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect index type and frequency\n",
    "print(type(y.index), y.index.freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Trend, seasonality, and noise (STL decomposition)\n",
    "\n",
    "A classic decomposition view:\n",
    "\n",
    "\\[y_t = T_t + S_t + e_t\\] (additive)\n",
    "\n",
    "or\n",
    "\n",
    "\\[y_t = T_t \times S_t \times e_t\\] (multiplicative)\n",
    "\n",
    "STL (Seasonal‑Trend decomposition using Loess) separates these components.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "stl = STL(y, period=12).fit()\n",
    "components = pd.DataFrame({\n",
    "    \"trend\": stl.trend,\n",
    "    \"seasonal\": stl.seasonal,\n",
    "    \"residual\": stl.resid,\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "for col in components.columns:\n",
    "    fig.add_trace(go.Scatter(x=components.index.to_timestamp(), y=components[col], name=col))\n",
    "fig.update_layout(title=\"STL components (trend / seasonal / residual)\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Stationarity\n",
    "\n",
    "Many classical models (ARMA/ARIMA) assume **stationarity**: the statistical properties (mean, variance, autocorrelation) do not change over time.\n",
    "\n",
    "Tests:\n",
    "- **ADF** (Augmented Dickey‑Fuller): null = non‑stationary\n",
    "- **KPSS**: null = stationary\n",
    "\n",
    "If the series is non‑stationary, we **difference** it:\n",
    "\n",
    "\\[\n",
    "\n",
    "abla y_t = y_t - y_{t-1}\n",
    "\n",
    "\\]\n",
    "\n",
    "Seasonal differencing:\n",
    "\n",
    "\\[\n",
    "\n",
    "abla_s y_t = y_t - y_{t-s}\n",
    "\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "adf_stat, adf_p, *_ = adfuller(y)\n",
    "kpss_stat, kpss_p, *_ = kpss(y, nlags=\"auto\")\n",
    "print(f\"ADF p-value: {adf_p:.4f}\")\n",
    "print(f\"KPSS p-value: {kpss_p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Autocorrelation (ACF) and Partial Autocorrelation (PACF)\n",
    "\n",
    "Autocorrelation at lag \\(k\\):\n",
    "\n",
    "\\[\rho_k = \frac{\\mathrm{Cov}(y_t, y_{t-k})}{\\mathrm{Var}(y_t)}\\]\n",
    "\n",
    "- **ACF** shows correlation with all previous lags\n",
    "- **PACF** isolates *direct* correlation at lag \\(k\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_acf(y, ax=axes[0], lags=36)\n",
    "plot_pacf(y, ax=axes[1], lags=36, method=\"ywm\")\n",
    "axes[0].set_title(\"ACF\")\n",
    "axes[1].set_title(\"PACF\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Forecasting horizons and splits\n",
    "\n",
    "Time series splits must be **temporal**. We define a **forecasting horizon** (fh): the steps ahead we want to predict.\n",
    "\n",
    "Common strategies:\n",
    "- **Single split**: train/test once\n",
    "- **Rolling (sliding)** window: fixed train length\n",
    "- **Expanding** window: grows over time\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import temporal_train_test_split, ForecastingHorizon\n",
    "\n",
    "# 24 months test\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=24)\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Baselines & metrics\n",
    "\n",
    "Baselines anchor your expectations. If your fancy model doesn't beat a **naive** forecast, you should not deploy it.\n",
    "\n",
    "Common metrics:\n",
    "- MAE, RMSE, MAPE, sMAPE\n",
    "\n",
    "**Note:** MAPE can explode when values approach zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Example: naive baseline\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "model = NaiveForecaster(strategy=\"last\")\n",
    "model.fit(y_train)\n",
    "\n",
    "pred = model.predict(fh)\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "rmse = mean_squared_error(y_test, pred, square_root=True)\n",
    "print(f\"MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Backtesting (rolling evaluation)\n",
    "\n",
    "Backtesting simulates how your model would have performed in the past by repeatedly training and forecasting on historical windows. This is essential for trustworthy evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "\n",
    "cv = SlidingWindowSplitter(fh=[1, 2, 3, 6, 12], window_length=60, step_length=12)\n",
    "results = evaluate(\n",
    "    model,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    strategy=\"refit\",\n",
    "    scoring=mean_absolute_error,\n",
    ")\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now have the conceptual toolkit for time‑series forecasting: decomposition, stationarity, autocorrelation, proper evaluation, and baseline discipline. The next notebooks implement core models and show how these ideas translate to practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}