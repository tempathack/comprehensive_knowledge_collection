{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeriesForestRegressor\n",
    "\n",
    "**Regression counterpart of Time Series Forest: interval-based features + ensemble of decision trees.**\n",
    "\n",
    "The Time Series Forest Regressor (TSF-R) adapts the classification-focused Time Series Forest algorithm for regression tasks. It extracts summary statistics from random intervals of time series data and uses these as features for an ensemble of decision tree regressors.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Interval Sampling**: Randomly select subsequences from the time series\n",
    "- **Feature Extraction**: Compute mean, standard deviation, and slope for each interval\n",
    "- **Ensemble Learning**: Combine predictions from multiple trees for robust regression\n",
    "- **Temporal Locality**: Intervals capture local patterns at different time scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9bfef",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation\n",
    "\n",
    "### Interval Sampling\n",
    "\n",
    "For a time series of length $T$, we sample $K$ random intervals. Each interval $[a_k, b_k]$ is drawn uniformly at random:\n",
    "\n",
    "$$[a_k, b_k] \\subset [1, T], \\quad \\text{where } a_k < b_k$$\n",
    "\n",
    "The interval endpoints are sampled such that:\n",
    "- $a_k \\sim \\text{Uniform}(1, T - \\ell_{\\min})$\n",
    "- $b_k \\sim \\text{Uniform}(a_k + \\ell_{\\min}, T)$\n",
    "\n",
    "where $\\ell_{\\min}$ is the minimum interval length.\n",
    "\n",
    "### Interval Summary Statistics\n",
    "\n",
    "For each interval $[a, b]$, we compute three summary statistics:\n",
    "\n",
    "**Mean (location):**\n",
    "$$\\mu_{[a,b]} = \\frac{1}{b-a+1} \\sum_{t=a}^{b} x_t$$\n",
    "\n",
    "**Standard Deviation (spread):**\n",
    "$$\\sigma_{[a,b]} = \\sqrt{\\frac{1}{b-a} \\sum_{t=a}^{b} (x_t - \\mu_{[a,b]})^2}$$\n",
    "\n",
    "**Slope (trend):**\n",
    "$$\\beta_{[a,b]} = \\frac{\\sum_{t=a}^{b}(t - \\bar{t})(x_t - \\mu_{[a,b]})}{\\sum_{t=a}^{b}(t - \\bar{t})^2}$$\n",
    "\n",
    "where $\\bar{t} = \\frac{a+b}{2}$ is the mean time index.\n",
    "\n",
    "### Feature Map\n",
    "\n",
    "The complete feature representation concatenates all interval features:\n",
    "\n",
    "$$\\phi(x) = [\\mu_1, \\sigma_1, \\beta_1, \\mu_2, \\sigma_2, \\beta_2, ..., \\mu_K, \\sigma_K, \\beta_K] \\in \\mathbb{R}^{3K}$$\n",
    "\n",
    "### Ensemble Prediction\n",
    "\n",
    "The final prediction averages across $B$ decision trees:\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B} T_b(\\phi(x))$$\n",
    "\n",
    "### MSE Split Criterion\n",
    "\n",
    "Each tree uses the Mean Squared Error criterion for node splitting:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$$\n",
    "\n",
    "The best split minimizes the weighted sum of child node MSEs:\n",
    "\n",
    "$$\\text{Split Score} = \\frac{n_L}{n}\\text{MSE}_L + \\frac{n_R}{n}\\text{MSE}_R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b091a6",
   "metadata": {},
   "source": [
    "## 2. Low-Level NumPy Implementation\n",
    "\n",
    "Let's build the Time Series Forest Regressor from scratch to understand each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a99686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def sample_intervals(series_length: int, n_intervals: int, min_length: int = 3, \n",
    "                     random_state: int = None) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Sample random intervals from a time series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series_length : int\n",
    "        Length of the time series T\n",
    "    n_intervals : int\n",
    "        Number of intervals K to sample\n",
    "    min_length : int\n",
    "        Minimum interval length (default: 3)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    intervals : List[Tuple[int, int]]\n",
    "        List of (start, end) tuples representing intervals\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    intervals = []\n",
    "    \n",
    "    for _ in range(n_intervals):\n",
    "        # Sample start position (leave room for min_length)\n",
    "        max_start = series_length - min_length\n",
    "        start = rng.integers(0, max_start + 1)\n",
    "        \n",
    "        # Sample end position (at least min_length from start)\n",
    "        min_end = start + min_length\n",
    "        end = rng.integers(min_end, series_length + 1)\n",
    "        \n",
    "        intervals.append((start, end))\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "\n",
    "def compute_interval_features(x: np.ndarray, start: int, end: int) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a single interval.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Time series values (1D array)\n",
    "    start : int\n",
    "        Interval start index (inclusive)\n",
    "    end : int\n",
    "        Interval end index (exclusive)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean : float\n",
    "        Mean of values in interval\n",
    "    std : float\n",
    "        Standard deviation of values in interval\n",
    "    slope : float\n",
    "        Linear regression slope of values in interval\n",
    "    \"\"\"\n",
    "    segment = x[start:end]\n",
    "    n = len(segment)\n",
    "    \n",
    "    # Mean (location statistic)\n",
    "    mean = np.mean(segment)\n",
    "    \n",
    "    # Standard deviation (spread statistic)\n",
    "    std = np.std(segment, ddof=1) if n > 1 else 0.0\n",
    "    \n",
    "    # Slope via linear regression (trend statistic)\n",
    "    # Using least squares: slope = Cov(t, x) / Var(t)\n",
    "    t = np.arange(n)\n",
    "    t_centered = t - t.mean()\n",
    "    x_centered = segment - mean\n",
    "    \n",
    "    var_t = np.sum(t_centered ** 2)\n",
    "    if var_t > 0:\n",
    "        slope = np.sum(t_centered * x_centered) / var_t\n",
    "    else:\n",
    "        slope = 0.0\n",
    "    \n",
    "    return mean, std, slope\n",
    "\n",
    "\n",
    "def build_regression_features(X: np.ndarray, intervals: List[Tuple[int, int]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build feature matrix from time series using interval features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Time series data, shape (n_samples, series_length)\n",
    "    intervals : List[Tuple[int, int]]\n",
    "        List of (start, end) interval tuples\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : np.ndarray\n",
    "        Feature matrix, shape (n_samples, 3 * n_intervals)\n",
    "        Each interval contributes [mean, std, slope]\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_intervals = len(intervals)\n",
    "    features = np.zeros((n_samples, 3 * n_intervals))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j, (start, end) in enumerate(intervals):\n",
    "            mean, std, slope = compute_interval_features(X[i], start, end)\n",
    "            features[i, 3*j] = mean\n",
    "            features[i, 3*j + 1] = std\n",
    "            features[i, 3*j + 2] = slope\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Demonstration with a simple example\n",
    "print(\"Interval Sampling Example:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "series_length = 50\n",
    "n_intervals = 5\n",
    "intervals = sample_intervals(series_length, n_intervals, min_length=5, random_state=42)\n",
    "\n",
    "for i, (start, end) in enumerate(intervals):\n",
    "    print(f\"Interval {i+1}: [{start:2d}, {end:2d}) → length = {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTimeSeriesForestRegressor:\n",
    "    \"\"\"\n",
    "    A simplified Time Series Forest Regressor implementation.\n",
    "    \n",
    "    This implementation demonstrates the core algorithm:\n",
    "    1. Sample random intervals for each tree\n",
    "    2. Extract interval features (mean, std, slope)\n",
    "    3. Train decision tree regressors on interval features\n",
    "    4. Average predictions from all trees\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators: int = 10, n_intervals: int = None,\n",
    "                 min_interval_length: int = 3, max_depth: int = None,\n",
    "                 random_state: int = None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_intervals = n_intervals\n",
    "        self.min_interval_length = min_interval_length\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.trees_ = []\n",
    "        self.intervals_ = []\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Fit the ensemble to training data.\"\"\"\n",
    "        n_samples, series_length = X.shape\n",
    "        \n",
    "        # Default: sqrt(series_length) intervals per tree\n",
    "        if self.n_intervals is None:\n",
    "            self.n_intervals = max(1, int(np.sqrt(series_length)))\n",
    "        \n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        \n",
    "        for b in range(self.n_estimators):\n",
    "            # Sample intervals for this tree\n",
    "            seed = rng.integers(0, 2**31)\n",
    "            intervals = sample_intervals(\n",
    "                series_length, \n",
    "                self.n_intervals, \n",
    "                self.min_interval_length,\n",
    "                random_state=seed\n",
    "            )\n",
    "            self.intervals_.append(intervals)\n",
    "            \n",
    "            # Build features and train tree\n",
    "            features = build_regression_features(X, intervals)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=seed)\n",
    "            tree.fit(features, y)\n",
    "            self.trees_.append(tree)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict using ensemble averaging.\"\"\"\n",
    "        predictions = np.zeros((X.shape[0], self.n_estimators))\n",
    "        \n",
    "        for b, (tree, intervals) in enumerate(zip(self.trees_, self.intervals_)):\n",
    "            features = build_regression_features(X, intervals)\n",
    "            predictions[:, b] = tree.predict(features)\n",
    "        \n",
    "        # Ensemble average\n",
    "        return predictions.mean(axis=1)\n",
    "    \n",
    "    def predict_individual(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get predictions from each individual tree.\"\"\"\n",
    "        predictions = np.zeros((X.shape[0], self.n_estimators))\n",
    "        \n",
    "        for b, (tree, intervals) in enumerate(zip(self.trees_, self.intervals_)):\n",
    "            features = build_regression_features(X, intervals)\n",
    "            predictions[:, b] = tree.predict(features)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Demonstrate feature extraction on a single series\n",
    "print(\"Feature Extraction Example:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a sample time series with trend and variation\n",
    "np.random.seed(42)\n",
    "sample_series = np.sin(np.linspace(0, 4*np.pi, 50)) + np.random.randn(50) * 0.2\n",
    "\n",
    "print(f\"Series length: {len(sample_series)}\")\n",
    "print(f\"\\nFeatures for each interval:\")\n",
    "for i, (start, end) in enumerate(intervals):\n",
    "    mean, std, slope = compute_interval_features(sample_series, start, end)\n",
    "    print(f\"  Interval {i+1} [{start:2d}:{end:2d}]: μ={mean:+.3f}, σ={std:.3f}, β={slope:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74140cb2",
   "metadata": {},
   "source": [
    "## 3. Plotly Visualizations\n",
    "\n",
    "### 3.1 Time Series with Interval Overlays\n",
    "\n",
    "**Intuition**: Visualizing how random intervals cover different portions of the time series helps understand how TSF captures patterns at multiple scales and locations. Each colored region represents an interval from which features are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a30cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "series_length = 50\n",
    "\n",
    "# Create time series with different patterns that influence the target\n",
    "X_synthetic = np.zeros((n_samples, series_length))\n",
    "y_synthetic = np.zeros(n_samples)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Random frequency and amplitude\n",
    "    freq = np.random.uniform(0.5, 2.0)\n",
    "    amp = np.random.uniform(0.5, 2.0)\n",
    "    trend = np.random.uniform(-0.02, 0.02)\n",
    "    noise = np.random.randn(series_length) * 0.2\n",
    "    \n",
    "    t = np.arange(series_length)\n",
    "    X_synthetic[i] = amp * np.sin(2 * np.pi * freq * t / series_length) + trend * t + noise\n",
    "    \n",
    "    # Target is a function of the patterns\n",
    "    y_synthetic[i] = 2 * amp + 5 * freq + 50 * trend + np.random.randn() * 0.5\n",
    "\n",
    "# Visualize a sample series with interval overlays\n",
    "sample_idx = 0\n",
    "sample_series = X_synthetic[sample_idx]\n",
    "\n",
    "# Sample intervals for visualization\n",
    "vis_intervals = sample_intervals(series_length, 5, min_length=5, random_state=123)\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the time series line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(series_length)),\n",
    "    y=sample_series,\n",
    "    mode='lines',\n",
    "    name='Time Series',\n",
    "    line=dict(color='black', width=2)\n",
    "))\n",
    "\n",
    "# Color palette for intervals\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "# Add colored regions for each interval\n",
    "for i, (start, end) in enumerate(vis_intervals):\n",
    "    color = colors[i % len(colors)]\n",
    "    \n",
    "    # Add shaded region\n",
    "    fig.add_vrect(\n",
    "        x0=start, x1=end,\n",
    "        fillcolor=color, opacity=0.3,\n",
    "        layer=\"below\", line_width=0,\n",
    "    )\n",
    "    \n",
    "    # Add interval segment highlight\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(start, end)),\n",
    "        y=sample_series[start:end],\n",
    "        mode='lines',\n",
    "        name=f'Interval {i+1}: [{start}, {end})',\n",
    "        line=dict(color=color, width=3)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Time Series with Random Interval Overlays\",\n",
    "    xaxis_title=\"Time Index\",\n",
    "    yaxis_title=\"Value\",\n",
    "    template=\"plotly_white\",\n",
    "    height=400,\n",
    "    showlegend=True,\n",
    "    legend=dict(x=1.02, y=1)\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b795a3",
   "metadata": {},
   "source": [
    "### 3.2 Feature Extraction Demonstration\n",
    "\n",
    "**Intuition**: This visualization shows how the three summary statistics (mean, std, slope) are computed for each interval. The mean captures the central tendency, std measures local variability, and slope captures the local trend direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98842162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Time Series with Intervals\",\n",
    "        \"Mean (μ) per Interval\",\n",
    "        \"Std Dev (σ) per Interval\",\n",
    "        \"Slope (β) per Interval\"\n",
    "    ],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Compute features for each interval\n",
    "interval_names = []\n",
    "means = []\n",
    "stds = []\n",
    "slopes = []\n",
    "\n",
    "for i, (start, end) in enumerate(vis_intervals):\n",
    "    mean, std, slope = compute_interval_features(sample_series, start, end)\n",
    "    interval_names.append(f\"[{start},{end})\")\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "    slopes.append(slope)\n",
    "\n",
    "# Plot 1: Time series with intervals\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(series_length)), y=sample_series, \n",
    "               mode='lines', name='Series', line=dict(color='black')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "for i, (start, end) in enumerate(vis_intervals):\n",
    "    color = colors[i % len(colors)]\n",
    "    # Add horizontal line for mean\n",
    "    mean = means[i]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[start, end], y=[mean, mean],\n",
    "            mode='lines', name=f'μ_{i+1}',\n",
    "            line=dict(color=color, dash='dash', width=2),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Plot 2: Mean bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=interval_names, y=means, marker_color=colors[:len(vis_intervals)], \n",
    "           name='Mean', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Plot 3: Std bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=interval_names, y=stds, marker_color=colors[:len(vis_intervals)],\n",
    "           name='Std', showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Plot 4: Slope bar chart with color coding\n",
    "slope_colors = ['green' if s > 0 else 'red' for s in slopes]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=interval_names, y=slopes, marker_color=slope_colors,\n",
    "           name='Slope', showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Extraction from Intervals: Mean (μ), Std (σ), Slope (β)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Interval\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Interval\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Interval\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"μ\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"σ\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"β\", row=2, col=2)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9db8d3",
   "metadata": {},
   "source": [
    "### 3.3 Individual Tree Predictions\n",
    "\n",
    "**Intuition**: Each tree in the ensemble makes its own prediction based on different random intervals. This visualization shows the variance in individual tree predictions. The ensemble average (dotted line) is typically more stable than any single tree, demonstrating the power of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training/testing\n",
    "train_size = 70\n",
    "X_train_synth = X_synthetic[:train_size]\n",
    "y_train_synth = y_synthetic[:train_size]\n",
    "X_test_synth = X_synthetic[train_size:]\n",
    "y_test_synth = y_synthetic[train_size:]\n",
    "\n",
    "# Train our custom TSF Regressor\n",
    "n_trees = 20\n",
    "tsf = SimpleTimeSeriesForestRegressor(\n",
    "    n_estimators=n_trees, \n",
    "    n_intervals=10,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "tsf.fit(X_train_synth, y_train_synth)\n",
    "\n",
    "# Get individual tree predictions\n",
    "individual_preds = tsf.predict_individual(X_test_synth)\n",
    "ensemble_preds = tsf.predict(X_test_synth)\n",
    "\n",
    "# Create scatter plot of individual predictions\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add individual tree predictions (jittered for visibility)\n",
    "for tree_idx in range(n_trees):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_test_synth + np.random.randn(len(y_test_synth)) * 0.1,  # slight jitter\n",
    "        y=individual_preds[:, tree_idx],\n",
    "        mode='markers',\n",
    "        name=f'Tree {tree_idx + 1}',\n",
    "        marker=dict(size=5, opacity=0.5),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "# Add ensemble predictions (larger markers)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=y_test_synth,\n",
    "    y=ensemble_preds,\n",
    "    mode='markers',\n",
    "    name='Ensemble Average',\n",
    "    marker=dict(size=12, color='red', symbol='diamond', line=dict(width=1, color='black'))\n",
    "))\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(y_test_synth.min(), individual_preds.min())\n",
    "max_val = max(y_test_synth.max(), individual_preds.max())\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[min_val, max_val],\n",
    "    y=[min_val, max_val],\n",
    "    mode='lines',\n",
    "    name='Perfect Prediction',\n",
    "    line=dict(color='gray', dash='dash', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Individual Tree Predictions vs Ensemble ({n_trees} trees)\",\n",
    "    xaxis_title=\"Actual Value\",\n",
    "    yaxis_title=\"Predicted Value\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print statistics\n",
    "tree_mses = np.mean((individual_preds - y_test_synth.reshape(-1, 1))**2, axis=0)\n",
    "ensemble_mse = np.mean((ensemble_preds - y_test_synth)**2)\n",
    "print(f\"Individual Tree MSE: mean={tree_mses.mean():.3f}, std={tree_mses.std():.3f}\")\n",
    "print(f\"Ensemble MSE: {ensemble_mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5812a",
   "metadata": {},
   "source": [
    "### 3.4 Ensemble Prediction vs Actual with Error Bars\n",
    "\n",
    "**Intuition**: The error bars represent the standard deviation of predictions across all trees in the ensemble. Narrow error bars indicate high agreement among trees (confident prediction), while wide bars suggest uncertainty. This is a form of uncertainty quantification inherent to ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction uncertainty (std across trees)\n",
    "pred_std = individual_preds.std(axis=1)\n",
    "\n",
    "# Sort by actual value for cleaner visualization\n",
    "sort_idx = np.argsort(y_test_synth)\n",
    "y_sorted = y_test_synth[sort_idx]\n",
    "pred_sorted = ensemble_preds[sort_idx]\n",
    "std_sorted = pred_std[sort_idx]\n",
    "\n",
    "# Create figure with error bars\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add perfect prediction line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(y_sorted))),\n",
    "    y=y_sorted,\n",
    "    mode='lines+markers',\n",
    "    name='Actual',\n",
    "    line=dict(color='blue', width=2),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "# Add predictions with error bars\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(y_sorted))),\n",
    "    y=pred_sorted,\n",
    "    mode='markers',\n",
    "    name='Ensemble Prediction',\n",
    "    marker=dict(size=10, color='red', symbol='diamond'),\n",
    "    error_y=dict(\n",
    "        type='data',\n",
    "        array=std_sorted,\n",
    "        visible=True,\n",
    "        color='rgba(255, 0, 0, 0.5)',\n",
    "        thickness=1.5,\n",
    "        width=4\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Ensemble Predictions with Uncertainty Intervals (±1 std)\",\n",
    "    xaxis_title=\"Sample Index (sorted by actual value)\",\n",
    "    yaxis_title=\"Value\",\n",
    "    template=\"plotly_white\",\n",
    "    height=450,\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Additional: Prediction residuals\n",
    "residuals = ensemble_preds - y_test_synth\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=y_test_synth,\n",
    "    y=residuals,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=pred_std,\n",
    "        colorscale='RdYlGn_r',\n",
    "        colorbar=dict(title=\"Prediction<br>Uncertainty\"),\n",
    "        showscale=True\n",
    "    ),\n",
    "    text=[f\"Uncertainty: {s:.2f}\" for s in pred_std],\n",
    "    hovertemplate=\"Actual: %{x:.2f}<br>Residual: %{y:.2f}<br>%{text}<extra></extra>\"\n",
    "))\n",
    "\n",
    "fig2.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=\"Residual Plot (colored by prediction uncertainty)\",\n",
    "    xaxis_title=\"Actual Value\",\n",
    "    yaxis_title=\"Residual (Predicted - Actual)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3064d7d",
   "metadata": {},
   "source": [
    "## 4. Sktime Implementation\n",
    "\n",
    "Now let's use the production-ready `TimeSeriesForestRegressor` from sktime and compare it with our low-level implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sktime.datasets import load_basic_motions, load_unit_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_unit_test(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_unit_test(split=\"test\", return_X_y=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.regression.interval_based import TimeSeriesForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = TimeSeriesForestRegressor(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda736a",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways\n",
    "\n",
    "### Algorithm Summary\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Interval Sampling** | Randomly select $K$ subsequences $[a_k, b_k]$ from each time series |\n",
    "| **Feature Extraction** | Compute mean $\\mu$, std $\\sigma$, and slope $\\beta$ for each interval |\n",
    "| **Feature Vector** | $\\phi(x) \\in \\mathbb{R}^{3K}$ concatenates all interval features |\n",
    "| **Ensemble** | $B$ decision trees trained on interval features |\n",
    "| **Prediction** | Average of tree predictions: $\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B} T_b(\\phi(x))$ |\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- ✅ **Interpretable features**: Mean, std, slope have clear meanings\n",
    "- ✅ **Captures multi-scale patterns**: Random intervals span different lengths\n",
    "- ✅ **Efficient**: Linear time feature extraction, no expensive distance computations\n",
    "- ✅ **Uncertainty estimation**: Tree variance provides confidence intervals\n",
    "- ✅ **Robust**: Ensemble averaging reduces overfitting\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- ⚠️ **Fixed statistics**: Only mean, std, slope may miss complex patterns\n",
    "- ⚠️ **Random intervals**: May miss important specific subsequences\n",
    "- ⚠️ **No explicit temporal ordering**: Features don't preserve interval order\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "| Parameter | Typical Range | Effect |\n",
    "|-----------|--------------|--------|\n",
    "| `n_estimators` | 100-500 | More trees → lower variance, higher compute |\n",
    "| `n_intervals` | $\\sqrt{T}$ to $T$ | More intervals → richer representation |\n",
    "| `min_interval_length` | 3-10 | Minimum feature resolution |\n",
    "| `max_depth` | None or 5-20 | Controls tree complexity |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}