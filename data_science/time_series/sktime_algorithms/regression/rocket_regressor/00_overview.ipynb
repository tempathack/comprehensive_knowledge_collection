{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RocketRegressor\n",
    "\n",
    "ROCKET (RandOm Convolutional KErnel Transform) features can be used for time series regression by pairing the transform with a linear regressor (e.g., Ridge). This approach achieves state-of-the-art accuracy with remarkable computational efficiency.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Random Kernels**: Generate thousands of random convolutional kernels with varying lengths, weights, biases, dilations, and paddings\n",
    "- **Feature Extraction**: Apply each kernel to time series, extracting PPV (proportion of positive values) and max pooling features\n",
    "- **Linear Regression**: Use Ridge regression on the extracted features to predict continuous targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d5062",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### 1. Random Kernel Convolution\n",
    "\n",
    "Each kernel produces a convolution output $z$ by sliding across the input time series $x$:\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{l-1} w_j \\cdot x_{i \\cdot d + j} + b$$\n",
    "\n",
    "Where:\n",
    "- $w_j$ are the kernel weights\n",
    "- $l$ is the kernel length\n",
    "- $d$ is the dilation factor (spacing between elements)\n",
    "- $b$ is the bias term\n",
    "\n",
    "### 2. Kernel Parameter Distributions\n",
    "\n",
    "ROCKET generates kernels with random parameters:\n",
    "\n",
    "| Parameter | Distribution |\n",
    "|-----------|-------------|\n",
    "| Weights | $w \\sim \\mathcal{N}(0, 1)$ (standard normal) |\n",
    "| Length | $l \\in \\{7, 9, 11\\}$ (uniform random choice) |\n",
    "| Bias | $b \\sim \\mathcal{U}(-1, 1)$ (uniform) |\n",
    "| Dilation | $d = \\lfloor 2^{\\mathcal{U}(0, \\log_2(\\frac{T-1}{l-1}))} \\rfloor$ |\n",
    "| Padding | Valid or same (random choice) |\n",
    "\n",
    "### 3. Feature Extraction\n",
    "\n",
    "For each kernel convolution output $z$, ROCKET extracts two features:\n",
    "\n",
    "**Proportion of Positive Values (PPV)**:\n",
    "$$\\text{PPV}(z) = \\frac{1}{T}\\sum_{t=1}^{T} \\mathbf{1}_{z_t > 0}$$\n",
    "\n",
    "**Maximum Value**:\n",
    "$$\\text{Max}(z) = \\max_{t} z_t$$\n",
    "\n",
    "This gives $2 \\times n_{kernels}$ features per time series.\n",
    "\n",
    "### 4. Ridge Regression\n",
    "\n",
    "The final prediction uses Ridge regression on extracted features:\n",
    "\n",
    "$$\\hat{y} = X\\beta$$\n",
    "\n",
    "Where the coefficients are computed as:\n",
    "\n",
    "$$\\beta = (X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "- $X$ is the feature matrix (n_samples × n_features)\n",
    "- $\\lambda$ is the regularization parameter\n",
    "- $I$ is the identity matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sktime.datasets import load_basic_motions, load_unit_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_unit_test(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_unit_test(split=\"test\", return_X_y=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = make_pipeline(Rocket(num_kernels=10_000, random_state=42), Ridge())\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145eb1a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Low-Level NumPy Implementation\n",
    "\n",
    "Let's build ROCKET from scratch to understand how it works internally. This implementation focuses on clarity over optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kernels(n_kernels: int, series_length: int, random_state: int = 42) -> list:\n",
    "    \"\"\"\n",
    "    Generate random convolutional kernels for ROCKET.\n",
    "    \n",
    "    Each kernel is a dictionary containing:\n",
    "    - weights: kernel weights drawn from N(0,1)\n",
    "    - length: kernel length (7, 9, or 11)\n",
    "    - bias: bias term from U(-1, 1)\n",
    "    - dilation: dilation factor for convolution\n",
    "    - padding: amount of zero-padding\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_kernels : int\n",
    "        Number of kernels to generate\n",
    "    series_length : int\n",
    "        Length of input time series (needed for dilation calculation)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        List of kernel parameter dictionaries\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    kernels = []\n",
    "    \n",
    "    candidate_lengths = [7, 9, 11]\n",
    "    \n",
    "    for _ in range(n_kernels):\n",
    "        # Random kernel length\n",
    "        length = np.random.choice(candidate_lengths)\n",
    "        \n",
    "        # Random weights from standard normal\n",
    "        weights = np.random.randn(length)\n",
    "        # Mean-center the weights (as in original ROCKET)\n",
    "        weights = weights - weights.mean()\n",
    "        \n",
    "        # Random bias from uniform distribution\n",
    "        bias = np.random.uniform(-1, 1)\n",
    "        \n",
    "        # Random dilation - ensures kernel can \"see\" entire series\n",
    "        max_dilation = max(1, (series_length - 1) // (length - 1))\n",
    "        dilation = 2 ** np.random.uniform(0, np.log2(max_dilation + 1))\n",
    "        dilation = int(dilation)\n",
    "        \n",
    "        # Random padding (valid or same)\n",
    "        use_padding = np.random.choice([True, False])\n",
    "        if use_padding:\n",
    "            padding = ((length - 1) * dilation) // 2\n",
    "        else:\n",
    "            padding = 0\n",
    "            \n",
    "        kernels.append({\n",
    "            'weights': weights,\n",
    "            'length': length,\n",
    "            'bias': bias,\n",
    "            'dilation': dilation,\n",
    "            'padding': padding\n",
    "        })\n",
    "    \n",
    "    return kernels\n",
    "\n",
    "# Generate example kernels\n",
    "series_length = 24  # Example series length\n",
    "kernels = generate_kernels(n_kernels=100, series_length=series_length)\n",
    "\n",
    "print(f\"Generated {len(kernels)} kernels\")\n",
    "print(f\"\\nExample kernel:\")\n",
    "for key, value in kernels[0].items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"  {key}: {value.round(3)}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d81dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel_to_series(x: np.ndarray, kernel: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a single kernel to a time series using convolution.\n",
    "    \n",
    "    Performs dilated convolution: z_i = sum(w_j * x[i*d + j]) + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input time series of shape (T,)\n",
    "    kernel : dict\n",
    "        Kernel parameters (weights, bias, dilation, padding)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Convolution output\n",
    "    \"\"\"\n",
    "    weights = kernel['weights']\n",
    "    bias = kernel['bias']\n",
    "    dilation = kernel['dilation']\n",
    "    padding = kernel['padding']\n",
    "    \n",
    "    # Apply padding\n",
    "    if padding > 0:\n",
    "        x_padded = np.pad(x, padding, mode='constant', constant_values=0)\n",
    "    else:\n",
    "        x_padded = x\n",
    "    \n",
    "    # Compute effective kernel length with dilation\n",
    "    length = len(weights)\n",
    "    effective_length = (length - 1) * dilation + 1\n",
    "    \n",
    "    # Number of valid positions\n",
    "    n_positions = len(x_padded) - effective_length + 1\n",
    "    \n",
    "    if n_positions <= 0:\n",
    "        return np.array([bias])  # Fallback for very short series\n",
    "    \n",
    "    # Perform dilated convolution\n",
    "    output = np.zeros(n_positions)\n",
    "    for i in range(n_positions):\n",
    "        # Extract dilated receptive field\n",
    "        indices = np.arange(length) * dilation + i\n",
    "        receptive_field = x_padded[indices]\n",
    "        # Compute convolution at this position\n",
    "        output[i] = np.dot(weights, receptive_field) + bias\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_features(X: np.ndarray, kernels: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract ROCKET features from time series dataset.\n",
    "    \n",
    "    For each kernel, extracts two features per series:\n",
    "    1. PPV (Proportion of Positive Values): fraction of output > 0\n",
    "    2. Max: maximum value of convolution output\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Time series data of shape (n_samples, series_length)\n",
    "    kernels : list\n",
    "        List of kernel dictionaries from generate_kernels()\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Feature matrix of shape (n_samples, 2 * n_kernels)\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_kernels = len(kernels)\n",
    "    \n",
    "    # 2 features per kernel: PPV and max\n",
    "    features = np.zeros((n_samples, 2 * n_kernels))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        x = X[i]\n",
    "        for k, kernel in enumerate(kernels):\n",
    "            # Apply convolution\n",
    "            conv_output = apply_kernel_to_series(x, kernel)\n",
    "            \n",
    "            # Extract PPV (proportion of positive values)\n",
    "            ppv = np.mean(conv_output > 0)\n",
    "            features[i, 2*k] = ppv\n",
    "            \n",
    "            # Extract max value\n",
    "            max_val = np.max(conv_output)\n",
    "            features[i, 2*k + 1] = max_val\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Test on a simple example\n",
    "np.random.seed(42)\n",
    "X_example = np.random.randn(5, 24)  # 5 samples, length 24\n",
    "\n",
    "features = extract_features(X_example, kernels)\n",
    "print(f\"Input shape: {X_example.shape}\")\n",
    "print(f\"Output features shape: {features.shape}\")\n",
    "print(f\"Features per sample: {features.shape[1]} (2 × {len(kernels)} kernels)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge_regression(X: np.ndarray, y: np.ndarray, alpha: float = 1.0) -> tuple:\n",
    "    \"\"\"\n",
    "    Fit Ridge regression using the normal equation.\n",
    "    \n",
    "    Solves: β = (X'X + αI)^(-1) X'y\n",
    "    \n",
    "    Using np.linalg.solve for numerical stability instead of explicit inverse.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target values of shape (n_samples,)\n",
    "    alpha : float\n",
    "        Regularization strength (λ in the formula)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (coefficients, intercept)\n",
    "    \"\"\"\n",
    "    # Center the target for intercept\n",
    "    y_mean = np.mean(y)\n",
    "    y_centered = y - y_mean\n",
    "    \n",
    "    # Center features (optional but improves numerical stability)\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = X - X_mean\n",
    "    \n",
    "    # Compute X'X + αI\n",
    "    n_features = X.shape[1]\n",
    "    XtX = X_centered.T @ X_centered\n",
    "    regularized = XtX + alpha * np.eye(n_features)\n",
    "    \n",
    "    # Compute X'y\n",
    "    Xty = X_centered.T @ y_centered\n",
    "    \n",
    "    # Solve for coefficients using Cholesky decomposition (more stable)\n",
    "    # (X'X + αI)β = X'y\n",
    "    coefficients = np.linalg.solve(regularized, Xty)\n",
    "    \n",
    "    # Compute intercept: y_mean - X_mean @ coefficients\n",
    "    intercept = y_mean - X_mean @ coefficients\n",
    "    \n",
    "    return coefficients, intercept\n",
    "\n",
    "\n",
    "def predict_ridge(X: np.ndarray, coefficients: np.ndarray, intercept: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make predictions using fitted Ridge coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    coefficients : np.ndarray\n",
    "        Fitted coefficients\n",
    "    intercept : float\n",
    "        Fitted intercept\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predictions\n",
    "    \"\"\"\n",
    "    return X @ coefficients + intercept\n",
    "\n",
    "\n",
    "# Test Ridge regression implementation\n",
    "np.random.seed(42)\n",
    "X_ridge_test = np.random.randn(100, 10)\n",
    "true_coef = np.random.randn(10)\n",
    "y_ridge_test = X_ridge_test @ true_coef + np.random.randn(100) * 0.1\n",
    "\n",
    "coef, intercept = fit_ridge_regression(X_ridge_test, y_ridge_test, alpha=1.0)\n",
    "y_pred = predict_ridge(X_ridge_test, coef, intercept)\n",
    "\n",
    "print(\"Ridge Regression Test:\")\n",
    "print(f\"  R² score: {1 - np.sum((y_ridge_test - y_pred)**2) / np.sum((y_ridge_test - np.mean(y_ridge_test))**2):.4f}\")\n",
    "print(f\"  Mean Absolute Error: {np.mean(np.abs(y_ridge_test - y_pred)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0d5b4",
   "metadata": {},
   "source": [
    "### Complete ROCKET Regressor Pipeline\n",
    "\n",
    "Now let's combine all components into a complete implementation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocketRegressorFromScratch:\n",
    "    \"\"\"\n",
    "    ROCKET Regressor implemented from scratch using NumPy.\n",
    "    \n",
    "    This is an educational implementation that demonstrates the core concepts\n",
    "    of ROCKET for time series regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_kernels: int = 1000, alpha: float = 1.0, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize ROCKET Regressor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_kernels : int\n",
    "            Number of random kernels to generate\n",
    "        alpha : float\n",
    "            Ridge regularization strength\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_kernels = n_kernels\n",
    "        self.alpha = alpha\n",
    "        self.random_state = random_state\n",
    "        self.kernels = None\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fit the ROCKET regressor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Training time series of shape (n_samples, series_length)\n",
    "        y : np.ndarray\n",
    "            Target values of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        series_length = X.shape[1]\n",
    "        \n",
    "        # Generate random kernels\n",
    "        self.kernels = generate_kernels(\n",
    "            self.n_kernels, \n",
    "            series_length, \n",
    "            self.random_state\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(X, self.kernels)\n",
    "        \n",
    "        # Fit Ridge regression\n",
    "        self.coefficients, self.intercept = fit_ridge_regression(\n",
    "            features, y, self.alpha\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions on new time series.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Time series of shape (n_samples, series_length)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predictions of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        features = extract_features(X, self.kernels)\n",
    "        return predict_ridge(features, self.coefficients, self.intercept)\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute R² score.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "# Create synthetic regression data for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "series_length = 50\n",
    "\n",
    "# Generate time series with a pattern related to target\n",
    "t = np.linspace(0, 4*np.pi, series_length)\n",
    "X_synth = np.zeros((n_samples, series_length))\n",
    "y_synth = np.zeros(n_samples)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    freq = 0.5 + np.random.rand() * 2  # Random frequency\n",
    "    amp = 0.5 + np.random.rand() * 2   # Random amplitude\n",
    "    phase = np.random.rand() * 2 * np.pi\n",
    "    X_synth[i] = amp * np.sin(freq * t + phase) + np.random.randn(series_length) * 0.1\n",
    "    y_synth[i] = freq + amp  # Target is sum of frequency and amplitude\n",
    "\n",
    "# Split data\n",
    "split = int(0.8 * n_samples)\n",
    "X_train_synth, X_test_synth = X_synth[:split], X_synth[split:]\n",
    "y_train_synth, y_test_synth = y_synth[:split], y_synth[split:]\n",
    "\n",
    "# Fit our custom implementation\n",
    "rocket_scratch = RocketRegressorFromScratch(n_kernels=500, alpha=1.0)\n",
    "rocket_scratch.fit(X_train_synth, y_train_synth)\n",
    "\n",
    "y_pred_scratch = rocket_scratch.predict(X_test_synth)\n",
    "\n",
    "print(\"Custom ROCKET Regressor Results:\")\n",
    "print(f\"  R² Score: {rocket_scratch.score(X_test_synth, y_test_synth):.4f}\")\n",
    "print(f\"  MAE: {np.mean(np.abs(y_test_synth - y_pred_scratch)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e7fb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plotly Visualizations\n",
    "\n",
    "Let's visualize the key components of ROCKET to build intuition about how it works.\n",
    "\n",
    "### 1. Sample Kernels Visualization\n",
    "\n",
    "**Intuition**: Each kernel has a unique shape determined by its random weights. The diversity of kernel shapes allows ROCKET to capture various patterns in time series - some kernels detect trends, others detect oscillations or spikes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd355020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample kernels\n",
    "fig_kernels = go.Figure()\n",
    "\n",
    "# Select 6 diverse kernels to display\n",
    "sample_indices = [0, 10, 25, 50, 75, 99]\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "for idx, kernel_idx in enumerate(sample_indices):\n",
    "    kernel = rocket_scratch.kernels[kernel_idx]\n",
    "    weights = kernel['weights']\n",
    "    \n",
    "    fig_kernels.add_trace(go.Scatter(\n",
    "        x=list(range(len(weights))),\n",
    "        y=weights,\n",
    "        mode='lines+markers',\n",
    "        name=f\"Kernel {kernel_idx} (len={kernel['length']}, dil={kernel['dilation']})\",\n",
    "        line=dict(color=colors[idx % len(colors)], width=2),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "\n",
    "fig_kernels.update_layout(\n",
    "    title=\"Sample ROCKET Kernels\",\n",
    "    xaxis_title=\"Position\",\n",
    "    yaxis_title=\"Weight Value\",\n",
    "    template=\"plotly_white\",\n",
    "    height=450,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.35,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    ),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"Each kernel has unique weights that detect different patterns in the time series\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.5, y=1.08,\n",
    "            showarrow=False,\n",
    "            font=dict(size=11, color=\"gray\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig_kernels.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3054f",
   "metadata": {},
   "source": [
    "### 2. Convolution Outputs for Different Kernels\n",
    "\n",
    "**Intuition**: When a kernel slides across a time series, it produces high values where the local pattern matches the kernel shape. Different kernels respond to different parts of the series, creating a rich feature representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28197ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convolution outputs for a single time series\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Take one sample time series\n",
    "sample_ts = X_train_synth[0]\n",
    "\n",
    "# Apply 3 different kernels\n",
    "kernel_indices = [0, 25, 75]\n",
    "conv_outputs = []\n",
    "for k_idx in kernel_indices:\n",
    "    conv_out = apply_kernel_to_series(sample_ts, rocket_scratch.kernels[k_idx])\n",
    "    conv_outputs.append(conv_out)\n",
    "\n",
    "# Create subplot\n",
    "fig_conv = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=(\"Input Time Series\", \"Convolution Outputs from Different Kernels\"),\n",
    "    row_heights=[0.35, 0.65],\n",
    "    vertical_spacing=0.12\n",
    ")\n",
    "\n",
    "# Plot original time series\n",
    "fig_conv.add_trace(\n",
    "    go.Scatter(x=list(range(len(sample_ts))), y=sample_ts, \n",
    "               mode='lines', name='Input Series',\n",
    "               line=dict(color='black', width=2)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Plot convolution outputs\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "for idx, (k_idx, conv_out) in enumerate(zip(kernel_indices, conv_outputs)):\n",
    "    kernel = rocket_scratch.kernels[k_idx]\n",
    "    ppv = np.mean(conv_out > 0)\n",
    "    max_val = np.max(conv_out)\n",
    "    \n",
    "    fig_conv.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(conv_out))), \n",
    "            y=conv_out,\n",
    "            mode='lines',\n",
    "            name=f\"Kernel {k_idx} (PPV={ppv:.2f}, Max={max_val:.2f})\",\n",
    "            line=dict(color=colors[idx], width=1.5)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Add zero line to show PPV threshold\n",
    "fig_conv.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", \n",
    "                   annotation_text=\"PPV threshold (y=0)\", row=2, col=1)\n",
    "\n",
    "fig_conv.update_layout(\n",
    "    height=550,\n",
    "    template=\"plotly_white\",\n",
    "    title=\"How Kernels Transform Time Series\",\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.25, xanchor=\"center\", x=0.5)\n",
    ")\n",
    "\n",
    "fig_conv.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "fig_conv.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
    "fig_conv.update_yaxes(title_text=\"Convolution Output\", row=2, col=1)\n",
    "\n",
    "fig_conv.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec507a",
   "metadata": {},
   "source": [
    "### 3. Feature Importance from Ridge Coefficients\n",
    "\n",
    "**Intuition**: Ridge regression assigns coefficients to each feature. Large absolute coefficients indicate features that strongly influence predictions. We can analyze which kernels (and which feature type - PPV vs Max) are most important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from Ridge coefficients\n",
    "coefficients = rocket_scratch.coefficients\n",
    "\n",
    "# Separate PPV and Max coefficients\n",
    "ppv_coefs = coefficients[0::2]  # Even indices are PPV\n",
    "max_coefs = coefficients[1::2]  # Odd indices are Max\n",
    "\n",
    "# Create importance visualization\n",
    "fig_importance = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"PPV Feature Coefficients\", \"Max Feature Coefficients\"),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Sort by absolute value for visualization\n",
    "ppv_sorted_idx = np.argsort(np.abs(ppv_coefs))[::-1][:30]  # Top 30\n",
    "max_sorted_idx = np.argsort(np.abs(max_coefs))[::-1][:30]\n",
    "\n",
    "fig_importance.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f\"K{i}\" for i in ppv_sorted_idx],\n",
    "        y=ppv_coefs[ppv_sorted_idx],\n",
    "        marker_color=['#2ecc71' if c > 0 else '#e74c3c' for c in ppv_coefs[ppv_sorted_idx]],\n",
    "        name='PPV'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig_importance.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f\"K{i}\" for i in max_sorted_idx],\n",
    "        y=max_coefs[max_sorted_idx],\n",
    "        marker_color=['#3498db' if c > 0 else '#e67e22' for c in max_coefs[max_sorted_idx]],\n",
    "        name='Max'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig_importance.update_layout(\n",
    "    height=400,\n",
    "    template=\"plotly_white\",\n",
    "    title=\"Top 30 Most Important Features by Ridge Coefficient Magnitude\",\n",
    "    showlegend=False,\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"Green/Blue = positive contribution, Red/Orange = negative contribution\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.5, y=-0.15,\n",
    "            showarrow=False,\n",
    "            font=dict(size=10, color=\"gray\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig_importance.update_xaxes(title_text=\"Kernel Index\", tickangle=45)\n",
    "fig_importance.update_yaxes(title_text=\"Coefficient\", row=1, col=1)\n",
    "\n",
    "fig_importance.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nFeature Importance Summary:\")\n",
    "print(f\"  Mean |PPV coefficient|: {np.mean(np.abs(ppv_coefs)):.4f}\")\n",
    "print(f\"  Mean |Max coefficient|: {np.mean(np.abs(max_coefs)):.4f}\")\n",
    "print(f\"  Max |PPV coefficient|: {np.max(np.abs(ppv_coefs)):.4f}\")\n",
    "print(f\"  Max |Max coefficient|: {np.max(np.abs(max_coefs)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcf24b",
   "metadata": {},
   "source": [
    "### 4. Predicted vs Actual Scatter Plot\n",
    "\n",
    "**Intuition**: A perfect regressor would have all points on the diagonal line (y = x). Points close to the diagonal indicate accurate predictions. The spread around the line shows prediction uncertainty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predicted vs actual plot\n",
    "fig_pred = go.Figure()\n",
    "\n",
    "# Scatter plot of predictions\n",
    "fig_pred.add_trace(go.Scatter(\n",
    "    x=y_test_synth,\n",
    "    y=y_pred_scratch,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=np.abs(y_test_synth - y_pred_scratch),  # Color by error\n",
    "        colorscale='RdYlGn_r',\n",
    "        colorbar=dict(title=\"Abs Error\"),\n",
    "        line=dict(width=1, color='white')\n",
    "    ),\n",
    "    name='Predictions',\n",
    "    text=[f\"Actual: {a:.2f}<br>Pred: {p:.2f}<br>Error: {a-p:.2f}\" \n",
    "          for a, p in zip(y_test_synth, y_pred_scratch)],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Perfect prediction line\n",
    "min_val = min(y_test_synth.min(), y_pred_scratch.min())\n",
    "max_val = max(y_test_synth.max(), y_pred_scratch.max())\n",
    "margin = (max_val - min_val) * 0.1\n",
    "\n",
    "fig_pred.add_trace(go.Scatter(\n",
    "    x=[min_val - margin, max_val + margin],\n",
    "    y=[min_val - margin, max_val + margin],\n",
    "    mode='lines',\n",
    "    line=dict(color='black', dash='dash', width=2),\n",
    "    name='Perfect Prediction (y=x)'\n",
    "))\n",
    "\n",
    "# Add regression line through predictions\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(y_test_synth, y_pred_scratch)\n",
    "x_line = np.linspace(min_val - margin, max_val + margin, 100)\n",
    "y_line = slope * x_line + intercept\n",
    "\n",
    "fig_pred.add_trace(go.Scatter(\n",
    "    x=x_line,\n",
    "    y=y_line,\n",
    "    mode='lines',\n",
    "    line=dict(color='#3498db', width=2),\n",
    "    name=f'Fitted Line (R²={r_value**2:.3f})'\n",
    "))\n",
    "\n",
    "fig_pred.update_layout(\n",
    "    title=\"Predicted vs Actual Values\",\n",
    "    xaxis_title=\"Actual Value\",\n",
    "    yaxis_title=\"Predicted Value\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.2,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    ),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=f\"R² = {r_value**2:.4f} | MAE = {np.mean(np.abs(y_test_synth - y_pred_scratch)):.4f}\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.02, y=0.98,\n",
    "            showarrow=False,\n",
    "            font=dict(size=12),\n",
    "            bgcolor=\"white\",\n",
    "            bordercolor=\"gray\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Make axes equal\n",
    "fig_pred.update_xaxes(range=[min_val - margin, max_val + margin])\n",
    "fig_pred.update_yaxes(range=[min_val - margin, max_val + margin], scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig_pred.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652b994",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **ROCKET is computationally efficient**: Random kernels eliminate the need for expensive feature engineering or kernel search\n",
    "2. **Two features per kernel**: PPV captures \"how often\" the pattern matches, Max captures \"how strongly\" it matches\n",
    "3. **Ridge regression provides regularization**: Prevents overfitting despite having thousands of features\n",
    "4. **Scalability**: Linear complexity in both number of kernels and time series length\n",
    "\n",
    "### When to Use ROCKET Regressor\n",
    "\n",
    "| Use Case | Recommendation |\n",
    "|----------|----------------|\n",
    "| Fast baseline for time series regression | ✅ Excellent choice |\n",
    "| Limited training data | ✅ Good generalization with regularization |\n",
    "| Interpretability required | ⚠️ Kernel coefficients provide some insight |\n",
    "| Real-time predictions needed | ✅ Fast inference |\n",
    "| Very long time series (>10,000 points) | ⚠️ Consider MiniRocket for efficiency |\n",
    "\n",
    "### References\n",
    "\n",
    "- Dempster, A., Petitjean, F., & Webb, G. I. (2020). ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. *Data Mining and Knowledge Discovery*, 34(5), 1454-1495.\n",
    "- [sktime ROCKET Documentation](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.transformations.panel.rocket.Rocket.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
