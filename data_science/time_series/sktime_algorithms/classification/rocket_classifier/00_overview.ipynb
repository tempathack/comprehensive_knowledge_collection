{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RocketClassifier\n",
    "\n",
    "ROCKET (RandOm Convolutional KErnel Transform) turns a time series into a high‑dimensional feature vector using many random convolutional kernels. A fast linear classifier on these features often achieves strong accuracy with minimal tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c893ff8a",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "ROCKET transforms time series into a rich feature space using **random convolutional kernels**. Understanding the math reveals why this simple idea works so well.\n",
    "\n",
    "### Random Convolutional Kernel\n",
    "\n",
    "A kernel $k$ of length $l$ convolves with a time series $x$ to produce an output:\n",
    "\n",
    "$$k(t) = w \\cdot x_{t:t+l}$$\n",
    "\n",
    "where the weights $w \\sim \\mathcal{N}(0, 1)$ are drawn from a standard normal distribution.\n",
    "\n",
    "### Kernel Parameters\n",
    "\n",
    "Each random kernel is defined by:\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| **Length** $l$ | Number of weights in kernel | $l \\in \\{7, 9, 11\\}$ (sampled uniformly) |\n",
    "| **Dilation** $d$ | Spacing between input elements | $d \\in \\{1, 2, ..., \\lfloor \\frac{T-1}{l-1} \\rfloor\\}$ |\n",
    "| **Bias** $b$ | Additive constant | $b \\sim \\mathcal{U}(-1, 1)$ |\n",
    "| **Padding** | Zero-padding at boundaries | Random choice |\n",
    "\n",
    "### Convolution Operation\n",
    "\n",
    "For a kernel with weights $w = [w_0, w_1, ..., w_{l-1}]$ and dilation $d$, the convolution output at position $i$ is:\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{l-1} w_j \\cdot x_{i \\cdot d + j} + b$$\n",
    "\n",
    "The dilation parameter allows the kernel to capture patterns at different temporal scales:\n",
    "- **Small dilation** ($d=1$): Captures fine-grained local patterns\n",
    "- **Large dilation** ($d=8$): Captures patterns spanning wider time intervals\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "ROCKET extracts **two features** from each kernel's convolution output $z$:\n",
    "\n",
    "#### 1. Proportion of Positive Values (PPV)\n",
    "\n",
    "$$\\text{PPV}(z) = \\frac{1}{T}\\sum_{t=1}^{T} \\mathbf{1}_{z_t > 0}$$\n",
    "\n",
    "**Intuition**: PPV measures \"how often\" the pattern matched. A high PPV means the time series frequently exhibits the pattern captured by this kernel.\n",
    "\n",
    "#### 2. Maximum Value (Max Pooling)\n",
    "\n",
    "$$\\text{max}(z) = \\max_t z_t$$\n",
    "\n",
    "**Intuition**: The max captures the \"best match\" — how strongly the pattern appeared at its peak location.\n",
    "\n",
    "With $N$ kernels, ROCKET produces $2N$ features (PPV + max for each kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd41dbc",
   "metadata": {},
   "source": [
    "## Visualizing Random Kernels\n",
    "\n",
    "Let's build intuition by visualizing what random kernels look like and how they interact with time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample random kernels with different lengths\n",
    "kernel_lengths = [7, 9, 11]\n",
    "kernels = []\n",
    "for length in kernel_lengths:\n",
    "    weights = np.random.randn(length)\n",
    "    kernels.append(weights)\n",
    "\n",
    "# Visualize kernel shapes\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=[f\"Kernel (length={l})\" for l in kernel_lengths])\n",
    "\n",
    "for i, (kernel, length) in enumerate(zip(kernels, kernel_lengths)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(length)),\n",
    "            y=kernel,\n",
    "            mode='lines+markers',\n",
    "            name=f'L={length}',\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", opacity=0.5, row=1, col=i+1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Random Convolutional Kernels (weights ~ N(0,1))\",\n",
    "    height=350,\n",
    "    showlegend=False,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Position\")\n",
    "fig.update_yaxes(title_text=\"Weight\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef7616",
   "metadata": {},
   "source": [
    "### Convolution Output Visualization\n",
    "\n",
    "When we convolve a kernel with a time series, we get an output signal. Let's see how different kernels respond to a sample time series containing distinct patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample time series with distinct patterns\n",
    "T = 100\n",
    "t = np.linspace(0, 4*np.pi, T)\n",
    "time_series = np.sin(t) + 0.5 * np.sin(3*t) + 0.2 * np.random.randn(T)\n",
    "\n",
    "# Apply convolution with one kernel (dilation=1, bias=0)\n",
    "def convolve_with_kernel(x, kernel, dilation=1, bias=0):\n",
    "    \"\"\"Apply a single kernel to a time series with given dilation.\"\"\"\n",
    "    l = len(kernel)\n",
    "    # Calculate effective kernel span\n",
    "    effective_length = (l - 1) * dilation + 1\n",
    "    output_length = len(x) - effective_length + 1\n",
    "    \n",
    "    if output_length <= 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    output = np.zeros(output_length)\n",
    "    for i in range(output_length):\n",
    "        indices = np.arange(l) * dilation + i\n",
    "        output[i] = np.sum(kernel * x[indices]) + bias\n",
    "    return output\n",
    "\n",
    "# Apply different kernels with different dilations\n",
    "kernel = kernels[0]  # Length 7 kernel\n",
    "dilations = [1, 2, 4]\n",
    "outputs = [convolve_with_kernel(time_series, kernel, d, bias=0) for d in dilations]\n",
    "\n",
    "# Visualization\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    subplot_titles=[\"Original Time Series\", \n",
    "                    \"Convolution Output (dilation=1)\",\n",
    "                    \"Convolution Output (dilation=2)\", \n",
    "                    \"Convolution Output (dilation=4)\"],\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# Original time series\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(T)), y=time_series, mode='lines', \n",
    "               name='Input', line=dict(color='blue', width=2)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Convolution outputs\n",
    "colors = ['green', 'orange', 'red']\n",
    "for i, (output, dilation) in enumerate(zip(outputs, dilations)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(range(len(output))), y=output, mode='lines',\n",
    "                   name=f'd={dilation}', line=dict(color=colors[i], width=2)),\n",
    "        row=i+2, col=1\n",
    "    )\n",
    "    # Add zero line and highlight positive regions\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", opacity=0.5, row=i+2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Kernel Convolution at Different Dilations\",\n",
    "    height=700,\n",
    "    showlegend=True,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3115c1",
   "metadata": {},
   "source": [
    "### PPV and Max Feature Distribution\n",
    "\n",
    "The PPV (Proportion of Positive Values) and Max features extracted from convolution outputs form the feature space for classification. Let's visualize how these features differ across time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple random time series of two classes\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "T = 100\n",
    "\n",
    "# Class A: Sine-dominant patterns\n",
    "class_a = [np.sin(np.linspace(0, 4*np.pi, T)) + 0.3*np.random.randn(T) for _ in range(n_samples//2)]\n",
    "# Class B: Sawtooth-like patterns  \n",
    "class_b = [np.sign(np.sin(np.linspace(0, 4*np.pi, T))) + 0.3*np.random.randn(T) for _ in range(n_samples//2)]\n",
    "\n",
    "all_series = class_a + class_b\n",
    "labels = ['Class A']*len(class_a) + ['Class B']*len(class_b)\n",
    "\n",
    "# Generate random kernels and compute features\n",
    "n_kernels = 50\n",
    "kernels_for_viz = []\n",
    "for _ in range(n_kernels):\n",
    "    length = np.random.choice([7, 9, 11])\n",
    "    weights = np.random.randn(length)\n",
    "    dilation = np.random.randint(1, 5)\n",
    "    bias = np.random.uniform(-1, 1)\n",
    "    kernels_for_viz.append({'weights': weights, 'dilation': dilation, 'bias': bias})\n",
    "\n",
    "# Extract PPV and Max features for all series\n",
    "def extract_features(series, kernels):\n",
    "    ppvs = []\n",
    "    maxs = []\n",
    "    for k in kernels:\n",
    "        output = convolve_with_kernel(series, k['weights'], k['dilation'], k['bias'])\n",
    "        if len(output) > 0:\n",
    "            ppv = np.mean(output > 0)\n",
    "            max_val = np.max(output)\n",
    "        else:\n",
    "            ppv, max_val = 0.5, 0\n",
    "        ppvs.append(ppv)\n",
    "        maxs.append(max_val)\n",
    "    return ppvs, maxs\n",
    "\n",
    "all_ppvs = []\n",
    "all_maxs = []\n",
    "for series in all_series:\n",
    "    ppvs, maxs = extract_features(series, kernels_for_viz)\n",
    "    all_ppvs.append(ppvs)\n",
    "    all_maxs.append(maxs)\n",
    "\n",
    "all_ppvs = np.array(all_ppvs)\n",
    "all_maxs = np.array(all_maxs)\n",
    "\n",
    "# Visualize PPV distribution for first kernel\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"PPV Feature (Kernel 1)\", \"Max Feature (Kernel 1)\"])\n",
    "\n",
    "# PPV histogram by class\n",
    "for i, cls in enumerate(['Class A', 'Class B']):\n",
    "    mask = np.array(labels) == cls\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=all_ppvs[mask, 0], name=cls, opacity=0.7, nbinsx=20),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=all_maxs[mask, 0], name=cls, opacity=0.7, nbinsx=20, showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Distributions by Class (Single Kernel)\",\n",
    "    height=400,\n",
    "    barmode='overlay',\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_xaxes(title_text=\"PPV Value\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Max Value\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243eee47",
   "metadata": {},
   "source": [
    "### Classification Decision Boundary\n",
    "\n",
    "With many kernels, ROCKET creates a high-dimensional feature space. A linear classifier finds hyperplanes to separate classes. Let's visualize this using PCA to reduce to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83779d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "# Combine PPV and Max features\n",
    "all_features = np.hstack([all_ppvs, all_maxs])\n",
    "y = np.array([0 if l == 'Class A' else 1 for l in labels])\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(all_features)\n",
    "\n",
    "# Fit classifier in 2D space\n",
    "clf_2d = RidgeClassifierCV()\n",
    "clf_2d.fit(features_2d, y)\n",
    "\n",
    "# Create decision boundary mesh\n",
    "x_min, x_max = features_2d[:, 0].min() - 1, features_2d[:, 0].max() + 1\n",
    "y_min, y_max = features_2d[:, 1].min() - 1, features_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "Z = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Decision boundary contour\n",
    "fig.add_trace(go.Contour(\n",
    "    x=np.linspace(x_min, x_max, 100),\n",
    "    y=np.linspace(y_min, y_max, 100),\n",
    "    z=Z,\n",
    "    showscale=False,\n",
    "    colorscale=[[0, 'rgba(66, 133, 244, 0.3)'], [1, 'rgba(234, 67, 53, 0.3)']],\n",
    "    contours=dict(showlines=False),\n",
    "    name='Decision Boundary'\n",
    "))\n",
    "\n",
    "# Scatter points\n",
    "for i, (cls, color, symbol) in enumerate([('Class A', 'blue', 'circle'), ('Class B', 'red', 'diamond')]):\n",
    "    mask = y == i\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=features_2d[mask, 0],\n",
    "        y=features_2d[mask, 1],\n",
    "        mode='markers',\n",
    "        name=cls,\n",
    "        marker=dict(color=color, size=10, symbol=symbol, line=dict(width=1, color='white'))\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"ROCKET Features in 2D (PCA) with Decision Boundary<br><sub>Accuracy: {clf_2d.score(features_2d, y):.1%}</sub>\",\n",
    "    xaxis_title=\"PC1\",\n",
    "    yaxis_title=\"PC2\",\n",
    "    height=500,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf5efd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Low-Level NumPy Implementation\n",
    "\n",
    "Understanding ROCKET deeply requires implementing it from scratch. Below we build a complete ROCKET classifier using only NumPy and a simple linear classifier.\n",
    "\n",
    "### Step 1: Generate Random Kernels\n",
    "\n",
    "Each kernel has randomly sampled:\n",
    "- **Weights**: Drawn from $\\mathcal{N}(0, 1)$\n",
    "- **Length**: Chosen from $\\{7, 9, 11\\}$\n",
    "- **Dilation**: Exponentially distributed to cover multiple time scales\n",
    "- **Bias**: Uniform on $[-1, 1]$\n",
    "- **Padding**: Random boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_kernels(n_kernels: int, max_length: int = 11, random_state: int = None) -> list:\n",
    "    \"\"\"\n",
    "    Generate random convolutional kernels for ROCKET.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_kernels : int\n",
    "        Number of kernels to generate.\n",
    "    max_length : int\n",
    "        Maximum kernel length. Lengths are sampled from {7, 9, 11} ∩ [1, max_length].\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    kernels : list of dict\n",
    "        Each kernel contains: weights, length, dilation, bias, padding.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Possible kernel lengths\n",
    "    candidate_lengths = np.array([7, 9, 11])\n",
    "    lengths = candidate_lengths[candidate_lengths <= max_length]\n",
    "    if len(lengths) == 0:\n",
    "        lengths = np.array([max_length])\n",
    "    \n",
    "    kernels = []\n",
    "    for _ in range(n_kernels):\n",
    "        # Random length\n",
    "        length = np.random.choice(lengths)\n",
    "        \n",
    "        # Random weights ~ N(0, 1)\n",
    "        weights = np.random.randn(length)\n",
    "        \n",
    "        # Random dilation (exponentially distributed to cover multiple scales)\n",
    "        # Max dilation ensures kernel fits in typical time series\n",
    "        max_dilation = 32  # Configurable\n",
    "        dilation = 2 ** np.random.uniform(0, np.log2(max_dilation + 1))\n",
    "        dilation = int(np.floor(dilation))\n",
    "        dilation = max(1, dilation)\n",
    "        \n",
    "        # Random bias ~ Uniform(-1, 1)\n",
    "        bias = np.random.uniform(-1, 1)\n",
    "        \n",
    "        # Random padding (True/False)\n",
    "        padding = np.random.choice([True, False])\n",
    "        \n",
    "        kernels.append({\n",
    "            'weights': weights,\n",
    "            'length': length,\n",
    "            'dilation': dilation,\n",
    "            'bias': bias,\n",
    "            'padding': padding\n",
    "        })\n",
    "    \n",
    "    return kernels\n",
    "\n",
    "\n",
    "# Example: Generate and inspect 5 kernels\n",
    "example_kernels = generate_random_kernels(5, random_state=42)\n",
    "for i, k in enumerate(example_kernels):\n",
    "    print(f\"Kernel {i+1}: length={k['length']}, dilation={k['dilation']}, \"\n",
    "          f\"bias={k['bias']:.3f}, padding={k['padding']}\")\n",
    "    print(f\"  weights: {k['weights'].round(3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753fca9",
   "metadata": {},
   "source": [
    "### Step 2: Apply Kernel to Time Series\n",
    "\n",
    "The core operation: convolve a kernel with a time series using the specified dilation. This implements:\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{l-1} w_j \\cdot x_{i \\cdot d + j} + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(X: np.ndarray, kernel: np.ndarray, dilation: int = 1, \n",
    "                  bias: float = 0.0, padding: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a single convolutional kernel to a time series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input time series of shape (T,) or (N, T) for batch processing.\n",
    "    kernel : np.ndarray\n",
    "        Kernel weights of shape (L,).\n",
    "    dilation : int\n",
    "        Dilation factor (spacing between kernel elements).\n",
    "    bias : float\n",
    "        Bias term added to convolution output.\n",
    "    padding : bool\n",
    "        If True, zero-pad the input to maintain output length.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Convolution output.\n",
    "    \"\"\"\n",
    "    # Handle 1D input\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    \n",
    "    N, T = X.shape\n",
    "    L = len(kernel)\n",
    "    \n",
    "    # Effective kernel span with dilation\n",
    "    effective_length = (L - 1) * dilation + 1\n",
    "    \n",
    "    # Calculate padding\n",
    "    if padding:\n",
    "        pad_total = effective_length - 1\n",
    "        pad_left = pad_total // 2\n",
    "        pad_right = pad_total - pad_left\n",
    "        X = np.pad(X, ((0, 0), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "        T = X.shape[1]\n",
    "    \n",
    "    # Output length\n",
    "    output_length = T - effective_length + 1\n",
    "    \n",
    "    if output_length <= 0:\n",
    "        return np.zeros((N, 1))\n",
    "    \n",
    "    # Compute convolution via explicit loop (educational, not optimized)\n",
    "    output = np.zeros((N, output_length))\n",
    "    \n",
    "    for i in range(output_length):\n",
    "        # Gather dilated indices\n",
    "        indices = np.arange(L) * dilation + i\n",
    "        # Dot product with kernel weights\n",
    "        output[:, i] = X[:, indices] @ kernel + bias\n",
    "    \n",
    "    return output.squeeze()\n",
    "\n",
    "\n",
    "# Demo: Apply a kernel to our sample time series\n",
    "demo_kernel = example_kernels[0]\n",
    "demo_output = apply_kernel(\n",
    "    time_series, \n",
    "    demo_kernel['weights'], \n",
    "    demo_kernel['dilation'], \n",
    "    demo_kernel['bias'],\n",
    "    demo_kernel['padding']\n",
    ")\n",
    "\n",
    "print(f\"Input length: {len(time_series)}\")\n",
    "print(f\"Kernel length: {demo_kernel['length']}, dilation: {demo_kernel['dilation']}\")\n",
    "print(f\"Output length: {len(demo_output)}\")\n",
    "print(f\"Output range: [{demo_output.min():.3f}, {demo_output.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b2a83",
   "metadata": {},
   "source": [
    "### Step 3: Extract ROCKET Features\n",
    "\n",
    "From each kernel's convolution output, we extract two features:\n",
    "1. **PPV** (Proportion of Positive Values): $\\text{PPV}(z) = \\frac{1}{T}\\sum_{t=1}^{T} \\mathbf{1}_{z_t > 0}$\n",
    "2. **Max**: $\\text{max}(z) = \\max_t z_t$\n",
    "\n",
    "This gives us $2 \\times n\\_kernels$ features per time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rocket_features(X: np.ndarray, kernels: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract ROCKET features (PPV and Max) from time series using given kernels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input time series of shape (N, T) where N is number of samples.\n",
    "    kernels : list\n",
    "        List of kernel dictionaries from generate_random_kernels().\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features : np.ndarray\n",
    "        Feature matrix of shape (N, 2 * n_kernels).\n",
    "        First n_kernels columns are PPV features, next n_kernels are Max features.\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    n_kernels = len(kernels)\n",
    "    \n",
    "    # Pre-allocate feature arrays\n",
    "    ppv_features = np.zeros((N, n_kernels))\n",
    "    max_features = np.zeros((N, n_kernels))\n",
    "    \n",
    "    for k_idx, kernel in enumerate(kernels):\n",
    "        # Apply kernel to all samples\n",
    "        for sample_idx in range(N):\n",
    "            output = apply_kernel(\n",
    "                X[sample_idx],\n",
    "                kernel['weights'],\n",
    "                kernel['dilation'],\n",
    "                kernel['bias'],\n",
    "                kernel['padding']\n",
    "            )\n",
    "            \n",
    "            if len(output) > 0:\n",
    "                # PPV: proportion of positive values\n",
    "                ppv_features[sample_idx, k_idx] = np.mean(output > 0)\n",
    "                # Max: maximum value\n",
    "                max_features[sample_idx, k_idx] = np.max(output)\n",
    "            else:\n",
    "                ppv_features[sample_idx, k_idx] = 0.5\n",
    "                max_features[sample_idx, k_idx] = 0.0\n",
    "    \n",
    "    # Concatenate PPV and Max features\n",
    "    features = np.hstack([ppv_features, max_features])\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Demo: Extract features from a batch of time series\n",
    "demo_series = np.array(class_a[:5] + class_b[:5])  # 10 samples\n",
    "demo_kernels = generate_random_kernels(100, random_state=123)\n",
    "demo_features = extract_rocket_features(demo_series, demo_kernels)\n",
    "\n",
    "print(f\"Input shape: {demo_series.shape}\")\n",
    "print(f\"Number of kernels: {len(demo_kernels)}\")\n",
    "print(f\"Feature shape: {demo_features.shape}\")\n",
    "print(f\"Features per sample: {demo_features.shape[1]} (100 PPV + 100 Max)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d454c2",
   "metadata": {},
   "source": [
    "### Step 4: Complete ROCKET Classifier\n",
    "\n",
    "Now we combine everything into a complete classifier. We use Ridge regression (a fast linear classifier) on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef339974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleROCKETClassifier:\n",
    "    \"\"\"\n",
    "    A from-scratch ROCKET classifier implementation.\n",
    "    \n",
    "    ROCKET = Random Convolutional Kernel Transform + Linear Classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_kernels : int\n",
    "        Number of random kernels (default: 10,000).\n",
    "    alpha : float\n",
    "        Ridge regularization strength (default: 1.0).\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_kernels: int = 10000, alpha: float = 1.0, random_state: int = None):\n",
    "        self.n_kernels = n_kernels\n",
    "        self.alpha = alpha\n",
    "        self.random_state = random_state\n",
    "        self.kernels_ = None\n",
    "        self.weights_ = None\n",
    "        self.bias_ = None\n",
    "        self.classes_ = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Fit the ROCKET classifier.\"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        # Store classes\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        # Generate random kernels\n",
    "        self.kernels_ = generate_random_kernels(\n",
    "            self.n_kernels, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_rocket_features(X, self.kernels_)\n",
    "        \n",
    "        # Standardize features (important for Ridge)\n",
    "        self.mean_ = features.mean(axis=0)\n",
    "        self.std_ = features.std(axis=0) + 1e-8\n",
    "        features_scaled = (features - self.mean_) / self.std_\n",
    "        \n",
    "        # Encode labels to 0/1 for binary, or one-hot for multiclass\n",
    "        if len(self.classes_) == 2:\n",
    "            y_encoded = (y == self.classes_[1]).astype(float)\n",
    "        else:\n",
    "            # One-hot encoding\n",
    "            y_encoded = np.zeros((len(y), len(self.classes_)))\n",
    "            for i, cls in enumerate(self.classes_):\n",
    "                y_encoded[y == cls, i] = 1\n",
    "        \n",
    "        # Ridge regression: (X^T X + αI)^{-1} X^T y\n",
    "        n_features = features_scaled.shape[1]\n",
    "        XtX = features_scaled.T @ features_scaled\n",
    "        XtY = features_scaled.T @ y_encoded\n",
    "        \n",
    "        # Solve normal equations with regularization\n",
    "        self.weights_ = np.linalg.solve(\n",
    "            XtX + self.alpha * np.eye(n_features),\n",
    "            XtY\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        # Extract and scale features\n",
    "        features = extract_rocket_features(X, self.kernels_)\n",
    "        features_scaled = (features - self.mean_) / self.std_\n",
    "        \n",
    "        # Compute scores\n",
    "        scores = features_scaled @ self.weights_\n",
    "        \n",
    "        if len(self.classes_) == 2:\n",
    "            # Binary: threshold at 0.5\n",
    "            predictions = (scores > 0.5).astype(int)\n",
    "            return self.classes_[predictions.ravel()]\n",
    "        else:\n",
    "            # Multiclass: argmax\n",
    "            return self.classes_[np.argmax(scores, axis=1)]\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93925d88",
   "metadata": {},
   "source": [
    "### Step 5: Test Our Implementation\n",
    "\n",
    "Let's test our from-scratch ROCKET classifier on the synthetic data and visualize the results with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/test split from synthetic data\n",
    "np.random.seed(42)\n",
    "n_train = 80\n",
    "\n",
    "# Shuffle data\n",
    "all_X = np.array(all_series)\n",
    "all_y = np.array([0 if l == 'Class A' else 1 for l in labels])\n",
    "indices = np.random.permutation(len(all_X))\n",
    "all_X, all_y = all_X[indices], all_y[indices]\n",
    "\n",
    "X_train_synth = all_X[:n_train]\n",
    "y_train_synth = all_y[:n_train]\n",
    "X_test_synth = all_X[n_train:]\n",
    "y_test_synth = all_y[n_train:]\n",
    "\n",
    "# Train our from-scratch ROCKET classifier (using fewer kernels for speed)\n",
    "print(\"Training SimpleROCKETClassifier...\")\n",
    "rocket_clf = SimpleROCKETClassifier(n_kernels=500, alpha=1.0, random_state=42)\n",
    "rocket_clf.fit(X_train_synth, y_train_synth)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = rocket_clf.score(X_train_synth, y_train_synth)\n",
    "test_acc = rocket_clf.score(X_test_synth, y_test_synth)\n",
    "\n",
    "print(f\"\\nResults (500 kernels):\")\n",
    "print(f\"  Train Accuracy: {train_acc:.1%}\")\n",
    "print(f\"  Test Accuracy:  {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = rocket_clf.predict(X_test_synth)\n",
    "cm = confusion_matrix(y_test_synth, y_pred)\n",
    "\n",
    "# Create annotated heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Pred: Class A', 'Pred: Class B'],\n",
    "    y=['True: Class A', 'True: Class B'],\n",
    "    colorscale='Blues',\n",
    "    showscale=True,\n",
    "    text=cm,\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 20}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Confusion Matrix (Test Accuracy: {test_acc:.1%})\",\n",
    "    xaxis_title=\"Predicted\",\n",
    "    yaxis_title=\"Actual\",\n",
    "    height=400,\n",
    "    width=500,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc392b",
   "metadata": {},
   "source": [
    "### Comparison: Our Implementation vs. sktime\n",
    "\n",
    "Let's compare our from-scratch implementation with sktime's optimized ROCKET transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create DataFrames for sktime (required format)\n",
    "import pandas as pd\n",
    "\n",
    "def to_sktime_format(X):\n",
    "    \"\"\"Convert numpy array to sktime nested DataFrame format.\"\"\"\n",
    "    n_samples, n_timepoints = X.shape\n",
    "    data = {'dim_0': [pd.Series(X[i]) for i in range(n_samples)]}\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "X_train_sk = to_sktime_format(X_train_synth)\n",
    "X_test_sk = to_sktime_format(X_test_synth)\n",
    "\n",
    "# sktime ROCKET\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Time our implementation\n",
    "start = time.time()\n",
    "our_clf = SimpleROCKETClassifier(n_kernels=500, random_state=42)\n",
    "our_clf.fit(X_train_synth, y_train_synth)\n",
    "our_time = time.time() - start\n",
    "our_acc = our_clf.score(X_test_synth, y_test_synth)\n",
    "\n",
    "# Time sktime implementation\n",
    "start = time.time()\n",
    "sktime_clf = make_pipeline(\n",
    "    Rocket(num_kernels=500, random_state=42),\n",
    "    RidgeClassifierCV()\n",
    ")\n",
    "sktime_clf.fit(X_train_sk, y_train_synth)\n",
    "sktime_time = time.time() - start\n",
    "sktime_acc = sktime_clf.score(X_test_sk, y_test_synth)\n",
    "\n",
    "# Results comparison\n",
    "print(\"=\"*50)\n",
    "print(\"COMPARISON: Our Implementation vs sktime ROCKET\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n{'Metric':<20} {'Ours':<15} {'sktime':<15}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Test Accuracy':<20} {our_acc:<15.1%} {sktime_acc:<15.1%}\")\n",
    "print(f\"{'Training Time':<20} {our_time:<15.3f}s {sktime_time:<15.3f}s\")\n",
    "print(f\"{'Num Kernels':<20} {500:<15} {500:<15}\")\n",
    "print(\"\\nNote: sktime is faster due to Numba JIT compilation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sktime.datasets import load_basic_motions, load_unit_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_basic_motions(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_basic_motions(split=\"test\", return_X_y=True)\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: ROCKET + RidgeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = make_pipeline(\n",
    "    Rocket(num_kernels=10_000, random_state=42),\n",
    "    RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ROCKET works\n",
    "\n",
    "ROCKET creates features that capture local shape patterns at many scales. The linear classifier then finds a decision boundary in this feature space. It is both **fast** and **accurate**, making it a top baseline for time‑series classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}