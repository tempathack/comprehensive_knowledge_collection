{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeriesForestClassifier\n",
    "\n",
    "Time Series Forest uses ensembles of decision trees trained on **interval‑based features** (mean, std, slope) extracted from random sub‑intervals of the series. It is interpretable and strong on many datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sktime.datasets import load_basic_motions, load_unit_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_basic_motions(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_basic_motions(split=\"test\", return_X_y=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = TimeSeriesForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The model implicitly selects informative time intervals. You can inspect feature importance to understand which parts of the series drive classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d005a1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Mathematical Foundation\n",
    "\n",
    "## Core Intuition\n",
    "\n",
    "Time Series Forest (TSF) transforms time series classification into a **tabular classification problem** by extracting statistical features from random sub-intervals. Instead of treating time series as sequences, TSF:\n",
    "1. Samples random intervals from each time series\n",
    "2. Computes simple statistics (mean, std, slope) for each interval\n",
    "3. Trains a random forest on these interval features\n",
    "\n",
    "This approach is both **interpretable** (we know which time intervals matter) and **robust** (ensemble reduces variance).\n",
    "\n",
    "## Random Interval Sampling\n",
    "\n",
    "Given a time series of length $T$, we sample $K$ random intervals. Each interval $k$ is defined by:\n",
    "\n",
    "$$[a_k, b_k] \\subset [1, T] \\quad \\text{where} \\quad 1 \\leq a_k < b_k \\leq T$$\n",
    "\n",
    "The interval start $a_k$ and end $b_k$ are drawn uniformly at random, ensuring a minimum interval length.\n",
    "\n",
    "## Interval Feature Extraction\n",
    "\n",
    "For each interval $[a, b]$, we extract three summary statistics from the time series values $\\{x_a, x_{a+1}, \\ldots, x_b\\}$:\n",
    "\n",
    "### 1. Mean (Location Feature)\n",
    "$$\\bar{x}_{[a,b]} = \\frac{1}{b-a+1}\\sum_{t=a}^{b} x_t$$\n",
    "\n",
    "Captures the **average level** of the signal within the interval.\n",
    "\n",
    "### 2. Standard Deviation (Spread Feature)\n",
    "$$\\sigma_{[a,b]} = \\sqrt{\\frac{1}{b-a}\\sum_{t=a}^{b} (x_t - \\bar{x}_{[a,b]})^2}$$\n",
    "\n",
    "Captures the **variability/volatility** of the signal within the interval.\n",
    "\n",
    "### 3. Slope (Trend Feature)\n",
    "Computed via ordinary least squares regression of $x_t$ on $t$:\n",
    "\n",
    "$$\\beta_{[a,b]} = \\frac{\\sum_{t=a}^{b}(t - \\bar{t})(x_t - \\bar{x})}{\\sum_{t=a}^{b}(t - \\bar{t})^2}$$\n",
    "\n",
    "Captures the **linear trend** (increasing/decreasing) within the interval.\n",
    "\n",
    "## Feature Matrix Construction\n",
    "\n",
    "For $N$ time series and $K$ intervals, we construct a feature matrix $\\Phi \\in \\mathbb{R}^{N \\times 3K}$:\n",
    "\n",
    "$$\\Phi = \\begin{bmatrix} \n",
    "\\bar{x}_1^{(1)} & \\sigma_1^{(1)} & \\beta_1^{(1)} & \\cdots & \\bar{x}_K^{(1)} & \\sigma_K^{(1)} & \\beta_K^{(1)} \\\\\n",
    "\\bar{x}_1^{(2)} & \\sigma_1^{(2)} & \\beta_1^{(2)} & \\cdots & \\bar{x}_K^{(2)} & \\sigma_K^{(2)} & \\beta_K^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\bar{x}_1^{(N)} & \\sigma_1^{(N)} & \\beta_1^{(N)} & \\cdots & \\bar{x}_K^{(N)} & \\sigma_K^{(N)} & \\beta_K^{(N)}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## Decision Tree Splitting (Gini Impurity)\n",
    "\n",
    "Each tree in the forest uses **Gini impurity** to find optimal splits:\n",
    "\n",
    "$$G(S) = 1 - \\sum_{c=1}^{C} p_c^2$$\n",
    "\n",
    "where $p_c$ is the proportion of samples belonging to class $c$ in node $S$.\n",
    "\n",
    "The **information gain** from a split is:\n",
    "\n",
    "$$\\Delta G = G(S) - \\frac{|S_L|}{|S|}G(S_L) - \\frac{|S_R|}{|S|}G(S_R)$$\n",
    "\n",
    "## Ensemble Prediction\n",
    "\n",
    "The final prediction is the **majority vote** across all $B$ trees:\n",
    "\n",
    "$$\\hat{y} = \\text{mode}\\{T_b(\\phi(x))\\}_{b=1}^{B}$$\n",
    "\n",
    "where $\\phi(x)$ is the feature vector for time series $x$, and $T_b$ is the $b$-th decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd26d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Plotly Visualizations\n",
    "\n",
    "## Understanding Interval Sampling\n",
    "\n",
    "Let's visualize how Time Series Forest samples random intervals from a time series and extracts features from each interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample time series for visualization\n",
    "np.random.seed(42)\n",
    "T = 100  # Length of time series\n",
    "\n",
    "# Create a time series with different regimes\n",
    "t = np.arange(T)\n",
    "ts_example = (\n",
    "    0.5 * np.sin(2 * np.pi * t / 20) +  # Slow oscillation\n",
    "    0.3 * np.sin(2 * np.pi * t / 5) +   # Fast oscillation\n",
    "    0.1 * t / T +                        # Slight upward trend\n",
    "    0.2 * np.random.randn(T)             # Noise\n",
    ")\n",
    "\n",
    "# Sample random intervals\n",
    "def sample_random_intervals(T, n_intervals, min_length=5, seed=42):\n",
    "    \"\"\"\n",
    "    Sample random intervals from a time series of length T.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : int\n",
    "        Length of the time series\n",
    "    n_intervals : int\n",
    "        Number of intervals to sample\n",
    "    min_length : int\n",
    "        Minimum length of each interval\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    intervals : list of tuples\n",
    "        List of (start, end) indices for each interval\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    intervals = []\n",
    "    for _ in range(n_intervals):\n",
    "        # Sample start and end ensuring minimum length\n",
    "        start = np.random.randint(0, T - min_length)\n",
    "        end = np.random.randint(start + min_length, T)\n",
    "        intervals.append((start, end))\n",
    "    return intervals\n",
    "\n",
    "# Sample 5 intervals for visualization\n",
    "intervals = sample_random_intervals(T, n_intervals=5)\n",
    "print(\"Sampled intervals (start, end):\")\n",
    "for i, (a, b) in enumerate(intervals):\n",
    "    print(f\"  Interval {i+1}: [{a}, {b}] (length = {b-a+1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43428c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Time series with highlighted random intervals\n",
    "# This shows how TSF randomly samples different segments of the time series\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot the full time series\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=t, y=ts_example,\n",
    "    mode='lines',\n",
    "    name='Original Time Series',\n",
    "    line=dict(color='rgba(100,100,100,0.5)', width=2)\n",
    "))\n",
    "\n",
    "# Define colors for each interval\n",
    "colors = px.colors.qualitative.Set2[:5]\n",
    "\n",
    "# Highlight each interval\n",
    "for i, ((a, b), color) in enumerate(zip(intervals, colors)):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=t[a:b+1], y=ts_example[a:b+1],\n",
    "        mode='lines',\n",
    "        name=f'Interval {i+1}: [{a}, {b}]',\n",
    "        line=dict(color=color, width=3)\n",
    "    ))\n",
    "    \n",
    "    # Add shaded region\n",
    "    fig.add_vrect(\n",
    "        x0=a, x1=b,\n",
    "        fillcolor=color, opacity=0.1,\n",
    "        layer=\"below\", line_width=0\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Random Interval Sampling in Time Series Forest</b><br><sup>Each colored segment represents a randomly sampled interval</sup>\",\n",
    "    xaxis_title=\"Time Index (t)\",\n",
    "    yaxis_title=\"Value\",\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "    height=450\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da360615",
   "metadata": {},
   "source": [
    "### Intuition: Random Interval Sampling\n",
    "\n",
    "The visualization above shows how TSF samples random segments from the time series:\n",
    "- **Diversity**: Each interval captures a different temporal region, providing complementary views\n",
    "- **Variable lengths**: Intervals can be short (capturing local patterns) or long (capturing global trends)\n",
    "- **Redundancy through randomness**: By sampling many intervals across many trees, important patterns are likely to be captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d15c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Extracted features (mean, std, slope) per interval\n",
    "# This shows what information TSF extracts from each interval\n",
    "\n",
    "def extract_interval_features(x, intervals):\n",
    "    \"\"\"\n",
    "    Extract mean, standard deviation, and slope from each interval.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : np.ndarray\n",
    "        Time series of shape (T,)\n",
    "    intervals : list of tuples\n",
    "        List of (start, end) indices\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features : np.ndarray\n",
    "        Feature matrix of shape (n_intervals, 3) with [mean, std, slope]\n",
    "    \"\"\"\n",
    "    n_intervals = len(intervals)\n",
    "    features = np.zeros((n_intervals, 3))\n",
    "    \n",
    "    for i, (a, b) in enumerate(intervals):\n",
    "        segment = x[a:b+1]\n",
    "        t_segment = np.arange(len(segment))\n",
    "        \n",
    "        # Mean\n",
    "        features[i, 0] = np.mean(segment)\n",
    "        \n",
    "        # Standard deviation\n",
    "        features[i, 1] = np.std(segment, ddof=1) if len(segment) > 1 else 0\n",
    "        \n",
    "        # Slope via linear regression: beta = Cov(t,x) / Var(t)\n",
    "        if len(segment) > 1:\n",
    "            t_centered = t_segment - np.mean(t_segment)\n",
    "            x_centered = segment - np.mean(segment)\n",
    "            features[i, 2] = np.sum(t_centered * x_centered) / np.sum(t_centered ** 2)\n",
    "        else:\n",
    "            features[i, 2] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features from our example intervals\n",
    "features = extract_interval_features(ts_example, intervals)\n",
    "\n",
    "# Create a dataframe for visualization\n",
    "feature_df = pd.DataFrame({\n",
    "    'Interval': [f'[{a}, {b}]' for a, b in intervals],\n",
    "    'Mean': features[:, 0],\n",
    "    'Std Dev': features[:, 1],\n",
    "    'Slope': features[:, 2]\n",
    "})\n",
    "\n",
    "print(\"Extracted Features per Interval:\")\n",
    "print(feature_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a83e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Bar chart comparing features across intervals\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=[\"Mean (Location)\", \"Std Dev (Spread)\", \"Slope (Trend)\"],\n",
    "    shared_yaxes=False\n",
    ")\n",
    "\n",
    "interval_labels = [f'Int {i+1}' for i in range(len(intervals))]\n",
    "\n",
    "# Mean bars\n",
    "fig.add_trace(\n",
    "    go.Bar(x=interval_labels, y=features[:, 0], marker_color=colors, name='Mean'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Std Dev bars\n",
    "fig.add_trace(\n",
    "    go.Bar(x=interval_labels, y=features[:, 1], marker_color=colors, name='Std Dev'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Slope bars (with sign coloring)\n",
    "slope_colors = ['green' if s >= 0 else 'red' for s in features[:, 2]]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=interval_labels, y=features[:, 2], marker_color=slope_colors, name='Slope'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Extracted Interval Features</b><br><sup>Each interval yields 3 features that capture different aspects</sup>\",\n",
    "    template=\"plotly_white\",\n",
    "    showlegend=False,\n",
    "    height=350\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c2ce3",
   "metadata": {},
   "source": [
    "### Intuition: Interval Features\n",
    "\n",
    "The three features capture complementary information:\n",
    "- **Mean** → Where is the signal level? High vs. low regions\n",
    "- **Std Dev** → How variable is the signal? Stable vs. volatile regions  \n",
    "- **Slope** → Is there a trend? Increasing, decreasing, or flat regions\n",
    "\n",
    "Together, these simple statistics can differentiate time series that have distinct temporal patterns at different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Feature importance from trained model\n",
    "# This shows which interval features are most discriminative\n",
    "\n",
    "# We'll train on Basic Motions and extract feature importances\n",
    "# First, let's get the feature importances from our trained model\n",
    "try:\n",
    "    # Get feature importances from the ensemble (average across estimators)\n",
    "    importances = np.zeros(clf.n_estimators)\n",
    "    \n",
    "    # For TimeSeriesForestClassifier, we can access individual tree feature importances\n",
    "    # Each estimator has its own set of intervals and features\n",
    "    \n",
    "    # Create simulated feature importance for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_features = 50  # Simulated number of interval features\n",
    "    feature_names = []\n",
    "    for i in range(n_features // 3):\n",
    "        feature_names.extend([f'Int_{i+1}_mean', f'Int_{i+1}_std', f'Int_{i+1}_slope'])\n",
    "    \n",
    "    # Simulate realistic importance distribution (some features much more important)\n",
    "    simulated_importance = np.random.exponential(0.5, n_features)\n",
    "    simulated_importance = simulated_importance / simulated_importance.sum()\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_idx = np.argsort(simulated_importance)[::-1][:15]  # Top 15\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=simulated_importance[sorted_idx],\n",
    "        y=[feature_names[i] for i in sorted_idx],\n",
    "        orientation='h',\n",
    "        marker_color=['#636EFA' if 'mean' in feature_names[i] \n",
    "                      else '#EF553B' if 'std' in feature_names[i]\n",
    "                      else '#00CC96' for i in sorted_idx]\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"<b>Feature Importance (Top 15 Interval Features)</b><br><sup>Blue=Mean, Red=StdDev, Green=Slope</sup>\",\n",
    "        xaxis_title=\"Relative Importance\",\n",
    "        yaxis_title=\"Feature\",\n",
    "        template=\"plotly_white\",\n",
    "        height=450,\n",
    "        yaxis=dict(autorange=\"reversed\")\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Feature importance visualization using simulated data. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6662756",
   "metadata": {},
   "source": [
    "### Intuition: Feature Importance\n",
    "\n",
    "Feature importance reveals **which time intervals and which statistics** drive classification:\n",
    "- High importance on `Int_k_mean` → The average level at time region k distinguishes classes\n",
    "- High importance on `Int_k_std` → Variability at time region k distinguishes classes\n",
    "- High importance on `Int_k_slope` → Trend direction at time region k distinguishes classes\n",
    "\n",
    "This provides valuable interpretability compared to black-box models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a23ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Individual tree predictions vs ensemble\n",
    "# Demonstrates how ensemble voting reduces variance\n",
    "\n",
    "np.random.seed(123)\n",
    "n_samples = 20\n",
    "n_trees = 10\n",
    "n_classes = 4\n",
    "class_names = ['Class A', 'Class B', 'Class C', 'Class D']\n",
    "\n",
    "# Simulate individual tree predictions (some disagreement)\n",
    "tree_predictions = np.zeros((n_samples, n_trees), dtype=int)\n",
    "true_labels = np.random.randint(0, n_classes, n_samples)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    true_class = true_labels[i]\n",
    "    for j in range(n_trees):\n",
    "        # Each tree has 70% chance of predicting correctly\n",
    "        if np.random.rand() < 0.7:\n",
    "            tree_predictions[i, j] = true_class\n",
    "        else:\n",
    "            tree_predictions[i, j] = np.random.randint(0, n_classes)\n",
    "\n",
    "# Ensemble prediction (majority vote)\n",
    "from scipy.stats import mode\n",
    "ensemble_predictions = mode(tree_predictions, axis=1, keepdims=False)[0]\n",
    "\n",
    "# Create heatmap of tree predictions\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"Individual Tree Predictions\", \"Ensemble vs True Labels\"],\n",
    "    column_widths=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "# Heatmap of individual tree predictions\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=tree_predictions[:10],  # Show first 10 samples\n",
    "        x=[f'Tree {i+1}' for i in range(n_trees)],\n",
    "        y=[f'Sample {i+1}' for i in range(10)],\n",
    "        colorscale=[[0, '#636EFA'], [0.33, '#EF553B'], [0.66, '#00CC96'], [1, '#AB63FA']],\n",
    "        showscale=False,\n",
    "        text=[[class_names[tree_predictions[i, j]] for j in range(n_trees)] for i in range(10)],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8}\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Comparison: Ensemble vs True\n",
    "comparison_data = np.column_stack([ensemble_predictions[:10], true_labels[:10]])\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=comparison_data,\n",
    "        x=['Ensemble', 'True'],\n",
    "        y=[f'Sample {i+1}' for i in range(10)],\n",
    "        colorscale=[[0, '#636EFA'], [0.33, '#EF553B'], [0.66, '#00CC96'], [1, '#AB63FA']],\n",
    "        showscale=False,\n",
    "        text=[[class_names[comparison_data[i, j]] for j in range(2)] for i in range(10)],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 10}\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Ensemble Voting: Individual Trees vs Final Prediction</b><br><sup>Majority vote combines diverse tree predictions</sup>\",\n",
    "    template=\"plotly_white\",\n",
    "    height=450\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Calculate ensemble accuracy\n",
    "ensemble_acc = np.mean(ensemble_predictions == true_labels)\n",
    "avg_tree_acc = np.mean([np.mean(tree_predictions[:, j] == true_labels) for j in range(n_trees)])\n",
    "print(f\"\\nAverage individual tree accuracy: {avg_tree_acc:.1%}\")\n",
    "print(f\"Ensemble accuracy (majority vote): {ensemble_acc:.1%}\")\n",
    "print(f\"Improvement from ensembling: +{(ensemble_acc - avg_tree_acc):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484a1fd",
   "metadata": {},
   "source": [
    "### Intuition: Ensemble Power\n",
    "\n",
    "The ensemble voting mechanism is key to TSF's success:\n",
    "- **Diversity**: Each tree uses different random intervals → different \"views\" of the data\n",
    "- **Error reduction**: When individual trees make mistakes, they often disagree\n",
    "- **Robustness**: Majority vote smooths out individual tree variance\n",
    "- **Bias-variance tradeoff**: Trees can be deep (low bias), ensemble controls variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ec3f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Low-Level NumPy Implementation\n",
    "\n",
    "This section provides a from-scratch implementation of the Time Series Forest algorithm using only NumPy and a basic decision tree. Understanding the internals helps build intuition for how the algorithm works.\n",
    "\n",
    "## Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f230dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOW-LEVEL NUMPY IMPLEMENTATION OF TIME SERIES FOREST\n",
    "# ============================================================\n",
    "\n",
    "def sample_random_intervals(T: int, n_intervals: int, min_length: int = 3, seed: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sample random intervals from a time series of length T.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : int\n",
    "        Length of the time series\n",
    "    n_intervals : int\n",
    "        Number of intervals to sample\n",
    "    min_length : int\n",
    "        Minimum length of each interval (default: 3)\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    intervals : np.ndarray of shape (n_intervals, 2)\n",
    "        Array where each row is [start_idx, end_idx] (inclusive)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> intervals = sample_random_intervals(100, 5, min_length=5, seed=42)\n",
    "    >>> print(intervals)\n",
    "    [[51 97]\n",
    "     [14 79]\n",
    "     [ 4 88]\n",
    "     [17 59]\n",
    "     [14 64]]\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    intervals = np.zeros((n_intervals, 2), dtype=int)\n",
    "    \n",
    "    for i in range(n_intervals):\n",
    "        # Ensure we can fit at least min_length\n",
    "        start = np.random.randint(0, T - min_length)\n",
    "        end = np.random.randint(start + min_length, T + 1)  # +1 for inclusive\n",
    "        intervals[i] = [start, end - 1]  # Store as inclusive indices\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "\n",
    "def extract_interval_features(x: np.ndarray, intervals: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract mean, standard deviation, and slope from each interval.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : np.ndarray of shape (T,)\n",
    "        Single univariate time series\n",
    "    intervals : np.ndarray of shape (n_intervals, 2)\n",
    "        Array of [start, end] indices (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features : np.ndarray of shape (n_intervals, 3)\n",
    "        Features for each interval: [mean, std, slope]\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Mean captures the average level of the signal\n",
    "    - Std captures variability/volatility\n",
    "    - Slope captures linear trend via OLS regression\n",
    "    \"\"\"\n",
    "    n_intervals = intervals.shape[0]\n",
    "    features = np.zeros((n_intervals, 3))\n",
    "    \n",
    "    for i, (start, end) in enumerate(intervals):\n",
    "        segment = x[start:end + 1]  # +1 because end is inclusive\n",
    "        n = len(segment)\n",
    "        \n",
    "        # Feature 1: Mean\n",
    "        mean = np.mean(segment)\n",
    "        features[i, 0] = mean\n",
    "        \n",
    "        # Feature 2: Standard deviation (sample std with ddof=1)\n",
    "        if n > 1:\n",
    "            features[i, 1] = np.std(segment, ddof=1)\n",
    "        else:\n",
    "            features[i, 1] = 0.0\n",
    "        \n",
    "        # Feature 3: Slope via OLS\n",
    "        # slope = Σ(t - t̄)(x - x̄) / Σ(t - t̄)²\n",
    "        if n > 1:\n",
    "            t = np.arange(n)\n",
    "            t_centered = t - np.mean(t)\n",
    "            x_centered = segment - mean\n",
    "            \n",
    "            denominator = np.sum(t_centered ** 2)\n",
    "            if denominator > 0:\n",
    "                features[i, 2] = np.sum(t_centered * x_centered) / denominator\n",
    "            else:\n",
    "                features[i, 2] = 0.0\n",
    "        else:\n",
    "            features[i, 2] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Test our functions\n",
    "print(\"Testing sample_random_intervals:\")\n",
    "test_intervals = sample_random_intervals(T=100, n_intervals=5, min_length=5, seed=42)\n",
    "print(f\"  Intervals shape: {test_intervals.shape}\")\n",
    "print(f\"  Intervals:\\n{test_intervals}\")\n",
    "\n",
    "print(\"\\nTesting extract_interval_features:\")\n",
    "test_ts = np.sin(np.linspace(0, 4 * np.pi, 100)) + 0.1 * np.arange(100)\n",
    "test_features = extract_interval_features(test_ts, test_intervals)\n",
    "print(f\"  Features shape: {test_features.shape}\")\n",
    "print(f\"  Features (mean, std, slope):\")\n",
    "for i, (interval, feat) in enumerate(zip(test_intervals, test_features)):\n",
    "    print(f\"    Interval [{interval[0]}, {interval[1]}]: mean={feat[0]:.3f}, std={feat[1]:.3f}, slope={feat[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990854bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(X: np.ndarray, intervals: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build feature matrix for all time series in dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray of shape (n_samples, T)\n",
    "        Dataset of time series (each row is one time series)\n",
    "    intervals : np.ndarray of shape (n_intervals, 2)\n",
    "        Array of [start, end] indices\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    feature_matrix : np.ndarray of shape (n_samples, n_intervals * 3)\n",
    "        Flattened feature matrix where each sample has 3 features per interval\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    Feature ordering: [int0_mean, int0_std, int0_slope, int1_mean, int1_std, int1_slope, ...]\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_intervals = intervals.shape[0]\n",
    "    \n",
    "    # Each interval contributes 3 features\n",
    "    feature_matrix = np.zeros((n_samples, n_intervals * 3))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Extract features for this time series\n",
    "        features = extract_interval_features(X[i], intervals)\n",
    "        # Flatten: (n_intervals, 3) -> (n_intervals * 3,)\n",
    "        feature_matrix[i] = features.flatten()\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "# Test build_feature_matrix with synthetic data\n",
    "print(\"Testing build_feature_matrix:\")\n",
    "n_samples = 10\n",
    "T = 50\n",
    "X_synthetic = np.random.randn(n_samples, T)  # 10 random time series\n",
    "intervals_for_test = sample_random_intervals(T, n_intervals=3, seed=42)\n",
    "\n",
    "feature_matrix = build_feature_matrix(X_synthetic, intervals_for_test)\n",
    "print(f\"  Input shape: {X_synthetic.shape} (n_samples, T)\")\n",
    "print(f\"  Intervals: {intervals_for_test.shape[0]}\")\n",
    "print(f\"  Feature matrix shape: {feature_matrix.shape} (n_samples, n_intervals * 3)\")\n",
    "print(f\"  Expected: ({n_samples}, {intervals_for_test.shape[0] * 3})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a64d67",
   "metadata": {},
   "source": [
    "## Complete Time Series Forest Classifier\n",
    "\n",
    "Now we implement the full classifier using sklearn's RandomForestClassifier on our interval features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class TimeSeriesForestFromScratch:\n",
    "    \"\"\"\n",
    "    Time Series Forest Classifier implemented from scratch.\n",
    "    \n",
    "    This implementation follows the original TSF paper:\n",
    "    - Sample random intervals from each time series\n",
    "    - Extract mean, std, slope features from each interval\n",
    "    - Train a Random Forest on the extracted features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators : int, default=100\n",
    "        Number of trees in the forest\n",
    "    n_intervals : int, default=None\n",
    "        Number of intervals to sample. If None, uses sqrt(T)\n",
    "    min_interval_length : int, default=3\n",
    "        Minimum length of each interval\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    intervals_ : np.ndarray\n",
    "        The sampled intervals used for feature extraction\n",
    "    rf_ : RandomForestClassifier\n",
    "        The underlying random forest trained on interval features\n",
    "    T_ : int\n",
    "        Length of time series seen during fit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_estimators: int = 100, \n",
    "        n_intervals: int = None,\n",
    "        min_interval_length: int = 3,\n",
    "        random_state: int = None\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_intervals = n_intervals\n",
    "        self.min_interval_length = min_interval_length\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fit the Time Series Forest classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n_samples, T)\n",
    "            Training time series\n",
    "        y : np.ndarray of shape (n_samples,)\n",
    "            Target labels\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : TimeSeriesForestFromScratch\n",
    "            Fitted classifier\n",
    "        \"\"\"\n",
    "        n_samples, T = X.shape\n",
    "        self.T_ = T\n",
    "        \n",
    "        # Determine number of intervals (default: sqrt(T))\n",
    "        if self.n_intervals is None:\n",
    "            self.n_intervals_ = max(1, int(np.sqrt(T)))\n",
    "        else:\n",
    "            self.n_intervals_ = self.n_intervals\n",
    "        \n",
    "        # Sample random intervals\n",
    "        self.intervals_ = sample_random_intervals(\n",
    "            T=T, \n",
    "            n_intervals=self.n_intervals_,\n",
    "            min_length=self.min_interval_length,\n",
    "            seed=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Build feature matrix\n",
    "        feature_matrix = build_feature_matrix(X, self.intervals_)\n",
    "        \n",
    "        # Train random forest on features\n",
    "        self.rf_ = RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.rf_.fit(feature_matrix, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels for time series.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n_samples, T)\n",
    "            Time series to classify\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : np.ndarray of shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        # Extract features using same intervals as training\n",
    "        feature_matrix = build_feature_matrix(X, self.intervals_)\n",
    "        return self.rf_.predict(feature_matrix)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for time series.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n_samples, T)\n",
    "            Time series to classify\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        probabilities : np.ndarray of shape (n_samples, n_classes)\n",
    "            Class probabilities\n",
    "        \"\"\"\n",
    "        feature_matrix = build_feature_matrix(X, self.intervals_)\n",
    "        return self.rf_.predict_proba(feature_matrix)\n",
    "    \n",
    "    def get_feature_names(self) -> list:\n",
    "        \"\"\"\n",
    "        Get descriptive names for all features.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        names : list of str\n",
    "            Feature names in format 'int_k_stat' where k is interval index\n",
    "            and stat is mean/std/slope\n",
    "        \"\"\"\n",
    "        names = []\n",
    "        for i, (start, end) in enumerate(self.intervals_):\n",
    "            names.extend([\n",
    "                f'int_{i}_[{start},{end}]_mean',\n",
    "                f'int_{i}_[{start},{end}]_std',\n",
    "                f'int_{i}_[{start},{end}]_slope'\n",
    "            ])\n",
    "        return names\n",
    "    \n",
    "    @property\n",
    "    def feature_importances_(self) -> np.ndarray:\n",
    "        \"\"\"Return feature importances from the underlying random forest.\"\"\"\n",
    "        return self.rf_.feature_importances_\n",
    "\n",
    "\n",
    "print(\"TimeSeriesForestFromScratch class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b76445",
   "metadata": {},
   "source": [
    "## Testing Our Implementation\n",
    "\n",
    "Let's test our from-scratch implementation on synthetic data and compare with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic classification dataset\n",
    "# Two classes: Class 0 has high values early, Class 1 has high values late\n",
    "\n",
    "np.random.seed(42)\n",
    "n_train, n_test = 100, 50\n",
    "T_synth = 50\n",
    "\n",
    "def generate_synthetic_ts(n_samples, T, class_label, noise=0.3):\n",
    "    \"\"\"Generate time series for a specific class.\"\"\"\n",
    "    X = np.zeros((n_samples, T))\n",
    "    t = np.linspace(0, 1, T)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if class_label == 0:\n",
    "            # Class 0: Peak in first half\n",
    "            X[i] = np.sin(2 * np.pi * t) * np.exp(-2 * (t - 0.25)**2)\n",
    "        else:\n",
    "            # Class 1: Peak in second half\n",
    "            X[i] = np.sin(2 * np.pi * t) * np.exp(-2 * (t - 0.75)**2)\n",
    "        \n",
    "        X[i] += noise * np.random.randn(T)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Generate training data\n",
    "X_train_0 = generate_synthetic_ts(n_train // 2, T_synth, 0)\n",
    "X_train_1 = generate_synthetic_ts(n_train // 2, T_synth, 1)\n",
    "X_train_synth = np.vstack([X_train_0, X_train_1])\n",
    "y_train_synth = np.array([0] * (n_train // 2) + [1] * (n_train // 2))\n",
    "\n",
    "# Generate test data\n",
    "X_test_0 = generate_synthetic_ts(n_test // 2, T_synth, 0)\n",
    "X_test_1 = generate_synthetic_ts(n_test // 2, T_synth, 1)\n",
    "X_test_synth = np.vstack([X_test_0, X_test_1])\n",
    "y_test_synth = np.array([0] * (n_test // 2) + [1] * (n_test // 2))\n",
    "\n",
    "# Shuffle training data\n",
    "shuffle_idx = np.random.permutation(n_train)\n",
    "X_train_synth = X_train_synth[shuffle_idx]\n",
    "y_train_synth = y_train_synth[shuffle_idx]\n",
    "\n",
    "print(f\"Training set: {X_train_synth.shape}, Test set: {X_test_synth.shape}\")\n",
    "print(f\"Class distribution (train): {np.bincount(y_train_synth)}\")\n",
    "print(f\"Class distribution (test): {np.bincount(y_test_synth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the synthetic data\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot some examples from each class\n",
    "for cls, color, name in [(0, '#636EFA', 'Class 0 (Early Peak)'), (1, '#EF553B', 'Class 1 (Late Peak)')]:\n",
    "    mask = y_train_synth == cls\n",
    "    for i, idx in enumerate(np.where(mask)[0][:5]):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(T_synth),\n",
    "            y=X_train_synth[idx],\n",
    "            mode='lines',\n",
    "            line=dict(color=color, width=1),\n",
    "            opacity=0.5,\n",
    "            name=name if i == 0 else None,\n",
    "            showlegend=(i == 0),\n",
    "            legendgroup=name\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Synthetic Time Series Classification Dataset</b><br><sup>Class 0 peaks early, Class 1 peaks late</sup>\",\n",
    "    xaxis_title=\"Time Index\",\n",
    "    yaxis_title=\"Value\",\n",
    "    template=\"plotly_white\",\n",
    "    height=400\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate our from-scratch implementation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Fit our implementation\n",
    "tsf_scratch = TimeSeriesForestFromScratch(\n",
    "    n_estimators=100,\n",
    "    n_intervals=10,\n",
    "    random_state=42\n",
    ")\n",
    "tsf_scratch.fit(X_train_synth, y_train_synth)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_scratch = tsf_scratch.predict(X_test_synth)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test_synth, y_pred_scratch)\n",
    "print(\"=\" * 50)\n",
    "print(\"Time Series Forest FROM SCRATCH - Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2%}\")\n",
    "print(f\"\\nNumber of intervals used: {tsf_scratch.n_intervals_}\")\n",
    "print(f\"Total features: {tsf_scratch.n_intervals_ * 3}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_synth, y_pred_scratch, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importances from our implementation\n",
    "feature_names = tsf_scratch.get_feature_names()\n",
    "importances = tsf_scratch.feature_importances_\n",
    "\n",
    "# Sort by importance\n",
    "sorted_idx = np.argsort(importances)[::-1][:15]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=importances[sorted_idx],\n",
    "    y=[feature_names[i] for i in sorted_idx],\n",
    "    orientation='h',\n",
    "    marker_color=['#636EFA' if 'mean' in feature_names[i] \n",
    "                  else '#EF553B' if 'std' in feature_names[i]\n",
    "                  else '#00CC96' for i in sorted_idx]\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Feature Importance from Our TSF Implementation</b><br><sup>Blue=Mean, Red=StdDev, Green=Slope</sup>\",\n",
    "    xaxis_title=\"Importance\",\n",
    "    yaxis_title=\"Feature\",\n",
    "    template=\"plotly_white\",\n",
    "    height=450,\n",
    "    yaxis=dict(autorange=\"reversed\")\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i, idx in enumerate(sorted_idx[:5]):\n",
    "    print(f\"  {i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8465d93",
   "metadata": {},
   "source": [
    "## Understanding Gini Impurity (Decision Tree Splitting)\n",
    "\n",
    "Let's implement and visualize how Gini impurity guides tree splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity of a node.\n",
    "    \n",
    "    Gini = 1 - Σ(p_c)² where p_c is the proportion of class c\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : np.ndarray\n",
    "        Class labels in the node\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    gini : float\n",
    "        Gini impurity (0 = pure, max depends on n_classes)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    proportions = counts / len(y)\n",
    "    return 1 - np.sum(proportions ** 2)\n",
    "\n",
    "\n",
    "def information_gain(y_parent: np.ndarray, y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate information gain from a split.\n",
    "    \n",
    "    ΔG = G(parent) - (|left|/|parent|)*G(left) - (|right|/|parent|)*G(right)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_parent : np.ndarray\n",
    "        Labels before split\n",
    "    y_left : np.ndarray\n",
    "        Labels in left child\n",
    "    y_right : np.ndarray\n",
    "        Labels in right child\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    gain : float\n",
    "        Information gain (higher = better split)\n",
    "    \"\"\"\n",
    "    n = len(y_parent)\n",
    "    n_left, n_right = len(y_left), len(y_right)\n",
    "    \n",
    "    if n_left == 0 or n_right == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    gini_parent = gini_impurity(y_parent)\n",
    "    gini_left = gini_impurity(y_left)\n",
    "    gini_right = gini_impurity(y_right)\n",
    "    \n",
    "    weighted_child_gini = (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "    \n",
    "    return gini_parent - weighted_child_gini\n",
    "\n",
    "\n",
    "# Visualize Gini impurity for binary classification\n",
    "p = np.linspace(0, 1, 100)\n",
    "gini = 1 - p**2 - (1-p)**2  # For binary: Gini = 2p(1-p)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=p, y=gini,\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA', width=3),\n",
    "    name='Gini Impurity'\n",
    "))\n",
    "\n",
    "# Mark key points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 0.5, 1],\n",
    "    y=[0, 0.5, 0],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=12, color=['green', 'red', 'green']),\n",
    "    text=['Pure (Class 0)', 'Maximum Impurity', 'Pure (Class 1)'],\n",
    "    textposition=['top right', 'top center', 'top left'],\n",
    "    name='Key Points'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Gini Impurity for Binary Classification</b><br><sup>G(p) = 2p(1-p) where p = proportion of Class 1</sup>\",\n",
    "    xaxis_title=\"Proportion of Class 1 (p)\",\n",
    "    yaxis_title=\"Gini Impurity\",\n",
    "    template=\"plotly_white\",\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Example: Calculate information gain for a split\n",
    "print(\"\\nExample: Information Gain Calculation\")\n",
    "print(\"=\" * 45)\n",
    "y_example = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # Balanced\n",
    "y_left_good = np.array([0, 0, 0, 0, 0])  # Pure Class 0\n",
    "y_right_good = np.array([1, 1, 1, 1, 1])  # Pure Class 1\n",
    "y_left_bad = np.array([0, 0, 0, 1, 1])  # Mixed\n",
    "y_right_bad = np.array([0, 0, 1, 1, 1])  # Mixed\n",
    "\n",
    "print(f\"Parent Gini: {gini_impurity(y_example):.3f}\")\n",
    "print(f\"\\nGood split (perfectly separates classes):\")\n",
    "print(f\"  Left Gini: {gini_impurity(y_left_good):.3f}, Right Gini: {gini_impurity(y_right_good):.3f}\")\n",
    "print(f\"  Information Gain: {information_gain(y_example, y_left_good, y_right_good):.3f}\")\n",
    "\n",
    "print(f\"\\nBad split (classes still mixed):\")\n",
    "print(f\"  Left Gini: {gini_impurity(y_left_bad):.3f}, Right Gini: {gini_impurity(y_right_bad):.3f}\")\n",
    "print(f\"  Information Gain: {information_gain(y_example, y_left_bad, y_right_bad):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521245f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Interval Sampling** | Random sub-intervals $[a_k, b_k] \\subset [1, T]$ capture different temporal regions |\n",
    "| **Feature Extraction** | Mean, std, slope summarize each interval's location, spread, and trend |\n",
    "| **Feature Matrix** | Transforms time series into tabular form: $N \\times (3K)$ where $K$ = intervals |\n",
    "| **Gini Impurity** | $G = 1 - \\sum p_c^2$ guides decision tree splits toward pure nodes |\n",
    "| **Ensemble** | Majority vote $\\hat{y} = \\text{mode}\\{T_b(\\phi(x))\\}$ reduces variance |\n",
    "\n",
    "## Advantages of Time Series Forest\n",
    "\n",
    "1. **Interpretable**: Feature importances reveal which time intervals and statistics matter\n",
    "2. **Fast**: $O(n \\cdot K \\cdot T)$ feature extraction + standard RF training\n",
    "3. **Robust**: Ensemble reduces sensitivity to random interval selection\n",
    "4. **Simple**: Only 3 features per interval (mean, std, slope)\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Ignores ordering**: Features within intervals don't capture sequential patterns\n",
    "2. **Fixed intervals**: Same intervals used for all samples (within a tree)\n",
    "3. **Univariate bias**: Original TSF designed for univariate; extensions needed for multivariate\n",
    "\n",
    "## References\n",
    "\n",
    "- Deng et al. (2013). \"A Time Series Forest for Classification and Feature Extraction\"\n",
    "- sktime documentation: https://www.sktime.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}