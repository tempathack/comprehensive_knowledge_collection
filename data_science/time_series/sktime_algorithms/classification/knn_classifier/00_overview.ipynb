{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsTimeSeriesClassifier\n",
    "\n",
    "K-Nearest Neighbors (KNN) for time series classification adapts the classic KNN algorithm by using **distance metrics designed for sequential data**. The most powerful is **Dynamic Time Warping (DTW)**, which allows elastic alignment between series of different lengths or with temporal distortions.\n",
    "\n",
    "**Key Intuition**: Instead of comparing time series point-by-point (which fails when signals are shifted or stretched), DTW finds the optimal alignment that minimizes the total distance between matched points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0e182",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### KNN Classification Rule\n",
    "\n",
    "Given a query time series $\\mathbf{x}$, KNN finds the $k$ nearest neighbors in the training set and assigns the majority class:\n",
    "\n",
    "$$\\hat{y} = \\text{mode}\\{y_i : i \\in \\mathcal{N}_k(\\mathbf{x})\\}$$\n",
    "\n",
    "where $\\mathcal{N}_k(\\mathbf{x})$ is the set of indices of the $k$ training samples closest to $\\mathbf{x}$.\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "**Euclidean Distance** (point-to-point, rigid alignment):\n",
    "\n",
    "$$d_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{t=1}^{T}(x_t - y_t)^2}$$\n",
    "\n",
    "This works well when series are perfectly aligned but fails with phase shifts or speed variations.\n",
    "\n",
    "**Dynamic Time Warping (DTW)** allows non-linear alignment via a recurrence relation:\n",
    "\n",
    "$$D(i,j) = d(x_i, y_j) + \\min\\begin{cases} D(i-1, j) & \\text{(insertion)} \\\\ D(i, j-1) & \\text{(deletion)} \\\\ D(i-1, j-1) & \\text{(match)} \\end{cases}$$\n",
    "\n",
    "where $d(x_i, y_j) = (x_i - y_j)^2$ is the local cost.\n",
    "\n",
    "**Warping Path Constraints**:\n",
    "- **Boundary conditions**: Path starts at $(1,1)$ and ends at $(T_x, T_y)$\n",
    "- **Monotonicity**: Indices can only increase (no going back in time)\n",
    "- **Continuity**: Steps limited to adjacent cells\n",
    "- **Sakoe-Chiba band**: $|i - j| \\leq w$ (window constraint for efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66285d",
   "metadata": {},
   "source": [
    "## Low-Level NumPy Implementation\n",
    "\n",
    "Building DTW and KNN from scratch helps understand the algorithm mechanics. These implementations prioritize clarity over optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954591ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between two time series.\n",
    "    \n",
    "    This is the L2 norm of the difference vector. Works best when \n",
    "    series are perfectly aligned in time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : np.ndarray of shape (T,)\n",
    "        Time series of equal length T\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float : Euclidean distance\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "def dtw_distance(x: np.ndarray, y: np.ndarray, return_matrix: bool = False):\n",
    "    \"\"\"\n",
    "    Compute Dynamic Time Warping distance with full cost matrix.\n",
    "    \n",
    "    DTW finds the optimal alignment between two sequences by computing\n",
    "    a cumulative cost matrix where each cell (i,j) represents the \n",
    "    minimum cost to align x[:i+1] with y[:j+1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray of shape (n,)\n",
    "        First time series\n",
    "    y : np.ndarray of shape (m,)\n",
    "        Second time series  \n",
    "    return_matrix : bool\n",
    "        If True, also return the cost matrix and warping path\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float : DTW distance (square root of accumulated cost)\n",
    "    D : np.ndarray (optional) : Full cost matrix\n",
    "    path : list (optional) : Optimal warping path as list of (i,j) tuples\n",
    "    \"\"\"\n",
    "    n, m = len(x), len(y)\n",
    "    \n",
    "    # Initialize cost matrix with infinity\n",
    "    D = np.full((n + 1, m + 1), np.inf)\n",
    "    D[0, 0] = 0\n",
    "    \n",
    "    # Fill the cost matrix using dynamic programming\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            # Local cost: squared difference\n",
    "            cost = (x[i-1] - y[j-1]) ** 2\n",
    "            # Cumulative cost: local + minimum of three predecessors\n",
    "            D[i, j] = cost + min(D[i-1, j],      # insertion (vertical step)\n",
    "                                  D[i, j-1],      # deletion (horizontal step)\n",
    "                                  D[i-1, j-1])    # match (diagonal step)\n",
    "    \n",
    "    # Backtrack to find optimal warping path\n",
    "    if return_matrix:\n",
    "        path = []\n",
    "        i, j = n, m\n",
    "        while i > 0 or j > 0:\n",
    "            path.append((i-1, j-1))\n",
    "            if i == 0:\n",
    "                j -= 1\n",
    "            elif j == 0:\n",
    "                i -= 1\n",
    "            else:\n",
    "                # Find which predecessor was used\n",
    "                argmin = np.argmin([D[i-1, j-1], D[i-1, j], D[i, j-1]])\n",
    "                if argmin == 0:\n",
    "                    i, j = i-1, j-1\n",
    "                elif argmin == 1:\n",
    "                    i -= 1\n",
    "                else:\n",
    "                    j -= 1\n",
    "        path.reverse()\n",
    "        return np.sqrt(D[n, m]), D[1:, 1:], path\n",
    "    \n",
    "    return np.sqrt(D[n, m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad118282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(X_train: np.ndarray, X_test: np.ndarray, \n",
    "                            metric: str = \"dtw\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute pairwise distance matrix between test and training samples.\n",
    "    \n",
    "    This is the computational bottleneck of KNN - we need O(n_test * n_train)\n",
    "    distance computations, each of which is O(T^2) for DTW.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray of shape (n_train, T)\n",
    "        Training time series\n",
    "    X_test : np.ndarray of shape (n_test, T)\n",
    "        Test time series\n",
    "    metric : str\n",
    "        Distance metric: \"euclidean\" or \"dtw\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray of shape (n_test, n_train) : Distance matrix\n",
    "    \"\"\"\n",
    "    n_test, n_train = len(X_test), len(X_train)\n",
    "    dist_matrix = np.zeros((n_test, n_train))\n",
    "    \n",
    "    # Select distance function\n",
    "    dist_fn = euclidean_distance if metric == \"euclidean\" else dtw_distance\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        for j in range(n_train):\n",
    "            dist_matrix[i, j] = dist_fn(X_test[i], X_train[j])\n",
    "            \n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "def knn_classify(distance_matrix: np.ndarray, y_train: np.ndarray, \n",
    "                 k: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Classify test samples using k-nearest neighbors voting.\n",
    "    \n",
    "    For each test sample, find k training samples with smallest distance\n",
    "    and return the most common class among them.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_matrix : np.ndarray of shape (n_test, n_train)\n",
    "        Precomputed distances from test to training samples\n",
    "    y_train : np.ndarray of shape (n_train,)\n",
    "        Training labels\n",
    "    k : int\n",
    "        Number of neighbors to consider\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray of shape (n_test,) : Predicted labels\n",
    "    \"\"\"\n",
    "    n_test = distance_matrix.shape[0]\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        # Find indices of k nearest neighbors\n",
    "        neighbor_indices = np.argsort(distance_matrix[i])[:k]\n",
    "        # Get their labels\n",
    "        neighbor_labels = y_train[neighbor_indices]\n",
    "        # Majority vote (mode)\n",
    "        unique, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "        predictions.append(unique[np.argmax(counts)])\n",
    "        \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab43fc1",
   "metadata": {},
   "source": [
    "## Visualizing DTW Alignment\n",
    "\n",
    "**Intuition**: The DTW cost matrix is like a terrain map where we're finding the cheapest path from corner to corner. Dark cells = cheap alignment, bright cells = expensive. The optimal path snakes through the valleys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create two synthetic time series with different phase/speed\n",
    "np.random.seed(42)\n",
    "t = np.linspace(0, 2*np.pi, 50)\n",
    "\n",
    "# Series 1: sine wave\n",
    "series1 = np.sin(t) + 0.1 * np.random.randn(len(t))\n",
    "\n",
    "# Series 2: sine wave with phase shift and stretch\n",
    "t2 = np.linspace(0.5, 2.5*np.pi, 50)\n",
    "series2 = np.sin(t2) + 0.1 * np.random.randn(len(t2))\n",
    "\n",
    "# Compute DTW with cost matrix and path\n",
    "dtw_dist, cost_matrix, warping_path = dtw_distance(series1, series2, return_matrix=True)\n",
    "eucl_dist = euclidean_distance(series1, series2)\n",
    "\n",
    "print(f\"Euclidean distance: {eucl_dist:.3f}\")\n",
    "print(f\"DTW distance: {dtw_dist:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfd4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DTW cost matrix with optimal warping path\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"DTW Cost Matrix with Optimal Path\",\n",
    "        \"Time Series Alignment via DTW\",\n",
    "        \"Series 1 (Query)\",\n",
    "        \"Series 2 (Reference)\"\n",
    "    ),\n",
    "    specs=[[{}, {}], [{}, {}]],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Cost matrix heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cost_matrix, \n",
    "        colorscale=\"Viridis\",\n",
    "        colorbar=dict(title=\"Cost\", x=0.45, len=0.45, y=0.78),\n",
    "        showscale=True\n",
    "    ), \n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Warping path overlay\n",
    "path_x = [p[1] for p in warping_path]\n",
    "path_y = [p[0] for p in warping_path]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=path_x, y=path_y, \n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(color=\"red\", width=2),\n",
    "        marker=dict(size=4),\n",
    "        name=\"Optimal Path\"\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# DTW alignment connections\n",
    "for i, j in warping_path[::3]:  # Sample every 3rd point for clarity\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[i, j], y=[series1[i], series2[j]],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"rgba(255,0,0,0.3)\", width=1),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Series on alignment plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(series1))), y=series1, name=\"Series 1\", \n",
    "               line=dict(color=\"blue\")),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(series2))), y=series2, name=\"Series 2\",\n",
    "               line=dict(color=\"green\")),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Individual series plots\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(series1))), y=series1, \n",
    "               line=dict(color=\"blue\"), showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(series2))), y=series2,\n",
    "               line=dict(color=\"green\"), showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700, \n",
    "    title_text=\"Dynamic Time Warping: Cost Matrix & Alignment\",\n",
    "    showlegend=True\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Series 2 index\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Series 1 index\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Value\", row=1, col=2)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160720b5",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Visualization\n",
    "\n",
    "**Intuition**: For a query time series, we compute distances to all training samples and select the k closest ones. The query is assigned the majority class among its neighbors. Choosing k is a bias-variance tradeoff: small k = flexible but noisy, large k = smooth but may miss local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880397b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification dataset\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_class_samples(n_samples, base_freq, noise=0.1):\n",
    "    \"\"\"Generate time series samples for a class with given frequency.\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        t = np.linspace(0, 4*np.pi, 60)\n",
    "        phase = np.random.uniform(0, np.pi)\n",
    "        amp = np.random.uniform(0.8, 1.2)\n",
    "        series = amp * np.sin(base_freq * t + phase) + noise * np.random.randn(len(t))\n",
    "        samples.append(series)\n",
    "    return np.array(samples)\n",
    "\n",
    "# Create training data: 2 classes with different frequencies\n",
    "X_class0 = generate_class_samples(15, base_freq=1.0)  # Low frequency\n",
    "X_class1 = generate_class_samples(15, base_freq=2.0)  # High frequency\n",
    "X_train_synth = np.vstack([X_class0, X_class1])\n",
    "y_train_synth = np.array([0]*15 + [1]*15)\n",
    "\n",
    "# Create a query sample (class 1)\n",
    "query = generate_class_samples(1, base_freq=2.0)[0]\n",
    "\n",
    "# Compute distances\n",
    "distances = np.array([dtw_distance(query, x) for x in X_train_synth])\n",
    "k = 5\n",
    "neighbor_indices = np.argsort(distances)[:k]\n",
    "neighbor_labels = y_train_synth[neighbor_indices]\n",
    "\n",
    "print(f\"Query's {k} nearest neighbors:\")\n",
    "print(f\"  Indices: {neighbor_indices}\")\n",
    "print(f\"  Labels: {neighbor_labels}\")\n",
    "print(f\"  Distances: {distances[neighbor_indices].round(3)}\")\n",
    "print(f\"  Predicted class: {np.bincount(neighbor_labels).argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KNN classification\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Query Time Series\",\n",
    "        f\"K={k} Nearest Neighbors\",\n",
    "        \"Distance to All Training Samples\",\n",
    "        \"Training Data (Sample per Class)\"\n",
    "    ),\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "# Query series\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=query, mode=\"lines\", line=dict(color=\"red\", width=3), name=\"Query\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# K nearest neighbors\n",
    "colors = [\"blue\" if y == 0 else \"green\" for y in neighbor_labels]\n",
    "for idx, (ni, color) in enumerate(zip(neighbor_indices, colors)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=X_train_synth[ni], mode=\"lines\",\n",
    "            line=dict(color=color, width=1.5),\n",
    "            name=f\"Neighbor {idx+1} (class {y_train_synth[ni]})\",\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Distance bar chart\n",
    "bar_colors = [\"blue\" if y == 0 else \"green\" for y in y_train_synth]\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(range(len(distances))), y=distances,\n",
    "        marker_color=bar_colors,\n",
    "        name=\"Distances\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "# Highlight k nearest\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=neighbor_indices.tolist(), y=distances[neighbor_indices],\n",
    "        mode=\"markers\", marker=dict(color=\"red\", size=12, symbol=\"circle-open\", line=dict(width=3)),\n",
    "        name=f\"K={k} Nearest\"\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Sample training data\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=X_class0[0], mode=\"lines\", line=dict(color=\"blue\"), name=\"Class 0 (low freq)\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=X_class1[0], mode=\"lines\", line=dict(color=\"green\"), name=\"Class 1 (high freq)\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"KNN Time Series Classification\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Training Sample Index\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"DTW Distance\", row=2, col=1)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee53a2",
   "metadata": {},
   "source": [
    "## Effect of K on Classification Accuracy\n",
    "\n",
    "**Intuition**: The choice of $k$ controls the smoothness of decision boundaries:\n",
    "- **k=1**: Most flexible, uses single nearest neighbor (prone to noise)\n",
    "- **Large k**: Smoother decisions, but may miss local patterns\n",
    "- **Optimal k**: Usually found via cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split for accuracy evaluation\n",
    "np.random.seed(456)\n",
    "\n",
    "# Generate more samples for proper evaluation\n",
    "X_c0_train = generate_class_samples(20, base_freq=1.0)\n",
    "X_c1_train = generate_class_samples(20, base_freq=2.0)\n",
    "X_c0_test = generate_class_samples(10, base_freq=1.0)\n",
    "X_c1_test = generate_class_samples(10, base_freq=2.0)\n",
    "\n",
    "X_tr = np.vstack([X_c0_train, X_c1_train])\n",
    "y_tr = np.array([0]*20 + [1]*20)\n",
    "X_te = np.vstack([X_c0_test, X_c1_test])\n",
    "y_te = np.array([0]*10 + [1]*10)\n",
    "\n",
    "# Compute distance matrix once (reuse for different k values)\n",
    "dist_mat = compute_distance_matrix(X_tr, X_te, metric=\"dtw\")\n",
    "\n",
    "# Evaluate accuracy for different k values\n",
    "k_values = list(range(1, 21, 2))\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    preds = knn_classify(dist_mat, y_tr, k=k)\n",
    "    acc = np.mean(preds == y_te)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "print(\"K vs Accuracy:\")\n",
    "for k, acc in zip(k_values, accuracies):\n",
    "    print(f\"  k={k:2d}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78562707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs k\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=k_values, y=accuracies,\n",
    "    mode=\"lines+markers\",\n",
    "    line=dict(color=\"royalblue\", width=3),\n",
    "    marker=dict(size=10),\n",
    "    name=\"Test Accuracy\"\n",
    "))\n",
    "\n",
    "# Highlight optimal k\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "best_acc = max(accuracies)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[best_k], y=[best_acc],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(color=\"red\", size=15, symbol=\"star\"),\n",
    "    name=f\"Best k={best_k}\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Classification Accuracy vs Number of Neighbors (k)\",\n",
    "    xaxis_title=\"k (Number of Neighbors)\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    yaxis=dict(range=[0, 1.05], tickformat=\".0%\"),\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=best_k, y=best_acc,\n",
    "    text=f\"Optimal k={best_k}<br>Acc={best_acc:.1%}\",\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    ax=40, ay=-40\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split for accuracy evaluation\n",
    "np.random.seed(456)\n",
    "\n",
    "# Generate more samples for proper evaluation\n",
    "X_c0_train = generate_class_samples(20, base_freq=1.0)\n",
    "X_c1_train = generate_class_samples(20, base_freq=2.0)\n",
    "X_c0_test = generate_class_samples(10, base_freq=1.0)\n",
    "X_c1_test = generate_class_samples(10, base_freq=2.0)\n",
    "\n",
    "X_tr = np.vstack([X_c0_train, X_c1_train])\n",
    "y_tr = np.array([0]*20 + [1]*20)\n",
    "X_te = np.vstack([X_c0_test, X_c1_test])\n",
    "y_te = np.array([0]*10 + [1]*10)\n",
    "\n",
    "# Compute distance matrix once (reuse for different k values)\n",
    "dist_mat = compute_distance_matrix(X_tr, X_te, metric=\"dtw\")\n",
    "\n",
    "# Evaluate accuracy for different k values\n",
    "k_values = list(range(1, 21, 2))\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    preds = knn_classify(dist_mat, y_tr, k=k)\n",
    "    acc = np.mean(preds == y_te)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "print(\"K vs Accuracy:\")\n",
    "for k, acc in zip(k_values, accuracies):\n",
    "    print(f\"  k={k:2d}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sktime.datasets import load_basic_motions, load_unit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_basic_motions(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_basic_motions(split=\"test\", return_X_y=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sktime's KNeighborsTimeSeriesClassifier\n",
    "\n",
    "Now let's use the optimized implementation from sktime on a real dataset. The `KNeighborsTimeSeriesClassifier` provides:\n",
    "- Multiple distance metrics (DTW, Euclidean, MSM, TWE, etc.)\n",
    "- Window constraints for faster DTW\n",
    "- Automatic handling of multivariate series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = KNeighborsTimeSeriesClassifier(n_neighbors=1, distance=\"dtw\")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways & Practical Notes\n",
    "\n",
    "### When to Use KNN-DTW\n",
    "✅ **Use when**: Small-to-medium datasets, interpretability needed, time warping expected  \n",
    "❌ **Avoid when**: Large datasets (O(n²T²) complexity), real-time requirements\n",
    "\n",
    "### Performance Tips\n",
    "1. **Window constraints**: Use `distance_params={\"window\": 0.1}` to limit DTW warping (10% of series length)\n",
    "2. **Early abandoning**: Stop distance computation once threshold exceeded\n",
    "3. **Downsampling**: Reduce series length for faster computation\n",
    "4. **Lower bounding**: Use LB_Keogh to prune candidates before full DTW\n",
    "\n",
    "### Distance Metric Selection\n",
    "| Metric | Best For | Complexity |\n",
    "|--------|----------|------------|\n",
    "| Euclidean | Aligned series, fast baseline | O(T) |\n",
    "| DTW | Phase shifts, speed variations | O(T²) |\n",
    "| DTW + window | Large-scale DTW | O(T·w) |\n",
    "| MSM (Move-Split-Merge) | Spike patterns | O(T²) |\n",
    "\n",
    "### Hyperparameter Guidelines\n",
    "- Start with **k=1** for DTW (often optimal for time series)\n",
    "- Use **odd k** values to avoid ties\n",
    "- Cross-validate both k and window size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}