{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sktime Model Selection & Time-Series Cross-Validation\n",
    "\n",
    "Time-series validation must respect **temporal order**. This notebook covers **sliding vs. expanding windows**, visualizes splits, and shows how to tune models with `ForecastingGridSearchCV`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting horizon\n",
    "\n",
    "For a forecast origin $T$, the **forecasting horizon** is\n",
    "\n",
    "\\[\n",
    "\text{fh} = \\{T+1, T+2, \\ldots, T+h\\}\n",
    "\\]\n",
    "\n",
    "sktime uses a `ForecastingHorizon` object to define these steps explicitly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159f76c",
   "metadata": {},
   "source": [
    "## Enhanced Mathematical Foundation\n",
    "\n",
    "### Cross-Validation Window Strategies\n",
    "\n",
    "**Sliding Window CV**: For each fold $k$, train on a fixed-length window and test on the subsequent horizon:\n",
    "\n",
    "$$\\text{Train}^{(k)}: [t_k - w, t_k), \\quad \\text{Test}^{(k)}: [t_k, t_k + h)$$\n",
    "\n",
    "where $w$ is the window size and $h$ is the forecast horizon.\n",
    "\n",
    "**Expanding Window CV**: Training set grows as we advance through time:\n",
    "\n",
    "$$\\text{Train}^{(k)}: [0, t_k), \\quad \\text{Test}^{(k)}: [t_k, t_k + h)$$\n",
    "\n",
    "### Cross-Validation Error Estimation\n",
    "\n",
    "The **cross-validation error** aggregates performance across $K$ folds:\n",
    "\n",
    "$$\\bar{e} = \\frac{1}{K}\\sum_{k=1}^{K} L(\\hat{y}^{(k)}, y^{(k)})$$\n",
    "\n",
    "where $L$ is the loss function (e.g., MSE, MAE).\n",
    "\n",
    "**Variance of CV estimate** (assuming independence):\n",
    "\n",
    "$$\\text{Var}(\\bar{e}) = \\frac{1}{K}\\sigma^2_e$$\n",
    "\n",
    "In practice, time-series folds are **dependent**, so variance is often underestimated.\n",
    "\n",
    "### Information Criteria for Model Selection\n",
    "\n",
    "**Akaike Information Criterion (AIC)**:\n",
    "\n",
    "$$\\text{AIC} = 2k - 2\\ln(\\hat{L})$$\n",
    "\n",
    "**Bayesian Information Criterion (BIC)**:\n",
    "\n",
    "$$\\text{BIC} = k\\ln(n) - 2\\ln(\\hat{L})$$\n",
    "\n",
    "where $k$ = number of parameters, $n$ = sample size, $\\hat{L}$ = maximized likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.model_selection import (\n",
    "    temporal_train_test_split,\n",
    "    ForecastingHorizon,\n",
    "    SlidingWindowSplitter,\n",
    "    ExpandingWindowSplitter,\n",
    ")\n",
    "\n",
    "# Load a classic monthly series\n",
    "y = load_airline()\n",
    "\n",
    "# Train/test split + forecasting horizon\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=24)\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding vs. expanding windows\n",
    "\n",
    "- **Sliding window** keeps a fixed training length.\n",
    "- **Expanding window** grows the training set as time advances.\n",
    "\n",
    "Both avoid leakage, but they answer different questions:\n",
    "- Sliding: \"How does the model perform on *recent* history?\"\n",
    "- Expanding: \"How does the model improve with *more data*?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def plot_cv_splits(y, splitter, max_splits=6, title=\"\"):\n",
    "    index = y.index\n",
    "    if hasattr(index, \"to_timestamp\"):\n",
    "        index = index.to_timestamp()\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for split, (train_idx, test_idx) in enumerate(splitter.split(y)):\n",
    "        if split >= max_splits:\n",
    "            break\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=index[train_idx],\n",
    "                y=[split] * len(train_idx),\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=\"rgba(120,120,120,0.6)\", size=6),\n",
    "                name=\"train\" if split == 0 else None,\n",
    "                showlegend=split == 0,\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=index[test_idx],\n",
    "                y=[split] * len(test_idx),\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=\"rgba(255,127,14,0.9)\", size=6),\n",
    "                name=\"test\" if split == 0 else None,\n",
    "                showlegend=split == 0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis=dict(title=\"Split #\", autorange=\"reversed\"),\n",
    "        height=320 + 40 * max_splits,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fh_steps = [1, 2, 3, 6, 12]\n",
    "cv_sliding = SlidingWindowSplitter(fh=fh_steps, window_length=60, step_length=12)\n",
    "cv_expanding = ExpandingWindowSplitter(fh=fh_steps, initial_window=60, step_length=12)\n",
    "\n",
    "fig = plot_cv_splits(y, cv_sliding, title=\"Sliding Window CV\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cv_splits(y, cv_expanding, title=\"Expanding Window CV\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with time-aware CV\n",
    "\n",
    "`sktime` provides `ForecastingGridSearchCV` to tune parameters while **respecting time order**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_error\n",
    "\n",
    "forecaster = NaiveForecaster()\n",
    "param_grid = {\n",
    "    \"strategy\": [\"last\", \"mean\", \"drift\"],\n",
    "    \"window_length\": [3, 6, 12],\n",
    "}\n",
    "\n",
    "# Use expanding windows for tuning\n",
    "cv = ExpandingWindowSplitter(fh=fh_steps, initial_window=60, step_length=12)\n",
    "\n",
    "gscv = ForecastingGridSearchCV(\n",
    "    forecaster=forecaster,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=mean_absolute_error,\n",
    ")\n",
    "\n",
    "gscv.fit(y_train)\n",
    "\n",
    "best_forecaster = gscv.best_forecaster_\n",
    "print(\"Best params:\", gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls checklist\n",
    "\n",
    "- **Leakage**: never use future data to compute features or scalers.\n",
    "- **Horizon mismatch**: ensure `fh` aligns with how you evaluate.\n",
    "- **Changing seasonality**: prefer windowed CV when regimes drift.\n",
    "- **Sparse data**: keep `window_length` large enough to capture seasonality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(\n",
    "    model_errors: dict, \n",
    "    confidence: float = 0.95,\n",
    "    title: str = \"Model Comparison (Cross-Validation)\",\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Bar chart comparing models with error bars (confidence intervals).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_errors : dict\n",
    "        Dictionary mapping model names to error arrays.\n",
    "    confidence : float\n",
    "        Confidence level for error bars.\n",
    "    title : str\n",
    "        Plot title.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fig : go.Figure\n",
    "    \"\"\"\n",
    "    model_names = list(model_errors.keys())\n",
    "    means = []\n",
    "    ci_lowers = []\n",
    "    ci_uppers = []\n",
    "    \n",
    "    for name, errors in model_errors.items():\n",
    "        stats = compute_cv_statistics(errors, confidence)\n",
    "        means.append(stats[\"mean\"])\n",
    "        ci_lowers.append(stats[\"mean\"] - stats[\"ci_lower\"])\n",
    "        ci_uppers.append(stats[\"ci_upper\"] - stats[\"mean\"])\n",
    "    \n",
    "    # Color scale based on mean (lower is better)\n",
    "    colors = [\"#2ecc71\" if m == min(means) else \"#3498db\" for m in means]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=model_names,\n",
    "            y=means,\n",
    "            error_y=dict(\n",
    "                type=\"data\",\n",
    "                symmetric=False,\n",
    "                array=ci_uppers,\n",
    "                arrayminus=ci_lowers,\n",
    "                color=\"black\",\n",
    "                thickness=1.5,\n",
    "                width=4,\n",
    "            ),\n",
    "            marker_color=colors,\n",
    "            text=[f\"{m:.2f}\" for m in means],\n",
    "            textposition=\"outside\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, font=dict(size=16)),\n",
    "        xaxis=dict(title=\"Model\", tickangle=0),\n",
    "        yaxis=dict(title=\"Mean Absolute Error\", gridcolor=\"lightgray\"),\n",
    "        height=400,\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "    \n",
    "    # Add annotation for best model\n",
    "    best_model = model_names[means.index(min(means))]\n",
    "    fig.add_annotation(\n",
    "        x=best_model,\n",
    "        y=min(means),\n",
    "        text=\"★ Best\",\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        arrowcolor=\"#27ae60\",\n",
    "        font=dict(color=\"#27ae60\", size=12),\n",
    "        yshift=30,\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Compare all forecasters\n",
    "fig_compare = plot_model_comparison(\n",
    "    all_errors,\n",
    "    title=\"Naive Forecaster Comparison (95% CI)\",\n",
    ")\n",
    "fig_compare.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nDetailed Cross-Validation Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "for name, errors in all_errors.items():\n",
    "    stats = compute_cv_statistics(errors)\n",
    "    print(f\"{name:15s} | MAE: {stats['mean']:6.2f} ± {stats['std']:5.2f} | \"\n",
    "          f\"95% CI: [{stats['ci_lower']:6.2f}, {stats['ci_upper']:6.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbbc89",
   "metadata": {},
   "source": [
    "### Model Comparison with Error Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49601e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distribution(errors: np.ndarray, title: str = \"CV Error Distribution\") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Plot histogram and box plot of CV errors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    errors : np.ndarray\n",
    "        Array of error values from each fold.\n",
    "    title : str\n",
    "        Plot title.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fig : go.Figure\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Error Histogram\", \"Error Box Plot\"),\n",
    "        column_widths=[0.6, 0.4],\n",
    "    )\n",
    "    \n",
    "    # Histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=errors,\n",
    "            nbinsx=max(5, len(errors) // 2),\n",
    "            marker_color=\"steelblue\",\n",
    "            opacity=0.7,\n",
    "            name=\"Errors\",\n",
    "        ),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    \n",
    "    # Mean line\n",
    "    mean_err = np.mean(errors)\n",
    "    fig.add_vline(\n",
    "        x=mean_err,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=f\"Mean: {mean_err:.2f}\",\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    \n",
    "    # Box plot\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=errors,\n",
    "            marker_color=\"darkorange\",\n",
    "            name=\"CV Errors\",\n",
    "            boxmean=\"sd\",\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, font=dict(size=16)),\n",
    "        showlegend=False,\n",
    "        height=350,\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"MAE\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"MAE\", row=1, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Collect errors for multiple forecasters\n",
    "forecasters = {\n",
    "    \"Naive Last\": naive_last_forecaster,\n",
    "    \"Naive Mean\": naive_mean_forecaster,\n",
    "    \"Naive Drift\": naive_drift_forecaster,\n",
    "}\n",
    "\n",
    "all_errors = {}\n",
    "for name, forecaster_fn in forecasters.items():\n",
    "    cv_gen = sliding_window_cv(len(y_synth), window_size=60, horizon=12, step=6)\n",
    "    all_errors[name] = cross_validate_forecaster(y_synth, cv_gen, forecaster_fn)\n",
    "\n",
    "# Plot error distribution for drift forecaster\n",
    "fig_dist = plot_error_distribution(all_errors[\"Naive Drift\"], title=\"Naive Drift - CV Error Distribution\")\n",
    "fig_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38abd4",
   "metadata": {},
   "source": [
    "### Error Distribution Across Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc208cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def plot_cv_splits_numpy(\n",
    "    n: int,\n",
    "    cv_generator_fn: Callable,\n",
    "    max_splits: int = 8,\n",
    "    title: str = \"Cross-Validation Splits\",\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Visualize CV splits as train/test windows using Plotly.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Total number of observations.\n",
    "    cv_generator_fn : Callable\n",
    "        Function that returns a CV split generator.\n",
    "    max_splits : int\n",
    "        Maximum number of splits to display.\n",
    "    title : str\n",
    "        Plot title.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fig : go.Figure\n",
    "        Plotly figure object.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    splits_shown = 0\n",
    "    for fold, (train_idx, test_idx) in enumerate(cv_generator_fn()):\n",
    "        if splits_shown >= max_splits:\n",
    "            break\n",
    "        \n",
    "        # Train markers\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=train_idx,\n",
    "                y=[fold] * len(train_idx),\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=\"steelblue\", size=6, symbol=\"square\"),\n",
    "                name=\"Train\" if fold == 0 else None,\n",
    "                showlegend=fold == 0,\n",
    "                legendgroup=\"train\",\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Test markers\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=test_idx,\n",
    "                y=[fold] * len(test_idx),\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=\"darkorange\", size=8, symbol=\"circle\"),\n",
    "                name=\"Test\" if fold == 0 else None,\n",
    "                showlegend=fold == 0,\n",
    "                legendgroup=\"test\",\n",
    "            )\n",
    "        )\n",
    "        splits_shown += 1\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, font=dict(size=16)),\n",
    "        xaxis=dict(title=\"Time Index\", gridcolor=\"lightgray\"),\n",
    "        yaxis=dict(\n",
    "            title=\"Fold #\",\n",
    "            tickmode=\"linear\",\n",
    "            dtick=1,\n",
    "            autorange=\"reversed\",\n",
    "            gridcolor=\"lightgray\",\n",
    "        ),\n",
    "        height=300 + 35 * max_splits,\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Sliding window visualization\n",
    "fig_sliding = plot_cv_splits_numpy(\n",
    "    n=120,\n",
    "    cv_generator_fn=lambda: sliding_window_cv(120, window_size=60, horizon=12, step=12),\n",
    "    title=\"Sliding Window CV (NumPy Implementation)\",\n",
    ")\n",
    "fig_sliding.show()\n",
    "\n",
    "# Expanding window visualization  \n",
    "fig_expanding = plot_cv_splits_numpy(\n",
    "    n=120,\n",
    "    cv_generator_fn=lambda: expanding_window_cv(120, min_train=60, horizon=12, step=12),\n",
    "    title=\"Expanding Window CV (NumPy Implementation)\",\n",
    ")\n",
    "fig_expanding.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd6035",
   "metadata": {},
   "source": [
    "### Plotly Visualization: CV Split Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309912b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_forecaster(\n",
    "    y: np.ndarray,\n",
    "    cv_splits: Generator,\n",
    "    forecaster_fn: Callable[[np.ndarray], Callable[[int], np.ndarray]],\n",
    "    metric_fn: Callable[[np.ndarray, np.ndarray], float] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a forecaster across CV splits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Time series values (1D array).\n",
    "    cv_splits : Generator\n",
    "        Generator yielding (train_idx, test_idx) tuples.\n",
    "    forecaster_fn : Callable\n",
    "        Function that takes training data and returns a forecast function.\n",
    "        The forecast function takes horizon length and returns predictions.\n",
    "    metric_fn : Callable, optional\n",
    "        Error metric function(y_true, y_pred) -> float.\n",
    "        Defaults to Mean Absolute Error.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    errors : np.ndarray\n",
    "        Array of error values for each fold.\n",
    "    \"\"\"\n",
    "    if metric_fn is None:\n",
    "        metric_fn = lambda y_true, y_pred: np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    errors = []\n",
    "    for train_idx, test_idx in cv_splits:\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        # Fit forecaster and predict\n",
    "        predict_fn = forecaster_fn(y_train)\n",
    "        y_pred = predict_fn(len(test_idx))\n",
    "        \n",
    "        # Compute error\n",
    "        error = metric_fn(y_test, y_pred)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.array(errors)\n",
    "\n",
    "\n",
    "def compute_cv_statistics(\n",
    "    errors: np.ndarray, confidence: float = 0.95\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute summary statistics for cross-validation errors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    errors : np.ndarray\n",
    "        Array of error values from each CV fold.\n",
    "    confidence : float\n",
    "        Confidence level for interval (default 0.95).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    stats : dict\n",
    "        Dictionary with mean, std, se, ci_lower, ci_upper.\n",
    "    \"\"\"\n",
    "    n = len(errors)\n",
    "    mean = np.mean(errors)\n",
    "    std = np.std(errors, ddof=1)\n",
    "    se = std / np.sqrt(n)\n",
    "    \n",
    "    # t-distribution for small samples\n",
    "    t_crit = stats.t.ppf((1 + confidence) / 2, df=n - 1)\n",
    "    ci_lower = mean - t_crit * se\n",
    "    ci_upper = mean + t_crit * se\n",
    "    \n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"se\": se,\n",
    "        \"ci_lower\": ci_lower,\n",
    "        \"ci_upper\": ci_upper,\n",
    "        \"n_folds\": n,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define simple forecasters (NumPy only)\n",
    "def naive_last_forecaster(y_train: np.ndarray) -> Callable[[int], np.ndarray]:\n",
    "    \"\"\"Naive forecaster: predict last observed value.\"\"\"\n",
    "    last_value = y_train[-1]\n",
    "    return lambda h: np.full(h, last_value)\n",
    "\n",
    "\n",
    "def naive_mean_forecaster(y_train: np.ndarray) -> Callable[[int], np.ndarray]:\n",
    "    \"\"\"Mean forecaster: predict mean of training data.\"\"\"\n",
    "    mean_value = np.mean(y_train)\n",
    "    return lambda h: np.full(h, mean_value)\n",
    "\n",
    "\n",
    "def naive_drift_forecaster(y_train: np.ndarray) -> Callable[[int], np.ndarray]:\n",
    "    \"\"\"Drift forecaster: extend linear trend from first to last observation.\"\"\"\n",
    "    n = len(y_train)\n",
    "    slope = (y_train[-1] - y_train[0]) / (n - 1)\n",
    "    last_value = y_train[-1]\n",
    "    return lambda h: last_value + slope * np.arange(1, h + 1)\n",
    "\n",
    "\n",
    "# Example: Cross-validate on synthetic data\n",
    "np.random.seed(42)\n",
    "y_synth = 100 + 0.5 * np.arange(120) + 10 * np.sin(np.arange(120) * 2 * np.pi / 12) + np.random.randn(120) * 5\n",
    "\n",
    "# Evaluate naive_last forecaster\n",
    "cv_gen = sliding_window_cv(len(y_synth), window_size=60, horizon=12, step=12)\n",
    "errors = cross_validate_forecaster(y_synth, cv_gen, naive_last_forecaster)\n",
    "\n",
    "cv_stats = compute_cv_statistics(errors)\n",
    "print(\"Cross-Validation Statistics (Naive Last):\")\n",
    "print(f\"  Mean MAE:  {cv_stats['mean']:.3f}\")\n",
    "print(f\"  Std:       {cv_stats['std']:.3f}\")\n",
    "print(f\"  95% CI:    [{cv_stats['ci_lower']:.3f}, {cv_stats['ci_upper']:.3f}]\")\n",
    "print(f\"  # Folds:   {cv_stats['n_folds']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d567aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Tuple, Callable\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "def sliding_window_cv(\n",
    "    n: int, window_size: int, horizon: int, step: int = 1\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    \"\"\"\n",
    "    Generate sliding window cross-validation splits.\n",
    "    \n",
    "    Train window is fixed-length, slides forward by `step` each fold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Total number of observations.\n",
    "    window_size : int\n",
    "        Fixed training window length.\n",
    "    horizon : int\n",
    "        Forecast horizon (test set length).\n",
    "    step : int\n",
    "        Step size between consecutive folds.\n",
    "        \n",
    "    Yields\n",
    "    ------\n",
    "    train_idx, test_idx : Tuple[np.ndarray, np.ndarray]\n",
    "        Index arrays for train and test sets.\n",
    "    \"\"\"\n",
    "    start = window_size\n",
    "    while start + horizon <= n:\n",
    "        train_idx = np.arange(start - window_size, start)\n",
    "        test_idx = np.arange(start, start + horizon)\n",
    "        yield train_idx, test_idx\n",
    "        start += step\n",
    "\n",
    "\n",
    "def expanding_window_cv(\n",
    "    n: int, min_train: int, horizon: int, step: int = 1\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    \"\"\"\n",
    "    Generate expanding window cross-validation splits.\n",
    "    \n",
    "    Training window starts at index 0 and grows each fold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Total number of observations.\n",
    "    min_train : int\n",
    "        Minimum initial training set size.\n",
    "    horizon : int\n",
    "        Forecast horizon (test set length).\n",
    "    step : int\n",
    "        Step size between consecutive folds.\n",
    "        \n",
    "    Yields\n",
    "    ------\n",
    "    train_idx, test_idx : Tuple[np.ndarray, np.ndarray]\n",
    "        Index arrays for train and test sets.\n",
    "    \"\"\"\n",
    "    start = min_train\n",
    "    while start + horizon <= n:\n",
    "        train_idx = np.arange(0, start)\n",
    "        test_idx = np.arange(start, start + horizon)\n",
    "        yield train_idx, test_idx\n",
    "        start += step\n",
    "\n",
    "\n",
    "# Example usage\n",
    "n_obs = 120\n",
    "print(\"Sliding Window CV splits (first 5):\")\n",
    "for i, (tr, te) in enumerate(sliding_window_cv(n_obs, window_size=60, horizon=12, step=12)):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"  Fold {i+1}: train [{tr[0]:3d}, {tr[-1]:3d}], test [{te[0]:3d}, {te[-1]:3d}]\")\n",
    "\n",
    "print(\"\\nExpanding Window CV splits (first 5):\")\n",
    "for i, (tr, te) in enumerate(expanding_window_cv(n_obs, min_train=60, horizon=12, step=12)):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"  Fold {i+1}: train [{tr[0]:3d}, {tr[-1]:3d}], test [{te[0]:3d}, {te[-1]:3d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325c9f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Low-Level NumPy Implementation\n",
    "\n",
    "Pure NumPy generators for time-series cross-validation splits, without external dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}