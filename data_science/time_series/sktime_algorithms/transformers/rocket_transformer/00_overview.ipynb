{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROCKET Transformer: Random Convolutional Kernel Transform\n",
    "\n",
    "**ROCKET** (RandOm Convolutional KErnel Transform) is a fast and accurate time series classification method that transforms time series into features using thousands of random convolutional kernels.\n",
    "\n",
    "## Key Intuition\n",
    "\n",
    "Instead of learning kernels (like CNNs), ROCKET generates **random kernels** with random:\n",
    "- **Weights** sampled from {-1, 0, 1}\n",
    "- **Lengths** ‚àà {7, 9, 11}\n",
    "- **Biases** sampled uniformly\n",
    "- **Dilations** for multi-scale pattern detection\n",
    "- **Padding** (valid or same)\n",
    "\n",
    "Each kernel extracts **two features**:\n",
    "1. **PPV (Proportion of Positive Values)**: Captures how often the pattern appears\n",
    "2. **Global Max Pooling**: Captures the strongest activation\n",
    "\n",
    "With 10,000 kernels ‚Üí 20,000 features, combined with a simple linear classifier (Ridge), this achieves state-of-the-art accuracy!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36312c1",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation\n",
    "\n",
    "### Kernel Definition\n",
    "\n",
    "A ROCKET kernel is defined as a tuple:\n",
    "\n",
    "$$k = (w, l, b, d, p)$$\n",
    "\n",
    "Where:\n",
    "- $w \\in \\{-1, 0, 1\\}^l$ ‚Äî **kernel weights** (randomly sampled)\n",
    "- $l \\in \\{7, 9, 11\\}$ ‚Äî **kernel length**\n",
    "- $b \\in \\mathbb{R}$ ‚Äî **bias** (sampled from quantiles of convolution output)\n",
    "- $d \\in \\mathbb{Z}^+$ ‚Äî **dilation factor** (exponentially distributed)\n",
    "- $p \\in \\{0, \\lfloor(l-1) \\cdot d / 2\\rfloor\\}$ ‚Äî **padding** (valid or same)\n",
    "\n",
    "### Dilated Convolution Operation\n",
    "\n",
    "For an input time series $x$ of length $T$, the dilated convolution output at position $i$ is:\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{l-1} w_j \\cdot x_{i + j \\cdot d} + b$$\n",
    "\n",
    "The **dilation factor** $d$ determines the spacing between kernel elements:\n",
    "- $d = 1$: Standard convolution (consecutive elements)\n",
    "- $d = 2$: Skip every other element (receptive field = $2l - 1$)\n",
    "- $d = k$: Receptive field spans $(l-1) \\cdot k + 1$ time steps\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "From each convolution output $z = (z_1, z_2, \\ldots, z_n)$, we extract **two features**:\n",
    "\n",
    "#### Proportion of Positive Values (PPV)\n",
    "\n",
    "$$\\text{PPV}(z) = \\frac{1}{|z|}\\sum_{i=1}^{|z|} \\mathbf{1}_{z_i > 0}$$\n",
    "\n",
    "**Intuition**: PPV measures *how frequently* a pattern appears in the series. A high PPV means the pattern (after bias adjustment) is common.\n",
    "\n",
    "#### Global Max Pooling (GMP)\n",
    "\n",
    "$$\\text{GMP}(z) = \\max_{i=1}^{|z|} z_i$$\n",
    "\n",
    "**Intuition**: GMP captures the *strongest match* to the pattern anywhere in the series.\n",
    "\n",
    "### Total Feature Count\n",
    "\n",
    "With $n_{\\text{kernels}}$ random kernels, the total feature dimension is:\n",
    "\n",
    "$$\\text{Features} = 2 \\times n_{\\text{kernels}}$$\n",
    "\n",
    "For the default 10,000 kernels ‚Üí **20,000 features**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Setup and Data Loading\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sktime.datasets import load_basic_motions, load_unit_test\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_basic_motions(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_basic_motions(split=\"test\", return_X_y=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cf1eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Using sktime's ROCKET (Production Ready)\n",
    "\n",
    "For production use, always prefer the optimized sktime implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Standardize features\n",
    "features_scaled = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)\n",
    "\n",
    "# Cross-validation\n",
    "clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "scores = cross_val_score(clf, features_scaled, y_synthetic, cv=5)\n",
    "\n",
    "print(f\"üéØ Classification Results (5-fold CV):\")\n",
    "print(f\"   Accuracy: {scores.mean():.2%} ¬± {scores.std():.2%}\")\n",
    "\n",
    "# Visualize the transformed feature space\n",
    "pca_custom = PCA(n_components=2)\n",
    "features_pca = pca_custom.fit_transform(features_scaled)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=features_pca[:, 0], y=features_pca[:, 1],\n",
    "    color=['Sine' if y == 0 else 'Sawtooth' for y in y_synthetic],\n",
    "    title=\"<b>Custom ROCKET Implementation: Feature Space</b><br><sup>Synthetic data: Sine waves vs Sawtooth waves</sup>\",\n",
    "    labels={'x': 'PC1', 'y': 'PC2', 'color': 'Class'},\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=12, opacity=0.8))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebba2be",
   "metadata": {},
   "source": [
    "## 8. Validate Our Implementation\n",
    "\n",
    "Let's verify that our low-level implementation produces reasonable results by training a classifier on our synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee48f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocket_transform(X, n_kernels, random_state=42):\n",
    "    \"\"\"\n",
    "    Full ROCKET transformation: time series ‚Üí feature vectors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        Time series data of shape (n_samples, series_length)\n",
    "    n_kernels : int\n",
    "        Number of random kernels to generate\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array : Feature matrix of shape (n_samples, 2 * n_kernels)\n",
    "        Each kernel produces 2 features (PPV and max)\n",
    "    \n",
    "    Algorithm\n",
    "    ---------\n",
    "    1. Generate n_kernels random kernels\n",
    "    2. For each kernel:\n",
    "       a. Compute convolution output for all series\n",
    "       b. Set bias based on quantile of outputs\n",
    "       c. Extract PPV and max features\n",
    "    3. Return concatenated features\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    n_samples, series_length = X.shape\n",
    "    \n",
    "    # Output: 2 features per kernel (PPV and max)\n",
    "    features = np.zeros((n_samples, 2 * n_kernels))\n",
    "    \n",
    "    # Store kernels for reference\n",
    "    kernels = []\n",
    "    \n",
    "    for k in range(n_kernels):\n",
    "        # Generate random kernel\n",
    "        kernel_params = generate_rocket_kernel(series_length, random_state=None)\n",
    "        kernels.append(kernel_params)\n",
    "        \n",
    "        weights = kernel_params['weights']\n",
    "        dilation = kernel_params['dilation']\n",
    "        padding = kernel_params['padding']\n",
    "        \n",
    "        # Compute convolution for all samples to determine bias\n",
    "        all_conv_outputs = []\n",
    "        for i in range(n_samples):\n",
    "            conv_out = dilated_convolution(X[i], weights, dilation, padding)\n",
    "            all_conv_outputs.append(conv_out)\n",
    "        \n",
    "        # Set bias based on quantile of all convolution outputs\n",
    "        # This makes PPV distribution roughly uniform across kernels\n",
    "        all_values = np.concatenate(all_conv_outputs)\n",
    "        if len(all_values) > 0:\n",
    "            bias = -np.quantile(all_values, np.random.uniform(0, 1))\n",
    "        else:\n",
    "            bias = 0.0\n",
    "        \n",
    "        # Extract features for each sample\n",
    "        for i in range(n_samples):\n",
    "            ppv, gmp = compute_ppv_and_max(all_conv_outputs[i], bias)\n",
    "            features[i, 2 * k] = ppv\n",
    "            features[i, 2 * k + 1] = gmp\n",
    "    \n",
    "    return features, kernels\n",
    "\n",
    "# Test on a small dataset\n",
    "print(\"üöÄ Testing ROCKET Implementation...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create simple synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples = 20\n",
    "series_length = 100\n",
    "\n",
    "# Two classes: sine waves vs sawtooth waves\n",
    "X_sine = np.sin(np.linspace(0, 4*np.pi, series_length)) + 0.1 * np.random.randn(n_samples//2, series_length)\n",
    "X_saw = np.tile(np.linspace(-1, 1, series_length//4), 4) + 0.1 * np.random.randn(n_samples//2, series_length)\n",
    "X_synthetic = np.vstack([X_sine, X_saw])\n",
    "y_synthetic = np.array([0]*(n_samples//2) + [1]*(n_samples//2))\n",
    "\n",
    "# Apply ROCKET\n",
    "features, kernels = rocket_transform(X_synthetic, n_kernels=100, random_state=42)\n",
    "\n",
    "print(f\"Input shape: {X_synthetic.shape}\")\n",
    "print(f\"Output shape: {features.shape}\")\n",
    "print(f\"Number of kernels: {len(kernels)}\")\n",
    "print(f\"\\nFirst 5 features of sample 0: {features[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e99ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppv_and_max(conv_output, bias=0.0):\n",
    "    \"\"\"\n",
    "    Compute the two ROCKET features from convolution output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    conv_output : np.array\n",
    "        Output from dilated convolution\n",
    "    bias : float\n",
    "        Bias term subtracted before computing PPV\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (ppv, global_max)\n",
    "        - ppv: Proportion of Positive Values after bias\n",
    "        - global_max: Maximum value in convolution output\n",
    "    \n",
    "    Mathematical Formulas\n",
    "    ---------------------\n",
    "    PPV = (1/n) * sum(1_{z_i > 0})  where z_i = conv_output[i] + bias\n",
    "    GMP = max(conv_output)\n",
    "    \n",
    "    Intuition\n",
    "    ---------\n",
    "    - PPV: Measures how frequently the pattern appears (high = common pattern)\n",
    "    - GMP: Captures the strongest match to the pattern anywhere in the series\n",
    "    \"\"\"\n",
    "    if len(conv_output) == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # Add bias to convolution output\n",
    "    z = conv_output + bias\n",
    "    \n",
    "    # PPV: Proportion of values > 0\n",
    "    ppv = np.mean(z > 0)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    global_max = np.max(conv_output)  # Note: max is computed before bias in original paper\n",
    "    \n",
    "    return ppv, global_max\n",
    "\n",
    "# Example\n",
    "example_conv = np.array([-2, -1, 0, 1, 2, 3, 2, 1, 0, -1])\n",
    "ppv, gmp = compute_ppv_and_max(example_conv, bias=0)\n",
    "print(f\"Example convolution output: {example_conv}\")\n",
    "print(f\"PPV (bias=0): {ppv:.2f} ({int(ppv * len(example_conv))}/{len(example_conv)} positive)\")\n",
    "print(f\"Global Max: {gmp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_convolution(x, kernel, dilation, padding):\n",
    "    \"\"\"\n",
    "    Perform 1D dilated convolution with optional padding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        Input time series of shape (T,)\n",
    "    kernel : np.array\n",
    "        Convolution kernel of shape (L,)\n",
    "    dilation : int\n",
    "        Dilation factor (spacing between kernel elements)\n",
    "    padding : int\n",
    "        Number of zeros to pad on each side\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array : Convolution output\n",
    "    \n",
    "    Mathematical Formula\n",
    "    --------------------\n",
    "    z_i = sum_{j=0}^{L-1} kernel[j] * x[i + j * dilation]\n",
    "    \n",
    "    where:\n",
    "    - L is kernel length\n",
    "    - dilation determines spacing between sampled input elements\n",
    "    - Receptive field = (L - 1) * dilation + 1\n",
    "    \"\"\"\n",
    "    L = len(kernel)\n",
    "    receptive_field = (L - 1) * dilation + 1\n",
    "    \n",
    "    # Apply padding\n",
    "    if padding > 0:\n",
    "        x_padded = np.pad(x, (padding, padding), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        x_padded = x\n",
    "    \n",
    "    # Calculate output length\n",
    "    output_length = len(x_padded) - receptive_field + 1\n",
    "    \n",
    "    if output_length <= 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Compute dilated convolution\n",
    "    output = np.zeros(output_length)\n",
    "    \n",
    "    for i in range(output_length):\n",
    "        # Gather input samples with dilation spacing\n",
    "        indices = i + np.arange(L) * dilation\n",
    "        output[i] = np.dot(kernel, x_padded[indices])\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test the dilated convolution\n",
    "test_x = np.sin(np.linspace(0, 4 * np.pi, 50))\n",
    "test_kernel = np.array([1, -2, 1])  # Second derivative approximation\n",
    "\n",
    "for d in [1, 2, 4]:\n",
    "    out = dilated_convolution(test_x, test_kernel, dilation=d, padding=0)\n",
    "    print(f\"Dilation={d}: Output length={len(out)}, Receptive field={(len(test_kernel)-1)*d + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf25b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rocket_kernel(input_length, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate a single random ROCKET kernel with all parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_length : int\n",
    "        Length of the input time series\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Kernel parameters\n",
    "        - weights: np.array of shape (length,) with values in {-1, 0, 1}\n",
    "        - length: int, kernel length from {7, 9, 11}\n",
    "        - bias: float, bias term (set later based on convolution output)\n",
    "        - dilation: int, dilation factor\n",
    "        - padding: int, padding amount\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # 1. Sample kernel length from {7, 9, 11}\n",
    "    length = np.random.choice([7, 9, 11])\n",
    "    \n",
    "    # 2. Sample weights from {-1, 0, 1} with equal probability\n",
    "    weights = np.random.choice([-1, 0, 1], size=length).astype(np.float64)\n",
    "    \n",
    "    # Ensure at least one non-zero weight\n",
    "    if np.all(weights == 0):\n",
    "        weights[np.random.randint(length)] = np.random.choice([-1, 1])\n",
    "    \n",
    "    # 3. Calculate maximum dilation to fit within input length\n",
    "    # Receptive field = (length - 1) * dilation + 1 <= input_length\n",
    "    max_dilation = (input_length - 1) // (length - 1)\n",
    "    \n",
    "    # 4. Sample dilation from exponential distribution (biased toward small values)\n",
    "    # This ensures we get both fine-grained and coarse patterns\n",
    "    dilation = int(2 ** np.random.uniform(0, np.log2(max_dilation + 1)))\n",
    "    dilation = max(1, min(dilation, max_dilation))  # Clamp to valid range\n",
    "    \n",
    "    # 5. Determine padding: either 'valid' (0) or 'same' ((length-1)*dilation // 2)\n",
    "    if np.random.random() < 0.5:\n",
    "        padding = 0  # Valid convolution\n",
    "    else:\n",
    "        padding = (length - 1) * dilation // 2  # Same-ish convolution\n",
    "    \n",
    "    # 6. Bias will be set later based on convolution output quantiles\n",
    "    bias = 0.0  # Placeholder\n",
    "    \n",
    "    return {\n",
    "        'weights': weights,\n",
    "        'length': length,\n",
    "        'bias': bias,\n",
    "        'dilation': dilation,\n",
    "        'padding': padding\n",
    "    }\n",
    "\n",
    "# Example: Generate and display a kernel\n",
    "example_kernel = generate_rocket_kernel(100, random_state=42)\n",
    "print(\"üîß Example ROCKET Kernel:\")\n",
    "for key, value in example_kernel.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa15b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Low-Level NumPy Implementation\n",
    "\n",
    "Let's implement ROCKET from scratch to understand every component. This implementation follows the original paper's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c728fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization\n",
    "fig_3d = px.scatter_3d(\n",
    "    x=X_pca[:, 0], y=X_pca[:, 1], z=X_pca[:, 2],\n",
    "    color=y_all,\n",
    "    labels={'x': 'PC1', 'y': 'PC2', 'z': 'PC3', 'color': 'Class'},\n",
    "    title=\"<b>3D ROCKET Feature Space</b><br><sup>Interactive: Rotate to explore class separability</sup>\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig_3d.update_traces(marker=dict(size=5, opacity=0.8))\n",
    "fig_3d.update_layout(height=500)\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b44ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Transform time series using ROCKET\n",
    "rocket = Rocket(num_kernels=1000, random_state=42)  # Fewer kernels for speed\n",
    "X_train_transformed = rocket.fit_transform(X_train)\n",
    "X_test_transformed = rocket.transform(X_test)\n",
    "\n",
    "# Combine for visualization\n",
    "X_all = np.vstack([X_train_transformed, X_test_transformed])\n",
    "y_all = np.concatenate([y_train, y_test])\n",
    "split_labels = ['Train'] * len(y_train) + ['Test'] * len(y_test)\n",
    "\n",
    "# Standardize and apply PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create 2D visualization\n",
    "fig = px.scatter(\n",
    "    x=X_pca[:, 0], y=X_pca[:, 1],\n",
    "    color=y_all,\n",
    "    symbol=split_labels,\n",
    "    labels={'x': f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)',\n",
    "            'y': f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)',\n",
    "            'color': 'Class', 'symbol': 'Split'},\n",
    "    title=\"<b>ROCKET Feature Space (PCA Projection)</b><br><sup>Each point is a time series transformed by 1000 random kernels</sup>\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "fig.show()\n",
    "\n",
    "print(f\"üìä PCA Explained Variance:\")\n",
    "print(f\"   PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"   PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"   PC3: {pca.explained_variance_ratio_[2]:.2%}\")\n",
    "print(f\"   Total (3 PCs): {sum(pca.explained_variance_ratio_[:3]):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee0f5e",
   "metadata": {},
   "source": [
    "## 6. Feature Space Visualization (PCA)\n",
    "\n",
    "After ROCKET transforms time series into 20,000 features, we can visualize the learned feature space using dimensionality reduction. If ROCKET works well, different classes should form separable clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_convolution_1d(x, weights, dilation, padding=0):\n",
    "    \"\"\"\n",
    "    Perform 1D dilated convolution.\n",
    "    \n",
    "    Dilated convolution samples input with spacing = dilation.\n",
    "    Receptive field = (len(weights) - 1) * dilation + 1\n",
    "    \"\"\"\n",
    "    l = len(weights)\n",
    "    receptive_field = (l - 1) * dilation + 1\n",
    "    \n",
    "    # Add padding\n",
    "    if padding > 0:\n",
    "        x_padded = np.pad(x, (padding, padding), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        x_padded = x\n",
    "    \n",
    "    # Output length\n",
    "    out_len = len(x_padded) - receptive_field + 1\n",
    "    output = np.zeros(out_len)\n",
    "    \n",
    "    for i in range(out_len):\n",
    "        # Sample with dilation\n",
    "        indices = i + np.arange(l) * dilation\n",
    "        output[i] = np.sum(weights * x_padded[indices.astype(int)])\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Demonstrate dilation effect\n",
    "kernel = np.array([1, -1, 1, -1, 1])  # Edge detector\n",
    "dilations = [1, 2, 4, 8]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(dilations) + 1, cols=1,\n",
    "    subplot_titles=[\"Original Time Series\"] + [f\"Dilation = {d} (Receptive Field = {(len(kernel)-1)*d + 1})\" for d in dilations],\n",
    "    vertical_spacing=0.06\n",
    ")\n",
    "\n",
    "# Original series\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_axis, y=sample_series, mode='lines', line=dict(color='#636EFA')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Convolution outputs with different dilations\n",
    "colors = ['#00CC96', '#EF553B', '#AB63FA', '#FFA15A']\n",
    "for idx, d in enumerate(dilations):\n",
    "    conv_out = dilated_convolution_1d(sample_series, kernel, dilation=d)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.arange(len(conv_out)), y=conv_out, mode='lines', \n",
    "                   line=dict(color=colors[idx]), name=f'd={d}'),\n",
    "        row=idx + 2, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Effect of Dilation on Convolution</b><br><sup>Higher dilation ‚Üí larger receptive field ‚Üí captures longer-range patterns</sup>\",\n",
    "    height=700,\n",
    "    template=\"plotly_white\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Visualize receptive field\n",
    "print(\"\\nüîç Receptive Field Visualization:\")\n",
    "print(\"   Kernel length: 5\")\n",
    "for d in dilations:\n",
    "    rf = (len(kernel) - 1) * d + 1\n",
    "    pattern = \"\".join([\"‚óè\" if i % d == 0 and i // d < len(kernel) else \"‚óã\" for i in range(rf)])\n",
    "    print(f\"   Dilation={d}: {pattern} (spans {rf} time steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676fc72",
   "metadata": {},
   "source": [
    "## 5. Effect of Dilation on Receptive Field\n",
    "\n",
    "Dilation is a key innovation in ROCKET. It allows small kernels to capture patterns at multiple time scales without increasing the number of parameters.\n",
    "\n",
    "**Intuition**: \n",
    "- **Dilation = 1**: Kernel looks at consecutive time steps (fine-grained patterns)\n",
    "- **Dilation = 4**: Kernel skips every 3 time steps (captures slower, larger-scale patterns)\n",
    "- **Receptive field** = $(l-1) \\times d + 1$ time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c013ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample time series\n",
    "sample_series = X_train.iloc[0, 0].values  # First series, first dimension\n",
    "time_axis = np.arange(len(sample_series))\n",
    "\n",
    "# Generate a sample kernel\n",
    "kernel_weights = np.array([1, 0, -1, 0, 1, -1, 1])\n",
    "bias = 0\n",
    "\n",
    "# Perform convolution (valid mode, dilation=1)\n",
    "conv_output = np.convolve(sample_series, kernel_weights[::-1], mode='valid') + bias\n",
    "\n",
    "# Calculate PPV and Max\n",
    "ppv = np.mean(conv_output > 0)\n",
    "gmp = np.max(conv_output)\n",
    "\n",
    "# Create visualization\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=[\"Original Time Series\", \"Kernel Weights\", \"Convolution Output\"],\n",
    "    vertical_spacing=0.12\n",
    ")\n",
    "\n",
    "# Original series\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_axis, y=sample_series, mode='lines', name='Original', line=dict(color='#636EFA')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Kernel\n",
    "kernel_colors = ['#EF553B' if w < 0 else '#00CC96' if w > 0 else '#AB63FA' for w in kernel_weights]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(len(kernel_weights))), y=kernel_weights, marker_color=kernel_colors, name='Kernel'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Convolution output with threshold line\n",
    "conv_time = np.arange(len(conv_output))\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=conv_time, y=conv_output, mode='lines', name='Conv Output', line=dict(color='#00CC96')),\n",
    "    row=3, col=1\n",
    ")\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=3, col=1)\n",
    "\n",
    "# Add annotation for features\n",
    "fig.add_annotation(\n",
    "    x=0.95, y=0.15, xref=\"paper\", yref=\"paper\",\n",
    "    text=f\"<b>PPV = {ppv:.3f}</b><br>GMP = {gmp:.3f}\",\n",
    "    showarrow=False, font=dict(size=14),\n",
    "    bgcolor=\"white\", bordercolor=\"black\", borderwidth=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>How ROCKET Transforms a Time Series</b><br><sup>Kernel slides across series ‚Üí convolution output ‚Üí extract PPV & Max features</sup>\",\n",
    "    height=600,\n",
    "    template=\"plotly_white\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"üìä Feature Extraction Results:\")\n",
    "print(f\"   PPV (Proportion of Positive Values): {ppv:.4f}\")\n",
    "print(f\"   GMP (Global Max Pooling): {gmp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929e93c",
   "metadata": {},
   "source": [
    "## 4. Convolution Output Visualization\n",
    "\n",
    "Let's see how a kernel transforms a time series. The convolution output shows where the pattern appears (high values) and where it doesn't (low values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize random kernels\n",
    "def generate_random_kernel_weights(length):\n",
    "    \"\"\"Generate random kernel weights from {-1, 0, 1}\"\"\"\n",
    "    weights = np.random.choice([-1, 0, 1], size=length)\n",
    "    # Ensure at least one non-zero weight\n",
    "    if np.all(weights == 0):\n",
    "        weights[np.random.randint(length)] = np.random.choice([-1, 1])\n",
    "    return weights\n",
    "\n",
    "# Generate 8 different kernels with varying lengths\n",
    "kernel_lengths = [7, 9, 11, 7, 9, 11, 7, 9]\n",
    "kernels = [generate_random_kernel_weights(l) for l in kernel_lengths]\n",
    "\n",
    "# Create visualization\n",
    "fig = make_subplots(rows=2, cols=4, subplot_titles=[f\"Kernel {i+1} (len={kernel_lengths[i]})\" for i in range(8)])\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    row = idx // 4 + 1\n",
    "    col = idx % 4 + 1\n",
    "    \n",
    "    # Create bar chart for kernel weights\n",
    "    colors = ['#EF553B' if w < 0 else '#00CC96' if w > 0 else '#636EFA' for w in kernel]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(range(len(kernel))), y=kernel, marker_color=colors, showlegend=False),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    fig.update_yaxes(range=[-1.5, 1.5], row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>Random ROCKET Kernel Weights</b><br><sup>Red = -1, Green = +1, Blue = 0</sup>\",\n",
    "    height=400,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5cdff",
   "metadata": {},
   "source": [
    "## 3. Visualizing Random Kernels\n",
    "\n",
    "Let's visualize what random ROCKET kernels look like. Each kernel has weights drawn from {-1, 0, 1} and varying lengths.\n",
    "\n",
    "**Intuition**: These random patterns act like \"detectors\" for different shapes in the time series. Some detect edges (alternating +1/-1), some detect level shifts, and some are more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create pipeline: ROCKET ‚Üí Ridge Classifier\n",
    "pipe = make_pipeline(\n",
    "    Rocket(num_kernels=10_000, random_state=42),\n",
    "    RidgeClassifier(alpha=1.0)\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"üìä Classification Report (sktime ROCKET):\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d50b1",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|---------|\n",
    "| **Core Idea** | Random kernels + simple features (PPV, max) = powerful representations |\n",
    "| **Kernels** | Weights ‚àà {-1, 0, 1}, lengths ‚àà {7, 9, 11}, random dilation & padding |\n",
    "| **Dilation** | Enables multi-scale pattern detection without more parameters |\n",
    "| **Features** | 2 per kernel: PPV (frequency of pattern) + GMP (strongest match) |\n",
    "| **Speed** | Much faster than learned methods (no backprop), scales linearly |\n",
    "| **Accuracy** | State-of-the-art on UCR archive with just 10K kernels + Ridge |\n",
    "\n",
    "### When to Use ROCKET\n",
    "\n",
    "‚úÖ **Use ROCKET when:**\n",
    "- You need fast training and inference\n",
    "- Dataset is small to medium (few hundred to thousands of series)\n",
    "- You want a strong baseline before trying complex models\n",
    "- Interpretability of individual kernels is not critical\n",
    "\n",
    "‚ùå **Consider alternatives when:**\n",
    "- You need end-to-end differentiable models (use InceptionTime, etc.)\n",
    "- Series are extremely long (>10K time steps) - consider MiniRocket\n",
    "- You need kernel interpretability - consider shapelets\n",
    "\n",
    "---\n",
    "\n",
    "**Reference**: Dempster, A., Petitjean, F., & Webb, G. I. (2020). ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. *Data Mining and Knowledge Discovery*, 34(5), 1454-1495."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}