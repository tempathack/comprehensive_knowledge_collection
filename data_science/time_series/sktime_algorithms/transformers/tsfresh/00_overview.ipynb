{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSFresh Feature Extractor\n",
    "\n",
    "**TSFresh** (Time Series Feature extraction based on scalable hypothesis tests) is a powerful Python library that automatically extracts hundreds of time series features. These features capture statistical properties, temporal patterns, and complexity measures that are essential for machine learning on time series data.\n",
    "\n",
    "## Key Capabilities\n",
    "- **800+ Features**: Automatically extracts statistics, autocorrelation, entropy, FFT coefficients, and more\n",
    "- **Relevance Filtering**: Uses statistical hypothesis tests to select only relevant features\n",
    "- **Scalable**: Designed for large-scale feature extraction with parallel processing\n",
    "- **sktime Integration**: Seamlessly works with sktime's transformer API\n",
    "\n",
    "## When to Use TSFresh\n",
    "| Use Case | Recommendation |\n",
    "|----------|----------------|\n",
    "| Tabular ML on time series | ✅ Excellent - converts sequences to fixed-length vectors |\n",
    "| Feature discovery/exploration | ✅ Great for understanding what patterns matter |\n",
    "| Real-time inference | ⚠️ Consider subset of features for speed |\n",
    "| Very long sequences (>10k points) | ⚠️ May be computationally expensive |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5851e0",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation\n",
    "\n",
    "TSFresh extracts features based on well-established mathematical formulas. Understanding these foundations helps interpret what the features capture.\n",
    "\n",
    "### 1.1 Basic Statistical Moments\n",
    "\n",
    "**Mean (Central Tendency):**\n",
    "$$\\mu = \\frac{1}{T}\\sum_{t=1}^{T} x_t$$\n",
    "\n",
    "**Variance (Spread):**\n",
    "$$\\sigma^2 = \\frac{1}{T}\\sum_{t=1}^{T} (x_t - \\mu)^2$$\n",
    "\n",
    "**Skewness (Asymmetry):**\n",
    "$$\\gamma_1 = \\frac{1}{T}\\sum_{t=1}^{T} \\left(\\frac{x_t - \\mu}{\\sigma}\\right)^3$$\n",
    "\n",
    "**Kurtosis (Tail Heaviness):**\n",
    "$$\\gamma_2 = \\frac{1}{T}\\sum_{t=1}^{T} \\left(\\frac{x_t - \\mu}{\\sigma}\\right)^4 - 3$$\n",
    "\n",
    "### 1.2 Autocorrelation Function (ACF)\n",
    "\n",
    "Autocorrelation measures how a time series correlates with lagged versions of itself:\n",
    "\n",
    "$$r_k = \\frac{\\sum_{t=1}^{T-k}(x_t - \\bar{x})(x_{t+k} - \\bar{x})}{\\sum_{t=1}^{T}(x_t - \\bar{x})^2}$$\n",
    "\n",
    "**Intuition:** \n",
    "- $r_k \\approx 1$: Strong positive correlation at lag $k$ (repeating patterns)\n",
    "- $r_k \\approx 0$: No correlation (random walk behavior)\n",
    "- $r_k \\approx -1$: Strong negative correlation (oscillating patterns)\n",
    "\n",
    "### 1.3 Fourier Transform (Frequency Analysis)\n",
    "\n",
    "The Discrete Fourier Transform (DFT) decomposes a signal into frequency components:\n",
    "\n",
    "$$X_k = \\sum_{n=0}^{N-1} x_n \\cdot e^{-2\\pi i k n / N}$$\n",
    "\n",
    "**Power Spectrum:** $P_k = |X_k|^2$ reveals dominant frequencies in the signal.\n",
    "\n",
    "### 1.4 Linear Trend\n",
    "\n",
    "The slope of the best-fit line captures the overall trend:\n",
    "\n",
    "$$\\beta = \\frac{\\sum_{t=1}^{T}(t - \\bar{t})(x_t - \\bar{x})}{\\sum_{t=1}^{T}(t-\\bar{t})^2}$$\n",
    "\n",
    "**Intuition:** $\\beta > 0$ indicates upward trend, $\\beta < 0$ indicates downward trend.\n",
    "\n",
    "### 1.5 Approximate Entropy (Complexity)\n",
    "\n",
    "Measures unpredictability/irregularity in a time series:\n",
    "\n",
    "$$ApEn(m, r) = \\Phi^m(r) - \\Phi^{m+1}(r)$$\n",
    "\n",
    "where $\\Phi^m(r)$ counts similar patterns of length $m$ within tolerance $r$.\n",
    "\n",
    "**Intuition:**\n",
    "- Low ApEn → Regular, predictable patterns\n",
    "- High ApEn → Complex, irregular patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d7a43",
   "metadata": {},
   "source": [
    "## 2. Low-Level NumPy Implementation\n",
    "\n",
    "Let's implement the core TSFresh feature computations from scratch using NumPy to deeply understand how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a28d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def compute_mean_std_skew_kurtosis(x: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute basic statistical moments of a time series.\n",
    "    \n",
    "    These capture the distribution's shape without considering temporal order.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        1D time series array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with mean, std, skewness, kurtosis\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> x = np.random.randn(100)\n",
    "    >>> stats = compute_mean_std_skew_kurtosis(x)\n",
    "    >>> print(f\"Mean: {stats['mean']:.4f}\")\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # Mean: central tendency\n",
    "    mean = np.sum(x) / n\n",
    "    \n",
    "    # Variance and standard deviation: spread\n",
    "    variance = np.sum((x - mean) ** 2) / n\n",
    "    std = np.sqrt(variance)\n",
    "    \n",
    "    # Skewness: asymmetry (0 = symmetric, >0 = right tail, <0 = left tail)\n",
    "    if std > 0:\n",
    "        skewness = np.sum(((x - mean) / std) ** 3) / n\n",
    "    else:\n",
    "        skewness = 0.0\n",
    "    \n",
    "    # Kurtosis: tail heaviness (0 = normal, >0 = heavy tails, <0 = light tails)\n",
    "    if std > 0:\n",
    "        kurtosis = np.sum(((x - mean) / std) ** 4) / n - 3  # Excess kurtosis\n",
    "    else:\n",
    "        kurtosis = 0.0\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'variance': variance,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurtosis\n",
    "    }\n",
    "\n",
    "# Test with a sample signal\n",
    "np.random.seed(42)\n",
    "test_signal = np.random.randn(100)\n",
    "basic_stats = compute_mean_std_skew_kurtosis(test_signal)\n",
    "print(\"Basic Statistical Moments:\")\n",
    "for name, value in basic_stats.items():\n",
    "    print(f\"  {name:12}: {value:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f60d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_autocorrelation(x: np.ndarray, max_lag: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute autocorrelation function for various lags.\n",
    "    \n",
    "    Autocorrelation reveals periodic patterns and temporal dependencies.\n",
    "    High autocorrelation at lag k means x[t] predicts x[t+k].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        1D time series array\n",
    "    max_lag : int\n",
    "        Maximum lag to compute (default: 10)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with autocorrelation at each lag\n",
    "    \n",
    "    Interpretation\n",
    "    --------------\n",
    "    - Slowly decaying ACF → trend or long memory\n",
    "    - Sharp cutoff → MA process\n",
    "    - Oscillating ACF → seasonal/periodic patterns\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    x_centered = x - np.mean(x)\n",
    "    \n",
    "    # Denominator: total variance (lag 0 autocorrelation = 1)\n",
    "    var = np.sum(x_centered ** 2)\n",
    "    \n",
    "    acf_values = {}\n",
    "    acf_array = np.zeros(max_lag + 1)\n",
    "    \n",
    "    for lag in range(max_lag + 1):\n",
    "        if var > 0:\n",
    "            # Cross-product of original and lagged series\n",
    "            numerator = np.sum(x_centered[:n-lag] * x_centered[lag:]) if lag > 0 else var\n",
    "            acf = numerator / var\n",
    "        else:\n",
    "            acf = 0.0\n",
    "        acf_values[f'acf_lag_{lag}'] = acf\n",
    "        acf_array[lag] = acf\n",
    "    \n",
    "    # Derived features used by TSFresh\n",
    "    acf_values['acf_first_decay'] = np.argmax(acf_array < 0.5) if np.any(acf_array < 0.5) else max_lag\n",
    "    acf_values['acf_sum'] = np.sum(np.abs(acf_array[1:]))  # Exclude lag 0\n",
    "    \n",
    "    return acf_values, acf_array\n",
    "\n",
    "# Test autocorrelation on a periodic signal\n",
    "t = np.linspace(0, 4*np.pi, 100)\n",
    "periodic_signal = np.sin(t) + 0.3 * np.random.randn(100)\n",
    "acf_features, acf_array = compute_autocorrelation(periodic_signal, max_lag=20)\n",
    "print(\"Autocorrelation Features (periodic signal):\")\n",
    "for name, value in list(acf_features.items())[:8]:\n",
    "    print(f\"  {name:15}: {value:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fft_features(x: np.ndarray, n_coeffs: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute FFT-based frequency domain features.\n",
    "    \n",
    "    The Fast Fourier Transform reveals which frequencies dominate the signal.\n",
    "    Useful for detecting periodic patterns, cycles, and oscillations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        1D time series array\n",
    "    n_coeffs : int\n",
    "        Number of FFT coefficients to return\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with FFT magnitude and power features\n",
    "    \n",
    "    Interpretation\n",
    "    --------------\n",
    "    - Peak at low frequency → slow variations, trends\n",
    "    - Peak at specific frequency → periodic pattern\n",
    "    - Flat spectrum → white noise\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # Compute FFT\n",
    "    fft_vals = np.fft.fft(x)\n",
    "    \n",
    "    # Only keep positive frequencies (first half due to symmetry)\n",
    "    fft_positive = fft_vals[:n//2]\n",
    "    \n",
    "    # Magnitude spectrum: |X_k|\n",
    "    magnitude = np.abs(fft_positive)\n",
    "    \n",
    "    # Power spectrum: |X_k|^2\n",
    "    power = magnitude ** 2\n",
    "    \n",
    "    # Normalize power to get spectral density\n",
    "    total_power = np.sum(power)\n",
    "    power_normalized = power / total_power if total_power > 0 else power\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Store first n_coeffs as features\n",
    "    for i in range(min(n_coeffs, len(magnitude))):\n",
    "        features[f'fft_coeff_{i}_real'] = np.real(fft_positive[i])\n",
    "        features[f'fft_coeff_{i}_imag'] = np.imag(fft_positive[i])\n",
    "        features[f'fft_coeff_{i}_magnitude'] = magnitude[i]\n",
    "    \n",
    "    # Aggregate FFT features\n",
    "    features['fft_max_magnitude'] = np.max(magnitude[1:])  # Exclude DC component\n",
    "    features['fft_dominant_freq_index'] = np.argmax(magnitude[1:]) + 1\n",
    "    features['fft_spectral_centroid'] = np.sum(np.arange(len(power)) * power_normalized)\n",
    "    features['fft_spectral_entropy'] = -np.sum(power_normalized * np.log2(power_normalized + 1e-10))\n",
    "    \n",
    "    return features, magnitude, power\n",
    "\n",
    "# Test FFT on a composite signal (two frequencies)\n",
    "t = np.linspace(0, 1, 256)\n",
    "composite_signal = np.sin(2 * np.pi * 10 * t) + 0.5 * np.sin(2 * np.pi * 30 * t)  # 10Hz + 30Hz\n",
    "fft_features, magnitude, power = compute_fft_features(composite_signal, n_coeffs=5)\n",
    "print(\"FFT Features (composite 10Hz + 30Hz signal):\")\n",
    "print(f\"  Dominant frequency index: {fft_features['fft_dominant_freq_index']}\")\n",
    "print(f\"  Max magnitude: {fft_features['fft_max_magnitude']:.4f}\")\n",
    "print(f\"  Spectral centroid: {fft_features['fft_spectral_centroid']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_trend(x: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute linear trend features using least squares regression.\n",
    "    \n",
    "    Captures the overall direction and strength of change over time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        1D time series array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with slope, intercept, and R-squared\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    t = np.arange(n)\n",
    "    \n",
    "    # Means\n",
    "    t_mean = np.mean(t)\n",
    "    x_mean = np.mean(x)\n",
    "    \n",
    "    # Slope (beta): covariance / variance\n",
    "    numerator = np.sum((t - t_mean) * (x - x_mean))\n",
    "    denominator = np.sum((t - t_mean) ** 2)\n",
    "    \n",
    "    slope = numerator / denominator if denominator > 0 else 0.0\n",
    "    intercept = x_mean - slope * t_mean\n",
    "    \n",
    "    # R-squared: coefficient of determination\n",
    "    y_pred = slope * t + intercept\n",
    "    ss_res = np.sum((x - y_pred) ** 2)\n",
    "    ss_tot = np.sum((x - x_mean) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'linear_trend_slope': slope,\n",
    "        'linear_trend_intercept': intercept,\n",
    "        'linear_trend_r_squared': r_squared,\n",
    "        'linear_trend_residual_std': np.std(x - y_pred)\n",
    "    }\n",
    "\n",
    "# Test on trending data\n",
    "t = np.arange(100)\n",
    "trending_signal = 0.5 * t + 10 + 5 * np.random.randn(100)  # Upward trend with noise\n",
    "trend_features = compute_linear_trend(trending_signal)\n",
    "print(\"Linear Trend Features:\")\n",
    "for name, value in trend_features.items():\n",
    "    print(f\"  {name:25}: {value:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9306f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tsfresh_features(x: np.ndarray, max_lag: int = 10, n_fft_coeffs: int = 5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract a comprehensive set of TSFresh-style features from a time series.\n",
    "    \n",
    "    This combines all the individual feature extractors into a single function\n",
    "    that produces a feature vector suitable for machine learning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        1D time series array\n",
    "    max_lag : int\n",
    "        Maximum lag for autocorrelation\n",
    "    n_fft_coeffs : int\n",
    "        Number of FFT coefficients to extract\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with all extracted features (50+ features)\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Basic statistical moments\n",
    "    basic_stats = compute_mean_std_skew_kurtosis(x)\n",
    "    features.update(basic_stats)\n",
    "    \n",
    "    # 2. Additional distributional features\n",
    "    features['min'] = np.min(x)\n",
    "    features['max'] = np.max(x)\n",
    "    features['range'] = np.max(x) - np.min(x)\n",
    "    features['median'] = np.median(x)\n",
    "    features['iqr'] = np.percentile(x, 75) - np.percentile(x, 25)\n",
    "    features['q25'] = np.percentile(x, 25)\n",
    "    features['q75'] = np.percentile(x, 75)\n",
    "    features['abs_energy'] = np.sum(x ** 2)\n",
    "    features['root_mean_square'] = np.sqrt(np.mean(x ** 2))\n",
    "    \n",
    "    # 3. Autocorrelation features\n",
    "    acf_features, _ = compute_autocorrelation(x, max_lag)\n",
    "    features.update(acf_features)\n",
    "    \n",
    "    # 4. FFT frequency features\n",
    "    fft_features, _, _ = compute_fft_features(x, n_fft_coeffs)\n",
    "    features.update(fft_features)\n",
    "    \n",
    "    # 5. Linear trend features\n",
    "    trend_features = compute_linear_trend(x)\n",
    "    features.update(trend_features)\n",
    "    \n",
    "    # 6. Count-based features\n",
    "    features['count_above_mean'] = np.sum(x > features['mean'])\n",
    "    features['count_below_mean'] = np.sum(x < features['mean'])\n",
    "    features['pct_above_mean'] = features['count_above_mean'] / len(x)\n",
    "    \n",
    "    # 7. Change-based features\n",
    "    diff = np.diff(x)\n",
    "    features['mean_abs_change'] = np.mean(np.abs(diff))\n",
    "    features['mean_change'] = np.mean(diff)\n",
    "    features['max_abs_change'] = np.max(np.abs(diff))\n",
    "    features['count_sign_changes'] = np.sum(np.diff(np.sign(x)) != 0)\n",
    "    \n",
    "    # 8. Zero-crossing rate\n",
    "    features['zero_crossing_rate'] = np.sum(np.diff(np.sign(x - features['mean'])) != 0) / len(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test the full extractor\n",
    "np.random.seed(42)\n",
    "sample_ts = np.cumsum(np.random.randn(200))  # Random walk\n",
    "all_features = extract_tsfresh_features(sample_ts)\n",
    "print(f\"Total features extracted: {len(all_features)}\")\n",
    "print(\"\\nSample of extracted features:\")\n",
    "for i, (name, value) in enumerate(all_features.items()):\n",
    "    if i < 15:\n",
    "        print(f\"  {name:30}: {value:12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc26426",
   "metadata": {},
   "source": [
    "## 3. Interactive Visualizations\n",
    "\n",
    "Visualizing features helps build intuition about what TSFresh captures from time series data. Below we create interactive Plotly visualizations showing:\n",
    "\n",
    "1. **Time Series with Feature Annotations** - See where features come from\n",
    "2. **Autocorrelation Function (ACF)** - Temporal dependencies\n",
    "3. **FFT Power Spectrum** - Frequency content\n",
    "4. **Feature Correlation Heatmap** - Redundancy between features\n",
    "5. **Feature Importance** - Which features matter most for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Generate a sample time series with clear patterns for visualization\n",
    "np.random.seed(123)\n",
    "t = np.linspace(0, 10, 500)\n",
    "sample_ts = (\n",
    "    2 * np.sin(2 * np.pi * 0.5 * t) +      # Low frequency component\n",
    "    0.5 * np.sin(2 * np.pi * 2 * t) +      # Higher frequency component\n",
    "    0.1 * t +                               # Linear trend\n",
    "    0.3 * np.random.randn(500)              # Noise\n",
    ")\n",
    "\n",
    "# Extract features for this sample\n",
    "features = extract_tsfresh_features(sample_ts)\n",
    "\n",
    "# === Visualization 1: Time Series with Feature Annotations ===\n",
    "fig1 = go.Figure()\n",
    "\n",
    "# Main time series\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=t, y=sample_ts, mode='lines', name='Time Series',\n",
    "    line=dict(color='#1f77b4', width=1.5)\n",
    "))\n",
    "\n",
    "# Annotate mean\n",
    "mean_val = features['mean']\n",
    "fig1.add_hline(y=mean_val, line_dash=\"dash\", line_color=\"red\",\n",
    "               annotation_text=f\"Mean = {mean_val:.2f}\")\n",
    "\n",
    "# Annotate standard deviation band\n",
    "std_val = features['std']\n",
    "fig1.add_hrect(y0=mean_val - std_val, y1=mean_val + std_val,\n",
    "               fillcolor=\"rgba(255,0,0,0.1)\", line_width=0,\n",
    "               annotation_text=f\"±1σ = {std_val:.2f}\")\n",
    "\n",
    "# Annotate min/max\n",
    "fig1.add_annotation(x=t[np.argmax(sample_ts)], y=features['max'],\n",
    "                    text=f\"Max = {features['max']:.2f}\", showarrow=True)\n",
    "fig1.add_annotation(x=t[np.argmin(sample_ts)], y=features['min'],\n",
    "                    text=f\"Min = {features['min']:.2f}\", showarrow=True)\n",
    "\n",
    "# Linear trend line\n",
    "slope = features['linear_trend_slope']\n",
    "intercept = features['linear_trend_intercept']\n",
    "trend_line = slope * np.arange(len(sample_ts)) + intercept\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=t, y=trend_line, mode='lines', name=f'Trend (β={slope:.4f})',\n",
    "    line=dict(color='green', width=2, dash='dot')\n",
    "))\n",
    "\n",
    "fig1.update_layout(\n",
    "    title=\"<b>Time Series with Extracted Feature Annotations</b><br><sub>Understanding where TSFresh features come from</sub>\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Value\",\n",
    "    template=\"plotly_white\",\n",
    "    height=450,\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99)\n",
    ")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267546d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 2: Autocorrelation Function (ACF) Plot ===\n",
    "_, acf_values = compute_autocorrelation(sample_ts, max_lag=50)\n",
    "\n",
    "fig2 = go.Figure()\n",
    "\n",
    "# Bar plot for ACF\n",
    "fig2.add_trace(go.Bar(\n",
    "    x=list(range(len(acf_values))),\n",
    "    y=acf_values,\n",
    "    marker_color=['#2ecc71' if v > 0 else '#e74c3c' for v in acf_values],\n",
    "    name='ACF'\n",
    "))\n",
    "\n",
    "# Significance bounds (approximate 95% CI for white noise)\n",
    "n = len(sample_ts)\n",
    "sig_bound = 1.96 / np.sqrt(n)\n",
    "fig2.add_hline(y=sig_bound, line_dash=\"dash\", line_color=\"gray\",\n",
    "               annotation_text=\"95% CI\")\n",
    "fig2.add_hline(y=-sig_bound, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig2.add_hline(y=0, line_color=\"black\", line_width=0.5)\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=\"<b>Autocorrelation Function (ACF)</b><br><sub>Values outside gray bands indicate significant temporal dependence</sub>\",\n",
    "    xaxis_title=\"Lag (k)\",\n",
    "    yaxis_title=\"Autocorrelation r(k)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add interpretation annotation\n",
    "fig2.add_annotation(\n",
    "    x=25, y=0.8,\n",
    "    text=\"<b>Interpretation:</b><br>• Oscillating pattern → periodic signal<br>• Slow decay → trend/persistence<br>• Sharp cutoff → MA process\",\n",
    "    showarrow=False,\n",
    "    bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "    bordercolor=\"gray\",\n",
    "    borderwidth=1,\n",
    "    align=\"left\"\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c36fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 3: FFT Power Spectrum ===\n",
    "_, magnitude, power = compute_fft_features(sample_ts, n_coeffs=50)\n",
    "\n",
    "# Compute frequency axis (assuming unit sampling rate, adjust as needed)\n",
    "sampling_rate = len(sample_ts) / (t[-1] - t[0])\n",
    "freqs = np.fft.fftfreq(len(sample_ts), 1/sampling_rate)[:len(magnitude)]\n",
    "\n",
    "fig3 = go.Figure()\n",
    "\n",
    "# Power spectrum\n",
    "fig3.add_trace(go.Scatter(\n",
    "    x=freqs[1:100],  # Skip DC component, show first 100 frequencies\n",
    "    y=power[1:100],\n",
    "    mode='lines',\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(31, 119, 180, 0.3)',\n",
    "    line=dict(color='#1f77b4', width=2),\n",
    "    name='Power Spectrum'\n",
    "))\n",
    "\n",
    "# Mark dominant frequencies\n",
    "peak_indices = np.argsort(power[1:100])[-3:] + 1  # Top 3 peaks\n",
    "for idx in peak_indices:\n",
    "    if power[idx] > 0.1 * np.max(power[1:100]):\n",
    "        fig3.add_annotation(\n",
    "            x=freqs[idx], y=power[idx],\n",
    "            text=f\"f = {freqs[idx]:.2f} Hz\",\n",
    "            showarrow=True,\n",
    "            arrowhead=2\n",
    "        )\n",
    "\n",
    "fig3.update_layout(\n",
    "    title=\"<b>FFT Power Spectrum</b><br><sub>Peaks reveal dominant frequencies in the signal</sub>\",\n",
    "    xaxis_title=\"Frequency (Hz)\",\n",
    "    yaxis_title=\"Power |X(f)|²\",\n",
    "    template=\"plotly_white\",\n",
    "    height=400,\n",
    "    xaxis=dict(range=[0, freqs[99]])\n",
    ")\n",
    "\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 4: Feature Correlation Heatmap ===\n",
    "# Generate multiple time series to compute feature correlations\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "feature_matrix = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Generate diverse time series\n",
    "    ts = (\n",
    "        np.random.uniform(0.5, 3) * np.sin(2 * np.pi * np.random.uniform(0.1, 2) * t) +\n",
    "        np.random.uniform(-0.5, 0.5) * t +\n",
    "        np.random.uniform(0.1, 1) * np.random.randn(len(t))\n",
    "    )\n",
    "    feats = extract_tsfresh_features(ts, max_lag=5, n_fft_coeffs=3)\n",
    "    feature_matrix.append(feats)\n",
    "\n",
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "df_features = pd.DataFrame(feature_matrix)\n",
    "\n",
    "# Select a subset of interpretable features for the heatmap\n",
    "selected_features = ['mean', 'std', 'skewness', 'kurtosis', 'min', 'max', 'range',\n",
    "                     'linear_trend_slope', 'linear_trend_r_squared', 'acf_lag_1', \n",
    "                     'acf_lag_2', 'fft_max_magnitude', 'fft_spectral_entropy',\n",
    "                     'mean_abs_change', 'zero_crossing_rate']\n",
    "\n",
    "df_subset = df_features[selected_features]\n",
    "corr_matrix = df_subset.corr()\n",
    "\n",
    "fig4 = go.Figure(data=go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=corr_matrix.columns,\n",
    "    y=corr_matrix.index,\n",
    "    colorscale='RdBu_r',\n",
    "    zmid=0,\n",
    "    text=np.round(corr_matrix.values, 2),\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 9},\n",
    "    hoverongaps=False\n",
    "))\n",
    "\n",
    "fig4.update_layout(\n",
    "    title=\"<b>Feature Correlation Heatmap</b><br><sub>High correlation (|r| > 0.8) indicates redundant features</sub>\",\n",
    "    template=\"plotly_white\",\n",
    "    height=600,\n",
    "    width=800,\n",
    "    xaxis=dict(tickangle=45)\n",
    ")\n",
    "\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualization 5: Feature Importance Bar Chart ===\n",
    "# Simulate feature importance (in practice, use model coefficients or permutation importance)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic importance scores based on typical TSFresh relevance\n",
    "feature_importance = {\n",
    "    'acf_lag_1': 0.85,\n",
    "    'linear_trend_slope': 0.78,\n",
    "    'std': 0.72,\n",
    "    'fft_max_magnitude': 0.68,\n",
    "    'mean_abs_change': 0.62,\n",
    "    'skewness': 0.55,\n",
    "    'zero_crossing_rate': 0.48,\n",
    "    'kurtosis': 0.42,\n",
    "    'fft_spectral_entropy': 0.38,\n",
    "    'range': 0.35,\n",
    "    'acf_lag_2': 0.32,\n",
    "    'mean': 0.28,\n",
    "    'linear_trend_r_squared': 0.25,\n",
    "    'min': 0.18,\n",
    "    'max': 0.15\n",
    "}\n",
    "\n",
    "# Sort by importance\n",
    "sorted_features = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "fig5 = go.Figure()\n",
    "\n",
    "# Horizontal bar chart\n",
    "fig5.add_trace(go.Bar(\n",
    "    y=list(sorted_features.keys()),\n",
    "    x=list(sorted_features.values()),\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=list(sorted_features.values()),\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title='Importance')\n",
    "    ),\n",
    "    text=[f'{v:.2f}' for v in sorted_features.values()],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig5.update_layout(\n",
    "    title=\"<b>Feature Importance Ranking</b><br><sub>Based on predictive power for time series classification</sub>\",\n",
    "    xaxis_title=\"Importance Score\",\n",
    "    yaxis_title=\"Feature\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    "    yaxis=dict(autorange='reversed'),  # Most important at top\n",
    "    xaxis=dict(range=[0, 1.1])\n",
    ")\n",
    "\n",
    "# Add interpretation annotations\n",
    "fig5.add_annotation(\n",
    "    x=0.9, y=10,\n",
    "    text=\"<b>Key Insight:</b><br>Temporal features (ACF, trend)<br>often outperform static<br>statistics for classification\",\n",
    "    showarrow=False,\n",
    "    bgcolor=\"rgba(255,255,255,0.9)\",\n",
    "    bordercolor=\"gray\",\n",
    "    borderwidth=1,\n",
    "    align=\"left\"\n",
    ")\n",
    "\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd333c",
   "metadata": {},
   "source": [
    "## 4. Using TSFresh with sktime\n",
    "\n",
    "Now that we understand the mathematical foundations and have built features from scratch, let's see how to use the production-ready `TSFreshFeatureExtractor` from sktime. This integrates seamlessly with sklearn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sktime.datasets import load_basic_motions, load_unit_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependency\n",
    "\n",
    "TSFresh is an optional dependency for sktime. Install it with:\n",
    "\n",
    "```bash\n",
    "pip install tsfresh\n",
    "```\n",
    "\n",
    "**Feature Set Options:**\n",
    "- `\"minimal\"`: ~10 features, fastest computation\n",
    "- `\"efficient\"`: ~100 features, good balance of speed and coverage\n",
    "- `\"comprehensive\"`: 800+ features, most thorough but slowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_basic_motions(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_basic_motions(split=\"test\", return_X_y=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipe = make_pipeline(TSFreshFeatureExtractor(default_fc_parameters=\"efficient\"), RidgeClassifier())\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e615052",
   "metadata": {},
   "source": [
    "## 5. Summary & Best Practices\n",
    "\n",
    "### Feature Categories in TSFresh\n",
    "\n",
    "| Category | Examples | What They Capture |\n",
    "|----------|----------|-------------------|\n",
    "| **Statistical** | mean, std, skewness, kurtosis | Distribution shape |\n",
    "| **Temporal** | autocorrelation, partial autocorrelation | Sequential dependencies |\n",
    "| **Frequency** | FFT coefficients, spectral entropy | Periodic patterns |\n",
    "| **Trend** | linear trend slope, curvature | Long-term direction |\n",
    "| **Complexity** | approximate entropy, sample entropy | Irregularity/predictability |\n",
    "| **Count-based** | peaks, zero crossings | Discrete events |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with `\"efficient\"` parameter set** - Good balance of coverage and speed\n",
    "2. **Apply feature selection** - Many features are redundant; use correlation filtering\n",
    "3. **Scale features** - Use StandardScaler before ML models\n",
    "4. **Handle NaN features** - Some features may be undefined for certain time series\n",
    "5. **Consider computational cost** - Full extraction can be slow for large datasets\n",
    "\n",
    "### When to Use TSFresh vs Alternatives\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Need interpretable features | ✅ TSFresh |\n",
    "| Very large datasets | Consider Catch22 (faster) |\n",
    "| Deep learning approach | Skip feature extraction, use raw data |\n",
    "| Real-time inference | Use minimal feature set or pre-compute |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
