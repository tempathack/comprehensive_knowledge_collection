{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Transformer Forecasting for Time Series\nTransformers replace recurrence with self-attention, letting the model compare every time step in a window at once. In forecasting, we typically feed a fixed lookback window and predict a multi-step horizon in one shot.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Takeaways\n- Transformers scale to long lookbacks because attention connects any pair of time steps directly.\n- Positional encodings inject order information that attention alone lacks.\n- Multi-horizon forecasting can be framed as a supervised window-to-horizon regression problem.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Problem setup: windowed forecasting\nGiven a series $y_1, \\dots, y_T$, we build input windows of length $L$ and predict a horizon of length $H$:\n\n$$\nX_t = [y_{t-L+1}, \\dots, y_t], \\quad \\hat{y}_{t+1:t+H} = f(X_t).\n$$\n\nThis notebook trains a Transformer encoder to map $X_t$ to a full horizon in a single forward pass.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Self-attention in one line\nScaled dot-product attention computes a weighted mix of value vectors $V$ using query-key similarity:\n\n$$\n\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\frac{QK^\top}{\\sqrt{d_k}}\right)V.\n$$\n\nMulti-head attention repeats this with several projections, letting the model attend to different temporal patterns simultaneously.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Positional encoding\nBecause attention is permutation-invariant, we add position signals. A common sinusoidal encoding is:\n\n$$\n\\mathrm{PE}(pos, 2i) = \\sin\\left(\frac{pos}{10000^{2i/d}}\right), \\quad\n\\mathrm{PE}(pos, 2i+1) = \\cos\\left(\frac{pos}{10000^{2i/d}}\right).\n$$\n\nThese features are added to input embeddings so the model can learn order-aware patterns.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## When to use\n- Long lookbacks or multiple seasonalities where attention can connect distant time steps.\n- Multivariate or multi-series settings where shared patterns matter.\n- You can afford more data and compute than classical baselines.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import os\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\n\npio.templates.default = \"plotly_white\"\npio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n\nSEED = 7\nrng = np.random.default_rng(SEED)\ntorch.manual_seed(SEED)\n\nFAST_RUN = True\nLOOKBACK = 96\nHORIZON = 24\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Synthetic series: trend + daily + weekly seasonality",
    "n = 720",
    "",
    "t = np.arange(n, dtype=float)",
    "trend = 0.02 * t",
    "daily = 1.5 * np.sin(2 * np.pi * t / 24)",
    "weekly = 0.6 * np.sin(2 * np.pi * t / 168)",
    "noise = rng.normal(0, 0.3, n)",
    "",
    "series = trend + daily + weekly + noise",
    "",
    "fig = go.Figure()",
    "fig.add_trace(go.Scatter(x=t, y=series, mode=\"lines\", name=\"series\"))",
    "fig.update_layout(title=\"Synthetic time series\", xaxis_title=\"t\", yaxis_title=\"value\")",
    "fig",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Windowing and scaling\nWe turn the series into a supervised dataset of sliding windows and normalize with the training statistics.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def make_windows(values: np.ndarray, lookback: int, horizon: int):\n    X, y = [], []\n    for i in range(lookback, len(values) - horizon + 1):\n        X.append(values[i - lookback : i])\n        y.append(values[i : i + horizon])\n    return np.array(X), np.array(y)\n\nX, y = make_windows(series, LOOKBACK, HORIZON)\n\nn_total = len(X)\ntrain_end = int(0.7 * n_total)\nval_end = int(0.85 * n_total)\n\nX_train, y_train = X[:train_end], y[:train_end]\nX_val, y_val = X[train_end:val_end], y[train_end:val_end]\nX_test, y_test = X[val_end:], y[val_end:]\n\nmu = X_train.mean()\nsigma = X_train.std()\n\nX_train_s = (X_train - mu) / sigma\ny_train_s = (y_train - mu) / sigma\nX_val_s = (X_val - mu) / sigma\ny_val_s = (y_val - mu) / sigma\nX_test_s = (X_test - mu) / sigma\ny_test_s = (y_test - mu) / sigma\n\nprint(\"windows:\", X.shape, \"train/val/test:\", X_train.shape, X_val.shape, X_test.shape)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Seasonal naive baseline using last daily season\nSEASON = 24\n\ndef seasonal_naive(window: np.ndarray, horizon: int, season: int = 24) -> np.ndarray:\n    if season <= 0:\n        return np.repeat(window[-1], horizon)\n    reps = int(np.ceil(horizon / season))\n    pattern = np.tile(window[-season:], reps)\n    return pattern[:horizon]\n\nbaseline_preds = np.vstack([\n    seasonal_naive(x, HORIZON, SEASON) for x in X_test\n])\n\nmae_baseline = np.mean(np.abs(y_test - baseline_preds))\nrmse_baseline = np.sqrt(np.mean((y_test - baseline_preds) ** 2))\n\nprint(f\"baseline MAE: {mae_baseline:.3f} | baseline RMSE: {rmse_baseline:.3f}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Transformer forecaster (PyTorch)\nWe use an encoder-only Transformer. The last token representation is mapped to the forecast horizon with a linear head.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class WindowDataset(Dataset):\n    def __init__(self, X: np.ndarray, y: np.ndarray):\n        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return x + self.pe[:, : x.size(1)]\n\n\nclass TransformerForecaster(nn.Module):\n    def __init__(\n        self,\n        input_size: int,\n        horizon: int,\n        d_model: int = 64,\n        nhead: int = 4,\n        num_layers: int = 2,\n        dim_ff: int = 128,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.input_proj = nn.Linear(input_size, d_model)\n        self.positional = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_ff,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.head = nn.Linear(d_model, horizon)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        z = self.input_proj(x)\n        z = self.positional(z)\n        z = self.encoder(z)\n        last = z[:, -1, :]\n        return self.head(last)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize a small positional encoding matrix",
    "pe = PositionalEncoding(32, max_len=48).pe.squeeze(0).numpy()",
    "",
    "fig = px.imshow(",
    "    pe,",
    "    aspect=\"auto\",",
    "    color_continuous_scale=\"RdBu\",",
    "    title=\"Sinusoidal positional encoding (48 positions x 32 dims)\",",
    ")",
    "fig.update_layout(xaxis_title=\"embedding dim\", yaxis_title=\"position\")",
    "fig",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntrain_ds = WindowDataset(X_train_s, y_train_s)\nval_ds = WindowDataset(X_val_s, y_val_s)\n\ntest_ds = WindowDataset(X_test_s, y_test_s)\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\ntorch.manual_seed(SEED)\n\nmodel = TransformerForecaster(input_size=1, horizon=HORIZON).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\nEPOCHS = 15 if FAST_RUN else 60\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def run_epoch(model, loader, optimizer=None, train: bool = True):\n    model.train(train)\n    total_loss = 0.0\n    for X_batch, y_batch in loader:\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        if train:\n            optimizer.zero_grad()\n        with torch.set_grad_enabled(train):\n            preds = model(X_batch)\n            loss = criterion(preds, y_batch)\n        if train:\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        total_loss += loss.item() * X_batch.size(0)\n    return total_loss / len(loader.dataset)\n\nhistory = {\"train\": [], \"val\": []}\nfor epoch in range(1, EPOCHS + 1):\n    train_loss = run_epoch(model, train_loader, optimizer=optimizer, train=True)\n    val_loss = run_epoch(model, val_loader, train=False)\n    history[\"train\"].append(train_loss)\n    history[\"val\"].append(val_loss)\n    if epoch == 1 or epoch % 5 == 0 or epoch == EPOCHS:\n        print(f\"epoch {epoch:02d} | train {train_loss:.4f} | val {val_loss:.4f}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig = go.Figure()",
    "fig.add_trace(go.Scatter(y=history[\"train\"], mode=\"lines+markers\", name=\"train\"))",
    "fig.add_trace(go.Scatter(y=history[\"val\"], mode=\"lines+markers\", name=\"val\"))",
    "fig.update_layout(title=\"Training curves\", xaxis_title=\"epoch\", yaxis_title=\"MSE\")",
    "fig",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model.eval()\nwith torch.no_grad():\n    preds_scaled = []\n    for X_batch, _ in DataLoader(test_ds, batch_size=128):\n        X_batch = X_batch.to(device)\n        preds_scaled.append(model(X_batch).cpu().numpy())\n\npreds_scaled = np.vstack(preds_scaled)\npreds = preds_scaled * sigma + mu\n\nmae = np.mean(np.abs(y_test - preds))\nrmse = np.sqrt(np.mean((y_test - preds) ** 2))\n\nprint(f\"transformer MAE: {mae:.3f} | transformer RMSE: {rmse:.3f}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot a single forecast example",
    "idx = 0",
    "history_vals = X_test[idx]",
    "truth = y_test[idx]",
    "forecast = preds[idx]",
    "",
    "hist_x = np.arange(len(history_vals))",
    "future_x = np.arange(len(history_vals), len(history_vals) + HORIZON)",
    "",
    "fig = go.Figure()",
    "fig.add_trace(go.Scatter(x=hist_x, y=history_vals, mode=\"lines\", name=\"history\"))",
    "fig.add_trace(go.Scatter(x=future_x, y=truth, mode=\"lines\", name=\"actual\"))",
    "fig.add_trace(go.Scatter(x=future_x, y=forecast, mode=\"lines\", name=\"forecast\"))",
    "fig.update_layout(title=\"Transformer forecast\", xaxis_title=\"t\", yaxis_title=\"value\")",
    "fig",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Practical tips\n- Scale inputs and targets (z-score or min-max) to stabilize training.\n- Use larger lookbacks for long-seasonality data, but keep batch size manageable.\n- Regularize with dropout and early stopping to avoid overfitting.\n- Compare against seasonal naive or ETS baselines before deploying.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercises\n1. Replace the single-token readout with mean pooling and compare accuracy.\n2. Add exogenous features (calendar flags, promotions) as extra input channels.\n3. Try a longer horizon and evaluate how error grows with forecast step.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further reading\n- Vaswani et al., *Attention Is All You Need* (2017).\n- Lim et al., *Temporal Fusion Transformers* (2019).\n- Wu et al., *Autoformer: Decomposition Transformers with Auto-Correlation* (2021).\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}