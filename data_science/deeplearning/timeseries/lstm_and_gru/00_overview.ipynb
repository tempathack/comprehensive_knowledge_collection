{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62421b06",
   "metadata": {},
   "source": [
    "# LSTM and GRU for Time Series Forecasting (from scratch + PyTorch)\n",
    "\n",
    "Recurrent neural networks model sequences by carrying state forward in time. **LSTMs** and **GRUs** add *gates* that make it much easier to learn long-range patterns (seasonality, delayed effects, regime changes) without gradients vanishing/exploding.\n",
    "\n",
    "In this notebook you will:\n",
    "- Build intuition for **memory + gates** in LSTM/GRU\n",
    "- Implement a **single-layer LSTM** and **single-layer GRU** in *pure NumPy* (forward + backprop through time)\n",
    "- Train on a small forecasting task and visualize predictions\n",
    "- Reproduce the same setup using **PyTorch** (`nn.LSTM`, `nn.GRU`)\n",
    "- Visualize learned dynamics with **Plotly**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87300f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "import plotly\n",
    "\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6b651",
   "metadata": {},
   "source": [
    "## 0) Problem setup: one-step-ahead forecasting\n",
    "\n",
    "We’ll learn a mapping:\n",
    "\n",
    "- **Input**: a window $(x_{t-L+1}, \\dots, x_t)$ of length $L$\n",
    "- **Target**: the next value $x_{t+1}$\n",
    "\n",
    "This is the simplest useful forecasting setup and keeps the math in the from-scratch section readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72193f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_toy_series(n: int = 800, noise: float = 0.15, seed: int = 42):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    t = np.arange(n, dtype=float)\n",
    "\n",
    "    # A mix of seasonality + trend + noise (toy, but non-trivial)\n",
    "    y = (\n",
    "        1.2 * np.sin(2 * np.pi * t / 50)\n",
    "        + 0.6 * np.sin(2 * np.pi * t / 17 + 0.8)\n",
    "        + 0.0025 * t\n",
    "    )\n",
    "    y += noise * rng_local.normal(size=n)\n",
    "    return t, y\n",
    "\n",
    "\n",
    "def make_windows(series_1d: np.ndarray, seq_len: int):\n",
    "    X = []\n",
    "    y = []\n",
    "    target_idx = []\n",
    "    for start in range(0, len(series_1d) - seq_len):\n",
    "        end = start + seq_len\n",
    "        X.append(series_1d[start:end])\n",
    "        y.append(series_1d[end])\n",
    "        target_idx.append(end)\n",
    "\n",
    "    X = np.asarray(X, dtype=float)[:, :, None]  # (N, T, 1)\n",
    "    y = np.asarray(y, dtype=float)[:, None]  # (N, 1)\n",
    "    target_idx = np.asarray(target_idx, dtype=int)\n",
    "    return X, y, target_idx\n",
    "\n",
    "\n",
    "t, y = make_toy_series(n=900, noise=0.18, seed=SEED)\n",
    "\n",
    "split_t = int(0.7 * len(y))\n",
    "\n",
    "train_mean = float(y[:split_t].mean())\n",
    "train_std = float(y[:split_t].std() + 1e-8)\n",
    "\n",
    "y_scaled = (y - train_mean) / train_std\n",
    "\n",
    "SEQ_LEN = 60\n",
    "X_all, y_all, target_idx = make_windows(y_scaled, seq_len=SEQ_LEN)\n",
    "\n",
    "train_mask = target_idx < split_t\n",
    "test_mask = ~train_mask\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
    "\n",
    "t_train = t[target_idx[train_mask]]\n",
    "t_test = t[target_idx[test_mask]]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=y, mode=\"lines\", name=\"series\"))\n",
    "fig.add_vline(x=t[split_t], line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(\n",
    "    title=\"Toy time series (train/test split)\",\n",
    "    xaxis_title=\"time index\",\n",
    "    yaxis_title=\"value\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efdbfb",
   "metadata": {},
   "source": [
    "## 1) Why gated RNNs (LSTM/GRU) help\n",
    "\n",
    "A vanilla RNN is:\n",
    "\n",
    "$$\n",
    "h_t = \tanh(W_x x_t + W_h h_{t-1} + b),\n",
    "$$\n",
    "\n",
    "and gradients backpropagate through many repeated Jacobian multiplications. Over long sequences this often leads to:\n",
    "\n",
    "- **vanishing gradients**: early time steps get almost no learning signal\n",
    "- **exploding gradients**: unstable updates without clipping\n",
    "\n",
    "**Gates** make it easier for the model to *choose* what to keep, overwrite, or expose at each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eed6ef",
   "metadata": {},
   "source": [
    "## 2) LSTM: explicit memory cell\n",
    "\n",
    "An LSTM keeps two states:\n",
    "\n",
    "- **cell state** $c_t$ (long-term memory)\n",
    "- **hidden state** $h_t$ (what the model exposes to the next layer / output)\n",
    "\n",
    "With $z_t = [x_t, h_{t-1}]$ (concatenation), the core equations are:\n",
    "\n",
    "$$\n",
    "\begin{aligned}\n",
    " i_t &= \\sigma(z_t W_i + b_i) && \text{(input gate)} \\\n",
    " f_t &= \\sigma(z_t W_f + b_f) && \text{(forget gate)} \\\n",
    " o_t &= \\sigma(z_t W_o + b_o) && \text{(output gate)} \\\n",
    " g_t &= \tanh(z_t W_g + b_g) && \text{(candidate)} \\\n",
    " c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\n",
    " h_t &= o_t \\odot \tanh(c_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Intuition:\n",
    "- $f_t$ chooses what memory to **keep** from $c_{t-1}$\n",
    "- $i_t$ chooses how much new information to **write**\n",
    "- $o_t$ chooses how much memory to **expose** to $h_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4ac7c",
   "metadata": {},
   "source": [
    "## 3) GRU: fewer gates, no separate cell state\n",
    "\n",
    "A GRU merges the cell and hidden state and uses two gates:\n",
    "\n",
    "$$\n",
    "\begin{aligned}\n",
    " z_t &= \\sigma([x_t, h_{t-1}] W_z + b_z) && \text{(update)} \\\n",
    " r_t &= \\sigma([x_t, h_{t-1}] W_r + b_r) && \text{(reset)} \\\n",
    " \tilde{h}_t &= \tanh([x_t, r_t \\odot h_{t-1}] W_h + b_h) \\\n",
    " h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \tilde{h}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Intuition:\n",
    "- $z_t$ interpolates between **keeping** the old state vs **writing** the candidate\n",
    "- $r_t$ decides how much of the old state to use when forming the candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c622d",
   "metadata": {},
   "source": [
    "## 4) NumPy from scratch (forward + BPTT)\n",
    "\n",
    "We’ll implement two tiny forecasters:\n",
    "\n",
    "- `NumpyLSTMForecaster`\n",
    "- `NumpyGRUForecaster`\n",
    "\n",
    "Both:\n",
    "- take `X` shaped `(batch, seq_len, 1)`\n",
    "- run a single recurrent layer\n",
    "- predict the next value via a linear head on the last hidden state\n",
    "\n",
    "We’ll also add:\n",
    "- **Adam** optimizer (NumPy)\n",
    "- **gradient clipping** (important for RNNs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4015ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.clip(x, -60, 60)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def mse_loss(y_hat: np.ndarray, y: np.ndarray):\n",
    "    diff = y_hat - y\n",
    "    loss = float(np.mean(diff**2))\n",
    "    dY = (2.0 / diff.size) * diff\n",
    "    return loss, dY\n",
    "\n",
    "\n",
    "def clip_grads_(grads: dict[str, np.ndarray], max_norm: float = 1.0) -> float:\n",
    "    total_sq = 0.0\n",
    "    for g in grads.values():\n",
    "        total_sq += float(np.sum(g * g))\n",
    "    total_norm = float(np.sqrt(total_sq))\n",
    "\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / (total_norm + 1e-12)\n",
    "        for k in grads:\n",
    "            grads[k] *= scale\n",
    "\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def init_weight(rng: np.random.Generator, fan_in: int, fan_out: int):\n",
    "    # Xavier/Glorot uniform\n",
    "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return rng.uniform(-limit, limit, size=(fan_in, fan_out)).astype(float)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: dict[str, np.ndarray],\n",
    "        lr: float = 1e-3,\n",
    "        betas: tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.t = 0\n",
    "        self.m = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "\n",
    "    def step(self, params: dict[str, np.ndarray], grads: dict[str, np.ndarray]):\n",
    "        self.t += 1\n",
    "        lr_t = self.lr * (np.sqrt(1.0 - self.beta2**self.t) / (1.0 - self.beta1**self.t))\n",
    "\n",
    "        for k in params:\n",
    "            g = grads[k]\n",
    "            self.m[k] = self.beta1 * self.m[k] + (1.0 - self.beta1) * g\n",
    "            self.v[k] = self.beta2 * self.v[k] + (1.0 - self.beta2) * (g * g)\n",
    "            params[k] -= lr_t * self.m[k] / (np.sqrt(self.v[k]) + self.eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e7067",
   "metadata": {},
   "source": [
    "### 4.1) NumPy LSTM forecaster\n",
    "\n",
    "This is a single-layer LSTM with a linear prediction head.\n",
    "\n",
    "Implementation notes:\n",
    "- We store per-timestep intermediates (gates, states, concatenated input) in a cache.\n",
    "- Backprop is standard **BPTT** over that cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cf714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLSTMForecaster:\n",
    "    def __init__(self, input_size: int, hidden_size: int, seed: int = 42):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        rng_local = np.random.default_rng(seed)\n",
    "        fan_in = input_size + hidden_size\n",
    "\n",
    "        self.params = {\n",
    "            'W_i': init_weight(rng_local, fan_in, hidden_size),\n",
    "            'b_i': np.zeros((hidden_size,), dtype=float),\n",
    "            'W_f': init_weight(rng_local, fan_in, hidden_size),\n",
    "            'b_f': np.zeros((hidden_size,), dtype=float),\n",
    "            'W_o': init_weight(rng_local, fan_in, hidden_size),\n",
    "            'b_o': np.zeros((hidden_size,), dtype=float),\n",
    "            'W_g': init_weight(rng_local, fan_in, hidden_size),\n",
    "            'b_g': np.zeros((hidden_size,), dtype=float),\n",
    "            'W_y': init_weight(rng_local, hidden_size, 1),\n",
    "            'b_y': np.zeros((1,), dtype=float),\n",
    "        }\n",
    "\n",
    "        self.cache = None\n",
    "        self.last_h = None\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        # X: (B, T, D)\n",
    "        B, T, D = X.shape\n",
    "        H = self.hidden_size\n",
    "\n",
    "        h = np.zeros((B, H), dtype=float)\n",
    "        c = np.zeros((B, H), dtype=float)\n",
    "\n",
    "        caches = []\n",
    "        p = self.params\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t, :]\n",
    "            z = np.concatenate([x_t, h], axis=1)  # (B, D+H)\n",
    "\n",
    "            i = sigmoid(z @ p['W_i'] + p['b_i'])\n",
    "            f = sigmoid(z @ p['W_f'] + p['b_f'])\n",
    "            o = sigmoid(z @ p['W_o'] + p['b_o'])\n",
    "            g = np.tanh(z @ p['W_g'] + p['b_g'])\n",
    "\n",
    "            c_next = f * c + i * g\n",
    "            tanh_c = np.tanh(c_next)\n",
    "            h_next = o * tanh_c\n",
    "\n",
    "            caches.append(\n",
    "                {\n",
    "                    'z': z,\n",
    "                    'i': i,\n",
    "                    'f': f,\n",
    "                    'o': o,\n",
    "                    'g': g,\n",
    "                    'c_prev': c,\n",
    "                    'c': c_next,\n",
    "                    'tanh_c': tanh_c,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            h, c = h_next, c_next\n",
    "\n",
    "        y_hat = h @ p['W_y'] + p['b_y']\n",
    "        self.cache = caches\n",
    "        self.last_h = h\n",
    "        return y_hat\n",
    "\n",
    "    def backward(self, dY: np.ndarray) -> dict[str, np.ndarray]:\n",
    "        assert self.cache is not None and self.last_h is not None\n",
    "\n",
    "        p = self.params\n",
    "        grads = {k: np.zeros_like(v) for k, v in p.items()}\n",
    "\n",
    "        grads['W_y'] = self.last_h.T @ dY\n",
    "        grads['b_y'] = np.sum(dY, axis=0)\n",
    "\n",
    "        dh = dY @ p['W_y'].T\n",
    "        dc = np.zeros_like(dh)\n",
    "\n",
    "        D = self.input_size\n",
    "\n",
    "        for step in reversed(self.cache):\n",
    "            z = step['z']\n",
    "            i = step['i']\n",
    "            f = step['f']\n",
    "            o = step['o']\n",
    "            g = step['g']\n",
    "            c_prev = step['c_prev']\n",
    "            c = step['c']\n",
    "            tanh_c = step['tanh_c']\n",
    "\n",
    "            # h = o * tanh(c)\n",
    "            do = dh * tanh_c\n",
    "            dc_total = dc + dh * o * (1.0 - tanh_c * tanh_c)\n",
    "\n",
    "            # c = f*c_prev + i*g\n",
    "            df = dc_total * c_prev\n",
    "            di = dc_total * g\n",
    "            dg = dc_total * i\n",
    "            dc = dc_total * f\n",
    "\n",
    "            # gate pre-activations\n",
    "            dai = di * i * (1.0 - i)\n",
    "            daf = df * f * (1.0 - f)\n",
    "            dao = do * o * (1.0 - o)\n",
    "            dag = dg * (1.0 - g * g)\n",
    "\n",
    "            grads['W_i'] += z.T @ dai\n",
    "            grads['b_i'] += np.sum(dai, axis=0)\n",
    "\n",
    "            grads['W_f'] += z.T @ daf\n",
    "            grads['b_f'] += np.sum(daf, axis=0)\n",
    "\n",
    "            grads['W_o'] += z.T @ dao\n",
    "            grads['b_o'] += np.sum(dao, axis=0)\n",
    "\n",
    "            grads['W_g'] += z.T @ dag\n",
    "            grads['b_g'] += np.sum(dag, axis=0)\n",
    "\n",
    "            dz = (\n",
    "                dai @ p['W_i'].T\n",
    "                + daf @ p['W_f'].T\n",
    "                + dao @ p['W_o'].T\n",
    "                + dag @ p['W_g'].T\n",
    "            )\n",
    "\n",
    "            dh = dz[:, D:]  # propagate to previous hidden state\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_numpy_model(\n",
    "    model,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    *,\n",
    "    epochs: int = 120,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 2e-3,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    seed: int = 42,\n",
    "    verbose_every: int = 20,\n",
    "):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    opt = Adam(model.params, lr=lr)\n",
    "\n",
    "    history = []\n",
    "    n = len(X_train)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        idx = rng_local.permutation(n)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for start in range(0, n, batch_size):\n",
    "            bidx = idx[start : start + batch_size]\n",
    "            Xb = X_train[bidx]\n",
    "            yb = y_train[bidx]\n",
    "\n",
    "            y_hat = model.forward(Xb)\n",
    "            loss, dY = mse_loss(y_hat, yb)\n",
    "\n",
    "            grads = model.backward(dY)\n",
    "            grad_norm = clip_grads_(grads, max_norm=max_grad_norm)\n",
    "            opt.step(model.params, grads)\n",
    "\n",
    "            total_loss += loss * len(bidx)\n",
    "\n",
    "        avg = total_loss / n\n",
    "        history.append(avg)\n",
    "\n",
    "        if epoch == 1 or epoch % verbose_every == 0 or epoch == epochs:\n",
    "            print(f\"epoch {epoch:>4}/{epochs} | loss={avg:.6f} | grad_norm={grad_norm:.3f}\")\n",
    "\n",
    "    return np.asarray(history, dtype=float)\n",
    "\n",
    "\n",
    "def predict_numpy(model, X: np.ndarray) -> np.ndarray:\n",
    "    return model.forward(X)\n",
    "\n",
    "\n",
    "def unscale(x_scaled: np.ndarray) -> np.ndarray:\n",
    "    return x_scaled * train_std + train_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_RUN = True\n",
    "\n",
    "EPOCHS_NUMPY = 80 if FAST_RUN else 200\n",
    "HIDDEN = 32\n",
    "\n",
    "lstm_np = NumpyLSTMForecaster(input_size=1, hidden_size=HIDDEN, seed=SEED)\n",
    "\n",
    "hist_lstm_np = train_numpy_model(\n",
    "    lstm_np,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS_NUMPY,\n",
    "    batch_size=64,\n",
    "    lr=2e-3,\n",
    "    max_grad_norm=1.0,\n",
    "    seed=SEED,\n",
    "    verbose_every=20,\n",
    ")\n",
    "\n",
    "fig = px.line(y=hist_lstm_np, title=\"NumPy LSTM: training loss\", labels={\"x\": \"epoch\", \"y\": \"MSE\"})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ed592",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = unscale(predict_numpy(lstm_np, X_test))[:, 0]\n",
    "y_true_test = unscale(y_test)[:, 0]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_true_test, mode=\"lines\", name=\"true\"))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred_test_lstm, mode=\"lines\", name=\"NumPy LSTM\"))\n",
    "fig.update_layout(\n",
    "    title=\"One-step-ahead predictions on the test region (NumPy LSTM)\",\n",
    "    xaxis_title=\"time index\",\n",
    "    yaxis_title=\"value\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "mse = float(np.mean((y_pred_test_lstm - y_true_test) ** 2))\n",
    "mae = float(np.mean(np.abs(y_pred_test_lstm - y_true_test)))\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67addde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_forecast_numpy(model, seed_window_scaled_1d: np.ndarray, steps: int):\n",
    "    window = seed_window_scaled_1d.astype(float).copy()\n",
    "    preds = []\n",
    "    for _ in range(steps):\n",
    "        y_hat = model.forward(window[None, :, None])[0, 0]\n",
    "        preds.append(float(y_hat))\n",
    "        window = np.concatenate([window[1:], [y_hat]])\n",
    "    return np.asarray(preds, dtype=float)\n",
    "\n",
    "\n",
    "# Roll out recursively from the first test window\n",
    "roll_steps = 200\n",
    "seed_window = y_scaled[split_t - SEQ_LEN : split_t]\n",
    "future_true = y[split_t : split_t + roll_steps]\n",
    "\n",
    "future_pred_scaled = rollout_forecast_numpy(lstm_np, seed_window, steps=roll_steps)\n",
    "future_pred = unscale(future_pred_scaled)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t[split_t : split_t + roll_steps], y=future_true, mode=\"lines\", name=\"true\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=t[split_t : split_t + roll_steps],\n",
    "        y=future_pred,\n",
    "        mode=\"lines\",\n",
    "        name=\"NumPy LSTM (rollout)\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Recursive rollout forecast (NumPy LSTM)\",\n",
    "    xaxis_title=\"time index\",\n",
    "    yaxis_title=\"value\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeec5ad",
   "metadata": {},
   "source": [
    "#### Visualizing LSTM gates\n",
    "\n",
    "To make the LSTM less of a black box, we can inspect the gate activations for a single input window.\n",
    "\n",
    "The plots below show **hidden units (rows)** × **time steps (columns)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47428df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_gate_mats(model: NumpyLSTMForecaster, X_one: np.ndarray):\n",
    "    _ = model.forward(X_one)\n",
    "    assert model.cache is not None\n",
    "\n",
    "    i = np.stack([step['i'][0] for step in model.cache], axis=0).T\n",
    "    f = np.stack([step['f'][0] for step in model.cache], axis=0).T\n",
    "    o = np.stack([step['o'][0] for step in model.cache], axis=0).T\n",
    "    g = np.stack([step['g'][0] for step in model.cache], axis=0).T\n",
    "    c = np.stack([step['c'][0] for step in model.cache], axis=0).T\n",
    "\n",
    "    return i, f, o, g, c\n",
    "\n",
    "\n",
    "X_one = X_test[:1]\n",
    "i, f, o, g, c = lstm_gate_mats(lstm_np, X_one)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=[\"input gate i\", \"forget gate f\", \"output gate o\", \"cell state c\"],\n",
    "    horizontal_spacing=0.08,\n",
    "    vertical_spacing=0.12,\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=i, coloraxis=\"coloraxis\"), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=f, coloraxis=\"coloraxis\"), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=o, coloraxis=\"coloraxis\"), row=2, col=1)\n",
    "fig.add_trace(go.Heatmap(z=c, coloraxis=\"coloraxis\"), row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"NumPy LSTM: gate activations for one window\",\n",
    "    height=750,\n",
    "    coloraxis=dict(colorscale=\"Viridis\"),\n",
    ")\n",
    "fig.update_xaxes(title_text=\"time step\")\n",
    "fig.update_yaxes(title_text=\"hidden unit\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed9698",
   "metadata": {},
   "source": [
    "### 4.2) NumPy GRU forecaster\n",
    "\n",
    "The GRU is a bit simpler than the LSTM: fewer gates and no separate cell state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyGRUForecaster:\n",
    "    def __init__(self, input_size: int, hidden_size: int, seed: int = 42):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        rng_local = np.random.default_rng(seed)\n",
    "        fan_in = input_size + hidden_size\n",
    "\n",
    "        self.params = {\n",
    "            'W_z': init_weight(rng_local, fan_in, hidden_size),\n",
    "            'b_z': np.zeros((hidden_size,), dtype=float),\n",
    "            'W_r': init_weight(rng_local, fan_in, hidden_size),\n",
    "            'b_r': np.zeros((hidden_size,), dtype=float),\n",
    "            'W_h': init_weight(rng_local, fan_in, hidden_size),\n",
    "            'b_h': np.zeros((hidden_size,), dtype=float),\n",
    "            'W_y': init_weight(rng_local, hidden_size, 1),\n",
    "            'b_y': np.zeros((1,), dtype=float),\n",
    "        }\n",
    "\n",
    "        self.cache = None\n",
    "        self.last_h = None\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        B, T, D = X.shape\n",
    "        H = self.hidden_size\n",
    "\n",
    "        h = np.zeros((B, H), dtype=float)\n",
    "        caches = []\n",
    "        p = self.params\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t, :]\n",
    "            concat = np.concatenate([x_t, h], axis=1)\n",
    "\n",
    "            z = sigmoid(concat @ p['W_z'] + p['b_z'])\n",
    "            r = sigmoid(concat @ p['W_r'] + p['b_r'])\n",
    "\n",
    "            concat2 = np.concatenate([x_t, r * h], axis=1)\n",
    "            h_tilde = np.tanh(concat2 @ p['W_h'] + p['b_h'])\n",
    "\n",
    "            h_next = (1.0 - z) * h + z * h_tilde\n",
    "\n",
    "            caches.append(\n",
    "                {\n",
    "                    'x_t': x_t,\n",
    "                    'h_prev': h,\n",
    "                    'concat': concat,\n",
    "                    'concat2': concat2,\n",
    "                    'z': z,\n",
    "                    'r': r,\n",
    "                    'h_tilde': h_tilde,\n",
    "                }\n",
    "            )\n",
    "            h = h_next\n",
    "\n",
    "        y_hat = h @ p['W_y'] + p['b_y']\n",
    "        self.cache = caches\n",
    "        self.last_h = h\n",
    "        return y_hat\n",
    "\n",
    "    def backward(self, dY: np.ndarray) -> dict[str, np.ndarray]:\n",
    "        assert self.cache is not None and self.last_h is not None\n",
    "\n",
    "        p = self.params\n",
    "        grads = {k: np.zeros_like(v) for k, v in p.items()}\n",
    "\n",
    "        grads['W_y'] = self.last_h.T @ dY\n",
    "        grads['b_y'] = np.sum(dY, axis=0)\n",
    "\n",
    "        dh = dY @ p['W_y'].T\n",
    "        D = self.input_size\n",
    "\n",
    "        for step in reversed(self.cache):\n",
    "            h_prev = step['h_prev']\n",
    "            concat = step['concat']\n",
    "            concat2 = step['concat2']\n",
    "            z = step['z']\n",
    "            r = step['r']\n",
    "            h_tilde = step['h_tilde']\n",
    "\n",
    "            # h = (1-z)*h_prev + z*h_tilde\n",
    "            dh_tilde = dh * z\n",
    "            dz = dh * (h_tilde - h_prev)\n",
    "            dh_prev = dh * (1.0 - z)\n",
    "\n",
    "            # h_tilde = tanh(concat2 @ W_h + b_h)\n",
    "            da_h = dh_tilde * (1.0 - h_tilde * h_tilde)\n",
    "            grads['W_h'] += concat2.T @ da_h\n",
    "            grads['b_h'] += np.sum(da_h, axis=0)\n",
    "\n",
    "            dconcat2 = da_h @ p['W_h'].T\n",
    "            drh_prev = dconcat2[:, D:]\n",
    "\n",
    "            dr = drh_prev * h_prev\n",
    "            dh_prev += drh_prev * r\n",
    "\n",
    "            # r gate\n",
    "            da_r = dr * r * (1.0 - r)\n",
    "            grads['W_r'] += concat.T @ da_r\n",
    "            grads['b_r'] += np.sum(da_r, axis=0)\n",
    "            dconcat_r = da_r @ p['W_r'].T\n",
    "\n",
    "            # z gate\n",
    "            da_z = dz * z * (1.0 - z)\n",
    "            grads['W_z'] += concat.T @ da_z\n",
    "            grads['b_z'] += np.sum(da_z, axis=0)\n",
    "            dconcat_z = da_z @ p['W_z'].T\n",
    "\n",
    "            dconcat = dconcat_r + dconcat_z\n",
    "            dh_prev += dconcat[:, D:]\n",
    "\n",
    "            dh = dh_prev\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_np = NumpyGRUForecaster(input_size=1, hidden_size=HIDDEN, seed=SEED)\n",
    "\n",
    "hist_gru_np = train_numpy_model(\n",
    "    gru_np,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS_NUMPY,\n",
    "    batch_size=64,\n",
    "    lr=2e-3,\n",
    "    max_grad_norm=1.0,\n",
    "    seed=SEED,\n",
    "    verbose_every=20,\n",
    ")\n",
    "\n",
    "fig = px.line(y=hist_gru_np, title=\"NumPy GRU: training loss\", labels={\"x\": \"epoch\", \"y\": \"MSE\"})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96978586",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_gru = unscale(predict_numpy(gru_np, X_test))[:, 0]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_true_test, mode=\"lines\", name=\"true\"))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred_test_gru, mode=\"lines\", name=\"NumPy GRU\"))\n",
    "fig.update_layout(\n",
    "    title=\"One-step-ahead predictions on the test region (NumPy GRU)\",\n",
    "    xaxis_title=\"time index\",\n",
    "    yaxis_title=\"value\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "mse = float(np.mean((y_pred_test_gru - y_true_test) ** 2))\n",
    "mae = float(np.mean(np.abs(y_pred_test_gru - y_true_test)))\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920ad35",
   "metadata": {},
   "source": [
    "#### Visualizing GRU gates\n",
    "\n",
    "We’ll inspect:\n",
    "- update gate $z_t$\n",
    "- reset gate $r_t$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_gate_mats(model: NumpyGRUForecaster, X_one: np.ndarray):\n",
    "    _ = model.forward(X_one)\n",
    "    assert model.cache is not None\n",
    "\n",
    "    z = np.stack([step['z'][0] for step in model.cache], axis=0).T\n",
    "    r = np.stack([step['r'][0] for step in model.cache], axis=0).T\n",
    "    h_tilde = np.stack([step['h_tilde'][0] for step in model.cache], axis=0).T\n",
    "    return z, r, h_tilde\n",
    "\n",
    "\n",
    "z, r, h_tilde = gru_gate_mats(gru_np, X_one)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=[\"update gate z\", \"reset gate r\", \"candidate h~\"],\n",
    "    horizontal_spacing=0.06,\n",
    ")\n",
    "fig.add_trace(go.Heatmap(z=z, coloraxis=\"coloraxis\"), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=r, coloraxis=\"coloraxis\"), row=1, col=2)\n",
    "fig.add_trace(go.Heatmap(z=h_tilde, coloraxis=\"coloraxis\"), row=1, col=3)\n",
    "fig.update_layout(\n",
    "    title=\"NumPy GRU: gate activations for one window\",\n",
    "    height=380,\n",
    "    coloraxis=dict(colorscale=\"Viridis\"),\n",
    ")\n",
    "fig.update_xaxes(title_text=\"time step\")\n",
    "fig.update_yaxes(title_text=\"hidden unit\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a0293",
   "metadata": {},
   "source": [
    "## 5) PyTorch: the same idea with `nn.LSTM` / `nn.GRU`\n",
    "\n",
    "PyTorch takes care of:\n",
    "- parameter initialization\n",
    "- BPTT + autograd\n",
    "- efficient kernels\n",
    "\n",
    "We’ll keep the *exact same dataset* and model shape to make the comparison fair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class TorchRNNForecaster(nn.Module):\n",
    "    def __init__(self, rnn_type: str, input_size: int = 1, hidden_size: int = 32):\n",
    "        super().__init__()\n",
    "        rnn_type = rnn_type.lower()\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"rnn_type must be 'lstm' or 'gru'\")\n",
    "\n",
    "        self.head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out, _ = self.rnn(x)\n",
    "        h_last = out[:, -1, :]\n",
    "        return self.head(h_last)\n",
    "\n",
    "\n",
    "def train_torch_model(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    *,\n",
    "    epochs: int = 40,\n",
    "    lr: float = 1e-3,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: str = 'cpu',\n",
    "    verbose_every: int = 10,\n",
    "):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            opt.step()\n",
    "\n",
    "            total += float(loss.item()) * xb.shape[0]\n",
    "\n",
    "        avg = total / len(loader.dataset)\n",
    "        history.append(avg)\n",
    "\n",
    "        if epoch == 1 or epoch % verbose_every == 0 or epoch == epochs:\n",
    "            print(f\"epoch {epoch:>4}/{epochs} | loss={avg:.6f}\")\n",
    "\n",
    "    return np.asarray(history, dtype=float)\n",
    "\n",
    "\n",
    "def predict_torch(model: nn.Module, X: np.ndarray, device: str = 'cpu') -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def pick_device() -> str:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", message=\"CUDA initialization.*\")\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "device = pick_device()\n",
    "print('device:', device)\n",
    "\n",
    "\n",
    "EPOCHS_TORCH = 40 if FAST_RUN else 120\n",
    "\n",
    "train_loader = DataLoader(WindowDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "\n",
    "torch_lstm = TorchRNNForecaster('lstm', input_size=1, hidden_size=HIDDEN)\n",
    "torch_gru = TorchRNNForecaster('gru', input_size=1, hidden_size=HIDDEN)\n",
    "\n",
    "hist_torch_lstm = train_torch_model(\n",
    "    torch_lstm,\n",
    "    train_loader,\n",
    "    epochs=EPOCHS_TORCH,\n",
    "    lr=1e-3,\n",
    "    max_grad_norm=1.0,\n",
    "    device=device,\n",
    "    verbose_every=10,\n",
    ")\n",
    "\n",
    "hist_torch_gru = train_torch_model(\n",
    "    torch_gru,\n",
    "    train_loader,\n",
    "    epochs=EPOCHS_TORCH,\n",
    "    lr=1e-3,\n",
    "    max_grad_norm=1.0,\n",
    "    device=device,\n",
    "    verbose_every=10,\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=hist_lstm_np, mode='lines', name='NumPy LSTM'))\n",
    "fig.add_trace(go.Scatter(y=hist_gru_np, mode='lines', name='NumPy GRU'))\n",
    "fig.add_trace(go.Scatter(y=hist_torch_lstm, mode='lines', name='Torch LSTM'))\n",
    "fig.add_trace(go.Scatter(y=hist_torch_gru, mode='lines', name='Torch GRU'))\n",
    "fig.update_layout(title='Training loss comparison', xaxis_title='epoch', yaxis_title='MSE')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_torch_lstm = unscale(predict_torch(torch_lstm, X_test, device=device))[:, 0]\n",
    "y_pred_test_torch_gru = unscale(predict_torch(torch_gru, X_test, device=device))[:, 0]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_true_test, mode=\"lines\", name=\"true\"))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred_test_lstm, mode=\"lines\", name=\"NumPy LSTM\"))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred_test_gru, mode=\"lines\", name=\"NumPy GRU\"))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred_test_torch_lstm, mode=\"lines\", name=\"Torch LSTM\"))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred_test_torch_gru, mode=\"lines\", name=\"Torch GRU\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"One-step-ahead predictions (test region)\",\n",
    "    xaxis_title=\"time index\",\n",
    "    yaxis_title=\"value\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed95ee7",
   "metadata": {},
   "source": [
    "## 6) Pitfalls + diagnostics (practical)\n",
    "\n",
    "- **Scale your data**: unscaled series often makes training unstable.\n",
    "- **Exploding gradients**: use **gradient clipping** (we did).\n",
    "- **Sequence length**: too short → misses long seasonality; too long → harder optimization.\n",
    "- **Train/test split**: split by **time**, not random shuffle.\n",
    "- **Rollout error accumulation**: one-step models can drift when rolled out recursively.\n",
    "\n",
    "If you want a stronger forecaster:\n",
    "- predict multiple future steps (seq2seq)\n",
    "- add exogenous features (calendar, holidays, covariates)\n",
    "- use teacher forcing / scheduled sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47a8d8",
   "metadata": {},
   "source": [
    "## 7) Exercises\n",
    "\n",
    "1. Increase the noise and compare LSTM vs GRU stability.\n",
    "2. Add a second seasonal component with a longer period and test different `SEQ_LEN`.\n",
    "3. Predict multiple steps ahead (e.g. next 10 points) and compare direct vs recursive forecasting.\n",
    "4. Add dropout (PyTorch) and see how it affects rollout drift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609a34f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Hochreiter & Schmidhuber (1997): *Long Short-Term Memory*\n",
    "- Cho et al. (2014): *Learning Phrase Representations using RNN Encoder–Decoder* (introduces GRU)\n",
    "- Goodfellow, Bengio, Courville: *Deep Learning* (sequence models chapters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
