{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d76cb6a",
   "metadata": {},
   "source": [
    "# DDPG (Deep Deterministic Policy Gradient) — Continuous Control\n",
    "\n",
    "DDPG is an **off-policy actor–critic** algorithm for **continuous action spaces**.\n",
    "\n",
    "It learns:\n",
    "\n",
    "- an **actor** $\\mu_\theta(s)$ that outputs a *deterministic* action\n",
    "- a **critic** $Q_\\phi(s,a)$ that estimates the action-value\n",
    "\n",
    "DDPG is a foundational algorithm: understanding it makes TD3/SAC much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef3949",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain DDPG’s **actor–critic** structure and why it’s **off-policy**\n",
    "- derive the **critic target** using **target networks**\n",
    "- write the **deterministic policy gradient** update for the actor\n",
    "- understand the role of **experience replay** and **exploration noise**\n",
    "\n",
    "For a full low-level PyTorch implementation (with Plotly diagnostics), see `01_ddpg_from_scratch.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8fe48",
   "metadata": {},
   "source": [
    "## The moving parts (precisely)\n",
    "\n",
    "### 1) Actor (policy)\n",
    "\n",
    "A deterministic policy network:\n",
    "\n",
    "$$a = \\mu_\theta(s)$$\n",
    "\n",
    "In continuous-control Gym/Gymnasium environments, we typically use a `tanh` head and **scale** to the action bounds.\n",
    "\n",
    "### 2) Critic (Q-function)\n",
    "\n",
    "A state–action value function approximator:\n",
    "\n",
    "$$Q_\\phi(s,a) \u0007pprox Q^{\\mu}(s,a) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_{t} \\mid s_0=s, a_0=a, a_{t>0}=\\mu(s_t)\r",
    "ight]$$\n",
    "\n",
    "### 3) Target networks (stabilization)\n",
    "\n",
    "Maintain slowly-updated copies:\n",
    "\n",
    "- target actor $\\mu_{\theta'}$\n",
    "- target critic $Q_{\\phi'}$\n",
    "\n",
    "**Soft update** after each gradient step:\n",
    "\n",
    "$$\theta' \\leftarrow \tau\\,\theta + (1-\tau)\\,\theta'$$\n",
    "$$\\phi' \\leftarrow \tau\\,\\phi + (1-\tau)\\,\\phi'$$\n",
    "\n",
    "### 4) Replay buffer (off-policy learning)\n",
    "\n",
    "Store transitions $(s,a,r,s',d)$ and sample i.i.d. minibatches to reduce correlation and improve data efficiency.\n",
    "\n",
    "### 5) Exploration noise\n",
    "\n",
    "Because $\\mu_\theta$ is deterministic, exploration usually uses additive noise:\n",
    "\n",
    "$$a_t = \\mu_\theta(s_t) + \\epsilon_t,\\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6864880",
   "metadata": {},
   "source": [
    "## Core updates (equations)\n",
    "\n",
    "### Critic update\n",
    "\n",
    "Given a replay minibatch, define the TD target (using target networks):\n",
    "\n",
    "$$y = r + \\gamma(1-d)\\,Q_{\\phi'}(s', \\mu_{\theta'}(s'))$$\n",
    "\n",
    "Minimize MSE:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = \\mathbb{E}\big[(Q_{\\phi}(s,a) - y)^2\big]$$\n",
    "\n",
    "### Actor update (deterministic policy gradient)\n",
    "\n",
    "Maximize the critic’s value under the actor:\n",
    "\n",
    "$$J(\theta) = \\mathbb{E}_{s\\sim\\mathcal{D}}\big[Q_\\phi(s, \\mu_\theta(s))\big]$$\n",
    "\n",
    "Gradient (applied by autograd in PyTorch):\n",
    "\n",
    "$$\n",
    "abla_\theta J(\theta) = \\mathbb{E}\\left[\n",
    "abla_a Q_\\phi(s,a)\r",
    "vert_{a=\\mu_\theta(s)}\\,\n",
    "abla_\theta \\mu_\theta(s)\r",
    "ight]$$\n",
    "\n",
    "In code we typically minimize **actor loss** $\\mathcal{L}_{actor} = -\\mathbb{E}[Q_\\phi(s, \\mu_\theta(s))]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee8bb0",
   "metadata": {},
   "source": [
    "## Next notebook\n",
    "\n",
    "- `01_ddpg_from_scratch.ipynb`: full DDPG in low-level PyTorch (replay buffer, target networks, training loop) + Plotly visuals for:\n",
    "  - score per episode (learning curve)\n",
    "  - Q-values / targets over training\n",
    "  - policy evolution on fixed probe states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2988d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "    _TORCH_IMPORT_ERROR = e\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    GYMNASIUM_AVAILABLE = True\n",
    "except Exception:\n",
    "    GYMNASIUM_AVAILABLE = False\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print('Python', platform.python_version())\n",
    "print('NumPy', np.__version__)\n",
    "print('Plotly', plotly.__version__)\n",
    "print('Torch', torch.__version__ if TORCH_AVAILABLE else _TORCH_IMPORT_ERROR)\n",
    "print('Gymnasium', gym.__version__ if GYMNASIUM_AVAILABLE else 'not installed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny Plotly sketch: actor–critic block diagram (conceptual)\n",
    "fig = go.Figure()\n",
    "fig.add_shape(type='rect', x0=0.05, x1=0.35, y0=0.55, y1=0.85, line=dict(width=2))\n",
    "fig.add_annotation(x=0.20, y=0.70, text='Actor\\n$\\\\mu_\\\\theta(s)$', showarrow=False, font=dict(size=14))\n",
    "\n",
    "fig.add_shape(type='rect', x0=0.55, x1=0.95, y0=0.55, y1=0.85, line=dict(width=2))\n",
    "fig.add_annotation(x=0.75, y=0.70, text='Critic\\n$Q_\\\\phi(s,a)$', showarrow=False, font=dict(size=14))\n",
    "\n",
    "# arrows\n",
    "fig.add_annotation(x=0.45, y=0.70, ax=0.35, ay=0.70, xref='paper', yref='paper', axref='paper', ayref='paper',\n",
    "                   text='', showarrow=True, arrowhead=3, arrowsize=1.2)\n",
    "fig.add_annotation(x=0.55, y=0.65, ax=0.35, ay=0.65, xref='paper', yref='paper', axref='paper', ayref='paper',\n",
    "                   text='', showarrow=True, arrowhead=3, arrowsize=1.2)\n",
    "\n",
    "fig.add_annotation(x=0.45, y=0.80, text='$s$', showarrow=False)\n",
    "fig.add_annotation(x=0.45, y=0.64, text='$a$', showarrow=False)\n",
    "\n",
    "fig.update_xaxes(visible=False, range=[0, 1])\n",
    "fig.update_yaxes(visible=False, range=[0, 1])\n",
    "fig.update_layout(title='DDPG: actor produces action; critic evaluates (s,a)', height=280)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}