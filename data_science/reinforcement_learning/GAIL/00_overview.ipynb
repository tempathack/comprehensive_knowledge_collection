{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e198b67",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning (GAIL) — low-level PyTorch\n",
    "\n",
    "GAIL is an **imitation learning** algorithm: it learns a policy \\(\\pi_\\theta(a\\mid s)\\) from **expert demonstrations** without access to the expert’s reward function.\n",
    "\n",
    "The core idea is adversarial training:\n",
    "\n",
    "- a **discriminator** \\(D_\\phi(s,a)\\) tries to tell **expert** \\((s,a)\\) pairs apart from **policy-generated** \\((s,a)\\) pairs\n",
    "- the **policy** is trained to **fool** the discriminator, using a reward derived from \\(D_\\phi\\)\n",
    "\n",
    "This notebook implements a small but complete GAIL loop **from scratch in PyTorch**:\n",
    "\n",
    "- a toy 2D navigation environment (no Gym dependency)\n",
    "- a hand-coded expert to generate demonstrations\n",
    "- a discriminator network \\(D_\\phi\\)\n",
    "- an actor-critic policy \\(\\pi_\\theta\\) trained with PPO using the discriminator reward\n",
    "- Plotly curves for discriminator loss, policy learning, and episodic rewards\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- write down the GAIL min–max objective and explain the GAN analogy\n",
    "- implement a discriminator over \\((s,a)\\) and train it with cross-entropy\n",
    "- turn discriminator outputs into a reward signal for RL\n",
    "- implement the PPO update (clipped objective + value loss + entropy bonus)\n",
    "- monitor training with Plotly: discriminator loss + episodic return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbca5e",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. GAIL objective + adversarial training equations\n",
    "2. A tiny offline-friendly environment + expert demonstrations\n",
    "3. Low-level PyTorch: policy/value networks + discriminator\n",
    "4. Training loop (alternate discriminator updates and policy PPO updates)\n",
    "5. Plotly diagnostics: discriminator loss, policy learning, episodic rewards\n",
    "6. Stable-Baselines GAIL notes + hyperparameters (end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Some environments emit a CUDA-availability warning even when using CPU tensors.\n",
    "warnings.filterwarnings(\"ignore\", message=r\"CUDA initialization:.*\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run configuration ---\n",
    "FAST_RUN = True  # set False for longer training\n",
    "\n",
    "# Environment\n",
    "N_ENVS = 32 if FAST_RUN else 128\n",
    "MAX_STEPS = 60\n",
    "STEP_SIZE = 0.20\n",
    "NOISE_STD = 0.00\n",
    "GOAL_RADIUS = 0.12\n",
    "\n",
    "# Expert dataset\n",
    "N_EXPERT_EPISODES = 300 if FAST_RUN else 1500\n",
    "\n",
    "# GAIL + PPO\n",
    "ITERATIONS = 25 if FAST_RUN else 200\n",
    "STEPS_PER_ITER = 128 if FAST_RUN else 512\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA_GAE = 0.95\n",
    "\n",
    "# Discriminator updates\n",
    "D_LR = 3e-4\n",
    "D_EPOCHS = 2 if FAST_RUN else 5\n",
    "D_BATCH_SIZE = 512\n",
    "\n",
    "# PPO updates\n",
    "PI_LR = 3e-4\n",
    "PPO_EPOCHS = 4 if FAST_RUN else 10\n",
    "PPO_BATCH_SIZE = 1024\n",
    "CLIP_EPS = 0.2\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0.01\n",
    "\n",
    "# Eval\n",
    "EVAL_EVERY = 5\n",
    "EVAL_EPISODES = 200 if FAST_RUN else 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78569db6",
   "metadata": {},
   "source": [
    "## 1) GAIL: adversarial training objective (equations)\n",
    "\n",
    "### Setup\n",
    "\n",
    "You have expert demonstrations (state-action pairs) sampled from an expert policy \\(\\pi_E\\):\n",
    "\n",
    "\\[\n",
    "(s,a) \\sim \\pi_E.\n",
    "\\]\n",
    "\n",
    "You want to learn a policy \\(\\pi_\\theta\\) that induces (approximately) the **same occupancy measure** as the expert.\n",
    "\n",
    "### Discriminator objective\n",
    "\n",
    "GAIL uses a discriminator \\(D_\\phi(s,a)\\in(0,1)\\) that outputs the probability that a \\((s,a)\\) pair came from the expert.\n",
    "It is trained like a GAN discriminator:\n",
    "\n",
    "\\[\n",
    "\\max_{\\phi}\\; \\mathbb{E}_{(s,a)\\sim \\pi_E}[\\log D_\\phi(s,a)]\n",
    "+ \\mathbb{E}_{(s,a)\\sim \\pi_\\theta}[\\log(1 - D_\\phi(s,a))].\n",
    "\\]\n",
    "\n",
    "### Policy (generator) objective\n",
    "\n",
    "The policy plays the role of the GAN generator: it tries to produce \\((s,a)\\) that the discriminator labels as expert.\n",
    "A common generator loss is:\n",
    "\n",
    "\\[\n",
    "\\min_{\\theta}\\; \\mathbb{E}_{(s,a)\\sim \\pi_\\theta}[\\log(1 - D_\\phi(s,a))] - \\lambda\\,H(\\pi_\\theta),\n",
    "\\]\n",
    "\n",
    "where \\(H(\\pi_\\theta)\\) is the policy entropy (encourages exploration).\n",
    "\n",
    "### Turning the discriminator into a reward\n",
    "\n",
    "To train \\(\\pi_\\theta\\) with RL, we convert discriminator outputs into a reward:\n",
    "\n",
    "\\[\n",
    "\\hat r_\\phi(s,a) = -\\log(1 - D_\\phi(s,a)).\n",
    "\\]\n",
    "\n",
    "If the discriminator uses a logit \\(f_\\phi(s,a)\\) (so \\(D_\\phi = \\sigma(f_\\phi)\\)), this reward has a numerically-stable form:\n",
    "\n",
    "\\[\n",
    "\\hat r_\\phi(s,a) = -\\log(\\sigma(-f_\\phi(s,a))) = \\mathrm{softplus}(f_\\phi(s,a)).\n",
    "\\]\n",
    "\n",
    "### Policy optimization (we’ll use PPO)\n",
    "\n",
    "We’ll optimize \\(\\pi_\\theta\\) with PPO. With advantage estimates \\(\\hat A_t\\), PPO’s clipped objective is:\n",
    "\n",
    "\\[\n",
    "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\Big[\\min\\big(r_t(\\theta)\\hat A_t,\\;\\mathrm{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat A_t\\big)\\Big],\n",
    "\\]\n",
    "\n",
    "where \\(r_t(\\theta)=\\exp(\\log\\pi_\\theta(a_t\\mid s_t)-\\log\\pi_{\\theta_{\\text{old}}}(a_t\\mid s_t))\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53015282",
   "metadata": {},
   "source": [
    "## 2) A tiny environment (no downloads, no Gym)\n",
    "\n",
    "We’ll use a simple **2D point navigation** task:\n",
    "\n",
    "- observation \\(s = (x,y)\\in[-1,1]^2\\)\n",
    "- 5 discrete actions: stay / up / down / left / right\n",
    "- start position is random\n",
    "- goal is the origin\n",
    "- episode ends when the point enters a goal radius or hits a step limit\n",
    "\n",
    "We’ll generate expert demonstrations using a greedy hand-coded expert that always moves along the largest coordinate toward 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0157c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorPointNav2D:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_envs: int,\n",
    "        max_steps: int = 60,\n",
    "        step_size: float = 0.20,\n",
    "        noise_std: float = 0.00,\n",
    "        goal: tuple[float, float] = (0.0, 0.0),\n",
    "        goal_radius: float = 0.12,\n",
    "        seed: int = 0,\n",
    "    ):\n",
    "        self.n_envs = int(n_envs)\n",
    "        self.max_steps = int(max_steps)\n",
    "        self.step_size = float(step_size)\n",
    "        self.noise_std = float(noise_std)\n",
    "        self.goal = np.array(goal, dtype=np.float32)\n",
    "        self.goal_radius = float(goal_radius)\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.pos = np.zeros((self.n_envs, 2), dtype=np.float32)\n",
    "        self.t = np.zeros(self.n_envs, dtype=np.int32)\n",
    "\n",
    "    @property\n",
    "    def obs_dim(self) -> int:\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def n_actions(self) -> int:\n",
    "        # 0 stay, 1 up, 2 down, 3 left, 4 right\n",
    "        return 5\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.t[:] = 0\n",
    "        self.pos[:] = self.rng.uniform(low=-1.0, high=1.0, size=(self.n_envs, 2)).astype(np.float32)\n",
    "        return self.pos.copy()\n",
    "\n",
    "    def reset_done(self, done_mask: np.ndarray) -> None:\n",
    "        idx = np.where(done_mask)[0]\n",
    "        if len(idx) == 0:\n",
    "            return\n",
    "        self.t[idx] = 0\n",
    "        self.pos[idx] = self.rng.uniform(low=-1.0, high=1.0, size=(len(idx), 2)).astype(np.float32)\n",
    "\n",
    "    def step(self, actions: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, dict]:\n",
    "        actions = np.asarray(actions, dtype=np.int64)\n",
    "        assert actions.shape == (self.n_envs,)\n",
    "\n",
    "        move = np.zeros((self.n_envs, 2), dtype=np.float32)\n",
    "        move[actions == 1, 1] = 1.0\n",
    "        move[actions == 2, 1] = -1.0\n",
    "        move[actions == 3, 0] = -1.0\n",
    "        move[actions == 4, 0] = 1.0\n",
    "\n",
    "        noise = self.rng.normal(loc=0.0, scale=self.noise_std, size=(self.n_envs, 2)).astype(np.float32)\n",
    "        self.pos = np.clip(self.pos + self.step_size * move + noise, -1.0, 1.0)\n",
    "        self.t += 1\n",
    "\n",
    "        dist = np.linalg.norm(self.pos - self.goal[None, :], axis=1)\n",
    "        success = dist < self.goal_radius\n",
    "        done = success | (self.t >= self.max_steps)\n",
    "\n",
    "        # True environment reward (for monitoring): small time penalty, big success bonus\n",
    "        reward = -0.01 * np.ones(self.n_envs, dtype=np.float32)\n",
    "        reward = reward + success.astype(np.float32) * 1.0\n",
    "\n",
    "        info = {\n",
    "            \"dist\": dist.astype(np.float32),\n",
    "            \"success\": success.astype(np.bool_),\n",
    "        }\n",
    "        return self.pos.copy(), reward, done.astype(np.bool_), info\n",
    "\n",
    "\n",
    "def expert_policy(obs: np.ndarray) -> np.ndarray:\n",
    "    # Greedy expert: move along the largest coordinate toward 0.\n",
    "    x = obs[:, 0]\n",
    "    y = obs[:, 1]\n",
    "    ax = np.abs(x)\n",
    "    ay = np.abs(y)\n",
    "\n",
    "    actions = np.zeros(len(obs), dtype=np.int64)\n",
    "    choose_x = ax >= ay\n",
    "\n",
    "    actions[choose_x & (x > 0)] = 3  # left\n",
    "    actions[choose_x & (x < 0)] = 4  # right\n",
    "\n",
    "    actions[(~choose_x) & (y > 0)] = 2  # down\n",
    "    actions[(~choose_x) & (y < 0)] = 1  # up\n",
    "\n",
    "    return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at one expert trajectory\n",
    "\n",
    "env_one = VectorPointNav2D(\n",
    "    n_envs=1,\n",
    "    max_steps=MAX_STEPS,\n",
    "    step_size=STEP_SIZE,\n",
    "    noise_std=NOISE_STD,\n",
    "    goal_radius=GOAL_RADIUS,\n",
    "    seed=SEED,\n",
    ")\n",
    "obs = env_one.reset()\n",
    "\n",
    "traj = [obs[0].copy()]\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "done = np.array([False])\n",
    "while not done[0]:\n",
    "    a = expert_policy(obs)[0]\n",
    "    obs, r, done, info = env_one.step(np.array([a]))\n",
    "    traj.append(obs[0].copy())\n",
    "    actions.append(int(a))\n",
    "    rewards.append(float(r[0]))\n",
    "\n",
    "traj = np.stack(traj)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=traj[:, 0], y=traj[:, 1], mode=\"lines+markers\", name=\"expert\"))\n",
    "fig.add_trace(go.Scatter(x=[0], y=[0], mode=\"markers\", marker=dict(size=12, symbol=\"x\"), name=\"goal\"))\n",
    "fig.update_layout(title=\"One expert trajectory (2D point → origin)\", xaxis_title=\"x\", yaxis_title=\"y\")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"steps:\", len(actions), \"episode_return:\", sum(rewards), \"success:\", info[\"success\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5ff83",
   "metadata": {},
   "source": [
    "## 3) Expert demonstrations dataset\n",
    "\n",
    "GAIL trains the discriminator on **expert** and **policy** \\((s,a)\\) samples.\n",
    "We’ll generate a dataset of expert state-action pairs by running the expert in the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47deb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expert_pairs(n_episodes: int, seed: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    env = VectorPointNav2D(\n",
    "        n_envs=1,\n",
    "        max_steps=MAX_STEPS,\n",
    "        step_size=STEP_SIZE,\n",
    "        noise_std=NOISE_STD,\n",
    "        goal_radius=GOAL_RADIUS,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    obs_list: list[np.ndarray] = []\n",
    "    act_list: list[int] = []\n",
    "\n",
    "    for _ in range(int(n_episodes)):\n",
    "        obs = env.reset()\n",
    "        done = np.array([False])\n",
    "        while not done[0]:\n",
    "            a = int(expert_policy(obs)[0])\n",
    "            obs_list.append(obs[0].copy())\n",
    "            act_list.append(a)\n",
    "            obs, r, done, info = env.step(np.array([a]))\n",
    "\n",
    "    expert_obs = np.stack(obs_list).astype(np.float32)\n",
    "    expert_acts = np.array(act_list, dtype=np.int64)\n",
    "    return expert_obs, expert_acts\n",
    "\n",
    "\n",
    "expert_obs, expert_acts = collect_expert_pairs(N_EXPERT_EPISODES, seed=SEED)\n",
    "print(\"expert_obs\", expert_obs.shape, \"expert_acts\", expert_acts.shape)\n",
    "\n",
    "fig = px.histogram(x=expert_acts, nbins=5, title=\"Expert action histogram\")\n",
    "fig.update_layout(xaxis_title=\"action (0 stay, 1 up, 2 down, 3 left, 4 right)\", yaxis_title=\"count\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23e808",
   "metadata": {},
   "source": [
    "## 4) Low-level PyTorch: policy/value network and discriminator\n",
    "\n",
    "We’ll use:\n",
    "\n",
    "- **Policy/value**: a small shared MLP with two heads\n",
    "  - policy head outputs categorical logits over 5 actions\n",
    "  - value head outputs \\(V_\\theta(s)\\)\n",
    "- **Discriminator**: an MLP over \\((s, \\text{one-hot}(a))\\) returning a **logit** \\(f_\\phi(s,a)\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feae54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(actions: torch.Tensor, n_actions: int) -> torch.Tensor:\n",
    "    return F.one_hot(actions.long(), num_classes=n_actions).float()\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden_sizes: tuple[int, ...] = (64, 64)):\n",
    "        super().__init__()\n",
    "        layers: list[nn.Module] = []\n",
    "        in_dim = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(in_dim, h), nn.Tanh()]\n",
    "            in_dim = h\n",
    "\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        self.pi = nn.Linear(in_dim, n_actions)\n",
    "        self.v = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.shared(obs)\n",
    "        logits = self.pi(x)\n",
    "        value = self.v(x).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "    def get_action_and_value(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        action: torch.Tensor | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        logp = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action, logp, entropy, value\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden_sizes: tuple[int, ...] = (128, 128)):\n",
    "        super().__init__()\n",
    "        in_dim = obs_dim + n_actions\n",
    "        layers: list[nn.Module] = []\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(in_dim, h), nn.Tanh()]\n",
    "            in_dim = h\n",
    "        layers += [nn.Linear(in_dim, 1)]\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([obs, one_hot(actions, self.n_actions)], dim=-1)\n",
    "        return self.net(x).squeeze(-1)  # logits\n",
    "\n",
    "\n",
    "policy = ActorCritic(obs_dim=2, n_actions=5).to(DEVICE)\n",
    "disc = Discriminator(obs_dim=2, n_actions=5).to(DEVICE)\n",
    "\n",
    "pi_opt = torch.optim.Adam(policy.parameters(), lr=PI_LR)\n",
    "d_opt = torch.optim.Adam(disc.parameters(), lr=D_LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20136a07",
   "metadata": {},
   "source": [
    "## 5) Rollouts, GAE, discriminator update, PPO update\n",
    "\n",
    "We’ll collect policy rollouts from a vectorized environment, then alternate:\n",
    "\n",
    "1. **Discriminator update(s)** using expert pairs and current policy pairs\n",
    "2. **Policy PPO update(s)** using discriminator-derived rewards\n",
    "\n",
    "We’ll use GAE(\\(\\gamma,\\lambda\\)) for advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env: VectorPointNav2D, policy: ActorCritic, n_steps: int) -> dict:\n",
    "    obs = env.reset()\n",
    "    n_envs = env.n_envs\n",
    "\n",
    "    obs_buf = []\n",
    "    act_buf = []\n",
    "    logp_buf = []\n",
    "    val_buf = []\n",
    "    done_buf = []\n",
    "    true_r_buf = []\n",
    "\n",
    "    ep_returns = np.zeros(n_envs, dtype=np.float32)\n",
    "    completed_returns: list[float] = []\n",
    "    completed_success: list[bool] = []\n",
    "\n",
    "    for _ in range(int(n_steps)):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            action, logp, entropy, value = policy.get_action_and_value(obs_t)\n",
    "\n",
    "        act_np = action.cpu().numpy()\n",
    "        next_obs, true_r, done, info = env.step(act_np)\n",
    "\n",
    "        obs_buf.append(obs.copy())\n",
    "        act_buf.append(act_np.copy())\n",
    "        logp_buf.append(logp.cpu().numpy())\n",
    "        val_buf.append(value.cpu().numpy())\n",
    "        done_buf.append(done.copy())\n",
    "        true_r_buf.append(true_r.copy())\n",
    "\n",
    "        ep_returns += true_r\n",
    "        for i in range(n_envs):\n",
    "            if done[i]:\n",
    "                completed_returns.append(float(ep_returns[i]))\n",
    "                completed_success.append(bool(info[\"success\"][i]))\n",
    "                ep_returns[i] = 0.0\n",
    "\n",
    "        env.reset_done(done)\n",
    "        obs = env.pos.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, last_values = policy.forward(torch.tensor(obs, dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "    return {\n",
    "        \"obs\": np.asarray(obs_buf, dtype=np.float32),\n",
    "        \"actions\": np.asarray(act_buf, dtype=np.int64),\n",
    "        \"logp\": np.asarray(logp_buf, dtype=np.float32),\n",
    "        \"values\": np.asarray(val_buf, dtype=np.float32),\n",
    "        \"dones\": np.asarray(done_buf, dtype=np.bool_),\n",
    "        \"true_rewards\": np.asarray(true_r_buf, dtype=np.float32),\n",
    "        \"last_values\": last_values.cpu().numpy().astype(np.float32),\n",
    "        \"completed_returns\": completed_returns,\n",
    "        \"completed_success\": completed_success,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_gae(\n",
    "    rewards: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    dones: np.ndarray,\n",
    "    last_values: np.ndarray,\n",
    "    gamma: float,\n",
    "    lam: float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    # GAE-Lambda. Shapes: rewards/values/dones are (T, N). last_values is (N,).\n",
    "    T, N = rewards.shape\n",
    "    adv = np.zeros((T, N), dtype=np.float32)\n",
    "\n",
    "    last_adv = np.zeros(N, dtype=np.float32)\n",
    "    next_values = last_values.astype(np.float32)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        mask = 1.0 - dones[t].astype(np.float32)\n",
    "        delta = rewards[t] + gamma * next_values * mask - values[t]\n",
    "        last_adv = delta + gamma * lam * mask * last_adv\n",
    "        adv[t] = last_adv\n",
    "        next_values = values[t]\n",
    "\n",
    "    returns = adv + values\n",
    "    return adv, returns\n",
    "\n",
    "\n",
    "def train_discriminator(\n",
    "    disc: Discriminator,\n",
    "    opt: torch.optim.Optimizer,\n",
    "    expert_obs: np.ndarray,\n",
    "    expert_acts: np.ndarray,\n",
    "    gen_obs: np.ndarray,\n",
    "    gen_acts: np.ndarray,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    ") -> float:\n",
    "    # BCE discriminator update. expert label=1, generator label=0.\n",
    "    disc.train()\n",
    "\n",
    "    n_gen = len(gen_obs)\n",
    "    n_exp = len(expert_obs)\n",
    "    n = min(n_gen, n_exp)\n",
    "\n",
    "    idx_g = np.random.randint(0, n_gen, size=n)\n",
    "    idx_e = np.random.randint(0, n_exp, size=n)\n",
    "\n",
    "    g_obs = torch.tensor(gen_obs[idx_g], dtype=torch.float32, device=DEVICE)\n",
    "    g_act = torch.tensor(gen_acts[idx_g], dtype=torch.int64, device=DEVICE)\n",
    "\n",
    "    e_obs = torch.tensor(expert_obs[idx_e], dtype=torch.float32, device=DEVICE)\n",
    "    e_act = torch.tensor(expert_acts[idx_e], dtype=torch.int64, device=DEVICE)\n",
    "\n",
    "    losses: list[float] = []\n",
    "    for _ in range(int(epochs)):\n",
    "        perm = torch.randperm(n, device=DEVICE)\n",
    "        for start in range(0, n, int(batch_size)):\n",
    "            mb = perm[start : start + int(batch_size)]\n",
    "\n",
    "            logits_g = disc(g_obs[mb], g_act[mb])\n",
    "            logits_e = disc(e_obs[mb], e_act[mb])\n",
    "\n",
    "            loss_g = F.binary_cross_entropy_with_logits(logits_g, torch.zeros_like(logits_g))\n",
    "            loss_e = F.binary_cross_entropy_with_logits(logits_e, torch.ones_like(logits_e))\n",
    "            loss = loss_g + loss_e\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "\n",
    "def ppo_update(\n",
    "    policy: ActorCritic,\n",
    "    opt: torch.optim.Optimizer,\n",
    "    obs: np.ndarray,\n",
    "    actions: np.ndarray,\n",
    "    old_logp: np.ndarray,\n",
    "    advantages: np.ndarray,\n",
    "    returns: np.ndarray,\n",
    "    clip_eps: float,\n",
    "    vf_coef: float,\n",
    "    ent_coef: float,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    ") -> dict:\n",
    "    policy.train()\n",
    "\n",
    "    n = len(obs)\n",
    "    obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    act_t = torch.tensor(actions, dtype=torch.int64, device=DEVICE)\n",
    "    old_logp_t = torch.tensor(old_logp, dtype=torch.float32, device=DEVICE)\n",
    "    adv_t = torch.tensor(advantages, dtype=torch.float32, device=DEVICE)\n",
    "    ret_t = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    adv_t = (adv_t - adv_t.mean()) / (adv_t.std() + 1e-8)\n",
    "\n",
    "    total_losses: list[float] = []\n",
    "    policy_losses: list[float] = []\n",
    "    value_losses: list[float] = []\n",
    "    entropies: list[float] = []\n",
    "    approx_kls: list[float] = []\n",
    "\n",
    "    for _ in range(int(epochs)):\n",
    "        perm = torch.randperm(n, device=DEVICE)\n",
    "        for start in range(0, n, int(batch_size)):\n",
    "            mb = perm[start : start + int(batch_size)]\n",
    "\n",
    "            _, logp, entropy, value = policy.get_action_and_value(obs_t[mb], act_t[mb])\n",
    "            ratio = torch.exp(logp - old_logp_t[mb])\n",
    "\n",
    "            pg1 = ratio * adv_t[mb]\n",
    "            pg2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv_t[mb]\n",
    "            policy_loss = -torch.mean(torch.minimum(pg1, pg2))\n",
    "\n",
    "            value_loss = F.mse_loss(value, ret_t[mb])\n",
    "            entropy_bonus = torch.mean(entropy)\n",
    "\n",
    "            loss = policy_loss + vf_coef * value_loss - ent_coef * entropy_bonus\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            approx_kl = torch.mean(old_logp_t[mb] - logp).detach().cpu().item()\n",
    "\n",
    "            total_losses.append(float(loss.detach().cpu()))\n",
    "            policy_losses.append(float(policy_loss.detach().cpu()))\n",
    "            value_losses.append(float(value_loss.detach().cpu()))\n",
    "            entropies.append(float(entropy_bonus.detach().cpu()))\n",
    "            approx_kls.append(float(approx_kl))\n",
    "\n",
    "    return {\n",
    "        \"loss\": float(np.mean(total_losses)),\n",
    "        \"policy_loss\": float(np.mean(policy_losses)),\n",
    "        \"value_loss\": float(np.mean(value_losses)),\n",
    "        \"entropy\": float(np.mean(entropies)),\n",
    "        \"approx_kl\": float(np.mean(approx_kls)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da534b99",
   "metadata": {},
   "source": [
    "## 6) Train GAIL (alternate D and PPO updates)\n",
    "\n",
    "We’ll track:\n",
    "\n",
    "- discriminator loss\n",
    "- PPO diagnostics (policy/value/entropy/KL)\n",
    "- episodic returns from the **true environment reward** (monitoring only)\n",
    "- evaluation return + success rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gail_reward_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
    "    # r = -log(1 - sigmoid(logits)) = softplus(logits)\n",
    "    return F.softplus(logits)\n",
    "\n",
    "\n",
    "def evaluate_policy(policy: ActorCritic, seed: int, n_episodes: int) -> dict:\n",
    "    env = VectorPointNav2D(\n",
    "        n_envs=1,\n",
    "        max_steps=MAX_STEPS,\n",
    "        step_size=STEP_SIZE,\n",
    "        noise_std=NOISE_STD,\n",
    "        goal_radius=GOAL_RADIUS,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    returns: list[float] = []\n",
    "    successes: list[bool] = []\n",
    "    steps: list[int] = []\n",
    "\n",
    "    for _ in range(int(n_episodes)):\n",
    "        obs = env.reset()\n",
    "        done = np.array([False])\n",
    "\n",
    "        ep_return = 0.0\n",
    "        ep_steps = 0\n",
    "        ep_success = False\n",
    "\n",
    "        while not done[0]:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits, _ = policy.forward(obs_t)\n",
    "                action = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            obs, r, done, info = env.step(action.cpu().numpy())\n",
    "            ep_return += float(r[0])\n",
    "            ep_steps += 1\n",
    "            ep_success = bool(info[\"success\"][0])\n",
    "\n",
    "        returns.append(ep_return)\n",
    "        successes.append(ep_success)\n",
    "        steps.append(ep_steps)\n",
    "\n",
    "    return {\n",
    "        \"return_mean\": float(np.mean(returns)),\n",
    "        \"return_std\": float(np.std(returns)),\n",
    "        \"success_rate\": float(np.mean(successes)),\n",
    "        \"steps_mean\": float(np.mean(steps)),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_expert(seed: int, n_episodes: int) -> dict:\n",
    "    env = VectorPointNav2D(\n",
    "        n_envs=1,\n",
    "        max_steps=MAX_STEPS,\n",
    "        step_size=STEP_SIZE,\n",
    "        noise_std=NOISE_STD,\n",
    "        goal_radius=GOAL_RADIUS,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    returns: list[float] = []\n",
    "    successes: list[bool] = []\n",
    "    steps: list[int] = []\n",
    "\n",
    "    for _ in range(int(n_episodes)):\n",
    "        obs = env.reset()\n",
    "        done = np.array([False])\n",
    "\n",
    "        ep_return = 0.0\n",
    "        ep_steps = 0\n",
    "        ep_success = False\n",
    "\n",
    "        while not done[0]:\n",
    "            a = int(expert_policy(obs)[0])\n",
    "            obs, r, done, info = env.step(np.array([a]))\n",
    "            ep_return += float(r[0])\n",
    "            ep_steps += 1\n",
    "            ep_success = bool(info[\"success\"][0])\n",
    "\n",
    "        returns.append(ep_return)\n",
    "        successes.append(ep_success)\n",
    "        steps.append(ep_steps)\n",
    "\n",
    "    return {\n",
    "        \"return_mean\": float(np.mean(returns)),\n",
    "        \"return_std\": float(np.std(returns)),\n",
    "        \"success_rate\": float(np.mean(successes)),\n",
    "        \"steps_mean\": float(np.mean(steps)),\n",
    "    }\n",
    "\n",
    "\n",
    "env = VectorPointNav2D(\n",
    "    n_envs=N_ENVS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    step_size=STEP_SIZE,\n",
    "    noise_std=NOISE_STD,\n",
    "    goal_radius=GOAL_RADIUS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Baseline: how good is the expert on this environment reward?\n",
    "expert_eval = evaluate_expert(seed=SEED + 123, n_episodes=EVAL_EPISODES)\n",
    "expert_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_loss_hist: list[float] = []\n",
    "ppo_loss_hist: list[float] = []\n",
    "ppo_policy_loss_hist: list[float] = []\n",
    "ppo_value_loss_hist: list[float] = []\n",
    "ppo_entropy_hist: list[float] = []\n",
    "ppo_kl_hist: list[float] = []\n",
    "\n",
    "train_ep_returns: list[float] = []\n",
    "train_ep_success: list[bool] = []\n",
    "train_ep_iter: list[int] = []\n",
    "\n",
    "eval_iters: list[int] = []\n",
    "eval_return_mean: list[float] = []\n",
    "eval_success_rate: list[float] = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for it in range(int(ITERATIONS)):\n",
    "    data = rollout(env, policy, n_steps=STEPS_PER_ITER)\n",
    "\n",
    "    # Flatten rollout buffers for discriminator/policy updates\n",
    "    obs = data[\"obs\"].reshape(-1, 2)\n",
    "    acts = data[\"actions\"].reshape(-1)\n",
    "    old_logp = data[\"logp\"].reshape(-1)\n",
    "\n",
    "    # 1) Discriminator update\n",
    "    dloss = train_discriminator(\n",
    "        disc=disc,\n",
    "        opt=d_opt,\n",
    "        expert_obs=expert_obs,\n",
    "        expert_acts=expert_acts,\n",
    "        gen_obs=obs,\n",
    "        gen_acts=acts,\n",
    "        epochs=D_EPOCHS,\n",
    "        batch_size=D_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # 2) Compute discriminator reward for the policy rollout\n",
    "    disc.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = disc(\n",
    "            torch.tensor(obs, dtype=torch.float32, device=DEVICE),\n",
    "            torch.tensor(acts, dtype=torch.int64, device=DEVICE),\n",
    "        )\n",
    "        gail_rewards = gail_reward_from_logits(logits).cpu().numpy().reshape(STEPS_PER_ITER, N_ENVS)\n",
    "\n",
    "    # 3) PPO update using GAE on the discriminator reward\n",
    "    adv, rets = compute_gae(\n",
    "        rewards=gail_rewards,\n",
    "        values=data[\"values\"],\n",
    "        dones=data[\"dones\"],\n",
    "        last_values=data[\"last_values\"],\n",
    "        gamma=GAMMA,\n",
    "        lam=LAMBDA_GAE,\n",
    "    )\n",
    "\n",
    "    ppo_stats = ppo_update(\n",
    "        policy=policy,\n",
    "        opt=pi_opt,\n",
    "        obs=obs,\n",
    "        actions=acts,\n",
    "        old_logp=old_logp,\n",
    "        advantages=adv.reshape(-1),\n",
    "        returns=rets.reshape(-1),\n",
    "        clip_eps=CLIP_EPS,\n",
    "        vf_coef=VF_COEF,\n",
    "        ent_coef=ENT_COEF,\n",
    "        epochs=PPO_EPOCHS,\n",
    "        batch_size=PPO_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    disc_loss_hist.append(dloss)\n",
    "    ppo_loss_hist.append(ppo_stats[\"loss\"])\n",
    "    ppo_policy_loss_hist.append(ppo_stats[\"policy_loss\"])\n",
    "    ppo_value_loss_hist.append(ppo_stats[\"value_loss\"])\n",
    "    ppo_entropy_hist.append(ppo_stats[\"entropy\"])\n",
    "    ppo_kl_hist.append(ppo_stats[\"approx_kl\"])\n",
    "\n",
    "    # Record true episodic returns completed during this iteration\n",
    "    for r, s in zip(data[\"completed_returns\"], data[\"completed_success\"]):\n",
    "        train_ep_returns.append(float(r))\n",
    "        train_ep_success.append(bool(s))\n",
    "        train_ep_iter.append(it)\n",
    "\n",
    "    # Evaluate periodically\n",
    "    if (it + 1) % int(EVAL_EVERY) == 0:\n",
    "        stats = evaluate_policy(policy, seed=SEED + 999 + it, n_episodes=EVAL_EPISODES)\n",
    "        eval_iters.append(it + 1)\n",
    "        eval_return_mean.append(stats[\"return_mean\"])\n",
    "        eval_success_rate.append(stats[\"success_rate\"])\n",
    "\n",
    "elapsed = time.time() - start\n",
    "elapsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef630b9",
   "metadata": {},
   "source": [
    "## 7) Plotly diagnostics\n",
    "\n",
    "Required plots:\n",
    "\n",
    "- discriminator loss\n",
    "- policy learning (evaluation return + success rate)\n",
    "- episodic rewards (environment return per episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator loss over iterations\n",
    "\n",
    "df_disc = pd.DataFrame({\n",
    "    \"iteration\": np.arange(1, len(disc_loss_hist) + 1),\n",
    "    \"disc_loss\": disc_loss_hist,\n",
    "})\n",
    "\n",
    "fig = px.line(df_disc, x=\"iteration\", y=\"disc_loss\", title=\"Discriminator loss\")\n",
    "fig.update_layout(xaxis_title=\"iteration\", yaxis_title=\"BCE loss\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304694e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy learning: evaluation return + success rate\n",
    "\n",
    "df_eval = pd.DataFrame({\n",
    "    \"iteration\": eval_iters,\n",
    "    \"eval_return_mean\": eval_return_mean,\n",
    "    \"eval_success_rate\": eval_success_rate,\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_eval[\"iteration\"], y=df_eval[\"eval_return_mean\"], mode=\"lines+markers\", name=\"eval return\"))\n",
    "fig.add_trace(go.Scatter(x=df_eval[\"iteration\"], y=df_eval[\"eval_success_rate\"], mode=\"lines+markers\", name=\"success rate\", yaxis=\"y2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Policy learning (evaluation)\",\n",
    "    xaxis=dict(title=\"iteration\"),\n",
    "    yaxis=dict(title=\"mean episodic return\"),\n",
    "    yaxis2=dict(title=\"success rate\", overlaying=\"y\", side=\"right\", range=[0, 1]),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"Expert baseline:\", expert_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9919863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodic rewards collected during training\n",
    "\n",
    "if len(train_ep_returns) == 0:\n",
    "    print(\"No completed episodes recorded (increase STEPS_PER_ITER or MAX_STEPS).\")\n",
    "else:\n",
    "    df_ep = pd.DataFrame({\n",
    "        \"episode\": np.arange(1, len(train_ep_returns) + 1),\n",
    "        \"return\": train_ep_returns,\n",
    "        \"success\": train_ep_success,\n",
    "        \"iteration\": train_ep_iter,\n",
    "    })\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df_ep,\n",
    "        x=\"episode\",\n",
    "        y=\"return\",\n",
    "        color=\"success\",\n",
    "        title=\"Episodic returns during training (true env reward)\",\n",
    "        opacity=0.6,\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"episode\", yaxis_title=\"episodic return\")\n",
    "    fig.show()\n",
    "\n",
    "    window = 25\n",
    "    if len(df_ep) >= window:\n",
    "        ma = df_ep[\"return\"].rolling(window=window).mean()\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df_ep[\"episode\"], y=df_ep[\"return\"], mode=\"markers\", name=\"return\", opacity=0.35))\n",
    "        fig.add_trace(go.Scatter(x=df_ep[\"episode\"], y=ma, mode=\"lines\", name=f\"moving avg (window={window})\"))\n",
    "        fig.update_layout(title=\"Episodic return with moving average\", xaxis_title=\"episode\", yaxis_title=\"return\")\n",
    "        fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60a2bf",
   "metadata": {},
   "source": [
    "## 8) Visual sanity check: expert vs learned trajectories\n",
    "\n",
    "A qualitative check: roll out the expert and the learned policy from the **same** start state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_single(policy: ActorCritic, seed: int, use_expert: bool) -> dict:\n",
    "    env = VectorPointNav2D(\n",
    "        n_envs=1,\n",
    "        max_steps=MAX_STEPS,\n",
    "        step_size=STEP_SIZE,\n",
    "        noise_std=NOISE_STD,\n",
    "        goal_radius=GOAL_RADIUS,\n",
    "        seed=seed,\n",
    "    )\n",
    "    obs = env.reset()\n",
    "    traj = [obs[0].copy()]\n",
    "    rewards = []\n",
    "\n",
    "    done = np.array([False])\n",
    "    while not done[0]:\n",
    "        if use_expert:\n",
    "            a = int(expert_policy(obs)[0])\n",
    "        else:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits, _ = policy.forward(obs_t)\n",
    "                a = int(torch.argmax(logits, dim=-1).cpu().item())\n",
    "\n",
    "        obs, r, done, info = env.step(np.array([a]))\n",
    "        traj.append(obs[0].copy())\n",
    "        rewards.append(float(r[0]))\n",
    "\n",
    "    return {\n",
    "        \"traj\": np.stack(traj),\n",
    "        \"return\": float(sum(rewards)),\n",
    "        \"success\": bool(info[\"success\"][0]),\n",
    "    }\n",
    "\n",
    "\n",
    "seed = SEED + 2025\n",
    "expert_roll = rollout_single(policy, seed=seed, use_expert=True)\n",
    "learned_roll = rollout_single(policy, seed=seed, use_expert=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=expert_roll[\"traj\"][:, 0], y=expert_roll[\"traj\"][:, 1], mode=\"lines+markers\", name=\"expert\"))\n",
    "fig.add_trace(go.Scatter(x=learned_roll[\"traj\"][:, 0], y=learned_roll[\"traj\"][:, 1], mode=\"lines+markers\", name=\"learned\"))\n",
    "fig.add_trace(go.Scatter(x=[0], y=[0], mode=\"markers\", marker=dict(size=12, symbol=\"x\"), name=\"goal\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Expert vs learned trajectory (same start)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"expert\", {k: expert_roll[k] for k in [\"return\", \"success\"]})\n",
    "print(\"learned\", {k: learned_roll[k] for k in [\"return\", \"success\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2762c9f",
   "metadata": {},
   "source": [
    "## 9) Stable-Baselines GAIL (implementation exists) + hyperparameters\n",
    "\n",
    "### Does Stable-Baselines implement GAIL?\n",
    "\n",
    "Yes: **Stable-Baselines (the TensorFlow-based library, not SB3)** ships a `GAIL` class. Upstream docs/source show:\n",
    "\n",
    "- `stable_baselines.GAIL` exists and is **TRPO-based** (inherits from `TRPO`)\n",
    "- it expects an `ExpertDataset`\n",
    "- it requires OpenMPI support (for the MPI-based TRPO implementation)\n",
    "\n",
    "Example from upstream docs (Pendulum):\n",
    "\n",
    "```python\n",
    "import gym\n",
    "\n",
    "from stable_baselines import GAIL, SAC\n",
    "from stable_baselines.gail import ExpertDataset, generate_expert_traj\n",
    "\n",
    "# Generate expert trajectories (train expert)\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "generate_expert_traj(model, 'expert_pendulum', n_timesteps=100, n_episodes=10)\n",
    "\n",
    "# Load the expert dataset\n",
    "dataset = ExpertDataset(expert_path='expert_pendulum.npz', traj_limitation=10, verbose=1)\n",
    "\n",
    "model = GAIL('MlpPolicy', 'Pendulum-v0', dataset, verbose=1)\n",
    "model.learn(total_timesteps=1000)\n",
    "model.save(\"gail_pendulum\")\n",
    "```\n",
    "\n",
    "### Hyperparameters (Stable-Baselines `GAIL`)\n",
    "\n",
    "From the Stable-Baselines `GAIL` docstring/source, the key knobs are:\n",
    "\n",
    "**TRPO / policy optimization (inherited)**\n",
    "\n",
    "- `gamma`: discount factor\n",
    "- `timesteps_per_batch`: rollout horizon per TRPO batch\n",
    "- `max_kl`: KL constraint threshold (trust-region size)\n",
    "- `cg_iters`: conjugate-gradient iterations (for TRPO step)\n",
    "- `lam`: GAE(\\(\\lambda\\))\n",
    "- `entcoeff`: entropy regularization coefficient\n",
    "- `cg_damping`: damping for conjugate gradient / Fisher-vector products\n",
    "- `vf_stepsize`: value function optimizer step size\n",
    "- `vf_iters`: value function training iterations per update\n",
    "- `hidden_size`: MLP hidden sizes for the policy/value network\n",
    "\n",
    "**GAIL-specific (how often/fast to train each player)**\n",
    "\n",
    "- `g_step`: number of generator/policy steps per epoch\n",
    "- `d_step`: number of discriminator steps per epoch\n",
    "- `d_stepsize`: discriminator/reward-giver learning rate\n",
    "- `hidden_size_adversary`: discriminator hidden size\n",
    "- `adversary_entcoeff`: entropy term used in the adversary loss (stabilization)\n",
    "\n",
    "Notes:\n",
    "\n",
    "- If `d_step` is too large (or `d_stepsize` too high), the discriminator can become too strong → sparse/unstable rewards.\n",
    "- If `g_step` is too large with a weak discriminator, the policy can overfit to a stale reward signal.\n",
    "\n",
    "### SB3 note\n",
    "\n",
    "Stable-Baselines3 does not ship GAIL in core; in practice, people often use the separate `imitation` library (HumanCompatibleAI) with SB3 policies.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Replace PPO with a TRPO-style update (harder, but closer to the original paper).\n",
    "2. Add reward normalization (as Stable-Baselines’ adversary optionally does) and see how curves change.\n",
    "3. Make the environment continuous-action and switch the policy to a Gaussian distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Ho & Ermon (2016), *Generative Adversarial Imitation Learning*: https://arxiv.org/abs/1606.03476\n",
    "- Stable-Baselines GAIL docs: https://stable-baselines.readthedocs.io/en/master/modules/gail.html\n",
    "- Stable-Baselines source (`stable_baselines/gail/model.py`): https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/gail/model.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}