{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ed1f05",
   "metadata": {},
   "source": [
    "# PPO1 (PPO-Clip) — low-level PyTorch implementation\n",
    "\n",
    "**Goal:** implement the classic *clipped surrogate objective* version of Proximal Policy Optimization (often referred to as **PPO1** in older codebases) using **plain PyTorch** (no RL libraries), and visualize:\n",
    "\n",
    "- policy probability ratios \\(r_t\\) and clipping behavior (Plotly)\n",
    "- learning curves and **reward per episode** (Plotly)\n",
    "\n",
    "This notebook is designed to be **offline-friendly** and runs on `CartPole-v1` (Gymnasium).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255399c",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. PPO1 objective: intuition + the clipped surrogate (LaTeX)\n",
    "2. A minimal PyTorch actor-critic\n",
    "3. Rollout collection + GAE(\\(\\gamma,\\lambda\\))\n",
    "4. PPO clipped update (multiple epochs + minibatches)\n",
    "5. Plotly visualizations: ratios, clipping, reward per episode\n",
    "6. Stable-Baselines PPO1 reference implementation (web research)\n",
    "7. Hyperparameters (what they do + tuning tips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b44a6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python + PyTorch\n",
    "- Gymnasium (`gymnasium`)\n",
    "- Plotly\n",
    "\n",
    "Everything is self-contained (no downloads).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e685bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"gymnasium:\", gym.__version__)\n",
    "print(\"plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ce331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reproducibility ---\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- Run configuration ---\n",
    "FAST_RUN = True  # set False for longer training\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "\n",
    "ROLLOUT_STEPS = 512 if FAST_RUN else 2048\n",
    "N_UPDATES = 40 if FAST_RUN else 200\n",
    "TOTAL_TIMESTEPS = N_UPDATES * ROLLOUT_STEPS\n",
    "\n",
    "UPDATE_EPOCHS = 4\n",
    "MINIBATCH_SIZE = 128\n",
    "\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "CLIP_EPS = 0.2\n",
    "LEARNING_RATE = 3e-4\n",
    "ADAM_EPS = 1e-5\n",
    "\n",
    "ENT_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# Extra logging\n",
    "LOG_EVERY_UPDATES = 1\n",
    "\n",
    "# Device (suppress noisy CUDA init warnings in restricted environments)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message='CUDA initialization:.*')\n",
    "\n",
    "cuda_ok = bool(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if cuda_ok else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"updates:\", N_UPDATES)\n",
    "print(\"total_timesteps:\", TOTAL_TIMESTEPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3396d",
   "metadata": {},
   "source": [
    "## 1) PPO1 / PPO-Clip objective (clipped surrogate)\n",
    "\n",
    "PPO maintains a *current* policy \\(\\pi_{\\theta}\\) and a *behavior* (old) policy \\(\\pi_{\\theta_{\\mathrm{old}}}\\) that generated a batch of data.\n",
    "\n",
    "Define the probability ratio:\n",
    "\n",
    "\\[\n",
    "r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t\\mid s_t)}{\\pi_{\\theta_{\\mathrm{old}}}(a_t\\mid s_t)}\n",
    "\\]\n",
    "\n",
    "Let \\(A_t\\) be an advantage estimate (commonly **GAE**). The clipped surrogate objective is:\n",
    "\n",
    "\\[\n",
    "L^{\\mathrm{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\Big( r_t(\\theta)A_t,\\; \\operatorname{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\,A_t \\Big)\\right]\n",
    "\\]\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- If \\(A_t > 0\\): we *want* \\(\\pi_\\theta\\) to increase probability of \\(a_t\\), but we **cap** the improvement when \\(r_t\\) exceeds \\(1+\\epsilon\\).\n",
    "- If \\(A_t < 0\\): we *want* \\(\\pi_\\theta\\) to decrease probability of \\(a_t\\), but we **cap** the degradation when \\(r_t\\) falls below \\(1-\\epsilon\\).\n",
    "\n",
    "In code we typically *minimize* the negative objective: `policy_loss = -mean(min(...))`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67801cf3",
   "metadata": {},
   "source": [
    "## 2) Environment\n",
    "\n",
    "We use `CartPole-v1` (discrete actions, low dimensional state). PPO also works for continuous actions; the PPO1 clipping logic is the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_ID)\n",
    "env.action_space.seed(SEED)\n",
    "\n",
    "obs_dim = int(np.prod(env.observation_space.shape))\n",
    "assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(\"obs_dim:\", obs_dim)\n",
    "print(\"action_dim:\", action_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457431b",
   "metadata": {},
   "source": [
    "## 3) Low-level PyTorch actor-critic\n",
    "\n",
    "We implement:\n",
    "\n",
    "- **actor**: outputs logits for a categorical action distribution\n",
    "- **critic**: outputs state-value \\(V(s)\\)\n",
    "\n",
    "No helper RL libraries; just `torch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6482259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden: int = 64):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(hidden, action_dim)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "        # Orthogonal init is common for PPO\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        nn.init.orthogonal_(self.policy_head.weight, gain=0.01)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor):\n",
    "        x = self.backbone(obs)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, obs: torch.Tensor):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action, log_prob, entropy, value\n",
    "\n",
    "    def evaluate_actions(self, obs: torch.Tensor, actions: torch.Tensor):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_prob = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_prob, entropy, value\n",
    "\n",
    "\n",
    "agent = ActorCritic(obs_dim, action_dim).to(device)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=ADAM_EPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898370b",
   "metadata": {},
   "source": [
    "## 4) Rollouts + GAE\n",
    "\n",
    "We collect an on-policy rollout of length `ROLLOUT_STEPS`, then compute:\n",
    "\n",
    "- advantages \\(A_t\\) via **Generalized Advantage Estimation** (GAE)\n",
    "- returns \\(R_t = A_t + V(s_t)\\)\n",
    "\n",
    "Finally we do multiple epochs of minibatch optimization on the same rollout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e04693",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Rollout:\n",
    "    obs: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    log_probs: torch.Tensor\n",
    "    values: torch.Tensor\n",
    "    rewards: torch.Tensor\n",
    "    dones: torch.Tensor\n",
    "    advantages: torch.Tensor\n",
    "    returns: torch.Tensor\n",
    "\n",
    "\n",
    "def compute_gae(\n",
    "    rewards: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    dones: np.ndarray,\n",
    "    last_value: float,\n",
    "    *,\n",
    "    gamma: float,\n",
    "    lam: float,\n",
    "):\n",
    "    \"\"\"GAE for a single-environment rollout.\"\"\"\n",
    "    T = len(rewards)\n",
    "    adv = np.zeros(T, dtype=np.float32)\n",
    "    gae = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        next_nonterminal = 1.0 - float(dones[t])\n",
    "        next_value = last_value if t == T - 1 else values[t + 1]\n",
    "        delta = rewards[t] + gamma * next_value * next_nonterminal - values[t]\n",
    "        gae = delta + gamma * lam * next_nonterminal * gae\n",
    "        adv[t] = gae\n",
    "    ret = adv + values\n",
    "    return adv, ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollout(env, agent: ActorCritic, rollout_steps: int, obs: np.ndarray):\n",
    "    obs_list = []\n",
    "    action_list = []\n",
    "    logp_list = []\n",
    "    value_list = []\n",
    "    reward_list = []\n",
    "    done_list = []\n",
    "\n",
    "    episode_returns = []\n",
    "    ep_return = 0.0\n",
    "\n",
    "    for _ in range(rollout_steps):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        action, logp, entropy, value = agent.act(obs_tensor)\n",
    "\n",
    "        action_item = int(action.item())\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action_item)\n",
    "        done = bool(terminated or truncated)\n",
    "\n",
    "        obs_list.append(obs)\n",
    "        action_list.append(action_item)\n",
    "        logp_list.append(float(logp.item()))\n",
    "        value_list.append(float(value.item()))\n",
    "        reward_list.append(float(reward))\n",
    "        done_list.append(done)\n",
    "\n",
    "        ep_return += float(reward)\n",
    "\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            episode_returns.append(ep_return)\n",
    "            ep_return = 0.0\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    # Bootstrap value at the end of rollout\n",
    "    with torch.no_grad():\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        _, last_value = agent.forward(obs_tensor)\n",
    "        last_value = float(last_value.item())\n",
    "\n",
    "    obs_arr = np.asarray(obs_list, dtype=np.float32)\n",
    "    actions_arr = np.asarray(action_list, dtype=np.int64)\n",
    "    logp_arr = np.asarray(logp_list, dtype=np.float32)\n",
    "    values_arr = np.asarray(value_list, dtype=np.float32)\n",
    "    rewards_arr = np.asarray(reward_list, dtype=np.float32)\n",
    "    dones_arr = np.asarray(done_list, dtype=np.bool_)\n",
    "\n",
    "    adv_arr, ret_arr = compute_gae(\n",
    "        rewards_arr,\n",
    "        values_arr,\n",
    "        dones_arr,\n",
    "        last_value,\n",
    "        gamma=GAMMA,\n",
    "        lam=GAE_LAMBDA,\n",
    "    )\n",
    "\n",
    "    # Advantage normalization is a common PPO trick\n",
    "    adv_arr = (adv_arr - adv_arr.mean()) / (adv_arr.std() + 1e-8)\n",
    "\n",
    "    rollout = Rollout(\n",
    "        obs=torch.tensor(obs_arr, dtype=torch.float32, device=device),\n",
    "        actions=torch.tensor(actions_arr, dtype=torch.int64, device=device),\n",
    "        log_probs=torch.tensor(logp_arr, dtype=torch.float32, device=device),\n",
    "        values=torch.tensor(values_arr, dtype=torch.float32, device=device),\n",
    "        rewards=torch.tensor(rewards_arr, dtype=torch.float32, device=device),\n",
    "        dones=torch.tensor(dones_arr.astype(np.float32), dtype=torch.float32, device=device),\n",
    "        advantages=torch.tensor(adv_arr, dtype=torch.float32, device=device),\n",
    "        returns=torch.tensor(ret_arr, dtype=torch.float32, device=device),\n",
    "    )\n",
    "\n",
    "    return rollout, episode_returns, obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b1e55",
   "metadata": {},
   "source": [
    "## 5) PPO1 update step\n",
    "\n",
    "For each rollout batch we optimize the clipped surrogate objective over several epochs/minibatches.\n",
    "\n",
    "We also log ratio statistics so we can visualize clipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38348156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(agent: ActorCritic, optimizer: torch.optim.Optimizer, rollout: Rollout):\n",
    "    batch_size = rollout.obs.shape[0]\n",
    "    b_inds = np.arange(batch_size)\n",
    "\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    clip_fracs = []\n",
    "    approx_kls = []\n",
    "\n",
    "    for _ in range(UPDATE_EPOCHS):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, MINIBATCH_SIZE):\n",
    "            end = start + MINIBATCH_SIZE\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            obs_b = rollout.obs[mb_inds]\n",
    "            actions_b = rollout.actions[mb_inds]\n",
    "            old_logp_b = rollout.log_probs[mb_inds]\n",
    "            adv_b = rollout.advantages[mb_inds]\n",
    "            ret_b = rollout.returns[mb_inds]\n",
    "\n",
    "            new_logp, entropy, value = agent.evaluate_actions(obs_b, actions_b)\n",
    "\n",
    "            log_ratio = new_logp - old_logp_b\n",
    "            ratio = log_ratio.exp()\n",
    "\n",
    "            # PPO clipped surrogate\n",
    "            unclipped = ratio * adv_b\n",
    "            clipped = ratio.clamp(1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * adv_b\n",
    "            policy_loss = -torch.mean(torch.minimum(unclipped, clipped))\n",
    "\n",
    "            value_loss = F.mse_loss(value, ret_b)\n",
    "            entropy_mean = torch.mean(entropy)\n",
    "\n",
    "            loss = policy_loss + VF_COEF * value_loss - ENT_COEF * entropy_mean\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Diagnostics\n",
    "            with torch.no_grad():\n",
    "                approx_kl = torch.mean(old_logp_b - new_logp).item()\n",
    "                clip_frac = torch.mean((torch.abs(ratio - 1.0) > CLIP_EPS).float()).item()\n",
    "\n",
    "            policy_losses.append(policy_loss.item())\n",
    "            value_losses.append(value_loss.item())\n",
    "            entropies.append(entropy_mean.item())\n",
    "            clip_fracs.append(clip_frac)\n",
    "            approx_kls.append(approx_kl)\n",
    "\n",
    "    return {\n",
    "        \"policy_loss\": float(np.mean(policy_losses)),\n",
    "        \"value_loss\": float(np.mean(value_losses)),\n",
    "        \"entropy\": float(np.mean(entropies)),\n",
    "        \"clip_frac\": float(np.mean(clip_fracs)),\n",
    "        \"approx_kl\": float(np.mean(approx_kls)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff0b53",
   "metadata": {},
   "source": [
    "## 6) Train PPO1 on CartPole\n",
    "\n",
    "We train for `TOTAL_TIMESTEPS` and record:\n",
    "\n",
    "- reward per episode\n",
    "- PPO diagnostics (losses, clip fraction, KL)\n",
    "- a final batch of ratios/advantages for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "logs = []\n",
    "\n",
    "last_ratio_snapshot = None\n",
    "last_adv_snapshot = None\n",
    "last_clip_active_snapshot = None\n",
    "\n",
    "obs, _ = env.reset(seed=SEED)\n",
    "\n",
    "for update in range(1, N_UPDATES + 1):\n",
    "    rollout, ep_returns, obs = collect_rollout(env, agent, ROLLOUT_STEPS, obs)\n",
    "    episode_rewards.extend(ep_returns)\n",
    "\n",
    "    metrics = ppo_update(agent, optimizer, rollout)\n",
    "\n",
    "    # Capture ratio/adv snapshots (after the update) for visualization\n",
    "    with torch.no_grad():\n",
    "        new_logp, _, _ = agent.evaluate_actions(rollout.obs, rollout.actions)\n",
    "        ratio = (new_logp - rollout.log_probs).exp()\n",
    "        adv = rollout.advantages\n",
    "        clip_active = ((adv >= 0) & (ratio > 1.0 + CLIP_EPS)) | (\n",
    "            (adv < 0) & (ratio < 1.0 - CLIP_EPS)\n",
    "        )\n",
    "\n",
    "        last_ratio_snapshot = ratio.detach().cpu().numpy()\n",
    "        last_adv_snapshot = adv.detach().cpu().numpy()\n",
    "        last_clip_active_snapshot = clip_active.detach().cpu().numpy().astype(bool)\n",
    "\n",
    "    logs.append({\"update\": update, \"timesteps\": update * ROLLOUT_STEPS, **metrics, \"episodes\": len(episode_rewards)})\n",
    "\n",
    "    if update % LOG_EVERY_UPDATES == 0:\n",
    "        recent = episode_rewards[-10:]\n",
    "        recent_mean = float(np.mean(recent)) if recent else float(\"nan\")\n",
    "        print(\n",
    "            f\"update {update:>3}/{N_UPDATES} | \"\n",
    "            f\"episodes={len(episode_rewards):>4} | \"\n",
    "            f\"recent_reward_mean(10)={recent_mean:>7.2f} | \"\n",
    "            f\"clip_frac={metrics['clip_frac']:.3f} | \"\n",
    "            f\"approx_kl={metrics['approx_kl']:.4f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs = pd.DataFrame(logs)\n",
    "df_logs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e47cc4",
   "metadata": {},
   "source": [
    "## 7) Plotly: reward per episode (learning curve)\n",
    "\n",
    "This is the most direct signal for whether the policy is improving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e40bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ep = pd.DataFrame({\"episode\": np.arange(len(episode_rewards)), \"reward\": episode_rewards})\n",
    "\n",
    "window = 20\n",
    "if len(df_ep) >= window:\n",
    "    df_ep[\"reward_ma\"] = df_ep[\"reward\"].rolling(window).mean()\n",
    "\n",
    "fig = px.line(df_ep, x=\"episode\", y=\"reward\", title=\"CartPole reward per episode\")\n",
    "if \"reward_ma\" in df_ep.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_ep[\"episode\"], y=df_ep[\"reward_ma\"], name=f\"MA({window})\")\n",
    "    )\n",
    "fig.update_layout(xaxis_title=\"Episode\", yaxis_title=\"Total reward\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f4b62",
   "metadata": {},
   "source": [
    "## 8) Plotly: PPO diagnostics over updates\n",
    "\n",
    "We visualize clipping behavior and losses over training updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_logs[\"update\"], y=df_logs[\"clip_frac\"], name=\"clip_frac\"))\n",
    "fig.add_trace(go.Scatter(x=df_logs[\"update\"], y=df_logs[\"approx_kl\"], name=\"approx_kl\"))\n",
    "fig.update_layout(title=\"PPO diagnostics\", xaxis_title=\"Update\", yaxis_title=\"Value\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_logs[\"update\"], y=df_logs[\"policy_loss\"], name=\"policy_loss\"))\n",
    "fig.add_trace(go.Scatter(x=df_logs[\"update\"], y=df_logs[\"value_loss\"], name=\"value_loss\"))\n",
    "fig.add_trace(go.Scatter(x=df_logs[\"update\"], y=df_logs[\"entropy\"], name=\"entropy\"))\n",
    "fig.update_layout(title=\"Losses over updates\", xaxis_title=\"Update\", yaxis_title=\"Loss / entropy\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7244c7",
   "metadata": {},
   "source": [
    "## 9) Plotly: policy ratios \\(r_t\\) and clipping\n",
    "\n",
    "Below we plot the distribution of \\(r_t\\) and highlight where clipping is active.\n",
    "\n",
    "- The histogram should concentrate near 1.0.\n",
    "- As training progresses, some mass moves outside \\([1-\\epsilon, 1+\\epsilon]\\), but PPO discourages large deviations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = last_ratio_snapshot\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=ratios, nbinsx=60, name=\"r_t\"))\n",
    "fig.add_vline(x=1.0 - CLIP_EPS, line_dash=\"dash\", line_color=\"orange\")\n",
    "fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.add_vline(x=1.0 + CLIP_EPS, line_dash=\"dash\", line_color=\"orange\")\n",
    "fig.update_layout(\n",
    "    title=\"Policy ratio distribution (last rollout)\",\n",
    "    xaxis_title=\"r_t = pi_new(a|s) / pi_old(a|s)\",\n",
    "    yaxis_title=\"Count\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratio = pd.DataFrame(\n",
    "    {\n",
    "        \"ratio\": last_ratio_snapshot,\n",
    "        \"advantage\": last_adv_snapshot,\n",
    "        \"clip_active\": last_clip_active_snapshot,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_ratio,\n",
    "    x=\"ratio\",\n",
    "    y=\"advantage\",\n",
    "    color=\"clip_active\",\n",
    "    title=\"Where clipping is active (last rollout)\",\n",
    "    labels={\"ratio\": \"r_t\", \"advantage\": \"A_t\"},\n",
    ")\n",
    "fig.add_vline(x=1.0 - CLIP_EPS, line_dash=\"dash\", line_color=\"orange\")\n",
    "fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.add_vline(x=1.0 + CLIP_EPS, line_dash=\"dash\", line_color=\"orange\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03cd8c8",
   "metadata": {},
   "source": [
    "## 10) Stable-Baselines PPO1 (web research)\n",
    "\n",
    "A Stable-Baselines implementation of **PPO1** exists (legacy TensorFlow 1.x codebase):\n",
    "\n",
    "- Repo: https://github.com/hill-a/stable-baselines\n",
    "- PPO1 package: https://github.com/hill-a/stable-baselines/tree/master/stable_baselines/ppo1\n",
    "- Main file: https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/ppo1/pposgd_simple.py\n",
    "  - Exposes `class PPO1(...)` (imported by `stable_baselines/ppo1/__init__.py`)\n",
    "\n",
    "The original OpenAI Baselines PPO implementation is also available:\n",
    "\n",
    "- https://github.com/openai/baselines/tree/master/baselines/ppo1\n",
    "\n",
    "Example usage (not run here):\n",
    "\n",
    "```python\n",
    "from stable_baselines import PPO1\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO1(\"MlpPolicy\", env, clip_param=0.2, timesteps_per_actorbatch=2048)\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "```\n",
    "\n",
    "Note: Stable-Baselines is archived/legacy and uses TF1/MPI; Stable-Baselines3 is PyTorch and offers `PPO` (conceptually closer to PPO2-style implementations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f48ffb",
   "metadata": {},
   "source": [
    "## 11) Stable-Baselines `PPO1` hyperparameters (explained)\n",
    "\n",
    "Stable-Baselines `PPO1` (legacy TensorFlow/MPI) exposes the following constructor signature (from `stable_baselines/ppo1/pposgd_simple.py`):\n",
    "\n",
    "```python\n",
    "PPO1(\n",
    "    policy,\n",
    "    env,\n",
    "    gamma=0.99,\n",
    "    timesteps_per_actorbatch=256,\n",
    "    clip_param=0.2,\n",
    "    entcoeff=0.01,\n",
    "    optim_epochs=4,\n",
    "    optim_stepsize=1e-3,\n",
    "    optim_batchsize=64,\n",
    "    lam=0.95,\n",
    "    adam_epsilon=1e-5,\n",
    "    schedule='linear',\n",
    "    verbose=0,\n",
    "    tensorboard_log=None,\n",
    "    _init_setup_model=True,\n",
    "    policy_kwargs=None,\n",
    "    full_tensorboard_log=False,\n",
    "    seed=None,\n",
    "    n_cpu_tf_sess=1,\n",
    ")\n",
    "```\n",
    "\n",
    "### What each hyperparameter does\n",
    "\n",
    "- `policy`: policy class (or registered string) like `MlpPolicy`, `CnnPolicy`, etc.\n",
    "- `env`: Gym env instance or an env id string (e.g. `'CartPole-v1'`).\n",
    "- `gamma`: discount factor $\\gamma$.\n",
    "- `timesteps_per_actorbatch`: number of environment steps collected per update **per actor** (batch size).\n",
    "- `clip_param`: PPO clip parameter $\\epsilon$.\n",
    "- `entcoeff`: entropy coefficient (larger → more exploration pressure).\n",
    "- `optim_epochs`: number of epochs over the on-policy batch per update.\n",
    "- `optim_stepsize`: optimizer step size (learning rate), optionally controlled by `schedule`.\n",
    "- `optim_batchsize`: minibatch size.\n",
    "- `lam`: GAE($\\lambda$) parameter.\n",
    "- `adam_epsilon`: Adam epsilon for numerical stability.\n",
    "- `schedule`: learning-rate schedule type (e.g. `'linear'`, `'constant'`, ...).\n",
    "\n",
    "### Mapping to this notebook\n",
    "\n",
    "- SB `timesteps_per_actorbatch` → this notebook’s `ROLLOUT_STEPS`\n",
    "- SB `clip_param` → `CLIP_EPS`\n",
    "- SB `entcoeff` → `ENT_COEF`\n",
    "- SB `optim_epochs` → `UPDATE_EPOCHS`\n",
    "- SB `optim_stepsize` → `LEARNING_RATE`\n",
    "- SB `optim_batchsize` → `MINIBATCH_SIZE`\n",
    "- SB `gamma` → `GAMMA`\n",
    "- SB `lam` → `GAE_LAMBDA`\n",
    "- SB `adam_epsilon` → `ADAM_EPS`\n",
    "- SB `schedule` → not implemented here (easy extension: linearly decay `LEARNING_RATE` over updates)\n",
    "\n",
    "### Practical tuning hints\n",
    "\n",
    "- If **reward collapses**: reduce `LEARNING_RATE`, reduce `UPDATE_EPOCHS`, or reduce `CLIP_EPS`.\n",
    "- If **learning is slow**: increase `ROLLOUT_STEPS`, increase `UPDATE_EPOCHS`, or slightly increase `LEARNING_RATE`.\n",
    "- Watch **`approx_kl`** and **`clip_frac`**: sustained high values mean policy updates are too aggressive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2976288",
   "metadata": {},
   "source": [
    "## Pitfalls + exercises\n",
    "\n",
    "- If training is unstable: lower `LEARNING_RATE`, check advantage normalization, and verify the done/bootstrapping logic.\n",
    "- If `clip_frac` is near 0.0: updates may be too small (try higher LR or more epochs).\n",
    "- If `clip_frac` is very high: updates are too aggressive (try smaller LR or smaller `CLIP_EPS`).\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Add an **entropy bonus** (`ENT_COEF > 0`) and compare learning curves.\n",
    "2. Implement **value function clipping** (as in some PPO variants) and compare critic stability.\n",
    "3. Switch to a continuous-action env (e.g., Pendulum) using a Gaussian policy.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Schulman et al., *Proximal Policy Optimization Algorithms* (2017): https://arxiv.org/abs/1707.06347\n",
    "- Stable-Baselines PPO1 source (TF1): https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/ppo1/pposgd_simple.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}