{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization 2 (PPO2) — from scratch in PyTorch\n",
    "\n",
    "This notebook builds a **low-level** PPO2 implementation in **PyTorch** and uses it to train an agent on a classic control environment.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- derive the PPO2 clipped objective and connect it to a trust-region intuition\n",
    "- implement PPO2 (rollout → GAE → multi-epoch mini-batch updates) in **raw PyTorch**\n",
    "- understand *exactly* how PPO2 differs from PPO1 (both in the paper and in Stable-Baselines naming)\n",
    "- plot **episodic rewards** and training diagnostics with Plotly\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- comfortable with gradients and backprop\n",
    "- basic RL notation: policy $\\pi_\\theta(a\\mid s)$, returns, value function $V_\\phi(s)$\n",
    "- packages: `torch`, `gymnasium` (or `gym`), `numpy`, `plotly`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Independent, Normal\n",
    "\n",
    "# Gymnasium first (new API), fallback to Gym (old API)\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except Exception:  # pragma: no cover\n",
    "    import gym  # type: ignore\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) The RL objective (notation)\n",
    "\n",
    "We’ll use the standard episodic discounted-return objective:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\Big[\\sum_{t=0}^{T-1} \\gamma^t r_t\\Big]\n",
    "$$\n",
    "\n",
    "- $\\tau = (s_0, a_0, r_0, s_1, \\dots)$ is a trajectory sampled by following the policy.\n",
    "- $\\gamma \\in (0, 1]$ is the discount factor.\n",
    "\n",
    "Two key helper objects:\n",
    "\n",
    "- **Value function**: $V_\\phi(s) \\approx \\mathbb{E}[\\sum_{k\\ge 0} \\gamma^k r_{t+k} \\mid s_t=s]$\n",
    "- **Advantage**: $A_t = Q(s_t, a_t) - V(s_t)$ — “how much better was this action than average?”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Policy gradients in one equation\n",
    "\n",
    "The policy-gradient theorem motivates the surrogate objective:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "= \\mathbb{E}_t\\big[\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\, A_t\\big]\n",
    "$$\n",
    "\n",
    "In practice we:\n",
    "\n",
    "1) sample data with an **old** policy $\\pi_{\\theta_{\\text{old}}}$\n",
    "\n",
    "2) estimate advantages $\\hat{A}_t$ (often via **GAE**)\n",
    "\n",
    "3) update the policy using mini-batch SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Why PPO exists: “big steps” break policy gradients\n",
    "\n",
    "A vanilla policy-gradient update can change the policy too much.\n",
    "\n",
    "PPO controls this by comparing the new policy to the old policy using the **probability ratio**:\n",
    "\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}\n",
    "$$\n",
    "\n",
    "If $r_t(\\theta)=1$ the new policy agrees with the old policy on that sampled action.\n",
    "\n",
    "The classic importance-sampled surrogate (CPI) is:\n",
    "\n",
    "$$\n",
    "L^{\\text{CPI}}(\\theta) = \\mathbb{E}_t\\big[r_t(\\theta)\\,\\hat{A}_t\\big]\n",
    "$$\n",
    "\n",
    "The problem: maximizing this can push $r_t$ to extreme values — effectively taking a **too-large** policy update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) PPO1 vs PPO2 (be precise about naming)\n",
    "\n",
    "People use “PPO1” vs “PPO2” in **two different ways**:\n",
    "\n",
    "### A) In the PPO paper (algorithmic variants)\n",
    "\n",
    "- **PPO-Penalty**: adds a KL penalty $\\beta\\,\\mathrm{KL}(\\pi_{\\text{old}}\\,\\|\\,\\pi_\\theta)$ and adapts $\\beta$.\n",
    "- **PPO-Clip**: uses a clipped surrogate objective (no explicit KL penalty term).\n",
    "\n",
    "A common PPO-Penalty surrogate is:\n",
    "\n",
    "$$\n",
    "L^{\\text{KLPEN}}(\\theta) = \\mathbb{E}_t\\Big[r_t(\\theta)\\,\\hat{A}_t - \\beta\\,\\mathrm{KL}\\big(\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid s_t)\\,\\|\\,\\pi_{\\theta}(\\cdot\\mid s_t)\\big)\\Big]\n",
    "$$\n",
    "\n",
    "with $\\beta$ tuned (often adaptively) to keep the KL near a target. PPO-Clip instead bakes the “keep it close” constraint into the objective via clipping.\n",
    "\n",
    "Many blogs call these “PPO1” (penalty) and “PPO2” (clip). When this notebook says **PPO2**, it means **PPO-Clip**.\n",
    "\n",
    "### B) In OpenAI Baselines / Stable-Baselines (implementation families)\n",
    "\n",
    "Stable-Baselines historically exposes **two codebases**:\n",
    "\n",
    "- `PPO1`: an older MPI-oriented implementation (requires `mpi4py`), with different batching and optimizer plumbing.\n",
    "- `PPO2`: a newer implementation that supports vectorized envs and (optionally) **value-function clipping** (`cliprange_vf`).\n",
    "\n",
    "**Important nuance**: Stable-Baselines `PPO1` also uses the clipped surrogate; the “1 vs 2” there is mostly *engineering*, not the core objective.\n",
    "\n",
    "Concretely, in Stable-Baselines:\n",
    "\n",
    "- `PPO1` is documented as an “MPI version”, with hyperparameters like `timesteps_per_actorbatch`, `optim_stepsize`, `optim_batchsize`, and a learning-rate `schedule`.\n",
    "- `PPO2` is documented as a “GPU version”, with hyperparameters like `n_steps` (per env), `nminibatches`, `noptepochs`, and the extra `cliprange_vf` option for value clipping.\n",
    "\n",
    "If you’re comparing results across implementations, these differences (batch construction + optimizer details + value clipping) can matter even when the high-level PPO objective looks similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) PPO2 clipped objective (the main idea)\n",
    "\n",
    "PPO2 replaces the CPI surrogate with the **clipped surrogate**:\n",
    "\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta)\n",
    "= \\mathbb{E}_t\\Big[\\min\\big(r_t(\\theta)\\,\\hat{A}_t,\\; \\mathrm{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\,\\hat{A}_t\\big)\\Big]\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- If $\\hat{A}_t > 0$ (action better than baseline), we don’t want $r_t$ to grow far above $1+\\epsilon$.\n",
    "- If $\\hat{A}_t < 0$ (action worse than baseline), we don’t want $r_t$ to shrink far below $1-\\epsilon$.\n",
    "\n",
    "So PPO2 constrains the *effective* improvement you can get from any single sample.\n",
    "\n",
    "### Full loss (actor + critic + entropy)\n",
    "\n",
    "In practice we minimize the negative surrogate plus a value loss and an entropy bonus:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi) =\n",
    "-L^{\\text{CLIP}}(\\theta)\n",
    "+ c_v\\,\\mathbb{E}_t[(V_\\phi(s_t) - \\hat{R}_t)^2]\n",
    "- c_e\\,\\mathbb{E}_t[\\mathcal{H}(\\pi_\\theta(\\cdot\\mid s_t))]\n",
    "$$\n",
    "\n",
    "where $\\hat{R}_t$ are “return targets” (often $\\hat{A}_t + V(s_t)$).\n",
    "\n",
    "### Value function clipping (SB/OpenAI variant)\n",
    "\n",
    "Stable-Baselines `PPO2` optionally clips value updates (not in the original PPO paper):\n",
    "\n",
    "$$\n",
    "V^{\\text{clip}}(s_t) = V_{\\text{old}}(s_t) + \\mathrm{clip}(V(s_t)-V_{\\text{old}}(s_t), -\\epsilon_v, \\epsilon_v)\n",
    "$$\n",
    "\n",
    "and uses the max of the unclipped/clipped squared error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual intuition: how clipping changes the surrogate\n",
    "eps = 0.2\n",
    "ratios = np.linspace(0.0, 2.0, 600)\n",
    "\n",
    "A_pos = 1.0\n",
    "A_neg = -1.0\n",
    "\n",
    "def clipped_surrogate(r, A, eps):\n",
    "    r_clipped = np.clip(r, 1.0 - eps, 1.0 + eps)\n",
    "    return np.minimum(r * A, r_clipped * A)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        'Surrogate term when $A_t > 0$',\n",
    "        'Surrogate term when $A_t < 0$',\n",
    "    ),\n",
    ")\n",
    "\n",
    "for col, A in [(1, A_pos), (2, A_neg)]:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ratios, y=ratios * A, name='CPI: $rA$', line=dict(width=2)),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=ratios,\n",
    "            y=clipped_surrogate(ratios, A, eps),\n",
    "            name='PPO2: $\\min(rA, \\mathrm{clip}(r)A)$',\n",
    "            line=dict(width=3),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_vline(x=1.0 - eps, line=dict(color='gray', dash='dot'), row=1, col=col)\n",
    "    fig.add_vline(x=1.0 + eps, line=dict(color='gray', dash='dot'), row=1, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='PPO2 clipping limits how much any sample can improve the objective',\n",
    "    xaxis_title='$r_t(\\theta)$',\n",
    "    height=380,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=-0.25, xanchor='left', x=0.0),\n",
    ")\n",
    "fig.update_xaxes(range=[0.0, 2.0])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Advantage estimation: GAE($\\lambda$)\n",
    "\n",
    "A practical choice is **Generalized Advantage Estimation**:\n",
    "\n",
    "$$\n",
    "\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{A}_t^{\\text{GAE}(\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l\\,\\delta_{t+l}\n",
    "$$\n",
    "\n",
    "- $\\lambda \\to 0$ → low variance, higher bias (more like TD)\n",
    "- $\\lambda \\to 1$ → lower bias, higher variance (more like Monte Carlo)\n",
    "\n",
    "We’ll also use $\\hat{R}_t = \\hat{A}_t + V(s_t)$ as the target return for the critic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Implementation roadmap (what we’ll code)\n",
    "\n",
    "PPO2 training loop per update:\n",
    "\n",
    "1) Collect a rollout of length $T$ (here: `n_steps`) with the current policy.\n",
    "2) Compute values $V(s_t)$, log-probs $\\log\\pi(a_t\\mid s_t)$, and rewards.\n",
    "3) Compute GAE advantages $\\hat{A}_t$ and returns $\\hat{R}_t$.\n",
    "4) For `n_epochs` epochs:\n",
    "   - shuffle the rollout into mini-batches\n",
    "   - optimize the clipped policy objective + value loss + entropy bonus.\n",
    "\n",
    "We’ll log:\n",
    "\n",
    "- episodic returns (what you care about)\n",
    "- policy loss, value loss, entropy\n",
    "- approximate KL and clip fraction (sanity checks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_reset(env, *, seed: Optional[int] = None):\n",
    "    out = env.reset(seed=seed) if seed is not None else env.reset()\n",
    "    if isinstance(out, tuple) and len(out) == 2:\n",
    "        obs, _info = out\n",
    "        return obs\n",
    "    return out\n",
    "\n",
    "\n",
    "def env_step(env, action):\n",
    "    out = env.step(action)\n",
    "    # Gymnasium: (obs, reward, terminated, truncated, info)\n",
    "    if isinstance(out, tuple) and len(out) == 5:\n",
    "        obs, reward, terminated, truncated, info = out\n",
    "        done = bool(terminated) or bool(truncated)\n",
    "        return obs, float(reward), done, info\n",
    "    # Gym: (obs, reward, done, info)\n",
    "    obs, reward, done, info = out\n",
    "    return obs, float(reward), bool(done), info\n",
    "\n",
    "\n",
    "def set_seed_everywhere(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def explained_variance(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"1 - Var[y_true - y_pred] / Var[y_true].\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    var_y = np.var(y_true)\n",
    "    if var_y < 1e-12:\n",
    "        return float('nan')\n",
    "    return float(1.0 - np.var(y_true - y_pred) / var_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_space, hidden_sizes=(64, 64)):\n",
    "        super().__init__()\n",
    "        self.obs_dim = int(obs_dim)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        in_dim = self.obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.Tanh())\n",
    "            in_dim = h\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        # Discrete actions: categorical over logits\n",
    "        if isinstance(action_space, gym.spaces.Discrete):\n",
    "            self.is_discrete = True\n",
    "            self.n_actions = int(action_space.n)\n",
    "            self.actor = nn.Linear(in_dim, self.n_actions)\n",
    "            self.log_std = None\n",
    "        # Continuous actions: diagonal Gaussian\n",
    "        elif isinstance(action_space, gym.spaces.Box):\n",
    "            self.is_discrete = False\n",
    "            self.action_dim = int(np.prod(action_space.shape))\n",
    "            self.actor_mean = nn.Linear(in_dim, self.action_dim)\n",
    "            self.log_std = nn.Parameter(torch.zeros(self.action_dim))\n",
    "        else:\n",
    "            raise TypeError(f'Unsupported action space: {type(action_space)}')\n",
    "\n",
    "        self.critic = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def _dist(self, obs: torch.Tensor):\n",
    "        h = self.backbone(obs)\n",
    "        if self.is_discrete:\n",
    "            logits = self.actor(h)\n",
    "            return Categorical(logits=logits)\n",
    "        mean = self.actor_mean(h)\n",
    "        std = torch.exp(self.log_std).expand_as(mean)\n",
    "        return Independent(Normal(mean, std), 1)\n",
    "\n",
    "    def value(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.backbone(obs)\n",
    "        return self.critic(h).squeeze(-1)\n",
    "\n",
    "    def act(self, obs: torch.Tensor, action: Optional[torch.Tensor] = None):\n",
    "        dist = self._dist(obs)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        value = self.value(obs)\n",
    "        return action, log_prob, entropy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Rollout:\n",
    "    obs: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    log_probs: np.ndarray\n",
    "    values: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    dones: np.ndarray\n",
    "\n",
    "\n",
    "def make_rollout_storage(n_steps: int, obs_dim: int, action_space) -> Rollout:\n",
    "    obs = np.zeros((n_steps, obs_dim), dtype=np.float32)\n",
    "    rewards = np.zeros((n_steps,), dtype=np.float32)\n",
    "    dones = np.zeros((n_steps,), dtype=np.float32)\n",
    "    values = np.zeros((n_steps,), dtype=np.float32)\n",
    "    log_probs = np.zeros((n_steps,), dtype=np.float32)\n",
    "\n",
    "    if isinstance(action_space, gym.spaces.Discrete):\n",
    "        actions = np.zeros((n_steps,), dtype=np.int64)\n",
    "    elif isinstance(action_space, gym.spaces.Box):\n",
    "        act_dim = int(np.prod(action_space.shape))\n",
    "        actions = np.zeros((n_steps, act_dim), dtype=np.float32)\n",
    "    else:\n",
    "        raise TypeError(f'Unsupported action space: {type(action_space)}')\n",
    "\n",
    "    return Rollout(obs=obs, actions=actions, log_probs=log_probs, values=values, rewards=rewards, dones=dones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    rewards: np.ndarray,\n",
    "    dones: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    next_value: float,\n",
    "    *,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns (advantages, returns).\"\"\"\n",
    "    n_steps = len(rewards)\n",
    "    advantages = np.zeros((n_steps,), dtype=np.float32)\n",
    "    last_gae = 0.0\n",
    "    for t in reversed(range(n_steps)):\n",
    "        next_nonterminal = 1.0 - dones[t]\n",
    "        next_v = next_value if t == n_steps - 1 else values[t + 1]\n",
    "        delta = rewards[t] + gamma * next_v * next_nonterminal - values[t]\n",
    "        last_gae = delta + gamma * gae_lambda * next_nonterminal * last_gae\n",
    "        advantages[t] = last_gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) PPO2 update step (PyTorch)\n",
    "\n",
    "The heart of PPO2 is computing:\n",
    "\n",
    "- the ratio $r_t(\\theta)$ using old and new log-probs\n",
    "- the clipped surrogate\n",
    "- the value loss (optionally clipped)\n",
    "- the entropy bonus\n",
    "\n",
    "and then doing standard backprop + optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo2_update(\n",
    "    model: ActorCritic,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    *,\n",
    "    obs: torch.Tensor,\n",
    "    actions: torch.Tensor,\n",
    "    old_log_probs: torch.Tensor,\n",
    "    old_values: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    returns: torch.Tensor,\n",
    "    clip_coef: float,\n",
    "    vf_clip_coef: Optional[float],\n",
    "    ent_coef: float,\n",
    "    vf_coef: float,\n",
    "    max_grad_norm: float,\n",
    ") -> Dict[str, float]:\n",
    "    action, log_prob, entropy, value = model.act(obs, action=actions)\n",
    "\n",
    "    log_ratio = log_prob - old_log_probs\n",
    "    ratio = torch.exp(log_ratio)\n",
    "\n",
    "    # Policy loss (clipped)\n",
    "    unclipped = ratio * advantages\n",
    "    clipped = torch.clamp(ratio, 1.0 - clip_coef, 1.0 + clip_coef) * advantages\n",
    "    policy_loss = -torch.mean(torch.min(unclipped, clipped))\n",
    "\n",
    "    # Value loss (optionally clipped, SB/OpenAI variant)\n",
    "    if vf_clip_coef is None:\n",
    "        value_loss = 0.5 * F.mse_loss(value, returns)\n",
    "    elif vf_clip_coef < 0:\n",
    "        # match original PPO paper: no value clipping\n",
    "        value_loss = 0.5 * F.mse_loss(value, returns)\n",
    "    else:\n",
    "        v_clipped = old_values + torch.clamp(value - old_values, -vf_clip_coef, vf_clip_coef)\n",
    "        v_loss1 = (value - returns).pow(2)\n",
    "        v_loss2 = (v_clipped - returns).pow(2)\n",
    "        value_loss = 0.5 * torch.mean(torch.max(v_loss1, v_loss2))\n",
    "\n",
    "    entropy_loss = -torch.mean(entropy)\n",
    "\n",
    "    loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    approx_kl = torch.mean(-log_ratio).item()\n",
    "    clipfrac = torch.mean((torch.abs(ratio - 1.0) > clip_coef).float()).item()\n",
    "\n",
    "    return {\n",
    "        'loss': float(loss.item()),\n",
    "        'policy_loss': float(policy_loss.item()),\n",
    "        'value_loss': float(value_loss.item()),\n",
    "        'entropy': float(torch.mean(entropy).item()),\n",
    "        'approx_kl': float(approx_kl),\n",
    "        'clipfrac': float(clipfrac),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Train PPO2 on CartPole-v1\n",
    "\n",
    "We’ll keep this as close as possible to the textbook PPO2 recipe:\n",
    "\n",
    "- rollout length: `n_steps`\n",
    "- multi-epoch mini-batch SGD updates\n",
    "- GAE($\\lambda$) advantages (normalized)\n",
    "- plot episodic rewards\n",
    "\n",
    "Tip: CartPole is fast. If you try harder environments, prefer **vectorized** envs (parallel rollouts) for more stable gradient estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo2(\n",
    "    *,\n",
    "    env_id: str = 'CartPole-v1',\n",
    "    total_timesteps: int = 150_000,\n",
    "    n_steps: int = 2048,\n",
    "    n_epochs: int = 10,\n",
    "    minibatch_size: int = 64,\n",
    "    gamma: float = 0.99,\n",
    "    gae_lambda: float = 0.95,\n",
    "    learning_rate: float = 3e-4,\n",
    "    clip_coef: float = 0.2,\n",
    "    vf_clip_coef: Optional[float] = None,\n",
    "    ent_coef: float = 0.0,\n",
    "    vf_coef: float = 0.5,\n",
    "    max_grad_norm: float = 0.5,\n",
    "    target_kl: Optional[float] = 0.03,\n",
    "    seed: int = 42,\n",
    ") -> Dict[str, List[float]]:\n",
    "    set_seed_everywhere(seed)\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "    obs0 = env_reset(env, seed=seed)\n",
    "    obs_dim = int(np.prod(env.observation_space.shape))\n",
    "\n",
    "    model = ActorCritic(obs_dim=obs_dim, action_space=env.action_space).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "    logs: Dict[str, List[float]] = {\n",
    "        'timesteps': [],\n",
    "        'episode_returns': [],\n",
    "        'policy_loss': [],\n",
    "        'value_loss': [],\n",
    "        'entropy': [],\n",
    "        'approx_kl': [],\n",
    "        'clipfrac': [],\n",
    "        'explained_variance': [],\n",
    "    }\n",
    "\n",
    "    obs = obs0\n",
    "    ep_return = 0.0\n",
    "\n",
    "    num_updates = math.ceil(total_timesteps / n_steps)\n",
    "    global_step = 0\n",
    "\n",
    "    for update in range(num_updates):\n",
    "        # Linear schedules (common PPO2 choice)\n",
    "        frac = 1.0 - (update / num_updates)\n",
    "        lr_now = learning_rate * frac\n",
    "        clip_now = clip_coef * frac\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = lr_now\n",
    "\n",
    "        rollout = make_rollout_storage(n_steps=n_steps, obs_dim=obs_dim, action_space=env.action_space)\n",
    "\n",
    "        # Collect on-policy data\n",
    "        for t in range(n_steps):\n",
    "            rollout.obs[t] = np.asarray(obs, dtype=np.float32).reshape(-1)\n",
    "\n",
    "            obs_t = torch.tensor(rollout.obs[t], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action_t, logp_t, _ent_t, value_t = model.act(obs_t)\n",
    "\n",
    "            if model.is_discrete:\n",
    "                action = int(action_t.item())\n",
    "            else:\n",
    "                action = action_t.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "\n",
    "            next_obs, reward, done, _info = env_step(env, action)\n",
    "\n",
    "            rollout.actions[t] = action\n",
    "            rollout.log_probs[t] = float(logp_t.item())\n",
    "            rollout.values[t] = float(value_t.item())\n",
    "            rollout.rewards[t] = float(reward)\n",
    "            rollout.dones[t] = float(done)\n",
    "\n",
    "            ep_return += reward\n",
    "            global_step += 1\n",
    "\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                logs['episode_returns'].append(float(ep_return))\n",
    "                ep_return = 0.0\n",
    "                obs = env_reset(env)\n",
    "\n",
    "        # Bootstrap value for the last observation\n",
    "        obs_last = torch.tensor(np.asarray(obs, dtype=np.float32).reshape(-1), device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            next_value = float(model.value(obs_last).item())\n",
    "\n",
    "        adv_np, ret_np = compute_gae(\n",
    "            rewards=rollout.rewards,\n",
    "            dones=rollout.dones,\n",
    "            values=rollout.values,\n",
    "            next_value=next_value,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "        )\n",
    "\n",
    "        # Flatten batch tensors\n",
    "        b_obs = torch.tensor(rollout.obs, dtype=torch.float32, device=device)\n",
    "        if model.is_discrete:\n",
    "            b_actions = torch.tensor(rollout.actions, dtype=torch.int64, device=device)\n",
    "        else:\n",
    "            b_actions = torch.tensor(rollout.actions, dtype=torch.float32, device=device)\n",
    "        b_old_logp = torch.tensor(rollout.log_probs, dtype=torch.float32, device=device)\n",
    "        b_old_values = torch.tensor(rollout.values, dtype=torch.float32, device=device)\n",
    "        b_adv = torch.tensor(adv_np, dtype=torch.float32, device=device)\n",
    "        b_returns = torch.tensor(ret_np, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Advantage normalization is standard PPO2 practice\n",
    "        b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
    "\n",
    "        # PPO update: multiple epochs over the same on-policy batch\n",
    "        batch_indices = np.arange(n_steps)\n",
    "\n",
    "        metrics_accum = {\n",
    "            'policy_loss': [],\n",
    "            'value_loss': [],\n",
    "            'entropy': [],\n",
    "            'approx_kl': [],\n",
    "            'clipfrac': [],\n",
    "        }\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            rng.shuffle(batch_indices)\n",
    "\n",
    "            for start in range(0, n_steps, minibatch_size):\n",
    "                mb_idx = batch_indices[start : start + minibatch_size]\n",
    "\n",
    "                out = ppo2_update(\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    obs=b_obs[mb_idx],\n",
    "                    actions=b_actions[mb_idx],\n",
    "                    old_log_probs=b_old_logp[mb_idx],\n",
    "                    old_values=b_old_values[mb_idx],\n",
    "                    advantages=b_adv[mb_idx],\n",
    "                    returns=b_returns[mb_idx],\n",
    "                    clip_coef=float(clip_now),\n",
    "                    vf_clip_coef=vf_clip_coef if vf_clip_coef is not None else None,\n",
    "                    ent_coef=float(ent_coef),\n",
    "                    vf_coef=float(vf_coef),\n",
    "                    max_grad_norm=float(max_grad_norm),\n",
    "                )\n",
    "\n",
    "                for k in metrics_accum:\n",
    "                    metrics_accum[k].append(out[k])\n",
    "\n",
    "            # Optional early stopping if KL explodes (common safety valve)\n",
    "            if target_kl is not None and np.mean(metrics_accum['approx_kl']) > 1.5 * target_kl:\n",
    "                break\n",
    "\n",
    "        # Logging at update granularity\n",
    "        logs['timesteps'].append(float(global_step))\n",
    "        logs['policy_loss'].append(float(np.mean(metrics_accum['policy_loss'])))\n",
    "        logs['value_loss'].append(float(np.mean(metrics_accum['value_loss'])))\n",
    "        logs['entropy'].append(float(np.mean(metrics_accum['entropy'])))\n",
    "        logs['approx_kl'].append(float(np.mean(metrics_accum['approx_kl'])))\n",
    "        logs['clipfrac'].append(float(np.mean(metrics_accum['clipfrac'])))\n",
    "        logs['explained_variance'].append(explained_variance(rollout.values, ret_np))\n",
    "\n",
    "    env.close()\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (adjust total_timesteps if you're on CPU and want it faster)\n",
    "logs = train_ppo2(\n",
    "    env_id='CartPole-v1',\n",
    "    total_timesteps=120_000,\n",
    "    n_steps=1024,\n",
    "    n_epochs=10,\n",
    "    minibatch_size=64,\n",
    "    learning_rate=3e-4,\n",
    "    ent_coef=0.0,\n",
    "    vf_clip_coef=0.2,  # SB/OpenAI-style value clipping (set -1 to disable)\n",
    ")\n",
    "\n",
    "len(logs['episode_returns']), logs['episode_returns'][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episodic rewards (and a rolling mean)\n",
    "episode_returns = np.asarray(logs['episode_returns'], dtype=np.float32)\n",
    "episodes = np.arange(1, len(episode_returns) + 1)\n",
    "\n",
    "window = 25\n",
    "if len(episode_returns) >= window:\n",
    "    rolling = np.convolve(episode_returns, np.ones(window) / window, mode='valid')\n",
    "    rolling_x = np.arange(window, len(episode_returns) + 1)\n",
    "else:\n",
    "    rolling = episode_returns\n",
    "    rolling_x = episodes\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=episodes, y=episode_returns, mode='lines', name='Episode return'))\n",
    "fig.add_trace(go.Scatter(x=rolling_x, y=rolling, mode='lines', name=f'Rolling mean ({window})', line=dict(width=4)))\n",
    "fig.update_layout(\n",
    "    title='PPO2 on CartPole-v1: episodic reward over training',\n",
    "    xaxis_title='Episode',\n",
    "    yaxis_title='Episodic return',\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training diagnostics per update\n",
    "df = {\n",
    "    'update': np.arange(len(logs['timesteps'])),\n",
    "    'timesteps': np.asarray(logs['timesteps']),\n",
    "    'policy_loss': np.asarray(logs['policy_loss']),\n",
    "    'value_loss': np.asarray(logs['value_loss']),\n",
    "    'entropy': np.asarray(logs['entropy']),\n",
    "    'approx_kl': np.asarray(logs['approx_kl']),\n",
    "    'clipfrac': np.asarray(logs['clipfrac']),\n",
    "    'explained_variance': np.asarray(logs['explained_variance']),\n",
    "}\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=3,\n",
    "    subplot_titles=(\n",
    "        'Policy loss',\n",
    "        'Value loss',\n",
    "        'Entropy',\n",
    "        'Approx KL',\n",
    "        'Clip fraction',\n",
    "        'Explained variance',\n",
    "    ),\n",
    ")\n",
    "\n",
    "def add_line(row, col, y, name):\n",
    "    fig.add_trace(go.Scatter(x=df['update'], y=y, mode='lines', name=name), row=row, col=col)\n",
    "\n",
    "add_line(1, 1, df['policy_loss'], 'policy_loss')\n",
    "add_line(1, 2, df['value_loss'], 'value_loss')\n",
    "add_line(1, 3, df['entropy'], 'entropy')\n",
    "add_line(2, 1, df['approx_kl'], 'approx_kl')\n",
    "add_line(2, 2, df['clipfrac'], 'clipfrac')\n",
    "add_line(2, 3, df['explained_variance'], 'explained_variance')\n",
    "\n",
    "fig.update_layout(title='Training diagnostics (per PPO update)', height=560, showlegend=False)\n",
    "fig.update_xaxes(title_text='Update')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Stable-Baselines `PPO2` (reference implementation)\n",
    "\n",
    "Stable-Baselines (the TensorFlow library, now in maintenance mode) provides a `PPO2` class.\n",
    "\n",
    "Example from the Stable-Baselines docs (CartPole with a vectorized env):\n",
    "\n",
    "```python\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save('ppo2_cartpole')\n",
    "```\n",
    "\n",
    "We’ll list and explain the Stable-Baselines `PPO2` hyperparameters in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Stable-Baselines `PPO2` hyperparameters (explained)\n",
    "\n",
    "Stable-Baselines `PPO2` (TensorFlow) exposes the following constructor signature (from `stable_baselines/ppo2/ppo2.py`):\n",
    "\n",
    "```python\n",
    "PPO2(\n",
    "    policy,\n",
    "    env,\n",
    "    gamma=0.99,\n",
    "    n_steps=128,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=2.5e-4,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    lam=0.95,\n",
    "    nminibatches=4,\n",
    "    noptepochs=4,\n",
    "    cliprange=0.2,\n",
    "    cliprange_vf=None,\n",
    "    verbose=0,\n",
    "    tensorboard_log=None,\n",
    "    _init_setup_model=True,\n",
    "    policy_kwargs=None,\n",
    "    full_tensorboard_log=False,\n",
    "    seed=None,\n",
    "    n_cpu_tf_sess=None,\n",
    ")\n",
    "```\n",
    "\n",
    "### What each hyperparameter does\n",
    "\n",
    "- `policy`: policy class (or registered string) like `MlpPolicy`, `CnnPolicy`, `MlpLstmPolicy`.\n",
    "- `env`: Gym env instance or an env id string (e.g. `'CartPole-v1'`).\n",
    "- `gamma`: discount factor $\\gamma$.\n",
    "- `n_steps`: rollout horizon per env per update. With vectorized envs, the batch size is:\n",
    "\n",
    "  $$\n",
    "  n_{\\text{batch}} = n_{\\text{steps}} \\cdot n_{\\text{envs}}\n",
    "  $$\n",
    "\n",
    "- `ent_coef`: entropy coefficient $c_e$ (larger → more exploration pressure).\n",
    "- `learning_rate`: learning rate (float) or a schedule function of training progress.\n",
    "- `vf_coef`: value-loss coefficient $c_v$.\n",
    "- `max_grad_norm`: global gradient norm clip threshold.\n",
    "- `lam`: GAE($\\lambda$) parameter.\n",
    "- `nminibatches`: number of minibatches per update (minibatch size is `n_batch / nminibatches`). For recurrent policies, SB recommends `n_envs` be a multiple of `nminibatches`.\n",
    "- `noptepochs`: number of epochs over the on-policy batch per update.\n",
    "- `cliprange`: PPO clip parameter $\\epsilon$ (float) or a schedule.\n",
    "- `cliprange_vf`: value-function clipping range.\n",
    "  - `None` (default): reuse `cliprange` for the value function (OpenAI baselines legacy behavior).\n",
    "  - negative value (e.g. `-1`): **disable** value clipping (closer to the original PPO paper).\n",
    "  - positive float/schedule: enable value clipping with that range.\n",
    "\n",
    "  Note: value clipping depends on reward scaling.\n",
    "\n",
    "- `verbose`: logging verbosity.\n",
    "- `tensorboard_log`: TensorBoard log directory (or `None`).\n",
    "- `_init_setup_model`: whether to build the TF graph at init.\n",
    "- `policy_kwargs`: extra kwargs forwarded to the policy network constructor.\n",
    "- `full_tensorboard_log`: log additional tensors/histograms (large disk usage).\n",
    "- `seed`: random seed (Python/NumPy/TF). For fully deterministic TF runs, SB notes you should set `n_cpu_tf_sess=1`.\n",
    "- `n_cpu_tf_sess`: number of TensorFlow threads.\n",
    "\n",
    "### Mapping to this notebook\n",
    "\n",
    "- SB `n_steps` → this notebook’s `n_steps`\n",
    "- SB `noptepochs` → this notebook’s `n_epochs`\n",
    "- SB `nminibatches` → this notebook’s `minibatch_size = n_steps / nminibatches` (single-env case)\n",
    "- SB `cliprange` → this notebook’s `clip_coef`\n",
    "- SB `cliprange_vf` → this notebook’s `vf_clip_coef`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
