{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80a3a7b",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC) for Continuous Action Spaces (low-level PyTorch)\n",
    "\n",
    "Soft Actor-Critic (SAC) is an **off-policy** actor-critic algorithm that learns a **stochastic** policy by maximizing expected return *and* **entropy**. The entropy term makes exploration a first-class objective, which tends to improve stability and robustness.\n",
    "\n",
    "In this notebook you will:\n",
    "- Derive the **maximum-entropy** objective and the SAC losses\n",
    "- Implement SAC **from scratch in PyTorch** (replay buffer, twin critics, target networks, squashed Gaussian policy, temperature tuning)\n",
    "- Train on a small **continuous-control** environment (no downloads required)\n",
    "- Visualize **episodic rewards**, **policy entropy**, and **Q-values** with **Plotly**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "- Understand why SAC maximizes entropy and how that changes the Bellman backup\n",
    "- Implement the core SAC update rules directly in PyTorch\n",
    "- Diagnose learning using reward/entropy/Q plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6e5fa",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1. Maximum-entropy RL: objective + key equations (LaTeX)\n",
    "2. Environment (continuous actions, offline-friendly)\n",
    "3. From scratch (PyTorch): replay buffer + networks\n",
    "4. From scratch (PyTorch): SAC update (critic, actor, temperature)\n",
    "5. Training loop + Plotly diagnostics (entropy, Q-values, rewards)\n",
    "6. Stable-Baselines3 SAC: reference code + hyperparameters (final section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.distributions import Normal\n",
    "\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch import failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c468b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\")\n",
    "    if TORCH_AVAILABLE and torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "import plotly\n",
    "\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "if TORCH_AVAILABLE:\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e5f9b",
   "metadata": {},
   "source": [
    "## 1) Maximum-entropy RL (entropy maximization)\n",
    "\n",
    "Classic RL maximizes expected return:\n",
    "\n",
    "$$\n",
    "\\max_\\pi\\; \\mathbb{E}_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{T-1} \\gamma^t\\, r(s_t,a_t)\\Big].\n",
    "$$\n",
    "\n",
    "SAC instead uses a **maximum-entropy** objective:\n",
    "\n",
    "$$\n",
    "\\max_\\pi\\; \\mathbb{E}_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{T-1} \\gamma^t\\,\\big(r(s_t,a_t) + \\alpha\\,\\mathcal{H}(\\pi(\\cdot\\mid s_t))\\big)\\Big],\n",
    "$$\n",
    "\n",
    "where the (differential) entropy of a continuous policy is\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(\\pi(\\cdot\\mid s)) = -\\mathbb{E}_{a\\sim\\pi(\\cdot\\mid s)}\\big[\\log \\pi(a\\mid s)\\big].\n",
    "$$\n",
    "\n",
    "- Larger entropy means a **broader** action distribution (more exploration).\n",
    "- $\\alpha>0$ is the **temperature**: it trades off reward vs. entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee915eaa",
   "metadata": {},
   "source": [
    "### Soft value functions\n",
    "\n",
    "SAC defines a **soft** value function:\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\mathbb{E}_{a\\sim\\pi}\\big[Q_\\pi(s,a) - \\alpha\\,\\log\\pi(a\\mid s)\\big].\n",
    "$$\n",
    "\n",
    "This leads to a soft Bellman backup for $Q$:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = r(s,a) + \\gamma\\,\\mathbb{E}_{s'\\sim P}\\big[V_\\pi(s')\\big].\n",
    "$$\n",
    "\n",
    "Intuition: the next-state value is high if we can both (1) get high Q and (2) keep the policy **stochastic** (high entropy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e3d9d",
   "metadata": {},
   "source": [
    "### Losses used in SAC (twin critics + temperature tuning)\n",
    "\n",
    "SAC typically uses **two** Q-networks $Q_{\\theta_1}, Q_{\\theta_2}$ to reduce overestimation bias.\n",
    "\n",
    "1) **Critic target** (with target networks and a sampled next action $a'\\sim\\pi_\\phi(\\cdot\\mid s')$):\n",
    "\n",
    "$$\n",
    "y = r + \\gamma(1-d)\\Big(\\min_i Q_{\\bar\\theta_i}(s', a') - \\alpha\\,\\log \\pi_\\phi(a'\\mid s')\\Big).\n",
    "$$\n",
    "\n",
    "2) **Critic loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_Q = \\mathbb{E}\\big[(Q_{\\theta_1}(s,a)-y)^2 + (Q_{\\theta_2}(s,a)-y)^2\\big].\n",
    "$$\n",
    "\n",
    "3) **Actor loss** (reparameterization trick):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\pi = \\mathbb{E}_{s\\sim\\mathcal{D},\\,a\\sim\\pi_\\phi}\\big[\\alpha\\,\\log\\pi_\\phi(a\\mid s) - \\min_i Q_{\\theta_i}(s,a)\\big].\n",
    "$$\n",
    "\n",
    "4) **Automatic temperature** (optional): learn $\\alpha$ to match a target entropy $\\mathcal{H}_{\\text{target}}$.\n",
    "\n",
    "A common form (optimize $\\log\\alpha$) is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\alpha = \\mathbb{E}_{a\\sim\\pi_\\phi}\\big[-\\alpha\\,(\\log\\pi_\\phi(a\\mid s) + \\mathcal{H}_{\\text{target}})\\big].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousPointMass1DEnv:\n",
    "    \"\"\"A tiny continuous-control environment (no external deps).\n",
    "\n",
    "    State:  [position, velocity]\n",
    "    Action: acceleration in [-action_limit, +action_limit]\n",
    "    Reward: negative quadratic cost (stabilize at 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dt: float = 0.05,\n",
    "        max_steps: int = 200,\n",
    "        action_limit: float = 2.0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.dt = float(dt)\n",
    "        self.max_steps = int(max_steps)\n",
    "        self._action_limit = float(action_limit)\n",
    "        self.action_low = np.array([-self._action_limit], dtype=np.float32)\n",
    "        self.action_high = np.array([+self._action_limit], dtype=np.float32)\n",
    "        self.obs_dim = 2\n",
    "        self.act_dim = 1\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.t = 0\n",
    "        self.state = np.zeros(self.obs_dim, dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed: int | None = None):\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        self.t = 0\n",
    "        pos = self.rng.uniform(-1.0, 1.0)\n",
    "        vel = self.rng.uniform(-1.0, 1.0)\n",
    "        self.state = np.array([pos, vel], dtype=np.float32)\n",
    "        return self.state.copy(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        a = float(np.asarray(action, dtype=np.float32).reshape(-1)[0])\n",
    "        a = float(np.clip(a, self.action_low[0], self.action_high[0]))\n",
    "\n",
    "        pos, vel = float(self.state[0]), float(self.state[1])\n",
    "        vel = vel + self.dt * a\n",
    "        pos = pos + self.dt * vel\n",
    "        self.state = np.array([pos, vel], dtype=np.float32)\n",
    "\n",
    "        reward = -(pos * pos + 0.1 * vel * vel + 0.001 * a * a)\n",
    "\n",
    "        self.t += 1\n",
    "        terminated = False\n",
    "        truncated = self.t >= self.max_steps\n",
    "        info = {}\n",
    "        return self.state.copy(), float(reward), terminated, truncated, info\n",
    "\n",
    "\n",
    "def reset_env(env, seed: int | None = None) -> np.ndarray:\n",
    "    out = env.reset(seed=seed) if seed is not None else env.reset()\n",
    "    if isinstance(out, tuple) and len(out) == 2:\n",
    "        obs, _info = out\n",
    "    else:\n",
    "        obs = out\n",
    "    return np.asarray(obs, dtype=np.float32)\n",
    "\n",
    "\n",
    "def step_env(env, action: np.ndarray):\n",
    "    out = env.step(action)\n",
    "    if isinstance(out, tuple) and len(out) == 5:\n",
    "        obs, reward, terminated, truncated, info = out\n",
    "        done = bool(terminated) or bool(truncated)\n",
    "    else:\n",
    "        obs, reward, done, info = out\n",
    "    return np.asarray(obs, dtype=np.float32), float(reward), bool(done), info\n",
    "\n",
    "\n",
    "def make_env(seed: int = 42, prefer_gym_pendulum: bool = False):\n",
    "    if prefer_gym_pendulum:\n",
    "        for pkg in (\"gymnasium\", \"gym\"):\n",
    "            try:\n",
    "                gym = __import__(pkg)\n",
    "                env = gym.make(\"Pendulum-v1\")\n",
    "                reset_env(env, seed)\n",
    "                name = f\"{pkg}:Pendulum-v1\"\n",
    "                action_low = env.action_space.low.astype(np.float32)\n",
    "                action_high = env.action_space.high.astype(np.float32)\n",
    "                obs_dim = int(env.observation_space.shape[0])\n",
    "                act_dim = int(env.action_space.shape[0])\n",
    "                return env, name, obs_dim, act_dim, action_low, action_high\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    env = ContinuousPointMass1DEnv(seed=seed)\n",
    "    reset_env(env, seed)\n",
    "    name = \"custom:ContinuousPointMass1DEnv\"\n",
    "    return env, name, env.obs_dim, env.act_dim, env.action_low, env.action_high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_name, obs_dim, act_dim, act_low, act_high = make_env(seed=SEED, prefer_gym_pendulum=False)\n",
    "print(\"Env:\", env_name)\n",
    "print(\"obs_dim:\", obs_dim, \"act_dim:\", act_dim)\n",
    "print(\"action bounds:\", act_low, act_high)\n",
    "\n",
    "# Quick sanity rollout (random actions)\n",
    "obs = reset_env(env, seed=SEED)\n",
    "positions, velocities, rewards = [], [], []\n",
    "for t in range(80):\n",
    "    a = rng.uniform(act_low, act_high)\n",
    "    obs, r, done, _ = step_env(env, a)\n",
    "    positions.append(float(obs[0]))\n",
    "    velocities.append(float(obs[1]))\n",
    "    rewards.append(float(r))\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "fig.add_trace(go.Scatter(y=positions, mode=\"lines\", name=\"position\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=velocities, mode=\"lines\", name=\"velocity\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=rewards, mode=\"lines\", name=\"reward\"), row=3, col=1)\n",
    "fig.update_layout(height=650, title=\"Sanity check rollout (random actions)\")\n",
    "fig.update_yaxes(title_text=\"pos\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"vel\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"r\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"step\", row=3, col=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53361e1",
   "metadata": {},
   "source": [
    "## 2) Replay buffer (from scratch)\n",
    "\n",
    "SAC is **off-policy**, so it stores transitions in a replay buffer and samples mini-batches for gradient updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5091b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim: int, act_dim: int, size: int, seed: int = 0):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "        self.done_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "\n",
    "        self.max_size = int(size)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr] = float(done)\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size: int, device: torch.device):\n",
    "        idx = self.rng.integers(0, self.size, size=batch_size)\n",
    "        batch = dict(\n",
    "            obs=torch.as_tensor(self.obs_buf[idx], device=device),\n",
    "            act=torch.as_tensor(self.act_buf[idx], device=device),\n",
    "            rew=torch.as_tensor(self.rew_buf[idx], device=device),\n",
    "            next_obs=torch.as_tensor(self.next_obs_buf[idx], device=device),\n",
    "            done=torch.as_tensor(self.done_buf[idx], device=device),\n",
    "        )\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a74ff",
   "metadata": {},
   "source": [
    "## 3) Networks (from scratch)\n",
    "\n",
    "We implement:\n",
    "- A **squashed Gaussian** actor $\\pi_\\phi(a\\mid s)$ using `tanh` to respect action bounds\n",
    "- Two critics $Q_{\\theta_1}$ and $Q_{\\theta_2}$\n",
    "\n",
    "The key low-level detail is the **log-prob correction** for the `tanh` squash (change of variables).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29073c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class SquashedGaussianActor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        act_dim: int,\n",
    "        act_low: np.ndarray,\n",
    "        act_high: np.ndarray,\n",
    "        hidden_sizes=(256, 256),\n",
    "        log_std_min: float = -20.0,\n",
    "        log_std_max: float = 2.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim, *hidden_sizes], activation=nn.ReLU, output_activation=nn.ReLU)\n",
    "        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "\n",
    "        self.log_std_min = float(log_std_min)\n",
    "        self.log_std_max = float(log_std_max)\n",
    "\n",
    "        act_low = np.asarray(act_low, dtype=np.float32)\n",
    "        act_high = np.asarray(act_high, dtype=np.float32)\n",
    "        action_scale = (act_high - act_low) / 2.0\n",
    "        action_bias = (act_high + act_low) / 2.0\n",
    "        self.register_buffer(\"action_scale\", torch.as_tensor(action_scale))\n",
    "        self.register_buffer(\"action_bias\", torch.as_tensor(action_bias))\n",
    "\n",
    "    def forward(self, obs: torch.Tensor):\n",
    "        h = self.net(obs)\n",
    "        mu = self.mu_layer(h)\n",
    "        log_std = self.log_std_layer(h)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        std = torch.exp(log_std)\n",
    "        return mu, std\n",
    "\n",
    "    def sample(self, obs: torch.Tensor):\n",
    "        mu, std = self(obs)\n",
    "        dist = Normal(mu, std)\n",
    "        u = dist.rsample()  # reparameterization\n",
    "        a = torch.tanh(u)\n",
    "\n",
    "        action = a * self.action_scale + self.action_bias\n",
    "\n",
    "        # Log prob with tanh + scaling correction (change of variables)\n",
    "        log_prob_u = dist.log_prob(u).sum(dim=-1, keepdim=True)\n",
    "        log_det = (\n",
    "            torch.log(self.action_scale + 1e-8)\n",
    "            + torch.log(1.0 - a.pow(2) + 1e-6)\n",
    "        ).sum(dim=-1, keepdim=True)\n",
    "        log_prob = log_prob_u - log_det\n",
    "\n",
    "        mu_action = torch.tanh(mu) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mu_action\n",
    "\n",
    "    def act(self, obs: np.ndarray, deterministic: bool = False):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.action_scale.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                mu, _std = self(obs_t)\n",
    "                a = torch.tanh(mu) * self.action_scale + self.action_bias\n",
    "            else:\n",
    "                a, _logp, _mu_a = self.sample(obs_t)\n",
    "        return a.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim, *hidden_sizes, 1], activation=nn.ReLU)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: torch.Tensor):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63f00c",
   "metadata": {},
   "source": [
    "## 4) SAC update (from scratch)\n",
    "\n",
    "This is the core of the algorithm: compute targets, optimize critics, optimize actor, optionally tune $\\alpha$, then Polyak-update target critics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fe57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(source: nn.Module, target: nn.Module, tau: float):\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(source.parameters(), target.parameters(), strict=True):\n",
    "            p_targ.data.mul_(1.0 - tau)\n",
    "            p_targ.data.add_(tau * p.data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SACConfig:\n",
    "    gamma: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    actor_lr: float = 3e-4\n",
    "    critic_lr: float = 3e-4\n",
    "    alpha_lr: float = 3e-4\n",
    "    batch_size: int = 256\n",
    "    replay_size: int = 200_000\n",
    "    updates_per_step: int = 1\n",
    "    start_steps: int = 1_000\n",
    "    auto_alpha: bool = True\n",
    "    init_alpha: float = 0.2\n",
    "\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        act_dim: int,\n",
    "        act_low: np.ndarray,\n",
    "        act_high: np.ndarray,\n",
    "        config: SACConfig,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.cfg = config\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = SquashedGaussianActor(obs_dim, act_dim, act_low, act_high).to(device)\n",
    "        self.q1 = QNetwork(obs_dim, act_dim).to(device)\n",
    "        self.q2 = QNetwork(obs_dim, act_dim).to(device)\n",
    "\n",
    "        self.q1_targ = QNetwork(obs_dim, act_dim).to(device)\n",
    "        self.q2_targ = QNetwork(obs_dim, act_dim).to(device)\n",
    "        self.q1_targ.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_targ.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=config.actor_lr)\n",
    "        self.q_opt = torch.optim.Adam(\n",
    "            list(self.q1.parameters()) + list(self.q2.parameters()), lr=config.critic_lr\n",
    "        )\n",
    "\n",
    "        self.auto_alpha = bool(config.auto_alpha)\n",
    "        if self.auto_alpha:\n",
    "            # A common heuristic for target entropy: -|A|\n",
    "            self.target_entropy = -float(act_dim)\n",
    "            self.log_alpha = torch.tensor(\n",
    "                math.log(config.init_alpha), dtype=torch.float32, device=device, requires_grad=True\n",
    "            )\n",
    "            self.alpha_opt = torch.optim.Adam([self.log_alpha], lr=config.alpha_lr)\n",
    "        else:\n",
    "            self.target_entropy = None\n",
    "            self.log_alpha = torch.tensor(math.log(config.init_alpha), dtype=torch.float32, device=device)\n",
    "            self.alpha_opt = None\n",
    "\n",
    "    @property\n",
    "    def alpha(self) -> torch.Tensor:\n",
    "        return self.log_alpha.exp()\n",
    "\n",
    "    def act(self, obs: np.ndarray, deterministic: bool = False) -> np.ndarray:\n",
    "        return self.actor.act(obs, deterministic=deterministic)\n",
    "\n",
    "    def update(self, batch: dict[str, torch.Tensor]):\n",
    "        obs = batch[\"obs\"]\n",
    "        act = batch[\"act\"]\n",
    "        rew = batch[\"rew\"]\n",
    "        next_obs = batch[\"next_obs\"]\n",
    "        done = batch[\"done\"]\n",
    "\n",
    "        # --- Critic update ---\n",
    "        with torch.no_grad():\n",
    "            next_a, next_logp, _next_mu_a = self.actor.sample(next_obs)\n",
    "            q1_next = self.q1_targ(next_obs, next_a)\n",
    "            q2_next = self.q2_targ(next_obs, next_a)\n",
    "            q_next = torch.min(q1_next, q2_next) - self.alpha * next_logp\n",
    "            backup = rew + self.cfg.gamma * (1.0 - done) * q_next\n",
    "\n",
    "        q1 = self.q1(obs, act)\n",
    "        q2 = self.q2(obs, act)\n",
    "        critic_loss = F.mse_loss(q1, backup) + F.mse_loss(q2, backup)\n",
    "\n",
    "        self.q_opt.zero_grad(set_to_none=True)\n",
    "        critic_loss.backward()\n",
    "        self.q_opt.step()\n",
    "\n",
    "        # --- Actor update ---\n",
    "        a_pi, logp_pi, _mu_a = self.actor.sample(obs)\n",
    "        q1_pi = self.q1(obs, a_pi)\n",
    "        q2_pi = self.q2(obs, a_pi)\n",
    "        q_pi = torch.min(q1_pi, q2_pi)\n",
    "        actor_loss = (self.alpha * logp_pi - q_pi).mean()\n",
    "\n",
    "        self.actor_opt.zero_grad(set_to_none=True)\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        # --- Temperature (alpha) update ---\n",
    "        alpha_loss = torch.tensor(0.0, device=self.device)\n",
    "        if self.auto_alpha:\n",
    "            alpha_loss = -(self.log_alpha * (logp_pi + self.target_entropy).detach()).mean()\n",
    "            self.alpha_opt.zero_grad(set_to_none=True)\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_opt.step()\n",
    "\n",
    "        # --- Target networks ---\n",
    "        polyak_update(self.q1, self.q1_targ, self.cfg.tau)\n",
    "        polyak_update(self.q2, self.q2_targ, self.cfg.tau)\n",
    "\n",
    "        metrics = {\n",
    "            \"critic_loss\": float(critic_loss.item()),\n",
    "            \"actor_loss\": float(actor_loss.item()),\n",
    "            \"alpha\": float(self.alpha.item()),\n",
    "            \"alpha_loss\": float(alpha_loss.item()),\n",
    "            \"mean_logp\": float(logp_pi.mean().item()),\n",
    "            \"mean_q\": float(q_pi.mean().item()),\n",
    "        }\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a08ea",
   "metadata": {},
   "source": [
    "## 5) Training loop + Plotly diagnostics\n",
    "\n",
    "We log per-episode:\n",
    "- Episodic return (sum of rewards)\n",
    "- Average entropy estimate $-\\log\\pi(a\\mid s)$\n",
    "- Average Q-value estimate $\\min(Q_1,Q_2)$ for actions taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e68be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_RUN = True  # set False for longer training\n",
    "\n",
    "cfg = SACConfig(\n",
    "    replay_size=50_000 if FAST_RUN else 200_000,\n",
    "    batch_size=128 if FAST_RUN else 256,\n",
    "    start_steps=500 if FAST_RUN else 1_000,\n",
    "    updates_per_step=1,\n",
    ")\n",
    "\n",
    "TOTAL_EPISODES = 60 if FAST_RUN else 200\n",
    "MAX_STEPS_PER_EP = 200\n",
    "\n",
    "if not TORCH_AVAILABLE:\n",
    "    raise RuntimeError(\"PyTorch is required for the from-scratch SAC implementation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(obs_dim, act_dim, size=cfg.replay_size, seed=SEED)\n",
    "agent = SACAgent(obs_dim, act_dim, act_low, act_high, config=cfg, device=device)\n",
    "\n",
    "\n",
    "def moving_average(x, window: int = 10):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x) < window:\n",
    "        return x\n",
    "    w = np.ones(window) / window\n",
    "    y = np.convolve(x, w, mode=\"valid\")\n",
    "    # pad left to match length\n",
    "    return np.concatenate([np.full(window - 1, np.nan), y])\n",
    "\n",
    "\n",
    "ep_returns = []\n",
    "ep_entropies = []\n",
    "ep_q_values = []\n",
    "ep_alphas = []\n",
    "\n",
    "total_steps = 0\n",
    "t0 = time.time()\n",
    "for ep in range(TOTAL_EPISODES):\n",
    "    obs = reset_env(env, seed=SEED + ep)\n",
    "    ep_ret = 0.0\n",
    "    ent_sum = 0.0\n",
    "    q_sum = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(MAX_STEPS_PER_EP):\n",
    "        if total_steps < cfg.start_steps:\n",
    "            act = rng.uniform(act_low, act_high).astype(np.float32)\n",
    "            # For plotting, estimate entropy from current policy anyway\n",
    "            obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                _a_pi, logp_pi, _mu_a = agent.actor.sample(obs_t)\n",
    "                entropy_est = float((-logp_pi).item())\n",
    "                q_est = float(\n",
    "                    torch.min(agent.q1(obs_t, _a_pi), agent.q2(obs_t, _a_pi)).item()\n",
    "                )\n",
    "        else:\n",
    "            obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                a_pi, logp_pi, _mu_a = agent.actor.sample(obs_t)\n",
    "                act = a_pi.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "                entropy_est = float((-logp_pi).item())\n",
    "                q_est = float(\n",
    "                    torch.min(agent.q1(obs_t, a_pi), agent.q2(obs_t, a_pi)).item()\n",
    "                )\n",
    "\n",
    "        next_obs, r, done, _info = step_env(env, act)\n",
    "\n",
    "        buffer.store(obs, act, r, next_obs, done)\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_ret += r\n",
    "        ent_sum += entropy_est\n",
    "        q_sum += q_est\n",
    "        steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # Gradient updates\n",
    "        if buffer.size >= cfg.batch_size:\n",
    "            for _ in range(cfg.updates_per_step):\n",
    "                batch = buffer.sample_batch(cfg.batch_size, device=device)\n",
    "                _metrics = agent.update(batch)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    ep_returns.append(ep_ret)\n",
    "    ep_entropies.append(ent_sum / max(1, steps))\n",
    "    ep_q_values.append(q_sum / max(1, steps))\n",
    "    ep_alphas.append(float(agent.alpha.item()))\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"Finished {TOTAL_EPISODES} episodes, {total_steps} steps in {dt:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b67b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1, len(ep_returns) + 1)\n",
    "ma_window = 10\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=4,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.06,\n",
    "    subplot_titles=(\n",
    "        \"Episodic rewards (return)\",\n",
    "        \"Policy entropy estimate (mean -log π(a|s))\",\n",
    "        \"Q-value estimate (mean min(Q1,Q2))\",\n",
    "        \"Temperature α\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=episodes, y=ep_returns, mode=\"lines\", name=\"return\"), row=1, col=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=episodes,\n",
    "        y=moving_average(ep_returns, ma_window),\n",
    "        mode=\"lines\",\n",
    "        name=f\"return (MA{ma_window})\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=episodes, y=ep_entropies, mode=\"lines\", name=\"entropy\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=episodes, y=ep_q_values, mode=\"lines\", name=\"Q\"), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=episodes, y=ep_alphas, mode=\"lines\", name=\"alpha\"), row=4, col=1)\n",
    "\n",
    "fig.update_layout(height=900, title=\"SAC training diagnostics\")\n",
    "fig.update_yaxes(title_text=\"return\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"-logπ\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Q\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"α\", row=4, col=1)\n",
    "fig.update_xaxes(title_text=\"episode\", row=4, col=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4e0f4",
   "metadata": {},
   "source": [
    "### A minimal evaluation run (deterministic actions)\n",
    "\n",
    "Evaluation uses the **mean** action (deterministic), which usually performs better than sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(agent: SACAgent, env, episodes: int = 5):\n",
    "    returns = []\n",
    "    for k in range(episodes):\n",
    "        obs = reset_env(env, seed=10_000 + k)\n",
    "        ep_ret = 0.0\n",
    "        for _ in range(MAX_STEPS_PER_EP):\n",
    "            act = agent.act(obs, deterministic=True)\n",
    "            obs, r, done, _ = step_env(env, act)\n",
    "            ep_ret += r\n",
    "            if done:\n",
    "                break\n",
    "        returns.append(ep_ret)\n",
    "    return returns\n",
    "\n",
    "\n",
    "eval_returns = eval_policy(agent, env, episodes=8)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=list(range(1, len(eval_returns) + 1)), y=eval_returns, name=\"eval return\"))\n",
    "fig.update_layout(\n",
    "    title=\"Deterministic evaluation (per-episode return)\",\n",
    "    xaxis_title=\"eval episode\",\n",
    "    yaxis_title=\"return\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"Eval mean return:\", float(np.mean(eval_returns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde9e1e",
   "metadata": {},
   "source": [
    "## 6) Stable-Baselines3 SAC (reference implementation + hyperparameters)\n",
    "\n",
    "A widely used reference implementation is **Stable-Baselines3 (SB3)**, which includes `stable_baselines3.SAC`.\n",
    "\n",
    "Useful links:\n",
    "- SB3 source: https://github.com/DLR-RM/stable-baselines3\n",
    "- SAC paper (Haarnoja et al.): https://arxiv.org/abs/1801.01290\n",
    "- Spinning Up SAC overview: https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "\n",
    "### SB3 usage example\n",
    "\n",
    "```python\n",
    "# pip install stable-baselines3 gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "model = SAC(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=1_000_000,\n",
    "    learning_starts=100,\n",
    "    batch_size=256,\n",
    "    tau=0.005,\n",
    "    gamma=0.99,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    ent_coef=\"auto\",\n",
    "    target_entropy=\"auto\",\n",
    ")\n",
    "model.learn(total_timesteps=100_000)\n",
    "```\n",
    "\n",
    "### SB3 SAC hyperparameters (what they mean)\n",
    "\n",
    "Below is a practical summary of the main `stable_baselines3.SAC(...)` constructor arguments (based on the SB3 implementation/docstring):\n",
    "\n",
    "- `policy`: Policy architecture wrapper (`\"MlpPolicy\"`, `\"CnnPolicy\"`, ...).\n",
    "- `env`: Gym/Gymnasium env instance (or env ID string).\n",
    "- `learning_rate`: Adam learning rate for actor/critics (can be a schedule).\n",
    "- `buffer_size`: Replay buffer capacity.\n",
    "- `learning_starts`: Number of environment steps to collect before training starts.\n",
    "- `batch_size`: Mini-batch size per gradient update.\n",
    "- `tau`: Target-network Polyak coefficient.\n",
    "- `gamma`: Discount factor.\n",
    "- `train_freq`: How often to train (e.g., every N steps, or `(N, \"episode\")`).\n",
    "- `gradient_steps`: Gradient updates after each rollout; `-1` means \"as many as steps collected\".\n",
    "- `action_noise`: Optional action noise process (often unused for SAC because the policy is stochastic).\n",
    "- `replay_buffer_class` / `replay_buffer_kwargs`: Swap replay buffer type/params (e.g., HER).\n",
    "- `optimize_memory_usage`: Memory-efficient replay buffer variant.\n",
    "- `n_steps`: If >1, use n-step returns via an n-step replay buffer.\n",
    "- `ent_coef`: Entropy coefficient $\\alpha$; set to `\"auto\"` to learn it (`\"auto_0.1\"` uses 0.1 init).\n",
    "- `target_update_interval`: Target network update interval (in gradient steps).\n",
    "- `target_entropy`: Target entropy when `ent_coef=\"auto\"` (often `\"auto\"` to use a heuristic).\n",
    "- `use_sde`: Use generalized State Dependent Exploration (gSDE) instead of action noise.\n",
    "- `sde_sample_freq`: Resample gSDE noise every N steps (`-1` means only at rollout start).\n",
    "- `use_sde_at_warmup`: Use gSDE during warmup instead of uniform random actions.\n",
    "- `stats_window_size`: Window size for logged rollout stats.\n",
    "- `tensorboard_log`: TensorBoard logging directory.\n",
    "- `policy_kwargs`: Extra kwargs for the policy/networks (e.g., `net_arch`, activation function).\n",
    "- `verbose`, `seed`, `device`: Usual runtime controls.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}