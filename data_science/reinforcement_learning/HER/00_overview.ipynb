{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24b00c8",
   "metadata": {},
   "source": [
    "# Hindsight Experience Replay (HER) — low-level PyTorch (DDPG) in a goal-based environment\n",
    "\n",
    "HER is a replay-buffer trick for **sparse-reward, goal-conditioned** reinforcement learning.\n",
    "\n",
    "Instead of throwing away failed episodes, we **reinterpret** them as successes for *other* goals that were actually achieved later in the same episode.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "- Understand goal-conditioned RL and why sparse rewards are hard.\n",
    "- Precisely define **goal relabeling** (HER) with math notation.\n",
    "- Implement **DDPG + HER from scratch** in PyTorch (networks, targets, replay buffer, training loop).\n",
    "- Use Plotly to visualize **reward/success per episode** and learning signals (losses, Q-values).\n",
    "- See how the same idea is used in **Stable-Baselines / Stable-Baselines3**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b32ac",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Basic RL: MDPs, value functions, off-policy learning.\n",
    "- DDPG idea: deterministic actor + critic, target networks, replay buffer.\n",
    "- PyTorch basics: `nn.Module`, optimizers, backprop.\n",
    "- Goal-based env interface: observations are a dict with `observation`, `achieved_goal`, `desired_goal`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c9d4e",
   "metadata": {},
   "source": [
    "## Why HER?\n",
    "\n",
    "In many tasks you only get a reward when you succeed (a **sparse reward**). Early on, success may be extremely rare, so learning stalls.\n",
    "\n",
    "HER addresses this by turning hindsight into supervision:\n",
    "- you attempted goal $g$ and failed,\n",
    "- but you *did* reach many other states / achieved-goals along the way,\n",
    "- so we can relabel the goal to something that actually happened and learn from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89b0d4",
   "metadata": {},
   "source": [
    "## Goal-conditioned RL notation\n",
    "\n",
    "A goal-conditioned environment exposes observations as a dict:\n",
    "$$o_t = \\{x_t,\\; a_t^g,\\; g\\}$$\n",
    "where:\n",
    "- $x_t$ is the \"regular\" observation (e.g. position/velocity),\n",
    "- $a_t^g$ is the **achieved goal** at time $t$ (often a subset of the state, e.g. end-effector position),\n",
    "- $g$ is the **desired goal**.\n",
    "\n",
    "The reward is usually defined through a task-specific distance function $d(\\cdot,\\cdot)$ and a threshold $\\epsilon$.\n",
    "\n",
    "**Sparse goal reward (common in HER benchmarks):**\n",
    "$$r_t = r(a_{t+1}^g, g) = -\\mathbb{1}[d(a_{t+1}^g, g) > \\epsilon]$$\n",
    "so reward is $0$ on success and $-1$ otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19092cf",
   "metadata": {},
   "source": [
    "## Goal relabeling (HER) — precise definition\n",
    "\n",
    "Consider an episode/trajectory of length $T$ collected under goal $g$:\n",
    "$$\\tau = \\big\\{(x_t, a_t, r_t, x_{t+1}, a_t^g, a_{t+1}^g, g)\\big\\}_{t=0}^{T-1}$$\n",
    "\n",
    "HER constructs **additional** replay transitions by sampling an alternative goal $g'$ from achieved goals in the same episode. The most common choice is the **future** strategy:\n",
    "$$g' \\sim \\mathrm{Uniform}\\big(\\{a_{t+1}^g, a_{t+2}^g, \\ldots, a_T^g\\}\\big)$$\n",
    "\n",
    "Then we *recompute* the reward under the relabeled goal:\n",
    "$$r'_t = r(a_{t+1}^g, g')$$\n",
    "\n",
    "and store the relabeled transition:\n",
    "$$\\big((x_t, g'),\\; a_t,\\; r'_t,\\; (x_{t+1}, g'),\\; \\text{done}_t\\big)$$\n",
    "\n",
    "### How many relabeled transitions?\n",
    "A common parameterization is `n_sampled_goal = k`: for each real transition, create $k$ hindsight transitions.\n",
    "Equivalently, if you sample a batch from the replay buffer, you can relabel each sampled transition with probability:\n",
    "$$p_{\\mathrm{HER}} = \\frac{k}{k+1}$$\n",
    "so on average you see $k$ hindsight samples per one real sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83d31d",
   "metadata": {},
   "source": [
    "## A tiny goal-based environment (Gymnasium `Dict` observation)\n",
    "\n",
    "We will build a simple 2D point-mass that must reach a randomly sampled 2D goal.\n",
    "\n",
    "- `observation`: position + velocity\n",
    "- `achieved_goal`: position\n",
    "- `desired_goal`: target position\n",
    "\n",
    "The reward is sparse: $0$ if the point is within a threshold of the goal, else $-1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointReachGoalEnv(gym.Env):\n",
    "    \"\"\"A minimal goal-based env compatible with HER-style replay buffers.\n",
    "\n",
    "    Observation is a dict with keys:\n",
    "      - observation: (pos_x, pos_y, vel_x, vel_y)\n",
    "      - achieved_goal: (pos_x, pos_y)\n",
    "      - desired_goal: (goal_x, goal_y)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_steps: int = 50,\n",
    "        dt: float = 0.05,\n",
    "        goal_range: float = 1.0,\n",
    "        max_speed: float = 1.0,\n",
    "        distance_threshold: float = 0.05,\n",
    "        action_scale: float = 10.0,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_steps = int(max_steps)\n",
    "        self.dt = float(dt)\n",
    "        self.goal_range = float(goal_range)\n",
    "        self.max_speed = float(max_speed)\n",
    "        self.distance_threshold = float(distance_threshold)\n",
    "        self.action_scale = float(action_scale)\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        # actions are accelerations, clipped to [-1, 1]\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
    "        goal_space = spaces.Box(low=-self.goal_range, high=self.goal_range, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"observation\": obs_space,\n",
    "                \"achieved_goal\": goal_space,\n",
    "                \"desired_goal\": goal_space,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self._t = 0\n",
    "        self._pos = np.zeros(2, dtype=np.float32)\n",
    "        self._vel = np.zeros(2, dtype=np.float32)\n",
    "        self._goal = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "    def _sample_goal(self) -> np.ndarray:\n",
    "        return self._rng.uniform(-self.goal_range, self.goal_range, size=(2,)).astype(np.float32)\n",
    "\n",
    "    def _get_obs(self) -> Dict[str, np.ndarray]:\n",
    "        obs = np.concatenate([self._pos, self._vel]).astype(np.float32)\n",
    "        return {\n",
    "            \"observation\": obs,\n",
    "            \"achieved_goal\": self._pos.copy(),\n",
    "            \"desired_goal\": self._goal.copy(),\n",
    "        }\n",
    "\n",
    "    def compute_reward(self, achieved_goal: np.ndarray, desired_goal: np.ndarray, info=None) -> np.ndarray:\n",
    "        \"\"\"Vectorized sparse reward: 0 if within threshold else -1.\"\"\"\n",
    "        achieved_goal = np.asarray(achieved_goal, dtype=np.float32)\n",
    "        desired_goal = np.asarray(desired_goal, dtype=np.float32)\n",
    "        d = np.linalg.norm(achieved_goal - desired_goal, axis=-1)\n",
    "        return -(d > self.distance_threshold).astype(np.float32)\n",
    "\n",
    "    def compute_success(self, achieved_goal: np.ndarray, desired_goal: np.ndarray) -> np.ndarray:\n",
    "        achieved_goal = np.asarray(achieved_goal, dtype=np.float32)\n",
    "        desired_goal = np.asarray(desired_goal, dtype=np.float32)\n",
    "        d = np.linalg.norm(achieved_goal - desired_goal, axis=-1)\n",
    "        return (d <= self.distance_threshold).astype(np.float32)\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        self._t = 0\n",
    "        self._pos = self._rng.uniform(-self.goal_range, self.goal_range, size=(2,)).astype(np.float32)\n",
    "        self._vel = np.zeros(2, dtype=np.float32)\n",
    "        self._goal = self._sample_goal()\n",
    "        obs = self._get_obs()\n",
    "        info = {\"is_success\": float(self.compute_success(obs[\"achieved_goal\"], obs[\"desired_goal\"]))}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        self._t += 1\n",
    "\n",
    "        action = np.asarray(action, dtype=np.float32)\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        accel = self.action_scale * action\n",
    "\n",
    "        self._vel = np.clip(self._vel + accel * self.dt, -self.max_speed, self.max_speed)\n",
    "        self._pos = np.clip(self._pos + self._vel * self.dt, -self.goal_range, self.goal_range)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        reward = float(self.compute_reward(obs[\"achieved_goal\"], obs[\"desired_goal\"]))\n",
    "        terminated = False\n",
    "        truncated = self._t >= self.max_steps\n",
    "        info = {\"is_success\": float(self.compute_success(obs[\"achieved_goal\"], obs[\"desired_goal\"]))}\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e41331",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointReachGoalEnv(max_steps=50, seed=SEED)\n",
    "obs, info = env.reset()\n",
    "print(obs.keys(), info)\n",
    "print('observation shape:', obs['observation'].shape)\n",
    "print('achieved_goal shape:', obs['achieved_goal'].shape)\n",
    "print('desired_goal shape:', obs['desired_goal'].shape)\n",
    "\n",
    "a = env.action_space.sample()\n",
    "obs2, r, terminated, truncated, info2 = env.step(a)\n",
    "print('reward:', r, 'done:', terminated or truncated, 'success:', info2['is_success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0d703",
   "metadata": {},
   "source": [
    "## DDPG (goal-conditioned)\n",
    "\n",
    "We use a standard DDPG setup:\n",
    "- **Actor** $\\mu_\\theta(x, g)$ outputs a deterministic action.\n",
    "- **Critic** $Q_\\phi(x, g, a)$ estimates the action-value.\n",
    "- Target networks $(\\theta', \\phi')$ are updated with Polyak averaging.\n",
    "\n",
    "The only goal-conditioning detail is the network input: we concatenate the observation with the goal.\n",
    "$$s_t := [x_t \\;\\|\\; g]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes: List[int], activation: nn.Module, output_activation: nn.Module | None = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        act = activation if i < len(sizes) - 2 else (output_activation or nn.Identity())\n",
    "        layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        layers.append(act)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp([state_dim, *hidden_sizes, action_dim], activation=nn.ReLU(), output_activation=nn.Tanh())\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp([state_dim + action_dim, *hidden_sizes, 1], activation=nn.ReLU(), output_activation=None)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "def soft_update(target: nn.Module, online: nn.Module, tau: float) -> None:\n",
    "    with torch.no_grad():\n",
    "        for tp, op in zip(target.parameters(), online.parameters(), strict=True):\n",
    "            tp.data.mul_(1.0 - tau).add_(op.data, alpha=tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d3f1b",
   "metadata": {},
   "source": [
    "## HER replay buffer (episodic + relabel-at-sample)\n",
    "\n",
    "We store full episodes so we can sample **future achieved goals** for relabeling.\n",
    "\n",
    "When sampling a batch, each transition is relabeled with probability $p_{\\mathrm{HER}} = k/(k+1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd692a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HERConfig:\n",
    "    buffer_capacity_transitions: int = 100_000\n",
    "    n_sampled_goal: int = 4\n",
    "    goal_selection_strategy: str = \"future\"  # future | final | episode\n",
    "\n",
    "\n",
    "class HindsightReplayBuffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: HERConfig,\n",
    "        compute_reward_fn: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.compute_reward_fn = compute_reward_fn\n",
    "        self.episodes: List[Dict[str, np.ndarray]] = []\n",
    "        self.n_transitions = 0\n",
    "\n",
    "        if self.cfg.goal_selection_strategy not in {\"future\", \"final\", \"episode\"}:\n",
    "            raise ValueError(f\"Unknown goal_selection_strategy: {self.cfg.goal_selection_strategy}\")\n",
    "\n",
    "    @property\n",
    "    def her_probability(self) -> float:\n",
    "        k = max(int(self.cfg.n_sampled_goal), 0)\n",
    "        return 0.0 if k == 0 else k / (k + 1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(self.n_transitions)\n",
    "\n",
    "    def add_episode(self, episode: Dict[str, np.ndarray]) -> None:\n",
    "        # expected keys: obs, actions, next_obs, achieved_goal, next_achieved_goal, desired_goal, dones\n",
    "        T = int(len(episode[\"actions\"]))\n",
    "        if T == 0:\n",
    "            return\n",
    "\n",
    "        self.episodes.append(episode)\n",
    "        self.n_transitions += T\n",
    "\n",
    "        while self.n_transitions > self.cfg.buffer_capacity_transitions and self.episodes:\n",
    "            removed = self.episodes.pop(0)\n",
    "            self.n_transitions -= int(len(removed[\"actions\"]))\n",
    "\n",
    "    def sample(self, batch_size: int, rng: np.random.Generator) -> Dict[str, np.ndarray]:\n",
    "        if not self.episodes:\n",
    "            raise RuntimeError(\"Replay buffer is empty\")\n",
    "\n",
    "        batch_size = int(batch_size)\n",
    "        ep_indices = rng.integers(0, len(self.episodes), size=(batch_size,))\n",
    "\n",
    "        obs_batch = []\n",
    "        next_obs_batch = []\n",
    "        act_batch = []\n",
    "        done_batch = []\n",
    "        goal_batch = []\n",
    "        next_achieved_batch = []\n",
    "        achieved_batch = []\n",
    "        t_batch = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            ep = self.episodes[int(ep_indices[i])]\n",
    "            T = int(len(ep[\"actions\"]))\n",
    "            t = int(rng.integers(0, T))\n",
    "\n",
    "            obs_batch.append(ep[\"obs\"][t])\n",
    "            next_obs_batch.append(ep[\"next_obs\"][t])\n",
    "            act_batch.append(ep[\"actions\"][t])\n",
    "            achieved_batch.append(ep[\"achieved_goal\"][t])\n",
    "            next_achieved_batch.append(ep[\"next_achieved_goal\"][t])\n",
    "            goal_batch.append(ep[\"desired_goal\"][t])\n",
    "            done_batch.append(ep[\"dones\"][t])\n",
    "            t_batch.append(t)\n",
    "\n",
    "        obs = np.asarray(obs_batch, dtype=np.float32)\n",
    "        next_obs = np.asarray(next_obs_batch, dtype=np.float32)\n",
    "        actions = np.asarray(act_batch, dtype=np.float32)\n",
    "        achieved_goal = np.asarray(achieved_batch, dtype=np.float32)\n",
    "        next_achieved_goal = np.asarray(next_achieved_batch, dtype=np.float32)\n",
    "        desired_goal = np.asarray(goal_batch, dtype=np.float32)\n",
    "        dones = np.asarray(done_batch, dtype=np.float32).reshape(-1, 1)\n",
    "        t_idx = np.asarray(t_batch, dtype=np.int64)\n",
    "\n",
    "        # base rewards (under original goal)\n",
    "        rewards = self.compute_reward_fn(next_achieved_goal, desired_goal).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "        # HER relabeling\n",
    "        her_mask = rng.random(size=(batch_size,)) < self.her_probability\n",
    "        if np.any(her_mask):\n",
    "            for i in np.where(her_mask)[0]:\n",
    "                ep = self.episodes[int(ep_indices[int(i)])]\n",
    "                T = int(len(ep[\"actions\"]))\n",
    "                t = int(t_idx[int(i)])\n",
    "\n",
    "                if self.cfg.goal_selection_strategy == \"future\":\n",
    "                    future_t = int(rng.integers(t, T))\n",
    "                elif self.cfg.goal_selection_strategy == \"final\":\n",
    "                    future_t = T - 1\n",
    "                elif self.cfg.goal_selection_strategy == \"episode\":\n",
    "                    future_t = int(rng.integers(0, T))\n",
    "                else:\n",
    "                    raise RuntimeError(\"unreachable\")\n",
    "\n",
    "                g_her = ep[\"next_achieved_goal\"][future_t].astype(np.float32)\n",
    "                desired_goal[int(i)] = g_her\n",
    "                rewards[int(i)] = self.compute_reward_fn(next_achieved_goal[int(i)], g_her).astype(np.float32)\n",
    "\n",
    "        states = np.concatenate([obs, desired_goal], axis=-1)\n",
    "        next_states = np.concatenate([next_obs, desired_goal], axis=-1)\n",
    "\n",
    "        return {\n",
    "            \"states\": states,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"next_states\": next_states,\n",
    "            \"dones\": dones,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c7d06",
   "metadata": {},
   "source": [
    "## DDPG agent (low-level PyTorch)\n",
    "\n",
    "This is a minimal DDPG implementation:\n",
    "- MSE critic loss against a bootstrapped target.\n",
    "- Deterministic policy gradient for the actor.\n",
    "- Soft updates for target networks.\n",
    "\n",
    "Exploration is done by adding Gaussian noise to the actor output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7126f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DDPGConfig:\n",
    "    gamma: float = 0.98\n",
    "    tau: float = 0.005\n",
    "    actor_lr: float = 1e-3\n",
    "    critic_lr: float = 1e-3\n",
    "    hidden_sizes: tuple[int, int] = (256, 256)\n",
    "    batch_size: int = 256\n",
    "    start_learning_after: int = 2_000  # transitions in buffer\n",
    "    exploration_noise_std: float = 0.2\n",
    "    grad_clip_norm: float | None = 1.0\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim: int, action_dim: int, cfg: DDPGConfig, device: torch.device):\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_sizes=self.cfg.hidden_sizes).to(device)\n",
    "        self.critic = Critic(state_dim, action_dim, hidden_sizes=self.cfg.hidden_sizes).to(device)\n",
    "\n",
    "        self.actor_targ = Actor(state_dim, action_dim, hidden_sizes=self.cfg.hidden_sizes).to(device)\n",
    "        self.critic_targ = Critic(state_dim, action_dim, hidden_sizes=self.cfg.hidden_sizes).to(device)\n",
    "        self.actor_targ.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_targ.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=self.cfg.actor_lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=self.cfg.critic_lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray, rng: np.random.Generator, noise_std: float) -> np.ndarray:\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        a = self.actor(state_t).cpu().numpy()[0]\n",
    "        a = a + rng.normal(0.0, noise_std, size=a.shape).astype(np.float32)\n",
    "        return np.clip(a, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "    def update(self, batch: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
    "        s = torch.as_tensor(batch[\"states\"], dtype=torch.float32, device=self.device)\n",
    "        a = torch.as_tensor(batch[\"actions\"], dtype=torch.float32, device=self.device)\n",
    "        r = torch.as_tensor(batch[\"rewards\"], dtype=torch.float32, device=self.device)\n",
    "        s2 = torch.as_tensor(batch[\"next_states\"], dtype=torch.float32, device=self.device)\n",
    "        d = torch.as_tensor(batch[\"dones\"], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # --- critic update ---\n",
    "        with torch.no_grad():\n",
    "            a2 = self.actor_targ(s2)\n",
    "            q_targ = self.critic_targ(s2, a2)\n",
    "            y = r + self.cfg.gamma * (1.0 - d) * q_targ\n",
    "\n",
    "        q = self.critic(s, a)\n",
    "        critic_loss = F.mse_loss(q, y)\n",
    "        self.critic_opt.zero_grad(set_to_none=True)\n",
    "        critic_loss.backward()\n",
    "        if self.cfg.grad_clip_norm is not None:\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=float(self.cfg.grad_clip_norm))\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        # --- actor update ---\n",
    "        a_pi = self.actor(s)\n",
    "        actor_loss = -self.critic(s, a_pi).mean()\n",
    "        self.actor_opt.zero_grad(set_to_none=True)\n",
    "        actor_loss.backward()\n",
    "        if self.cfg.grad_clip_norm is not None:\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=float(self.cfg.grad_clip_norm))\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        # targets\n",
    "        soft_update(self.actor_targ, self.actor, tau=self.cfg.tau)\n",
    "        soft_update(self.critic_targ, self.critic, tau=self.cfg.tau)\n",
    "\n",
    "        return {\n",
    "            \"critic_loss\": float(critic_loss.item()),\n",
    "            \"actor_loss\": float(actor_loss.item()),\n",
    "            \"q_mean\": float(q.mean().item()),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ba3c3",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "We will:\n",
    "1. collect one full episode,\n",
    "2. add it to the episodic replay buffer,\n",
    "3. run a number of gradient updates.\n",
    "\n",
    "We log per-episode reward and success, and per-update losses/Q-values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    n_episodes: int = 150\n",
    "    updates_per_episode: int = 50\n",
    "\n",
    "    env_max_steps: int = 50\n",
    "    env_distance_threshold: float = 0.05\n",
    "    env_dt: float = 0.05\n",
    "    env_goal_range: float = 1.0\n",
    "    env_max_speed: float = 1.0\n",
    "    env_action_scale: float = 10.0\n",
    "\n",
    "    her: HERConfig = field(default_factory=HERConfig)\n",
    "    ddpg: DDPGConfig = field(default_factory=DDPGConfig)\n",
    "\n",
    "\n",
    "cfg = TrainConfig(\n",
    "    n_episodes=150,\n",
    "    updates_per_episode=50,\n",
    "    env_max_steps=50,\n",
    "    env_distance_threshold=0.05,\n",
    "    env_dt=0.05,\n",
    "    env_goal_range=1.0,\n",
    "    env_max_speed=1.0,\n",
    "    env_action_scale=10.0,\n",
    "    her=HERConfig(buffer_capacity_transitions=100_000, n_sampled_goal=4, goal_selection_strategy=\"future\"),\n",
    "    ddpg=DDPGConfig(\n",
    "        gamma=0.98,\n",
    "        tau=0.005,\n",
    "        actor_lr=1e-3,\n",
    "        critic_lr=1e-3,\n",
    "        batch_size=256,\n",
    "        start_learning_after=2_000,\n",
    "        exploration_noise_std=0.2,\n",
    "        grad_clip_norm=1.0,\n",
    "    ),\n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointReachGoalEnv(\n",
    "    max_steps=cfg.env_max_steps,\n",
    "    distance_threshold=cfg.env_distance_threshold,\n",
    "    dt=cfg.env_dt,\n",
    "    goal_range=cfg.env_goal_range,\n",
    "    max_speed=cfg.env_max_speed,\n",
    "    action_scale=cfg.env_action_scale,\n",
    "    seed=SEED,\n",
    ")\n",
    "obs_dim = int(env.observation_space[\"observation\"].shape[0])\n",
    "goal_dim = int(env.observation_space[\"desired_goal\"].shape[0])\n",
    "act_dim = int(env.action_space.shape[0])\n",
    "state_dim = obs_dim + goal_dim\n",
    "\n",
    "buffer = HindsightReplayBuffer(cfg.her, compute_reward_fn=lambda ag, g: env.compute_reward(ag, g))\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=act_dim, cfg=cfg.ddpg, device=DEVICE)\n",
    "\n",
    "episode_returns: List[float] = []\n",
    "episode_success: List[float] = []\n",
    "episode_final_dist: List[float] = []\n",
    "\n",
    "update_logs: List[Dict[str, float]] = []\n",
    "\n",
    "t0 = time.time()\n",
    "for ep in range(cfg.n_episodes):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    ep_obs = []\n",
    "    ep_next_obs = []\n",
    "    ep_actions = []\n",
    "    ep_achieved = []\n",
    "    ep_next_achieved = []\n",
    "    ep_desired = []\n",
    "    ep_dones = []\n",
    "    ep_rewards = []\n",
    "\n",
    "    for t in range(cfg.env_max_steps):\n",
    "        state = np.concatenate([obs[\"observation\"], obs[\"desired_goal\"]], axis=-1).astype(np.float32)\n",
    "        action = agent.act(state, rng=rng, noise_std=cfg.ddpg.exploration_noise_std)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = float(terminated or truncated)\n",
    "\n",
    "        ep_obs.append(obs[\"observation\"].copy())\n",
    "        ep_next_obs.append(next_obs[\"observation\"].copy())\n",
    "        ep_actions.append(action.copy())\n",
    "        ep_achieved.append(obs[\"achieved_goal\"].copy())\n",
    "        ep_next_achieved.append(next_obs[\"achieved_goal\"].copy())\n",
    "        ep_desired.append(obs[\"desired_goal\"].copy())\n",
    "        ep_dones.append(done)\n",
    "        ep_rewards.append(float(reward))\n",
    "\n",
    "        obs = next_obs\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # add episode to buffer\n",
    "    episode = {\n",
    "        \"obs\": np.asarray(ep_obs, dtype=np.float32),\n",
    "        \"next_obs\": np.asarray(ep_next_obs, dtype=np.float32),\n",
    "        \"actions\": np.asarray(ep_actions, dtype=np.float32),\n",
    "        \"achieved_goal\": np.asarray(ep_achieved, dtype=np.float32),\n",
    "        \"next_achieved_goal\": np.asarray(ep_next_achieved, dtype=np.float32),\n",
    "        \"desired_goal\": np.asarray(ep_desired, dtype=np.float32),\n",
    "        \"dones\": np.asarray(ep_dones, dtype=np.float32),\n",
    "    }\n",
    "    buffer.add_episode(episode)\n",
    "\n",
    "    ep_return = float(np.sum(ep_rewards))\n",
    "    ep_success = float(info.get(\"is_success\", 0.0))\n",
    "    final_dist = float(np.linalg.norm(obs[\"achieved_goal\"] - obs[\"desired_goal\"]))\n",
    "    episode_returns.append(ep_return)\n",
    "    episode_success.append(ep_success)\n",
    "    episode_final_dist.append(final_dist)\n",
    "\n",
    "    # updates\n",
    "    if len(buffer) >= cfg.ddpg.start_learning_after:\n",
    "        for _ in range(cfg.updates_per_episode):\n",
    "            batch = buffer.sample(cfg.ddpg.batch_size, rng=rng)\n",
    "            stats = agent.update(batch)\n",
    "            stats[\"episode\"] = float(ep)\n",
    "            update_logs.append(stats)\n",
    "\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        sr = float(np.mean(episode_success[-25:]))\n",
    "        print(\n",
    "            f\"ep {ep+1:4d}/{cfg.n_episodes} | \"\n",
    "            f\"avg_success(last25)={sr:.2f} | \"\n",
    "            f\"buffer={len(buffer):6d} | \"\n",
    "            f\"elapsed={time.time()-t0:.1f}s\"\n",
    "        )\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fa0b4",
   "metadata": {},
   "source": [
    "## Plot reward and success per episode (Plotly)\n",
    "\n",
    "We visualize:\n",
    "- episode return (sum of sparse rewards)\n",
    "- episode success (0/1) and a moving average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ep = pd.DataFrame(\n",
    "    {\n",
    "        \"episode\": np.arange(len(episode_returns)),\n",
    "        \"return\": episode_returns,\n",
    "        \"success\": episode_success,\n",
    "        \"final_dist\": episode_final_dist,\n",
    "    }\n",
    ")\n",
    "df_ep[\"success_ma_10\"] = df_ep[\"success\"].rolling(10, min_periods=1).mean()\n",
    "df_ep[\"return_ma_10\"] = df_ep[\"return\"].rolling(10, min_periods=1).mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_ep[\"episode\"], y=df_ep[\"return\"], mode=\"lines\", name=\"return\"))\n",
    "fig.add_trace(go.Scatter(x=df_ep[\"episode\"], y=df_ep[\"return_ma_10\"], mode=\"lines\", name=\"return (ma10)\"))\n",
    "fig.update_layout(title=\"Episode return (sparse)\", xaxis_title=\"episode\", yaxis_title=\"return\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_ep[\"episode\"], y=df_ep[\"success\"], mode=\"markers\", name=\"success\"))\n",
    "fig.add_trace(go.Scatter(x=df_ep[\"episode\"], y=df_ep[\"success_ma_10\"], mode=\"lines\", name=\"success (ma10)\"))\n",
    "fig.update_layout(title=\"Success per episode\", xaxis_title=\"episode\", yaxis_title=\"success\", yaxis=dict(range=[-0.05, 1.05]))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719c2ad",
   "metadata": {},
   "source": [
    "## Plot learning signals (losses and Q-values)\n",
    "\n",
    "Loss curves are noisy in RL, but you should still see them stabilize as learning progresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up = pd.DataFrame(update_logs)\n",
    "if len(df_up) == 0:\n",
    "    print(\"No updates ran yet (buffer too small). Try increasing n_episodes or lowering start_learning_after.\")\n",
    "else:\n",
    "    df_up[\"update\"] = np.arange(len(df_up))\n",
    "    df_up[\"critic_loss_ma_200\"] = df_up[\"critic_loss\"].rolling(200, min_periods=1).mean()\n",
    "    df_up[\"actor_loss_ma_200\"] = df_up[\"actor_loss\"].rolling(200, min_periods=1).mean()\n",
    "    df_up[\"q_mean_ma_200\"] = df_up[\"q_mean\"].rolling(200, min_periods=1).mean()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df_up[\"update\"], y=df_up[\"critic_loss\"], mode=\"lines\", name=\"critic_loss\"))\n",
    "    fig.add_trace(go.Scatter(x=df_up[\"update\"], y=df_up[\"critic_loss_ma_200\"], mode=\"lines\", name=\"critic_loss (ma200)\"))\n",
    "    fig.update_layout(title=\"Critic loss\", xaxis_title=\"update\", yaxis_title=\"MSE\")\n",
    "    fig.show()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df_up[\"update\"], y=df_up[\"actor_loss\"], mode=\"lines\", name=\"actor_loss\"))\n",
    "    fig.add_trace(go.Scatter(x=df_up[\"update\"], y=df_up[\"actor_loss_ma_200\"], mode=\"lines\", name=\"actor_loss (ma200)\"))\n",
    "    fig.update_layout(title=\"Actor loss\", xaxis_title=\"update\", yaxis_title=\"-Q(s, pi(s))\")\n",
    "    fig.show()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df_up[\"update\"], y=df_up[\"q_mean\"], mode=\"lines\", name=\"Q mean\"))\n",
    "    fig.add_trace(go.Scatter(x=df_up[\"update\"], y=df_up[\"q_mean_ma_200\"], mode=\"lines\", name=\"Q mean (ma200)\"))\n",
    "    fig.update_layout(title=\"Average Q-value\", xaxis_title=\"update\", yaxis_title=\"Q\")\n",
    "    fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b5b91",
   "metadata": {},
   "source": [
    "## Stable-Baselines / Stable-Baselines3 (web-researched equivalents)\n",
    "\n",
    "Stable-Baselines has an explicit `HER` wrapper class. Stable-Baselines3 moved HER into the replay buffer: use `HerReplayBuffer` with an off-policy algorithm.\n",
    "\n",
    "References:\n",
    "- HER paper: https://arxiv.org/abs/1707.01495\n",
    "- SB3 HER docs: https://github.com/dlr-rm/stable-baselines3/blob/master/docs/modules/her.rst\n",
    "- Stable-Baselines HER docs: https://github.com/stable-baselines-team/stable-baselines/blob/master/docs/modules/her.rst\n",
    "\n",
    "### Stable-Baselines3 (SB3)\n",
    "From the SB3 docs (`docs/modules/her.rst`):\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import HerReplayBuffer, DDPG, DQN, SAC, TD3\n",
    "from stable_baselines3.common.envs import BitFlippingEnv\n",
    "\n",
    "model_class = DQN  # works also with SAC, DDPG and TD3\n",
    "env = BitFlippingEnv(n_bits=15, continuous=model_class in [DDPG, SAC, TD3], max_steps=15)\n",
    "\n",
    "model = model_class(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=4,\n",
    "        goal_selection_strategy=\"future\",\n",
    "    ),\n",
    "    verbose=1,\n",
    ")\n",
    "model.learn(1000)\n",
    "```\n",
    "\n",
    "Using the environment from this notebook (SB3):\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import HerReplayBuffer, SAC\n",
    "\n",
    "env = PointReachGoalEnv(max_steps=50)\n",
    "model = SAC(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(n_sampled_goal=4, goal_selection_strategy=\"future\"),\n",
    ")\n",
    "model.learn(100_000)\n",
    "```\n",
    "\n",
    "### Stable-Baselines (SB)\n",
    "From the Stable-Baselines docs (`docs/modules/her.rst`):\n",
    "\n",
    "```python\n",
    "from stable_baselines import HER, DQN, SAC, DDPG, TD3\n",
    "from stable_baselines.common.bit_flipping_env import BitFlippingEnv\n",
    "\n",
    "model_class = DQN  # works also with SAC, DDPG and TD3\n",
    "env = BitFlippingEnv(15, continuous=model_class in [DDPG, SAC, TD3], max_steps=15)\n",
    "\n",
    "model = HER(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    model_class,\n",
    "    n_sampled_goal=4,\n",
    "    goal_selection_strategy=\"future\",\n",
    "    verbose=1,\n",
    ")\n",
    "model.learn(1000)\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- both libraries assume the environment exposes `compute_reward(achieved_goal, desired_goal, info)`.\n",
    "- for dict observations, SB3 uses `\"MultiInputPolicy\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6f6b8",
   "metadata": {},
   "source": [
    "## Hyperparameters (explained)\n",
    "\n",
    "This section explains every hyperparameter used above.\n",
    "\n",
    "### Environment\n",
    "- `env_max_steps`: episode horizon (more steps = easier exploration, but longer credit assignment).\n",
    "- `env_distance_threshold`: success tolerance (smaller = harder, sparser successes).\n",
    "- `env_dt`: simulation timestep (smaller = smoother dynamics, but harder long-horizon control).\n",
    "- `env_goal_range`: coordinate bounds for start/goal sampling.\n",
    "- `env_max_speed`: velocity clipping (too low can make goals unreachable within the horizon).\n",
    "- `env_action_scale`: converts normalized actions in $[-1, 1]$ into acceleration.\n",
    "\n",
    "### HER\n",
    "- `buffer_capacity_transitions`: maximum number of stored transitions (in episodes).\n",
    "- `n_sampled_goal`: number of hindsight goals per real transition; larger means stronger HER signal but more bias toward hindsight goals.\n",
    "- `goal_selection_strategy`:\n",
    "  - `future`: sample a goal from a future achieved goal in the same episode (most common).\n",
    "  - `final`: always use the final achieved goal.\n",
    "  - `episode`: sample an achieved goal uniformly from the whole episode.\n",
    "\n",
    "### DDPG\n",
    "- `gamma`: discount factor (higher = longer-horizon planning).\n",
    "- `tau`: target-network Polyak averaging rate (smaller = more stable, slower tracking).\n",
    "- `actor_lr`, `critic_lr`: learning rates.\n",
    "- `hidden_sizes`: MLP hidden layer sizes for both actor and critic.\n",
    "- `batch_size`: SGD batch size from replay buffer.\n",
    "- `start_learning_after`: number of transitions before updates begin (stabilizes early learning).\n",
    "- `exploration_noise_std`: Gaussian action noise scale for exploration.\n",
    "- `grad_clip_norm`: gradient norm clipping to reduce instability.\n",
    "\n",
    "### Training\n",
    "- `n_episodes`: total training episodes.\n",
    "- `updates_per_episode`: gradient steps after each collected episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "    # env\n",
    "    \"env_max_steps\": cfg.env_max_steps,\n",
    "    \"env_distance_threshold\": cfg.env_distance_threshold,\n",
    "    \"env_dt\": cfg.env_dt,\n",
    "    \"env_goal_range\": cfg.env_goal_range,\n",
    "    \"env_max_speed\": cfg.env_max_speed,\n",
    "    \"env_action_scale\": cfg.env_action_scale,\n",
    "    # her\n",
    "    **{f\"her.{k}\": v for k, v in asdict(cfg.her).items()},\n",
    "    # ddpg\n",
    "    **{f\"ddpg.{k}\": v for k, v in asdict(cfg.ddpg).items()},\n",
    "    # train\n",
    "    \"train.n_episodes\": cfg.n_episodes,\n",
    "    \"train.updates_per_episode\": cfg.updates_per_episode,\n",
    "}\n",
    "pd.DataFrame({\"hyperparameter\": list(hp.keys()), \"value\": list(hp.values())})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}