{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9d0a4d",
   "metadata": {},
   "source": [
    "# Deep Q-Networks (DQN) for Discrete Action Spaces — Low-Level PyTorch\n",
    "\n",
    "DQN combines **Q-learning** with a neural network to approximate an action-value function $Q_\\theta(s, a)$ when actions are **discrete**.\n",
    "\n",
    "## What you’ll learn\n",
    "- the Bellman expectation and optimality equations (precisely, with LaTeX)\n",
    "- why experience replay stabilizes learning, and how to implement it\n",
    "- a minimal DQN in PyTorch: replay buffer, target network, $\\epsilon$-greedy exploration\n",
    "- Plotly diagnostics: reward per episode, TD loss, and learned $Q$-values\n",
    "- a Stable-Baselines3 DQN reference + hyperparameter explanations (end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbedf0",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) RL notation + Bellman equations\n",
    "2) DQN targets + loss\n",
    "3) Experience replay (precise)\n",
    "4) Target network updates\n",
    "5) Low-level PyTorch implementation (from scratch)\n",
    "6) Train on a tiny toy environment (no extra RL dependencies)\n",
    "7) Plotly diagnostics: reward per episode, loss, and $Q$-values\n",
    "8) Stable-Baselines3 DQN reference + hyperparameters (end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b40e8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- basic probability and expectation\n",
    "- comfort with vectors/matrices and simple neural nets\n",
    "- familiarity with PyTorch tensors and optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb9e1f",
   "metadata": {},
   "source": [
    "## 1) Setup: MDPs and returns\n",
    "\n",
    "We assume an episodic Markov Decision Process (MDP):\n",
    "\n",
    "$$\n",
    "(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)\n",
    "$$\n",
    "\n",
    "- states $s \\in \\mathcal{S}$\n",
    "- actions $a \\in \\mathcal{A}$ (here: **discrete**)\n",
    "- transition kernel $P(s' \\mid s, a)$\n",
    "- reward $r_{t+1}$ after taking action $a_t$ in state $s_t$\n",
    "- discount $\\gamma \\in [0, 1)$\n",
    "\n",
    "The (discounted) return from time $t$ is\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17557c",
   "metadata": {},
   "source": [
    "## 2) Action-value functions\n",
    "\n",
    "For a policy $\\pi(a\\mid s)$, the action-value function is\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi}\\left[ G_t \\mid s_t = s,\\, a_t = a \\right]\n",
    "$$\n",
    "\n",
    "If actions are discrete, we can represent the action-values as a vector\n",
    "\n",
    "$$\n",
    "Q_\\theta(s, \\cdot) \\in \\mathbb{R}^{|\\mathcal{A}|}\n",
    "$$\n",
    "\n",
    "and pick greedy actions via\n",
    "\n",
    "$$\n",
    "a^*(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_\\theta(s,a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e804c9",
   "metadata": {},
   "source": [
    "## 3) Bellman equations (precise)\n",
    "\n",
    "### 3.1 Bellman expectation equation\n",
    "\n",
    "Under policy $\\pi$, the Bellman expectation equation for $Q^{\\pi}$ is\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s,a)\n",
    "= \\mathbb{E}\\_{s' \\sim P(\\cdot \\mid s,a),\\, a' \\sim \\pi(\\cdot \\mid s')}\n",
    "\\left[ r_{t+1} + \\gamma \\, Q^{\\pi}(s', a') \\right]\n",
    "$$\n",
    "\n",
    "### 3.2 Bellman optimality equation\n",
    "\n",
    "Define the optimal action-value function\n",
    "\n",
    "$$\n",
    "Q^*(s,a) = \\max_{\\pi} Q^{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "Then $Q^*$ satisfies\n",
    "\n",
    "$$\n",
    "Q^*(s,a)\n",
    "= \\mathbb{E}\\_{s' \\sim P(\\cdot \\mid s,a)}\n",
    "\\left[ r_{t+1} + \\gamma \\, \\max_{a' \\in \\mathcal{A}} Q^*(s', a') \\right]\n",
    "$$\n",
    "\n",
    "### 3.3 Tabular Q-learning update (intuition)\n",
    "\n",
    "Q-learning performs stochastic approximation to the Bellman optimality equation:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big( r_{t+1} + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\Big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6daaa",
   "metadata": {},
   "source": [
    "## 4) From Q-learning to DQN (targets + loss)\n",
    "\n",
    "When $\\mathcal{S}$ is large or continuous, we approximate action-values with a neural network $Q_\\theta(s,a)$.\n",
    "\n",
    "Given a transition $(s, a, r, s', d)$ where $d \\in \\{0,1\\}$ indicates a **terminal** transition (1 if episode ended due to an environment terminal condition), DQN uses the TD target\n",
    "\n",
    "$$\n",
    "y = r + \\gamma (1-d) \\max_{a'} Q_{\\theta^-}(s', a')\n",
    "$$\n",
    "\n",
    "where $\\theta^-$ are parameters of a **target network** (a delayed copy of $\\theta$).\n",
    "\n",
    "We learn $\\theta$ by minimizing a TD regression loss over replayed transitions:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}\\_{(s,a,r,s',d) \\sim \\mathcal{D}}\\Big[ \\ell\\big(Q_\\theta(s,a),\\, y\\big) \\Big]\n",
    "$$\n",
    "\n",
    "Two common choices for $\\ell$:\n",
    "\n",
    "- squared loss: $\\ell(u,v) = (u-v)^2$\n",
    "- Huber loss (Smooth L1), more robust to large TD errors:\n",
    "\n",
    "$$\n",
    "\\ell_\\delta(u,v)=\n",
    "\\begin{cases}\n",
    "\\frac12 (u-v)^2 & \\text{if } |u-v| \\le \\delta \\\\\n",
    "\\delta\\left(|u-v| - \\frac12\\delta\\right) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a175ed7",
   "metadata": {},
   "source": [
    "## 5) Experience replay (precisely)\n",
    "\n",
    "DQN is **off-policy**: we generate experience using a behavior policy (typically $\\epsilon$-greedy) while learning the greedy $Q$-function.\n",
    "\n",
    "We store transitions in a replay buffer:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(s_i, a_i, r_i, s'_i, d_i)\\}_{i=1}^{N}\n",
    "$$\n",
    "\n",
    "At each gradient step we sample an (approximately) i.i.d. mini-batch uniformly:\n",
    "\n",
    "$$\n",
    "(s_j, a_j, r_j, s'_j, d_j) \\sim U(\\mathcal{D}), \\quad j=1,\\dots,B\n",
    "$$\n",
    "\n",
    "Why this helps:\n",
    "\n",
    "1. **Temporal decorrelation**: online trajectories yield highly correlated samples; replay makes SGD closer to its i.i.d. assumptions.\n",
    "2. **Data reuse**: each transition can be used for many gradient steps, improving sample-efficiency.\n",
    "3. **Stabilization**: mixing older and newer experience reduces non-stationarity of the training distribution.\n",
    "\n",
    "We use *uniform* replay here for clarity (prioritized replay is a common extension).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455014e",
   "metadata": {},
   "source": [
    "## 6) Target network updates\n",
    "\n",
    "A moving target $y$ can destabilize training. DQN stabilizes learning by using a target network $Q_{\\theta^-}$.\n",
    "\n",
    "Two standard update schemes:\n",
    "\n",
    "- **hard update** every $C$ steps:\n",
    "\n",
    "$$\n",
    "\\theta^- \\leftarrow \\theta\n",
    "$$\n",
    "\n",
    "- **soft / Polyak update**:\n",
    "\n",
    "$$\n",
    "\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-\n",
    "$$\n",
    "\n",
    "In the implementation below, setting $\\tau=1$ with periodic updates is a hard update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f3b2e",
   "metadata": {},
   "source": [
    "## 7) Low-level PyTorch implementation (from scratch)\n",
    "\n",
    "We implement:\n",
    "- a tiny environment (so the notebook runs without external RL dependencies)\n",
    "- replay buffer with uniform sampling\n",
    "- an MLP $Q$-network\n",
    "- the DQN loop with $\\epsilon$ scheduling + target network updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DQNConfig:\n",
    "    gamma: float = 0.99\n",
    "    learning_rate: float = 1e-3\n",
    "    buffer_size: int = 50_000\n",
    "    batch_size: int = 64\n",
    "    learning_starts: int = 1_000\n",
    "    train_freq: int = 1\n",
    "    gradient_steps: int = 1\n",
    "    target_update_interval: int = 500\n",
    "    tau: float = 1.0\n",
    "    eps_start: float = 1.0\n",
    "    eps_end: float = 0.05\n",
    "    eps_fraction: float = 0.8\n",
    "    max_grad_norm: float = 10.0\n",
    "    hidden_sizes: tuple = (128, 128)\n",
    "\n",
    "\n",
    "def linear_schedule(start: float, end: float, duration: int, t: int) -> float:\n",
    "    if duration <= 0:\n",
    "        return end\n",
    "    frac = min(max(t / float(duration), 0.0), 1.0)\n",
    "    return start + frac * (end - start)\n",
    "\n",
    "\n",
    "def moving_average(x, window: int):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if window <= 1:\n",
    "        return x\n",
    "    if x.size < window:\n",
    "        return np.full_like(x, np.nan, dtype=np.float64)\n",
    "    kernel = np.ones(int(window), dtype=np.float64) / float(window)\n",
    "    ma = np.convolve(x, kernel, mode=\"valid\")\n",
    "    return np.concatenate([np.full(window - 1, np.nan), ma])\n",
    "\n",
    "\n",
    "def reset_env(env, seed=None):\n",
    "    out = env.reset(seed=int(seed)) if seed is not None else env.reset()\n",
    "    if isinstance(out, tuple) and len(out) == 2:\n",
    "        obs, info = out\n",
    "    else:\n",
    "        obs, info = out, {}\n",
    "    return obs, info\n",
    "\n",
    "\n",
    "def step_env(env, action: int):\n",
    "    out = env.step(int(action))\n",
    "    if isinstance(out, tuple) and len(out) == 5:\n",
    "        obs, reward, terminated, truncated, info = out\n",
    "        return obs, float(reward), bool(terminated), bool(truncated), info\n",
    "    if isinstance(out, tuple) and len(out) == 4:\n",
    "        obs, reward, done, info = out\n",
    "        return obs, float(reward), bool(done), False, info\n",
    "    raise ValueError(f\"Unexpected step() return of length {len(out)}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def polyak_update(target_net: nn.Module, online_net: nn.Module, tau: float):\n",
    "    for p_targ, p in zip(target_net.parameters(), online_net.parameters()):\n",
    "        p_targ.data.mul_(1.0 - tau)\n",
    "        p_targ.data.add_(p.data, alpha=tau)\n",
    "\n",
    "\n",
    "def infer_n_actions(env) -> int:\n",
    "    if hasattr(env, \"action_space\") and hasattr(env.action_space, \"n\"):\n",
    "        return int(env.action_space.n)\n",
    "    if hasattr(env, \"n_actions\"):\n",
    "        return int(env.n_actions)\n",
    "    raise ValueError(\"Cannot infer number of actions: expected env.action_space.n or env.n_actions\")\n",
    "\n",
    "\n",
    "def select_action(q_net: nn.Module, obs: np.ndarray, epsilon: float, n_actions: int, rng: np.random.Generator) -> int:\n",
    "    if rng.random() < epsilon:\n",
    "        return int(rng.integers(0, n_actions))\n",
    "    obs_t = torch.as_tensor(obs, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q = q_net(obs_t)\n",
    "    return int(torch.argmax(q, dim=1).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f88d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineWorldEnv:\n",
    "    \"\"\"A tiny 1D environment with discrete actions.\n",
    "\n",
    "    - states are positions {0, 1, ..., n_states-1}\n",
    "    - actions: 0=left, 1=right\n",
    "    - observation is a one-hot vector in R^{n_states}\n",
    "    - reward: step_penalty each step, +goal_reward when reaching the goal\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_states: int = 15,\n",
    "        max_steps: int = 40,\n",
    "        step_penalty: float = -0.01,\n",
    "        goal_reward: float = 1.0,\n",
    "        slip_prob: float = 0.0,\n",
    "    ):\n",
    "        if n_states < 2:\n",
    "            raise ValueError(\"n_states must be >= 2\")\n",
    "        if max_steps < 1:\n",
    "            raise ValueError(\"max_steps must be >= 1\")\n",
    "        if not (0.0 <= slip_prob <= 1.0):\n",
    "            raise ValueError(\"slip_prob must be in [0, 1]\")\n",
    "\n",
    "        self.n_states = int(n_states)\n",
    "        self.n_actions = 2\n",
    "        self.max_steps = int(max_steps)\n",
    "        self.step_penalty = float(step_penalty)\n",
    "        self.goal_reward = float(goal_reward)\n",
    "        self.slip_prob = float(slip_prob)\n",
    "\n",
    "        self._pos = 0\n",
    "        self._t = 0\n",
    "        self._rng = np.random.default_rng(0)\n",
    "\n",
    "    def _obs(self) -> np.ndarray:\n",
    "        obs = np.zeros((self.n_states,), dtype=np.float32)\n",
    "        obs[self._pos] = 1.0\n",
    "        return obs\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self._rng = np.random.default_rng(int(seed))\n",
    "        self._pos = 0\n",
    "        self._t = 0\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action: int):\n",
    "        action = int(action)\n",
    "        if self.slip_prob > 0.0 and self._rng.random() < self.slip_prob:\n",
    "            action = 1 - action  # slip: flip action\n",
    "\n",
    "        if action == 0:\n",
    "            self._pos = max(0, self._pos - 1)\n",
    "        elif action == 1:\n",
    "            self._pos = min(self.n_states - 1, self._pos + 1)\n",
    "        else:\n",
    "            raise ValueError(\"action must be 0 (left) or 1 (right)\")\n",
    "\n",
    "        self._t += 1\n",
    "        terminated = self._pos == (self.n_states - 1)\n",
    "        truncated = self._t >= self.max_steps\n",
    "\n",
    "        reward = self.step_penalty + (self.goal_reward if terminated else 0.0)\n",
    "        info = {\"pos\": self._pos}\n",
    "        return self._obs(), float(reward), bool(terminated), bool(truncated), info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba00d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, obs_shape, device: torch.device):\n",
    "        self.capacity = int(capacity)\n",
    "        self.device = device\n",
    "\n",
    "        self.obs_buf = np.zeros((self.capacity, *obs_shape), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((self.capacity, *obs_shape), dtype=np.float32)\n",
    "        self.actions_buf = np.zeros((self.capacity,), dtype=np.int64)\n",
    "        self.rewards_buf = np.zeros((self.capacity,), dtype=np.float32)\n",
    "        self.dones_buf = np.zeros((self.capacity,), dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, obs, action: int, reward: float, next_obs, done: float):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.actions_buf[self.ptr] = int(action)\n",
    "        self.rewards_buf[self.ptr] = float(reward)\n",
    "        self.dones_buf[self.ptr] = float(done)\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int, rng: np.random.Generator):\n",
    "        if self.size < batch_size:\n",
    "            raise ValueError(\"Not enough samples in buffer\")\n",
    "        idx = rng.integers(0, self.size, size=int(batch_size))\n",
    "\n",
    "        obs = torch.as_tensor(self.obs_buf[idx], device=self.device, dtype=torch.float32)\n",
    "        next_obs = torch.as_tensor(self.next_obs_buf[idx], device=self.device, dtype=torch.float32)\n",
    "        actions = torch.as_tensor(self.actions_buf[idx], device=self.device, dtype=torch.int64)\n",
    "        rewards = torch.as_tensor(self.rewards_buf[idx], device=self.device, dtype=torch.float32)\n",
    "        dones = torch.as_tensor(self.dones_buf[idx], device=self.device, dtype=torch.float32)\n",
    "\n",
    "        return obs, actions, rewards, next_obs, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden_sizes=(128, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = int(obs_dim)\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, int(h)))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = int(h)\n",
    "        layers.append(nn.Linear(in_dim, int(n_actions)))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c19d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_update(\n",
    "    q_net: nn.Module,\n",
    "    target_net: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    batch,\n",
    "    gamma: float,\n",
    "    max_grad_norm: float,\n",
    ") -> float:\n",
    "    obs, actions, rewards, next_obs, dones = batch\n",
    "\n",
    "    q_values = q_net(obs)  # (B, A)\n",
    "    q_sa = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = target_net(next_obs)\n",
    "        max_next_q = next_q.max(dim=1).values\n",
    "        target = rewards + gamma * (1.0 - dones) * max_next_q\n",
    "\n",
    "    loss = F.smooth_l1_loss(q_sa, target)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(q_net.parameters(), max_norm=max_grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss.item())\n",
    "\n",
    "\n",
    "def train_dqn(env, config: DQNConfig, num_episodes: int = 300, log_every: int = 50):\n",
    "    n_actions = infer_n_actions(env)\n",
    "\n",
    "    obs0, _ = reset_env(env, seed=int(rng.integers(0, 1_000_000)))\n",
    "    obs0 = np.asarray(obs0, dtype=np.float32)\n",
    "    if obs0.ndim != 1:\n",
    "        raise ValueError(\"This minimal notebook assumes 1D vector observations\")\n",
    "\n",
    "    obs_dim = int(obs0.shape[0])\n",
    "    obs_shape = obs0.shape\n",
    "    max_steps_per_episode = int(getattr(env, \"max_steps\", 200))\n",
    "    eps_decay_steps = max(1, int(config.eps_fraction * num_episodes * max_steps_per_episode))\n",
    "\n",
    "    q_net = QNetwork(obs_dim, n_actions, hidden_sizes=config.hidden_sizes).to(device)\n",
    "    target_net = QNetwork(obs_dim, n_actions, hidden_sizes=config.hidden_sizes).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=config.learning_rate)\n",
    "    buffer = ReplayBuffer(config.buffer_size, obs_shape=obs_shape, device=device)\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilons = []\n",
    "    loss_steps = []\n",
    "    loss_values = []\n",
    "    q_probe_history = []\n",
    "\n",
    "    probe_obs = obs0.copy()\n",
    "\n",
    "    for ep in range(int(num_episodes)):\n",
    "        obs, _ = reset_env(env, seed=int(rng.integers(0, 1_000_000)))\n",
    "        obs = np.asarray(obs, dtype=np.float32)\n",
    "\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            epsilon = linear_schedule(config.eps_start, config.eps_end, eps_decay_steps, global_step)\n",
    "            action = select_action(q_net, obs, epsilon, n_actions, rng)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = step_env(env, action)\n",
    "            next_obs = np.asarray(next_obs, dtype=np.float32)\n",
    "\n",
    "            done_for_bootstrap = float(terminated)  # time-limit truncation should still bootstrap\n",
    "            buffer.add(obs, action, reward, next_obs, done_for_bootstrap)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += float(reward)\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            if (\n",
    "                buffer.size >= config.batch_size\n",
    "                and global_step >= config.learning_starts\n",
    "                and (global_step % config.train_freq == 0)\n",
    "            ):\n",
    "                for _ in range(config.gradient_steps):\n",
    "                    batch = buffer.sample(config.batch_size, rng)\n",
    "                    loss = dqn_update(\n",
    "                        q_net=q_net,\n",
    "                        target_net=target_net,\n",
    "                        optimizer=optimizer,\n",
    "                        batch=batch,\n",
    "                        gamma=config.gamma,\n",
    "                        max_grad_norm=config.max_grad_norm,\n",
    "                    )\n",
    "                    loss_steps.append(global_step)\n",
    "                    loss_values.append(loss)\n",
    "\n",
    "            if global_step % config.target_update_interval == 0:\n",
    "                polyak_update(target_net, q_net, tau=config.tau)\n",
    "\n",
    "            if steps >= max_steps_per_episode:\n",
    "                truncated = True\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilons.append(epsilon)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_probe = q_net(torch.as_tensor(probe_obs, device=device, dtype=torch.float32).unsqueeze(0))\n",
    "        q_probe_history.append(q_probe.squeeze(0).cpu().numpy())\n",
    "\n",
    "        if log_every and (ep + 1) % int(log_every) == 0:\n",
    "            print(\n",
    "                f\"Episode {ep+1:4d} | reward {total_reward:7.2f} | \"\n",
    "                f\"eps {epsilon:5.3f} | buffer {buffer.size:6d} | steps {global_step:7d}\"\n",
    "            )\n",
    "\n",
    "    logs = {\n",
    "        \"episode_rewards\": np.asarray(episode_rewards, dtype=np.float64),\n",
    "        \"episode_lengths\": np.asarray(episode_lengths, dtype=np.int64),\n",
    "        \"epsilons\": np.asarray(epsilons, dtype=np.float64),\n",
    "        \"loss_steps\": np.asarray(loss_steps, dtype=np.int64),\n",
    "        \"loss_values\": np.asarray(loss_values, dtype=np.float64),\n",
    "        \"q_probe\": np.asarray(q_probe_history, dtype=np.float64),\n",
    "        \"n_actions\": n_actions,\n",
    "        \"obs_dim\": obs_dim,\n",
    "    }\n",
    "    return q_net, logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54af49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LineWorldEnv(n_states=15, max_steps=40, step_penalty=-0.01, goal_reward=1.0, slip_prob=0.05)\n",
    "\n",
    "config = DQNConfig(\n",
    "    gamma=0.99,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=25_000,\n",
    "    batch_size=64,\n",
    "    learning_starts=500,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    target_update_interval=200,\n",
    "    tau=1.0,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.05,\n",
    "    eps_fraction=0.8,\n",
    "    max_grad_norm=10.0,\n",
    "    hidden_sizes=(128, 128),\n",
    ")\n",
    "\n",
    "q_net, logs = train_dqn(env, config=config, num_episodes=300, log_every=50)\n",
    "logs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ee0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward per episode (learning curve)\n",
    "rewards = logs[\"episode_rewards\"]\n",
    "ma = moving_average(rewards, window=20)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=rewards, mode=\"lines\", name=\"reward/episode\"))\n",
    "fig.add_trace(go.Scatter(y=ma, mode=\"lines\", name=\"moving avg (20)\", line=dict(width=3)))\n",
    "fig.update_layout(\n",
    "    title=\"DQN learning curve (reward per episode)\",\n",
    "    xaxis_title=\"episode\",\n",
    "    yaxis_title=\"total reward\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25963c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD loss over training steps\n",
    "loss_steps = logs[\"loss_steps\"]\n",
    "loss_values = logs[\"loss_values\"]\n",
    "\n",
    "if loss_values.size == 0:\n",
    "    print(\"No loss values recorded (try lowering learning_starts or increasing episodes).\")\n",
    "else:\n",
    "    fig = px.line(\n",
    "        x=loss_steps,\n",
    "        y=loss_values,\n",
    "        labels={\"x\": \"environment step\", \"y\": \"Huber TD loss\"},\n",
    "        title=\"DQN TD loss (Smooth L1)\",\n",
    "    )\n",
    "    fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration schedule ($\\epsilon$) over episodes\n",
    "eps = logs[\"epsilons\"]\n",
    "fig = px.line(\n",
    "    x=np.arange(len(eps)),\n",
    "    y=eps,\n",
    "    labels={\"x\": \"episode\", \"y\": \"epsilon\"},\n",
    "    title=\"Epsilon schedule\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-values over training for a fixed probe state (the start state)\n",
    "q_probe = logs[\"q_probe\"]  # (episodes, n_actions)\n",
    "n_actions = logs[\"n_actions\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in range(n_actions):\n",
    "    fig.add_trace(go.Scatter(y=q_probe[:, a], mode=\"lines\", name=f\"Q(probe, a={a})\"))\n",
    "fig.update_layout(\n",
    "    title=\"Learned Q-values over training (probe state)\",\n",
    "    xaxis_title=\"episode\",\n",
    "    yaxis_title=\"Q-value\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Q(s,a) for all LineWorld states (since observations are one-hot)\n",
    "states = np.eye(env.n_states, dtype=np.float32)\n",
    "with torch.no_grad():\n",
    "    q_all = q_net(torch.as_tensor(states, device=device, dtype=torch.float32)).cpu().numpy()\n",
    "\n",
    "fig = px.imshow(\n",
    "    q_all,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    labels={\"x\": \"action\", \"y\": \"state\", \"color\": \"Q(s,a)\"},\n",
    "    title=\"Q-values heatmap across states and actions\",\n",
    "    color_continuous_scale=\"RdBu\",\n",
    ")\n",
    "fig.update_xaxes(tickmode=\"array\", tickvals=list(range(env.n_actions)), ticktext=[\"left\", \"right\"])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54e3f1",
   "metadata": {},
   "source": [
    "## Pitfalls + diagnostics\n",
    "\n",
    "- **Diverging loss / exploding Q-values**: try a smaller learning rate, larger replay buffer, or gradient clipping.\n",
    "- **No learning**: ensure enough exploration ($\\epsilon$ schedule), lower `learning_starts`, increase training episodes.\n",
    "- **Time-limit truncation**: if an episode ends due to a time limit (not a true terminal state), bootstrapping should still happen. In the code above we store $d=1$ only when `terminated=True`.\n",
    "- **Target update too frequent/rare**: too frequent makes targets chase the online net; too rare slows learning.\n",
    "- **Reward scale**: large rewards can increase TD errors; Huber loss helps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92df01",
   "metadata": {},
   "source": [
    "## (Optional) Run the same DQN on Gymnasium environments\n",
    "\n",
    "If you have `gymnasium` installed, you can plug it into the same training code as long as:\n",
    "\n",
    "- the action space is discrete ($|\\mathcal{A}| < \\infty$)\n",
    "- the observation can be represented as a 1D float vector (this notebook uses an MLP)\n",
    "\n",
    "Example (CartPole):\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "q_net, logs = train_dqn(env, config=config, num_episodes=400, log_every=50)\n",
    "```\n",
    "\n",
    "If `gymnasium` is missing, install with:\n",
    "\n",
    "```bash\n",
    "pip install gymnasium\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a0bb7",
   "metadata": {},
   "source": [
    "## Stable-Baselines3 DQN (reference implementation)\n",
    "\n",
    "Stable-Baselines3 includes a DQN implementation for discrete action spaces. Example from the SB3 docs:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000, log_interval=4)\n",
    "model.save(\"dqn_cartpole\")\n",
    "```\n",
    "\n",
    "Docs:\n",
    "- https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "- https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "Original DQN paper:\n",
    "- Mnih et al. (2015), *Human-level control through deep reinforcement learning*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83cf6f",
   "metadata": {},
   "source": [
    "## Stable-Baselines3 `DQN(...)` hyperparameters (explained)\n",
    "\n",
    "Below are the most important SB3 DQN constructor hyperparameters and what they control:\n",
    "\n",
    "- `policy`: network/policy type (e.g., `\"MlpPolicy\"`, `\"CnnPolicy\"`, `\"MultiInputPolicy\"`).\n",
    "- `learning_rate`: optimizer step size (can be a float or a schedule).\n",
    "- `buffer_size`: replay buffer capacity (number of transitions stored).\n",
    "- `learning_starts`: number of environment steps collected before training begins.\n",
    "- `batch_size`: mini-batch size sampled from the replay buffer.\n",
    "- `gamma`: discount factor $\\gamma$.\n",
    "- `train_freq`: how often to run training updates while collecting data (e.g., every 1 or 4 environment steps).\n",
    "- `gradient_steps`: how many gradient steps to take per training iteration.\n",
    "- `target_update_interval`: how often to update the target network (in environment steps).\n",
    "- `tau`: Polyak coefficient for target updates ($\\tau=1$ corresponds to a hard copy).\n",
    "- `exploration_initial_eps`, `exploration_final_eps`, `exploration_fraction`: linear $\\epsilon$-greedy exploration schedule.\n",
    "- `max_grad_norm`: gradient clipping threshold.\n",
    "- `policy_kwargs`: network architecture and optimizer details (e.g., `net_arch`, activation, optimizer settings).\n",
    "- `replay_buffer_class`, `replay_buffer_kwargs`: swap/parameterize the replay buffer (e.g., HER buffers for goal-conditioned tasks).\n",
    "- `optimize_memory_usage`: enables a more memory-efficient replay buffer variant (useful with large observations).\n",
    "- `device`: where to run the model (`\"cpu\"` or `\"cuda\"`).\n",
    "- `seed`, `verbose`, `tensorboard_log`: reproducibility and logging controls.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}