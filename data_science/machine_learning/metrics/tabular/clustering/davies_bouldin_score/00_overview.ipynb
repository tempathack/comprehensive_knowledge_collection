{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# davies_bouldin_score (Davies–Bouldin Index, DBI)\n",
    "\n",
    "The Davies–Bouldin index (DBI) is an **internal** clustering metric: it scores a clustering using only the data $X$ and the predicted cluster labels.\n",
    "\n",
    "Intuition: good clusterings have **compact clusters** that are **far from each other**.\n",
    "DBI summarizes this as an average of each cluster’s *worst-case similarity* to another cluster.\n",
    "\n",
    "**Lower is better.**\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import davies_bouldin_score as sk_davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) When DBI is a good choice\n",
    "\n",
    "Use DBI when you:\n",
    "- have **no ground-truth labels** (unsupervised setting)\n",
    "- want a **fast** sanity check / model-selection criterion for a clustering\n",
    "- expect clusters to be reasonably **blob-like / convex** (centroid-based geometry)\n",
    "\n",
    "DBI is commonly used to compare settings like the number of clusters $k$ in **$k$-means**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) A quick visual example\n",
    "\n",
    "DBI needs predicted cluster labels. Below we fit $k$-means on a 2D toy dataset and compute DBI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D toy dataset (3 blobs)\n",
    "X, _ = make_blobs(\n",
    "    n_samples=600,\n",
    "    centers=[(-4, -1), (0, 3), (4, -1)],\n",
    "    cluster_std=[0.7, 1.0, 0.8],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "dbi = sk_davies_bouldin_score(X, labels)\n",
    "\n",
    "df = pd.DataFrame({\"x1\": X[:, 0], \"x2\": X[:, 1], \"cluster\": labels.astype(str)})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x1\",\n",
    "    y=\"x2\",\n",
    "    color=\"cluster\",\n",
    "    title=f\"K-means clustering (k=3) — DBI={dbi:.3f} (lower is better)\",\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=kmeans.cluster_centers_[:, 0],\n",
    "        y=kmeans.cluster_centers_[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(symbol=\"x\", size=12, color=\"black\"),\n",
    "        name=\"centroid\",\n",
    "    )\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n",
    "\n",
    "dbi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Definition (math)\n",
    "\n",
    "Assume data points $x_n \\in \\mathbb{R}^d$ and a clustering into $k$ **non-empty** clusters $C_1,\\dots,C_k$.\n",
    "\n",
    "Centroid of cluster $i$:\n",
    "$$\\mu_i = \\frac{1}{|C_i|}\\sum_{x \\in C_i} x.$$\n",
    "\n",
    "Within-cluster scatter (as used by scikit-learn):\n",
    "$$S_i = \\frac{1}{|C_i|}\\sum_{x \\in C_i} \\lVert x - \\mu_i \\rVert_2.$$\n",
    "\n",
    "Between-cluster separation (centroid distance):\n",
    "$$M_{ij} = \\lVert \\mu_i - \\mu_j \\rVert_2.$$\n",
    "\n",
    "Pairwise “similarity” (high means bad: large scatter and/or small separation):\n",
    "$$R_{ij} = \\frac{S_i + S_j}{M_{ij}}, \\quad i \\ne j.$$\n",
    "\n",
    "For each cluster, take the worst-case neighbor:\n",
    "$$D_i = \\max_{j \\ne i} R_{ij}.$$\n",
    "\n",
    "Finally, DBI is the average:\n",
    "$$\\mathrm{DBI} = \\frac{1}{k}\\sum_{i=1}^k D_i.$$\n",
    "\n",
    "A perfect value is $0$ (zero scatter with distinct centroids). In practice, **lower is better**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) A low-level NumPy implementation\n",
    "\n",
    "Below is a from-scratch implementation that matches `sklearn.metrics.davies_bouldin_score` for Euclidean distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def davies_bouldin_index_np(X, labels):\n",
    "    \"\"\"Compute the Davies–Bouldin index (DBI) from scratch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data matrix.\n",
    "    labels : array-like, shape (n_samples,)\n",
    "        Cluster labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Davies–Bouldin index. Lower is better.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be 2D of shape (n_samples, n_features).\")\n",
    "    if labels.ndim != 1 or labels.shape[0] != X.shape[0]:\n",
    "        raise ValueError(\"labels must be 1D with length n_samples.\")\n",
    "\n",
    "    _, inv = np.unique(labels, return_inverse=True)\n",
    "    k = int(inv.max() + 1)\n",
    "    if k < 2:\n",
    "        raise ValueError(\"Davies–Bouldin index requires at least 2 clusters.\")\n",
    "\n",
    "    counts = np.bincount(inv, minlength=k)\n",
    "    if np.any(counts == 0):\n",
    "        raise ValueError(\"Empty cluster detected.\")\n",
    "\n",
    "    # Centroids: mu_i\n",
    "    centroids = np.zeros((k, X.shape[1]), dtype=float)\n",
    "    np.add.at(centroids, inv, X)\n",
    "    centroids /= counts[:, None]\n",
    "\n",
    "    # Scatter: S_i = mean(||x - mu_i||)\n",
    "    dists = np.linalg.norm(X - centroids[inv], axis=1)\n",
    "    scatters = np.bincount(inv, weights=dists, minlength=k) / counts\n",
    "\n",
    "    # Separation: M_ij = ||mu_i - mu_j||\n",
    "    diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "    centroid_dist = np.linalg.norm(diff, axis=2)\n",
    "\n",
    "    # R_ij = (S_i + S_j) / M_ij\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        R = (scatters[:, None] + scatters[None, :]) / centroid_dist\n",
    "    np.fill_diagonal(R, -np.inf)\n",
    "\n",
    "    D = np.max(R, axis=1)  # worst neighbor for each cluster\n",
    "    return float(np.mean(D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: NumPy vs scikit-learn\n",
    "dbi_np = davies_bouldin_index_np(X, labels)\n",
    "dbi_sk = sk_davies_bouldin_score(X, labels)\n",
    "dbi_np, dbi_sk, abs(dbi_np - dbi_sk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBI is lower when clusters are compact and well-separated (try random labels)\n",
    "labels_random = rng.integers(0, 3, size=X.shape[0])\n",
    "dbi_random = davies_bouldin_index_np(X, labels_random)\n",
    "dbi_np, dbi_random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Visualizing the ingredients ($S_i$, $M_{ij}$, and the “worst neighbor”)\n",
    "\n",
    "DBI is easiest to understand by looking at its building blocks:\n",
    "- $S_i$: how spread-out cluster $i$ is around its centroid (average radius)\n",
    "- $M_{ij}$: distance between centroids (how far two clusters are)\n",
    "- $R_{ij} = (S_i + S_j)/M_{ij}$: “overlap risk” between clusters $i$ and $j$\n",
    "- $D_i = \\max_{j\\ne i} R_{ij}$: cluster $i$ only cares about its *most confusing* neighbor\n",
    "\n",
    "DBI averages the $D_i$ values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbi_components_np(X, labels):\n",
    "    \"\"\"Return DBI components for inspection/plotting.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    unique_labels, inv = np.unique(labels, return_inverse=True)\n",
    "    k = unique_labels.size\n",
    "    if k < 2:\n",
    "        raise ValueError(\"Need at least 2 non-empty clusters.\")\n",
    "\n",
    "    counts = np.bincount(inv, minlength=k)\n",
    "    if np.any(counts == 0):\n",
    "        raise ValueError(\"Empty cluster detected.\")\n",
    "\n",
    "    centroids = np.zeros((k, X.shape[1]), dtype=float)\n",
    "    np.add.at(centroids, inv, X)\n",
    "    centroids /= counts[:, None]\n",
    "\n",
    "    dists = np.linalg.norm(X - centroids[inv], axis=1)\n",
    "    S = np.bincount(inv, weights=dists, minlength=k) / counts\n",
    "\n",
    "    diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "    M = np.linalg.norm(diff, axis=2)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        R = (S[:, None] + S[None, :]) / M\n",
    "    np.fill_diagonal(R, -np.inf)\n",
    "\n",
    "    worst_neighbor = np.argmax(R, axis=1)\n",
    "    D = R[np.arange(k), worst_neighbor]\n",
    "    dbi = float(D.mean())\n",
    "\n",
    "    return {\n",
    "        \"unique_labels\": unique_labels,\n",
    "        \"centroids\": centroids,\n",
    "        \"S\": S,\n",
    "        \"M\": M,\n",
    "        \"R\": R,\n",
    "        \"worst_neighbor\": worst_neighbor,\n",
    "        \"D\": D,\n",
    "        \"dbi\": dbi,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = dbi_components_np(X, labels)\n",
    "centroids = comp[\"centroids\"]\n",
    "S = comp[\"S\"]\n",
    "worst = comp[\"worst_neighbor\"]\n",
    "D = comp[\"D\"]\n",
    "dbi = comp[\"dbi\"]\n",
    "\n",
    "# Points + centroids + (approx) scatter circles + arrows to worst neighbor\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x1\",\n",
    "    y=\"x2\",\n",
    "    color=\"cluster\",\n",
    "    title=f\"DBI ingredients — DBI={dbi:.3f}\",\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centroids[:, 0],\n",
    "        y=centroids[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(symbol=\"x\", size=12, color=\"black\"),\n",
    "        name=\"centroid\",\n",
    "    )\n",
    ")\n",
    "\n",
    "shapes = []\n",
    "for i, (cx, cy) in enumerate(centroids[:, :2]):\n",
    "    r = float(S[i])\n",
    "    shapes.append(\n",
    "        dict(\n",
    "            type=\"circle\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            x0=cx - r,\n",
    "            x1=cx + r,\n",
    "            y0=cy - r,\n",
    "            y1=cy + r,\n",
    "            line=dict(color=\"rgba(0,0,0,0.35)\", dash=\"dot\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(shapes=shapes)\n",
    "\n",
    "for i, j in enumerate(worst):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[centroids[i, 0], centroids[j, 0]],\n",
    "            y=[centroids[i, 1], centroids[j, 1]],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"rgba(0,0,0,0.5)\", dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"skip\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=centroids[i, 0],\n",
    "        y=centroids[i, 1],\n",
    "        text=f\"S={S[i]:.2f}<br>D={D[i]:.2f}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        ax=25,\n",
    "        ay=-25,\n",
    "    )\n",
    "\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n",
    "\n",
    "# For small k, R_ij is nice to inspect directly\n",
    "R_show = comp[\"R\"].copy()\n",
    "np.fill_diagonal(R_show, np.nan)\n",
    "fig2 = px.imshow(\n",
    "    R_show,\n",
    "    text_auto=\".2f\",\n",
    "    aspect=\"auto\",\n",
    "    title=\"Pairwise ratios R_ij = (S_i+S_j)/M_ij (diagonal undefined)\",\n",
    ")\n",
    "fig2.update_layout(xaxis_title=\"j\", yaxis_title=\"i\")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) How DBI reacts to separation and dispersion\n",
    "\n",
    "DBI decreases when you pull clusters apart (larger $M_{ij}$), and increases when you make clusters more spread-out (larger $S_i$).\n",
    "\n",
    "To isolate the effect, we keep *true* labels fixed and only change the geometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build two Gaussian blobs from shared base noise to reduce Monte Carlo noise\n",
    "rng_demo = np.random.default_rng(123)\n",
    "n1, n2 = 500, 500\n",
    "Z1 = rng_demo.standard_normal((n1, 2))\n",
    "Z2 = rng_demo.standard_normal((n2, 2))\n",
    "labels_true_2 = np.r_[np.zeros(n1, dtype=int), np.ones(n2, dtype=int)]\n",
    "\n",
    "\n",
    "def make_two_blobs_np(separation, std):\n",
    "    c1 = np.array([-separation / 2.0, 0.0])\n",
    "    c2 = np.array([separation / 2.0, 0.0])\n",
    "    X1 = Z1 * std + c1\n",
    "    X2 = Z2 * std + c2\n",
    "    return np.vstack([X1, X2])\n",
    "\n",
    "\n",
    "separations = np.linspace(0.5, 8.0, 16)\n",
    "rows = []\n",
    "for sep in separations:\n",
    "    X_sep = make_two_blobs_np(sep, std=1.0)\n",
    "    rows.append({\"separation\": float(sep), \"dbi\": davies_bouldin_index_np(X_sep, labels_true_2)})\n",
    "\n",
    "df_sep = pd.DataFrame(rows)\n",
    "\n",
    "fig = px.line(\n",
    "    df_sep,\n",
    "    x=\"separation\",\n",
    "    y=\"dbi\",\n",
    "    markers=True,\n",
    "    title=\"DBI decreases as clusters are pulled apart (spread fixed)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Separation\", yaxis_title=\"Davies–Bouldin index\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = np.linspace(0.3, 3.0, 16)\n",
    "rows = []\n",
    "for std in stds:\n",
    "    X_std = make_two_blobs_np(separation=5.0, std=float(std))\n",
    "    rows.append({\"std\": float(std), \"dbi\": davies_bouldin_index_np(X_std, labels_true_2)})\n",
    "\n",
    "df_std = pd.DataFrame(rows)\n",
    "\n",
    "fig = px.line(\n",
    "    df_std,\n",
    "    x=\"std\",\n",
    "    y=\"dbi\",\n",
    "    markers=True,\n",
    "    title=\"DBI increases as clusters get more diffuse (separation fixed)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Cluster standard deviation\", yaxis_title=\"Davies–Bouldin index\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Common pitfall: DBI is scale-dependent\n",
    "\n",
    "DBI is based on Euclidean distances, so **units matter**. If one feature has a much larger scale than the others, it can dominate both the clustering and the DBI.\n",
    "\n",
    "Rule of thumb: when using DBI with distance-based clustering, **standardize** features (e.g., z-score) unless you have a strong reason not to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, _ = make_blobs(\n",
    "    n_samples=600,\n",
    "    centers=[(-3, 0), (0, 4), (3, 0)],\n",
    "    cluster_std=[0.8, 0.8, 0.8],\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# Make x1 much larger-scale than x2\n",
    "X_bad = X0.copy()\n",
    "X_bad[:, 0] *= 12.0\n",
    "\n",
    "k_bad = KMeans(n_clusters=3, n_init=10, random_state=0).fit(X_bad)\n",
    "labels_bad = k_bad.labels_\n",
    "dbi_bad = sk_davies_bouldin_score(X_bad, labels_bad)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_bad_std = scaler.fit_transform(X_bad)\n",
    "k_std = KMeans(n_clusters=3, n_init=10, random_state=0).fit(X_bad_std)\n",
    "labels_std = k_std.labels_\n",
    "dbi_std = sk_davies_bouldin_score(X_bad_std, labels_std)\n",
    "\n",
    "# Plot both clusterings in the original (scaled) coordinates\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        f\"Raw scaled features (DBI={dbi_bad:.3f})\",\n",
    "        f\"After StandardScaler (DBI={dbi_std:.3f})\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for cl in np.unique(labels_bad):\n",
    "    m = labels_bad == cl\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_bad[m, 0], y=X_bad[m, 1], mode=\"markers\", marker=dict(size=4), name=f\"cluster {cl}\"),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "for cl in np.unique(labels_std):\n",
    "    m = labels_std == cl\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_bad[m, 0],\n",
    "            y=X_bad[m, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4),\n",
    "            name=f\"cluster {cl}\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=k_bad.cluster_centers_[:, 0],\n",
    "        y=k_bad.cluster_centers_[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(symbol=\"x\", size=10, color=\"black\"),\n",
    "        name=\"centroid\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Centroids are in standardized space; invert for plotting\n",
    "centroids_std_original = scaler.inverse_transform(k_std.cluster_centers_)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centroids_std_original[:, 0],\n",
    "        y=centroids_std_original[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(symbol=\"x\", size=10, color=\"black\"),\n",
    "        name=\"centroid\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"x1 (scaled units)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x1 (scaled units)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"x2\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"x2\", row=1, col=2)\n",
    "fig.update_layout(title=\"Pitfall: DBI depends on feature scaling\")\n",
    "fig.show()\n",
    "\n",
    "dbi_bad, dbi_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Using DBI to optimize a simple algorithm (from scratch $k$-means)\n",
    "\n",
    "DBI is a **clustering** metric: it needs cluster assignments, so it’s not a natural objective for supervised models like linear/logistic regression.\n",
    "\n",
    "A common “optimization” use is **model selection**: pick the hyperparameter (often $k$) that minimizes DBI.\n",
    "\n",
    "Below is a low-level $k$-means implementation (NumPy) and a DBI-based search over $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_pp_init_np(X, k, rng):\n",
    "    \"\"\"k-means++ initialization (NumPy).\n",
    "\n",
    "    Returns centroids of shape (k, n_features).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n = X.shape[0]\n",
    "    centroids = np.empty((k, X.shape[1]), dtype=float)\n",
    "\n",
    "    # First centroid uniformly at random\n",
    "    i0 = int(rng.integers(0, n))\n",
    "    centroids[0] = X[i0]\n",
    "\n",
    "    # Dist^2 to closest centroid so far\n",
    "    closest_dist2 = np.sum((X - centroids[0]) ** 2, axis=1)\n",
    "\n",
    "    for c in range(1, k):\n",
    "        total = float(closest_dist2.sum())\n",
    "        if total == 0.0:\n",
    "            # All points are identical.\n",
    "            centroids[c:] = X[i0]\n",
    "            break\n",
    "\n",
    "        probs = closest_dist2 / total\n",
    "        idx = int(rng.choice(n, p=probs))\n",
    "        centroids[c] = X[idx]\n",
    "\n",
    "        dist2_new = np.sum((X - centroids[c]) ** 2, axis=1)\n",
    "        closest_dist2 = np.minimum(closest_dist2, dist2_new)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def kmeans_np(X, k, n_init=10, max_iter=200, tol=1e-6, rng=None):\n",
    "    \"\"\"Simple k-means (Lloyd) with k-means++ init.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : (n_samples,)\n",
    "    centroids : (k, n_features)\n",
    "    inertia : float\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    n, d = X.shape\n",
    "    best_inertia = np.inf\n",
    "    best_labels = None\n",
    "    best_centroids = None\n",
    "\n",
    "    for _ in range(int(n_init)):\n",
    "        centroids = kmeans_pp_init_np(X, k, rng)\n",
    "\n",
    "        for _it in range(int(max_iter)):\n",
    "            # Assign to nearest centroid (squared Euclidean)\n",
    "            dist2 = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n",
    "            labels = np.argmin(dist2, axis=1)\n",
    "\n",
    "            counts = np.bincount(labels, minlength=k)\n",
    "            if np.any(counts == 0):\n",
    "                # Rare but possible: re-seed empty centroid to a random data point\n",
    "                empty = np.where(counts == 0)[0]\n",
    "                centroids[empty] = X[rng.integers(0, n, size=empty.size)]\n",
    "                continue\n",
    "\n",
    "            new_centroids = np.zeros((k, d), dtype=float)\n",
    "            np.add.at(new_centroids, labels, X)\n",
    "            new_centroids /= counts[:, None]\n",
    "\n",
    "            shift = float(np.linalg.norm(new_centroids - centroids))\n",
    "            centroids = new_centroids\n",
    "            if shift <= tol:\n",
    "                break\n",
    "\n",
    "        # Final inertia\n",
    "        dist2 = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n",
    "        labels = np.argmin(dist2, axis=1)\n",
    "        inertia = float(dist2[np.arange(n), labels].sum())\n",
    "\n",
    "        if inertia < best_inertia:\n",
    "            best_inertia = inertia\n",
    "            best_labels = labels\n",
    "            best_centroids = centroids\n",
    "\n",
    "    return best_labels, best_centroids, best_inertia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with a \"true\" k (4 blobs)\n",
    "Xk, _ = make_blobs(n_samples=900, centers=4, cluster_std=[0.6, 0.7, 0.5, 0.8], random_state=7)\n",
    "Xk = StandardScaler().fit_transform(Xk)\n",
    "\n",
    "k_grid = range(2, 9)\n",
    "rows = []\n",
    "best = None\n",
    "\n",
    "for k in k_grid:\n",
    "    labels_k, centroids_k, inertia_k = kmeans_np(Xk, k, n_init=10, max_iter=200, rng=np.random.default_rng(0))\n",
    "    dbi_k = davies_bouldin_index_np(Xk, labels_k)\n",
    "    rows.append({\"k\": int(k), \"dbi\": dbi_k, \"inertia\": inertia_k})\n",
    "    if best is None or dbi_k < best[\"dbi\"]:\n",
    "        best = {\"k\": int(k), \"dbi\": dbi_k, \"labels\": labels_k, \"centroids\": centroids_k, \"inertia\": inertia_k}\n",
    "\n",
    "df_k = pd.DataFrame(rows)\n",
    "\n",
    "fig = px.line(\n",
    "    df_k,\n",
    "    x=\"k\",\n",
    "    y=\"dbi\",\n",
    "    markers=True,\n",
    "    title=\"Pick k by minimizing DBI (k-means, from scratch)\",\n",
    ")\n",
    "fig.update_layout(xaxis=dict(dtick=1), xaxis_title=\"k\", yaxis_title=\"Davies–Bouldin index\")\n",
    "fig.add_vline(x=best[\"k\"], line_dash=\"dash\", line_color=\"black\")\n",
    "fig.show()\n",
    "\n",
    "df_k.sort_values(\"dbi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: a \"wrong\" k vs the DBI-selected k\n",
    "k_wrong = 2\n",
    "labels_wrong, centroids_wrong, _ = kmeans_np(Xk, k_wrong, n_init=10, rng=np.random.default_rng(1))\n",
    "dbi_wrong = davies_bouldin_index_np(Xk, labels_wrong)\n",
    "\n",
    "labels_best = best[\"labels\"]\n",
    "dbi_best = best[\"dbi\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(f\"k={k_wrong} (DBI={dbi_wrong:.3f})\", f\"k={best['k']} (DBI={dbi_best:.3f})\"),\n",
    ")\n",
    "\n",
    "for cl in np.unique(labels_wrong):\n",
    "    m = labels_wrong == cl\n",
    "    fig.add_trace(go.Scatter(x=Xk[m, 0], y=Xk[m, 1], mode=\"markers\", marker=dict(size=4), name=f\"cluster {cl}\"), row=1, col=1)\n",
    "\n",
    "for cl in np.unique(labels_best):\n",
    "    m = labels_best == cl\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=Xk[m, 0], y=Xk[m, 1], mode=\"markers\", marker=dict(size=4), name=f\"cluster {cl}\", showlegend=False),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"feature 1 (standardized)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"feature 1 (standardized)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"feature 2 (standardized)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"feature 2 (standardized)\", row=1, col=2)\n",
    "fig.update_layout(title=\"DBI as a model-selection criterion\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Practical usage (`sklearn`)\n",
    "\n",
    "In practice, you usually compute DBI on the same representation you used to cluster:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "labels = KMeans(n_clusters=4, n_init=10, random_state=0).fit_predict(X_std)\n",
    "score = davies_bouldin_score(X_std, labels)  # lower is better\n",
    "```\n",
    "\n",
    "A quick pattern for selecting $k$ is to scan a small grid and pick the minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_grid = range(2, 9)\n",
    "rows = []\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "for k in k_grid:\n",
    "    labels_k = KMeans(n_clusters=k, n_init=10, random_state=0).fit_predict(X_std)\n",
    "    rows.append({\"k\": int(k), \"dbi\": sk_davies_bouldin_score(X_std, labels_k)})\n",
    "\n",
    "df_scan = pd.DataFrame(rows)\n",
    "fig = px.line(df_scan, x=\"k\", y=\"dbi\", markers=True, title=\"sklearn: scan k and pick the minimum DBI\")\n",
    "fig.update_layout(xaxis=dict(dtick=1), xaxis_title=\"k\", yaxis_title=\"Davies–Bouldin index\")\n",
    "fig.show()\n",
    "\n",
    "df_scan.sort_values(\"dbi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Pros, cons, and common pitfalls\n",
    "\n",
    "### Pros\n",
    "- **No labels needed**: internal metric for unsupervised learning.\n",
    "- **Fast**: $O(n\\,d)$ to compute scatters plus $O(k^2 d)$ for centroid distances.\n",
    "- **Interpretable pieces**: explicitly trades off within-cluster scatter vs centroid separation.\n",
    "- **Good fit for centroid-based clustering**: especially $k$-means-like settings.\n",
    "\n",
    "### Cons\n",
    "- **Centroid assumption**: can be misleading for non-convex / manifold clusters (e.g., moons) where “centroid distance” is not the right notion of separation.\n",
    "- **Scale-sensitive**: feature units strongly affect Euclidean distances (standardization is often required).\n",
    "- **Not a supervised metric**: DBI needs cluster labels; it’s not meaningful for regression/classification objectives directly.\n",
    "- **Can favor too many clusters**: increasing $k$ often reduces within-cluster scatter, which can sometimes push DBI down even when clusters become less meaningful.\n",
    "\n",
    "### Common pitfalls / diagnostics\n",
    "- DBI is **undefined for $k=1$**.\n",
    "- If two clusters have identical centroids ($M_{ij}=0$), the ratio can blow up.\n",
    "- Always compare DBI **within a fixed preprocessing + distance setup** (don’t mix scaled/unscaled).\n",
    "\n",
    "### Where it tends to work well\n",
    "- Picking $k$ for **$k$-means** on roughly spherical blobs.\n",
    "- Fast internal validation when you want an alternative to silhouette score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Implement a variant where scatter uses RMS distance: $S_i = \\sqrt{\\frac{1}{|C_i|}\\sum_{x\\in C_i}\\lVert x-\\mu_i\\rVert_2^2}$. Compare it to scikit-learn’s definition on random datasets.\n",
    "\n",
    "2) Construct a dataset where DBI keeps decreasing as $k$ increases (over-segmentation). What does the geometry look like?\n",
    "\n",
    "3) Compare DBI and silhouette score on:\n",
    "- spherical blobs\n",
    "- two moons (non-convex)\n",
    "\n",
    "Which metric aligns better with human intuition in each case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Original paper: Davies, D. L., & Bouldin, D. W. (1979). *A cluster separation measure*.\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html\n",
    "- scikit-learn model evaluation guide: https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}