{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-measure score (`v_measure_score`)\n",
    "\n",
    "V-measure is an **external** clustering metric: it evaluates a clustering `labels_pred` using known ground-truth labels `labels_true`.\n",
    "\n",
    "It combines two complementary requirements:\n",
    "\n",
    "- **Homogeneity**: each predicted cluster contains only members of a single class (pure clusters)\n",
    "- **Completeness**: all members of a given class are assigned to the same cluster (do not split classes)\n",
    "\n",
    "V-measure is the (weighted) harmonic mean of homogeneity and completeness, so it is high only when **both** are high.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Build intuition for homogeneity vs completeness\n",
    "- Derive V-measure from entropy / mutual information\n",
    "- Implement `v_measure_score` from scratch in NumPy\n",
    "- Use V-measure to tune a simple clustering algorithm (K-means) when labels are available\n",
    "- Know pros/cons, pitfalls, and when to use it\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import v_measure_score\n",
    "```\n",
    "\n",
    "## When should you use it?\n",
    "\n",
    "Use V-measure when you have **ground-truth categories** (or a labeled validation set) and want to evaluate or compare clustering results.\n",
    "\n",
    "If you do *not* have labels, prefer **internal** metrics (e.g., silhouette) or task-specific evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition: merge vs split\n",
    "\n",
    "Think of the true labels as **colors** (classes) and the clustering output as **groups** (clusters).\n",
    "\n",
    "- If a cluster mixes many colors → it is **not homogeneous**.\n",
    "- If a color is scattered across many clusters → it is **not complete**.\n",
    "\n",
    "V-measure forces a balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    completeness_score,\n",
    "    homogeneity_score,\n",
    "    normalized_mutual_info_score,\n",
    "    v_measure_score,\n",
    ")\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up: four clusterings of the same labeled dataset\n",
    "\n",
    "Below we create four different `labels_pred` arrays for the same `labels_true`:\n",
    "\n",
    "1. Perfect (up to permutation of cluster IDs)\n",
    "2. Under-clustering: everything in one cluster (complete but not homogeneous)\n",
    "3. Over-clustering: split each class into multiple clusters (homogeneous but not complete)\n",
    "4. Random assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_class = 60\n",
    "labels_true = np.repeat(np.arange(3), n_per_class)\n",
    "\n",
    "# 1) perfect, but with a permutation of cluster IDs\n",
    "perm_map = {0: 2, 1: 0, 2: 1}\n",
    "labels_pred_perfect = np.vectorize(perm_map.get)(labels_true)\n",
    "\n",
    "# 2) under-clustering: everything in one cluster\n",
    "labels_pred_one_cluster = np.zeros_like(labels_true)\n",
    "\n",
    "# 3) over-clustering: split each class into 2 clusters\n",
    "#    class 0 -> clusters 0/1, class 1 -> clusters 2/3, class 2 -> clusters 4/5\n",
    "split_bit = np.arange(labels_true.size) % 2\n",
    "labels_pred_split_each_class = labels_true * 2 + split_bit\n",
    "\n",
    "# 4) random assignment into 3 clusters\n",
    "labels_pred_random = rng.integers(0, 3, size=labels_true.size)\n",
    "\n",
    "cases = {\n",
    "    'perfect (permute ids)': labels_pred_perfect,\n",
    "    'one cluster (merge classes)': labels_pred_one_cluster,\n",
    "    'split each class (over-cluster)': labels_pred_split_each_class,\n",
    "    'random (3 clusters)': labels_pred_random,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, labels_pred in cases.items():\n",
    "    h = homogeneity_score(labels_true, labels_pred)\n",
    "    c = completeness_score(labels_true, labels_pred)\n",
    "    v = v_measure_score(labels_true, labels_pred)\n",
    "    rows.extend(\n",
    "        [\n",
    "            {'case': name, 'metric': 'homogeneity', 'value': h},\n",
    "            {'case': name, 'metric': 'completeness', 'value': c},\n",
    "            {'case': name, 'metric': 'v_measure', 'value': v},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "fig = px.bar(\n",
    "    rows,\n",
    "    x='case',\n",
    "    y='value',\n",
    "    color='metric',\n",
    "    barmode='group',\n",
    "    title='Homogeneity vs completeness vs V-measure on simple labelings',\n",
    ")\n",
    "fig.update_layout(yaxis=dict(range=[0, 1.05]))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read the plot\n",
    "\n",
    "- **One cluster**: completeness is 1 (each class is fully contained in one cluster), but homogeneity is low (the cluster mixes classes).\n",
    "- **Split each class**: homogeneity is 1 (each cluster is pure), but completeness is low (each class is spread across clusters).\n",
    "- V-measure penalizes both extremes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition (information-theoretic)\n",
    "\n",
    "Let:\n",
    "\n",
    "- $C$ be the random variable for the true class label\n",
    "- $K$ be the random variable for the predicted cluster label\n",
    "- $n_{ck}$ be the contingency table counts (how many samples of class $c$ are assigned to cluster $k$)\n",
    "- $N = \\sum_{c,k} n_{ck}$\n",
    "\n",
    "From the contingency table we get marginals:\n",
    "\n",
    "- $n_c = \\sum_k n_{ck}$\n",
    "- $n_k = \\sum_c n_{ck}$\n",
    "- $p(c) = n_c / N$, $p(k) = n_k / N$, $p(c,k) = n_{ck} / N$\n",
    "\n",
    "### Entropy and mutual information\n",
    "\n",
    "$$\n",
    "H(C) = -\\sum_c p(c) \\log p(c), \\quad\n",
    "H(K) = -\\sum_k p(k) \\log p(k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "I(C;K) = \\sum_{c,k} p(c,k) \\log\\frac{p(c,k)}{p(c)p(k)}\n",
    "$$\n",
    "\n",
    "(Any log base works; V-measure is a ratio, so the base cancels.)\n",
    "\n",
    "### Homogeneity and completeness\n",
    "\n",
    "Homogeneity:\n",
    "\n",
    "$$\n",
    "h = 1 - \\frac{H(C\\mid K)}{H(C)} = \\frac{I(C;K)}{H(C)}\n",
    "$$\n",
    "\n",
    "Completeness:\n",
    "\n",
    "$$\n",
    "c = 1 - \\frac{H(K\\mid C)}{H(K)} = \\frac{I(C;K)}{H(K)}\n",
    "$$\n",
    "\n",
    "Edge cases:\n",
    "\n",
    "- If $H(C)=0$ (only one true class), define $h=1$.\n",
    "- If $H(K)=0$ (only one predicted cluster), define $c=1$.\n",
    "\n",
    "### V-measure\n",
    "\n",
    "With a weight $\\beta \\ge 0$ (bigger $\\beta$ emphasizes completeness):\n",
    "\n",
    "$$\n",
    "V_{\\beta} = \\frac{(1+\\beta)\\, h\\, c}{\\beta\\, h + c}\n",
    "$$\n",
    "\n",
    "For $\\beta=1$ it is symmetric and equivalent to *normalized mutual information* with arithmetic normalization:\n",
    "\n",
    "$$\n",
    "V_1 = \\frac{2 I(C;K)}{H(C)+H(K)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy implementation (from scratch)\n",
    "\n",
    "We will implement everything from the contingency table up.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- We use natural logarithms (`np.log`), matching scikit-learn.\n",
    "- The score is invariant to label permutations: only the contingency table matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_labels(labels):\n",
    "    labels = np.asarray(labels)\n",
    "    if labels.ndim != 1:\n",
    "        raise ValueError('labels must be 1D')\n",
    "    uniques, inv = np.unique(labels, return_inverse=True)\n",
    "    return uniques, inv\n",
    "\n",
    "\n",
    "def contingency_matrix_np(labels_true, labels_pred):\n",
    "    \"\"\"Contingency matrix n_{ck} with shape (n_classes, n_clusters).\"\"\"\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "\n",
    "    if labels_true.shape != labels_pred.shape:\n",
    "        raise ValueError('labels_true and labels_pred must have the same shape')\n",
    "    if labels_true.size == 0:\n",
    "        raise ValueError('empty label arrays')\n",
    "\n",
    "    classes, class_idx = _encode_labels(labels_true)\n",
    "    clusters, cluster_idx = _encode_labels(labels_pred)\n",
    "\n",
    "    cont = np.zeros((classes.size, clusters.size), dtype=np.int64)\n",
    "    np.add.at(cont, (class_idx, cluster_idx), 1)\n",
    "    return cont, classes, clusters\n",
    "\n",
    "\n",
    "def entropy_from_counts(counts):\n",
    "    \"\"\"Shannon entropy of a discrete distribution given counts (natural log).\"\"\"\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    total = counts.sum()\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    p = counts / total\n",
    "    p = p[p > 0]\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "\n",
    "def mutual_info_from_contingency(cont):\n",
    "    \"\"\"Mutual information I(C;K) from contingency matrix (natural log).\"\"\"\n",
    "    cont = np.asarray(cont, dtype=float)\n",
    "    n = cont.sum()\n",
    "    if n <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    pi = cont.sum(axis=1)  # class marginals\n",
    "    pj = cont.sum(axis=0)  # cluster marginals\n",
    "\n",
    "    i_idx, j_idx = np.nonzero(cont)\n",
    "    n_ij = cont[i_idx, j_idx]\n",
    "    return float(np.sum((n_ij / n) * np.log((n * n_ij) / (pi[i_idx] * pj[j_idx]))))\n",
    "\n",
    "\n",
    "def homogeneity_completeness_v_measure_np(labels_true, labels_pred, beta=1.0):\n",
    "    if beta < 0:\n",
    "        raise ValueError('beta must be >= 0')\n",
    "\n",
    "    cont, _, _ = contingency_matrix_np(labels_true, labels_pred)\n",
    "    h_c = entropy_from_counts(cont.sum(axis=1))\n",
    "    h_k = entropy_from_counts(cont.sum(axis=0))\n",
    "    mi = mutual_info_from_contingency(cont)\n",
    "\n",
    "    homogeneity = 1.0 if h_c == 0.0 else mi / h_c\n",
    "    completeness = 1.0 if h_k == 0.0 else mi / h_k\n",
    "\n",
    "    homogeneity = float(np.clip(homogeneity, 0.0, 1.0))\n",
    "    completeness = float(np.clip(completeness, 0.0, 1.0))\n",
    "\n",
    "    if homogeneity == 0.0 or completeness == 0.0:\n",
    "        v = 0.0\n",
    "    else:\n",
    "        v = (1.0 + beta) * homogeneity * completeness / (beta * homogeneity + completeness)\n",
    "\n",
    "    return homogeneity, completeness, float(v)\n",
    "\n",
    "\n",
    "def v_measure_score_np(labels_true, labels_pred, beta=1.0):\n",
    "    return homogeneity_completeness_v_measure_np(labels_true, labels_pred, beta=beta)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: compare to scikit-learn on random labelings\n",
    "def _check_against_sklearn(n_trials=200, n=200, n_classes=5, n_clusters=7):\n",
    "    for _ in range(n_trials):\n",
    "        y_true = rng.integers(0, n_classes, size=n)\n",
    "        y_pred = rng.integers(0, n_clusters, size=n)\n",
    "\n",
    "        h_np, c_np, v_np = homogeneity_completeness_v_measure_np(y_true, y_pred, beta=1.0)\n",
    "        h_sk = homogeneity_score(y_true, y_pred)\n",
    "        c_sk = completeness_score(y_true, y_pred)\n",
    "        v_sk = v_measure_score(y_true, y_pred)\n",
    "\n",
    "        if not (\n",
    "            np.isclose(h_np, h_sk, atol=1e-12, rtol=0)\n",
    "            and np.isclose(c_np, c_sk, atol=1e-12, rtol=0)\n",
    "            and np.isclose(v_np, v_sk, atol=1e-12, rtol=0)\n",
    "        ):\n",
    "            return False, (h_np, h_sk, c_np, c_sk, v_np, v_sk)\n",
    "\n",
    "    return True, None\n",
    "\n",
    "\n",
    "ok, debug = _check_against_sklearn()\n",
    "ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V-measure (beta=1) equals normalized mutual information with arithmetic normalization\n",
    "for name, labels_pred in cases.items():\n",
    "    v = v_measure_score(labels_true, labels_pred)\n",
    "    nmi = normalized_mutual_info_score(labels_true, labels_pred, average_method='arithmetic')\n",
    "    print(f'{name:30s}  v={v:.6f}  nmi(arithmetic)={nmi:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing what the metric sees: contingency tables\n",
    "\n",
    "V-measure only depends on how true classes and predicted clusters overlap.\n",
    "\n",
    "A perfect clustering produces a contingency table that looks like a permutation of the identity matrix.\n",
    "\n",
    "- Under-clustering (merging classes) creates columns with many non-zero rows.\n",
    "- Over-clustering (splitting classes) creates rows with many non-zero columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=list(cases.keys()),\n",
    "    horizontal_spacing=0.08,\n",
    "    vertical_spacing=0.14,\n",
    ")\n",
    "\n",
    "for i, (name, labels_pred) in enumerate(cases.items()):\n",
    "    cont, classes, clusters = contingency_matrix_np(labels_true, labels_pred)\n",
    "    r, c = i // 2 + 1, i % 2 + 1\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=cont,\n",
    "            x=[str(k) for k in clusters],\n",
    "            y=[str(c_) for c_ in classes],\n",
    "            colorscale='Blues',\n",
    "            showscale=i == 0,\n",
    "            hovertemplate='true=%{y}<br>cluster=%{x}<br>count=%{z}<extra></extra>',\n",
    "        ),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Contingency tables: true class (rows) × predicted cluster (cols)',\n",
    "    height=650,\n",
    ")\n",
    "fig.update_xaxes(title_text='predicted cluster')\n",
    "fig.update_yaxes(title_text='true class')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\beta$ parameter: choose which mistake hurts more\n",
    "\n",
    "- Larger $\\beta$ emphasizes **completeness** (do not split classes).\n",
    "- Smaller $\\beta$ emphasizes **homogeneity** (do not mix classes).\n",
    "\n",
    "Below, compare how $V_{\\beta}$ changes for a merge-style mistake vs a split-style mistake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.logspace(-2, 2, 250)\n",
    "\n",
    "beta_rows = []\n",
    "for beta in betas:\n",
    "    for name, labels_pred in {\n",
    "        'merge classes (one cluster)': labels_pred_one_cluster,\n",
    "        'split classes (over-cluster)': labels_pred_split_each_class,\n",
    "    }.items():\n",
    "        v = v_measure_score_np(labels_true, labels_pred, beta=float(beta))\n",
    "        beta_rows.append({'beta': beta, 'case': name, 'v_measure': v})\n",
    "\n",
    "fig = px.line(\n",
    "    beta_rows,\n",
    "    x='beta',\n",
    "    y='v_measure',\n",
    "    color='case',\n",
    "    log_x=True,\n",
    "    title='Effect of beta on V-measure',\n",
    ")\n",
    "fig.add_vline(x=1.0, line_dash='dash', line_color='gray')\n",
    "fig.update_layout(yaxis=dict(range=[0, 1.05]))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using V-measure to optimize a simple algorithm (K-means)\n",
    "\n",
    "V-measure is **not differentiable** w.r.t. cluster assignments, so you typically do not optimize it with gradient descent.\n",
    "\n",
    "Instead, you use it for **model selection** when labels are available, e.g.:\n",
    "\n",
    "- pick the number of clusters $k$\n",
    "- pick the best initialization / run among many\n",
    "- tune algorithm hyperparameters\n",
    "\n",
    "Below we:\n",
    "\n",
    "1. Generate a labeled 2D dataset (three Gaussian blobs)\n",
    "2. Run a low-level NumPy K-means implementation for different $k$\n",
    "3. Choose the $k$ that maximizes V-measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.array([[-2.0, 0.0], [2.0, 0.0], [0.0, 3.0]])\n",
    "cluster_std = 0.6\n",
    "n_per_center = 200\n",
    "\n",
    "X_parts = []\n",
    "y_parts = []\n",
    "for i, mu in enumerate(centers):\n",
    "    X_i = rng.normal(loc=mu, scale=cluster_std, size=(n_per_center, 2))\n",
    "    y_i = np.full(n_per_center, i)\n",
    "    X_parts.append(X_i)\n",
    "    y_parts.append(y_i)\n",
    "\n",
    "X = np.vstack(X_parts)\n",
    "y_true = np.concatenate(y_parts)\n",
    "\n",
    "perm = rng.permutation(X.shape[0])\n",
    "X = X[perm]\n",
    "y_true = y_true[perm]\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y_true.astype(str),\n",
    "    title='Synthetic dataset (colored by true class)',\n",
    "    labels={'x': 'x1', 'y': 'x2', 'color': 'true class'},\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_np(X, k, *, n_init=10, max_iter=100, rng=None):\n",
    "    \"\"\"A small NumPy K-means implementation (Lloyd's algorithm).\n",
    "\n",
    "    Returns: (labels, centroids, inertia)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError('X must be 2D')\n",
    "    if k <= 0:\n",
    "        raise ValueError('k must be >= 1')\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    rng = np.random.default_rng(rng)\n",
    "\n",
    "    best_inertia = np.inf\n",
    "    best_labels = None\n",
    "    best_centroids = None\n",
    "\n",
    "    for _ in range(n_init):\n",
    "        init_idx = rng.choice(n_samples, size=k, replace=n_samples < k)\n",
    "        centroids = X[init_idx].copy()\n",
    "\n",
    "        labels = None\n",
    "        for _ in range(max_iter):\n",
    "            d2 = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n",
    "            new_labels = d2.argmin(axis=1)\n",
    "\n",
    "            if labels is not None and np.array_equal(new_labels, labels):\n",
    "                break\n",
    "            labels = new_labels\n",
    "\n",
    "            new_centroids = centroids.copy()\n",
    "            for j in range(k):\n",
    "                mask = labels == j\n",
    "                if not np.any(mask):\n",
    "                    new_centroids[j] = X[rng.integers(0, n_samples)]\n",
    "                else:\n",
    "                    new_centroids[j] = X[mask].mean(axis=0)\n",
    "            centroids = new_centroids\n",
    "\n",
    "        inertia = float(((X - centroids[labels]) ** 2).sum())\n",
    "        if inertia < best_inertia:\n",
    "            best_inertia = inertia\n",
    "            best_labels = labels.copy()\n",
    "            best_centroids = centroids.copy()\n",
    "\n",
    "    return best_labels, best_centroids, best_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for k in range(2, 9):\n",
    "    labels_pred, centroids_k, inertia = kmeans_np(X, k, n_init=20, max_iter=200, rng=123)\n",
    "    h, c, v = homogeneity_completeness_v_measure_np(y_true, labels_pred, beta=1.0)\n",
    "    results.append(\n",
    "        {\n",
    "            'k': k,\n",
    "            'homogeneity': h,\n",
    "            'completeness': c,\n",
    "            'v_measure': v,\n",
    "            'inertia': inertia,\n",
    "            'labels_pred': labels_pred,\n",
    "            'centroids': centroids_k,\n",
    "        }\n",
    "    )\n",
    "\n",
    "best = max(results, key=lambda r: r['v_measure'])\n",
    "best['k'], best['v_measure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = []\n",
    "for r in results:\n",
    "    for m in ['homogeneity', 'completeness', 'v_measure']:\n",
    "        long.append({'k': r['k'], 'metric': m, 'value': r[m]})\n",
    "\n",
    "fig1 = px.line(\n",
    "    long,\n",
    "    x='k',\n",
    "    y='value',\n",
    "    color='metric',\n",
    "    markers=True,\n",
    "    title='External model selection with labels: maximize V-measure',\n",
    ")\n",
    "fig1.update_layout(yaxis=dict(range=[0, 1.05]))\n",
    "\n",
    "results_summary = [{'k': r['k'], 'inertia': r['inertia']} for r in results]\n",
    "fig2 = px.line(\n",
    "    results_summary,\n",
    "    x='k',\n",
    "    y='inertia',\n",
    "    markers=True,\n",
    "    title='Inertia (K-means objective) always improves with larger k',\n",
    ")\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[fig1.layout.title.text, fig2.layout.title.text])\n",
    "for tr in fig1.data:\n",
    "    fig.add_trace(tr, row=1, col=1)\n",
    "for tr in fig2.data:\n",
    "    fig.add_trace(tr, row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=420, showlegend=True)\n",
    "fig.update_yaxes(range=[0, 1.05], row=1, col=1)\n",
    "fig.add_vline(x=best['k'], line_dash='dash', line_color='gray', row=1, col=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_best = best['labels_pred']\n",
    "centroids_best = best['centroids']\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=labels_best.astype(str),\n",
    "    title=f\"K-means clustering for k={best['k']} (colored by predicted cluster)\",\n",
    "    labels={'x': 'x1', 'y': 'x2', 'color': 'cluster'},\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centroids_best[:, 0],\n",
    "        y=centroids_best[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(color='black', size=12, symbol='x'),\n",
    "        name='centroids',\n",
    "    )\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros, cons, and where it is useful\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Permutation-invariant (cluster IDs do not matter)\n",
    "- Bounded in $[0,1]$ and easy to compare across runs\n",
    "- Works when the number of clusters differs from the number of classes\n",
    "- Decomposes into two interpretable parts (homogeneity vs completeness)\n",
    "- With $\\beta=1$ it equals NMI with arithmetic normalization\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- Requires ground-truth labels (external metric)\n",
    "- Can be pushed toward extremes:\n",
    "  - many tiny clusters → high homogeneity\n",
    "  - one giant cluster → high completeness\n",
    "- Ignores geometry/distances: it only evaluates the final label assignments\n",
    "- Not differentiable: typically used for evaluation or hyperparameter search, not gradient-based training\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- Benchmarking clustering algorithms on labeled datasets\n",
    "- Hyperparameter selection when you have a labeled validation set (semi-supervised model selection)\n",
    "- Comparing runs/initializations of a clustering method when labels are available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common diagnostics and pitfalls\n",
    "\n",
    "- Always inspect the contingency matrix: it explains why V-measure is high/low.\n",
    "- Check homogeneity and completeness separately before trusting the combined score.\n",
    "- Choose $\\beta$ based on what errors matter:\n",
    "  - if splitting a class is very bad → use larger $\\beta$\n",
    "  - if mixing classes is very bad → use smaller $\\beta$\n",
    "- If you do not have labels, V-measure cannot be computed; use internal metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Increase the number of clusters in the random case and see how homogeneity changes.\n",
    "2. Create an imbalanced dataset (one class much larger) and see how the score behaves.\n",
    "3. Compare V-measure to adjusted mutual information (AMI) and adjusted Rand index (ARI).\n",
    "4. Change $\\beta$ and pick the $k$ that maximizes $V_{\\beta}$ in the K-means section.\n",
    "\n",
    "## References\n",
    "\n",
    "- Rosenberg & Hirschberg (2007): *V-measure: A conditional entropy-based external cluster evaluation measure*\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure\n",
    "- API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}