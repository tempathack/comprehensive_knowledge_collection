{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "cells": [
    {
      "id": "8456fdbb",
      "cell_type": "markdown",
      "source": "# Rand Score (Rand Index)\n\n`rand_score` (the **Rand index**) measures how similar two clusterings / partitions are by looking at **all pairs** of samples.\n\nA good mental model:\n\n- Each clustering turns the dataset into a binary relation: for every pair `(i, j)`, are they in the **same cluster** or **different clusters**?\n- The Rand index is the **accuracy** of those pairwise decisions.\n\n## Quick import\n\n```python\nfrom sklearn.metrics import rand_score\n```\n\n---\n\n## Learning goals\n\n- Understand Rand index as **pairwise agreement** (TP/TN/FP/FN over pairs)\n- Implement `rand_score` from scratch in NumPy (efficiently, without an `O(n^2)` loop)\n- Visualize where the score comes from on a small example\n- See common pitfalls (why it can be **inflated** when there are many clusters)\n- Use Rand score to **select** a simple clustering hyperparameter\n\n## Prerequisites\n\n- NumPy basics (`shape`, broadcasting, `np.unique`)\n- Combinatorics: number of pairs `\\(\binom{n}{2}\\)`\n",
      "metadata": {}
    },
    {
      "id": "482ee8b8",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.metrics import rand_score\nfrom sklearn.metrics.cluster import pair_confusion_matrix\nfrom sklearn.metrics import adjusted_rand_score\n\npio.templates.default = \"plotly_white\"\npio.renderers.default = \"notebook\"\nnp.set_printoptions(precision=4, suppress=True)\n\nrng = np.random.default_rng(42)\n",
      "outputs": []
    },
    {
      "id": "28f6997e",
      "cell_type": "markdown",
      "source": "## 1) Definition: Rand index as pairwise accuracy\n\nLet there be `n` samples and two labelings:\n\n- ground truth (or reference) labels: \\(y\\)\n- predicted clustering labels: \\(\\hat{y}\\)\n\nFor every unordered pair \\((i, j)\\) with \\(i < j\\), define two *same-cluster indicators*:\n\n$$\nS_y(i, j) = \\mathbb{1}[y_i = y_j],\n\\qquad\nS_{\\hat{y}}(i, j) = \\mathbb{1}[\\hat{y}_i = \\hat{y}_j]\n$$\n\nNow count pairs into the familiar 2\u00d72 confusion matrix:\n\n- **TP**: same in `y` and same in `\\(\\hat{y}\\)`\n- **TN**: different in `y` and different in `\\(\\hat{y}\\)`\n- **FP**: different in `y` but same in `\\(\\hat{y}\\)` (a *merge* error)\n- **FN**: same in `y` but different in `\\(\\hat{y}\\)` (a *split* error)\n\nFormally:\n\n$$\n\begin{aligned}\n\\mathrm{TP} &= \\sum_{i<j} S_y(i,j)\\,S_{\\hat{y}}(i,j) \\\n\\mathrm{TN} &= \\sum_{i<j} (1-S_y(i,j))\\,(1-S_{\\hat{y}}(i,j)) \\\n\\mathrm{FP} &= \\sum_{i<j} (1-S_y(i,j))\\,S_{\\hat{y}}(i,j) \\\n\\mathrm{FN} &= \\sum_{i<j} S_y(i,j)\\,(1-S_{\\hat{y}}(i,j))\n\\end{aligned}\n$$\n\nThere are \\(\binom{n}{2}\\) total pairs, so the **Rand index** is:\n\n$$\n\\mathrm{RI}(y, \\hat{y}) = \frac{\\mathrm{TP} + \\mathrm{TN}}{\binom{n}{2}}\n$$\n\nNote: scikit-learn's `pair_confusion_matrix` counts **ordered** pairs `(i, j)` with `i != j`,\nso its entries sum to `n(n-1)` and are exactly **2\u00d7** the `i<j` counts above.\n\nKey properties:\n\n- **Label permutation invariance**: only equality matters; relabeling clusters doesn't change the score.\n- **Symmetry**: \\(\\mathrm{RI}(y,\\hat{y}) = \\mathrm{RI}(\\hat{y},y)\\).\n- **Range**: \\([0,1]\\). For `n < 2`, scikit-learn returns `1.0` by convention.\n",
      "metadata": {}
    },
    {
      "id": "dc035d4b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# A tiny toy example (small n so we can visualize all pairs)\ny_true = np.array([0, 0, 0, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 0, 0, 0, 2, 3, 3])\n\nprint('y_true:', y_true)\nprint('y_pred:', y_pred)\n\nprint('rand_score (sklearn):', rand_score(y_true, y_pred))\n\npcm_ord = pair_confusion_matrix(y_true, y_pred)\nprint('pair_confusion_matrix (sklearn, ordered pairs):')\nprint(pcm_ord)\nprint('pair confusion on i<j (unordered) is pcm_ord // 2:')\nprint(pcm_ord // 2)\n",
      "outputs": []
    },
    {
      "id": "ca806e66",
      "cell_type": "markdown",
      "source": "### Visual intuition: every clustering is a pairwise matrix\n\nFor a labeling `y`, define an \\(n\times n\\) matrix:\n\n$$\nA_y[i, j] = \\mathbb{1}[y_i = y_j]\n$$\n\n- Blocks of 1s correspond to clusters.\n- Comparing \\(A_y\\) and \\(A_{\\hat{y}}\\) tells you which pairs are treated the same.\n\nBelow we color each pair `(i, j)` as:\n\n- **TP**: same in both\n- **TN**: different in both\n- **FP**: different in true, same in pred\n- **FN**: same in true, different in pred\n",
      "metadata": {}
    },
    {
      "id": "38788ee6",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def pair_category_matrix(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n\n    same_true = y_true[:, None] == y_true[None, :]\n    same_pred = y_pred[:, None] == y_pred[None, :]\n\n    # 0=TN, 1=FP, 2=FN, 3=TP\n    cat = np.zeros_like(same_true, dtype=int)\n    cat[(~same_true) & same_pred] = 1\n    cat[same_true & (~same_pred)] = 2\n    cat[same_true & same_pred] = 3\n\n    np.fill_diagonal(cat, -1)  # diagonal isn't a pair\n    return cat\n\n\ncat = pair_category_matrix(y_true, y_pred)\n\n# Count only i<j pairs\nupper_mask = np.triu(np.ones_like(cat, dtype=bool), k=1)\nvals = cat[upper_mask]\n\ncounts = {\n    \"TN (different/different)\": int((vals == 0).sum()),\n    \"FP (merge error)\": int((vals == 1).sum()),\n    \"FN (split error)\": int((vals == 2).sum()),\n    \"TP (same/same)\": int((vals == 3).sum()),\n}\n\naxis_labels = [f\"{i}<br>t={y_true[i]}, p={y_pred[i]}\" for i in range(len(y_true))]\n\nname_map = {\n    -1: \"diag\",\n    0: \"TN (different/different)\",\n    1: \"FP (merge error)\",\n    2: \"FN (split error)\",\n    3: \"TP (same/same)\",\n}\ncat_name = np.vectorize(name_map.get)(cat)\n\nc_diag = \"#ffffff\"\nc_tn = \"#d9d9d9\"\nc_fp = \"#ff7f0e\"\nc_fn = \"#1f77b4\"\nc_tp = \"#2ca02c\"\n\n# With zmin=-1, zmax=3, normalized boundaries between integers are:\n# -0.5 -> 0.125, 0.5 -> 0.375, 1.5 -> 0.625, 2.5 -> 0.875\ncolorscale = [\n    [0.0, c_diag],\n    [0.1249, c_diag],\n    [0.125, c_tn],\n    [0.3749, c_tn],\n    [0.375, c_fp],\n    [0.6249, c_fp],\n    [0.625, c_fn],\n    [0.8749, c_fn],\n    [0.875, c_tp],\n    [1.0, c_tp],\n]\n\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    column_widths=[0.72, 0.28],\n    subplot_titles=(\n        \"Pair categories (upper triangle is unique)\",\n        \"Pair counts (i<j)\",\n    ),\n)\n\nfig.add_trace(\n    go.Heatmap(\n        z=cat,\n        x=axis_labels,\n        y=axis_labels,\n        zmin=-1,\n        zmax=3,\n        colorscale=colorscale,\n        showscale=False,\n        customdata=cat_name,\n        hovertemplate=\"%{y} vs %{x}<br>%{customdata}<extra></extra>\",\n    ),\n    row=1,\n    col=1,\n)\n\nfig.add_trace(\n    go.Bar(\n        x=list(counts.keys()),\n        y=list(counts.values()),\n        text=list(counts.values()),\n        textposition=\"outside\",\n        marker_color=[c_tn, c_fp, c_fn, c_tp],\n    ),\n    row=1,\n    col=2,\n)\n\nfig.update_layout(\n    title=\"Rand index works on pairs: agreements (TP/TN) vs disagreements (FP/FN)\",\n    xaxis=dict(tickangle=0),\n)\n\nfig.update_xaxes(title_text=\"sample index (with labels)\", row=1, col=1)\nfig.update_yaxes(title_text=\"sample index (with labels)\", row=1, col=1)\nfig.update_xaxes(tickangle=45, row=1, col=2)\nfig.update_yaxes(title_text=\"count\", row=1, col=2)\n\nfig.show()\n",
      "outputs": []
    },
    {
      "id": "26975ef8",
      "cell_type": "markdown",
      "source": "## 2) Efficient computation (without looping over all pairs)\n\nA naive implementation checks all \\(\binom{n}{2}\\) pairs, which is `O(n^2)`.\n\nWe can compute the same TP/FP/FN/TN counts using a **contingency table**.\n\nLet \\(n_{ij}\\) be the number of samples that are in:\n\n- true cluster \\(i\\), and\n- predicted cluster \\(j\\).\n\nThis forms a matrix \\(N = [n_{ij}]\\).\n\n### Key combinatorial identity\n\nThe number of unordered pairs inside a group of size \\(m\\) is:\n\n$$\n\binom{m}{2} = \frac{m(m-1)}{2}\n$$\n\n### Counting pairs via the contingency table\n\n- Pairs that are together in **both** partitions:\n\n$$\n\\mathrm{TP} = \\sum_{i,j} \binom{n_{ij}}{2}\n$$\n\n- Pairs that are together in **true** labels:\n\n$$\n\\sum_i \binom{n_{i\\cdot}}{2}\n\\quad\text{where } n_{i\\cdot}=\\sum_j n_{ij}\n$$\n\n- Pairs that are together in **pred** labels:\n\n$$\n\\sum_j \binom{n_{\\cdot j}}{2}\n\\quad\text{where } n_{\\cdot j}=\\sum_i n_{ij}\n$$\n\nThen:\n\n$$\n\\mathrm{FN} = \\sum_i \binom{n_{i\\cdot}}{2} - \\mathrm{TP}\n\\qquad\n\\mathrm{FP} = \\sum_j \binom{n_{\\cdot j}}{2} - \\mathrm{TP}\n$$\n\nFinally:\n\n$$\n\\mathrm{TN} = \binom{n}{2} - (\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN})\n$$\n\nThis gives an `O(n + k_y k_\\hat{y})` implementation (where `k` is number of clusters).\n",
      "metadata": {}
    },
    {
      "id": "b875320c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def comb2(x):\n    '''Unordered pairs: C(m, 2) = m(m-1)/2.'''\n    x = np.asarray(x, dtype=np.int64)\n    return (x * (x - 1)) // 2\n\n\ndef pairs_ordered(x):\n    '''Ordered pairs with i!=j inside a group: m(m-1) = 2*C(m,2).'''\n    x = np.asarray(x, dtype=np.int64)\n    return x * (x - 1)\n\n\ndef _labels_to_integers(labels):\n    labels = np.asarray(labels)\n    _, inv = np.unique(labels, return_inverse=True)\n    return inv\n\n\ndef contingency_matrix_np(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n\n    true_inv = _labels_to_integers(y_true)\n    pred_inv = _labels_to_integers(y_pred)\n\n    n_true = int(true_inv.max(initial=-1)) + 1\n    n_pred = int(pred_inv.max(initial=-1)) + 1\n\n    cont = np.zeros((n_true, n_pred), dtype=np.int64)\n    np.add.at(cont, (true_inv, pred_inv), 1)\n    return cont\n\n\ndef pair_confusion_matrix_np(y_true, y_pred):\n    '''Match sklearn.metrics.cluster.pair_confusion_matrix.\n\n    scikit-learn counts ordered pairs (i, j) with i != j, so totals sum to n*(n-1).\n    '''\n\n    y_true = np.asarray(y_true)\n    n = int(y_true.size)\n\n    if n < 2:\n        return np.array([[0, 0], [0, 0]], dtype=np.int64)\n\n    cont = contingency_matrix_np(y_true, y_pred)\n\n    tp = int(pairs_ordered(cont).sum())\n    same_true = int(pairs_ordered(cont.sum(axis=1)).sum())\n    same_pred = int(pairs_ordered(cont.sum(axis=0)).sum())\n\n    fn = same_true - tp\n    fp = same_pred - tp\n\n    total_pairs = n * (n - 1)\n    tn = total_pairs - tp - fp - fn\n\n    return np.array([[tn, fp], [fn, tp]], dtype=np.int64)\n\n\ndef rand_score_np(y_true, y_pred):\n    '''Rand index over unordered pairs (i<j), matching sklearn.metrics.rand_score.'''\n\n    y_true = np.asarray(y_true)\n    n = int(y_true.size)\n    if n < 2:\n        return 1.0\n\n    cont = contingency_matrix_np(y_true, y_pred)\n\n    tp = float(comb2(cont).sum())\n    same_true = float(comb2(cont.sum(axis=1)).sum())\n    same_pred = float(comb2(cont.sum(axis=0)).sum())\n\n    fn = same_true - tp\n    fp = same_pred - tp\n\n    total_pairs = float(comb2(n))\n    tn = total_pairs - tp - fp - fn\n\n    return (tp + tn) / total_pairs\n",
      "outputs": []
    },
    {
      "id": "b5046147",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Sanity checks vs scikit-learn\n\nfor n in [0, 1, 5, 50]:\n    y_true = rng.integers(0, 4, size=n)\n    y_pred = rng.integers(0, 6, size=n)\n\n    ri_np = rand_score_np(y_true, y_pred)\n    ri_sk = rand_score(y_true, y_pred)\n\n    pcm_np = pair_confusion_matrix_np(y_true, y_pred)\n    pcm_sk = pair_confusion_matrix(y_true, y_pred)\n\n    print(f\"n={n:>2}  rand_score close? {np.isclose(ri_np, ri_sk)}  pair_confusion close? {np.all(pcm_np == pcm_sk)}\")\n",
      "outputs": []
    },
    {
      "id": "dee9c79f",
      "cell_type": "markdown",
      "source": "## 3) Properties worth remembering\n\n### 3.1) Invariant to relabeling\n\nIf you permute cluster IDs (e.g., swap labels `0` and `1`), the Rand score **does not change**.\n\nThat\u2019s essential for clustering evaluation: cluster IDs are arbitrary.\n",
      "metadata": {}
    },
    {
      "id": "b7dc36e4",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "y_true = np.array([0, 0, 1, 1, 2, 2])\ny_pred = np.array([10, 10, 11, 11, 12, 12])\n\nperm = {10: 12, 11: 10, 12: 11}\ny_pred_perm = np.vectorize(perm.get)(y_pred)\n\nprint('original:', rand_score_np(y_true, y_pred))\nprint('permuted:', rand_score_np(y_true, y_pred_perm))\n",
      "outputs": []
    },
    {
      "id": "887f50cb",
      "cell_type": "markdown",
      "source": "### 3.2) Extremes and trivial solutions\n\nSome degenerate clusterings can get surprisingly high Rand scores.\n\nTwo extremes:\n\n- **All-in-one cluster**: predicts every pair is \"same\".\n- **All-singletons**: predicts every pair is \"different\".\n\nBecause real datasets often have *far more different-cluster pairs than same-cluster pairs*, the \"all-singletons\" solution can score high even though it's useless.\n",
      "metadata": {}
    },
    {
      "id": "ebadbb0f",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def all_in_one(n):\n    return np.zeros(n, dtype=int)\n\ndef all_singletons(n):\n    return np.arange(n, dtype=int)\n\nn = 200\n# True labels: 4 balanced clusters\nk_true = 4\nsizes = np.full(k_true, n // k_true)\ny_true = np.repeat(np.arange(k_true), sizes)\n\ny_all1 = all_in_one(n)\ny_single = all_singletons(n)\n\ndef summarize(y_pred, name):\n    pcm_ord = pair_confusion_matrix_np(y_true, y_pred)\n    ri = rand_score_np(y_true, y_pred)\n    return name, ri, pcm_ord\n\nfor name, ri, pcm_ord in [\n    summarize(y_true, 'perfect'),\n    summarize(y_all1, 'all-in-one'),\n    summarize(y_single, 'all-singletons'),\n]:\n    tn, fp, fn, tp = pcm_ord.ravel()\n    print(f\"{name:>14}  RI={ri:.4f}  TP={tp:,}  TN={tn:,}  FP={fp:,}  FN={fn:,}  (ordered pairs)\")\n",
      "outputs": []
    },
    {
      "id": "0106faa9",
      "cell_type": "markdown",
      "source": "## 4) Pitfall: Rand score is *not* adjusted for chance\n\nIf you assign labels **at random**, the expected Rand score is not a constant baseline like 0.\n\nIn fact, when the predicted clustering has many clusters, most pairs become \"different\" in the prediction, which creates lots of **TN agreements**.\n\nThis is a big reason why practitioners often prefer:\n\n- **Adjusted Rand index** (`adjusted_rand_score` / ARI): corrected so random labelings score around 0.\n\nWe'll simulate this effect.\n",
      "metadata": {}
    },
    {
      "id": "2a416a6f",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "n = 200\nk_true = 4\nsizes = np.full(k_true, n // k_true)\ny_true = np.repeat(np.arange(k_true), sizes)\n\nk_pred_values = [2, 4, 8, 16, 32, 64, 128]\nreps = 200\n\nrand_scores = {k: [] for k in k_pred_values}\nadj_rand_scores = {k: [] for k in k_pred_values}\n\nfor k in k_pred_values:\n    for _ in range(reps):\n        y_pred = rng.integers(0, k, size=n)\n        rand_scores[k].append(rand_score_np(y_true, y_pred))\n        adj_rand_scores[k].append(adjusted_rand_score(y_true, y_pred))\n\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    subplot_titles=(\"Rand score (unadjusted)\", \"Adjusted Rand score (ARI)\"),\n)\n\nfor k in k_pred_values:\n    fig.add_trace(go.Box(y=rand_scores[k], name=f\"k={k}\", boxmean=True, showlegend=False), row=1, col=1)\n    fig.add_trace(go.Box(y=adj_rand_scores[k], name=f\"k={k}\", boxmean=True, showlegend=False), row=1, col=2)\n\nfig.update_layout(title=\"Random predictions: Rand score inflates as #clusters increases\")\nfig.update_yaxes(title_text=\"score\", row=1, col=1)\nfig.update_yaxes(title_text=\"score\", row=1, col=2)\nfig.show()\n",
      "outputs": []
    },
    {
      "id": "1acfd82a",
      "cell_type": "markdown",
      "source": "## 5) Using Rand score to select a simple clustering model (k-means example)\n\nRand score is an **external** clustering metric: it requires reference labels.\n\nThat makes it useful when:\n\n- you're benchmarking clustering algorithms on labeled datasets, or\n- you're tuning hyperparameters in a *semi-supervised evaluation* setting.\n\nBelow we:\n\n1. create synthetic blobs with known cluster IDs\n2. run a small **NumPy k-means** implementation for different `k`\n3. pick `k` that maximizes Rand score\n\nImportant: in real unsupervised tasks you typically don't have `y_true`, so you'd use internal metrics (e.g., silhouette) instead.\n",
      "metadata": {}
    },
    {
      "id": "7d140196",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Synthetic 2D blobs\ncenters = np.array([[-2.0, -2.0], [2.0, -2.0], [-2.0, 2.0], [2.0, 2.0]])\ncluster_std = 0.55\nn_per = 80\n\nX_list = []\ny_true = []\nfor k, c in enumerate(centers):\n    Xk = rng.normal(loc=c, scale=cluster_std, size=(n_per, 2))\n    X_list.append(Xk)\n    y_true.append(np.full(n_per, k, dtype=int))\n\nX = np.vstack(X_list)\ny_true = np.concatenate(y_true)\n\n# Shuffle\nperm = rng.permutation(len(X))\nX = X[perm]\ny_true = y_true[perm]\n\nfig = px.scatter(\n    x=X[:, 0],\n    y=X[:, 1],\n    color=y_true.astype(str),\n    title=\"Synthetic data (ground truth clusters)\",\n    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"true cluster\"},\n)\nfig.show()\n\n\ndef kmeans_np(X, k, *, n_init=10, max_iter=100, tol=1e-4, rng=None):\n    if rng is None:\n        rng = np.random.default_rng(0)\n\n    n, d = X.shape\n    best_inertia = np.inf\n    best_centroids = None\n    best_labels = None\n\n    for _ in range(n_init):\n        centroids = X[rng.choice(n, size=k, replace=False)].copy()\n\n        for _ in range(max_iter):\n            # squared distances: (n, k)\n            d2 = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n            labels = d2.argmin(axis=1)\n\n            new_centroids = np.empty_like(centroids)\n            for j in range(k):\n                mask = labels == j\n                if mask.any():\n                    new_centroids[j] = X[mask].mean(axis=0)\n                else:\n                    # handle empty clusters by re-seeding to a random point\n                    new_centroids[j] = X[rng.integers(0, n)]\n\n            shift = np.linalg.norm(new_centroids - centroids)\n            centroids = new_centroids\n\n            if shift < tol:\n                break\n\n        inertia = float(((X - centroids[labels]) ** 2).sum())\n        if inertia < best_inertia:\n            best_inertia = inertia\n            best_centroids = centroids\n            best_labels = labels\n\n    return best_centroids, best_labels, best_inertia\n\n\nk_values = list(range(2, 9))\nresults = []\n\nfor k in k_values:\n    centroids, labels, inertia = kmeans_np(X, k, n_init=15, max_iter=100, tol=1e-4, rng=rng)\n    ri = rand_score_np(y_true, labels)\n    ari = adjusted_rand_score(y_true, labels)\n    results.append((k, ri, ari, inertia))\n\nresults = np.array(results, dtype=float)\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Rand score vs k\", \"ARI vs k\"))\n\nfig.add_trace(go.Scatter(x=results[:, 0], y=results[:, 1], mode='lines+markers', name='RI'), row=1, col=1)\nfig.add_trace(go.Scatter(x=results[:, 0], y=results[:, 2], mode='lines+markers', name='ARI'), row=1, col=2)\n\nfig.update_xaxes(title_text=\"k\", row=1, col=1)\nfig.update_xaxes(title_text=\"k\", row=1, col=2)\nfig.update_yaxes(title_text=\"score\", row=1, col=1)\nfig.update_yaxes(title_text=\"score\", row=1, col=2)\n\nfig.update_layout(title=\"Selecting k with external labels (demo)\")\nfig.show()\n\nbest_k = int(results[results[:, 1].argmax(), 0])\nprint('best k by Rand score:', best_k)\n\ncentroids, labels_best, _ = kmeans_np(X, best_k, n_init=25, rng=rng)\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Ground truth\", f\"k-means prediction (k={best_k})\"))\n\nfig.add_trace(\n    go.Scatter(x=X[:, 0], y=X[:, 1], mode='markers', marker=dict(color=y_true, colorscale='Viridis', size=6), showlegend=False),\n    row=1,\n    col=1,\n)\n\nfig.add_trace(\n    go.Scatter(x=X[:, 0], y=X[:, 1], mode='markers', marker=dict(color=labels_best, colorscale='Viridis', size=6), showlegend=False),\n    row=1,\n    col=2,\n)\n\nfig.update_xaxes(title_text=\"x1\", row=1, col=1)\nfig.update_xaxes(title_text=\"x1\", row=1, col=2)\nfig.update_yaxes(title_text=\"x2\", row=1, col=1)\nfig.update_yaxes(title_text=\"x2\", row=1, col=2)\n\nfig.update_layout(title=\"Clustering visualization (labels are arbitrary, Rand index is invariant)\")\nfig.show()\n",
      "outputs": []
    },
    {
      "id": "2a39a1b6",
      "cell_type": "markdown",
      "source": "## 6) Pros, cons, and when to use it\n\n### Pros\n\n- **Permutation invariant**: cluster IDs don't matter.\n- **Interpretable**: literally the fraction of pairwise agreements.\n- **Works for any number of clusters** and doesn't require distances/geometry.\n- **Fast to compute** via contingency counts (no need to enumerate all pairs).\n\n### Cons / pitfalls\n\n- **Not adjusted for chance**: random clusterings can get high RI, especially with many clusters.\n- Often dominated by **TN** (different/different) pairs, which can hide meaningful errors.\n- Can reward **over-segmentation** (too many clusters) in some settings.\n- **Not differentiable**: not suitable as a gradient-based training loss.\n\n### Good use cases\n\n- External evaluation of clustering when you have reference labels (benchmarks, ablations).\n- Comparing two clusterings of the same dataset when you care about pairwise co-membership.\n\nIf you need a chance-corrected baseline, prefer **ARI** (`adjusted_rand_score`).\n",
      "metadata": {}
    },
    {
      "id": "77dd9822",
      "cell_type": "markdown",
      "source": "## 7) Practical usage (scikit-learn)\n\n```python\nfrom sklearn.metrics import rand_score, adjusted_rand_score\n\nri  = rand_score(y_true, y_pred)\nari = adjusted_rand_score(y_true, y_pred)\n```\n\n`rand_score` is a thin wrapper around the pair confusion matrix.\nIf you need debugging insight, inspect:\n\n```python\nfrom sklearn.metrics.cluster import pair_confusion_matrix\n\npair_confusion_matrix(y_true, y_pred)\n```\n",
      "metadata": {}
    },
    {
      "id": "3c1e93b3",
      "cell_type": "markdown",
      "source": "## Exercises\n\n1. Create a clustering that **splits** one true cluster into two; verify how TP/FN change.\n2. Create a clustering that **merges** two true clusters; verify how TN/FP change.\n3. Simulate random labelings with different `k` and estimate the expected Rand score.\n4. Implement `adjusted_rand_score` (ARI) from the contingency table and compare to scikit-learn.\n\n## References\n\n- W. M. Rand (1971). *Objective criteria for the evaluation of clustering methods*. Journal of the American Statistical Association.\n- L. Hubert, P. Arabie (1985). *Comparing partitions*. Journal of Classification. (Adjusted Rand index)\n- scikit-learn docs: https://scikit-learn.org/stable/modules/clustering.html#rand-index\n",
      "metadata": {}
    }
  ]
}
