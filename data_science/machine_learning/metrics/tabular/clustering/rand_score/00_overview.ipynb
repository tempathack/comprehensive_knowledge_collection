{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "cells": [
  {
   "id": "8456fdbb",
   "cell_type": "markdown",
   "source": "# Rand Score (Rand Index)\n\n`rand_score` (the **Rand index**) measures how similar two clusterings / partitions are by looking at **all pairs** of samples.\n\nA good mental model:\n\n- Each clustering turns the dataset into a binary relation: for every pair `(i, j)`, are they in the **same cluster** or **different clusters**?\n- The Rand index is the **accuracy** of those pairwise decisions.\n\n## Quick import\n\n```python\nfrom sklearn.metrics import rand_score\n```\n\n---\n\n## Learning goals\n\n- Understand Rand index as **pairwise agreement** (TP/TN/FP/FN over pairs)\n- Implement `rand_score` from scratch in NumPy (efficiently, without an `O(n^2)` loop)\n- Visualize where the score comes from on a small example\n- See common pitfalls (why it can be **inflated** when there are many clusters)\n- Use Rand score to **select** a simple clustering hyperparameter\n\n## Prerequisites\n\n- NumPy basics (`shape`, broadcasting, `np.unique`)\n- Combinatorics: number of pairs `\\(\binom{n}{2}\\)`\n",
   "metadata": {}
  },
  {
   "id": "482ee8b8",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.metrics import rand_score\nfrom sklearn.metrics.cluster import pair_confusion_matrix\nfrom sklearn.metrics import adjusted_rand_score\n\npio.templates.default = \"plotly_white\"\npio.renderers.default = \"notebook\"\nnp.set_printoptions(precision=4, suppress=True)\n\nrng = np.random.default_rng(42)\n",
   "outputs": []
  },
  {
   "id": "28f6997e",
   "cell_type": "markdown",
   "source": "## 1) Definition: Rand index as pairwise accuracy\n\nLet there be `n` samples and two labelings:\n\n- ground truth (or reference) labels: \\(y\\)\n- predicted clustering labels: \\(\\hat{y}\\)\n\nFor every unordered pair \\((i, j)\\) with \\(i < j\\), define two *same-cluster indicators*:\n\n$$\nS_y(i, j) = \\mathbb{1}[y_i = y_j],\n\\qquad\nS_{\\hat{y}}(i, j) = \\mathbb{1}[\\hat{y}_i = \\hat{y}_j]\n$$\n\nNow count pairs into the familiar 2×2 confusion matrix:\n\n- **TP**: same in `y` and same in `\\(\\hat{y}\\)`\n- **TN**: different in `y` and different in `\\(\\hat{y}\\)`\n- **FP**: different in `y` but same in `\\(\\hat{y}\\)` (a *merge* error)\n- **FN**: same in `y` but different in `\\(\\hat{y}\\)` (a *split* error)\n\nFormally:\n\n$$\n\begin{aligned}\n\\mathrm{TP} &= \\sum_{i<j} S_y(i,j)\\,S_{\\hat{y}}(i,j) \\\n\\mathrm{TN} &= \\sum_{i<j} (1-S_y(i,j))\\,(1-S_{\\hat{y}}(i,j)) \\\n\\mathrm{FP} &= \\sum_{i<j} (1-S_y(i,j))\\,S_{\\hat{y}}(i,j) \\\n\\mathrm{FN} &= \\sum_{i<j} S_y(i,j)\\,(1-S_{\\hat{y}}(i,j))\n\\end{aligned}\n$$\n\nThere are \\(\binom{n}{2}\\) total pairs, so the **Rand index** is:\n\n$$\n\\mathrm{RI}(y, \\hat{y}) = \frac{\\mathrm{TP} + \\mathrm{TN}}{\binom{n}{2}}\n$$\n\nNote: scikit-learn's `pair_confusion_matrix` counts **ordered** pairs `(i, j)` with `i != j`,\nso its entries sum to `n(n-1)` and are exactly **2×** the `i<j` counts above.\n\nKey properties:\n\n- **Label permutation invariance**: only equality matters; relabeling clusters doesn't change the score.\n- **Symmetry**: \\(\\mathrm{RI}(y,\\hat{y}) = \\mathrm{RI}(\\hat{y},y)\\).\n- **Range**: \\([0,1]\\). For `n < 2`, scikit-learn returns `1.0` by convention.\n",
   "metadata": {}
  },
  {
   "id": "dc035d4b",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# A tiny toy example (small n so we can visualize all pairs)\ny_true = np.array([0, 0, 0, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 0, 0, 0, 2, 3, 3])\n\nprint('y_true:', y_true)\nprint('y_pred:', y_pred)\n\nprint('rand_score (sklearn):', rand_score(y_true, y_pred))\n\npcm_ord = pair_confusion_matrix(y_true, y_pred)\nprint('pair_confusion_matrix (sklearn, ordered pairs):')\nprint(pcm_ord)\nprint('pair confusion on i<j (unordered) is pcm_ord // 2:')\nprint(pcm_ord // 2)\n",
   "outputs": []
  },
  {
   "id": "ca806e66",
   "cell_type": "markdown",
   "source": "### Visual intuition: every clustering is a pairwise matrix\n\nFor a labeling `y`, define an \\(n\times n\\) matrix:\n\n$$\nA_y[i, j] = \\mathbb{1}[y_i = y_j]\n$$\n\n- Blocks of 1s correspond to clusters.\n- Comparing \\(A_y\\) and \\(A_{\\hat{y}}\\) tells you which pairs are treated the same.\n\nBelow we color each pair `(i, j)` as:\n\n- **TP**: same in both\n- **TN**: different in both\n- **FP**: different in true, same in pred\n- **FN**: same in true, different in pred\n",
   "metadata": {}
  },
  {
   "id": "38788ee6",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "def pair_category_matrix(y_true, y_pred):",
    "    y_true = np.asarray(y_true)",
    "    y_pred = np.asarray(y_pred)",
    "    if y_true.shape != y_pred.shape:",
    "        raise ValueError('y_true and y_pred must have the same shape')",
    "",
    "    same_true = y_true[:, None] == y_true[None, :]",
    "    same_pred = y_pred[:, None] == y_pred[None, :]",
    "",
    "    # 0=TN, 1=FP, 2=FN, 3=TP",
    "    cat = np.zeros_like(same_true, dtype=int)",
    "    cat[(~same_true) & same_pred] = 1",
    "    cat[same_true & (~same_pred)] = 2",
    "    cat[same_true & same_pred] = 3",
    "",
    "    np.fill_diagonal(cat, -1)  # diagonal isn't a pair",
    "    return cat",
    "",
    "",
    "cat = pair_category_matrix(y_true, y_pred)",
    "",
    "# Count only i<j pairs",
    "upper_mask = np.triu(np.ones_like(cat, dtype=bool), k=1)",
    "vals = cat[upper_mask]",
    "",
    "counts = {",
    "    \"TN (different/different)\": int((vals == 0).sum()),",
    "    \"FP (merge error)\": int((vals == 1).sum()),",
    "    \"FN (split error)\": int((vals == 2).sum()),",
    "    \"TP (same/same)\": int((vals == 3).sum()),",
    "}",
    "",
    "axis_labels = [f\"{i}<br>t={y_true[i]}, p={y_pred[i]}\" for i in range(len(y_true))]",
    "",
    "name_map = {",
    "    -1: \"diag\",",
    "    0: \"TN (different/different)\",",
    "    1: \"FP (merge error)\",",
    "    2: \"FN (split error)\",",
    "    3: \"TP (same/same)\",",
    "}",
    "cat_name = np.vectorize(name_map.get)(cat)",
    "",
    "c_diag = \"#ffffff\"",
    "c_tn = \"#d9d9d9\"",
    "c_fp = \"#ff7f0e\"",
    "c_fn = \"#1f77b4\"",
    "c_tp = \"#2ca02c\"",
    "",
    "# With zmin=-1, zmax=3, normalized boundaries between integers are:",
    "# -0.5 -> 0.125, 0.5 -> 0.375, 1.5 -> 0.625, 2.5 -> 0.875",
    "colorscale = [",
    "    [0.0, c_diag],",
    "    [0.1249, c_diag],",
    "    [0.125, c_tn],",
    "    [0.3749, c_tn],",
    "    [0.375, c_fp],",
    "    [0.6249, c_fp],",
    "    [0.625, c_fn],",
    "    [0.8749, c_fn],",
    "    [0.875, c_tp],",
    "    [1.0, c_tp],",
    "]",
    "",
    "fig = make_subplots(",
    "    rows=1,",
    "    cols=2,",
    "    column_widths=[0.72, 0.28],",
    "    subplot_titles=(",
    "        \"Pair categories (upper triangle is unique)\",",
    "        \"Pair counts (i<j)\",",
    "    ),",
    ")",
    "",
    "fig.add_trace(",
    "    go.Heatmap(",
    "        z=cat,",
    "        x=axis_labels,",
    "        y=axis_labels,",
    "        zmin=-1,",
    "        zmax=3,",
    "        colorscale=colorscale,",
    "        showscale=False,",
    "        customdata=cat_name,",
    "        hovertemplate=\"%{y} vs %{x}<br>%{customdata}<extra></extra>\",",
    "    ),",
    "    row=1,",
    "    col=1,",
    ")",
    "",
    "fig.add_trace(",
    "    go.Bar(",
    "        x=list(counts.keys()),",
    "        y=list(counts.values()),",
    "        text=list(counts.values()),",
    "        textposition=\"outside\",",
    "        marker_color=[c_tn, c_fp, c_fn, c_tp],",
    "    ),",
    "    row=1,",
    "    col=2,",
    ")",
    "",
    "fig.update_layout(",
    "    title=\"Rand index works on pairs: agreements (TP/TN) vs disagreements (FP/FN)\",",
    "    xaxis=dict(tickangle=0),",
    ")",
    "",
    "fig.update_xaxes(title_text=\"sample index (with labels)\", row=1, col=1)",
    "fig.update_yaxes(title_text=\"sample index (with labels)\", row=1, col=1)",
    "fig.update_xaxes(tickangle=45, row=1, col=2)",
    "fig.update_yaxes(title_text=\"count\", row=1, col=2)",
    "",
    "fig",
    ""
   ],
   "outputs": []
  },
  {
   "id": "26975ef8",
   "cell_type": "markdown",
   "source": "## 2) Efficient computation (without looping over all pairs)\n\nA naive implementation checks all \\(\binom{n}{2}\\) pairs, which is `O(n^2)`.\n\nWe can compute the same TP/FP/FN/TN counts using a **contingency table**.\n\nLet \\(n_{ij}\\) be the number of samples that are in:\n\n- true cluster \\(i\\), and\n- predicted cluster \\(j\\).\n\nThis forms a matrix \\(N = [n_{ij}]\\).\n\n### Key combinatorial identity\n\nThe number of unordered pairs inside a group of size \\(m\\) is:\n\n$$\n\binom{m}{2} = \frac{m(m-1)}{2}\n$$\n\n### Counting pairs via the contingency table\n\n- Pairs that are together in **both** partitions:\n\n$$\n\\mathrm{TP} = \\sum_{i,j} \binom{n_{ij}}{2}\n$$\n\n- Pairs that are together in **true** labels:\n\n$$\n\\sum_i \binom{n_{i\\cdot}}{2}\n\\quad\text{where } n_{i\\cdot}=\\sum_j n_{ij}\n$$\n\n- Pairs that are together in **pred** labels:\n\n$$\n\\sum_j \binom{n_{\\cdot j}}{2}\n\\quad\text{where } n_{\\cdot j}=\\sum_i n_{ij}\n$$\n\nThen:\n\n$$\n\\mathrm{FN} = \\sum_i \binom{n_{i\\cdot}}{2} - \\mathrm{TP}\n\\qquad\n\\mathrm{FP} = \\sum_j \binom{n_{\\cdot j}}{2} - \\mathrm{TP}\n$$\n\nFinally:\n\n$$\n\\mathrm{TN} = \binom{n}{2} - (\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN})\n$$\n\nThis gives an `O(n + k_y k_\\hat{y})` implementation (where `k` is number of clusters).\n",
   "metadata": {}
  },
  {
   "id": "b875320c",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "def comb2(x):\n    '''Unordered pairs: C(m, 2) = m(m-1)/2.'''\n    x = np.asarray(x, dtype=np.int64)\n    return (x * (x - 1)) // 2\n\n\ndef pairs_ordered(x):\n    '''Ordered pairs with i!=j inside a group: m(m-1) = 2*C(m,2).'''\n    x = np.asarray(x, dtype=np.int64)\n    return x * (x - 1)\n\n\ndef _labels_to_integers(labels):\n    labels = np.asarray(labels)\n    _, inv = np.unique(labels, return_inverse=True)\n    return inv\n\n\ndef contingency_matrix_np(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n\n    true_inv = _labels_to_integers(y_true)\n    pred_inv = _labels_to_integers(y_pred)\n\n    n_true = int(true_inv.max(initial=-1)) + 1\n    n_pred = int(pred_inv.max(initial=-1)) + 1\n\n    cont = np.zeros((n_true, n_pred), dtype=np.int64)\n    np.add.at(cont, (true_inv, pred_inv), 1)\n    return cont\n\n\ndef pair_confusion_matrix_np(y_true, y_pred):\n    '''Match sklearn.metrics.cluster.pair_confusion_matrix.\n\n    scikit-learn counts ordered pairs (i, j) with i != j, so totals sum to n*(n-1).\n    '''\n\n    y_true = np.asarray(y_true)\n    n = int(y_true.size)\n\n    if n < 2:\n        return np.array([[0, 0], [0, 0]], dtype=np.int64)\n\n    cont = contingency_matrix_np(y_true, y_pred)\n\n    tp = int(pairs_ordered(cont).sum())\n    same_true = int(pairs_ordered(cont.sum(axis=1)).sum())\n    same_pred = int(pairs_ordered(cont.sum(axis=0)).sum())\n\n    fn = same_true - tp\n    fp = same_pred - tp\n\n    total_pairs = n * (n - 1)\n    tn = total_pairs - tp - fp - fn\n\n    return np.array([[tn, fp], [fn, tp]], dtype=np.int64)\n\n\ndef rand_score_np(y_true, y_pred):\n    '''Rand index over unordered pairs (i<j), matching sklearn.metrics.rand_score.'''\n\n    y_true = np.asarray(y_true)\n    n = int(y_true.size)\n    if n < 2:\n        return 1.0\n\n    cont = contingency_matrix_np(y_true, y_pred)\n\n    tp = float(comb2(cont).sum())\n    same_true = float(comb2(cont.sum(axis=1)).sum())\n    same_pred = float(comb2(cont.sum(axis=0)).sum())\n\n    fn = same_true - tp\n    fp = same_pred - tp\n\n    total_pairs = float(comb2(n))\n    tn = total_pairs - tp - fp - fn\n\n    return (tp + tn) / total_pairs\n",
   "outputs": []
  },
  {
   "id": "b5046147",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Sanity checks vs scikit-learn\n\nfor n in [0, 1, 5, 50]:\n    y_true = rng.integers(0, 4, size=n)\n    y_pred = rng.integers(0, 6, size=n)\n\n    ri_np = rand_score_np(y_true, y_pred)\n    ri_sk = rand_score(y_true, y_pred)\n\n    pcm_np = pair_confusion_matrix_np(y_true, y_pred)\n    pcm_sk = pair_confusion_matrix(y_true, y_pred)\n\n    print(f\"n={n:>2}  rand_score close? {np.isclose(ri_np, ri_sk)}  pair_confusion close? {np.all(pcm_np == pcm_sk)}\")\n",
   "outputs": []
  },
  {
   "id": "dee9c79f",
   "cell_type": "markdown",
   "source": "## 3) Properties worth remembering\n\n### 3.1) Invariant to relabeling\n\nIf you permute cluster IDs (e.g., swap labels `0` and `1`), the Rand score **does not change**.\n\nThat’s essential for clustering evaluation: cluster IDs are arbitrary.\n",
   "metadata": {}
  },
  {
   "id": "b7dc36e4",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "y_true = np.array([0, 0, 1, 1, 2, 2])\ny_pred = np.array([10, 10, 11, 11, 12, 12])\n\nperm = {10: 12, 11: 10, 12: 11}\ny_pred_perm = np.vectorize(perm.get)(y_pred)\n\nprint('original:', rand_score_np(y_true, y_pred))\nprint('permuted:', rand_score_np(y_true, y_pred_perm))\n",
   "outputs": []
  },
  {
   "id": "887f50cb",
   "cell_type": "markdown",
   "source": "### 3.2) Extremes and trivial solutions\n\nSome degenerate clusterings can get surprisingly high Rand scores.\n\nTwo extremes:\n\n- **All-in-one cluster**: predicts every pair is \"same\".\n- **All-singletons**: predicts every pair is \"different\".\n\nBecause real datasets often have *far more different-cluster pairs than same-cluster pairs*, the \"all-singletons\" solution can score high even though it's useless.\n",
   "metadata": {}
  },
  {
   "id": "ebadbb0f",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "def all_in_one(n):\n    return np.zeros(n, dtype=int)\n\ndef all_singletons(n):\n    return np.arange(n, dtype=int)\n\nn = 200\n# True labels: 4 balanced clusters\nk_true = 4\nsizes = np.full(k_true, n // k_true)\ny_true = np.repeat(np.arange(k_true), sizes)\n\ny_all1 = all_in_one(n)\ny_single = all_singletons(n)\n\ndef summarize(y_pred, name):\n    pcm_ord = pair_confusion_matrix_np(y_true, y_pred)\n    ri = rand_score_np(y_true, y_pred)\n    return name, ri, pcm_ord\n\nfor name, ri, pcm_ord in [\n    summarize(y_true, 'perfect'),\n    summarize(y_all1, 'all-in-one'),\n    summarize(y_single, 'all-singletons'),\n]:\n    tn, fp, fn, tp = pcm_ord.ravel()\n    print(f\"{name:>14}  RI={ri:.4f}  TP={tp:,}  TN={tn:,}  FP={fp:,}  FN={fn:,}  (ordered pairs)\")\n",
   "outputs": []
  },
  {
   "id": "0106faa9",
   "cell_type": "markdown",
   "source": "## 4) Pitfall: Rand score is *not* adjusted for chance\n\nIf you assign labels **at random**, the expected Rand score is not a constant baseline like 0.\n\nIn fact, when the predicted clustering has many clusters, most pairs become \"different\" in the prediction, which creates lots of **TN agreements**.\n\nThis is a big reason why practitioners often prefer:\n\n- **Adjusted Rand index** (`adjusted_rand_score` / ARI): corrected so random labelings score around 0.\n\nWe'll simulate this effect.\n",
   "metadata": {}
  },
  {
   "id": "2a416a6f",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "n = 200",
    "k_true = 4",
    "sizes = np.full(k_true, n // k_true)",
    "y_true = np.repeat(np.arange(k_true), sizes)",
    "",
    "k_pred_values = [2, 4, 8, 16, 32, 64, 128]",
    "reps = 200",
    "",
    "rand_scores = {k: [] for k in k_pred_values}",
    "adj_rand_scores = {k: [] for k in k_pred_values}",
    "",
    "for k in k_pred_values:",
    "    for _ in range(reps):",
    "        y_pred = rng.integers(0, k, size=n)",
    "        rand_scores[k].append(rand_score_np(y_true, y_pred))",
    "        adj_rand_scores[k].append(adjusted_rand_score(y_true, y_pred))",
    "",
    "fig = make_subplots(",
    "    rows=1,",
    "    cols=2,",
    "    subplot_titles=(\"Rand score (unadjusted)\", \"Adjusted Rand score (ARI)\"),",
    ")",
    "",
    "for k in k_pred_values:",
    "    fig.add_trace(go.Box(y=rand_scores[k], name=f\"k={k}\", boxmean=True, showlegend=False), row=1, col=1)",
    "    fig.add_trace(go.Box(y=adj_rand_scores[k], name=f\"k={k}\", boxmean=True, showlegend=False), row=1, col=2)",
    "",
    "fig.update_layout(title=\"Random predictions: Rand score inflates as #clusters increases\")",
    "fig.update_yaxes(title_text=\"score\", row=1, col=1)",
    "fig.update_yaxes(title_text=\"score\", row=1, col=2)",
    "fig",
    ""
   ],
   "outputs": []
  },
  {
   "id": "1acfd82a",
   "cell_type": "markdown",
   "source": "## 5) Using Rand score to select a simple clustering model (k-means example)\n\nRand score is an **external** clustering metric: it requires reference labels.\n\nThat makes it useful when:\n\n- you're benchmarking clustering algorithms on labeled datasets, or\n- you're tuning hyperparameters in a *semi-supervised evaluation* setting.\n\nBelow we:\n\n1. create synthetic blobs with known cluster IDs\n2. run a small **NumPy k-means** implementation for different `k`\n3. pick `k` that maximizes Rand score\n\nImportant: in real unsupervised tasks you typically don't have `y_true`, so you'd use internal metrics (e.g., silhouette) instead.\n",
   "metadata": {}
  },
  {
   "id": "7d140196",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Synthetic 2D blobs",
    "centers = np.array([[-2.0, -2.0], [2.0, -2.0], [-2.0, 2.0], [2.0, 2.0]])",
    "cluster_std = 0.55",
    "n_per = 80",
    "",
    "X_list = []",
    "y_true = []",
    "for k, c in enumerate(centers):",
    "    Xk = rng.normal(loc=c, scale=cluster_std, size=(n_per, 2))",
    "    X_list.append(Xk)",
    "    y_true.append(np.full(n_per, k, dtype=int))",
    "",
    "X = np.vstack(X_list)",
    "y_true = np.concatenate(y_true)",
    "",
    "# Shuffle",
    "perm = rng.permutation(len(X))",
    "X = X[perm]",
    "y_true = y_true[perm]",
    "",
    "fig = px.scatter(",
    "    x=X[:, 0],",
    "    y=X[:, 1],",
    "    color=y_true.astype(str),",
    "    title=\"Synthetic data (ground truth clusters)\",",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"true cluster\"},",
    ")",
    "fig.show()",
    "",
    "",
    "def kmeans_np(X, k, *, n_init=10, max_iter=100, tol=1e-4, rng=None):",
    "    if rng is None:",
    "        rng = np.random.default_rng(0)",
    "",
    "    n, d = X.shape",
    "    best_inertia = np.inf",
    "    best_centroids = None",
    "    best_labels = None",
    "",
    "    for _ in range(n_init):",
    "        centroids = X[rng.choice(n, size=k, replace=False)].copy()",
    "",
    "        for _ in range(max_iter):",
    "            # squared distances: (n, k)",
    "            d2 = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)",
    "            labels = d2.argmin(axis=1)",
    "",
    "            new_centroids = np.empty_like(centroids)",
    "            for j in range(k):",
    "                mask = labels == j",
    "                if mask.any():",
    "                    new_centroids[j] = X[mask].mean(axis=0)",
    "                else:",
    "                    # handle empty clusters by re-seeding to a random point",
    "                    new_centroids[j] = X[rng.integers(0, n)]",
    "",
    "            shift = np.linalg.norm(new_centroids - centroids)",
    "            centroids = new_centroids",
    "",
    "            if shift < tol:",
    "                break",
    "",
    "        inertia = float(((X - centroids[labels]) ** 2).sum())",
    "        if inertia < best_inertia:",
    "            best_inertia = inertia",
    "            best_centroids = centroids",
    "            best_labels = labels",
    "",
    "    return best_centroids, best_labels, best_inertia",
    "",
    "",
    "k_values = list(range(2, 9))",
    "results = []",
    "",
    "for k in k_values:",
    "    centroids, labels, inertia = kmeans_np(X, k, n_init=15, max_iter=100, tol=1e-4, rng=rng)",
    "    ri = rand_score_np(y_true, labels)",
    "    ari = adjusted_rand_score(y_true, labels)",
    "    results.append((k, ri, ari, inertia))",
    "",
    "results = np.array(results, dtype=float)",
    "",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Rand score vs k\", \"ARI vs k\"))",
    "",
    "fig.add_trace(go.Scatter(x=results[:, 0], y=results[:, 1], mode='lines+markers', name='RI'), row=1, col=1)",
    "fig.add_trace(go.Scatter(x=results[:, 0], y=results[:, 2], mode='lines+markers', name='ARI'), row=1, col=2)",
    "",
    "fig.update_xaxes(title_text=\"k\", row=1, col=1)",
    "fig.update_xaxes(title_text=\"k\", row=1, col=2)",
    "fig.update_yaxes(title_text=\"score\", row=1, col=1)",
    "fig.update_yaxes(title_text=\"score\", row=1, col=2)",
    "",
    "fig.update_layout(title=\"Selecting k with external labels (demo)\")",
    "fig.show()",
    "",
    "best_k = int(results[results[:, 1].argmax(), 0])",
    "print('best k by Rand score:', best_k)",
    "",
    "centroids, labels_best, _ = kmeans_np(X, best_k, n_init=25, rng=rng)",
    "",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Ground truth\", f\"k-means prediction (k={best_k})\"))",
    "",
    "fig.add_trace(",
    "    go.Scatter(x=X[:, 0], y=X[:, 1], mode='markers', marker=dict(color=y_true, colorscale='Viridis', size=6), showlegend=False),",
    "    row=1,",
    "    col=1,",
    ")",
    "",
    "fig.add_trace(",
    "    go.Scatter(x=X[:, 0], y=X[:, 1], mode='markers', marker=dict(color=labels_best, colorscale='Viridis', size=6), showlegend=False),",
    "    row=1,",
    "    col=2,",
    ")",
    "",
    "fig.update_xaxes(title_text=\"x1\", row=1, col=1)",
    "fig.update_xaxes(title_text=\"x1\", row=1, col=2)",
    "fig.update_yaxes(title_text=\"x2\", row=1, col=1)",
    "fig.update_yaxes(title_text=\"x2\", row=1, col=2)",
    "",
    "fig.update_layout(title=\"Clustering visualization (labels are arbitrary, Rand index is invariant)\")",
    "fig",
    ""
   ],
   "outputs": []
  },
  {
   "id": "2a39a1b6",
   "cell_type": "markdown",
   "source": "## 6) Pros, cons, and when to use it\n\n### Pros\n\n- **Permutation invariant**: cluster IDs don't matter.\n- **Interpretable**: literally the fraction of pairwise agreements.\n- **Works for any number of clusters** and doesn't require distances/geometry.\n- **Fast to compute** via contingency counts (no need to enumerate all pairs).\n\n### Cons / pitfalls\n\n- **Not adjusted for chance**: random clusterings can get high RI, especially with many clusters.\n- Often dominated by **TN** (different/different) pairs, which can hide meaningful errors.\n- Can reward **over-segmentation** (too many clusters) in some settings.\n- **Not differentiable**: not suitable as a gradient-based training loss.\n\n### Good use cases\n\n- External evaluation of clustering when you have reference labels (benchmarks, ablations).\n- Comparing two clusterings of the same dataset when you care about pairwise co-membership.\n\nIf you need a chance-corrected baseline, prefer **ARI** (`adjusted_rand_score`).\n",
   "metadata": {}
  },
  {
   "id": "77dd9822",
   "cell_type": "markdown",
   "source": "## 7) Practical usage (scikit-learn)\n\n```python\nfrom sklearn.metrics import rand_score, adjusted_rand_score\n\nri  = rand_score(y_true, y_pred)\nari = adjusted_rand_score(y_true, y_pred)\n```\n\n`rand_score` is a thin wrapper around the pair confusion matrix.\nIf you need debugging insight, inspect:\n\n```python\nfrom sklearn.metrics.cluster import pair_confusion_matrix\n\npair_confusion_matrix(y_true, y_pred)\n```\n",
   "metadata": {}
  },
  {
   "id": "3c1e93b3",
   "cell_type": "markdown",
   "source": "## Exercises\n\n1. Create a clustering that **splits** one true cluster into two; verify how TP/FN change.\n2. Create a clustering that **merges** two true clusters; verify how TN/FP change.\n3. Simulate random labelings with different `k` and estimate the expected Rand score.\n4. Implement `adjusted_rand_score` (ARI) from the contingency table and compare to scikit-learn.\n\n## References\n\n- W. M. Rand (1971). *Objective criteria for the evaluation of clustering methods*. Journal of the American Statistical Association.\n- L. Hubert, P. Arabie (1985). *Comparing partitions*. Journal of Classification. (Adjusted Rand index)\n- scikit-learn docs: https://scikit-learn.org/stable/modules/clustering.html#rand-index\n",
   "metadata": {}
  }
 ]
}