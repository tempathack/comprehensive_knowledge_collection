{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c1fc2c",
   "metadata": {},
   "source": [
    "# adjusted_rand_score (Adjusted Rand Index / ARI)\n",
    "\n",
    "The **Adjusted Rand Index (ARI)** is an **external clustering metric**: it compares two *partitions* of the same $n$ samples\n",
    "(e.g., **ground truth** labels vs a clustering algorithm’s output).\n",
    "\n",
    "- **$1.0$** means the two partitions are identical (up to a permutation of label names).\n",
    "- **$0.0$** is the score you expect **by chance** under a standard random-labeling model.\n",
    "- **Negative** values mean *worse-than-chance* agreement.\n",
    "\n",
    "## Quick import (scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "```\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Build intuition via **pair agreements** (the Rand index).\n",
    "- See why “adjusting for chance” matters.\n",
    "- Derive the ARI formula using a **contingency table**.\n",
    "- Implement ARI **from scratch in NumPy** and verify vs `sklearn`.\n",
    "- Use ARI to **tune/compare** a simple clustering algorithm on labeled benchmark data.\n",
    "\n",
    "## When it's useful\n",
    "\n",
    "- Benchmarking a clustering algorithm against known labels (synthetic or labeled datasets)\n",
    "- Comparing two clustering outputs (agreement / stability)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic clustering vocabulary (cluster labels as a partition)\n",
    "- Combinatorics: number of unordered pairs $\\binom{n}{2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9766de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f253cbd",
   "metadata": {},
   "source": [
    "## 1) Intuition: clustering as “pair decisions” (Rand index)\n",
    "\n",
    "Any clustering/labeling answers this yes/no question for every *unordered pair* of samples $(i, j)$:\n",
    "\n",
    "> Are $i$ and $j$ in the **same** cluster?\n",
    "\n",
    "Given two partitions (say **true** labels and **predicted** labels), every pair falls into one of four buckets:\n",
    "\n",
    "- **$a$**: same in true **and** same in pred\n",
    "- **$b$**: different in true **and** different in pred\n",
    "- **$c$**: same in true, different in pred\n",
    "- **$d$**: different in true, same in pred\n",
    "\n",
    "The **Rand Index (RI)** is the fraction of agreeing decisions:\n",
    "\n",
    "$$\n",
    "\\mathrm{RI} = \\frac{a + b}{\\binom{n}{2}}.\n",
    "$$\n",
    "\n",
    "RI is easy to understand, but it has an issue: **it can be large even for random clusterings**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6998106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_confusion_counts(labels_true: np.ndarray, labels_pred: np.ndarray) -> dict[str, int]:\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "    if labels_true.shape != labels_pred.shape:\n",
    "        raise ValueError(\"labels_true and labels_pred must have the same shape\")\n",
    "\n",
    "    n = labels_true.size\n",
    "    same_true = labels_true[:, None] == labels_true[None, :]\n",
    "    same_pred = labels_pred[:, None] == labels_pred[None, :]\n",
    "\n",
    "    upper = np.triu(np.ones((n, n), dtype=bool), k=1)\n",
    "\n",
    "    a = int(np.sum(same_true & same_pred & upper))\n",
    "    b = int(np.sum(~same_true & ~same_pred & upper))\n",
    "    c = int(np.sum(same_true & ~same_pred & upper))\n",
    "    d = int(np.sum(~same_true & same_pred & upper))\n",
    "\n",
    "    return {\"a_same_same\": a, \"b_diff_diff\": b, \"c_same_diff\": c, \"d_diff_same\": d}\n",
    "\n",
    "\n",
    "# A tiny example\n",
    "y_true = np.array([0, 0, 0, 1, 1, 2])\n",
    "y_pred = np.array([1, 1, 0, 0, 0, 2])\n",
    "\n",
    "counts = pair_confusion_counts(y_true, y_pred)\n",
    "n_pairs = y_true.size * (y_true.size - 1) // 2\n",
    "ri = (counts[\"a_same_same\"] + counts[\"b_diff_diff\"]) / n_pairs\n",
    "\n",
    "counts, n_pairs, ri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ac586",
   "metadata": {},
   "source": [
    "## 2) Why “adjust for chance”?\n",
    "\n",
    "In many datasets, **most pairs are in different clusters**. If two random partitions happen to put lots of pairs into “different/different”,\n",
    "RI becomes large — even though the clusterings are not meaningfully similar.\n",
    "\n",
    "ARI fixes this by applying the classic “adjusted-for-chance” idea:\n",
    "\n",
    "$$\n",
    "\\mathrm{ARI} = \\frac{\\mathrm{Index} - \\mathbb{E}[\\mathrm{Index}]}{\\mathrm{MaxIndex} - \\mathbb{E}[\\mathrm{Index}]}\n",
    "$$\n",
    "\n",
    "The remaining question is: what is “Index” and how do we compute its expectation efficiently?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e627805",
   "metadata": {},
   "source": [
    "## 3) The contingency table view (efficient computation)\n",
    "\n",
    "Let the true partition be $U = \\{U_1, \\dots, U_r\\}$ and the predicted partition be $V = \\{V_1, \\dots, V_s\\}$.\n",
    "\n",
    "Define the **contingency table** counts:\n",
    "\n",
    "$$\n",
    "n_{ij} = |U_i \\cap V_j|, \\quad a_i = \\sum_j n_{ij}, \\quad b_j = \\sum_i n_{ij}, \\quad n = \\sum_{ij} n_{ij}.\n",
    "$$\n",
    "\n",
    "Think in terms of unordered pairs. The number of pairs that are in the **same** cluster in *both* partitions is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Index} = \\sum_{ij} \\binom{n_{ij}}{2}.\n",
    "$$\n",
    "\n",
    "The number of pairs that are in the same true cluster (regardless of prediction) is:\n",
    "\n",
    "$$\n",
    "\\sum_i \\binom{a_i}{2}\n",
    "$$\n",
    "\n",
    "and similarly for the predicted clusters:\n",
    "\n",
    "$$\n",
    "\\sum_j \\binom{b_j}{2}.\n",
    "$$\n",
    "\n",
    "Under the standard “random labeling with fixed cluster sizes” model (hypergeometric), the expected index becomes:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathrm{Index}] = \\frac{\\left(\\sum_i \\binom{a_i}{2}\\right)\\left(\\sum_j \\binom{b_j}{2}\\right)}{\\binom{n}{2}}.\n",
    "$$\n",
    "\n",
    "The adjusted Rand index is then (Hubert & Arabie, 1985):\n",
    "\n",
    "$$\n",
    "\\mathrm{ARI} = \\frac{\n",
    "    \\sum_{ij} \\binom{n_{ij}}{2} - \\frac{\\left(\\sum_i \\binom{a_i}{2}\\right)\\left(\\sum_j \\binom{b_j}{2}\\right)}{\\binom{n}{2}}\n",
    "}{\n",
    "    \\tfrac{1}{2}\\left[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}\\right]\n",
    "    - \\frac{\\left(\\sum_i \\binom{a_i}{2}\\right)\\left(\\sum_j \\binom{b_j}{2}\\right)}{\\binom{n}{2}}\n",
    "}.\n",
    "$$\n",
    "\n",
    "This avoids the $O(n^2)$ pair enumeration and instead works in roughly $O(n + rs)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b166db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb2(x: np.ndarray | int) -> np.ndarray | float:\n",
    "    x_arr = np.asarray(x, dtype=np.int64)\n",
    "    return x_arr * (x_arr - 1) / 2.0\n",
    "\n",
    "\n",
    "def contingency_matrix_numpy(\n",
    "    labels_true: np.ndarray,\n",
    "    labels_pred: np.ndarray,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "\n",
    "    if labels_true.shape != labels_pred.shape:\n",
    "        raise ValueError(\"labels_true and labels_pred must have the same shape\")\n",
    "\n",
    "    true_labels, true_inv = np.unique(labels_true, return_inverse=True)\n",
    "    pred_labels, pred_inv = np.unique(labels_pred, return_inverse=True)\n",
    "\n",
    "    cont = np.zeros((true_labels.size, pred_labels.size), dtype=np.int64)\n",
    "    np.add.at(cont, (true_inv, pred_inv), 1)\n",
    "\n",
    "    return cont, true_labels, pred_labels\n",
    "\n",
    "\n",
    "def rand_index_numpy(labels_true: np.ndarray, labels_pred: np.ndarray) -> float:\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "\n",
    "    n = labels_true.size\n",
    "    if n < 2:\n",
    "        return 1.0\n",
    "\n",
    "    cont, _, _ = contingency_matrix_numpy(labels_true, labels_pred)\n",
    "    sum_nij = float(np.sum(comb2(cont)))\n",
    "    sum_ai = float(np.sum(comb2(cont.sum(axis=1))))\n",
    "    sum_bj = float(np.sum(comb2(cont.sum(axis=0))))\n",
    "    n_pairs = float(comb2(n))\n",
    "\n",
    "    a = sum_nij\n",
    "    c = sum_ai - a\n",
    "    d = sum_bj - a\n",
    "    b = n_pairs - a - c - d\n",
    "\n",
    "    return (a + b) / n_pairs\n",
    "\n",
    "\n",
    "def adjusted_rand_index_numpy(labels_true: np.ndarray, labels_pred: np.ndarray) -> float:\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "\n",
    "    n = labels_true.size\n",
    "    if n < 2:\n",
    "        return 1.0\n",
    "\n",
    "    cont, _, _ = contingency_matrix_numpy(labels_true, labels_pred)\n",
    "\n",
    "    sum_nij = float(np.sum(comb2(cont)))\n",
    "    sum_ai = float(np.sum(comb2(cont.sum(axis=1))))\n",
    "    sum_bj = float(np.sum(comb2(cont.sum(axis=0))))\n",
    "    n_pairs = float(comb2(n))\n",
    "\n",
    "    expected = (sum_ai * sum_bj) / n_pairs\n",
    "    max_index = 0.5 * (sum_ai + sum_bj)\n",
    "    denom = max_index - expected\n",
    "\n",
    "    if denom == 0.0:\n",
    "        return 1.0\n",
    "\n",
    "    return (sum_nij - expected) / denom\n",
    "\n",
    "\n",
    "# Quick check: pair-based RI agrees with the contingency-based RI\n",
    "ri_pairs = (counts[\"a_same_same\"] + counts[\"b_diff_diff\"]) / n_pairs\n",
    "ri_cont = rand_index_numpy(y_true, y_pred)\n",
    "ri_pairs, ri_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks + sklearn parity\n",
    "\n",
    "tests = [\n",
    "    (np.array([0]), np.array([1])),\n",
    "    (np.array([0, 1]), np.array([1, 0])),  # label permutation\n",
    "    (np.array([0, 0, 0]), np.array([1, 1, 1])),  # one cluster vs one cluster\n",
    "    (np.array([0, 0, 0]), np.array([0, 1, 2])),  # one cluster vs all singletons\n",
    "    (np.array([0, 1, 2]), np.array([2, 1, 0])),  # all singletons vs all singletons\n",
    "    (y_true, y_pred),\n",
    "]\n",
    "\n",
    "for yt, yp in tests:\n",
    "    ari_np = adjusted_rand_index_numpy(yt, yp)\n",
    "    ri_np = rand_index_numpy(yt, yp)\n",
    "\n",
    "    ari_sk = float(adjusted_rand_score(yt, yp))\n",
    "    print(f\"ARI numpy={ari_np: .6f}  sklearn={ari_sk: .6f}   |  RI={ri_np: .6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d3732",
   "metadata": {},
   "source": [
    "## 4) ARI vs RI under random labelings (chance correction in action)\n",
    "\n",
    "Below, we keep a fixed “true” clustering, then generate **random predicted labels** with different numbers of clusters.\n",
    "\n",
    "- RI tends to increase when you have many clusters, because “different/different” pairs dominate.\n",
    "- ARI stays centered around **0**, because it subtracts the expected agreement under the random model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "\n",
    "# A fixed reference partition (imbalanced on purpose)\n",
    "y_ref = np.repeat([0, 1, 2], [120, 60, 20])\n",
    "rng.shuffle(y_ref)\n",
    "\n",
    "ks = [2, 3, 5, 10, 20]\n",
    "n_trials = 150\n",
    "\n",
    "ri_by_k = {k: [] for k in ks}\n",
    "ari_by_k = {k: [] for k in ks}\n",
    "\n",
    "for k in ks:\n",
    "    for _ in range(n_trials):\n",
    "        y_rand = rng.integers(0, k, size=n)\n",
    "        ri_by_k[k].append(rand_index_numpy(y_ref, y_rand))\n",
    "        ari_by_k[k].append(adjusted_rand_index_numpy(y_ref, y_rand))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Rand Index (RI) under random predictions\",\n",
    "        \"Adjusted Rand Index (ARI) under random predictions\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for k in ks:\n",
    "    fig.add_trace(go.Box(y=ri_by_k[k], name=f\"k={k}\", boxmean=True, showlegend=False), row=1, col=1)\n",
    "    fig.add_trace(go.Box(y=ari_by_k[k], name=f\"k={k}\", boxmean=True, showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"score\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"score\", row=1, col=2)\n",
    "fig.update_layout(title=\"Chance correction: RI vs ARI\", height=420)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d780727",
   "metadata": {},
   "source": [
    "## 5) Visual intuition on 2D data\n",
    "\n",
    "ARI ignores the *geometry* of points — it only looks at the **partition**.\n",
    "Still, plotting helps connect “good/bad partitions” with the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d05c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D blobs (NumPy only)\n",
    "centers = np.array([[-2.0, 0.0], [2.0, 0.0], [0.0, 3.0]])\n",
    "std = 0.6\n",
    "n_per = 150\n",
    "\n",
    "X = np.vstack([rng.normal(loc=c, scale=std, size=(n_per, 2)) for c in centers])\n",
    "y_true_vis = np.repeat(np.arange(len(centers)), n_per)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y_true_vis.astype(str),\n",
    "    title=\"Ground truth partition (for illustration)\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"true cluster\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.85))\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eff3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_labels(labels: np.ndarray, frac: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    labels = np.asarray(labels)\n",
    "    if not (0.0 <= frac <= 1.0):\n",
    "        raise ValueError(\"frac must be in [0, 1]\")\n",
    "\n",
    "    unique = np.unique(labels)\n",
    "    if unique.size < 2:\n",
    "        return labels.copy()\n",
    "\n",
    "    n = labels.size\n",
    "    m = int(round(frac * n))\n",
    "    idx = rng.choice(n, size=m, replace=False)\n",
    "\n",
    "    out = labels.copy()\n",
    "    for i in idx:\n",
    "        current = out[i]\n",
    "        choices = unique[unique != current]\n",
    "        out[i] = rng.choice(choices)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Four different predicted partitions\n",
    "y_pred_perfect = y_true_vis.copy()\n",
    "\n",
    "perm = {0: 2, 1: 0, 2: 1}\n",
    "y_pred_permuted = np.vectorize(perm.get)(y_true_vis)\n",
    "\n",
    "y_pred_noisy = corrupt_labels(y_true_vis, frac=0.15, rng=rng)\n",
    "y_pred_random = rng.integers(0, 3, size=y_true_vis.size)\n",
    "\n",
    "preds = [\n",
    "    (\"perfect match\", y_pred_perfect),\n",
    "    (\"label permutation\", y_pred_permuted),\n",
    "    (\"15% label noise\", y_pred_noisy),\n",
    "    (\"random labels\", y_pred_random),\n",
    "]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        f\"{name}<br>ARI={adjusted_rand_index_numpy(y_true_vis, yp):.3f}\"\n",
    "        for name, yp in preds\n",
    "    ],\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    ")\n",
    "\n",
    "palette = px.colors.qualitative.Safe\n",
    "\n",
    "for idx, (name, yp) in enumerate(preds):\n",
    "    row = 1 + idx // 2\n",
    "    col = 1 + idx % 2\n",
    "\n",
    "    for j, lab in enumerate(np.unique(yp)):\n",
    "        mask = yp == lab\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X[mask, 0],\n",
    "                y=X[mask, 1],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=5, opacity=0.85, color=palette[j % len(palette)]),\n",
    "                name=str(lab),\n",
    "                showlegend=(idx == 0),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text=\"x1\")\n",
    "fig.update_yaxes(title_text=\"x2\", scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_layout(height=650, title=\"Same data, different partitions\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5e0cd",
   "metadata": {},
   "source": [
    "## 6) Contingency table: what ARI “sees”\n",
    "\n",
    "ARI only uses the contingency counts $n_{ij}$ between two labelings.\n",
    "A useful diagnostic is to visualize this table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont, t_labels, p_labels = contingency_matrix_numpy(y_true_vis, y_pred_noisy)\n",
    "\n",
    "fig = px.imshow(\n",
    "    cont,\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    labels={\"x\": \"pred cluster\", \"y\": \"true cluster\", \"color\": \"count\"},\n",
    "    x=p_labels.astype(str),\n",
    "    y=t_labels.astype(str),\n",
    "    title=\"Contingency matrix (true vs predicted)\",\n",
    ")\n",
    "fig.update_layout(height=420)\n",
    "fig.show()\n",
    "\n",
    "sum_nij = float(np.sum(comb2(cont)))\n",
    "sum_ai = float(np.sum(comb2(cont.sum(axis=1))))\n",
    "sum_bj = float(np.sum(comb2(cont.sum(axis=0))))\n",
    "n_pairs = float(comb2(cont.sum()))\n",
    "\n",
    "sum_nij, sum_ai, sum_bj, n_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc7bd5",
   "metadata": {},
   "source": [
    "## 7) Sensitivity: ARI vs “label noise”\n",
    "\n",
    "If we take a reference partition and randomly reassign a fraction of points to different clusters,\n",
    "ARI should smoothly decrease toward $0$ and sometimes go negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fracs = np.linspace(0, 0.8, 17)\n",
    "n_trials = 80\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for frac in fracs:\n",
    "    vals = [adjusted_rand_index_numpy(y_true_vis, corrupt_labels(y_true_vis, frac, rng)) for _ in range(n_trials)]\n",
    "    means.append(float(np.mean(vals)))\n",
    "    stds.append(float(np.std(vals)))\n",
    "\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fracs, y=means, mode=\"lines+markers\", name=\"mean ARI\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.concatenate([fracs, fracs[::-1]]),\n",
    "        y=np.concatenate([means - stds, (means + stds)[::-1]]),\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(0,0,0,0.12)\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        name=\"±1 std\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"ARI vs fraction of corrupted labels\",\n",
    "    xaxis_title=\"fraction corrupted\",\n",
    "    yaxis_title=\"ARI\",\n",
    "    height=420,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203300c6",
   "metadata": {},
   "source": [
    "## 8) Using ARI to “optimize” a simple algorithm (model selection)\n",
    "\n",
    "ARI requires **reference labels** (it’s an *external* metric), so it’s typically used for:\n",
    "\n",
    "- benchmarking clustering algorithms on labeled datasets\n",
    "- tuning hyperparameters (e.g., number of clusters $k$) when you *do* have labels (synthetic data, weak labels, semi-supervised)\n",
    "\n",
    "Below we implement a tiny **k-means** from scratch and choose $k$ by maximizing ARI against the known labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_single_run(\n",
    "    X: np.ndarray,\n",
    "    k: int,\n",
    "    n_iters: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> tuple[np.ndarray, np.ndarray, float]:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    if k <= 0 or k > n:\n",
    "        raise ValueError(\"k must satisfy 1 <= k <= n_samples\")\n",
    "\n",
    "    init_idx = rng.choice(n, size=k, replace=False)\n",
    "    centers = X[init_idx].copy()\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        d2 = np.sum((X[:, None, :] - centers[None, :, :]) ** 2, axis=2)\n",
    "        labels = np.argmin(d2, axis=1)\n",
    "\n",
    "        new_centers = centers.copy()\n",
    "        for j in range(k):\n",
    "            mask = labels == j\n",
    "            if np.any(mask):\n",
    "                new_centers[j] = X[mask].mean(axis=0)\n",
    "\n",
    "        if np.allclose(new_centers, centers):\n",
    "            centers = new_centers\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    inertia = float(np.sum((X - centers[labels]) ** 2))\n",
    "    return labels, centers, inertia\n",
    "\n",
    "\n",
    "def kmeans_from_scratch(\n",
    "    X: np.ndarray,\n",
    "    k: int,\n",
    "    n_init: int = 10,\n",
    "    n_iters: int = 50,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> tuple[np.ndarray, np.ndarray, float]:\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    best_labels = None\n",
    "    best_centers = None\n",
    "    best_inertia = np.inf\n",
    "\n",
    "    for _ in range(n_init):\n",
    "        labels, centers, inertia = kmeans_single_run(X, k=k, n_iters=n_iters, rng=rng)\n",
    "        if inertia < best_inertia:\n",
    "            best_labels, best_centers, best_inertia = labels, centers, inertia\n",
    "\n",
    "    return best_labels, best_centers, float(best_inertia)\n",
    "\n",
    "\n",
    "# Choose k by maximizing ARI on this labeled benchmark\n",
    "k_grid = range(2, 7)\n",
    "results = []\n",
    "\n",
    "for k in k_grid:\n",
    "    labels_k, centers_k, inertia_k = kmeans_from_scratch(X, k=k, n_init=15, n_iters=60, rng=rng)\n",
    "    ari_k = adjusted_rand_index_numpy(y_true_vis, labels_k)\n",
    "    results.append((k, ari_k, inertia_k))\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd96110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.array([r[0] for r in results])\n",
    "aris = np.array([r[1] for r in results])\n",
    "inertias = np.array([r[2] for r in results])\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"ARI vs k\", \"Inertia vs k (k-means objective)\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=ks, y=aris, mode=\"lines+markers\", name=\"ARI\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=ks, y=inertias, mode=\"lines+markers\", name=\"inertia\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"ARI\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"inertia (SSE)\", row=1, col=2)\n",
    "fig.update_layout(height=420, title=\"Using ARI for model selection (requires labels)\")\n",
    "fig.show()\n",
    "\n",
    "best_idx = int(np.argmax(aris))\n",
    "best_k = int(ks[best_idx])\n",
    "best_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_labels, best_centers, best_inertia = kmeans_from_scratch(X, k=best_k, n_init=30, n_iters=80, rng=rng)\n",
    "best_ari = adjusted_rand_index_numpy(y_true_vis, best_labels)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=best_labels.astype(str),\n",
    "    title=f\"Best k by ARI: k={best_k}  (ARI={best_ari:.3f})\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"pred cluster\"},\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=best_centers[:, 0],\n",
    "        y=best_centers[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(symbol=\"x\", size=14, color=\"black\"),\n",
    "        name=\"centers\",\n",
    "    )\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.85), selector=dict(mode=\"markers\"))\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad208fdd",
   "metadata": {},
   "source": [
    "## Pros and cons\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- **Label permutation invariant** (cluster IDs don’t matter)\n",
    "- **Chance-corrected**: random partitions have expected score near $0$\n",
    "- Works when the number of clusters differs between partitions\n",
    "- Interpretable endpoints: $1$ = identical partitions\n",
    "\n",
    "**Cons / limitations**\n",
    "\n",
    "- Requires **reference labels** (not usable for pure unsupervised model selection)\n",
    "- Based on **pair counting**: can be dominated by large clusters; treats all pair errors equally\n",
    "- Ignores geometry/distances (two very different geometric clusterings can get the same ARI if labels match)\n",
    "- Not a smooth/differentiable objective → generally not used directly in gradient-based optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec934f",
   "metadata": {},
   "source": [
    "## Common pitfalls\n",
    "\n",
    "- **Using ARI without ground truth**: prefer internal metrics (silhouette, Davies–Bouldin, etc.) when labels are absent.\n",
    "- **Confusing “label names” with clusters**: ARI doesn’t care about the numeric IDs (0/1/2), only the partition structure.\n",
    "- **Naive $O(n^2)$ implementation**: pair enumeration is slow for large $n$; prefer the contingency-table formula.\n",
    "- **Interpreting negative scores**: ARI < 0 means worse-than-chance agreement under the reference random model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdc9f3",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Implement the **unadjusted** Rand index (RI) in two ways: pair counting and contingency table. Confirm they match.\n",
    "\n",
    "2) Create two different partitions of the same data that have similar RI but different ARI. Explain why.\n",
    "\n",
    "3) For imbalanced cluster sizes, run the random-labeling experiment again and see how RI behaves as you change the number of predicted clusters.\n",
    "\n",
    "4) Replace k-means initialization with **k-means++** and compare ARI and inertia across runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482ff87",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn: `sklearn.metrics.adjusted_rand_score`\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html\n",
    "- Rand, W. M. (1971). *Objective criteria for the evaluation of clustering methods.* Journal of the American Statistical Association.\n",
    "- Hubert, L., & Arabie, P. (1985). *Comparing partitions.* Journal of Classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}