{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean_pinball_loss (Pinball / Quantile Loss)\n",
    "\n",
    "Mean pinball loss (pinball loss / quantile loss) measures the average error of **quantile predictions**.\n",
    "\n",
    "If you predict an $\\alpha$-quantile (e.g. $\\alpha=0.9$ for the 90th percentile), pinball loss answers:\n",
    "\n",
    "> “How far off are my quantile predictions, with the correct asymmetric cost?”\n",
    "\n",
    "It is the standard loss behind **quantile regression** and a building block for **prediction intervals** (fit two quantiles, e.g. 0.05 and 0.95).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Define pinball loss precisely (with math)\n",
    "- Build intuition for the asymmetric penalty controlled by $\\alpha$\n",
    "- Implement `mean_pinball_loss` from scratch in NumPy (including weights / multi-output)\n",
    "- Visualize why the minimizer is the $\\alpha$-quantile\n",
    "- Use mean pinball loss to fit a simple linear quantile regressor (subgradient descent)\n",
    "- Understand pros/cons, pitfalls, and best use cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Definition\n",
    "\n",
    "For a target $y$ and a prediction $\\hat{y}$, the **pinball loss** at quantile level $\\alpha \\in [0, 1]$ is:\n",
    "\n",
    "$$\n",
    "\\ell_{\\alpha}(y, \\hat{y}) =\n",
    "\\begin{cases}\n",
    "\\alpha\\,(y - \\hat{y}) & \\text{if } y \\ge \\hat{y} \\\\\n",
    "(1-\\alpha)\\,(\\hat{y} - y) & \\text{if } y < \\hat{y}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Equivalently, using the residual $u = y - \\hat{y}$:\n",
    "\n",
    "$$\n",
    "\\ell_{\\alpha}(y, \\hat{y}) = \\alpha\\,\\max(u, 0) + (1-\\alpha)\\,\\max(-u, 0)\n",
    "$$\n",
    "\n",
    "Or with an indicator:\n",
    "\n",
    "$$\n",
    "\\ell_{\\alpha}(y, \\hat{y}) = \\bigl(\\alpha - \\mathbf{1}\\{y < \\hat{y}\\}\\bigr)\\,(y - \\hat{y})\n",
    "$$\n",
    "\n",
    "For $n$ samples, **mean pinball loss** is:\n",
    "\n",
    "$$\n",
    "\\mathrm{MPL}_{\\alpha}(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell_{\\alpha}(y_i, \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### Connection to MAE\n",
    "\n",
    "At $\\alpha = 0.5$, the loss is symmetric and proportional to absolute error:\n",
    "\n",
    "$$\n",
    "\\ell_{0.5}(y, \\hat{y}) = 0.5\\,|y-\\hat{y}|\n",
    "$$\n",
    "\n",
    "So it has the **same minimizer** as MAE (the median), but differs by a constant scaling factor.\n",
    "\n",
    "### Weighted version\n",
    "\n",
    "With non-negative sample weights $w_i$:\n",
    "\n",
    "$$\n",
    "\\mathrm{MPL}_{\\alpha,w}(y, \\hat{y}) = \\frac{\\sum_{i=1}^{n} w_i\\,\\ell_{\\alpha}(y_i, \\hat{y}_i)}{\\sum_{i=1}^{n} w_i}\n",
    "$$\n",
    "\n",
    "The loss is in the **same unit** as $y$, is always non-negative, and the best value is $0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny example: alpha controls the asymmetry\n",
    "\n",
    "y_true = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# under-predict the first sample vs over-predict the last sample\n",
    "pred_under = np.array([0.0, 2.0, 3.0])\n",
    "pred_over = np.array([1.0, 2.0, 4.0])\n",
    "\n",
    "for alpha in [0.1, 0.5, 0.9]:\n",
    "    loss_under = mean_pinball_loss(y_true, pred_under, alpha=alpha)\n",
    "    loss_over = mean_pinball_loss(y_true, pred_over, alpha=alpha)\n",
    "    print(f\"alpha={alpha:.1f} | loss(under)={float(loss_under):.4f} | loss(over)={float(loss_over):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition: a “tilted” absolute error\n",
    "\n",
    "Pinball loss is a piecewise-linear function of the prediction $\\hat{y}$.\n",
    "\n",
    "- If you **under-predict** ($\\hat{y} < y$), the loss slope is $-\\alpha$.\n",
    "- If you **over-predict** ($\\hat{y} > y$), the loss slope is $(1-\\alpha)$.\n",
    "\n",
    "So $\\alpha$ chooses which mistake is more expensive:\n",
    "\n",
    "- $\\alpha=0.9$ (upper quantile): under-prediction is expensive, over-prediction is cheap → pushes predictions upward.\n",
    "- $\\alpha=0.1$ (lower quantile): over-prediction is expensive, under-prediction is cheap → pushes predictions downward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss for a single sample\n",
    "\n",
    "y0 = 2.0\n",
    "\n",
    "yhat_grid = np.linspace(y0 - 4, y0 + 4, 400)\n",
    "\n",
    "fig = go.Figure()\n",
    "for alpha in [0.1, 0.5, 0.9]:\n",
    "    diff = y0 - yhat_grid\n",
    "    loss = alpha * np.maximum(diff, 0) + (1 - alpha) * np.maximum(-diff, 0)\n",
    "    fig.add_trace(go.Scatter(x=yhat_grid, y=loss, mode=\"lines\", name=f\"alpha={alpha}\"))\n",
    "\n",
    "fig.add_vline(x=y0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(\n",
    "    title=\"Pinball loss for one sample (y = 2.0)\",\n",
    "    xaxis_title=\"prediction ŷ\",\n",
    "    yaxis_title=\"loss ℓα(y, ŷ)\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Why it targets quantiles\n",
    "\n",
    "Consider a **constant** predictor $\\hat{y}=c$ (no features).\n",
    "\n",
    "A key fact:\n",
    "\n",
    "> The value of $c$ that minimizes the expected pinball loss is an **$\\alpha$-quantile** of $Y$.\n",
    "\n",
    "Sketch (subgradient): for one sample,\n",
    "\n",
    "- if $c < y$ then $\\ell_\\alpha(y,c) = \\alpha(y-c)$ and $\\partial_c \\ell_\\alpha = -\\alpha$\n",
    "- if $c > y$ then $\\ell_\\alpha(y,c) = (1-\\alpha)(c-y)$ and $\\partial_c \\ell_\\alpha = (1-\\alpha)$\n",
    "\n",
    "So the expected subgradient at $c$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\partial_c\\,\\ell_\\alpha(Y,c)]\n",
    "= (1-\\alpha)\\,\\mathbb{P}(Y < c) - \\alpha\\,\\mathbb{P}(Y > c)\n",
    "$$\n",
    "\n",
    "The optimum is where this crosses $0$, which gives the quantile condition.\n",
    "\n",
    "In a finite sample, minimizing mean pinball loss over $c$ yields the **sample $\\alpha$-quantile**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: the best constant predictor is the sample quantile\n",
    "\n",
    "n = 500\n",
    "# A skewed distribution to make the shift visible\n",
    "y = rng.lognormal(mean=0.0, sigma=0.6, size=n)\n",
    "\n",
    "alphas = [0.1, 0.5, 0.9]\n",
    "\n",
    "lo, hi = np.quantile(y, [0.01, 0.99])\n",
    "c_grid = np.linspace(lo, hi, 400)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for alpha in alphas:\n",
    "    diff = y[:, None] - c_grid[None, :]\n",
    "    loss = alpha * np.maximum(diff, 0) + (1 - alpha) * np.maximum(-diff, 0)\n",
    "    mpl = loss.mean(axis=0)\n",
    "\n",
    "    q = float(np.quantile(y, alpha))\n",
    "    c_star = float(c_grid[np.argmin(mpl)])\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=c_grid, y=mpl, mode=\"lines\", name=f\"alpha={alpha}\"))\n",
    "    fig.add_vline(x=q, line_dash=\"dash\", line_color=\"gray\")\n",
    "    fig.add_vline(x=c_star, line_dash=\"dot\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Constant prediction: minimizer is the α-quantile\",\n",
    "    xaxis_title=\"constant prediction c\",\n",
    "    yaxis_title=\"mean pinball loss\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) NumPy implementation (from scratch)\n",
    "\n",
    "Below is a small implementation that mirrors scikit-learn's API shape:\n",
    "\n",
    "- supports `sample_weight`\n",
    "- supports multi-output targets (`(n_samples, n_outputs)`)\n",
    "- supports `multioutput='raw_values'` or averaging across outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_2d(a):\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim == 1:\n",
    "        return a.reshape(-1, 1)\n",
    "    return a\n",
    "\n",
    "\n",
    "def mean_pinball_loss_np(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    alpha=0.5,\n",
    "    sample_weight=None,\n",
    "    multioutput=\"uniform_average\",\n",
    "):\n",
    "    y_true = _as_2d(y_true)\n",
    "    y_pred = _as_2d(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true{y_true.shape} vs y_pred{y_pred.shape}\")\n",
    "\n",
    "    if not (0.0 <= alpha <= 1.0):\n",
    "        raise ValueError(\"alpha must be in [0, 1]\")\n",
    "\n",
    "    diff = y_true - y_pred\n",
    "    loss = alpha * np.maximum(diff, 0.0) + (1.0 - alpha) * np.maximum(-diff, 0.0)\n",
    "\n",
    "    # average over samples -> per-output errors\n",
    "    output_errors = np.average(loss, weights=sample_weight, axis=0)\n",
    "\n",
    "    if multioutput == \"raw_values\":\n",
    "        return output_errors\n",
    "\n",
    "    if multioutput == \"uniform_average\":\n",
    "        return float(np.mean(output_errors))\n",
    "\n",
    "    # multioutput is array-like weights for outputs\n",
    "    multioutput = np.asarray(multioutput)\n",
    "    return float(np.average(output_errors, weights=multioutput))\n",
    "\n",
    "\n",
    "# Quick equivalence checks with scikit-learn\n",
    "\n",
    "y_true = rng.normal(size=50)\n",
    "y_pred = y_true + rng.normal(scale=0.5, size=50)\n",
    "w = rng.uniform(0.5, 2.0, size=50)\n",
    "\n",
    "for alpha in [0.1, 0.5, 0.9]:\n",
    "    a0 = float(mean_pinball_loss(y_true, y_pred, alpha=alpha))\n",
    "    b0 = mean_pinball_loss_np(y_true, y_pred, alpha=alpha)\n",
    "\n",
    "    a1 = float(mean_pinball_loss(y_true, y_pred, alpha=alpha, sample_weight=w))\n",
    "    b1 = mean_pinball_loss_np(y_true, y_pred, alpha=alpha, sample_weight=w)\n",
    "\n",
    "    print(f\"alpha={alpha:.1f} | unweighted diff={a0-b0:+.2e} | weighted diff={a1-b1:+.2e}\")\n",
    "\n",
    "# Multi-output example\n",
    "Y_true = rng.normal(size=(40, 2))\n",
    "Y_pred = Y_true + rng.normal(scale=0.3, size=(40, 2))\n",
    "\n",
    "mean_pinball_loss_np(Y_true, Y_pred, alpha=0.5, multioutput=\"raw_values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Using mean pinball loss to optimize a model (linear quantile regression)\n",
    "\n",
    "Let a linear model be:\n",
    "\n",
    "$$\n",
    "\\hat{y} = Xw + b\n",
    "$$\n",
    "\n",
    "Quantile regression fits the $\\alpha$-quantile by minimizing mean pinball loss:\n",
    "\n",
    "$$\n",
    "\\min_{w,b}\\; \\frac{1}{n}\\sum_{i=1}^{n} \\ell_{\\alpha}\\bigl(y_i, x_i^\\top w + b\\bigr)\n",
    "$$\n",
    "\n",
    "For linear models this objective is **convex**, but it is **not differentiable** at $y=\\hat{y}$.\n",
    "A simple low-level optimizer is **subgradient descent**.\n",
    "\n",
    "### Subgradient w.r.t. prediction\n",
    "\n",
    "For a single sample, with $u = y - \\hat{y}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_{\\alpha}}{\\partial \\hat{y}} =\n",
    "\\begin{cases}\n",
    "-\\alpha & u > 0 \\\\\n",
    "(1-\\alpha) & u < 0 \\\\\n",
    "[-\\alpha,\\; 1-\\alpha] & u = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then the parameter subgradients are:\n",
    "\n",
    "$$\n",
    "\\nabla_w L = \\frac{1}{n}X^\\top g,\n",
    "\\qquad\n",
    "\\nabla_b L = \\frac{1}{n}\\sum_{i=1}^{n} g_i\n",
    "$$\n",
    "\n",
    "where $g_i$ is a chosen subgradient for sample $i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_quantile_regression_subgd(\n",
    "    X,\n",
    "    y,\n",
    "    *,\n",
    "    alpha=0.5,\n",
    "    lr=0.2,\n",
    "    n_iters=3000,\n",
    "    log_every=50,\n",
    "):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "\n",
    "    steps = []\n",
    "    losses = []\n",
    "\n",
    "    for t in range(1, n_iters + 1):\n",
    "        y_pred = X @ w + b\n",
    "        diff = y - y_pred\n",
    "\n",
    "        # subgradient wrt prediction ŷ\n",
    "        g = np.where(diff > 0, -alpha, 1.0 - alpha)\n",
    "        g = np.where(diff == 0, 0.0, g)  # pick 0 at the kink\n",
    "\n",
    "        grad_w = (X.T @ g) / n\n",
    "        grad_b = float(np.mean(g))\n",
    "\n",
    "        lr_t = lr / np.sqrt(t)  # diminishing step size\n",
    "        w -= lr_t * grad_w\n",
    "        b -= lr_t * grad_b\n",
    "\n",
    "        if t % log_every == 0 or t == 1:\n",
    "            y_pred = X @ w + b\n",
    "            losses.append(mean_pinball_loss_np(y, y_pred, alpha=alpha))\n",
    "            steps.append(t)\n",
    "\n",
    "    return w, b, np.array(steps), np.array(losses)\n",
    "\n",
    "\n",
    "# Synthetic heteroscedastic data where the true conditional quantiles are linear\n",
    "n = 600\n",
    "x = rng.uniform(0, 10, size=n)\n",
    "\n",
    "beta0 = 1.0\n",
    "beta1 = 2.0\n",
    "sigma0 = 0.5\n",
    "sigma1 = 0.2  # noise scale increases with x\n",
    "\n",
    "sigma = sigma0 + sigma1 * x\n",
    "noise = rng.normal(loc=0.0, scale=sigma, size=n)\n",
    "\n",
    "y = beta0 + beta1 * x + noise\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "alphas = [0.1, 0.5, 0.9]\n",
    "\n",
    "fits = {}\n",
    "for a in alphas:\n",
    "    w, b, steps, losses = fit_linear_quantile_regression_subgd(X, y, alpha=a)\n",
    "    fits[a] = {\"w\": w, \"b\": b, \"steps\": steps, \"losses\": losses}\n",
    "\n",
    "{k: (v['w'][0], v['b']) for k, v in fits.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fitted quantile lines\n",
    "\n",
    "x_grid = np.linspace(x.min(), x.max(), 250)\n",
    "X_grid = x_grid.reshape(-1, 1)\n",
    "\n",
    "colors = {0.1: \"#1f77b4\", 0.5: \"#2ca02c\", 0.9: \"#d62728\"}\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode=\"markers\",\n",
    "        name=\"data\",\n",
    "        marker=dict(size=6, opacity=0.45),\n",
    "    )\n",
    ")\n",
    "\n",
    "for a in alphas:\n",
    "    w, b = fits[a][\"w\"], fits[a][\"b\"]\n",
    "    y_hat = X_grid @ w + b\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_grid,\n",
    "            y=y_hat,\n",
    "            mode=\"lines\",\n",
    "            name=f\"fit α={a}\",\n",
    "            line=dict(color=colors[a], width=3),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# (Optional) true quantile lines (we know the data-generating process)\n",
    "for a in alphas:\n",
    "    z = norm.ppf(a)\n",
    "    y_true_q = (beta0 + z * sigma0) + (beta1 + z * sigma1) * x_grid\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_grid,\n",
    "            y=y_true_q,\n",
    "            mode=\"lines\",\n",
    "            name=f\"true α={a}\",\n",
    "            line=dict(color=colors[a], width=2, dash=\"dash\"),\n",
    "            opacity=0.85,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Linear quantile regression trained with mean pinball loss\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in alphas:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fits[a][\"steps\"],\n",
    "            y=fits[a][\"losses\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"α={a}\",\n",
    "            line=dict(width=3, color=colors[a]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Subgradient descent on mean pinball loss\",\n",
    "    xaxis_title=\"iteration\",\n",
    "    yaxis_title=\"mean pinball loss\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: scikit-learn's `QuantileRegressor`\n",
    "\n",
    "`QuantileRegressor` solves the same optimization problem (with optional regularization) using linear programming.\n",
    "We'll fit it (without regularization) and compare parameters and scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_fits = {}\n",
    "for a in alphas:\n",
    "    model = QuantileRegressor(quantile=a, alpha=0.0, solver=\"highs\")\n",
    "    model.fit(X, y)\n",
    "    sk_fits[a] = model\n",
    "\n",
    "for a in alphas:\n",
    "    w_subgd, b_subgd = fits[a][\"w\"][0], fits[a][\"b\"]\n",
    "    w_sk, b_sk = sk_fits[a].coef_[0], sk_fits[a].intercept_\n",
    "\n",
    "    print(\n",
    "        f\"α={a}: subGD w={w_subgd:.3f}, b={b_subgd:.3f} | sklearn w={w_sk:.3f}, b={b_sk:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Practical usage: scoring quantile predictions\n",
    "\n",
    "In practice you typically:\n",
    "\n",
    "- train a model to predict a specific quantile (say $\\alpha=0.9$)\n",
    "- evaluate it with `mean_pinball_loss(y_true, y_pred, alpha=0.9)`\n",
    "\n",
    "If you predict multiple quantiles, compute the metric for each $\\alpha$ separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score subgradient-descent vs sklearn solutions on the same data\n",
    "\n",
    "for a in alphas:\n",
    "    y_hat_subgd = X @ fits[a][\"w\"] + fits[a][\"b\"]\n",
    "    y_hat_sklearn = sk_fits[a].predict(X)\n",
    "\n",
    "    score_subgd = mean_pinball_loss(y, y_hat_subgd, alpha=a)\n",
    "    score_sklearn = mean_pinball_loss(y, y_hat_sklearn, alpha=a)\n",
    "\n",
    "    print(f\"α={a}: mean_pinball_loss subGD={float(score_subgd):.4f} | sklearn={float(score_sklearn):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Pros / cons and when to use\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Targets quantiles directly** → prediction intervals and risk-sensitive forecasting\n",
    "- **Asymmetric costs**: tune $\\alpha$ to match business penalties (stockouts vs overstock, SLA breaches, etc.)\n",
    "- **Robust** to outliers compared to squared losses (linear penalty in the tails)\n",
    "- For linear models, the objective is **convex** (global optimum)\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Non-smooth** at $y=\\hat{y}$ → optimization uses subgradients / linear programming and may be slower\n",
    "- Requires choosing $\\alpha$ (often multiple values)\n",
    "- Not a single-number summary of full predictive uncertainty (you need multiple quantiles)\n",
    "- Fitting multiple quantiles independently can lead to **quantile crossing** (e.g. $\\hat{q}_{0.9}(x) < \\hat{q}_{0.5}(x)$)\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- Forecasting with uncertainty bands (delivery times, demand, energy load)\n",
    "- Finance / risk (Value-at-Risk style quantiles)\n",
    "- Any regression where over- vs under-prediction costs differ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Pitfalls & diagnostics\n",
    "\n",
    "- **Scale dependence**: like MAE, mean pinball loss is in target units; compare across datasets only after scaling/normalizing.\n",
    "- **Choose $\\alpha$ intentionally**: align it with decisions (e.g. $\\alpha=0.9$ for “plan conservatively”).\n",
    "- **Quantile crossing**: when fitting multiple $\\alpha$ values, check ordering; consider joint methods that enforce monotonicity.\n",
    "- **Coverage check**: if you fit lower/upper quantiles, verify empirical coverage (e.g. fraction of points between $\\alpha=0.05$ and $0.95$ predictions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Exercises\n",
    "\n",
    "1) Implement `mean_pinball_loss_np` without `np.maximum` (use only `np.where`).\n",
    "2) For a fixed dataset, plot the constant-prediction loss curves for $\\alpha \\in \\{0.1, 0.5, 0.9\\}$ on the same chart.\n",
    "3) Fit two quantile regressors (0.1 and 0.9) and compute the empirical coverage of the resulting interval.\n",
    "4) Add L2 regularization to the subgradient descent objective and observe the effect on the fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn: `sklearn.metrics.mean_pinball_loss`\n",
    "- Koenker, R. (2005). *Quantile Regression*. Cambridge University Press.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}