{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean_absolute_error (MAE)\n",
    "\n",
    "Mean Absolute Error (MAE) measures the average **absolute** difference between targets and predictions.\n",
    "\n",
    "If $y$ is the true target and $\\hat{y}$ is the prediction, MAE answers:\n",
    "\n",
    "> “On average, how many units am I off?”\n",
    "\n",
    "It is one of the most interpretable regression metrics because it is expressed in the **same units** as the target.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Define MAE precisely (with math)\n",
    "- Build intuition with plots\n",
    "- Implement MAE from scratch in NumPy (including weights / multi-output)\n",
    "- Use MAE as an optimization objective (L1 / median regression) with subgradient descent\n",
    "- Understand pros/cons and when to use MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Definition\n",
    "\n",
    "For $n$ samples, MAE is:\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE}(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Let the **residual** be $r_i = \\hat{y}_i - y_i$. Then MAE is the mean of $|r_i|$.\n",
    "\n",
    "Equivalently (vector form for 1D targets): $\\mathrm{MAE}(y, \\hat{y}) = \\frac{1}{n}\\lVert y - \\hat{y}\\rVert_1$.\n",
    "\n",
    "### Weighted MAE\n",
    "\n",
    "With non-negative sample weights $w_i$:\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE}_w(y, \\hat{y}) = \\frac{\\sum_{i=1}^{n} w_i |y_i - \\hat{y}_i|}{\\sum_{i=1}^{n} w_i}\n",
    "$$\n",
    "\n",
    "### Multi-output targets\n",
    "\n",
    "If $y \\in \\mathbb{R}^{n \\times m}$ (multiple outputs), you typically compute MAE **per output** and then aggregate (e.g. uniform average).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition: MAE is “average distance”\n",
    "\n",
    "Absolute error is a **distance on the number line**:\n",
    "\n",
    "- it never cancels out (negative errors do not offset positive ones)\n",
    "- it is linear: doubling an error doubles its contribution\n",
    "- it’s in the same unit as $y$ (e.g. dollars, °C, minutes)\n",
    "\n",
    "A useful mental model is “typical miss size”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny example\n",
    "y_true = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n",
    "\n",
    "residuals = y_pred - y_true\n",
    "abs_errors = np.abs(residuals)\n",
    "\n",
    "mae_manual = abs_errors.mean()\n",
    "mae_sklearn = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "residuals, abs_errors, mae_manual, mae_sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual: each absolute error is a segment length\n",
    "\n",
    "For each sample $i$, MAE takes the length of the segment between $y_i$ and $\\hat{y}_i$, then averages those lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(y_true))\n",
    "mid = 0.5 * (y_true + y_pred)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=idx, y=y_true, mode=\"markers\", name=\"y_true\", marker=dict(size=10)))\n",
    "fig.add_trace(go.Scatter(x=idx, y=y_pred, mode=\"markers\", name=\"y_pred\", marker=dict(size=10)))\n",
    "\n",
    "for i in idx:\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=int(i), x1=int(i),\n",
    "        y0=float(y_true[i]), y1=float(y_pred[i]),\n",
    "        line=dict(color=\"gray\", width=2),\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=idx,\n",
    "        y=mid,\n",
    "        mode=\"text\",\n",
    "        text=[f\"|e|={e:.2f}\" for e in abs_errors],\n",
    "        showlegend=False,\n",
    "        textposition=\"middle right\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Absolute errors per sample (MAE = {mae_manual:.3f})\",\n",
    "    xaxis_title=\"sample index i\",\n",
    "    yaxis_title=\"value\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) MAE vs MSE: different penalty shapes\n",
    "\n",
    "Both MAE and MSE summarize residuals $r = \\hat{y} - y$, but they penalize them differently:\n",
    "\n",
    "- **MAE** uses $|r|$ (linear penalty)\n",
    "- **MSE** uses $r^2$ (quadratic penalty)\n",
    "\n",
    "So MSE puts **much more weight on large errors**, while MAE is more **robust to outliers**.\n",
    "\n",
    "### Subgradient of the absolute value\n",
    "\n",
    "The absolute value is not differentiable at 0, but it has a **subgradient**:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dr}|r| =\n",
    "\\begin{cases}\n",
    "+1 & r > 0 \\\\\n",
    "-1 & r < 0 \\\\\n",
    "\\text{any value in }[-1, 1] & r = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In code we typically use `np.sign(r)` (and treat exactly-zero residuals as 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace(-5, 5, 401)\n",
    "l1 = np.abs(r)\n",
    "l2 = r**2\n",
    "\n",
    "# Huber (smooth L1) is often used as a differentiable alternative\n",
    "delta = 1.0\n",
    "huber = np.where(np.abs(r) <= delta, 0.5 * r**2, delta * (np.abs(r) - 0.5 * delta))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=r, y=l1, name=\"MAE pointwise: |r|\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=r, y=l2, name=\"MSE pointwise: r²\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=r, y=huber, name=\"Huber (smooth L1)\", line=dict(width=3, dash=\"dot\")))\n",
    "fig.update_layout(\n",
    "    title=\"Pointwise loss vs residual r\",\n",
    "    xaxis_title=\"residual r = ŷ - y\",\n",
    "    yaxis_title=\"loss for a single sample\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier sensitivity: one bad point\n",
    "\n",
    "Suppose $n-1$ predictions are perfect ($r=0$) and **one** sample has residual $m$.\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE} = \\frac{|m|}{n}\n",
    "\\qquad\\qquad\n",
    "\\mathrm{MSE} = \\frac{m^2}{n}\n",
    "$$\n",
    "\n",
    "Even though both are averaged, MSE still grows **quadratically** in $|m|$, so a single large error can dominate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "m = np.linspace(0, 20, 401)\n",
    "\n",
    "mae_one = m / n_samples\n",
    "mse_one = (m**2) / n_samples\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=m, y=mae_one, name=\"MAE (one outlier)\", line=dict(width=3)))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=m,\n",
    "        y=mse_one,\n",
    "        name=\"MSE (one outlier)\",\n",
    "        yaxis=\"y2\",\n",
    "        line=dict(width=3, dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Effect of 1 outlier among n={n_samples} samples\",\n",
    "    xaxis_title=\"outlier magnitude |m|\",\n",
    "    yaxis_title=\"MAE (same units as y)\",\n",
    "    yaxis2=dict(title=\"MSE (squared units)\", overlaying=\"y\", side=\"right\"),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) A key property: MAE is minimized by the median\n",
    "\n",
    "Suppose your model can only output a **constant** prediction $c$ (no features).\n",
    "The objective becomes:\n",
    "\n",
    "$$\n",
    "J(c) = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - c|\n",
    "$$\n",
    "\n",
    "The value(s) of $c$ that minimize $J(c)$ are the **median(s)** of $y$.\n",
    "\n",
    "Intuition: moving $c$ slightly to the right increases the loss by $+\\varepsilon$ for every point left of $c$,\n",
    "and decreases it by $-\\varepsilon$ for every point right of $c$. At the optimum those counts balance — that’s the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rng.normal(loc=0.0, scale=1.0, size=250)\n",
    "y[:6] += 8  # a few big outliers\n",
    "\n",
    "c_grid = np.linspace(y.min() - 1, y.max() + 1, 500)\n",
    "mae_c = np.mean(np.abs(y[:, None] - c_grid[None, :]), axis=0)\n",
    "\n",
    "c_argmin = float(c_grid[np.argmin(mae_c)])\n",
    "y_median = float(np.median(y))\n",
    "y_mean = float(y.mean())\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=c_grid, y=mae_c, name=\"MAE(c)\", line=dict(width=3)))\n",
    "fig.add_vline(x=y_median, line=dict(color=\"green\", dash=\"dash\"), annotation_text=\"median(y)\")\n",
    "fig.add_vline(x=y_mean, line=dict(color=\"red\", dash=\"dot\"), annotation_text=\"mean(y)\")\n",
    "fig.add_vline(x=c_argmin, line=dict(color=\"black\"), annotation_text=\"argmin\")\n",
    "fig.update_layout(\n",
    "    title=\"For a constant predictor, MAE is minimized at the median\",\n",
    "    xaxis_title=\"constant prediction c\",\n",
    "    yaxis_title=\"MAE(c)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "y_mean, y_median, c_argmin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) NumPy implementation from scratch\n",
    "\n",
    "Below is a small NumPy implementation that matches scikit-learn’s behavior for:\n",
    "\n",
    "- `sample_weight` (weighted average over samples)\n",
    "- multi-output targets with `multioutput`:\n",
    "  - `\"raw_values\"` (per-output MAE)\n",
    "  - `\"uniform_average\"` (average of per-output MAEs)\n",
    "  - an array of output weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error_np(y_true, y_pred, *, sample_weight=None, multioutput=\"uniform_average\"):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true{y_true.shape} vs y_pred{y_pred.shape}\")\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        abs_err = np.abs(y_true - y_pred)\n",
    "        if sample_weight is None:\n",
    "            return float(abs_err.mean())\n",
    "\n",
    "        w = np.asarray(sample_weight)\n",
    "        if w.shape != abs_err.shape:\n",
    "            raise ValueError(f\"sample_weight must have shape {abs_err.shape}, got {w.shape}\")\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "        return float(np.sum(w * abs_err) / np.sum(w))\n",
    "\n",
    "    if y_true.ndim != 2:\n",
    "        raise ValueError(\"y_true must be 1D or 2D\")\n",
    "\n",
    "    abs_err = np.abs(y_true - y_pred)  # (n_samples, n_outputs)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        output_errors = abs_err.mean(axis=0)\n",
    "    else:\n",
    "        w = np.asarray(sample_weight)\n",
    "        if w.ndim != 1 or w.shape[0] != abs_err.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"sample_weight must have shape (n_samples,), got {w.shape} for n_samples={abs_err.shape[0]}\"\n",
    "            )\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "        output_errors = np.sum(abs_err * w[:, None], axis=0) / np.sum(w)\n",
    "\n",
    "    if multioutput == \"raw_values\":\n",
    "        return output_errors\n",
    "    if multioutput == \"uniform_average\":\n",
    "        return float(output_errors.mean())\n",
    "\n",
    "    weights = np.asarray(multioutput)\n",
    "    if weights.shape != (abs_err.shape[1],):\n",
    "        raise ValueError(\n",
    "            f\"multioutput weights must have shape (n_outputs,), got {weights.shape} for n_outputs={abs_err.shape[1]}\"\n",
    "        )\n",
    "    if np.any(weights < 0):\n",
    "        raise ValueError(\"multioutput weights must be non-negative\")\n",
    "    return float(np.average(output_errors, weights=weights))\n",
    "\n",
    "# Quick check vs scikit-learn\n",
    "y_true_2d = rng.normal(size=(12, 3))\n",
    "y_pred_2d = y_true_2d + rng.normal(scale=0.2, size=(12, 3))\n",
    "w = rng.uniform(0.5, 2.0, size=12)\n",
    "\n",
    "ours = mean_absolute_error_np(y_true_2d, y_pred_2d, sample_weight=w, multioutput=\"raw_values\")\n",
    "sk = mean_absolute_error(y_true_2d, y_pred_2d, sample_weight=w, multioutput=\"raw_values\")\n",
    "\n",
    "ours, sk, np.allclose(ours, sk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Using MAE to *fit* a model: L1 regression (median regression)\n",
    "\n",
    "If we use MAE as the training objective, we get **L1 regression**.\n",
    "For a linear model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = Xw + b\n",
    "$$\n",
    "\n",
    "the MAE objective is:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{n}\\sum_{i=1}^{n} |(Xw + b)_i - y_i|\n",
    "$$\n",
    "\n",
    "This is convex, but not differentiable everywhere. A common low-level optimizer is **subgradient descent**.\n",
    "\n",
    "Let $r = Xw + b - y$ and $s = \\mathrm{sign}(r)$ (using 0 when $r=0$). One valid subgradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_w J = \\frac{1}{n} X^\\top s\\qquad\\qquad\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n s_i\n",
    "$$\n",
    "\n",
    "Statistical interpretation: minimizing MAE corresponds to maximum likelihood under **Laplace (double exponential)** noise, and it targets the **conditional median** (robust to outliers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic regression data with outliers\n",
    "n = 220\n",
    "x = rng.uniform(0, 10, size=n)\n",
    "X = x[:, None]\n",
    "\n",
    "beta0_true = 1.0\n",
    "beta1_true = 2.0\n",
    "\n",
    "# Laplace noise matches the MAE/L1 modeling assumption\n",
    "noise = rng.laplace(loc=0.0, scale=1.0, size=n)\n",
    "y = beta0_true + beta1_true * x + noise\n",
    "\n",
    "# Inject a few large outliers\n",
    "outlier_idx = rng.choice(n, size=7, replace=False)\n",
    "y[outlier_idx] += rng.normal(loc=25.0, scale=5.0, size=outlier_idx.size)\n",
    "\n",
    "outlier_idx[:5], x.min(), x.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ordinary least squares (OLS): minimizes MSE, not MAE\n",
    "ols = LinearRegression().fit(X, y)\n",
    "y_pred_ols = ols.predict(X)\n",
    "\n",
    "mae_ols = mean_absolute_error(y, y_pred_ols)\n",
    "mse_ols = mean_squared_error(y, y_pred_ols)\n",
    "\n",
    "(ols.intercept_, ols.coef_[0]), (mae_ols, mse_ols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression_mae_subgradient(X, y, *, lr0=0.8, n_iters=3000):\n",
    "    \"\"\"Minimize MAE for y_hat = X @ w + b using subgradient descent.\n",
    "\n",
    "    Uses a decaying learning rate: lr_t = lr0 / sqrt(t+1).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    w = np.zeros(n_features)\n",
    "    b = float(np.median(y))  # good starting point when w=0\n",
    "    history = np.empty(n_iters)\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        y_hat = X @ w + b\n",
    "        r = y_hat - y\n",
    "        s = np.sign(r)  # subgradient of |r|\n",
    "\n",
    "        grad_w = (X.T @ s) / n_samples\n",
    "        grad_b = s.mean()\n",
    "\n",
    "        lr = lr0 / np.sqrt(t + 1)\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "        history[t] = np.mean(np.abs(r))\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "# Feature scaling improves optimization stability\n",
    "x_mean = float(x.mean())\n",
    "x_std = float(x.std())\n",
    "X_scaled = ((x - x_mean) / x_std)[:, None]\n",
    "\n",
    "w_scaled, b_scaled, hist = fit_linear_regression_mae_subgradient(X_scaled, y)\n",
    "\n",
    "# Convert back to original x scale\n",
    "w_mae = w_scaled[0] / x_std\n",
    "b_mae = b_scaled - w_scaled[0] * x_mean / x_std\n",
    "\n",
    "y_pred_mae = w_mae * x + b_mae\n",
    "\n",
    "mae_mae = mean_absolute_error(y, y_pred_mae)\n",
    "mse_mae = mean_squared_error(y, y_pred_mae)\n",
    "\n",
    "(b_mae, w_mae), (mae_mae, mse_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    y=hist,\n",
    "    title=\"Subgradient descent: MAE objective vs iteration\",\n",
    "    labels={\"index\": \"iteration\", \"y\": \"train MAE\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.linspace(x.min(), x.max(), 250)\n",
    "\n",
    "y_line_true = beta0_true + beta1_true * x_line\n",
    "y_line_ols = ols.intercept_ + ols.coef_[0] * x_line\n",
    "y_line_mae = b_mae + w_mae * x_line\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x, y=y, mode=\"markers\", name=\"data\",\n",
    "        marker=dict(size=7, color=\"rgba(0,0,0,0.55)\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x[outlier_idx], y=y[outlier_idx], mode=\"markers\", name=\"outliers\",\n",
    "        marker=dict(size=10, color=\"crimson\", symbol=\"x\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_line, y=y_line_true, mode=\"lines\", name=\"true line\",\n",
    "        line=dict(color=\"green\", dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_line, y=y_line_ols, mode=\"lines\",\n",
    "        name=f\"OLS (MSE) fit | MAE={mae_ols:.2f}\",\n",
    "        line=dict(width=3),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_line, y=y_line_mae, mode=\"lines\",\n",
    "        name=f\"L1 (MAE) fit | MAE={mae_mae:.2f}\",\n",
    "        line=dict(width=3),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Outliers pull OLS more than L1/MAE regression\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Practical usage (scikit-learn)\n",
    "\n",
    "MAE is typically used as an **evaluation** metric on validation/test sets and for model selection.\n",
    "\n",
    "In scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "For cross-validation, scikit-learn uses a “bigger is better” convention, so it exposes MAE as **negative** MAE:\n",
    "\n",
    "```python\n",
    "cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(LinearRegression(), X, y, scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "mae_scores = -scores\n",
    "\n",
    "mae_scores, mae_scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros, cons, and when to use MAE\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Interpretable**: same units as the target (easy to communicate)\n",
    "- **Robust-ish to outliers**: large errors grow linearly, not quadratically\n",
    "- **Median-targeting**: minimizing MAE targets the conditional median (useful when outliers/heavy tails exist)\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Not differentiable at 0**: requires subgradients or smooth approximations (Huber / smooth L1)\n",
    "- **Doesn’t emphasize large errors**: if big misses are especially costly, MAE may under-penalize them\n",
    "- **Scale-dependent**: MAE values can’t be compared across targets with different units/scales\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- When the cost of an error grows roughly **linearly** with its magnitude\n",
    "- When your data has **outliers** or heavy-tailed noise and you want a stable “typical error” number\n",
    "- When you care about the **median** behavior rather than the mean\n",
    "\n",
    "---\n",
    "\n",
    "## Common pitfalls / diagnostics\n",
    "\n",
    "- If $y$ has a wide range, MAE can look “large” even for good models; compare to a baseline (e.g. predict median).\n",
    "- MAE alone hides error distribution; pair it with a plot of residuals or a percentile-based metric.\n",
    "- If your business cost is asymmetric (over-prediction vs under-prediction differ), consider **pinball loss** / quantile regression instead of plain MAE.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Prove that the minimizer of $\\sum_i |y_i - c|$ is any median of $y$.\n",
    "2. Modify `mean_absolute_error_np` to support an `axis` argument.\n",
    "3. Compare MAE vs RMSE on the same dataset as you add more outliers. What changes first?\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn: `sklearn.metrics.mean_absolute_error`\n",
    "- Robust regression / L1 loss: connection to Laplace noise and median regression\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
