{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52eb5069",
   "metadata": {},
   "source": [
    "# R² Score (`r2_score`)\n",
    "\n",
    "The **coefficient of determination** \\(R^2\\) is a regression metric that answers:\n",
    "\n",
    "> “How much better are my predictions than the simplest baseline that always predicts the mean?”\n",
    "\n",
    "It’s scale-free and easy to interpret on a *single dataset*, but it has some gotchas (negative values, constant targets, and the fact that “variance explained” has conditions).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- derive and interpret \\(R^2\\)\n",
    "- understand why \\(R^2\\in(-\\infty, 1]\\) and what \\(R^2=0\\) means\n",
    "- implement `r2_score` from scratch in NumPy (including weights + multioutput)\n",
    "- connect maximizing \\(R^2\\) to minimizing squared error\n",
    "- track \\(R^2\\) while fitting linear regression with gradient descent\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- mean and variance\n",
    "- squared error\n",
    "- basic regression notation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import r2_score as sk_r2_score\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46a9a0",
   "metadata": {},
   "source": [
    "## 1) Definition: compare to the mean baseline\n",
    "\n",
    "Given true targets \\(y_1,\\dots,y_n\\) and predictions \\(\\hat{y}_1,\\dots,\\hat{y}_n\\):\n",
    "\n",
    "- residuals: \\(e_i = y_i - \\hat{y}_i\\)\n",
    "- baseline prediction: the mean \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i\\)\n",
    "\n",
    "Define the **sum of squared errors** and the **total sum of squares**:\n",
    "\n",
    "\\[\n",
    "\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2,\\qquad\n",
    "\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n",
    "\\]\n",
    "\n",
    "Then the \\(R^2\\) score is:\n",
    "\n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "\\]\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- \\(R^2 = 1\\): perfect predictions (\\(\\text{SSE}=0\\))\n",
    "- \\(R^2 = 0\\): **no improvement** over predicting the mean (\\(\\text{SSE}=\\text{SST}\\))\n",
    "- \\(R^2 < 0\\): **worse** than predicting the mean (\\(\\text{SSE}>\\text{SST}\\))\n",
    "\n",
    "So \\(R^2\\) is a *relative-to-baseline* score, not an “absolute error” measure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small synthetic example: four predictors on the same y_true\n",
    "n = 60\n",
    "x = np.linspace(0, 10, n)\n",
    "y_true = 1.5 + 0.8 * x + rng.normal(0, 1.2, size=n)\n",
    "\n",
    "mean_pred = np.full_like(y_true, y_true.mean())\n",
    "\n",
    "models = {\n",
    "    \"Perfect\": y_true,\n",
    "    \"Good (small noise)\": y_true + rng.normal(0, 0.5, size=n),\n",
    "    \"Mean baseline\": mean_pred,\n",
    "    \"Worse-than-mean (anti)\": 2 * y_true.mean() - y_true,  # guarantees R² = -3\n",
    "}\n",
    "\n",
    "sst = np.sum((y_true - y_true.mean()) ** 2)\n",
    "\n",
    "rows = []\n",
    "for name, y_pred in models.items():\n",
    "    sse = np.sum((y_true - y_pred) ** 2)\n",
    "    rows.append((name, sse, 1 - sse / sst, sk_r2_score(y_true, y_pred)))\n",
    "\n",
    "print(\"SST (baseline SSE) =\", sst)\n",
    "print(\"\\nModel comparison\")\n",
    "print(\"-\" * 70)\n",
    "for name, sse, r2_manual, r2_sklearn in rows:\n",
    "    print(f\"{name:24s} SSE={sse:8.2f}  R²(manual)={r2_manual:7.3f}  R²(sklearn)={r2_sklearn:7.3f}\")\n",
    "\n",
    "# Bar view: R² is a rescaling of SSE (on a fixed dataset)\n",
    "names = [r[0] for r in rows]\n",
    "sse_vals = np.array([r[1] for r in rows], dtype=float)\n",
    "r2_vals = np.array([r[3] for r in rows], dtype=float)\n",
    "sse_over_sst = sse_vals / sst\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Relative error: SSE / SST\", \"R² = 1 - SSE/SST\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(x=names, y=sse_over_sst, name=\"SSE/SST\", marker_color=\"#E45756\"), row=1, col=1)\n",
    "fig.add_hline(\n",
    "    y=1.0,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    row=1,\n",
    "    col=1,\n",
    "    annotation_text=\"mean baseline\",\n",
    "    annotation_position=\"top left\",\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(x=names, y=r2_vals, name=\"R²\", marker_color=\"#4C78A8\"), row=1, col=2)\n",
    "fig.add_hline(\n",
    "    y=0.0,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    row=1,\n",
    "    col=2,\n",
    "    annotation_text=\"mean baseline\",\n",
    "    annotation_position=\"top left\",\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"SSE / SST\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"R²\", row=1, col=2)\n",
    "fig.update_xaxes(tickangle=-20, row=1, col=1)\n",
    "fig.update_xaxes(tickangle=-20, row=1, col=2)\n",
    "fig.update_layout(title=\"Same ordering, different scale\", height=420, showlegend=False)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b02912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual intuition: y_true vs y_pred (each panel shows a different model)\n",
    "subplot_titles = [\n",
    "    f\"{name}<br>R² = {sk_r2_score(y_true, y_pred):.3f}\" for name, y_pred in models.items()\n",
    "]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=subplot_titles)\n",
    "\n",
    "mn = float(np.min(y_true))\n",
    "mx = float(np.max(y_true))\n",
    "\n",
    "for k, (name, y_pred) in enumerate(models.items()):\n",
    "    row = 1 if k < 2 else 2\n",
    "    col = 1 if (k % 2) == 0 else 2\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=y_true, y=y_pred, mode=\"markers\", name=name, showlegend=False),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[mn, mx],\n",
    "            y=[mn, mx],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\", dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"y_true\", row=row, col=col)\n",
    "    fig.update_yaxes(title_text=\"y_pred\", row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"R² compares SSE to the mean baseline (R² can be negative)\",\n",
    "    height=650,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b174d",
   "metadata": {},
   "source": [
    "### Why the mean baseline makes sense\n",
    "\n",
    "The baseline prediction \\(\\hat{y}_i = \\bar{y}\\) minimizes squared error among all *constant* predictors.\n",
    "\n",
    "- Any constant \\(c\\) gives \\(\\sum_i (y_i - c)^2\\).\n",
    "- The derivative w.r.t. \\(c\\) is \\(-2\\sum_i(y_i-c)\\), which is 0 at \\(c=\\bar{y}\\).\n",
    "\n",
    "So \\(\\text{SST}\\) is literally “the best SSE you can do without using features”.\n",
    "\n",
    "That’s why \\(R^2=0\\) means “no better than doing nothing smarter than predicting the mean”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize why the mean is the best constant predictor\n",
    "c_grid = np.linspace(float(y_true.min()) - 2.0, float(y_true.max()) + 2.0, 200)\n",
    "sse_grid = np.array([np.sum((y_true - c) ** 2) for c in c_grid])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=c_grid, y=sse_grid, mode=\"lines\", name=\"SSE(c)\"))\n",
    "fig.add_vline(\n",
    "    x=float(y_true.mean()),\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=\"mean(y)\",\n",
    "    annotation_position=\"top\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Constant predictor: SSE is minimized at the mean\",\n",
    "    xaxis_title=\"constant prediction c\",\n",
    "    yaxis_title=\"SSE\",\n",
    "    height=350,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d522e",
   "metadata": {},
   "source": [
    "## 2) Sums of squares and “variance explained”\n",
    "\n",
    "A common phrase is that \\(R^2\\) is the “fraction of variance explained”. That’s true in a specific (important) setting:\n",
    "\n",
    "- you fit **ordinary least squares (OLS)** with an **intercept**.\n",
    "\n",
    "In that case, define the **explained sum of squares**:\n",
    "\n",
    "\\[\n",
    "\\text{SSR} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\n",
    "\\]\n",
    "\n",
    "Then we get the classic decomposition:\n",
    "\n",
    "\\[\n",
    "\\text{SST} = \\text{SSR} + \\text{SSE}\n",
    "\\]\n",
    "\n",
    "and so\n",
    "\n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = \\frac{\\text{SSR}}{\\text{SST}}.\n",
    "\\]\n",
    "\n",
    "For **arbitrary predictions** (not OLS fitted values), the decomposition \\(\\text{SST}=\\text{SSR}+\\text{SSE}\\) need not hold, but \\(R^2 = 1-\\text{SSE}/\\text{SST}\\) is still the standard definition used by `sklearn.metrics.r2_score`.\n",
    "\n",
    "### A useful rewrite (same dataset)\n",
    "\n",
    "If you define\n",
    "\n",
    "- \\(\\text{MSE} = \\frac{1}{n}\\sum_i (y_i-\\hat{y}_i)^2\\)\n",
    "- \\(\\mathrm{Var}(y) = \\frac{1}{n}\\sum_i (y_i-\\bar{y})^2\\)  (population variance, with \\(n\\) not \\(n-1\\))\n",
    "\n",
    "then\n",
    "\n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\text{MSE}}{\\mathrm{Var}(y)}.\n",
    "\\]\n",
    "\n",
    "This highlights something important: **on a fixed dataset, maximizing \\(R^2\\) is equivalent to minimizing MSE/SSE**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show SST = SSR + SSE for an OLS fit with an intercept\n",
    "n = 120\n",
    "x = rng.uniform(-3, 3, size=n)\n",
    "y = 2.0 + 1.5 * x + rng.normal(0, 1.0, size=n)\n",
    "\n",
    "X = np.column_stack([np.ones(n), x])  # intercept + feature\n",
    "w_ols, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "y_hat = X @ w_ols\n",
    "\n",
    "y_bar = y.mean()\n",
    "SST = np.sum((y - y_bar) ** 2)\n",
    "SSE = np.sum((y - y_hat) ** 2)\n",
    "SSR = np.sum((y_hat - y_bar) ** 2)\n",
    "\n",
    "print(\"w_ols =\", w_ols)\n",
    "print(f\"SST={SST:.3f}  SSR={SSR:.3f}  SSE={SSE:.3f}  (SST-(SSR+SSE))={SST-(SSR+SSE):.3e}\")\n",
    "print(f\"R² (manual) = {1 - SSE / SST:.4f}\")\n",
    "print(f\"R² (sklearn) = {sk_r2_score(y, y_hat):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690582c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance decomposition as a stacked bar\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=\"Explained (SSR)\", x=[\"SST\"], y=[SSR], marker_color=\"#4C78A8\"))\n",
    "fig.add_trace(go.Bar(name=\"Residual (SSE)\", x=[\"SST\"], y=[SSE], marker_color=\"#E45756\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    title=f\"OLS with intercept: SST = SSR + SSE  (R² = {1 - SSE / SST:.3f})\",\n",
    "    yaxis_title=\"Sum of squares\",\n",
    "    height=350,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c042e",
   "metadata": {},
   "source": [
    "## 3) From-scratch NumPy implementation\n",
    "\n",
    "### Weighted + multioutput definitions\n",
    "\n",
    "For sample weights \\(w_i \\ge 0\\), define the weighted mean:\n",
    "\n",
    "\\[\n",
    "\\bar{y}_w = \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i}\n",
    "\\]\n",
    "\n",
    "Then (per output):\n",
    "\n",
    "\\[\n",
    "\\text{SSE}_w = \\sum_{i=1}^n w_i (y_i - \\hat{y}_i)^2,\\qquad\n",
    "\\text{SST}_w = \\sum_{i=1}^n w_i (y_i - \\bar{y}_w)^2,\\qquad\n",
    "R^2_w = 1 - \\frac{\\text{SSE}_w}{\\text{SST}_w}.\n",
    "\\]\n",
    "\n",
    "For multioutput \\(y \\in \\mathbb{R}^{n\\times m}\\), compute \\(R^2\\) per output and then aggregate.\n",
    "\n",
    "Below is a small NumPy implementation that mirrors the core ideas in scikit-learn:\n",
    "\n",
    "- supports 1D targets \\((n,)\\) and multioutput targets \\((n, m)\\)\n",
    "- supports `sample_weight`\n",
    "- supports `multioutput` aggregation:\n",
    "  - `'raw_values'` (per-output scores)\n",
    "  - `'uniform_average'` (simple mean)\n",
    "  - `'variance_weighted'` (weights each output by its \\(\\text{SST}\\))\n",
    "- handles constant targets with `force_finite=True` (sklearn’s default):\n",
    "  - perfect predictions \\(\\Rightarrow 1.0\\)\n",
    "  - imperfect predictions \\(\\Rightarrow 0.0\\)\n",
    "\n",
    "Note: with fewer than 2 samples, \\(R^2\\) is undefined; scikit-learn returns `nan`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e07618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_numpy(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput=\"uniform_average\",\n",
    "    force_finite=True,\n",
    "):\n",
    "    \"\"\"NumPy implementation of the R² score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true, y_pred : array-like\n",
    "        Shape (n_samples,) or (n_samples, n_outputs)\n",
    "\n",
    "    sample_weight : array-like, optional\n",
    "        Shape (n_samples,)\n",
    "\n",
    "    multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or array-like\n",
    "\n",
    "    force_finite : bool\n",
    "        If True (default), replace non-finite scores for constant targets:\n",
    "        - perfect predictions -> 1.0\n",
    "        - imperfect predictions -> 0.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float or ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"y_true and y_pred must have the same shape, got {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        y_true_2d = y_true.reshape(-1, 1)\n",
    "        y_pred_2d = y_pred.reshape(-1, 1)\n",
    "    elif y_true.ndim == 2:\n",
    "        y_true_2d = y_true\n",
    "        y_pred_2d = y_pred\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 1D or 2D arrays, got ndim={y_true.ndim}\")\n",
    "\n",
    "    n_samples, n_outputs = y_true_2d.shape\n",
    "\n",
    "    if n_samples < 2:\n",
    "        # R² is undefined with fewer than 2 samples (sklearn returns nan + a warning)\n",
    "        return np.nan\n",
    "\n",
    "    if sample_weight is None:\n",
    "        w = np.ones((n_samples, 1), dtype=float)\n",
    "    else:\n",
    "        w = np.asarray(sample_weight, dtype=float).reshape(-1, 1)\n",
    "        if w.shape[0] != n_samples:\n",
    "            raise ValueError(f\"sample_weight must have shape (n_samples,), got {w.shape}\")\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "\n",
    "    w_sum = np.sum(w)\n",
    "    if w_sum == 0:\n",
    "        raise ValueError(\"Sum of sample_weight must be > 0\")\n",
    "\n",
    "    # Weighted mean per output\n",
    "    y_bar = np.sum(w * y_true_2d, axis=0) / w_sum\n",
    "\n",
    "    numerator = np.sum(w * (y_true_2d - y_pred_2d) ** 2, axis=0)  # SSE per output\n",
    "    denominator = np.sum(w * (y_true_2d - y_bar) ** 2, axis=0)     # SST per output\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        output_scores = 1.0 - (numerator / denominator)\n",
    "\n",
    "    if force_finite:\n",
    "        # Constant targets -> denominator == 0\n",
    "        mask = denominator == 0\n",
    "        if np.any(mask):\n",
    "            # Perfect prediction => numerator == 0 -> score 1.0\n",
    "            perfect = mask & (numerator == 0)\n",
    "            output_scores = output_scores.copy()\n",
    "            output_scores[perfect] = 1.0\n",
    "            output_scores[mask & ~perfect] = 0.0\n",
    "\n",
    "    # Multioutput aggregation\n",
    "    if multioutput == \"raw_values\":\n",
    "        scores = output_scores\n",
    "    elif multioutput == \"uniform_average\":\n",
    "        scores = float(np.mean(output_scores))\n",
    "    elif multioutput == \"variance_weighted\":\n",
    "        # Weight by SST (denominator)\n",
    "        denom_sum = float(np.sum(denominator))\n",
    "        scores = float(np.sum(output_scores * denominator) / denom_sum) if denom_sum > 0 else float(np.mean(output_scores))\n",
    "    else:\n",
    "        # array-like weights per output\n",
    "        weights = np.asarray(multioutput, dtype=float)\n",
    "        if weights.shape != (n_outputs,):\n",
    "            raise ValueError(f\"multioutput weights must have shape (n_outputs,), got {weights.shape}\")\n",
    "        if np.any(weights < 0):\n",
    "            raise ValueError(\"multioutput weights must be non-negative\")\n",
    "        w_out_sum = float(np.sum(weights))\n",
    "        if w_out_sum == 0:\n",
    "            raise ValueError(\"Sum of multioutput weights must be > 0\")\n",
    "        scores = float(np.sum(output_scores * weights) / w_out_sum)\n",
    "\n",
    "    if n_outputs == 1 and multioutput != \"raw_values\":\n",
    "        return float(scores)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5be839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks vs scikit-learn\n",
    "\n",
    "# 1D\n",
    "y_true = rng.normal(size=50)\n",
    "y_pred = y_true + rng.normal(0, 0.3, size=50)\n",
    "\n",
    "print(\"1D\")\n",
    "print(\"numpy :\", r2_score_numpy(y_true, y_pred))\n",
    "print(\"sklearn:\", sk_r2_score(y_true, y_pred))\n",
    "\n",
    "# Multioutput\n",
    "Y_true = rng.normal(size=(80, 3))\n",
    "Y_pred = Y_true + rng.normal(0, 0.5, size=(80, 3))\n",
    "w = rng.uniform(0.5, 2.0, size=80)\n",
    "\n",
    "print(\"\\nMultioutput (raw)\")\n",
    "print(\"numpy :\", r2_score_numpy(Y_true, Y_pred, sample_weight=w, multioutput=\"raw_values\"))\n",
    "print(\"sklearn:\", sk_r2_score(Y_true, Y_pred, sample_weight=w, multioutput=\"raw_values\"))\n",
    "\n",
    "print(\"\\nMultioutput (uniform_average)\")\n",
    "print(\"numpy :\", r2_score_numpy(Y_true, Y_pred, sample_weight=w, multioutput=\"uniform_average\"))\n",
    "print(\"sklearn:\", sk_r2_score(Y_true, Y_pred, sample_weight=w, multioutput=\"uniform_average\"))\n",
    "\n",
    "print(\"\\nMultioutput (variance_weighted)\")\n",
    "print(\"numpy :\", r2_score_numpy(Y_true, Y_pred, sample_weight=w, multioutput=\"variance_weighted\"))\n",
    "print(\"sklearn:\", sk_r2_score(Y_true, Y_pred, sample_weight=w, multioutput=\"variance_weighted\"))\n",
    "\n",
    "# Constant target edge case\n",
    "print(\"\\nConstant y_true\")\n",
    "y_const = np.ones(5)\n",
    "print(\"perfect (force_finite=True):\", r2_score_numpy(y_const, np.ones(5), force_finite=True))\n",
    "print(\"bad     (force_finite=True):\", r2_score_numpy(y_const, np.zeros(5), force_finite=True))\n",
    "print(\"perfect (force_finite=False):\", r2_score_numpy(y_const, np.ones(5), force_finite=False))\n",
    "print(\"bad     (force_finite=False):\", r2_score_numpy(y_const, np.zeros(5), force_finite=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9df0ff",
   "metadata": {},
   "source": [
    "## 4) Using R² while optimizing a model (from scratch)\n",
    "\n",
    "Because \\(\\text{SST}\\) depends **only** on \\(y\\), it is constant w.r.t. model parameters \\(\\theta\\) on a fixed dataset.\n",
    "\n",
    "\\[\n",
    "R^2(\\theta) = 1 - \\frac{\\text{SSE}(\\theta)}{\\text{SST}}\n",
    "\\]\n",
    "\n",
    "So:\n",
    "\n",
    "- maximizing \\(R^2\\) \\(\\Leftrightarrow\\) minimizing \\(\\text{SSE}\\) (or MSE)\n",
    "- their gradients differ only by a constant scale:\n",
    "\n",
    "\\[\n",
    "\\nabla_\\theta R^2(\\theta) = -\\frac{1}{\\text{SST}}\\nabla_\\theta \\text{SSE}(\\theta)\n",
    "\\]\n",
    "\n",
    "In practice we optimize MSE/SSE (a proper loss), and use \\(R^2\\) as a **training/validation score**.\n",
    "\n",
    "### Example: linear regression with gradient descent\n",
    "\n",
    "We’ll fit a line \\(\\hat{y} = w_0 + w_1 x\\) by minimizing MSE, and track \\(R^2\\) over iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: y = w0 + w1 x + noise\n",
    "n = 200\n",
    "x = rng.uniform(-2, 4, size=n)\n",
    "y = 1.0 + 2.5 * x + rng.normal(0, 1.0, size=n)\n",
    "\n",
    "X = np.column_stack([np.ones(n), x])\n",
    "\n",
    "# Closed-form (for reference)\n",
    "w_closed, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "\n",
    "# Gradient descent\n",
    "w = np.zeros(2)\n",
    "lr = 0.05\n",
    "n_steps = 300\n",
    "\n",
    "r2_hist = []\n",
    "mse_hist = []\n",
    "w_hist = []\n",
    "\n",
    "for _ in range(n_steps):\n",
    "    y_hat = X @ w\n",
    "    err = y_hat - y\n",
    "\n",
    "    mse = float(np.mean(err**2))\n",
    "    grad = (2.0 / n) * (X.T @ err)\n",
    "\n",
    "    r2_hist.append(r2_score_numpy(y, y_hat))\n",
    "    mse_hist.append(mse)\n",
    "    w_hist.append(w.copy())\n",
    "\n",
    "    w = w - lr * grad\n",
    "\n",
    "w_hist = np.asarray(w_hist)\n",
    "\n",
    "print(\"Closed-form w:\", w_closed)\n",
    "print(\"GD final w   :\", w)\n",
    "print(\"Final R²     :\", r2_hist[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization diagnostics\n",
    "iters = np.arange(n_steps)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"R² vs iteration\", \"MSE vs iteration\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=iters, y=r2_hist, mode=\"lines\", name=\"R²\"), row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"R²\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"iteration\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=iters, y=mse_hist, mode=\"lines\", name=\"MSE\", line=dict(color=\"#E45756\")), row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"MSE\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"iteration\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=350, title=\"Gradient descent improves MSE and (equivalently) R²\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit visualization: data + mean baseline + fitted line\n",
    "x_line = np.linspace(x.min(), x.max(), 200)\n",
    "X_line = np.column_stack([np.ones_like(x_line), x_line])\n",
    "\n",
    "y_line_gd = X_line @ w\n",
    "y_line_closed = X_line @ w_closed\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode=\"markers\", name=\"data\", marker=dict(size=6, opacity=0.6)))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=np.full_like(x_line, y.mean()), mode=\"lines\", name=\"mean baseline\", line=dict(dash=\"dash\", color=\"gray\")))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_closed, mode=\"lines\", name=\"closed-form\", line=dict(color=\"#4C78A8\", width=3)))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_gd, mode=\"lines\", name=\"gradient descent\", line=dict(color=\"#F58518\", width=3)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Linear regression fit (R² ≈ {r2_score_numpy(y, X @ w):.3f})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6c7d9",
   "metadata": {},
   "source": [
    "## 5) Pros, cons, and when to use\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Baseline-relative**: interpretable against a meaningful reference (predicting \\(\\bar{y}\\))\n",
    "- **Scale-free**: shifting/scaling \\(y\\) (and predictions accordingly) does not change \\(R^2\\)\n",
    "- **Widely used**: default `.score()` for many sklearn regressors\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- **Can be negative**: a model can be arbitrarily worse than the mean baseline\n",
    "- **Not defined for constant targets**: if \\(\\text{SST}=0\\), `sklearn` uses `force_finite=True` to map `(nan, -inf)` to `(1.0, 0.0)`\n",
    "- **“Variance explained” is conditional**: the \\(\\text{SST}=\\text{SSR}+\\text{SSE}\\) story holds for OLS with an intercept, not arbitrary predictions\n",
    "- **Can reward overfitting**: on training data, \\(R^2\\) almost always increases as you add features (even useless ones)\n",
    "- **Outlier-sensitive**: uses squared error, so large residuals dominate\n",
    "\n",
    "### When it’s a good choice\n",
    "\n",
    "- comparing regression models on the **same target** and **same dataset split**\n",
    "- reporting an intuitive “better than mean?” summary alongside an absolute metric (MAE/RMSE)\n",
    "- model selection with **cross-validated** \\(R^2\\) rather than training \\(R^2\\)\n",
    "\n",
    "### When to be careful\n",
    "\n",
    "- comparing across datasets with very different target variance\n",
    "- heavy-tailed noise / many outliers (consider MAE, Huber loss, or robust metrics)\n",
    "- non-stationary time series (always evaluate out-of-sample; consider rolling/forward validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf927d6",
   "metadata": {},
   "source": [
    "## 6) Exercises\n",
    "\n",
    "1. Show that predicting \\(\\bar{y}\\) is the best constant predictor by plotting SSE as a function of constant \\(c\\).\n",
    "2. Create a case where \\(R^2\\) is strongly negative and explain it in terms of SSE vs SST.\n",
    "3. Implement **adjusted** \\(R^2\\):\n",
    "\n",
    "\\[\n",
    "\\bar{R}^2 = 1 - (1 - R^2)\\frac{n-1}{n-d-1}\n",
    "\\]\n",
    "\n",
    "and compare it to \\(R^2\\) while adding random (useless) features.\n",
    "4. Compute **cross-validated** \\(R^2\\) for a linear model vs a constant baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c2c48",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "- Wikipedia: https://en.wikipedia.org/wiki/Coefficient_of_determination\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}