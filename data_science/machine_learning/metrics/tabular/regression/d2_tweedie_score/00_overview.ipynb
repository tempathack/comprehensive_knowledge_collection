{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7b9a1f",
   "metadata": {},
   "source": [
    "# D² Tweedie Score (`d2_tweedie_score`)\n",
    "\n",
    "`d2_tweedie_score` is a **relative-to-baseline** regression score:\n",
    "\n",
    "> “What fraction of the Tweedie deviance did my model explain compared to just predicting the mean?”\n",
    "\n",
    "It generalizes $R^2$ to non-Gaussian targets (counts, positive skewed data, and zero-mass + continuous data) via the **Tweedie family**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- define $D^2$ as “fraction of Tweedie deviance explained”\n",
    "- understand the power parameter $p$ and the allowed target/prediction domains\n",
    "- implement `mean_tweedie_deviance` and `d2_tweedie_score` from scratch in NumPy (incl. sample weights)\n",
    "- visualize how different $p$ values change the penalty for under/over-prediction\n",
    "- optimize a simple Poisson regression (a Tweedie GLM with $p=1$) and track $D^2$ during training\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import d2_tweedie_score\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- logs and exponentials\n",
    "- weighted averages\n",
    "- basic regression notation ($y$, $\\hat{y}$)\n",
    "- (optional) GLMs / deviance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import d2_tweedie_score as sk_d2_tweedie_score\n",
    "from sklearn.metrics import mean_tweedie_deviance as sk_mean_tweedie_deviance\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c4f8a",
   "metadata": {},
   "source": [
    "## 1) Definition: “fraction of Tweedie deviance explained”\n",
    "\n",
    "Given targets $y_1,\\dots,y_n$ and predictions $\\hat{\\mu}_1,\\dots,\\hat{\\mu}_n$, define the (possibly weighted) **mean Tweedie deviance**:\n",
    "\n",
    "$$\n",
    "\\bar{d}_p(y,\\hat{\\mu})\n",
    "= \\frac{1}{\\sum_i w_i}\\sum_{i=1}^n w_i\\, d_p(y_i, \\hat{\\mu}_i),\n",
    "\\qquad w_i \\ge 0\n",
    "$$\n",
    "\n",
    "The $D^2$ Tweedie score compares your model to the constant baseline that predicts the (weighted) mean:\n",
    "\n",
    "$$\n",
    "\\bar{y}_w = \\frac{\\sum_i w_i y_i}{\\sum_i w_i},\n",
    "\\qquad\n",
    "\\hat{\\mu}^{(0)}_i = \\bar{y}_w\n",
    "$$\n",
    "\n",
    "$$\n",
    "D^2_p = 1 - \\frac{\\bar{d}_p(y,\\hat{\\mu})}{\\bar{d}_p(y,\\hat{\\mu}^{(0)})}\n",
    "$$\n",
    "\n",
    "Interpretation (just like $R^2$):\n",
    "\n",
    "- $D^2_p = 1$: perfect predictions (zero deviance)\n",
    "- $D^2_p = 0$: no improvement over predicting the mean\n",
    "- $D^2_p < 0$: worse than the mean baseline\n",
    "\n",
    "Special case: $p=0$ uses squared-error deviance, so $D^2_0$ equals $R^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example: same y_true, different predictors, different powers\n",
    "\n",
    "y_true = np.array([0.5, 1.0, 2.5, 7.0])\n",
    "\n",
    "preds = {\n",
    "    \"Mean baseline\": np.full_like(y_true, y_true.mean()),\n",
    "    \"Example model\": np.array([1.0, 1.0, 5.0, 3.5]),\n",
    "    \"Perfect\": y_true.copy(),\n",
    "    \"Bad constant\": np.full_like(y_true, 10.0),\n",
    "}\n",
    "\n",
    "powers = [0, 1, 2]\n",
    "\n",
    "fig = go.Figure()\n",
    "for p in powers:\n",
    "    scores = [sk_d2_tweedie_score(y_true, y_pred, power=p) for y_pred in preds.values()]\n",
    "    fig.add_trace(go.Bar(name=f\"power={p}\", x=list(preds.keys()), y=scores))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"D² Tweedie score across different powers (same data, same predictions)\",\n",
    "    barmode=\"group\",\n",
    "    xaxis_title=\"Predictor\",\n",
    "    yaxis_title=\"D² (higher is better)\",\n",
    "    width=950,\n",
    "    height=420,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2a4d6",
   "metadata": {},
   "source": [
    "## 2) Tweedie family + Tweedie deviance (the loss behind $D^2$)\n",
    "\n",
    "The Tweedie family is an exponential-dispersion model where the variance scales with the mean:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(Y) = \\phi\\,\\mu^p\n",
    "$$\n",
    "\n",
    "Common cases:\n",
    "\n",
    "| $p$ | Distribution | Typical target domain |\n",
    "|---:|---|---|\n",
    "| $0$ | Normal | $y\\in\\mathbb{R}$ |\n",
    "| $1$ | Poisson | $y\\ge 0$ (counts) |\n",
    "| $1<p<2$ | Compound Poisson–Gamma | $y\\ge 0$ (mass at 0 + continuous $>0$) |\n",
    "| $2$ | Gamma | $y>0$ |\n",
    "| $3$ | Inverse Gaussian | $y>0$ |\n",
    "\n",
    "In a GLM, we model a mean $\\mu_i = \\mathbb{E}[y_i\\mid x_i]$. To keep $\\mu_i>0$ we often use a **log link**:\n",
    "\n",
    "$$\n",
    "\\eta_i = x_i^\\top \\beta,\\qquad \\mu_i = \\exp(\\eta_i)\n",
    "$$\n",
    "\n",
    "### Unit deviance $d_p(y,\\mu)$\n",
    "\n",
    "`mean_tweedie_deviance` is the average of a per-sample unit deviance $d_p(y,\\mu)$ (with the convention $0\\log 0 = 0$).\n",
    "\n",
    "Special cases:\n",
    "\n",
    "$$\n",
    "d_0(y,\\mu) = (y-\\mu)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_1(y,\\mu) = 2\\left(y\\log\\frac{y}{\\mu} - y + \\mu\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_2(y,\\mu) = 2\\left(\\log\\frac{\\mu}{y} + \\frac{y}{\\mu} - 1\\right)\n",
    "$$\n",
    "\n",
    "General case ($p \\notin \\{0,1,2\\}$):\n",
    "\n",
    "$$\n",
    "d_p(y,\\mu)= 2\\left[\\frac{y^{2-p}}{(1-p)(2-p)} - \\frac{y\\mu^{1-p}}{1-p} + \\frac{\\mu^{2-p}}{2-p}\\right]\n",
    "$$\n",
    "\n",
    "Domain requirements (matching scikit-learn):\n",
    "\n",
    "- $p < 0$: $\\mu>0$ (and the formula uses $\\max(y,0)^{2-p}$ to avoid non-real powers)\n",
    "- $p = 0$: $y,\\mu\\in\\mathbb{R}$\n",
    "- $1 \\le p < 2$: $y\\ge 0$, $\\mu>0$\n",
    "- $p \\ge 2$: $y>0$, $\\mu>0$\n",
    "\n",
    "### Derivative (useful for optimization)\n",
    "\n",
    "For all $p$ (via limits at $p=0,1,2$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d_p(y,\\mu)}{\\partial \\mu}\n",
    "= 2\\left(\\mu^{1-p} - y\\mu^{-p}\\right)\n",
    "= 2\\mu^{-p}(\\mu - y)\n",
    "$$\n",
    "\n",
    "So $d_p$ is minimized at $\\mu=y$, but the curvature / asymmetry depends on $p$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _xlogy(x, y):\n",
    "    \"\"\"Compute x * log(y) with the convention 0 * log(y) = 0.\"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    bx, by = np.broadcast_arrays(x, y)\n",
    "\n",
    "    out = np.zeros_like(bx, dtype=float)\n",
    "    mask = bx != 0\n",
    "    out[mask] = bx[mask] * np.log(by[mask])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _validate_power(power: float) -> float:\n",
    "    p = float(power)\n",
    "    if not (p <= 0 or p >= 1):\n",
    "        raise ValueError(f\"power must satisfy p <= 0 or p >= 1, got p={p}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def _check_1d_targets(y_true, y_pred, sample_weight=None):\n",
    "    y_true = np.asarray(y_true, dtype=float).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred, dtype=float).reshape(-1)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"y_true and y_pred must have the same shape, got {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return y_true, y_pred, None\n",
    "\n",
    "    w = np.asarray(sample_weight, dtype=float).reshape(-1)\n",
    "    if w.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(f\"sample_weight must have shape (n_samples,), got {w.shape}\")\n",
    "    if np.any(w < 0):\n",
    "        raise ValueError(\"sample_weight must be non-negative\")\n",
    "    if float(np.sum(w)) == 0.0:\n",
    "        raise ValueError(\"Sum of sample_weight must be > 0\")\n",
    "\n",
    "    return y_true, y_pred, w\n",
    "\n",
    "\n",
    "def tweedie_unit_deviance_numpy(y_true, y_pred, *, power: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"Per-sample Tweedie unit deviance d_p(y, μ) (NumPy).\n",
    "\n",
    "    This matches scikit-learn's conventions and input domain checks.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.asarray(y_true, dtype=float).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred, dtype=float).reshape(-1)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"y_true and y_pred must have the same shape, got {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "    p = _validate_power(power)\n",
    "\n",
    "    message = f\"Mean Tweedie deviance error with power={p} can only be used on \"\n",
    "    if p < 0:\n",
    "        # 'Extreme stable', y any real number, y_pred > 0\n",
    "        if np.any(y_pred <= 0):\n",
    "            raise ValueError(message + \"strictly positive y_pred.\")\n",
    "    elif p == 0:\n",
    "        # Normal, y and y_pred can be any real number\n",
    "        pass\n",
    "    elif 1 <= p < 2:\n",
    "        # Poisson and compound Poisson distribution, y >= 0, y_pred > 0\n",
    "        if np.any(y_true < 0) or np.any(y_pred <= 0):\n",
    "            raise ValueError(message + \"non-negative y and strictly positive y_pred.\")\n",
    "    elif p >= 2:\n",
    "        # Gamma / Inverse Gaussian / positive stable, y and y_pred > 0\n",
    "        if np.any(y_true <= 0) or np.any(y_pred <= 0):\n",
    "            raise ValueError(message + \"strictly positive y and y_pred.\")\n",
    "\n",
    "    zero = 0.0\n",
    "    if p < 0:\n",
    "        # Prevent non-real powers for negative targets\n",
    "        y_pos = np.where(y_true > 0, y_true, zero)\n",
    "        dev = 2 * (\n",
    "            np.power(y_pos, 2 - p) / ((1 - p) * (2 - p))\n",
    "            - y_true * np.power(y_pred, 1 - p) / (1 - p)\n",
    "            + np.power(y_pred, 2 - p) / (2 - p)\n",
    "        )\n",
    "    elif p == 0:\n",
    "        dev = (y_true - y_pred) ** 2\n",
    "    elif p == 1:\n",
    "        dev = 2 * (_xlogy(y_true, y_true / y_pred) - y_true + y_pred)\n",
    "    elif p == 2:\n",
    "        dev = 2 * (np.log(y_pred / y_true) + y_true / y_pred - 1)\n",
    "    else:\n",
    "        dev = 2 * (\n",
    "            np.power(y_true, 2 - p) / ((1 - p) * (2 - p))\n",
    "            - y_true * np.power(y_pred, 1 - p) / (1 - p)\n",
    "            + np.power(y_pred, 2 - p) / (2 - p)\n",
    "        )\n",
    "\n",
    "    return dev\n",
    "\n",
    "\n",
    "def mean_tweedie_deviance_numpy(y_true, y_pred, *, power: float = 0.0, sample_weight=None) -> float:\n",
    "    \"\"\"Mean Tweedie deviance (NumPy).\"\"\"\n",
    "\n",
    "    y_true, y_pred, w = _check_1d_targets(y_true, y_pred, sample_weight)\n",
    "    dev = tweedie_unit_deviance_numpy(y_true, y_pred, power=power)\n",
    "\n",
    "    if w is None:\n",
    "        return float(np.mean(dev))\n",
    "    return float(np.average(dev, weights=w))\n",
    "\n",
    "\n",
    "def d2_tweedie_score_numpy(y_true, y_pred, *, power: float = 0.0, sample_weight=None) -> float:\n",
    "    \"\"\"NumPy implementation of scikit-learn's d2_tweedie_score.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Best score is 1.0; it can be negative.\n",
    "    - Returns NaN if n_samples < 2.\n",
    "    - Can also return NaN if the baseline deviance is 0 (e.g., constant targets).\n",
    "    \"\"\"\n",
    "\n",
    "    y_true, y_pred, w = _check_1d_targets(y_true, y_pred, sample_weight)\n",
    "    if y_true.shape[0] < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    numerator = mean_tweedie_deviance_numpy(y_true, y_pred, power=power, sample_weight=w)\n",
    "\n",
    "    y_avg = float(np.average(y_true, weights=w)) if w is not None else float(np.mean(y_true))\n",
    "    denominator = mean_tweedie_deviance_numpy(\n",
    "        y_true,\n",
    "        np.full_like(y_true, y_avg),\n",
    "        power=power,\n",
    "        sample_weight=w,\n",
    "    )\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        return float(1.0 - numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks vs scikit-learn (should match exactly)\n",
    "\n",
    "for p in [0, 1, 1.5, 2, 3, -0.5]:\n",
    "    if p == 0:\n",
    "        y_true = rng.normal(size=200)\n",
    "        y_pred = y_true + rng.normal(scale=0.3, size=200)\n",
    "    elif p < 0:\n",
    "        y_true = rng.normal(size=200)\n",
    "        y_pred = np.exp(rng.normal(size=200))\n",
    "    elif 1 <= p < 2:\n",
    "        y_true = rng.poisson(3.0, size=200).astype(float)\n",
    "        y_pred = np.exp(rng.normal(size=200))\n",
    "    else:\n",
    "        y_true = rng.gamma(shape=2.0, scale=2.0, size=200)\n",
    "        y_pred = np.exp(rng.normal(size=200))\n",
    "\n",
    "    w = rng.uniform(0.2, 2.0, size=200)\n",
    "\n",
    "    sk_loss = sk_mean_tweedie_deviance(y_true, y_pred, power=p, sample_weight=w)\n",
    "    np_loss = mean_tweedie_deviance_numpy(y_true, y_pred, power=p, sample_weight=w)\n",
    "\n",
    "    sk_d2 = sk_d2_tweedie_score(y_true, y_pred, power=p, sample_weight=w)\n",
    "    np_d2 = d2_tweedie_score_numpy(y_true, y_pred, power=p, sample_weight=w)\n",
    "\n",
    "    print(f\"power={p:>4}: mean dev diff={abs(sk_loss-np_loss):.3e} | D² diff={abs(sk_d2-np_d2):.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b7c9e",
   "metadata": {},
   "source": [
    "## 3) Intuition: how $p$ changes the penalty\n",
    "\n",
    "Two important intuitions:\n",
    "\n",
    "1. **Asymmetry** for $p\\ge 1$: underpredicting towards $\\mu\\to 0^+$ can be extremely costly when $y>0$.\n",
    "2. **Mean-dependent scaling**: the derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d_p(y,\\mu)}{\\partial \\mu} = 2\\mu^{-p}(\\mu-y)\n",
    "$$\n",
    "\n",
    "shows that (for $p>0$) errors at larger $\\mu$ are downweighted by $\\mu^{-p}$. This is the idea behind using Tweedie losses when variance grows with the mean.\n",
    "\n",
    "Below are deviance curves (lower is better). Notice how the shape changes with $p$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit deviance curves for different powers\n",
    "\n",
    "mu_pos = np.linspace(1e-3, 15, 500)\n",
    "y_fixed = 5.0\n",
    "powers_main = [0, 1, 1.5, 2, 3]\n",
    "\n",
    "mu_zero = np.linspace(1e-6, 10, 500)\n",
    "y_zero = 0.0\n",
    "powers_zero = [0, 1, 1.5]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"y = 5 (compare powers)\", \"y = 0 (compare how predicting >0 is penalized)\"),\n",
    ")\n",
    "\n",
    "for p in powers_main:\n",
    "    dev = tweedie_unit_deviance_numpy(np.full_like(mu_pos, y_fixed), mu_pos, power=p)\n",
    "    fig.add_trace(go.Scatter(x=mu_pos, y=dev, mode=\"lines\", name=f\"p={p}\"), row=1, col=1)\n",
    "\n",
    "for p in powers_zero:\n",
    "    dev = tweedie_unit_deviance_numpy(np.full_like(mu_zero, y_zero), mu_zero, power=p)\n",
    "    fig.add_trace(go.Scatter(x=mu_zero, y=dev, mode=\"lines\", name=f\"p={p}\"), row=1, col=2)\n",
    "\n",
    "fig.add_vline(x=y_fixed, line_dash=\"dash\", line_color=\"gray\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Predicted mean μ\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Unit deviance d_p(y,μ)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Predicted mean μ\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Unit deviance d_p(y,μ)\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Tweedie unit deviance curves\",\n",
    "    width=1100,\n",
    "    height=450,\n",
    "    legend_title_text=\"power\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D² as a function of multiplicative mis-calibration of the predicted mean\n",
    "\n",
    "n = 600\n",
    "x = rng.normal(size=n)\n",
    "X = np.c_[np.ones(n), x]\n",
    "\n",
    "beta_true = np.array([0.2, 0.7])\n",
    "mu_true = np.exp(X @ beta_true)\n",
    "y = rng.poisson(mu_true)\n",
    "\n",
    "scales = np.linspace(0.2, 3.0, 90)\n",
    "powers = [0, 1, 1.5]\n",
    "\n",
    "fig = go.Figure()\n",
    "for p in powers:\n",
    "    d2_vals = [d2_tweedie_score_numpy(y, mu_true * s, power=p) for s in scales]\n",
    "    fig.add_trace(go.Scatter(x=scales, y=d2_vals, mode=\"lines\", name=f\"power={p}\"))\n",
    "\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"D² vs multiplicative mis-calibration of μ (same y_true, same mu_true)\",\n",
    "    xaxis_title=\"Scale factor s in μ_pred = s · μ_true\",\n",
    "    yaxis_title=\"D² (higher is better)\",\n",
    "    width=950,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d0f2b",
   "metadata": {},
   "source": [
    "## 4) Using $D^2$ while optimizing a model (from scratch)\n",
    "\n",
    "In\n",
    "\n",
    "$$\n",
    "D^2_p = 1 - \\frac{\\bar{d}_p(y,\\hat{\\mu})}{\\bar{d}_p(y,\\hat{\\mu}^{(0)})},\n",
    "$$\n",
    "\n",
    "the denominator depends only on $y$ (and weights), so during training it is a constant.\n",
    "\n",
    "Maximizing $D^2_p$ is therefore equivalent to minimizing the mean Tweedie deviance $\\bar{d}_p(y,\\hat{\\mu})$.\n",
    "\n",
    "We'll fit a simple **Poisson regression** (Tweedie with $p=1$) using a log link:\n",
    "\n",
    "$$\n",
    "\\eta = X\\beta,\\qquad \\mu = \\exp(\\eta)\n",
    "$$\n",
    "\n",
    "Using the derivative from above and the chain rule, for general $p$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\eta_i} d_p(y_i,\\mu_i)\n",
    "= \\frac{\\partial d_p}{\\partial \\mu_i}\\frac{\\partial \\mu_i}{\\partial \\eta_i}\n",
    "= 2\\left(\\mu_i^{2-p} - y_i\\mu_i^{1-p}\\right)\n",
    "$$\n",
    "\n",
    "For Poisson ($p=1$): $\\frac{\\partial}{\\partial \\eta_i} d_1 = 2(\\mu_i - y_i)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Poisson GLM with gradient descent and track both deviance and D²\n",
    "\n",
    "n = 400\n",
    "x = rng.uniform(-2.0, 2.0, size=n)\n",
    "X = np.c_[np.ones(n), x]\n",
    "\n",
    "beta_true = np.array([0.3, 0.8])\n",
    "mu_true = np.exp(X @ beta_true)\n",
    "y = rng.poisson(mu_true)\n",
    "\n",
    "\n",
    "def fit_tweedie_glm_gd(X, y, *, power=1.0, lr=0.05, n_iter=3000, record_every=10):\n",
    "    \"\"\"Gradient descent for a Tweedie GLM with log link: μ = exp(Xβ).\"\"\"\n",
    "\n",
    "    p = float(power)\n",
    "    n_samples, n_features = X.shape\n",
    "    beta = np.zeros(n_features, dtype=float)\n",
    "\n",
    "    hist = {\"iter\": [], \"loss\": [], \"d2\": []}\n",
    "    for t in range(n_iter + 1):\n",
    "        eta = X @ beta\n",
    "        mu = np.exp(np.clip(eta, -20, 20))\n",
    "\n",
    "        if t % record_every == 0:\n",
    "            hist[\"iter\"].append(t)\n",
    "            hist[\"loss\"].append(mean_tweedie_deviance_numpy(y, mu, power=p))\n",
    "            hist[\"d2\"].append(d2_tweedie_score_numpy(y, mu, power=p))\n",
    "\n",
    "        # ∂/∂η d_p(y, μ) = 2(μ^{2-p} - y μ^{1-p}) for μ = exp(η)\n",
    "        g_eta = 2.0 * (mu ** (2.0 - p) - y * (mu ** (1.0 - p)))\n",
    "        grad = (X.T @ g_eta) / n_samples\n",
    "        beta -= lr * grad\n",
    "\n",
    "    return beta, hist\n",
    "\n",
    "\n",
    "beta_hat, hist = fit_tweedie_glm_gd(X, y, power=1.0, lr=0.05, n_iter=3000, record_every=10)\n",
    "beta_hat\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Mean Poisson deviance\", \"D² (power=1)\"))\n",
    "fig.add_trace(go.Scatter(x=hist[\"iter\"], y=hist[\"loss\"], mode=\"lines\", name=\"loss\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=hist[\"iter\"], y=hist[\"d2\"], mode=\"lines\", name=\"D²\"), row=1, col=2)\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Iteration\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Mean deviance\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Iteration\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"D²\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=1050, height=420, title=\"Training diagnostics\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fitted mean function μ(x)\n",
    "\n",
    "mu_hat = np.exp(np.clip(X @ beta_hat, -20, 20))\n",
    "\n",
    "order = np.argsort(x)\n",
    "x_s = x[order]\n",
    "y_s = y[order]\n",
    "mu_true_s = mu_true[order]\n",
    "mu_hat_s = mu_hat[order]\n",
    "\n",
    "mu_baseline = np.full_like(mu_hat_s, y.mean())\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_s, y=y_s, mode=\"markers\", name=\"y (counts)\", opacity=0.35))\n",
    "fig.add_trace(go.Scatter(x=x_s, y=mu_true_s, mode=\"lines\", name=\"true μ(x)\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=x_s, y=mu_hat_s, mode=\"lines\", name=\"fitted μ(x)\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=x_s, y=mu_baseline, mode=\"lines\", name=\"mean baseline\", line=dict(dash=\"dash\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Poisson regression fit (log-link): fitted mean vs true mean\",\n",
    "    xaxis_title=\"feature x\",\n",
    "    yaxis_title=\"count / mean\",\n",
    "    width=950,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a3d5f",
   "metadata": {},
   "source": [
    "## 5) Practical usage in scikit-learn\n",
    "\n",
    "The score itself is just:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import d2_tweedie_score\n",
    "d2 = d2_tweedie_score(y_true, y_pred, power=p)\n",
    "```\n",
    "\n",
    "For modeling, scikit-learn provides GLM-style estimators such as `PoissonRegressor`, `GammaRegressor`, and the general `TweedieRegressor`.\n",
    "\n",
    "Below is a minimal Poisson example (power=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_feat = x.reshape(-1, 1)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_feat, y, test_size=0.3, random_state=0)\n",
    "\n",
    "model = PoissonRegressor(alpha=0.0, max_iter=1000)\n",
    "model.fit(X_tr, y_tr)\n",
    "mu_te = model.predict(X_te)\n",
    "\n",
    "print(\"sklearn PoissonRegressor D² (power=1):\", sk_d2_tweedie_score(y_te, mu_te, power=1))\n",
    "\n",
    "# Compare to our gradient descent fit on the same split\n",
    "X_tr_i = np.c_[np.ones(X_tr.shape[0]), X_tr[:, 0]]\n",
    "X_te_i = np.c_[np.ones(X_te.shape[0]), X_te[:, 0]]\n",
    "beta_hat_tr, _ = fit_tweedie_glm_gd(X_tr_i, y_tr, power=1.0, lr=0.05, n_iter=3000, record_every=50)\n",
    "mu_te_gd = np.exp(np.clip(X_te_i @ beta_hat_tr, -20, 20))\n",
    "\n",
    "print(\"NumPy GD D² (power=1):\", d2_tweedie_score_numpy(y_te, mu_te_gd, power=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d5f7b",
   "metadata": {},
   "source": [
    "## 6) Pros, cons, and when to use\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- $D^2$ is interpretable like $R^2$: 1 is best, 0 is “mean baseline”, negative means worse than baseline.\n",
    "- It is **distribution-aware** via deviance, making it a natural score for GLMs.\n",
    "- Works well for heteroscedastic targets where variance grows with the mean (counts, positive skew, etc.).\n",
    "- Supports sample weights.\n",
    "\n",
    "**Cons / pitfalls**\n",
    "\n",
    "- You must choose a power $p$; the value (and model ranking) can change with $p$.\n",
    "- Domain restrictions: for most $p\\ge 1$ you need $y\\ge 0$ and **strictly positive** predictions $\\hat{\\mu}>0$.\n",
    "- Not defined for $n<2$ samples; can be NaN when the baseline deviance is 0 (e.g., constant targets).\n",
    "- It is a *relative* score, not an absolute error, and is not symmetric in its arguments.\n",
    "\n",
    "**Good use cases**\n",
    "\n",
    "- $p=1$ (Poisson): counts / event rates.\n",
    "- $1<p<2$ (compound Poisson–Gamma): data with many zeros plus positive continuous outcomes (common in insurance claims).\n",
    "- $p=2$ (Gamma) or $p=3$ (Inverse Gaussian): strictly positive, right-skewed continuous targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e6a8c",
   "metadata": {},
   "source": [
    "## 7) Exercises\n",
    "\n",
    "1. For a fixed $y>0$, plot $d_p(y,\\mu)$ for several powers and compare the under- vs over-prediction penalty.\n",
    "2. Construct a dataset with constant targets and see what happens to $D^2$ (baseline deviance $=0$).\n",
    "3. Add sample weights and verify that the baseline prediction becomes the **weighted** mean.\n",
    "4. Simulate data from a Poisson model and compare $D^2$ for powers $p=0$ vs $p=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f7b9d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_tweedie_score.html\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_tweedie_deviance.html\n",
    "- Hastie, Tibshirani, Wainwright (2015), *Statistical Learning with Sparsity*, Eq. (3.11)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}