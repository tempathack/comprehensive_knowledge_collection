{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbcd2a7",
   "metadata": {},
   "source": [
    "# `mean_poisson_deviance`\n",
    "\n",
    "`mean_poisson_deviance` is a regression loss for **non-negative targets** (typically counts). It is the (weighted) average of the **Poisson deviance**:\n",
    "\n",
    "- best value: `0.0` (when `y_pred == y_true`)\n",
    "- lower is better\n",
    "\n",
    "**Goals**\n",
    "\n",
    "- define the metric and its domain\n",
    "- derive the formula from the Poisson likelihood\n",
    "- implement it from scratch in NumPy (and validate against scikit-learn)\n",
    "- build intuition with plots\n",
    "- use it as an optimization objective for Poisson regression (GLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import mean_poisson_deviance\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79cacc",
   "metadata": {},
   "source": [
    "## When should you use it?\n",
    "\n",
    "Use Poisson deviance when:\n",
    "\n",
    "- `y_true` represents **counts** ($0,1,2,\\dots$) or other non-negative quantities\n",
    "- the conditional variance grows with the mean (a Poisson-like setting)\n",
    "- you want a loss that matches the **Poisson likelihood** (GLM / Poisson regression)\n",
    "\n",
    "**Domain constraints**\n",
    "\n",
    "For every sample:\n",
    "\n",
    "- $y_i \\ge 0$\n",
    "- $\\mu_i > 0$ (predicted mean)\n",
    "\n",
    "A common way to ensure $\\mu_i>0$ is a **log link**:\n",
    "\n",
    "$$\n",
    "\\eta_i = \\beta_0 + x_i^\\top\\beta,\\qquad \\mu_i = \\exp(\\eta_i) > 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b63f5",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "For a single observation $y \\ge 0$ and a prediction $\\mu > 0$, the **Poisson unit deviance** is:\n",
    "\n",
    "$$\n",
    "d(y, \\mu)\n",
    "= 2 \\left[ y \\log\\left(\\frac{y}{\\mu}\\right) - (y - \\mu) \\right],\n",
    "\\qquad \\text{with the convention } 0\\log(0/\\mu)=0.\n",
    "$$\n",
    "\n",
    "The **mean Poisson deviance** is the average over samples:\n",
    "\n",
    "$$\n",
    "\\mathrm{MPD}(y, \\mu)\n",
    "= \\frac{1}{n}\\sum_{i=1}^n d(y_i, \\mu_i)\n",
    "\\quad (\\text{or a weighted average}).\n",
    "$$\n",
    "\n",
    "Useful special case:\n",
    "\n",
    "$$\n",
    "d(0, \\mu)=2\\mu.\n",
    "$$\n",
    "\n",
    "For $y>0$, writing $r = \\mu/y$ (multiplicative error):\n",
    "\n",
    "$$\n",
    "d(y,\\mu) = 2y\\,[ -\\log r - 1 + r ].\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "\n",
    "- $d(y,\\mu) \\ge 0$\n",
    "- $d(y,\\mu)=0$ iff $\\mu=y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bbf59",
   "metadata": {},
   "source": [
    "## Where does this formula come from?\n",
    "\n",
    "For a Poisson model:\n",
    "\n",
    "$$\n",
    "y \\mid \\mu \\sim \\mathrm{Poisson}(\\mu), \\qquad \\mu>0,\n",
    "$$\n",
    "\n",
    "the log-likelihood for one observation is:\n",
    "\n",
    "$$\n",
    "\\log p(y\\mid \\mu) = y\\log\\mu - \\mu - \\log(y!).\n",
    "$$\n",
    "\n",
    "The **deviance** is a log-likelihood ratio between the fitted model and a *saturated* model that predicts perfectly ($\\mu=y$):\n",
    "\n",
    "$$\n",
    "d(y,\\mu)\n",
    "= 2\\Big(\\log p(y\\mid y) - \\log p(y\\mid \\mu)\\Big)\n",
    "= 2\\left[y\\log\\left(\\frac{y}{\\mu}\\right) - (y-\\mu)\\right].\n",
    "$$\n",
    "\n",
    "So minimizing mean Poisson deviance is equivalent to maximizing the Poisson likelihood (the saturated term depends only on $y$).\n",
    "\n",
    "**KL view**\n",
    "\n",
    "The KL divergence between Poisson distributions satisfies:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}\\big(\\mathrm{Poisson}(y)\\ \\|\\ \\mathrm{Poisson}(\\mu)\\big)\n",
    "= y\\log\\left(\\frac{y}{\\mu}\\right) + \\mu - y,\n",
    "$$\n",
    "\n",
    "so:\n",
    "\n",
    "$$\n",
    "d(y,\\mu) = 2\\,\\mathrm{KL}\\big(\\mathrm{Poisson}(y)\\ \\|\\ \\mathrm{Poisson}(\\mu)\\big).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_1d_float(a, name: str) -> np.ndarray:\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    a = a.reshape(-1)\n",
    "    if not np.all(np.isfinite(a)):\n",
    "        raise ValueError(f\"{name} must contain only finite values\")\n",
    "    return a\n",
    "\n",
    "\n",
    "def poisson_deviance_per_sample(y_true, y_pred) -> np.ndarray:\n",
    "    # Per-sample Poisson deviance d(y, μ)\n",
    "    y_true = _to_1d_float(y_true, \"y_true\")\n",
    "    y_pred = _to_1d_float(y_pred, \"y_pred\")\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape\")\n",
    "    if np.any(y_true < 0):\n",
    "        raise ValueError(\"y_true must be non-negative\")\n",
    "    if np.any(y_pred <= 0):\n",
    "        raise ValueError(\"y_pred must be strictly positive\")\n",
    "\n",
    "    dev = np.empty_like(y_true, dtype=float)\n",
    "\n",
    "    mask = y_true > 0\n",
    "    dev[~mask] = 2.0 * y_pred[~mask]  # y=0 => d(0, μ) = 2μ\n",
    "\n",
    "    dev[mask] = 2.0 * (\n",
    "        y_true[mask] * np.log(y_true[mask] / y_pred[mask])\n",
    "        - (y_true[mask] - y_pred[mask])\n",
    "    )\n",
    "\n",
    "    return dev\n",
    "\n",
    "\n",
    "def mean_poisson_deviance_np(y_true, y_pred, sample_weight=None) -> float:\n",
    "    dev = poisson_deviance_per_sample(y_true, y_pred)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return float(dev.mean())\n",
    "\n",
    "    w = _to_1d_float(sample_weight, \"sample_weight\")\n",
    "    if w.shape != dev.shape:\n",
    "        raise ValueError(\"sample_weight must have the same shape as y_true\")\n",
    "    if np.any(w < 0):\n",
    "        raise ValueError(\"sample_weight must be non-negative\")\n",
    "\n",
    "    w_sum = float(w.sum())\n",
    "    if w_sum == 0.0:\n",
    "        raise ValueError(\"sample_weight sum must be positive\")\n",
    "\n",
    "    return float(np.sum(w * dev) / w_sum)\n",
    "\n",
    "\n",
    "def mean_poisson_deviance_grad_mu(y_true, y_pred, sample_weight=None) -> np.ndarray:\n",
    "    # Gradient of the mean deviance wrt μ (elementwise), shape (n,)\n",
    "    y_true = _to_1d_float(y_true, \"y_true\")\n",
    "    y_pred = _to_1d_float(y_pred, \"y_pred\")\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape\")\n",
    "    if np.any(y_true < 0):\n",
    "        raise ValueError(\"y_true must be non-negative\")\n",
    "    if np.any(y_pred <= 0):\n",
    "        raise ValueError(\"y_pred must be strictly positive\")\n",
    "\n",
    "    # d(y, μ) = 2 [ y log(y/μ) - (y - μ) ]\n",
    "    # ∂/∂μ d(y, μ) = 2 (1 - y/μ)\n",
    "    grad = 2.0 * (1.0 - y_true / y_pred)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return grad / y_true.size\n",
    "\n",
    "    w = _to_1d_float(sample_weight, \"sample_weight\")\n",
    "    if w.shape != grad.shape:\n",
    "        raise ValueError(\"sample_weight must have the same shape as y_true\")\n",
    "\n",
    "    w_sum = float(w.sum())\n",
    "    if w_sum == 0.0:\n",
    "        raise ValueError(\"sample_weight sum must be positive\")\n",
    "\n",
    "    return (w * grad) / w_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against scikit-learn\n",
    "\n",
    "y_true = rng.poisson(lam=3.0, size=200)\n",
    "y_pred = rng.gamma(shape=2.0, scale=2.0, size=200)  # positive\n",
    "\n",
    "print(\"sklearn:\", mean_poisson_deviance(y_true, y_pred))\n",
    "print(\"numpy  :\", mean_poisson_deviance_np(y_true, y_pred))\n",
    "\n",
    "w = rng.uniform(0.0, 1.0, size=y_true.size)\n",
    "print(\"sklearn (w):\", mean_poisson_deviance(y_true, y_pred, sample_weight=w))\n",
    "print(\"numpy   (w):\", mean_poisson_deviance_np(y_true, y_pred, sample_weight=w))\n",
    "\n",
    "# Tiny gradient sanity check (finite differences)\n",
    "y_true_s = np.array([0.0, 1.0, 4.0])\n",
    "y_pred_s = np.array([0.7, 1.2, 3.5])\n",
    "\n",
    "g_analytical = mean_poisson_deviance_grad_mu(y_true_s, y_pred_s)\n",
    "\n",
    "eps = 1e-6\n",
    "base = mean_poisson_deviance_np(y_true_s, y_pred_s)\n",
    "g_numeric = np.zeros_like(y_pred_s)\n",
    "for i in range(y_pred_s.size):\n",
    "    y_pred_eps = y_pred_s.copy()\n",
    "    y_pred_eps[i] += eps\n",
    "    g_numeric[i] = (mean_poisson_deviance_np(y_true_s, y_pred_eps) - base) / eps\n",
    "\n",
    "print(\"grad analytical:\", g_analytical)\n",
    "print(\"grad numeric   :\", g_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7b3a2",
   "metadata": {},
   "source": [
    "## Intuition: how does the penalty behave?\n",
    "\n",
    "Key behaviors to look for in the plots below:\n",
    "\n",
    "- The minimum is at $\\mu=y$.\n",
    "- For $y>0$, as $\\mu\\to 0^+$, the loss diverges ($\\log(y/\\mu)\\to\\infty$).\n",
    "- For $y=0$, the deviance becomes linear: $d(0,\\mu)=2\\mu$.\n",
    "- In terms of multiplicative error $r=\\mu/y$ (for $y>0$), the *shape* is independent of $y$:\n",
    "\n",
    "$$\n",
    "\\frac{d(y,\\mu)}{y} = 2\\,[ -\\log r - 1 + r ].\n",
    "$$\n",
    "\n",
    "So larger counts scale the deviance roughly linearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da735506",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.logspace(-3, np.log10(50), 600)\n",
    "ys = [0, 1, 5, 20]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Unit deviance d(y, μ) as a function of μ\",\n",
    "        \"Shape vs multiplicative error r = μ / y (for y>0)\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Left: d(y, μ) vs μ for several y\n",
    "for y in ys:\n",
    "    y_vec = np.full_like(mu, float(y))\n",
    "    d = poisson_deviance_per_sample(y_vec, mu)\n",
    "    fig.add_trace(go.Scatter(x=mu, y=d, mode=\"lines\", name=f\"y={y}\"), row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"μ (prediction)\", type=\"log\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"d(y, μ)\", row=1, col=1)\n",
    "\n",
    "# Right: normalized deviance shape as a function of r\n",
    "r = np.logspace(-2, 2, 600)\n",
    "d_over_y = 2.0 * (-np.log(r) - 1.0 + r)  # since d(y, y r) / y\n",
    "fig.add_trace(go.Scatter(x=r, y=d_over_y, mode=\"lines\", name=\"d/y (any y>0)\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"r\", type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"d(y, y·r) / y\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Mean Poisson deviance: shape and scaling\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ef807",
   "metadata": {},
   "source": [
    "## Using it as a loss: Poisson regression (log link)\n",
    "\n",
    "Given features $X\\in\\mathbb{R}^{n\\times p}$ and counts $y\\in\\{0,1,2,\\dots\\}$, Poisson regression models:\n",
    "\n",
    "$$\n",
    "\\eta = X\\beta,\\qquad \\mu = \\exp(\\eta).\n",
    "$$\n",
    "\n",
    "We can fit $\\beta$ by minimizing mean Poisson deviance:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\frac{1}{n} \\sum_{i=1}^n d(y_i, \\mu_i).\n",
    "$$\n",
    "\n",
    "Because $y_i\\log y_i$ does not depend on $\\beta$, the gradient is simple. For the **unweighted mean**:\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta L = \\frac{2}{n} X^\\top(\\mu - y).\n",
    "$$\n",
    "\n",
    "(With weights $w_i$, replace $\\frac{2}{n}$ with $\\frac{2}{\\sum_i w_i}$ and multiply $(\\mu-y)$ elementwise by $w$.)\n",
    "\n",
    "Below is a from-scratch batch gradient descent solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def poisson_mean_from_eta(eta: np.ndarray, eta_clip: float = 20.0) -> np.ndarray:\n",
    "    # Clip to avoid overflow/underflow in exp in from-scratch code\n",
    "    eta = np.clip(eta, -eta_clip, eta_clip)\n",
    "    return np.exp(eta)\n",
    "\n",
    "\n",
    "def poisson_regression_loss_and_grad_beta(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    beta: np.ndarray,\n",
    "    sample_weight=None,\n",
    "    l2: float = 0.0,\n",
    "    eta_clip: float = 20.0,\n",
    "):\n",
    "    y = _to_1d_float(y, \"y\")\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    beta = np.asarray(beta, dtype=float).reshape(-1)\n",
    "\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"X and y have incompatible shapes\")\n",
    "    if X.shape[1] != beta.shape[0]:\n",
    "        raise ValueError(\"beta has incompatible shape\")\n",
    "    if np.any(y < 0):\n",
    "        raise ValueError(\"y must be non-negative\")\n",
    "\n",
    "    eta = X @ beta\n",
    "    mu = poisson_mean_from_eta(eta, eta_clip=eta_clip)\n",
    "\n",
    "    loss = mean_poisson_deviance_np(y, mu, sample_weight=sample_weight)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        grad = (2.0 / X.shape[0]) * (X.T @ (mu - y))\n",
    "    else:\n",
    "        w = _to_1d_float(sample_weight, \"sample_weight\")\n",
    "        if w.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"sample_weight must have shape (n,)\")\n",
    "        W = float(w.sum())\n",
    "        if W == 0.0:\n",
    "            raise ValueError(\"sample_weight sum must be positive\")\n",
    "        grad = (2.0 / W) * (X.T @ (w * (mu - y)))\n",
    "\n",
    "    if l2 > 0.0:\n",
    "        # Ridge penalty on slopes (not the intercept)\n",
    "        loss = loss + l2 * float(np.sum(beta[1:] ** 2))\n",
    "        grad = grad.copy()\n",
    "        grad[1:] = grad[1:] + 2.0 * l2 * beta[1:]\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "def fit_poisson_regression_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    sample_weight=None,\n",
    "    l2: float = 0.0,\n",
    "    lr: float = 0.05,\n",
    "    max_iter: int = 4000,\n",
    "    tol: float = 1e-10,\n",
    "    eta_clip: float = 20.0,\n",
    "):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = _to_1d_float(y, \"y\")\n",
    "\n",
    "    beta = np.zeros(X.shape[1], dtype=float)\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        loss, grad = poisson_regression_loss_and_grad_beta(\n",
    "            X,\n",
    "            y,\n",
    "            beta,\n",
    "            sample_weight=sample_weight,\n",
    "            l2=l2,\n",
    "            eta_clip=eta_clip,\n",
    "        )\n",
    "        losses.append(loss)\n",
    "\n",
    "        beta_next = beta - lr * grad\n",
    "\n",
    "        if np.linalg.norm(beta_next - beta) <= tol * (1.0 + np.linalg.norm(beta)):\n",
    "            beta = beta_next\n",
    "            break\n",
    "\n",
    "        beta = beta_next\n",
    "\n",
    "    return beta, np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Poisson regression problem\n",
    "\n",
    "n = 800\n",
    "x = rng.uniform(-2.5, 2.5, size=n)\n",
    "X_raw = x.reshape(-1, 1)\n",
    "\n",
    "# Standardize feature for easier GD tuning\n",
    "X_mean = X_raw.mean(axis=0)\n",
    "X_std = X_raw.std(axis=0)\n",
    "X = (X_raw - X_mean) / X_std\n",
    "\n",
    "X_i = add_intercept(X)\n",
    "\n",
    "beta_true = np.array([0.3, 0.7])\n",
    "eta_true = X_i @ beta_true\n",
    "mu_true = np.exp(eta_true)\n",
    "\n",
    "y = rng.poisson(lam=mu_true)\n",
    "\n",
    "# Train/test split\n",
    "idx = rng.permutation(n)\n",
    "tr = idx[: int(0.8 * n)]\n",
    "te = idx[int(0.8 * n) :]\n",
    "\n",
    "X_tr, y_tr = X_i[tr], y[tr]\n",
    "X_te, y_te = X_i[te], y[te]\n",
    "\n",
    "# Baseline: constant mean\n",
    "mu_baseline = np.full_like(y_te, y_tr.mean(), dtype=float)\n",
    "baseline_dev = mean_poisson_deviance_np(y_te, mu_baseline)\n",
    "\n",
    "# Fit from scratch\n",
    "beta_hat, losses = fit_poisson_regression_gd(X_tr, y_tr, lr=0.05, max_iter=4000)\n",
    "mu_hat_te = poisson_mean_from_eta(X_te @ beta_hat)\n",
    "\n",
    "dev_te = mean_poisson_deviance_np(y_te, mu_hat_te)\n",
    "\n",
    "print(\"true beta:\", beta_true)\n",
    "print(\"  hat beta:\", beta_hat)\n",
    "print(f\"baseline MPD (constant mean): {baseline_dev:.4f}\")\n",
    "print(f\"test MPD (from-scratch poisson reg): {dev_te:.4f}\")\n",
    "\n",
    "# Compare with scikit-learn's PoissonRegressor (same feature matrix incl. intercept)\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "sk = PoissonRegressor(alpha=0.0, fit_intercept=False, max_iter=1000)\n",
    "sk.fit(X_tr, y_tr)\n",
    "mu_sk_te = sk.predict(X_te)\n",
    "\n",
    "print(\"sklearn beta:\", sk.coef_)\n",
    "print(f\"test MPD (sklearn): {mean_poisson_deviance(y_te, mu_sk_te):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization + fit\n",
    "\n",
    "iters = np.arange(len(losses))\n",
    "\n",
    "x_te = x[te]\n",
    "mu_true_te = mu_true[te]\n",
    "\n",
    "order = np.argsort(x_te)\n",
    "\n",
    "dev_baseline_per = poisson_deviance_per_sample(y_te, mu_baseline)\n",
    "dev_model_per = poisson_deviance_per_sample(y_te, mu_hat_te)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Training loss (mean Poisson deviance)\",\n",
    "        \"Test: observed y vs predicted mean μ̂\",\n",
    "        \"Test: y vs x with mean functions\",\n",
    "        \"Test: per-sample deviance distribution\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# (1) Loss curve\n",
    "fig.add_trace(go.Scatter(x=iters, y=losses, mode=\"lines\", name=\"train loss\"), row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"iteration\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"mean deviance\", type=\"log\", row=1, col=1)\n",
    "\n",
    "# (2) Predicted mean vs observed\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=mu_hat_te, y=y_te, mode=\"markers\", name=\"(μ̂, y)\", opacity=0.6),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "max_axis = float(max(mu_hat_te.max(), y_te.max()))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, max_axis],\n",
    "        y=[0, max_axis],\n",
    "        mode=\"lines\",\n",
    "        name=\"y=μ\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.update_xaxes(title_text=\"predicted mean μ̂\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"observed y\", row=1, col=2)\n",
    "\n",
    "# (3) Mean function vs x\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_te, y=y_te, mode=\"markers\", name=\"y (test)\", opacity=0.35),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_te[order], y=mu_true_te[order], mode=\"lines\", name=\"μ true\"),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_te[order], y=mu_hat_te[order], mode=\"lines\", name=\"μ̂ fit\"),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.update_xaxes(title_text=\"x\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"count / mean\", row=2, col=1)\n",
    "\n",
    "# (4) Per-sample deviance distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=dev_baseline_per, name=\"baseline\", opacity=0.55, nbinsx=30),\n",
    "    row=2,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=dev_model_per, name=\"poisson reg\", opacity=0.55, nbinsx=30),\n",
    "    row=2,\n",
    "    col=2,\n",
    ")\n",
    "fig.update_xaxes(title_text=\"d(y, μ)\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"count\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Fitting Poisson regression by minimizing mean Poisson deviance\",\n",
    "    barmode=\"overlay\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.18),\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506e2ac",
   "metadata": {},
   "source": [
    "## Pros, cons, and common pitfalls\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- **Proper loss for counts**: corresponds to a Poisson likelihood / GLM.\n",
    "- **Respects positivity**: large penalties for predicting near-zero when events occur.\n",
    "- **Multiplicative intuition**: depends on the ratio $r=\\mu/y$ (for $y>0$), not just additive error.\n",
    "- **Convenient for optimization**: with a log link, the gradient is simple: $\\nabla_\\beta L \\propto X^\\top(\\mu-y)$.\n",
    "\n",
    "**Cons / pitfalls**\n",
    "\n",
    "- Requires **$y\\ge 0$** and **$\\mu>0$**.\n",
    "- Can be dominated by **large counts / outliers** (since deviance scales with $y$).\n",
    "- Assumes a Poisson-like mean–variance relationship ($\\mathrm{Var}(y\\mid x)\\approx\\mu$). For **over-dispersion**, consider Negative Binomial or Tweedie models.\n",
    "\n",
    "**Practical tips**\n",
    "\n",
    "- Enforce $\\mu>0$ with a log link (`μ = exp(η)`).\n",
    "- For **rates** with exposures $E_i$ (time at risk, population, etc.), use an offset:\n",
    "\n",
    "$$\n",
    "\\mu_i = E_i\\,\\exp(x_i^\\top\\beta)\n",
    "\\quad\\Leftrightarrow\\quad\n",
    "\\log\\mu_i = \\log E_i + x_i^\\top\\beta.\n",
    "$$\n",
    "\n",
    "- In from-scratch code, clip $\\eta$ before `exp` to avoid overflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe59f1",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Add an **exposure offset** and simulate data where events are proportional to exposure.\n",
    "2. Implement **mini-batch** gradient descent and compare convergence.\n",
    "3. Create an **over-dispersed** dataset (e.g., Gamma–Poisson / Negative Binomial) and compare Poisson deviance against another deviance (e.g., Tweedie).\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn: `sklearn.metrics.mean_poisson_deviance`\n",
    "- scikit-learn: `sklearn.linear_model.PoissonRegressor`\n",
    "- McCullagh & Nelder, *Generalized Linear Models*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}