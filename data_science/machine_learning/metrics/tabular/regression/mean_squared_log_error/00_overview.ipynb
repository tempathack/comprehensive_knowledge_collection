{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Logarithmic Error (MSLE)\n",
    "\n",
    "Mean Squared Logarithmic Error (MSLE) is a **regression** metric for **non-negative** targets.\n",
    "It computes mean squared error **after a log transform** (`log(1 + y)`), so it mainly measures **relative / multiplicative** differences.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Define MSLE precisely (domain, weights, multioutput)\n",
    "- Build intuition for what MSLE does (and does *not*) penalize\n",
    "- Implement MSLE from scratch in NumPy\n",
    "- Visualize MSLE behavior across scales with Plotly\n",
    "- Use MSLE as a training objective via log-space linear regression\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NumPy basics (vectorization, shapes)\n",
    "- Comfort with derivatives for least squares (or gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Definition\n",
    "\n",
    "Let $y \\in \\mathbb{R}^n$ be targets and $\\hat{y} \\in \\mathbb{R}^n$ predictions.\n",
    "MSLE is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSLE}(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^n\\Big(\\log(1+y_i) - \\log(1+\\hat{y}_i)\\Big)^2\n",
    "$$\n",
    "\n",
    "Equivalently, with the shorthand $\\log1p(t)=\\log(1+t)$:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSLE}(y, \\hat{y}) = \\mathrm{MSE}(\\log1p(y),\\; \\log1p(\\hat{y}))\n",
    "$$\n",
    "\n",
    "### Domain constraints\n",
    "\n",
    "Because we evaluate $\\log(1+y)$, we must have:\n",
    "\n",
    "$$\n",
    "y_i \\ge 0 \\quad \\text{and} \\quad \\hat{y}_i \\ge 0\n",
    "$$\n",
    "\n",
    "(scikit-learn enforces non-negativity).\n",
    "\n",
    "### Weighted MSLE\n",
    "\n",
    "With non-negative sample weights $w_i$:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSLE}_w(y,\\hat{y}) = \\frac{\\sum_{i=1}^n w_i\\,\\big(\\log1p(y_i)-\\log1p(\\hat{y}_i)\\big)^2}{\\sum_{i=1}^n w_i}\n",
    "$$\n",
    "\n",
    "### Multioutput\n",
    "\n",
    "For $y,\\hat{y}\\in\\mathbb{R}^{n\\times d}$ you compute MSLE per output and then aggregate (uniform average or a weighted average).\n",
    "\n",
    "### Units / interpretation\n",
    "\n",
    "MSLE is **dimensionless** (it operates on log values). A common variant is:\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSLE} = \\sqrt{\\mathrm{MSLE}}\n",
    "$$\n",
    "\n",
    "Because $\\log1p(\\hat{y})-\\log1p(y)=\\log\\frac{1+\\hat{y}}{1+y}$, RMSLE is the root-mean-square of a **log-ratio** error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny example\n",
    "y_true = np.array([3.0, 5.0, 2.5, 7.0])\n",
    "y_pred = np.array([2.5, 5.0, 4.0, 8.0])\n",
    "\n",
    "msle = mean_squared_log_error(y_true, y_pred)\n",
    "rmsle = float(np.sqrt(msle))\n",
    "\n",
    "msle, rmsle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition: squared error in log-space\n",
    "\n",
    "MSLE is just MSE applied to the transformed values $z=\\log(1+y)$.\n",
    "Two key consequences:\n",
    "\n",
    "1) **Large values are compressed.** Differences among big targets contribute less than they would under MSE on the original scale.\n",
    "\n",
    "2) MSLE mostly measures **relative** error. Using a first-order Taylor approximation:\n",
    "\n",
    "$$\n",
    "\\log(1+y+\\varepsilon) = \\log(1+y) + \\frac{\\varepsilon}{1+y} + O(\\varepsilon^2)\n",
    "$$\n",
    "\n",
    "so for small errors $\\varepsilon = \\hat{y}-y$:\n",
    "\n",
    "$$\n",
    "\\log1p(\\hat{y}) - \\log1p(y) \\approx \\frac{\\hat{y}-y}{1+y}\n",
    "$$\n",
    "\n",
    "and therefore:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSLE}(y,\\hat{y}) \\approx \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{\\hat{y}_i-y_i}{1+y_i}\\right)^2\n",
    "$$\n",
    "\n",
    "which is a **smoothed squared relative error** (for large $y$, $1+y\\approx y$; near zero, $1+y\\approx 1$ so MSLE behaves closer to MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the log1p transform and its inverse\n",
    "y_grid = np.linspace(0, 2000, 600)\n",
    "z_grid = np.linspace(0, np.log1p(y_grid.max()), 600)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Transform: z = log(1+y)\", \"Inverse: y = exp(z) - 1\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=y_grid, y=np.log1p(y_grid), mode=\"lines\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=z_grid, y=np.expm1(z_grid), mode=\"lines\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"log(1+y)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"z\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"exp(z)-1\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"log1p compresses large values (expm1 reverses it)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Multiplicative symmetry (ratios, not differences)\n",
    "\n",
    "A single-sample log error can be rewritten as a ratio:\n",
    "\n",
    "$$\n",
    "\\log1p(\\hat{y}) - \\log1p(y) = \\log\\left(\\frac{1+\\hat{y}}{1+y}\\right)\n",
    "$$\n",
    "\n",
    "So the per-sample squared log error is:\n",
    "\n",
    "$$\n",
    "\\big(\\log\\tfrac{1+\\hat{y}}{1+y}\\big)^2\n",
    "$$\n",
    "\n",
    "This means MSLE treats over- and under-prediction **symmetrically in multiplicative terms**:\n",
    "predicting a factor $k$ too large (in $1+y$) has the same loss as predicting a factor $k$ too small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample MSLE as a function of the ratio r = (1+y_pred)/(1+y_true)\n",
    "r = np.geomspace(0.05, 20.0, 600)\n",
    "sq_log_error = np.log(r) ** 2\n",
    "\n",
    "fig = px.line(\n",
    "    x=r,\n",
    "    y=sq_log_error,\n",
    "    log_x=True,\n",
    "    title=\"Per-sample squared log error depends on the ratio (1+y_pred)/(1+y_true)\",\n",
    "    labels={\"x\": \"ratio r = (1+y_pred)/(1+y_true)\", \"y\": \"(log r)^2\"},\n",
    ")\n",
    "fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "# Show the symmetry: r and 1/r yield the same loss\n",
    "for r0 in [2.0, 3.0, 5.0]:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[r0, 1.0 / r0],\n",
    "            y=[np.log(r0) ** 2, np.log(1.0 / r0) ** 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10),\n",
    "            name=f\"r={r0} and 1/r\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) MSLE vs MSE across scales\n",
    "\n",
    "Suppose the model makes a constant *relative* error, e.g. it always over-predicts by 20%.\n",
    "\n",
    "- Under **MSE**, the per-sample contribution $(\\hat{y}-y)^2$ grows like $y^2$.\n",
    "  Large targets dominate the metric.\n",
    "\n",
    "- Under **MSLE**, the per-sample contribution is approximately a constant (for large $y$)\n",
    "  because it is driven by the ratio $(1+\\hat{y})/(1+y)$.\n",
    "\n",
    "The plot below shows this effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.geomspace(0.1, 2000.0, 200)\n",
    "ratio = 1.20\n",
    "y_pred = ratio * y_true\n",
    "\n",
    "sq_err_mse = (y_pred - y_true) ** 2\n",
    "sq_err_msle = (np.log1p(y_pred) - np.log1p(y_true)) ** 2\n",
    "\n",
    "target_log_ratio_sq = float(np.log(ratio) ** 2)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"MSE contribution\", \"MSLE contribution\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=y_true, y=sq_err_mse, mode=\"markers\", name=\"(y_pred - y_true)^2\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=y_true, y=sq_err_msle, mode=\"markers\", name=\"(log1p diff)^2\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_hline(y=target_log_ratio_sq, line_dash=\"dash\", line_color=\"black\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"y_true\", type=\"log\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"squared error\", type=\"log\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"y_true\", type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"squared log error\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Constant 20% relative error: MSE explodes with scale, MSLE stays (nearly) constant\",\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) NumPy implementation from scratch\n",
    "\n",
    "Below is a small NumPy implementation that mirrors the core ideas in `sklearn.metrics.mean_squared_log_error`:\n",
    "\n",
    "- supports 1D targets `(n,)` and multioutput targets `(n, d)`\n",
    "- optional `sample_weight` (weighted average over samples)\n",
    "- `multioutput`: `'raw_values'`, `'uniform_average'`, or output weights `(d,)`\n",
    "\n",
    "It also enforces the MSLE domain constraint: **no negative values**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_log_error_np(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput=\"uniform_average\",\n",
    "):\n",
    "    \"\"\"Compute MSLE with optional sample weights and multioutput support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true, y_pred:\n",
    "        Array-like with shape (n,) or (n, d). Must be non-negative.\n",
    "    sample_weight:\n",
    "        None or array-like with shape (n,)\n",
    "    multioutput:\n",
    "        - 'raw_values'        -> return array of shape (d,)\n",
    "        - 'uniform_average'   -> return scalar (mean over outputs)\n",
    "        - array-like (d,)     -> weighted average over outputs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    y_true_arr = np.asarray(y_true, dtype=float)\n",
    "    y_pred_arr = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    if y_true_arr.shape != y_pred_arr.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true {y_true_arr.shape} vs y_pred {y_pred_arr.shape}\")\n",
    "\n",
    "    if np.any(y_true_arr < 0) or np.any(y_pred_arr < 0):\n",
    "        raise ValueError(\"MSLE is only defined for non-negative y_true and y_pred\")\n",
    "\n",
    "    if y_true_arr.ndim == 1:\n",
    "        y_true_arr = y_true_arr[:, None]\n",
    "        y_pred_arr = y_pred_arr[:, None]\n",
    "    elif y_true_arr.ndim != 2:\n",
    "        raise ValueError(\"y_true/y_pred must be 1D or 2D\")\n",
    "\n",
    "    n_samples, n_outputs = y_true_arr.shape\n",
    "\n",
    "    log_true = np.log1p(y_true_arr)\n",
    "    log_pred = np.log1p(y_pred_arr)\n",
    "\n",
    "    sq_log_error = (log_true - log_pred) ** 2  # (n, d)\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        w = np.asarray(sample_weight, dtype=float)\n",
    "        if w.ndim != 1 or w.shape[0] != n_samples:\n",
    "            raise ValueError(f\"sample_weight must have shape ({n_samples},), got {w.shape}\")\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "\n",
    "        denom = float(np.sum(w))\n",
    "        if denom == 0.0:\n",
    "            raise ValueError(\"sum(sample_weight) must be > 0\")\n",
    "\n",
    "        msle_per_output = np.sum(sq_log_error * w[:, None], axis=0) / denom\n",
    "    else:\n",
    "        msle_per_output = np.mean(sq_log_error, axis=0)\n",
    "\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == \"raw_values\":\n",
    "            return msle_per_output\n",
    "        if multioutput == \"uniform_average\":\n",
    "            return float(np.mean(msle_per_output))\n",
    "        raise ValueError(\n",
    "            \"multioutput must be 'raw_values', 'uniform_average', or array-like of output weights\"\n",
    "        )\n",
    "\n",
    "    output_weights = np.asarray(multioutput, dtype=float)\n",
    "    if output_weights.shape != (n_outputs,):\n",
    "        raise ValueError(f\"multioutput weights must have shape ({n_outputs},), got {output_weights.shape}\")\n",
    "\n",
    "    weight_sum = float(np.sum(output_weights))\n",
    "    if weight_sum == 0.0:\n",
    "        raise ValueError(\"sum(multioutput weights) must be > 0\")\n",
    "\n",
    "    return float(np.dot(msle_per_output, output_weights) / weight_sum)\n",
    "\n",
    "\n",
    "# Quick check vs scikit-learn\n",
    "y_true_2d = rng.uniform(0.0, 8.0, size=(30, 3))\n",
    "y_pred_2d = np.clip(y_true_2d + rng.normal(scale=0.8, size=(30, 3)), 0.0, None)\n",
    "w = rng.uniform(0.5, 2.0, size=30)\n",
    "\n",
    "ours = mean_squared_log_error_np(y_true_2d, y_pred_2d, sample_weight=w, multioutput=\"raw_values\")\n",
    "sk = mean_squared_log_error(y_true_2d, y_pred_2d, sample_weight=w, multioutput=\"raw_values\")\n",
    "\n",
    "ours, sk, np.allclose(ours, sk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Gradient w.r.t. predictions (useful for optimization)\n",
    "\n",
    "For one sample, define the log residual:\n",
    "\n",
    "$$\n",
    "e_i = \\log1p(\\hat{y}_i) - \\log1p(y_i)\n",
    "$$\n",
    "\n",
    "MSLE is $J = \\frac{1}{n}\\sum_i e_i^2$. Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\hat{y}_i} = \\frac{2}{n}\\,e_i\\,\\frac{\\partial}{\\partial \\hat{y}_i}\\log1p(\\hat{y}_i)\n",
    " = \\frac{2}{n}\\,\\frac{\\log1p(\\hat{y}_i) - \\log1p(y_i)}{1+\\hat{y}_i}\n",
    "$$\n",
    "\n",
    "Notice the factor $\\frac{1}{1+\\hat{y}_i}$: gradients are naturally scaled like **relative errors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msle_grad_y_pred(y_true, y_pred):\n",
    "    \"\"\"Gradient of MSLE w.r.t y_pred (1D arrays, uniform averaging).\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"shape mismatch\")\n",
    "    if np.any(y_true < 0) or np.any(y_pred < 0):\n",
    "        raise ValueError(\"MSLE is only defined for non-negative values\")\n",
    "\n",
    "    e = np.log1p(y_pred) - np.log1p(y_true)\n",
    "    return (2.0 / y_true.size) * e / (1.0 + y_pred)\n",
    "\n",
    "\n",
    "# Finite-difference check\n",
    "y_true = rng.uniform(0.0, 5.0, size=7)\n",
    "y_pred = rng.uniform(0.0, 5.0, size=7)\n",
    "\n",
    "g_ana = msle_grad_y_pred(y_true, y_pred)\n",
    "\n",
    "eps = 1e-6\n",
    "g_num = np.zeros_like(y_pred)\n",
    "for i in range(y_pred.size):\n",
    "    y_pred_p = y_pred.copy()\n",
    "    y_pred_m = y_pred.copy()\n",
    "    y_pred_p[i] += eps\n",
    "    y_pred_m[i] -= eps\n",
    "    y_pred_m[i] = max(0.0, y_pred_m[i])\n",
    "\n",
    "    f_p = mean_squared_log_error_np(y_true, y_pred_p)\n",
    "    f_m = mean_squared_log_error_np(y_true, y_pred_m)\n",
    "    g_num[i] = (f_p - f_m) / (2 * eps)\n",
    "\n",
    "g_ana, g_num, np.allclose(g_ana, g_num, rtol=2e-4, atol=2e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Using MSLE to optimize a model: log-space linear regression\n",
    "\n",
    "A very common way to **train** a model for MSLE is to predict in log-space.\n",
    "Define:\n",
    "\n",
    "$$\n",
    "z_i = \\log1p(y_i)\n",
    "$$\n",
    "\n",
    "and fit a linear model in $z$:\n",
    "\n",
    "$$\n",
    "\\hat{z}_i = b_0 + b_1 x_i\n",
    "$$\n",
    "\n",
    "Then convert back to the original scale with:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\exp(\\hat{z}_i) - 1\n",
    "$$\n",
    "\n",
    "This guarantees $\\hat{y}_i \\ge 0$, and the objective becomes:\n",
    "\n",
    "$$\n",
    "J(b_0, b_1) = \\frac{1}{n}\\sum_{i=1}^n (z_i - (b_0 + b_1 x_i))^2\n",
    "$$\n",
    "\n",
    "which is just least squares on $z$ (a convex quadratic).\n",
    "\n",
    "### Gradients\n",
    "\n",
    "Let $r_i = \\hat{z}_i - z_i$. Then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_0} = \\frac{2}{n}\\sum_{i=1}^n r_i\n",
    "\\qquad\n",
    "\\frac{\\partial J}{\\partial b_1} = \\frac{2}{n}\\sum_{i=1}^n x_i r_i\n",
    "$$\n",
    "\n",
    "We can minimize this with gradient descent (or solve it in closed form).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data with multiplicative / log-normal noise\n",
    "n = 260\n",
    "x = rng.uniform(0.0, 6.0, size=n)\n",
    "\n",
    "b0_true = 0.8\n",
    "b1_true = 0.9\n",
    "sigma = 0.35\n",
    "\n",
    "z_true = b0_true + b1_true * x\n",
    "z_obs = z_true + rng.normal(0.0, sigma, size=n)\n",
    "y = np.expm1(z_obs)  # y >= 0\n",
    "\n",
    "# Visualize in original space vs log space\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Data in original y scale\", \"Same data in z = log(1+y) scale\"),\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode=\"markers\", name=\"y\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=np.log1p(y), mode=\"markers\", name=\"log1p(y)\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"y\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"log(1+y)\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Log space often reveals a simpler (linear) relationship\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line_msle_gd(x, y, *, lr=0.05, n_steps=400):\n",
    "    \"\"\"Fit y_hat = exp(b0 + b1*x) - 1 by minimizing MSLE via GD in log-space.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if np.any(y < 0):\n",
    "        raise ValueError(\"y must be non-negative for MSLE\")\n",
    "\n",
    "    z = np.log1p(y)\n",
    "    n = x.size\n",
    "\n",
    "    b0 = float(z.mean())\n",
    "    b1 = 0.0\n",
    "\n",
    "    history = {\"step\": [], \"msle\": [], \"b0\": [], \"b1\": []}\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        z_hat = b0 + b1 * x\n",
    "        r = z_hat - z\n",
    "        msle_t = float(np.mean(r**2))\n",
    "\n",
    "        grad_b0 = (2.0 / n) * np.sum(r)\n",
    "        grad_b1 = (2.0 / n) * np.sum(x * r)\n",
    "\n",
    "        b0 -= lr * grad_b0\n",
    "        b1 -= lr * grad_b1\n",
    "\n",
    "        history[\"step\"].append(t)\n",
    "        history[\"msle\"].append(msle_t)\n",
    "        history[\"b0\"].append(b0)\n",
    "        history[\"b1\"].append(b1)\n",
    "\n",
    "    return (b0, b1), history\n",
    "\n",
    "\n",
    "(b0_gd, b1_gd), hist = fit_line_msle_gd(x, y, lr=0.06, n_steps=350)\n",
    "\n",
    "# Closed-form least squares in z-space\n",
    "X = np.column_stack([np.ones_like(x), x])\n",
    "z = np.log1p(y)\n",
    "b0_ols, b1_ols = np.linalg.lstsq(X, z, rcond=None)[0]\n",
    "\n",
    "(b0_gd, b1_gd), (b0_ols, b1_ols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curve\n",
    "fig = px.line(\n",
    "    x=hist[\"step\"],\n",
    "    y=hist[\"msle\"],\n",
    "    title=\"Gradient descent minimizing MSLE (in log space)\",\n",
    "    labels={\"x\": \"step\", \"y\": \"MSLE\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MSLE-trained model to an MSE-on-y linear baseline\n",
    "b0_mse, b1_mse = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "x_line = np.linspace(x.min(), x.max(), 300)\n",
    "X_line = np.column_stack([np.ones_like(x_line), x_line])\n",
    "\n",
    "y_pred_msle = np.expm1(b0_ols + b1_ols * x)\n",
    "y_line_msle = np.expm1(X_line @ np.array([b0_ols, b1_ols]))\n",
    "\n",
    "y_pred_mse = b0_mse + b1_mse * x\n",
    "y_line_mse = X_line @ np.array([b0_mse, b1_mse])\n",
    "y_pred_mse_clip = np.clip(y_pred_mse, 0.0, None)\n",
    "\n",
    "metrics = {\n",
    "    \"MSE (baseline linear)\": mean_squared_error(y, y_pred_mse),\n",
    "    \"MSE (MSLE-fit model)\": mean_squared_error(y, y_pred_msle),\n",
    "    \"MSLE (baseline linear, clipped)\": mean_squared_log_error(y, y_pred_mse_clip),\n",
    "    \"MSLE (MSLE-fit model)\": mean_squared_log_error(y, y_pred_msle),\n",
    "}\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode=\"markers\", name=\"data\", opacity=0.65))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_msle, mode=\"lines\", name=\"MSLE-fit: exp(linear)-1\"))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_mse, mode=\"lines\", name=\"MSE baseline: linear\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"MSLE-fit model tracks multiplicative structure; MSE-linear baseline can underfit small values\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss landscape in (b0, b1) for the log-space objective (convex bowl)\n",
    "b0_grid = np.linspace(b0_ols - 0.8, b0_ols + 0.8, 140)\n",
    "b1_grid = np.linspace(b1_ols - 0.8, b1_ols + 0.8, 140)\n",
    "\n",
    "B0, B1 = np.meshgrid(b0_grid, b1_grid)\n",
    "residuals = z[None, None, :] - (B0[:, :, None] + B1[:, :, None] * x[None, None, :])\n",
    "Z = np.mean(residuals**2, axis=2)\n",
    "\n",
    "stride = max(1, len(hist[\"step\"]) // 120)\n",
    "b0_path = np.array(hist[\"b0\"])[::stride]\n",
    "b1_path = np.array(hist[\"b1\"])[::stride]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=b0_grid,\n",
    "        y=b1_grid,\n",
    "        z=Z,\n",
    "        contours_coloring=\"heatmap\",\n",
    "        colorbar=dict(title=\"MSLE\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=b0_path,\n",
    "        y=b1_path,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"GD path\",\n",
    "        marker=dict(size=4, color=\"black\"),\n",
    "        line=dict(color=\"black\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[b0_ols],\n",
    "        y=[b1_ols],\n",
    "        mode=\"markers\",\n",
    "        name=\"OLS optimum\",\n",
    "        marker=dict(symbol=\"x\", size=10, color=\"white\", line=dict(color=\"black\", width=2)),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"MSLE landscape for log-space line: convex bowl + gradient descent trajectory\",\n",
    "    xaxis_title=\"b0\",\n",
    "    yaxis_title=\"b1\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Practical usage (scikit-learn)\n",
    "\n",
    "### Scoring\n",
    "\n",
    "MSLE is available as:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "```\n",
    "\n",
    "In model selection, scikit-learn uses the (negated) scorer string:\n",
    "\n",
    "```text\n",
    "neg_mean_squared_log_error\n",
    "```\n",
    "\n",
    "### Training for MSLE\n",
    "\n",
    "A common pattern is to train a regressor on `log1p(y)` and invert with `expm1`.\n",
    "In scikit-learn, `TransformedTargetRegressor` provides this neatly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_1d = x[:, None]\n",
    "\n",
    "reg = TransformedTargetRegressor(\n",
    "    regressor=LinearRegression(),\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1,\n",
    ")\n",
    "reg.fit(X_1d, y)\n",
    "y_pred = reg.predict(X_1d)\n",
    "\n",
    "mean_squared_log_error(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros, cons, and when to use MSLE\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Relative-error focus**: penalizes multiplicative differences (good for targets spanning orders of magnitude)\n",
    "- **Less dominated by large targets** than MSE on the original scale (log compresses large values)\n",
    "- **Handles zeros** (thanks to `log(1+y)`)\n",
    "- **Symmetric for over/under prediction in ratio terms**: $r$ and $1/r$ have the same per-sample loss\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Requires non-negative values**: cannot be used if $y$ (or predictions) can be negative\n",
    "- **The +1 matters**: near zero, MSLE behaves more like MSE and depends on the unit/scale implied by the `+1`\n",
    "- **Can hide large absolute errors** on large targets (by design)\n",
    "- **Less directly interpretable** than MAE/RMSE in the original units\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- Demand/count forecasting, sales, traffic, inventory: targets are non-negative and often long-tailed\n",
    "- Problems where a 20% miss on a small value should matter similarly to a 20% miss on a large value\n",
    "- Situations where you expect multiplicative noise (log-normal style)\n",
    "\n",
    "---\n",
    "\n",
    "## Common pitfalls / diagnostics\n",
    "\n",
    "- If your model can output negatives (e.g., plain linear regression), MSLE will error; consider:\n",
    "  - predicting in log-space and using `expm1`, or\n",
    "  - using a non-negative link function (e.g., `softplus`) or clipping (clipping changes the objective).\n",
    "- Plot residuals both in the original scale and in log-space; MSLE can look good while large absolute errors remain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Show that MSLE is exactly MSE on $z=\\log1p(y)$.\n",
    "2. Modify `mean_squared_log_error_np` to return RMSLE (square root) as an option.\n",
    "3. Create a dataset with additive noise instead of multiplicative noise and compare models trained with MSE vs MSLE.\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn: `sklearn.metrics.mean_squared_log_error`\n",
    "- Log transform modeling pattern: `sklearn.compose.TransformedTargetRegressor`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
