{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D² Log Loss Score (`d2_log_loss_score`)\n",
    "\n",
    "`d2_log_loss_score` is an **R²-style** metric for **probabilistic classification**.\n",
    "\n",
    "Instead of asking *\"How accurate are the predicted labels?\"* it asks:\n",
    "\n",
    "> **How much did my model reduce log loss compared to a null model that ignores features and only predicts class proportions?**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain log loss as “surprise” / negative log-likelihood\n",
    "- define and interpret the D² log loss score\n",
    "- implement `log_loss` and `d2_log_loss_score` from scratch in NumPy\n",
    "- see (with plots) why overconfident wrong predictions are punished harshly\n",
    "- optimize a simple logistic regression with gradient descent and track D² during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import d2_log_loss_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import d2_log_loss_score\n",
    "```\n",
    "\n",
    "`d2_log_loss_score` expects **predicted probabilities** (`predict_proba` output), not hard class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- binary vs multiclass classification\n",
    "- probabilities and logs\n",
    "- log loss / cross-entropy\n",
    "\n",
    "If you know **R²** for regression:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "then D² is the same “**fraction explained**” idea, but with **log loss** instead of squared error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Intuition: baseline vs your model\n",
    "\n",
    "### Log loss as “surprise”\n",
    "\n",
    "For a single binary sample with true label $y \\in \\{0, 1\\}$ and predicted probability $p = \\Pr(y=1)$:\n",
    "\n",
    "$$\n",
    "\\ell_{\\log}(y, p) = -\\Bigl(y\\log(p) + (1-y)\\log(1-p)\\Bigr)\n",
    "$$\n",
    "\n",
    "- If you say **\"99%\"** and you’re right, you get a tiny loss.\n",
    "- If you say **\"99%\"** and you’re wrong, you get a *huge* loss.\n",
    "\n",
    "That’s why log loss is great when probabilities matter: it rewards **calibration** and penalizes **overconfidence**.\n",
    "\n",
    "### D² turns log loss into an “explained fraction”\n",
    "\n",
    "D² compares your model to a **null model** that ignores features and always predicts the **class proportions** in the evaluation set.\n",
    "\n",
    "- If your model is no better than the null model: D² = 0\n",
    "- If your model is perfect: D² = 1\n",
    "- If your model is worse than the null model: D² < 0 (can be very negative)\n",
    "\n",
    "So you can read D² as:\n",
    "\n",
    "> “How much of the baseline log loss did we eliminate?”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Definition (math)\n",
    "\n",
    "Let there be $n$ samples and $K$ classes.\n",
    "\n",
    "- True labels as one-hot indicators: $y_{i,k} \\in \\{0,1\\}$ with $\\sum_k y_{i,k} = 1$\n",
    "- Predicted probabilities: $p_{i,k}$ with $\\sum_k p_{i,k} = 1$\n",
    "- Optional sample weights: $w_i \\ge 0$\n",
    "\n",
    "### Log loss (sum form)\n",
    "\n",
    "Using the natural logarithm (units = **nats**):\n",
    "\n",
    "$$\n",
    "\\operatorname{LL}(p) = -\\sum_{i=1}^n w_i \\sum_{k=1}^K y_{i,k}\\,\\log(p_{i,k})\n",
    "$$\n",
    "\n",
    "(You can divide by $\\sum_i w_i$ to get a mean log loss; the D² ratio is unchanged.)\n",
    "\n",
    "### Null model\n",
    "\n",
    "The null model predicts constant probabilities equal to the weighted class proportions:\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{\\sum_{i=1}^n w_i\\,y_{i,k}}{\\sum_{i=1}^n w_i}\n",
    "\\qquad\\Rightarrow\\qquad\n",
    "p^{\\text{null}}_{i,k} = \\pi_k\n",
    "$$\n",
    "\n",
    "### D² log loss score\n",
    "\n",
    "$$\n",
    "D^2_{\\log} = 1 - \\frac{\\operatorname{LL}(p^{\\text{model}})}{\\operatorname{LL}(p^{\\text{null}})}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "\n",
    "- Best possible score is **1**.\n",
    "- **0** means “no improvement over predicting class proportions”.\n",
    "- Can be **negative** and is unbounded below (log loss can blow up with confident wrong predictions).\n",
    "\n",
    "### Relationship to optimization\n",
    "\n",
    "For a fixed evaluation set, $\\operatorname{LL}(p^{\\text{null}})$ is a constant.\n",
    "\n",
    "So **maximizing** $D^2_{\\log}$ is equivalent to **minimizing** log loss:\n",
    "\n",
    "$$\\arg\\max D^2_{\\log} \\;\\equiv\\; \\arg\\min \\operatorname{LL}(p^{\\text{model}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) NumPy from-scratch implementation\n",
    "\n",
    "We’ll implement two functions:\n",
    "\n",
    "- `log_loss_numpy`: the (weighted) cross-entropy / negative log-likelihood\n",
    "- `d2_log_loss_score_numpy`: the “fraction of log loss explained” relative to the null model\n",
    "\n",
    "Notes:\n",
    "- We **clip** probabilities to avoid `log(0)`.\n",
    "- For binary classification we allow probabilities shaped `(n_samples,)` (positive class) or `(n_samples, 2)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    # Numerically stable sigmoid\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    out = np.empty_like(z, dtype=float)\n",
    "\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "\n",
    "    exp_z = np.exp(z[~pos])\n",
    "    out[~pos] = exp_z / (1.0 + exp_z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def softmax(z: np.ndarray, axis: int = 1) -> np.ndarray:\n",
    "    # Numerically stable softmax\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    z = z - np.max(z, axis=axis, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=axis, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_float_array(a):\n",
    "    return np.asarray(a, dtype=float)\n",
    "\n",
    "\n",
    "def _check_sample_weight(sample_weight, n_samples: int) -> np.ndarray:\n",
    "    if sample_weight is None:\n",
    "        return np.ones(n_samples, dtype=float)\n",
    "    w = _as_float_array(sample_weight)\n",
    "    if w.shape != (n_samples,):\n",
    "        raise ValueError(f\"sample_weight must have shape ({n_samples},), got {w.shape}.\")\n",
    "    if np.any(w < 0):\n",
    "        raise ValueError(\"sample_weight must be non-negative.\")\n",
    "    return w\n",
    "\n",
    "\n",
    "def _infer_classes(y_true, labels):\n",
    "    y_true = np.asarray(y_true)\n",
    "\n",
    "    if labels is None:\n",
    "        classes = np.unique(y_true)\n",
    "        if classes.size < 2:\n",
    "            raise ValueError(\n",
    "                f\"y_true contains only one label ({classes[0]!r}). \"\n",
    "                \"Please provide the true labels explicitly through the labels argument.\"\n",
    "            )\n",
    "        return classes\n",
    "\n",
    "    classes = np.asarray(labels)\n",
    "    if classes.size < 2:\n",
    "        raise ValueError(\"labels needs to contain at least two classes.\")\n",
    "    return classes\n",
    "\n",
    "\n",
    "def _label_to_index(classes: np.ndarray) -> dict:\n",
    "    return {label: i for i, label in enumerate(classes.tolist())}\n",
    "\n",
    "\n",
    "def log_loss_numpy(\n",
    "    y_true,\n",
    "    y_proba,\n",
    "    *,\n",
    "    normalize: bool = True,\n",
    "    sample_weight=None,\n",
    "    labels=None,\n",
    "    eps: float | None = None,\n",
    ") -> float:\n",
    "    # NumPy log loss for 1D label arrays.\n",
    "    # Parameters mirror sklearn.metrics.log_loss (subset).\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = _as_float_array(y_proba)\n",
    "\n",
    "    n_samples = y_true.shape[0]\n",
    "    if y_proba.shape[0] != n_samples:\n",
    "        raise ValueError(\"y_true and y_proba must have the same number of samples.\")\n",
    "\n",
    "    classes = _infer_classes(y_true, labels)\n",
    "    label_to_idx = _label_to_index(classes)\n",
    "\n",
    "    try:\n",
    "        y_idx = np.array([label_to_idx[y] for y in y_true.tolist()], dtype=int)\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Found unknown label {e.args[0]!r} in y_true.\") from None\n",
    "\n",
    "    # Shape handling: allow binary probs as (n,) or (n, 1)\n",
    "    if y_proba.ndim == 1:\n",
    "        if classes.size != 2:\n",
    "            raise ValueError(\"1D y_proba is only supported for binary classification.\")\n",
    "        y_proba = np.column_stack([1.0 - y_proba, y_proba])\n",
    "\n",
    "    if y_proba.ndim != 2:\n",
    "        raise ValueError(\"y_proba must be 1D or 2D.\")\n",
    "\n",
    "    if y_proba.shape[1] == 1:\n",
    "        if classes.size != 2:\n",
    "            raise ValueError(\"(n, 1) y_proba is only supported for binary classification.\")\n",
    "        p = y_proba[:, 0]\n",
    "        y_proba = np.column_stack([1.0 - p, p])\n",
    "\n",
    "    if y_proba.shape[1] != classes.size:\n",
    "        raise ValueError(\n",
    "            f\"Number of probability columns ({y_proba.shape[1]}) does not match number of classes ({classes.size}).\"\n",
    "        )\n",
    "\n",
    "    if eps is None:\n",
    "        eps = np.finfo(y_proba.dtype).eps\n",
    "\n",
    "    row_sums = y_proba.sum(axis=1)\n",
    "    if not np.allclose(row_sums, 1.0, rtol=np.sqrt(eps)):\n",
    "        warnings.warn(\n",
    "            \"The y_proba rows do not sum to 1. Make sure to pass probabilities.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "\n",
    "    y_proba = np.clip(y_proba, eps, 1.0 - eps)\n",
    "\n",
    "    p_true = y_proba[np.arange(n_samples), y_idx]\n",
    "    per_sample_loss = -np.log(p_true)\n",
    "\n",
    "    w = _check_sample_weight(sample_weight, n_samples)\n",
    "    loss_sum = float(np.sum(w * per_sample_loss))\n",
    "\n",
    "    if normalize:\n",
    "        return loss_sum / float(np.sum(w))\n",
    "    return loss_sum\n",
    "\n",
    "\n",
    "def d2_log_loss_score_numpy(y_true, y_proba, *, sample_weight=None, labels=None) -> float:\n",
    "    # D² log loss score (fraction of log loss explained).\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = _as_float_array(y_proba)\n",
    "\n",
    "    n_samples = y_true.shape[0]\n",
    "    if y_proba.shape[0] != n_samples:\n",
    "        raise ValueError(\"y_true and y_proba must have the same number of samples.\")\n",
    "\n",
    "    if n_samples < 2:\n",
    "        warnings.warn(\n",
    "            \"D^2 score is not well-defined with less than two samples.\",\n",
    "            RuntimeWarning,\n",
    "        )\n",
    "        return float(\"nan\")\n",
    "\n",
    "    numerator = log_loss_numpy(\n",
    "        y_true,\n",
    "        y_proba,\n",
    "        normalize=False,\n",
    "        sample_weight=sample_weight,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    classes = _infer_classes(y_true, labels)\n",
    "    label_to_idx = _label_to_index(classes)\n",
    "    y_idx = np.array([label_to_idx[y] for y in y_true.tolist()], dtype=int)\n",
    "\n",
    "    w = _check_sample_weight(sample_weight, n_samples)\n",
    "    counts = np.bincount(y_idx, weights=w, minlength=classes.size)\n",
    "    priors = counts / float(np.sum(w))\n",
    "    y_proba_null = np.tile(priors, (n_samples, 1))\n",
    "\n",
    "    denominator = log_loss_numpy(\n",
    "        y_true,\n",
    "        y_proba_null,\n",
    "        normalize=False,\n",
    "        sample_weight=sample_weight,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    return 1.0 - (numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check vs scikit-learn\n",
    "\n",
    "We’ll compare our NumPy implementation to:\n",
    "\n",
    "- `sklearn.metrics.log_loss`\n",
    "- `sklearn.metrics.d2_log_loss_score`\n",
    "\n",
    "On typical inputs (at least 2 classes present), the values should match up to tiny floating-point differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary sanity check\n",
    "n = 500\n",
    "y_true_bin = rng.integers(0, 2, size=n)\n",
    "\n",
    "# Random probabilities (not great, but valid)\n",
    "p_pos = np.clip(rng.uniform(0.02, 0.98, size=n), 1e-15, 1 - 1e-15)\n",
    "\n",
    "print('log_loss sklearn:', log_loss(y_true_bin, p_pos))\n",
    "print('log_loss numpy :', log_loss_numpy(y_true_bin, p_pos))\n",
    "print('d2 sklearn      :', d2_log_loss_score(y_true_bin, p_pos))\n",
    "print('d2 numpy        :', d2_log_loss_score_numpy(y_true_bin, p_pos))\n",
    "\n",
    "# Multiclass sanity check\n",
    "n = 600\n",
    "k = 3\n",
    "y_true_mc = rng.integers(0, k, size=n)\n",
    "logits = rng.normal(size=(n, k))\n",
    "proba = softmax(logits, axis=1)\n",
    "\n",
    "print()\n",
    "print('multiclass log_loss sklearn:', log_loss(y_true_mc, proba, labels=[0, 1, 2]))\n",
    "print('multiclass log_loss numpy :', log_loss_numpy(y_true_mc, proba, labels=[0, 1, 2]))\n",
    "print('multiclass d2 sklearn      :', d2_log_loss_score(y_true_mc, proba, labels=[0, 1, 2]))\n",
    "print('multiclass d2 numpy        :', d2_log_loss_score_numpy(y_true_mc, proba, labels=[0, 1, 2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Plots: how log loss and D² behave\n",
    "\n",
    "### 4.1) Per-sample log loss curves (binary)\n",
    "\n",
    "For $y=1$ the loss is $-\\log(p)$. For $y=0$ it is $-\\log(1-p)$.\n",
    "\n",
    "These curves explain the “**overconfident wrong** is catastrophic” behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(1e-6, 1 - 1e-6, 600)\n",
    "loss_y1 = -np.log(p)\n",
    "loss_y0 = -np.log(1 - p)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=p, y=loss_y1, mode='lines', name='y=1:  -log(p)'))\n",
    "fig.add_trace(go.Scatter(x=p, y=loss_y0, mode='lines', name='y=0:  -log(1-p)'))\n",
    "fig.update_layout(\n",
    "    title='Binary log loss for a single sample',\n",
    "    xaxis_title='Predicted probability p = P(y=1)',\n",
    "    yaxis_title='Per-sample log loss (nats)',\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) The null-model log loss is (empirical) entropy\n",
    "\n",
    "In the binary case, the null model predicts the positive rate $\\hat{p}$ for everyone.\n",
    "\n",
    "Its mean log loss is the empirical Bernoulli entropy:\n",
    "\n",
    "$$\n",
    "H(\\hat{p}) = -\\bigl(\\hat{p}\\log\\hat{p} + (1-\\hat{p})\\log(1-\\hat{p})\\bigr)\n",
    "$$\n",
    "\n",
    "- Maximum at $\\hat{p}=0.5$ (most uncertainty)\n",
    "- Small near 0 or 1 (almost-deterministic labels)\n",
    "\n",
    "This matters because D² scales improvements by this baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = np.linspace(1e-6, 1 - 1e-6, 600)\n",
    "entropy = -(p_hat * np.log(p_hat) + (1 - p_hat) * np.log(1 - p_hat))\n",
    "\n",
    "fig = go.Figure(go.Scatter(x=p_hat, y=entropy, mode='lines'))\n",
    "fig.update_layout(\n",
    "    title='Baseline (null-model) mean log loss = Bernoulli entropy H(p̂)',\n",
    "    xaxis_title='Positive rate p̂',\n",
    "    yaxis_title='H(p̂) (nats)',\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) D² for constant predictors\n",
    "\n",
    "Let’s generate a dataset with a fixed class balance and score a **constant** predictor that always outputs $q$.\n",
    "\n",
    "- At $q = \\hat{p}$ (the empirical positive rate), D² = 0 (that’s the null model).\n",
    "- Moving away from $\\hat{p}$ makes log loss worse → D² becomes negative.\n",
    "\n",
    "This is a nice way to see D² as a *scaled* version of log loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 800\n",
    "p_true = 0.25\n",
    "y = rng.binomial(1, p_true, size=n)\n",
    "\n",
    "q = np.linspace(1e-4, 1 - 1e-4, 500)\n",
    "\n",
    "# Score constant predictors\n",
    "logloss_vals = np.array([log_loss_numpy(y, np.full_like(y, qi, dtype=float)) for qi in q])\n",
    "d2_vals = np.array([d2_log_loss_score_numpy(y, np.full_like(y, qi, dtype=float)) for qi in q])\n",
    "\n",
    "p_hat = y.mean()\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Mean log loss', 'D² log loss score'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=q, y=logloss_vals, mode='lines', name='log loss'), row=1, col=1)\n",
    "fig.add_vline(x=p_hat, line_dash='dash', line_color='gray', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Constant probability q', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Mean log loss', row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=q, y=d2_vals, mode='lines', name='D²'), row=1, col=2)\n",
    "fig.add_hline(y=0.0, line_dash='dash', line_color='gray', row=1, col=2)\n",
    "fig.add_vline(x=p_hat, line_dash='dash', line_color='gray', row=1, col=2)\n",
    "fig.update_xaxes(title_text='Constant probability q', row=1, col=2)\n",
    "fig.update_yaxes(title_text='D²', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Constant predictors on a dataset with p̂={p_hat:.3f}',\n",
    "    width=1000,\n",
    "    height=430,\n",
    "    showlegend=False,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Using log loss (and D²) to optimize logistic regression (from scratch)\n",
    "\n",
    "Because D² is just a rescaling of log loss (by a constant baseline), training usually minimizes **log loss** directly.\n",
    "\n",
    "### Binary logistic regression\n",
    "\n",
    "Model with parameters $\\beta$:\n",
    "\n",
    "$$\n",
    "\\eta_i = x_i^\\top\\beta\n",
    "\\qquad\n",
    "p_i = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n",
    "$$\n",
    "\n",
    "Average log loss:\n",
    "\n",
    "$$\n",
    "J(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n \\Bigl(y_i\\log(p_i) + (1-y_i)\\log(1-p_i)\\Bigr)\n",
    "$$\n",
    "\n",
    "Gradient (classic result):\n",
    "\n",
    "$$\n",
    "\\nabla J(\\beta) = \\frac{1}{n} X^\\top (p - y)\n",
    "$$\n",
    "\n",
    "We’ll run gradient descent and track both:\n",
    "\n",
    "- log loss (lower is better)\n",
    "- D² log loss score (higher is better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D classification problem\n",
    "X, y = make_classification(\n",
    "    n_samples=1200,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.4,\n",
    "    flip_y=0.06,\n",
    "    weights=[0.6, 0.4],\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize (train statistics only)\n",
    "mu = X_train.mean(axis=0)\n",
    "sigma = X_train.std(axis=0) + 1e-12\n",
    "\n",
    "X_train_s = (X_train - mu) / sigma\n",
    "X_test_s = (X_test - mu) / sigma\n",
    "\n",
    "# Add intercept term\n",
    "X_train_i = np.column_stack([np.ones(X_train_s.shape[0]), X_train_s])\n",
    "X_test_i = np.column_stack([np.ones(X_test_s.shape[0]), X_test_s])\n",
    "\n",
    "# Quick look at the data\n",
    "fig = px.scatter(\n",
    "    x=X_train_s[:, 0],\n",
    "    y=X_train_s[:, 1],\n",
    "    color=y_train.astype(str),\n",
    "    title='Training data (standardized)',\n",
    "    labels={'x': 'x1 (standardized)', 'y': 'x2 (standardized)', 'color': 'class'},\n",
    ")\n",
    "fig.update_layout(width=750, height=450)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    lr: float = 0.2,\n",
    "    n_epochs: int = 250,\n",
    "    l2: float = 0.0,\n",
    "):\n",
    "    # Gradient descent for binary logistic regression.\n",
    "    # X: (n, d) with intercept column already included.\n",
    "    # y: (n,) in {0,1}\n",
    "\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d, dtype=float)\n",
    "\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_log_loss': [],\n",
    "        'train_d2': [],\n",
    "        'w': [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        p = sigmoid(X @ w)\n",
    "\n",
    "        # Mean log loss (for nicer learning-rate behavior)\n",
    "        loss = log_loss_numpy(y, p)\n",
    "\n",
    "        # Gradient of mean log loss + optional L2\n",
    "        grad = (X.T @ (p - y)) / n\n",
    "        grad[1:] += l2 * w[1:]  # don't regularize intercept\n",
    "\n",
    "        w -= lr * grad\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_log_loss'].append(loss)\n",
    "        history['train_d2'].append(d2_log_loss_score_numpy(y, p))\n",
    "        history['w'].append(w.copy())\n",
    "\n",
    "    return w, history\n",
    "\n",
    "\n",
    "w_hat, hist = fit_logreg_gd(X_train_i, y_train, lr=0.25, n_epochs=240, l2=0.0)\n",
    "\n",
    "p_train = sigmoid(X_train_i @ w_hat)\n",
    "p_test = sigmoid(X_test_i @ w_hat)\n",
    "\n",
    "print('Final (train) log loss:', log_loss_numpy(y_train, p_train))\n",
    "print('Final (test)  log loss:', log_loss_numpy(y_test, p_test))\n",
    "print('Final (train) D²      :', d2_log_loss_score_numpy(y_train, p_train))\n",
    "print('Final (test)  D²      :', d2_log_loss_score_numpy(y_test, p_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Log loss (train)', 'D² log loss score (train)'))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hist['epoch'], y=hist['train_log_loss'], mode='lines', name='log loss'),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.update_xaxes(title_text='epoch', row=1, col=1)\n",
    "fig.update_yaxes(title_text='mean log loss', row=1, col=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hist['epoch'], y=hist['train_d2'], mode='lines', name='D²'),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_hline(y=0.0, line_dash='dash', line_color='gray', row=1, col=2)\n",
    "fig.update_xaxes(title_text='epoch', row=1, col=2)\n",
    "fig.update_yaxes(title_text='D²', row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=1000, height=420, showlegend=False)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with scikit-learn\n",
    "\n",
    "We’ll fit `sklearn.linear_model.LogisticRegression` and score the predicted probabilities.\n",
    "\n",
    "Because D² is computed from log loss, you should typically see the same ranking as with log loss (higher D² ↔ lower log loss) on the same dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=None, solver='lbfgs', max_iter=2000, random_state=7)\n",
    "clf.fit(X_train_s, y_train)\n",
    "\n",
    "p_test_sklearn = clf.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "print('sklearn log loss:', log_loss(y_test, p_test_sklearn))\n",
    "print('numpy  log loss:', log_loss_numpy(y_test, p_test_sklearn))\n",
    "\n",
    "print('sklearn D²      :', d2_log_loss_score(y_test, p_test_sklearn))\n",
    "print('numpy  D²      :', d2_log_loss_score_numpy(y_test, p_test_sklearn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Pros, cons, and when to use it\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Interpretable baseline**: 0 means “no better than predicting class proportions”.\n",
    "- **Proper-probability sensitivity** (via log loss): rewards well-calibrated probabilities, not just correct labels.\n",
    "- **Works for multiclass** naturally.\n",
    "- **R²-like scale**: best is 1, worse-than-baseline is negative.\n",
    "\n",
    "### Cons / caveats\n",
    "\n",
    "- **Unbounded below**: very confident wrong predictions can drive the score to large negative values.\n",
    "- **Dataset-dependent scaling**: the baseline log loss depends on class balance; comparing D² across very different datasets can be misleading.\n",
    "- **Requires probabilities**: hard labels (`predict`) aren’t enough.\n",
    "- **Not meaningful with too few samples**: scikit-learn returns NaN for `n_samples < 2`.\n",
    "- **Edge cases with missing classes**: if an evaluation fold is missing a class, you may need to pass `labels=` consistently.\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- evaluating probabilistic classifiers where **calibration matters** (risk scores, medical, churn probability, etc.)\n",
    "- reporting an “explained fraction” style score to audiences familiar with **R²**\n",
    "- model selection *within the same dataset / split* (it ranks models the same way as log loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Pitfalls & diagnostics\n",
    "\n",
    "- **Overconfidence hurts**: if your D² is very negative, inspect examples with high loss (often miscalibrated or shifted data).\n",
    "- **Always check the null score**: D²=0 is not “bad” — it means “you didn’t beat class proportions”.\n",
    "- **Class imbalance**: on highly imbalanced datasets, the null model can already achieve low log loss; large D² improvements may be harder.\n",
    "- **Label ordering** (binary with `(n_samples,)` probabilities): scikit-learn assumes the probability is for the “positive” class (the second class in sorted order).\n",
    "- **Use calibration tools** when needed: reliability diagrams, isotonic regression, Platt scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Create two classifiers with the same accuracy but different probability calibration. Compare log loss and D².\n",
    "2) For multiclass data, show how D² changes as you apply “temperature scaling” to logits.\n",
    "3) Add L2 regularization to the gradient descent logistic regression and observe how it affects log loss and D².\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn API: `sklearn.metrics.d2_log_loss_score`\n",
    "- scikit-learn user guide: “D² score for classification” (see the docs page linked from the API)\n",
    "- C.M. Bishop (2006), *Pattern Recognition and Machine Learning* (cross-entropy / logistic regression)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}