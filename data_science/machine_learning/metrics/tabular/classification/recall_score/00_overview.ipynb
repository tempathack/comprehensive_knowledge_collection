{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recall_score\n",
    "\n",
    "Compute the recall.\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import recall_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Intuition**: recall (sensitivity) is the fraction of actual positives captured: $\\text{recall}=\\frac{TP}{TP+FN}$.\n",
    "- **Example**: if there are 10 true positives and you detect 7, recall = 0.7.\n",
    "- **Pitfalls**: high recall can come with many false positives; consider precision or the PR curve.\n",
    "- **Multiclass**: use `average` (`'macro'`, `'micro'`, `'weighted'`) and `pos_label` for binary tasks.\n",
    "\n",
    "## Example\n",
    "\n",
    "Small, self-contained example:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Toy binary classification\n",
    "y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "recall_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/api/sklearn.metrics.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}