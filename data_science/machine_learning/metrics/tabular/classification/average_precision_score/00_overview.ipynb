{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# average_precision_score (Average Precision, AP)\n",
    "\n",
    "Average Precision (AP) summarizes a **precision–recall (PR) curve** into a single number.\n",
    "It is a *ranking* metric: it cares about how well you order examples by “how positive” they are.\n",
    "\n",
    "**When to reach for AP**: imbalanced binary classification (fraud, rare disease, anomaly detection), information retrieval, recommender ranking.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Build intuition for precision/recall and the PR curve\n",
    "- Derive AP and compute it by hand on a tiny example\n",
    "- Implement `average_precision_score` from scratch (NumPy)\n",
    "- Visualize what AP measures (Plotly)\n",
    "- Use AP for model selection / early stopping in a simple from-scratch logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score as sk_average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve as sk_precision_recall_curve\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Why AP? (especially when positives are rare)\n",
    "\n",
    "In many real problems, the positive class is rare:\n",
    "- fraud vs non-fraud\n",
    "- diseased vs healthy\n",
    "- defective vs non-defective\n",
    "\n",
    "If only 1% of examples are positive, a model that predicts “negative” for everyone gets **99% accuracy** — yet it is useless.\n",
    "\n",
    "A PR curve focuses on the positive class:\n",
    "- **Precision** answers: “Of the items I flagged, how many are truly positive?”\n",
    "- **Recall** answers: “Of all true positives, how many did I find?”\n",
    "\n",
    "AP compresses the entire PR curve into one number (higher is better). The “no-skill” baseline is the positive prevalence\n",
    "$$\\pi = \\frac{\\#\\text{positives}}{n}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Precision/recall at a threshold\n",
    "\n",
    "Assume binary labels $y_i \\in \\{0,1\\}$ and model scores $s_i \\in \\mathbb{R}$ (larger means “more positive”).\n",
    "\n",
    "For a threshold $\\tau$, predict:\n",
    "$$\\hat{y}_i(\\tau) = \\mathbb{1}[s_i \\ge \\tau]$$\n",
    "\n",
    "Define counts:\n",
    "- $\\mathrm{TP}(\\tau)$: predicted 1 and actually 1\n",
    "- $\\mathrm{FP}(\\tau)$: predicted 1 but actually 0\n",
    "- $\\mathrm{FN}(\\tau)$: predicted 0 but actually 1\n",
    "\n",
    "Then:\n",
    "$$\\mathrm{Precision}(\\tau) = \\frac{\\mathrm{TP}(\\tau)}{\\mathrm{TP}(\\tau)+\\mathrm{FP}(\\tau)}$$\n",
    "$$\\mathrm{Recall}(\\tau) = \\frac{\\mathrm{TP}(\\tau)}{\\mathrm{TP}(\\tau)+\\mathrm{FN}(\\tau)}$$\n",
    "\n",
    "Sweeping $\\tau$ from high to low traces out the **PR curve**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking view (top-k)\n",
    "\n",
    "Instead of thinking in thresholds, you can sort examples by score (descending) and take the top-$k$ as “predicted positive”.\n",
    "Let $P@k$ be the precision among the top-$k$ items, and $R@k$ be the recall among the top-$k$ items.\n",
    "\n",
    "As $k$ grows, recall is non-decreasing, and we trace the PR curve point-by-point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny ranked example (8 items)\n",
    "y_true_toy = np.array([1, 0, 1, 0, 1, 0, 0, 1])\n",
    "y_score_toy = np.array([0.90, 0.85, 0.80, 0.70, 0.65, 0.60, 0.40, 0.30])\n",
    "\n",
    "order = np.argsort(-y_score_toy, kind=\"mergesort\")\n",
    "y_sorted = y_true_toy[order]\n",
    "s_sorted = y_score_toy[order]\n",
    "\n",
    "tp = np.cumsum(y_sorted)\n",
    "fp = np.cumsum(1 - y_sorted)\n",
    "\n",
    "precision_at_k = tp / (tp + fp)\n",
    "recall_at_k = tp / tp[-1]  # tp[-1] == number of positives\n",
    "\n",
    "# AP = mean precision at every rank where we encounter a true positive\n",
    "ap_toy = precision_at_k[y_sorted == 1].mean()\n",
    "prevalence_toy = y_true_toy.mean()\n",
    "\n",
    "df_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"rank\": np.arange(1, len(y_true_toy) + 1),\n",
    "        \"score\": s_sorted,\n",
    "        \"y_true\": y_sorted,\n",
    "        \"TP_cum\": tp,\n",
    "        \"FP_cum\": fp,\n",
    "        \"precision@k\": precision_at_k,\n",
    "        \"recall@k\": recall_at_k,\n",
    "        \"contributes_to_AP\": (y_sorted == 1),\n",
    "    }\n",
    ")\n",
    "\n",
    "df_toy, ap_toy, prevalence_toy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP as “average precision at each hit”\n",
    "\n",
    "In a ranked list, AP has a convenient interpretation:\n",
    "\n",
    "$$\\mathrm{AP} = \\frac{1}{m} \\sum_{k: y_{(k)}=1} \\mathrm{Precision}@k$$\n",
    "\n",
    "where $y_{(k)}$ is the label at rank $k$, and $m$ is the number of positives.\n",
    "\n",
    "So every time you “hit” a true positive in the ranking, you record the current precision; AP is the average of those recorded precisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = np.arange(1, len(y_sorted) + 1)\n",
    "pos_mask = y_sorted == 1\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=rank,\n",
    "        y=precision_at_k,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"precision@k\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=rank[pos_mask],\n",
    "        y=precision_at_k[pos_mask],\n",
    "        mode=\"markers\",\n",
    "        name=\"positions where y=1\",\n",
    "        marker=dict(size=10, color=\"crimson\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_hline(\n",
    "    y=ap_toy,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"AP = {ap_toy:.3f}\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Ranked list view: precision@k (AP is the mean at positive hits)\",\n",
    "    xaxis_title=\"Rank k (top-k predicted positives)\",\n",
    "    yaxis_title=\"Precision@k\",\n",
    "    yaxis=dict(range=[0, 1.05]),\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a step PR curve from the ranked list\n",
    "precision_curve = np.r_[1.0, precision_at_k]\n",
    "recall_curve = np.r_[0.0, recall_at_k]\n",
    "\n",
    "# Step-wise area under PR curve (this equals AP)\n",
    "ap_area = np.sum(np.diff(recall_curve) * precision_curve[1:])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall_curve,\n",
    "        y=precision_curve,\n",
    "        mode=\"lines\",\n",
    "        line_shape=\"hv\",\n",
    "        fill=\"tozeroy\",\n",
    "        name=\"PR curve (step)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall_curve[1:][pos_mask],\n",
    "        y=precision_curve[1:][pos_mask],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10, color=\"crimson\"),\n",
    "        name=\"points where recall increases (true positives)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[prevalence_toy, prevalence_toy],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=f\"baseline prevalence = {prevalence_toy:.3f}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Precision–Recall curve (toy example) — AP = {ap_toy:.3f} (area = {ap_area:.3f})\",\n",
    "    xaxis_title=\"Recall\",\n",
    "    yaxis_title=\"Precision\",\n",
    "    xaxis=dict(range=[0, 1.02]),\n",
    "    yaxis=dict(range=[0, 1.02]),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "ap_toy, ap_area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Average Precision (formal definition)\n",
    "\n",
    "A PR curve is a set of points $(R(\\tau), P(\\tau))$ as we sweep the threshold $\\tau$.\n",
    "\n",
    "In scikit-learn, **average precision** is computed as the area under the PR curve using a **step function**:\n",
    "$$\\mathrm{AP} = \\sum_{n} (R_n - R_{n-1})\\, P_n,$$\n",
    "where the index $n$ runs over the curve points in **increasing recall**.\n",
    "\n",
    "This is equivalent to the ranked-list formula shown earlier (mean of $\\mathrm{Precision}@k$ at every true positive rank).\n",
    "\n",
    "Two practical notes:\n",
    "- AP only depends on the **ordering** of scores. Any strictly increasing transform of scores (e.g., $s \\mapsto 10s + 3$) leaves AP unchanged.\n",
    "- AP is *not* the same as trapezoidal “AUPRC”. If you need a specific integration convention, be explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve_np(y_true, y_score):\n",
    "    \"\"\"Simple PR curve from scores.\n",
    "\n",
    "    Returns precision and recall in *increasing recall* order, with a starting point (recall=0, precision=1).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "\n",
    "    if y_true.ndim != 1 or y_score.ndim != 1 or y_true.shape[0] != y_score.shape[0]:\n",
    "        raise ValueError(\"y_true and y_score must be 1D arrays of the same length\")\n",
    "\n",
    "    order = np.argsort(-y_score, kind=\"mergesort\")\n",
    "    y_sorted = y_true[order]\n",
    "    s_sorted = y_score[order]\n",
    "\n",
    "    n_pos = int(y_sorted.sum())\n",
    "    if n_pos == 0:\n",
    "        return np.array([1.0]), np.array([0.0]), np.array([])\n",
    "\n",
    "    tp = np.cumsum(y_sorted)\n",
    "    fp = np.cumsum(1 - y_sorted)\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / n_pos\n",
    "\n",
    "    precision = np.r_[1.0, precision]\n",
    "    recall = np.r_[0.0, recall]\n",
    "\n",
    "    # Thresholds corresponding to each added item (top-k): predict positive if score >= threshold\n",
    "    thresholds = s_sorted\n",
    "    return precision, recall, thresholds\n",
    "\n",
    "\n",
    "def average_precision_score_np(y_true, y_score):\n",
    "    \"\"\"Average precision (AP) from scratch.\n",
    "\n",
    "    Equivalent to the mean of precision@k over ranks k where y_true=1 after sorting by score (descending).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "\n",
    "    if y_true.ndim != 1 or y_score.ndim != 1 or y_true.shape[0] != y_score.shape[0]:\n",
    "        raise ValueError(\"y_true and y_score must be 1D arrays of the same length\")\n",
    "\n",
    "    order = np.argsort(-y_score, kind=\"mergesort\")\n",
    "    y_sorted = y_true[order]\n",
    "\n",
    "    n_pos = int(y_sorted.sum())\n",
    "    if n_pos == 0:\n",
    "        return 0.0\n",
    "\n",
    "    tp = np.cumsum(y_sorted)\n",
    "    precision_at_k = tp / (np.arange(len(y_sorted)) + 1)\n",
    "    ap = precision_at_k[y_sorted == 1].sum() / n_pos\n",
    "    return float(ap)\n",
    "\n",
    "\n",
    "# Quick sanity checks vs scikit-learn\n",
    "y_check = rng.integers(0, 2, size=50)\n",
    "s_check = rng.normal(size=50)\n",
    "\n",
    "ap_np = average_precision_score_np(y_check, s_check)\n",
    "ap_sk = sk_average_precision_score(y_check, s_check)\n",
    "\n",
    "# AP is invariant to strictly increasing transforms of the scores\n",
    "ap_np_affine = average_precision_score_np(y_check, 10 * s_check + 3)\n",
    "ap_np_cubic = average_precision_score_np(y_check, s_check**3)\n",
    "\n",
    "ap_np, ap_sk, ap_np_affine, ap_np_cubic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visual intuition: random vs informative vs (almost) perfect scores\n",
    "\n",
    "Let’s keep the *labels fixed* and only change the *quality of the scores*.\n",
    "A random scoring function should have AP close to the prevalence $\\pi$.\n",
    "As the ranking improves, the PR curve bends upward and AP increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3000\n",
    "prevalence = 0.05\n",
    "y_sim = (rng.random(n) < prevalence).astype(int)\n",
    "\n",
    "scores_random = rng.normal(size=n)\n",
    "scores_some_signal = 2.0 * y_sim + rng.normal(scale=1.0, size=n)\n",
    "scores_almost_perfect = y_sim + 0.001 * rng.normal(size=n)\n",
    "\n",
    "models = {\n",
    "    \"random\": scores_random,\n",
    "    \"some signal\": scores_some_signal,\n",
    "    \"almost perfect\": scores_almost_perfect,\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, scores in models.items():\n",
    "    prec, rec, _ = precision_recall_curve_np(y_sim, scores)\n",
    "    ap = average_precision_score_np(y_sim, scores)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=rec,\n",
    "            y=prec,\n",
    "            mode=\"lines\",\n",
    "            line_shape=\"hv\",\n",
    "            name=f\"{name} (AP={ap:.3f})\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[prevalence, prevalence],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=f\"baseline prevalence={prevalence:.3f}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PR curves for different score qualities (same labels)\",\n",
    "    xaxis_title=\"Recall\",\n",
    "    yaxis_title=\"Precision\",\n",
    "    xaxis=dict(range=[0, 1.02]),\n",
    "    yaxis=dict(range=[0, 1.02]),\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Practical usage (`sklearn`)\n",
    "\n",
    "For binary classification you should pass **scores**, not hard labels:\n",
    "- `y_true`: 0/1 labels\n",
    "- `y_score`: probability for the positive class, or a decision score (any real-valued ranking signal)\n",
    "\n",
    "In multilabel settings, `average_precision_score` supports `average={\"micro\",\"macro\",\"weighted\",\"samples\"}`.\n",
    "Conceptually it computes AP per label (one-vs-rest) and then combines them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn comparison (binary)\n",
    "ap_np = average_precision_score_np(y_sim, scores_some_signal)\n",
    "ap_sk = sk_average_precision_score(y_sim, scores_some_signal)\n",
    "\n",
    "prec_sk, rec_sk, thr_sk = sk_precision_recall_curve(y_sim, scores_some_signal)\n",
    "# sklearn returns recall in decreasing order; reverse for plotting\n",
    "prec_sk_inc = prec_sk[::-1]\n",
    "rec_sk_inc = rec_sk[::-1]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=rec_sk_inc,\n",
    "        y=prec_sk_inc,\n",
    "        mode=\"lines\",\n",
    "        line_shape=\"hv\",\n",
    "        name=\"sklearn precision_recall_curve\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"sklearn PR curve (AP={ap_sk:.3f})\",\n",
    "    xaxis_title=\"Recall\",\n",
    "    yaxis_title=\"Precision\",\n",
    "    xaxis=dict(range=[0, 1.02]),\n",
    "    yaxis=dict(range=[0, 1.02]),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "ap_np, ap_sk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Using AP to optimize a simple model (from-scratch logistic regression)\n",
    "\n",
    "AP depends on **sorting/ranking**, so it is not nicely differentiable with respect to model parameters.\n",
    "In practice you usually:\n",
    "- **train** a model with a differentiable surrogate loss (e.g., log-loss),\n",
    "- **select** hyperparameters / stop training using a metric like AP on a validation set.\n",
    "\n",
    "Below we train logistic regression from scratch with gradient descent, and use **validation AP** to pick:\n",
    "- the best epoch (early stopping)\n",
    "- an $\\ell_2$ regularization strength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic imbalanced classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.95, 0.05],\n",
    "    class_sep=1.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, random_state=42, stratify=y_tmp\n",
    ")\n",
    "\n",
    "# Standardize features using train statistics\n",
    "mu = X_train.mean(axis=0)\n",
    "sigma = X_train.std(axis=0)\n",
    "sigma = np.where(sigma == 0, 1.0, sigma)\n",
    "\n",
    "X_train_s = (X_train - mu) / sigma\n",
    "X_val_s = (X_val - mu) / sigma\n",
    "X_test_s = (X_test - mu) / sigma\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "(y_train.mean(), y_val.mean(), y_test.mean(), X_train_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    z = np.asarray(z)\n",
    "    out = np.empty_like(z, dtype=float)\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    exp_z = np.exp(z[~pos])\n",
    "    out[~pos] = exp_z / (1.0 + exp_z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict_proba_logreg(X, w, b):\n",
    "    return sigmoid(X @ w + b)\n",
    "\n",
    "\n",
    "def log_loss(y_true, y_prob, l2=0.0, w=None, eps=1e-15):\n",
    "    y_true = np.asarray(y_true).astype(float)\n",
    "    y_prob = np.clip(np.asarray(y_prob).astype(float), eps, 1 - eps)\n",
    "    loss = -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n",
    "    if l2 and w is not None:\n",
    "        loss = loss + 0.5 * l2 * float(np.sum(w * w))\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def fit_logreg_gd(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    lr=0.1,\n",
    "    l2=0.0,\n",
    "    n_epochs=800,\n",
    "    eval_every=10,\n",
    "):\n",
    "    n_samples, n_features = X_train.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "\n",
    "    best = {\"ap\": -np.inf, \"epoch\": 0, \"w\": w.copy(), \"b\": b}\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"val_ap\": []}\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        p = predict_proba_logreg(X_train, w, b)\n",
    "        err = p - y_train\n",
    "\n",
    "        grad_w = (X_train.T @ err) / n_samples + l2 * w\n",
    "        grad_b = float(np.mean(err))\n",
    "\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "        if epoch == 1 or epoch % eval_every == 0:\n",
    "            train_loss = log_loss(y_train, predict_proba_logreg(X_train, w, b), l2=l2, w=w)\n",
    "            val_scores = predict_proba_logreg(X_val, w, b)\n",
    "            val_ap = average_precision_score_np(y_val, val_scores)\n",
    "\n",
    "            history[\"epoch\"].append(epoch)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_ap\"].append(val_ap)\n",
    "\n",
    "            if val_ap > best[\"ap\"]:\n",
    "                best = {\"ap\": val_ap, \"epoch\": epoch, \"w\": w.copy(), \"b\": b}\n",
    "\n",
    "    return best, pd.DataFrame(history)\n",
    "\n",
    "\n",
    "# Train once and track validation AP (early stopping)\n",
    "best_run, hist = fit_logreg_gd(\n",
    "    X_train_s, y_train, X_val_s, y_val, lr=0.1, l2=0.1, n_epochs=800, eval_every=10\n",
    ")\n",
    "\n",
    "best_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist[\"epoch\"],\n",
    "        y=hist[\"train_loss\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"train log-loss\",\n",
    "        yaxis=\"y1\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist[\"epoch\"],\n",
    "        y=hist[\"val_ap\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"val AP\",\n",
    "        yaxis=\"y2\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=best_run[\"epoch\"],\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"best val AP @ epoch {best_run['epoch']}\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Training curve: optimize log-loss, select by validation AP\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis=dict(title=\"Train log-loss\"),\n",
    "    yaxis2=dict(title=\"Validation AP\", overlaying=\"y\", side=\"right\", range=[0, 1.0]),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "test_ap = average_precision_score_np(\n",
    "    y_test, predict_proba_logreg(X_test_s, best_run[\"w\"], best_run[\"b\"])\n",
    ")\n",
    "test_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use validation AP to choose a hyperparameter (L2 regularization strength)\n",
    "l2_grid = np.logspace(-4, 0, 7)\n",
    "results = []\n",
    "best_overall = None\n",
    "\n",
    "for l2 in l2_grid:\n",
    "    best, _hist = fit_logreg_gd(\n",
    "        X_train_s, y_train, X_val_s, y_val, lr=0.1, l2=float(l2), n_epochs=800, eval_every=20\n",
    "    )\n",
    "    results.append({\"l2\": float(l2), \"best_val_ap\": best[\"ap\"], \"best_epoch\": best[\"epoch\"]})\n",
    "    if best_overall is None or best[\"ap\"] > best_overall[\"best_val_ap\"]:\n",
    "        best_overall = {\"l2\": float(l2), \"best_val_ap\": best[\"ap\"], \"best\": best}\n",
    "\n",
    "df_grid = pd.DataFrame(results).sort_values(\"l2\")\n",
    "df_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    df_grid,\n",
    "    x=\"l2\",\n",
    "    y=\"best_val_ap\",\n",
    "    markers=True,\n",
    "    log_x=True,\n",
    "    title=\"Validation AP vs L2 strength (pick the maximum)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"L2 strength (lambda)\", yaxis_title=\"Best validation AP\")\n",
    "fig.show()\n",
    "\n",
    "best_l2 = best_overall[\"l2\"]\n",
    "w_star = best_overall[\"best\"][\"w\"]\n",
    "b_star = best_overall[\"best\"][\"b\"]\n",
    "\n",
    "test_ap_star = average_precision_score_np(y_test, predict_proba_logreg(X_test_s, w_star, b_star))\n",
    "best_l2, best_overall[\"best_val_ap\"], test_ap_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Pros, cons, and common pitfalls\n",
    "\n",
    "### Pros\n",
    "- **Good for imbalanced data**: focuses on the positive class; baseline is the prevalence.\n",
    "- **Threshold-free** summary: evaluates the whole ranking, not just one operating point.\n",
    "- **Ranking metric**: invariant to any strictly increasing transform of scores.\n",
    "\n",
    "### Cons / pitfalls\n",
    "- **Depends on prevalence**: AP values aren’t directly comparable across datasets with different base rates.\n",
    "- **High variance with few positives**: a handful of positives can swing AP a lot.\n",
    "- **Not symmetric**: swapping the positive/negative label changes AP.\n",
    "- **Not differentiable** in a simple way: optimizing AP directly usually requires surrogate losses (pairwise ranking, smooth approximations, etc.).\n",
    "- **Not an operating-point metric**: if you care about a specific constraint (e.g., precision ≥ 0.9), also inspect the PR curve or use precision@k / recall@k.\n",
    "- **Integration convention**: AP (step function) differs from trapezoidal AUPRC; don’t mix them silently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Show numerically (with random data) that:\n",
    "- the step-area formula $\\sum_n (R_n - R_{n-1})P_n$ equals the “mean precision at true-positive ranks” formula.\n",
    "\n",
    "2) Compare ROC AUC vs AP on a heavily imbalanced dataset. Create two models with similar ROC AUC but very different AP.\n",
    "\n",
    "3) Multilabel: generate labels with different prevalences per label and compare `average=\"macro\"` vs `average=\"micro\"`.\n",
    "\n",
    "## References\n",
    "- scikit-learn API docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
    "- scikit-learn PR curve: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}