{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion_matrix_at_thresholds\n",
    "\n",
    "Compute confusion matrices across multiple probability thresholds.\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Intuition**: compute confusion matrices across multiple thresholds to see the precision/recall trade-off.\n",
    "- **Example use**: thresholding calibrated probabilities or decision scores.\n",
    "- **Pitfalls**: threshold choice should reflect business costs; uncalibrated scores can mislead.\n",
    "\n",
    "## Example\n",
    "\n",
    "Small, self-contained example:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Binary labels and probability-like scores\n",
    "y_true = np.array([0, 1, 1, 0, 1, 0])\n",
    "y_score = np.array([0.05, 0.9, 0.6, 0.4, 0.2, 0.8])\n",
    "\n",
    "thresholds = [0.2, 0.5, 0.8]\n",
    "matrices = {t: confusion_matrix(y_true, y_score >= t) for t in thresholds}\n",
    "matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/api/sklearn.metrics.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}