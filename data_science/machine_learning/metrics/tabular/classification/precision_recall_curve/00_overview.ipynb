{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precision_recall_curve\n",
    "\n",
    "Compute precision-recall pairs for different probability thresholds.\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Intuition**: shows precision/recall trade-offs across thresholds using **scores**, not hard labels.\n",
    "- **Outputs**: returns arrays of precision, recall, and thresholds for the positive class.\n",
    "- **Pitfalls**: defined for binary; for multiclass use one-vs-rest. Use `y_score` from `predict_proba` or `decision_function`.\n",
    "\n",
    "## Example\n",
    "\n",
    "Small, self-contained example:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Scores for the positive class\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_score = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "precision, recall, thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/api/sklearn.metrics.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}