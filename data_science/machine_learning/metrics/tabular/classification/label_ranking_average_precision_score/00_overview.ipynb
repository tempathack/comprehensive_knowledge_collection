{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label_ranking_average_precision_score\n",
    "\n",
    "Compute label ranking average precision (LRAP).\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Intuition**: for multilabel ranking, measures how high true labels are ranked; 1.0 is perfect.\n",
    "- **Inputs**: `y_true` is a binary indicator matrix; `y_score` contains ranking scores.\n",
    "- **Pitfalls**: requires scores, not hard predictions; ties can reduce the score.\n",
    "\n",
    "## Example\n",
    "\n",
    "Small, self-contained example:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "# 3 samples, 3 labels\n",
    "y_true = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0],\n",
    "])\n",
    "y_score = np.array([\n",
    "    [0.9, 0.1, 0.8],\n",
    "    [0.2, 0.7, 0.1],\n",
    "    [0.6, 0.4, 0.3],\n",
    "])\n",
    "\n",
    "label_ranking_average_precision_score(y_true, y_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/api/sklearn.metrics.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}