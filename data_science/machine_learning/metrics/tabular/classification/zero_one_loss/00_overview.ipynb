{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zero_one_loss (classification error / 1 - accuracy)\n",
    "\n",
    "`zero_one_loss` is the simplest classification loss: it counts how often the predicted label differs from the true label.\n",
    "\n",
    "It is a great **evaluation** metric for \"did we get the label right?\", but a poor **training** objective for gradient-based optimization because it is discontinuous / non-differentiable.\n",
    "\n",
    "## Learning goals\n",
    "- write the binary and multiclass definitions in clean notation\n",
    "- understand the link to accuracy and (for binary) the confusion matrix\n",
    "- implement `zero_one_loss` in NumPy (with optional `sample_weight`)\n",
    "- build intuition via threshold and parameter-surface plots (Plotly)\n",
    "- see how 0-1 loss is used for selection/optimization in practice (threshold tuning)\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import zero_one_loss\n",
    "```\n",
    "\n",
    "## Table of contents\n",
    "1. Definition and notation\n",
    "2. Intuition: thresholds and decision rules (plots)\n",
    "3. NumPy implementation + sanity checks\n",
    "4. Using 0-1 loss for selection/optimization\n",
    "5. Pros, cons, pitfalls\n",
    "\n",
    "## References (quick)\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/api/sklearn.metrics.html\n",
    "- ESL (Hastie, Tibshirani, Friedman): \"The Elements of Statistical Learning\" (classification + empirical risk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score, zero_one_loss as sk_zero_one_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Definition and notation\n",
    "\n",
    "Assume we have $n$ examples.\n",
    "\n",
    "- True label: $y_i$\n",
    "- Predicted label: $\\hat{y}_i$\n",
    "\n",
    "### Per-example 0-1 loss\n",
    "\n",
    "$$\n",
    "\\ell_i = \\mathbb{1}[\\hat{y}_i \\ne y_i]\n",
    "$$\n",
    "\n",
    "### Aggregate (count vs mean)\n",
    "\n",
    "Unnormalized (count of mistakes):\n",
    "\n",
    "$$\n",
    "L_{\\text{count}} = \\sum_{i=1}^n \\mathbb{1}[\\hat{y}_i \\ne y_i]\n",
    "$$\n",
    "\n",
    "Normalized (fraction of mistakes):\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}[\\hat{y}_i \\ne y_i]\n",
    "$$\n",
    "\n",
    "This is exactly:\n",
    "\n",
    "$$\n",
    "L = 1 - \\text{accuracy}.\n",
    "$$\n",
    "\n",
    "### Sample-weighted version\n",
    "\n",
    "Given weights $w_i \\ge 0$ (e.g. importance weights, class weights), the normalized weighted 0-1 loss is:\n",
    "\n",
    "$$\n",
    "L_w = \\frac{\\sum_{i=1}^n w_i\\,\\mathbb{1}[\\hat{y}_i \\ne y_i]}{\\sum_{i=1}^n w_i}.\n",
    "$$\n",
    "\n",
    "### Multiclass and multilabel\n",
    "\n",
    "- Multiclass ($K$ classes): $y_i \\in \\{0,\\dots,K-1\\}$ and the same formula applies.\n",
    "- Multilabel / multioutput: $y_i$ is a vector. scikit-learn's `zero_one_loss` uses **subset 0-1 loss**:\n",
    "\n",
    "$$\n",
    "\\ell_i = \\mathbb{1}[\\hat{\\mathbf{y}}_i \\ne \\mathbf{y}_i]\n",
    "$$\n",
    "\n",
    "i.e. the whole label vector must match exactly. (This is often stricter than what you want; see pitfalls.)\n",
    "\n",
    "### Bayes optimal decision rule (why argmax probability is optimal)\n",
    "\n",
    "Let the model output class probabilities $p_k(x) = P(Y=k\\mid X=x)$. The classifier that minimizes the **expected** 0-1 loss is:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\arg\\max_k\\ p_k(x).\n",
    "$$\n",
    "\n",
    "Binary case with $\\eta(x)=P(Y=1\\mid X=x)$ and equal misclassification costs:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x)=\\mathbb{1}[\\eta(x)\\ge 1/2].\n",
    "$$\n",
    "\n",
    "With costs $c_{01}$ (false positive) and $c_{10}$ (false negative), the optimal threshold becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x)=\\mathbb{1}\\Big[\\eta(x)\\ge \\frac{c_{01}}{c_{01}+c_{10}}\\Big].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return np.where(z >= 0, 1.0 / (1.0 + np.exp(-z)), np.exp(z) / (1.0 + np.exp(z)))\n",
    "\n",
    "\n",
    "def zero_one_loss_np(y_true, y_pred, *, normalize=True, sample_weight=None):\n",
    "    \"\"\"NumPy implementation of scikit-learn's zero_one_loss.\n",
    "\n",
    "    - If y is 1D: counts elementwise mismatches.\n",
    "    - If y is 2D (multilabel / multioutput): uses subset 0-1 loss (row must match exactly).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true {y_true.shape} vs y_pred {y_pred.shape}\")\n",
    "    if y_true.ndim == 0:\n",
    "        raise ValueError(\"y_true must be 1D or 2D\")\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        incorrect = (y_true != y_pred)\n",
    "    else:\n",
    "        incorrect = np.any(y_true != y_pred, axis=1)\n",
    "\n",
    "    incorrect = incorrect.astype(float)\n",
    "    n = incorrect.shape[0]\n",
    "\n",
    "    if sample_weight is None:\n",
    "        total = float(incorrect.sum())\n",
    "        return total / n if normalize else total\n",
    "\n",
    "    w = np.asarray(sample_weight, dtype=float)\n",
    "    if w.ndim != 1 or w.shape[0] != n:\n",
    "        raise ValueError(f\"sample_weight must be shape (n,), got {w.shape}\")\n",
    "\n",
    "    total = float(np.sum(w * incorrect))\n",
    "    if not normalize:\n",
    "        return total\n",
    "\n",
    "    w_sum = float(w.sum())\n",
    "    if w_sum == 0:\n",
    "        return 0.0\n",
    "    return total / w_sum\n",
    "\n",
    "\n",
    "def predict_labels_from_proba(p, *, threshold=0.5):\n",
    "    \"\"\"Convert probabilities to hard labels.\n",
    "\n",
    "    - Binary: p is (n,) or (n,2) (assumes column 1 is P(y=1)).\n",
    "    - Multiclass: p is (n,K) -> argmax.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if p.ndim == 1:\n",
    "        return (p >= threshold).astype(int)\n",
    "    if p.ndim == 2 and p.shape[1] == 2:\n",
    "        return (p[:, 1] >= threshold).astype(int)\n",
    "    if p.ndim == 2:\n",
    "        return np.argmax(p, axis=1)\n",
    "    raise ValueError(f\"p must be 1D or 2D, got shape {p.shape}\")\n",
    "\n",
    "\n",
    "def zero_one_loss_from_proba(\n",
    "    y_true,\n",
    "    p,\n",
    "    *,\n",
    "    threshold=0.5,\n",
    "    normalize=True,\n",
    "    sample_weight=None,\n",
    "):\n",
    "    y_pred = predict_labels_from_proba(p, threshold=threshold)\n",
    "    return zero_one_loss_np(y_true, y_pred, normalize=normalize, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "def log_loss_binary(y_true, p, *, sample_weight=None, eps=1e-15):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    per_sample = -(y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n",
    "    if sample_weight is None:\n",
    "        return float(per_sample.mean())\n",
    "    w = np.asarray(sample_weight, dtype=float)\n",
    "    w_sum = float(w.sum())\n",
    "    if w_sum == 0:\n",
    "        return 0.0\n",
    "    return float(np.sum(w * per_sample) / w_sum)\n",
    "\n",
    "\n",
    "def best_threshold_zero_one(y_true, p, *, sample_weight=None, normalize=True):\n",
    "    \"\"\"Find an exact minimizer over thresholds t in [0, 1] (binary, rule: p>=t -> 1).\n",
    "\n",
    "    The predictions only change when t crosses a value in p, so evaluating t over unique p values\n",
    "    (plus the endpoints 0 and 1) is enough to find the exact optimum.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if y_true.shape != p.shape or p.ndim != 1:\n",
    "        raise ValueError(\"y_true and p must be 1D arrays of the same shape\")\n",
    "\n",
    "    if sample_weight is None:\n",
    "        w = np.ones_like(p, dtype=float)\n",
    "    else:\n",
    "        w = np.asarray(sample_weight, dtype=float)\n",
    "        if w.shape != p.shape:\n",
    "            raise ValueError(\"sample_weight must have the same shape as p\")\n",
    "\n",
    "    order = np.argsort(p)\n",
    "    p_s = p[order]\n",
    "    y_s = y_true[order]\n",
    "    w_s = w[order]\n",
    "\n",
    "    w_pos = w_s * (y_s == 1)\n",
    "    w_neg = w_s * (y_s == 0)\n",
    "    cum_pos = np.cumsum(w_pos)\n",
    "    cum_neg = np.cumsum(w_neg)\n",
    "    total_neg = float(cum_neg[-1])\n",
    "\n",
    "    uniq = np.unique(p_s)\n",
    "    thresholds = np.unique(np.concatenate(([0.0], uniq, [1.0])))\n",
    "    start = np.searchsorted(p_s, thresholds, side=\"left\")\n",
    "    before = start - 1\n",
    "    pos_below = np.where(before >= 0, cum_pos[before], 0.0)\n",
    "    neg_below = np.where(before >= 0, cum_neg[before], 0.0)\n",
    "\n",
    "    misclassified = pos_below + (total_neg - neg_below)\n",
    "    if normalize:\n",
    "        denom = float(w_s.sum())\n",
    "        losses = misclassified / denom if denom > 0 else np.zeros_like(misclassified)\n",
    "    else:\n",
    "        losses = misclassified\n",
    "\n",
    "    best_j = int(np.argmin(losses))\n",
    "    return float(thresholds[best_j]), float(losses[best_j])\n",
    "\n",
    "\n",
    "def standardize_fit_transform(X):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    std = np.where(std == 0, 1.0, std)\n",
    "    return (X - mean) / std, mean, std\n",
    "\n",
    "\n",
    "def standardize_transform(X, mean, std):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    std = np.where(std == 0, 1.0, std)\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def fit_logistic_regression_gd(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val=None,\n",
    "    y_val=None,\n",
    "    *,\n",
    "    lr=0.2,\n",
    "    n_steps=300,\n",
    "    l2=0.0,\n",
    "    threshold=0.5,\n",
    "):\n",
    "    X_train = np.asarray(X_train, dtype=float)\n",
    "    y_train = np.asarray(y_train, dtype=int)\n",
    "\n",
    "    n, d = X_train.shape\n",
    "    w = np.zeros(d, dtype=float)\n",
    "    b = 0.0\n",
    "\n",
    "    hist = {\n",
    "        \"step\": [],\n",
    "        \"train_log_loss\": [],\n",
    "        \"train_zero_one\": [],\n",
    "        \"val_log_loss\": [],\n",
    "        \"val_zero_one\": [],\n",
    "    }\n",
    "\n",
    "    for step in range(n_steps + 1):\n",
    "        z_train = X_train @ w + b\n",
    "        p_train = sigmoid(z_train)\n",
    "\n",
    "        hist[\"step\"].append(step)\n",
    "        hist[\"train_log_loss\"].append(log_loss_binary(y_train, p_train))\n",
    "        hist[\"train_zero_one\"].append(zero_one_loss_from_proba(y_train, p_train, threshold=threshold))\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            z_val = np.asarray(X_val, dtype=float) @ w + b\n",
    "            p_val = sigmoid(z_val)\n",
    "            hist[\"val_log_loss\"].append(log_loss_binary(y_val, p_val))\n",
    "            hist[\"val_zero_one\"].append(zero_one_loss_from_proba(y_val, p_val, threshold=threshold))\n",
    "        else:\n",
    "            hist[\"val_log_loss\"].append(np.nan)\n",
    "            hist[\"val_zero_one\"].append(np.nan)\n",
    "\n",
    "        if step == n_steps:\n",
    "            break\n",
    "\n",
    "        # gradient of mean log loss (plus optional L2 penalty)\n",
    "        grad = p_train - y_train\n",
    "        grad_w = (X_train.T @ grad) / n + l2 * w\n",
    "        grad_b = float(grad.mean())\n",
    "\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    return w, b, hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition: thresholds and decision rules (plots)\n",
    "\n",
    "0-1 loss depends only on **hard labels**.\n",
    "\n",
    "In binary classification, many models output a score or probability $\\hat{p}(y=1\\mid x)$.\n",
    "To turn that into a label we pick a threshold $t$:\n",
    "\n",
    "$$\n",
    "\\hat{y}(t) = \\mathbb{1}[\\hat{p} \\ge t].\n",
    "$$\n",
    "\n",
    "As you vary $t$, the predictions only change when $t$ crosses one of the predicted probabilities.\n",
    "So the empirical 0-1 loss as a function of $t$ is a **step function** (flat most of the time, then jumps).\n",
    "\n",
    "This is a key reason 0-1 loss is not used as a smooth training objective: small parameter changes often produce *no* change in 0-1 loss until a point flips sides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 250\n",
    "x = rng.normal(size=n)\n",
    "p_true = sigmoid(1.5 * x - 0.3)\n",
    "y = rng.binomial(1, p_true)\n",
    "\n",
    "# Pretend these are predicted probabilities from an imperfect model\n",
    "p_hat = np.clip(p_true + 0.15 * rng.normal(size=n), 1e-3, 1 - 1e-3)\n",
    "\n",
    "thresholds = np.linspace(0.0, 1.0, 601)\n",
    "losses = np.array([zero_one_loss_from_proba(y, p_hat, threshold=t) for t in thresholds])\n",
    "acc = 1.0 - losses\n",
    "\n",
    "t_best, _ = best_threshold_zero_one(y, p_hat)\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=thresholds,\n",
    "        y=losses,\n",
    "        name=\"zero-one loss\",\n",
    "        mode=\"lines\",\n",
    "        line_shape=\"hv\",\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=thresholds,\n",
    "        y=acc,\n",
    "        name=\"accuracy (1 - loss)\",\n",
    "        mode=\"lines\",\n",
    "        line_shape=\"hv\",\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig.add_vline(x=0.5, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
    "fig.add_vline(x=t_best, line_dash=\"dot\", line_color=\"crimson\")\n",
    "\n",
    "fig.update_xaxes(title_text=\"threshold t\")\n",
    "fig.update_yaxes(title_text=\"0-1 loss\", secondary_y=False, range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"accuracy\", secondary_y=True, range=[0, 1])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"0-1 loss is a step function of the threshold (one optimal t ≈ {t_best:.3f})\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0),\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) NumPy implementation: sanity checks\n",
    "\n",
    "A key property: 0-1 loss is **insensitive to confidence**.\n",
    "\n",
    "- predicting 0.51 vs 0.99 for the positive class gives the same 0-1 outcome (as long as the thresholded label is the same)\n",
    "- but a probabilistic loss like log loss will strongly prefer 0.99 over 0.51 when the true label is 1\n",
    "\n",
    "Let's verify that our NumPy version matches scikit-learn and highlight the \"confidence blindness\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 1, 1, 0, 0])\n",
    "y_pred = np.array([1, 0, 0, 1, 0, 1])\n",
    "\n",
    "print(\"numpy (mean):\", zero_one_loss_np(y_true, y_pred))\n",
    "print(\"sklearn (mean):\", sk_zero_one_loss(y_true, y_pred))\n",
    "print(\"1 - accuracy_score:\", 1 - accuracy_score(y_true, y_pred))\n",
    "print(\"numpy (count):\", zero_one_loss_np(y_true, y_pred, normalize=False))\n",
    "print(\"sklearn (count):\", sk_zero_one_loss(y_true, y_pred, normalize=False))\n",
    "\n",
    "w = np.array([1, 1, 5, 1, 1, 1], dtype=float)\n",
    "print(\"\\nweighted numpy (mean):\", zero_one_loss_np(y_true, y_pred, sample_weight=w))\n",
    "print(\"weighted sklearn (mean):\", sk_zero_one_loss(y_true, y_pred, sample_weight=w))\n",
    "\n",
    "# multilabel / multioutput: subset 0-1 loss (row must match exactly)\n",
    "y_true_ml = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1]])\n",
    "y_pred_ml = np.array([[1, 0, 1], [1, 0, 0], [0, 1, 1]])\n",
    "print(\"\\nmultilabel numpy:\", zero_one_loss_np(y_true_ml, y_pred_ml))\n",
    "print(\"multilabel sklearn:\", sk_zero_one_loss(y_true_ml, y_pred_ml))\n",
    "\n",
    "# confidence blindness: same hard predictions, different probabilities\n",
    "y_true = np.array([1, 1, 1, 0, 0])\n",
    "p_soft = np.array([0.51, 0.55, 0.52, 0.49, 0.45])\n",
    "p_confident = np.array([0.99, 0.90, 0.80, 0.20, 0.01])\n",
    "\n",
    "print(\"\\n0-1 loss (soft):\", zero_one_loss_from_proba(y_true, p_soft))\n",
    "print(\"0-1 loss (confident):\", zero_one_loss_from_proba(y_true, p_confident))\n",
    "print(\"log loss (soft):\", log_loss_binary(y_true, p_soft))\n",
    "print(\"log loss (confident):\", log_loss_binary(y_true, p_confident))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Using 0-1 loss for selection/optimization\n",
    "\n",
    "Because 0-1 loss is a **step function** in the threshold (and in the model parameters), it is typically used as a **selection criterion** rather than a differentiable training objective.\n",
    "\n",
    "A very common and practical \"optimization\" task is threshold tuning:\n",
    "\n",
    "$$\n",
    "t^* \\in \\arg\\min_{t\\in[0,1]}\\ L\\bigl(y,\\ \\mathbb{1}[\\hat{p}\\ge t]\\bigr).\n",
    "$$\n",
    "\n",
    "This works well because it is a **1D search** (grid search or exact search over unique probabilities).\n",
    "\n",
    "If you care more about one class (asymmetric costs), you can encode that with `sample_weight` (or with an explicit cost-sensitive threshold rule).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search threshold (approximate)\n",
    "thresholds = np.linspace(0.0, 1.0, 2001)\n",
    "losses_grid = np.array([zero_one_loss_from_proba(y, p_hat, threshold=t) for t in thresholds])\n",
    "min_loss_grid = float(losses_grid.min())\n",
    "min_idx = np.where(losses_grid == min_loss_grid)[0]\n",
    "t_grid = float(thresholds[int(min_idx[0])])\n",
    "t_grid_low = float(thresholds[int(min_idx[0])])\n",
    "t_grid_high = float(thresholds[int(min_idx[-1])])\n",
    "\n",
    "# Exact threshold search (evaluate unique p_hat values)\n",
    "t_exact, loss_exact = best_threshold_zero_one(y, p_hat)\n",
    "\n",
    "print(f\"grid-search min loss: {min_loss_grid:.4f} (t in [{t_grid_low:.4f}, {t_grid_high:.4f}])\")\n",
    "print(f\"exact-search min loss: {loss_exact:.4f} (one optimal t={t_exact:.4f})\")\n",
    "\n",
    "# Weighted: make mistakes on positives 3x more costly\n",
    "w_pos = np.where(y == 1, 3.0, 1.0)\n",
    "t_w, loss_w = best_threshold_zero_one(y, p_hat, sample_weight=w_pos)\n",
    "print(f\"weighted best t: {t_w:.4f} (loss={loss_w:.4f})\")\n",
    "\n",
    "losses_unweighted = np.array([zero_one_loss_from_proba(y, p_hat, threshold=t) for t in thresholds])\n",
    "losses_weighted = np.array([zero_one_loss_from_proba(y, p_hat, threshold=t, sample_weight=w_pos) for t in thresholds])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=losses_unweighted, mode=\"lines\", line_shape=\"hv\", name=\"unweighted\"))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=losses_weighted, mode=\"lines\", line_shape=\"hv\", name=\"weighted (pos×3)\"))\n",
    "fig.add_vline(x=t_exact, line_dash=\"dot\", line_color=\"black\")\n",
    "fig.add_vline(x=t_w, line_dash=\"dot\", line_color=\"crimson\")\n",
    "fig.update_layout(title=\"Threshold tuning for 0-1 loss (unweighted vs weighted)\")\n",
    "fig.update_xaxes(title_text=\"threshold t\")\n",
    "fig.update_yaxes(title_text=\"0-1 loss\", range=[0, 1])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Why 0-1 loss is hard to optimize directly (and what we do instead)\n",
    "\n",
    "If a classifier depends on parameters $\\theta$ (e.g. linear model weights), the empirical 0-1 loss is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}[\\hat{y}(x_i;\\theta) \\ne y_i].\n",
    "$$\n",
    "\n",
    "This function is:\n",
    "- **discontinuous / non-differentiable** (jumps when a point flips sides)\n",
    "- typically **non-convex** and full of plateaus\n",
    "- hard to minimize exactly for most hypothesis classes\n",
    "\n",
    "So in practice we train with a **surrogate loss** that is smooth and easier to optimize (e.g. log loss / cross-entropy for logistic regression), and then evaluate with 0-1 loss.\n",
    "\n",
    "The plots below compare the loss landscapes for a simple 1D logistic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 120\n",
    "x = rng.normal(size=n)\n",
    "x = (x - x.mean()) / x.std()\n",
    "\n",
    "p_true = sigmoid(2.0 * x - 0.4)\n",
    "y = rng.binomial(1, p_true)\n",
    "\n",
    "w_grid = np.linspace(-6, 6, 151)\n",
    "b_grid = np.linspace(-6, 6, 151)\n",
    "\n",
    "Z = x[:, None, None] * w_grid[None, None, :] + b_grid[None, :, None]\n",
    "P = sigmoid(Z)\n",
    "\n",
    "y_pred = (P >= 0.5).astype(int)\n",
    "loss01 = (y[:, None, None] != y_pred).mean(axis=0)\n",
    "\n",
    "eps = 1e-12\n",
    "P_clip = np.clip(P, eps, 1 - eps)\n",
    "losslog = -(y[:, None, None] * np.log(P_clip) + (1 - y[:, None, None]) * np.log(1 - P_clip)).mean(axis=0)\n",
    "\n",
    "# Gradient descent on log loss (same simple 1D model)\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "lr = 0.8\n",
    "w_path = [w]\n",
    "b_path = [b]\n",
    "for _ in range(40):\n",
    "    z = w * x + b\n",
    "    p = sigmoid(z)\n",
    "    grad = p - y\n",
    "    grad_w = float(np.mean(grad * x))\n",
    "    grad_b = float(np.mean(grad))\n",
    "    w -= lr * grad_w\n",
    "    b -= lr * grad_b\n",
    "    w_path.append(w)\n",
    "    b_path.append(b)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"0-1 loss (threshold=0.5)\", \"log loss (smooth surrogate)\"),\n",
    "    horizontal_spacing=0.12,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(x=w_grid, y=b_grid, z=loss01, zmin=0, zmax=1, colorbar=dict(title=\"0-1\")),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Heatmap(x=w_grid, y=b_grid, z=losslog, colorbar=dict(title=\"log\")),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=w_path, y=b_path, mode=\"lines+markers\", name=\"GD path\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=w_path, y=b_path, mode=\"lines+markers\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"w\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"w\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"b\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"b\", row=1, col=2)\n",
    "fig.update_layout(title=\"0-1 loss is piecewise-constant; log loss provides a smooth optimization landscape\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Example: train logistic regression (from scratch), evaluate 0-1 loss\n",
    "\n",
    "We'll fit a simple logistic regression model by minimizing **log loss** with gradient descent, while tracking **0-1 loss** on train/validation.\n",
    "\n",
    "Model:\n",
    "\n",
    "$$\n",
    "\\hat{p}_i = \\sigma(x_i^\\top w + b),\\qquad \\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "Training objective (mean log loss):\n",
    "\n",
    "$$\n",
    "J(w,b) = -\\frac{1}{n}\\sum_{i=1}^n \\Big(y_i\\log\\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\Big).\n",
    "$$\n",
    "\n",
    "Then we compute 0-1 loss by thresholding $\\hat{p}$ at $t=0.5$ (and optionally tuning $t$ on validation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(\n",
    "    n_samples=900,\n",
    "    centers=2,\n",
    "    n_features=2,\n",
    "    cluster_std=2.2,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=0,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "X_train_s, mean, std = standardize_fit_transform(X_train)\n",
    "X_val_s = standardize_transform(X_val, mean, std)\n",
    "\n",
    "w, b, hist = fit_logistic_regression_gd(\n",
    "    X_train_s,\n",
    "    y_train,\n",
    "    X_val=X_val_s,\n",
    "    y_val=y_val,\n",
    "    lr=0.2,\n",
    "    n_steps=250,\n",
    "    l2=0.01,\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "p_val = sigmoid(X_val_s @ w + b)\n",
    "val_loss_05 = zero_one_loss_from_proba(y_val, p_val, threshold=0.5)\n",
    "t_best, val_loss_best = best_threshold_zero_one(y_val, p_val)\n",
    "\n",
    "print(f\"val 0-1 loss @ t=0.5: {val_loss_05:.4f}\")\n",
    "print(f\"best val threshold: {t_best:.4f} (val 0-1 loss={val_loss_best:.4f})\")\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=hist[\"step\"], y=hist[\"train_log_loss\"], name=\"train log loss\"), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=hist[\"step\"], y=hist[\"val_log_loss\"], name=\"val log loss\"), secondary_y=False)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hist[\"step\"], y=hist[\"train_zero_one\"], name=\"train 0-1 loss\", line_shape=\"hv\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hist[\"step\"], y=hist[\"val_zero_one\"], name=\"val 0-1 loss\", line_shape=\"hv\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"gradient descent step\")\n",
    "fig.update_yaxes(title_text=\"log loss\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"0-1 loss\", secondary_y=True, range=[0, 1])\n",
    "fig.update_layout(title=\"Training with log loss; tracking 0-1 loss (step-like)\")\n",
    "fig.show()\n",
    "\n",
    "# Decision boundary visualization\n",
    "x0_min, x0_max = X_train_s[:, 0].min() - 0.8, X_train_s[:, 0].max() + 0.8\n",
    "x1_min, x1_max = X_train_s[:, 1].min() - 0.8, X_train_s[:, 1].max() + 0.8\n",
    "\n",
    "x0 = np.linspace(x0_min, x0_max, 220)\n",
    "x1 = np.linspace(x1_min, x1_max, 220)\n",
    "xx0, xx1 = np.meshgrid(x0, x1)\n",
    "grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "\n",
    "prob_grid = sigmoid(grid @ w + b).reshape(xx0.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=x0,\n",
    "        y=x1,\n",
    "        z=prob_grid,\n",
    "        contours=dict(start=0.0, end=1.0, size=0.1),\n",
    "        colorscale=\"RdBu\",\n",
    "        opacity=0.85,\n",
    "        colorbar=dict(title=\"P(y=1)\"),\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_train_s[:, 0],\n",
    "        y=X_train_s[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=y_train, colorscale=\"Viridis\", opacity=0.9, line=dict(width=0.2, color=\"black\")),\n",
    "        name=\"train points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(title=\"Logistic regression probabilities (0-1 loss comes from thresholding)\")\n",
    "fig.update_xaxes(title_text=\"x0 (standardized)\")\n",
    "fig.update_yaxes(title_text=\"x1 (standardized)\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / cons and when to use 0-1 loss\n",
    "\n",
    "### Pros\n",
    "- **Highly interpretable**: \"error rate\" (or # mistakes)\n",
    "- **Threshold/decision-rule focused**: directly measures what many applications care about (correct label)\n",
    "- **Works for multiclass** with no extra machinery\n",
    "- **Aligns with the Bayes classifier** under equal misclassification costs (argmax posterior)\n",
    "\n",
    "### Cons\n",
    "- **Non-differentiable / discontinuous** → not suitable as a gradient-based training loss\n",
    "- **Ignores confidence and calibration**: 0.51 and 0.99 are treated the same after thresholding\n",
    "- **Can be misleading under class imbalance** (a majority-class classifier can look good)\n",
    "- **Depends on the decision rule** (threshold choice, argmax ties, cost-sensitive adjustments)\n",
    "- **Multilabel subset 0-1 is very strict** (one wrong label makes the whole example wrong)\n",
    "\n",
    "### When it's a good choice\n",
    "- Reporting final performance when **all errors are equally costly**\n",
    "- Comparing classifiers after you have a clear, fixed **threshold / decision policy**\n",
    "- Hyperparameter selection when you truly care about **accuracy/error rate** (using a validation set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common pitfalls + diagnostics\n",
    "\n",
    "- **Class imbalance**: 0-1 loss/accuracy may hide poor minority performance. Also inspect the confusion matrix; consider balanced accuracy, F1, PR AUC.\n",
    "- **Wrong threshold**: if your positive class is rare or costs are asymmetric, $t=0.5$ is often not optimal; tune $t$ or use cost-sensitive decision rules.\n",
    "- **Multilabel strictness**: subset 0-1 can be too harsh; consider Hamming loss, Jaccard score, or per-label F1.\n",
    "- **Probability quality not measured**: two models can have the same 0-1 loss but very different calibration; also report log loss / Brier score if probabilities matter.\n",
    "- **Test-set threshold tuning**: choose thresholds/hyperparameters on validation (or via cross-validation), not on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Prove that normalized 0-1 loss is exactly $1-\\text{accuracy}$.\n",
    "2) Derive the cost-sensitive threshold $\\eta(x)\\ge \\frac{c_{01}}{c_{01}+c_{10}}$ from expected cost minimization.\n",
    "3) Construct two classifiers with the same 0-1 loss but very different log loss. When would you prefer each?\n",
    "4) Extend `best_threshold_zero_one` to return *all* thresholds achieving the minimum.\n",
    "5) For multilabel data, compare subset 0-1 loss vs Hamming loss on a synthetic example and interpret the difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn `zero_one_loss`: https://scikit-learn.org/stable/api/generated/sklearn.metrics.zero_one_loss.html\n",
    "- scikit-learn `accuracy_score`: https://scikit-learn.org/stable/api/generated/sklearn.metrics.accuracy_score.html\n",
    "- Hastie, Tibshirani, Friedman: *The Elements of Statistical Learning*, Ch. 2 (classification), Ch. 4 (linear methods for classification)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}