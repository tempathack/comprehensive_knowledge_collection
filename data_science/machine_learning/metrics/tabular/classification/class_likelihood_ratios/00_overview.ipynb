{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7823438",
   "metadata": {},
   "source": [
    "# class_likelihood_ratios (LR+ / LR-)\n",
    "\n",
    "Compute the **positive** and **negative likelihood ratios** for a **binary** classifier.\n",
    "\n",
    "In scikit-learn this is `sklearn.metrics.class_likelihood_ratios`.\n",
    "\n",
    "## Learning goals\n",
    "- Derive \\(LR_+\\) and \\(LR_-\\) from the confusion matrix\n",
    "- Interpret them as **odds multipliers** (pre-test \\(\\to\\) post-test probabilities)\n",
    "- Implement the metric from scratch in NumPy (weights + label ordering)\n",
    "- Visualize how likelihood ratios change with the decision threshold\n",
    "- Use likelihood ratios to pick an operating point (screening vs confirmation)\n",
    "\n",
    "## Prerequisites\n",
    "- Confusion matrix, sensitivity/specificity\n",
    "- Basic Bayes rule / odds\n",
    "- Logistic regression + ROC curves (helpful, but not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a896c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import class_likelihood_ratios, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399bf9e",
   "metadata": {},
   "source": [
    "## 1) Definition: likelihood ratios as conditional probability ratios\n",
    "\n",
    "Treat a classifier's prediction as a diagnostic test:\n",
    "\n",
    "- **test positive** \\(\\iff\\) predict the positive class\n",
    "- **test negative** \\(\\iff\\) predict the negative class\n",
    "\n",
    "The likelihood ratios compare how often the test is positive/negative under each true class:\n",
    "\n",
    "\\[\n",
    "LR_+ = \\frac{P(\\hat{y}=1 \\mid y=1)}{P(\\hat{y}=1 \\mid y=0)}\n",
    "\\qquad\n",
    "LR_- = \\frac{P(\\hat{y}=0 \\mid y=1)}{P(\\hat{y}=0 \\mid y=0)}.\n",
    "\\]\n",
    "\n",
    "**Why this is useful:** in *odds form* Bayes rule becomes a multiplication.\n",
    "\n",
    "Define odds for a probability \\(p\\):\n",
    "\n",
    "\\[\n",
    "\\operatorname{odds}(p) = \\frac{p}{1-p}.\n",
    "\\]\n",
    "\n",
    "Then the update is:\n",
    "\n",
    "\\[\n",
    "\\operatorname{odds}(y=1 \\mid \\text{test}+) = \\operatorname{odds}(y=1)\\cdot LR_+,\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\operatorname{odds}(y=1 \\mid \\text{test}-) = \\operatorname{odds}(y=1)\\cdot LR_-.\n",
    "\\]\n",
    "\n",
    "Converting odds back to probability:\n",
    "\n",
    "\\[\n",
    "p = \\frac{\\operatorname{odds}}{1 + \\operatorname{odds}}.\n",
    "\\]\n",
    "\n",
    "Equivalently in log-odds:\n",
    "\n",
    "\\[\n",
    "\\operatorname{logit}(p_{post}) = \\operatorname{logit}(p_{pre}) + \\log(LR).\n",
    "\\]\n",
    "\n",
    "**Key point:** \\(LR_+\\) and \\(LR_-\\) are functions of *sensitivity* and *specificity* (not prevalence),\n",
    "but turning them into **post-test probabilities** requires a **prior** (pre-test probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262be32",
   "metadata": {},
   "source": [
    "## 2) From confusion matrix to \\(LR_+\\) and \\(LR_-\\)\n",
    "\n",
    "For a binary classifier with positive class \\(y=1\\) and negative class \\(y=0\\):\n",
    "\n",
    "\\[\n",
    "\\begin{array}{c|cc}\n",
    "& \\hat{y}=0 & \\hat{y}=1\\\\\\hline\n",
    "y=0 & TN & FP\\\\\n",
    "y=1 & FN & TP\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "Define:\n",
    "\n",
    "- **Sensitivity / recall / true positive rate (TPR)**\n",
    "\n",
    "  \\[\n",
    "  \\text{TPR} = \\frac{TP}{TP+FN}\n",
    "  \\]\n",
    "\n",
    "- **Specificity / true negative rate (TNR)**\n",
    "\n",
    "  \\[\n",
    "  \\text{TNR} = \\frac{TN}{TN+FP}\n",
    "  \\]\n",
    "\n",
    "- **False positive rate (FPR)**: \\(\\text{FPR} = 1-\\text{TNR} = \\frac{FP}{TN+FP}\\)\n",
    "- **False negative rate (FNR)**: \\(\\text{FNR} = 1-\\text{TPR} = \\frac{FN}{TP+FN}\\)\n",
    "\n",
    "Then:\n",
    "\n",
    "\\[\n",
    "LR_+ = \\frac{\\text{TPR}}{\\text{FPR}} = \\frac{\\text{sensitivity}}{1-\\text{specificity}}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "LR_- = \\frac{\\text{FNR}}{\\text{TNR}} = \\frac{1-\\text{sensitivity}}{\\text{specificity}}.\n",
    "\\]\n",
    "\n",
    "A commonly used single-number summary is the **diagnostic odds ratio**:\n",
    "\n",
    "\\[\n",
    "\\text{DOR} = \\frac{LR_+}{LR_-} = \\frac{TP\\cdot TN}{FP\\cdot FN},\n",
    "\\]\n",
    "\n",
    "but note it can be undefined/infinite when \\(FP=0\\) or \\(FN=0\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae92979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_binary_labels(y_true, y_pred, labels=None):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([np.unique(y_true), np.unique(y_pred)]))\n",
    "        if labels.shape[0] != 2:\n",
    "            raise ValueError(f\"Expected 2 labels for binary classification, got {labels!r}\")\n",
    "        labels = np.sort(labels)  # sklearn default\n",
    "    else:\n",
    "        labels = np.asarray(labels)\n",
    "        if labels.shape[0] != 2:\n",
    "            raise ValueError(\"labels must be of length 2: [negative_class, positive_class]\")\n",
    "\n",
    "    neg_label, pos_label = labels[0], labels[1]\n",
    "    return neg_label, pos_label\n",
    "\n",
    "\n",
    "def confusion_counts_binary(y_true, y_pred, *, labels=None, sample_weight=None):\n",
    "    '''Return (tp, fp, tn, fn) as floats.'''\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    neg_label, pos_label = _infer_binary_labels(y_true, y_pred, labels=labels)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        w = np.ones_like(y_true, dtype=float)\n",
    "    else:\n",
    "        w = np.asarray(sample_weight, dtype=float)\n",
    "        if w.shape != y_true.shape:\n",
    "            raise ValueError(\"sample_weight must have shape (n_samples,)\")\n",
    "\n",
    "    is_pos_true = y_true == pos_label\n",
    "    is_pos_pred = y_pred == pos_label\n",
    "\n",
    "    tp = np.sum(w * (is_pos_true & is_pos_pred))\n",
    "    fp = np.sum(w * (~is_pos_true & is_pos_pred))\n",
    "    tn = np.sum(w * (~is_pos_true & ~is_pos_pred))\n",
    "    fn = np.sum(w * (is_pos_true & ~is_pos_pred))\n",
    "\n",
    "    return float(tp), float(fp), float(tn), float(fn)\n",
    "\n",
    "\n",
    "def class_likelihood_ratios_numpy(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    labels=None,\n",
    "    sample_weight=None,\n",
    "    raise_warning=True,\n",
    "):\n",
    "    '''NumPy implementation matching sklearn.metrics.class_likelihood_ratios.'''\n",
    "    tp, fp, tn, fn = confusion_counts_binary(\n",
    "        y_true, y_pred, labels=labels, sample_weight=sample_weight\n",
    "    )\n",
    "\n",
    "    pos_total = tp + fn\n",
    "    neg_total = tn + fp\n",
    "\n",
    "    if pos_total == 0 or neg_total == 0:\n",
    "        if raise_warning:\n",
    "            warnings.warn(\n",
    "                \"No positive or no negative samples in y_true; likelihood ratios are undefined.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    tpr = tp / pos_total\n",
    "    fnr = fn / pos_total\n",
    "    fpr = fp / neg_total\n",
    "    tnr = tn / neg_total\n",
    "\n",
    "    lr_plus = np.nan\n",
    "    lr_minus = np.nan\n",
    "\n",
    "    if fpr == 0:\n",
    "        if raise_warning:\n",
    "            warnings.warn(\"When false positive == 0, the positive likelihood ratio is undefined.\")\n",
    "    else:\n",
    "        lr_plus = tpr / fpr\n",
    "\n",
    "    if tnr == 0:\n",
    "        if raise_warning:\n",
    "            warnings.warn(\"When true negative == 0, the negative likelihood ratio is undefined.\")\n",
    "    else:\n",
    "        lr_minus = fnr / tnr\n",
    "\n",
    "    return (lr_plus, lr_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb932e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks vs scikit-learn\n",
    "\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [1, 1, 0, 0, 0]\n",
    "\n",
    "print(\"sklearn:\", class_likelihood_ratios(y_true, y_pred))\n",
    "print(\"numpy :\", class_likelihood_ratios_numpy(y_true, y_pred, raise_warning=False))\n",
    "\n",
    "y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n",
    "y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n",
    "\n",
    "print()\n",
    "print(\"Default label order (sorted):\")\n",
    "print(\"sklearn:\", class_likelihood_ratios(y_true, y_pred))\n",
    "\n",
    "print()\n",
    "print(\"Explicit labels=[negative, positive]:\")\n",
    "print(\"sklearn:\", class_likelihood_ratios(y_true, y_pred, labels=[\"non-cat\", \"cat\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858887d",
   "metadata": {},
   "source": [
    "## 3) Interpretation and common pitfalls\n",
    "\n",
    "**Valid ranges (for a useful classifier):**\n",
    "\n",
    "- \\(LR_+ \\ge 1\\). Values close to 1 mean “a positive prediction barely changes the odds”.\n",
    "- \\(0 \\le LR_- \\le 1\\). Values close to 1 mean “a negative prediction barely changes the odds”.\n",
    "\n",
    "If you ever see \\(LR_+ < 1\\) or \\(LR_- > 1\\), the classifier is often behaving like it has the labels flipped\n",
    "(or your `labels=[negative, positive]` ordering is wrong).\n",
    "\n",
    "**Rule-of-thumb strength of evidence (very domain dependent):**\n",
    "\n",
    "| Evidence | \\(LR_+\\) | \\(LR_-\\) |\n",
    "|---|---:|---:|\n",
    "| small | 2–5 | 0.5–0.2 |\n",
    "| moderate | 5–10 | 0.2–0.1 |\n",
    "| large | > 10 | < 0.1 |\n",
    "\n",
    "**Pitfalls**\n",
    "- The metric needs **hard predictions** (class labels). If your model outputs probabilities, you must choose a **threshold** first.\n",
    "- \\(LR_+\\) is undefined when \\(FP=0\\) (\\(\\text{FPR}=0\\)). \\(LR_-\\) is undefined when \\(TN=0\\) (\\(\\text{TNR}=0\\)).\n",
    "  Small datasets can make this happen easily.\n",
    "- Multi-class problems need a one-vs-rest reduction; scikit-learn's `class_likelihood_ratios` is **binary-only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds(p):\n",
    "    p = np.asarray(p)\n",
    "    return p / (1.0 - p)\n",
    "\n",
    "\n",
    "def prob_from_odds(o):\n",
    "    o = np.asarray(o)\n",
    "    return o / (1.0 + o)\n",
    "\n",
    "\n",
    "def update_probability(p_pre, lr):\n",
    "    '''Bayes update in odds form.'''\n",
    "    return prob_from_odds(odds(p_pre) * lr)\n",
    "\n",
    "\n",
    "p_pre = np.linspace(0.001, 0.999, 400)\n",
    "\n",
    "lr_plus_values = [2, 5, 10]\n",
    "lr_minus_values = [0.5, 0.2, 0.1]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Post-test probability after a POSITIVE prediction (use LR+)\",\n",
    "        \"Post-test probability after a NEGATIVE prediction (use LR-)\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for lr in lr_plus_values:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=p_pre, y=update_probability(p_pre, lr), mode=\"lines\", name=f\"LR+={lr}\"),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "for lr in lr_minus_values:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=p_pre, y=update_probability(p_pre, lr), mode=\"lines\", name=f\"LR-={lr}\"),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "# Reference line: no change\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=p_pre, y=p_pre, mode=\"lines\", line=dict(dash=\"dash\"), name=\"no change\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=p_pre, y=p_pre, mode=\"lines\", line=dict(dash=\"dash\"), showlegend=False),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"pre-test probability\", range=[0, 1], row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"pre-test probability\", range=[0, 1], row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"post-test probability\", range=[0, 1], row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"post-test probability\", range=[0, 1], row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=1000, height=420)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3ed6d",
   "metadata": {},
   "source": [
    "## 4) Threshold dependence and ROC geometry\n",
    "\n",
    "If your model outputs a **score** or **probability** \\(\\hat{p}\\), you get hard predictions via a threshold \\(t\\):\n",
    "\n",
    "\\[\n",
    "\\hat{y}(t) = \\mathbb{1}[\\hat{p} \\ge t].\n",
    "\\]\n",
    "\n",
    "So \\(LR_+\\) and \\(LR_-\\) are *functions of the threshold*.\n",
    "\n",
    "On the ROC plane (x = FPR, y = TPR) for a particular threshold:\n",
    "\n",
    "- \\(LR_+ = \\frac{\\text{TPR}}{\\text{FPR}}\\) is the **slope of the line from \\((0,0)\\)** to the ROC point.\n",
    "- \\(LR_- = \\frac{1-\\text{TPR}}{1-\\text{FPR}}\\) is the **slope of the line from \\((1,1)\\)** to the ROC point.\n",
    "\n",
    "This makes the metric visually interpretable: to get a large \\(LR_+\\) you want a ROC point that is steep above the origin;\n",
    "to get a small \\(LR_-\\) you want a point close to the top-left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D dataset for visualization\n",
    "X, y = make_classification(\n",
    "    n_samples=2200,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.2,\n",
    "    flip_y=0.05,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=7\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=7\n",
    ")\n",
    "\n",
    "# Standardize (helps gradient descent)\n",
    "mean_ = X_train.mean(axis=0)\n",
    "std_ = X_train.std(axis=0)\n",
    "X_train_s = (X_train - mean_) / std_\n",
    "X_val_s = (X_val - mean_) / std_\n",
    "X_test_s = (X_test - mean_) / std_\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Stable sigmoid\n",
    "    z = np.asarray(z)\n",
    "    out = np.empty_like(z, dtype=float)\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    ez = np.exp(z[~pos])\n",
    "    out[~pos] = ez / (1.0 + ez)\n",
    "    return out\n",
    "\n",
    "\n",
    "def fit_logreg_gd(X, y, *, lr=0.15, n_steps=2500, l2=0.01, seed=7):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    n, d = X.shape\n",
    "    w = rng_local.normal(scale=0.1, size=d)\n",
    "    b = 0.0\n",
    "\n",
    "    eps = 1e-12\n",
    "    losses = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        z = X @ w + b\n",
    "        p = sigmoid(z)\n",
    "\n",
    "        # Binary cross-entropy + L2\n",
    "        loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps)) + 0.5 * l2 * np.sum(w * w)\n",
    "\n",
    "        # Gradients\n",
    "        grad_w = (X.T @ (p - y)) / n + l2 * w\n",
    "        grad_b = np.mean(p - y)\n",
    "\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "        if step % 25 == 0:\n",
    "            losses.append(loss)\n",
    "\n",
    "    return w, b, np.array(losses)\n",
    "\n",
    "\n",
    "w, b, losses = fit_logreg_gd(X_train_s, y_train)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=losses, mode=\"lines\", name=\"train loss\"))\n",
    "fig.update_layout(\n",
    "    title=\"Logistic regression from scratch (gradient descent)\",\n",
    "    xaxis_title=\"checkpoint (every 25 steps)\",\n",
    "    yaxis_title=\"cross-entropy loss\",\n",
    "    width=900,\n",
    "    height=380,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "p_val = sigmoid(X_val_s @ w + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016afa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability distributions by class (validation set)\n",
    "\n",
    "df = {\n",
    "    \"p_hat\": p_val,\n",
    "    \"y\": y_val.astype(int),\n",
    "}\n",
    "\n",
    "fig = px.histogram(\n",
    "    df,\n",
    "    x=\"p_hat\",\n",
    "    color=\"y\",\n",
    "    nbins=50,\n",
    "    opacity=0.6,\n",
    "    barmode=\"overlay\",\n",
    "    histnorm=\"probability\",\n",
    "    title=\"Predicted probabilities by true class (validation set)\",\n",
    "    labels={\"p_hat\": \"predicted P(y=1|x)\", \"y\": \"true class\"},\n",
    ")\n",
    "fig.update_layout(width=900, height=420)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_thresholds(y_true, y_proba, thresholds):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        tp, fp, tn, fn = confusion_counts_binary(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "        pos_total = tp + fn\n",
    "        neg_total = tn + fp\n",
    "\n",
    "        tpr = tp / pos_total if pos_total > 0 else np.nan\n",
    "        fnr = fn / pos_total if pos_total > 0 else np.nan\n",
    "        fpr = fp / neg_total if neg_total > 0 else np.nan\n",
    "        tnr = tn / neg_total if neg_total > 0 else np.nan\n",
    "\n",
    "        lr_plus = tpr / fpr if (np.isfinite(fpr) and fpr > 0) else np.nan\n",
    "        lr_minus = fnr / tnr if (np.isfinite(tnr) and tnr > 0) else np.nan\n",
    "\n",
    "        dor = (\n",
    "            lr_plus / lr_minus\n",
    "            if (np.isfinite(lr_plus) and np.isfinite(lr_minus) and lr_plus > 0 and lr_minus > 0)\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        rows.append((t, tp, fp, tn, fn, tpr, tnr, lr_plus, lr_minus, dor))\n",
    "\n",
    "    arr = np.array(rows, dtype=float)\n",
    "    return {\n",
    "        \"threshold\": arr[:, 0],\n",
    "        \"tp\": arr[:, 1],\n",
    "        \"fp\": arr[:, 2],\n",
    "        \"tn\": arr[:, 3],\n",
    "        \"fn\": arr[:, 4],\n",
    "        \"tpr\": arr[:, 5],\n",
    "        \"tnr\": arr[:, 6],\n",
    "        \"lr_plus\": arr[:, 7],\n",
    "        \"lr_minus\": arr[:, 8],\n",
    "        \"dor\": arr[:, 9],\n",
    "    }\n",
    "\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "sweep = sweep_thresholds(y_val, p_val, thresholds)\n",
    "\n",
    "\n",
    "def pick_operating_points(sweep, *, min_sensitivity=0.95, min_specificity=0.95):\n",
    "    thresholds = sweep[\"threshold\"]\n",
    "\n",
    "    sens = sweep[\"tpr\"]  # sensitivity\n",
    "    spec = sweep[\"tnr\"]  # specificity\n",
    "    lr_plus = sweep[\"lr_plus\"]\n",
    "    lr_minus = sweep[\"lr_minus\"]\n",
    "\n",
    "    # A generic way to combine LR+ and LR- into one objective: diagnostic odds ratio (DOR)\n",
    "    # Fallback: Youden's J = sensitivity + specificity - 1 (always defined as long as rates are defined)\n",
    "    dor = sweep[\"dor\"]\n",
    "    youden_j = sens + spec - 1\n",
    "\n",
    "    if np.any(np.isfinite(dor)):\n",
    "        t_best = thresholds[np.nanargmax(dor)]\n",
    "        best_label = \"max DOR\"\n",
    "    else:\n",
    "        t_best = thresholds[np.nanargmax(youden_j)]\n",
    "        best_label = \"max Youden J (fallback)\"\n",
    "\n",
    "    # Screening: prioritize ruling OUT => minimize LR- while keeping sensitivity high\n",
    "    mask_screen = (sens >= min_sensitivity) & np.isfinite(lr_minus)\n",
    "    if mask_screen.any():\n",
    "        t_screen = thresholds[mask_screen][np.nanargmin(lr_minus[mask_screen])]\n",
    "    else:\n",
    "        t_screen = thresholds[np.nanargmin(lr_minus)]\n",
    "\n",
    "    # Confirmation: prioritize ruling IN => maximize LR+ while keeping specificity high\n",
    "    mask_confirm = (spec >= min_specificity) & np.isfinite(lr_plus)\n",
    "    if mask_confirm.any():\n",
    "        t_confirm = thresholds[mask_confirm][np.nanargmax(lr_plus[mask_confirm])]\n",
    "    else:\n",
    "        t_confirm = thresholds[np.nanargmax(lr_plus)]\n",
    "\n",
    "    return t_best, t_screen, t_confirm, best_label\n",
    "\n",
    "\n",
    "t_best, t_screen, t_confirm, best_label = pick_operating_points(sweep)\n",
    "\n",
    "(t_best, t_screen, t_confirm, best_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b178a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vline(fig, x, *, label, color):\n",
    "    fig.add_vline(x=x, line_width=2, line_dash=\"dash\", line_color=color)\n",
    "    fig.add_annotation(\n",
    "        x=x,\n",
    "        y=1.02,\n",
    "        xref=\"x\",\n",
    "        yref=\"paper\",\n",
    "        text=label,\n",
    "        showarrow=False,\n",
    "        font=dict(color=color),\n",
    "    )\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"LR+ vs threshold\", \"LR- vs threshold\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sweep[\"threshold\"], y=sweep[\"lr_plus\"], mode=\"lines\", name=\"LR+\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sweep[\"threshold\"], y=sweep[\"lr_minus\"], mode=\"lines\", name=\"LR-\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "for x, label, color in [\n",
    "    (t_best, best_label, \"#1f77b4\"),\n",
    "    (t_screen, \"screening\", \"#2ca02c\"),\n",
    "    (t_confirm, \"confirm\", \"#d62728\"),\n",
    "]:\n",
    "    _vline(fig, x, label=label, color=color)\n",
    "\n",
    "fig.update_yaxes(type=\"log\", row=1, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"threshold t\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"threshold t\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"LR+ (log scale)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"LR- (log scale)\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=1000, height=420)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve (validation set) + geometric interpretation of LR\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_val, p_val)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode=\"lines\", name=\"ROC\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", line=dict(dash=\"dash\"), name=\"random\")\n",
    ")\n",
    "\n",
    "# Get the ROC point closest to our chosen threshold t_best\n",
    "# (roc_curve returns thresholds in decreasing order)\n",
    "idx = np.argmin(np.abs(thr - t_best))\n",
    "x_pt, y_pt = fpr[idx], tpr[idx]\n",
    "\n",
    "# LR slopes at that operating point\n",
    "lr_plus = y_pt / x_pt if x_pt > 0 else np.inf\n",
    "lr_minus = (1 - y_pt) / (1 - x_pt) if (1 - x_pt) > 0 else np.inf\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[x_pt],\n",
    "        y=[y_pt],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10, color=\"#1f77b4\"),\n",
    "        name=f\"t≈{t_best:.2f}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Lines showing the slopes\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, x_pt], y=[0, y_pt], mode=\"lines\", line=dict(color=\"#1f77b4\"), showlegend=False)\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[1, x_pt], y=[1, y_pt], mode=\"lines\", line=dict(color=\"#d62728\"), showlegend=False)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"ROC geometry at t≈{t_best:.2f}:  LR+≈{lr_plus:.2f},  LR-≈{lr_minus:.2f}\",\n",
    "    xaxis_title=\"FPR\",\n",
    "    yaxis_title=\"TPR\",\n",
    "    width=900,\n",
    "    height=500,\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_at_threshold(y_true, y_proba, t):\n",
    "    y_pred = (y_proba >= t).astype(int)\n",
    "    lr_p, lr_m = class_likelihood_ratios_numpy(y_true, y_pred, labels=[0, 1], raise_warning=False)\n",
    "    tp, fp, tn, fn = confusion_counts_binary(y_true, y_pred, labels=[0, 1])\n",
    "    tpr = tp / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "    return {\n",
    "        \"t\": t,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"lr_plus\": lr_p,\n",
    "        \"lr_minus\": lr_m,\n",
    "    }\n",
    "\n",
    "\n",
    "m_best = metrics_at_threshold(y_val, p_val, t_best)\n",
    "m_screen = metrics_at_threshold(y_val, p_val, t_screen)\n",
    "m_confirm = metrics_at_threshold(y_val, p_val, t_confirm)\n",
    "\n",
    "m_best, m_screen, m_confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\n",
    "        f\"Screening (t={t_screen:.2f})\",\n",
    "        f\"Max DOR (t={t_best:.2f})\",\n",
    "        f\"Confirm (t={t_confirm:.2f})\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for j, m in enumerate([m_screen, m_best, m_confirm], start=1):\n",
    "    cm = np.array([[m[\"tn\"], m[\"fp\"]], [m[\"fn\"], m[\"tp\"]]], dtype=float)\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=cm,\n",
    "            x=[\"pred 0\", \"pred 1\"],\n",
    "            y=[\"true 0\", \"true 1\"],\n",
    "            colorscale=\"Blues\",\n",
    "            showscale=False,\n",
    "            text=cm.astype(int),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont=dict(size=16),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=j,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1050,\n",
    "    height=420,\n",
    "    title=\"Confusion matrices at three operating points (validation set)\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the chosen operating point changes post-test probability\n",
    "\n",
    "p_pre = 0.10  # example prior/prevalence\n",
    "\n",
    "for name, m in [(\"screening\", m_screen), (\"max_dor\", m_best), (\"confirm\", m_confirm)]:\n",
    "    p_pos = update_probability(p_pre, m[\"lr_plus\"])   # after a positive prediction\n",
    "    p_neg = update_probability(p_pre, m[\"lr_minus\"])  # after a negative prediction\n",
    "    print(\n",
    "        f\"{name:9s}  t={m['t']:.2f}  LR+={m['lr_plus']:.2f}  LR-={m['lr_minus']:.2f}  \"\n",
    "        f\"p(y=1|+)= {p_pos:.3f}  p(y=1|-)= {p_neg:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c54f24",
   "metadata": {},
   "source": [
    "## 5) Using likelihood ratios to optimize a simple algorithm\n",
    "\n",
    "\\(LR_+\\) and \\(LR_-\\) are defined through *counts* (TP/FP/TN/FN), so they are **not differentiable** w.r.t. model parameters.\n",
    "\n",
    "A common workflow is therefore:\n",
    "\n",
    "1) Train a probabilistic model (e.g. logistic regression) using a differentiable loss (cross-entropy)\n",
    "2) Use likelihood ratios on a **validation set** to pick an operating point (decision threshold)\n",
    "\n",
    "Example strategies:\n",
    "- **Screening test** (rule out): pick a threshold with high sensitivity and minimal \\(LR_-\\)\n",
    "- **Confirmatory test** (rule in): pick a threshold with high specificity and maximal \\(LR_+\\)\n",
    "- **Single-number optimization**: maximize DOR = \\(LR_+/LR_-\\) (useful, but can be unstable if FP or FN are small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2062294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check on a held-out test set using the max-DOR threshold from validation\n",
    "\n",
    "p_test = sigmoid(X_test_s @ w + b)\n",
    "y_pred_test = (p_test >= t_best).astype(int)\n",
    "\n",
    "print(\"Test set LR (sklearn):\", class_likelihood_ratios(y_test, y_pred_test, labels=[0, 1]))\n",
    "print(\"Test set LR (numpy) :\", class_likelihood_ratios_numpy(y_test, y_pred_test, labels=[0, 1], raise_warning=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff78f4b",
   "metadata": {},
   "source": [
    "## Pros / cons / when to use\n",
    "\n",
    "**Pros**\n",
    "- Interpretable: directly tells you how to update odds (prevalence + test result \\(\\to\\) posterior)\n",
    "- Uses sensitivity/specificity, so it is more stable across different prevalences than precision/NPV\n",
    "- Naturally supports “rule-in” (large \\(LR_+\\)) vs “rule-out” (small \\(LR_-\\)) thinking\n",
    "\n",
    "**Cons**\n",
    "- Threshold-dependent and based on hard predictions (not a ranking metric like AUC)\n",
    "- Can be undefined/infinite when \\(FP=0\\) or \\(TN=0\\), especially on small datasets\n",
    "- Binary-only; multi-class needs one-vs-rest and careful reporting\n",
    "\n",
    "**Good fits**\n",
    "- Medical diagnostic tests, screening vs confirmation\n",
    "- Any binary decision where base rate/prevalence is known or can be estimated and you need a domain-friendly “odds update” explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0ea4a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1) On a dataset you care about, sweep thresholds and compare:\n",
    "   - max \\(LR_+\\) at specificity \\(\\ge 0.95\\)\n",
    "   - min \\(LR_-\\) at sensitivity \\(\\ge 0.95\\)\n",
    "   - max DOR\n",
    "   Do these thresholds match what you would pick using accuracy or F1?\n",
    "\n",
    "2) Implement one-vs-rest likelihood ratios for multi-class classification and report the per-class \\(LR_+\\) and \\(LR_-\\).\n",
    "\n",
    "## References\n",
    "- scikit-learn: `sklearn.metrics.class_likelihood_ratios`\n",
    "- Wikipedia: https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}