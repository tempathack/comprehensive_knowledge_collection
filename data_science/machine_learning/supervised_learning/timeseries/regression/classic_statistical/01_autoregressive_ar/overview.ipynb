{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1a0f78",
   "metadata": {},
   "source": [
    "# Autoregressive (AR) models (from scratch NumPy + Plotly)\n",
    "\n",
    "An **autoregressive (AR)** model explains a time series using its own past values.\n",
    "\n",
    "In this notebook you will:\n",
    "- Precisely define the **AR(p)** model in math (LaTeX)\n",
    "- Understand the **algorithm** (fit → diagnose → forecast)\n",
    "- Learn the **assumptions**, especially **stationarity**\n",
    "- Choose the lag order **p** (AIC/BIC intuition + implementation)\n",
    "- Implement AR fitting + forecasting in **pure NumPy** (no statsmodels)\n",
    "- Visualize with **Plotly**:\n",
    "  - effect of different lag orders\n",
    "  - prediction vs actual\n",
    "  - residual behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Plotly notebooks: force the Jupyter renderer.\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b5d98",
   "metadata": {},
   "source": [
    "## 1) What the AR model is (algorithm view)\n",
    "\n",
    "### Model idea\n",
    "An AR model says:\n",
    "> **Today is a linear combination of the last _p_ values plus noise.**\n",
    "\n",
    "### Fit / forecast algorithm (high-level)\n",
    "Given a univariate series \\(y_0, y_1, \\dots, y_{T-1}\\):\n",
    "\n",
    "1. **Choose a lag order** \\(p\\) (domain knowledge, AIC/BIC, validation).\n",
    "2. Build a **lagged design matrix** \\(X\\) where each row contains \\([1, y_{t-1}, \\dots, y_{t-p}]\\).\n",
    "3. **Estimate coefficients** \\(\\beta = [c, \\varphi_1, \\dots, \\varphi_p]^\\top\\) by least squares.\n",
    "4. Produce predictions:\n",
    "   - **one-step ahead**: use real past values (best-case prediction)\n",
    "   - **multi-step forecast**: feed your own predictions back in (real forecasting)\n",
    "5. **Diagnose residuals** (should look like white noise):\n",
    "   - mean near 0, constant variance\n",
    "   - weak autocorrelation (ACF near 0 for lags > 0)\n",
    "6. Iterate on \\(p\\) (and preprocessing like differencing/detrending) until diagnostics are acceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7d0b2",
   "metadata": {},
   "source": [
    "## 2) The AR(p) model in LaTeX + step-by-step math\n",
    "\n",
    "### Scalar form\n",
    "The **AR(p)** model is:\n",
    "\n",
    "$$\n",
    "y_t = c + \\sum_{i=1}^{p} \\varphi_i\\, y_{t-i} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "- \\(y_t\\): value at time \\(t\\)\n",
    "- \\(c\\): intercept (drift)\n",
    "- \\(\\varphi_i\\): lag-\\(i\\) coefficient\n",
    "- \\(\\varepsilon_t\\): noise term (ideally **white noise**)\n",
    "\n",
    "If the process is stationary, the unconditional mean exists and is\n",
    "$$\n",
    "\\mu = \\mathbb{E}[y_t] = \\frac{c}{1 - \\sum_{i=1}^p \\varphi_i} \\quad \\text{(when the denominator is nonzero and the process is stationary)}.\n",
    "$$\n",
    "\n",
    "### Matrix (regression) form\n",
    "For \\(t = p, p+1, \\dots, T-1\\), define a row vector\n",
    "$$\n",
    "x_t = [1,\\; y_{t-1},\\; y_{t-2},\\; \\dots,\\; y_{t-p}] \n",
    "$$\n",
    "and stack these into a matrix \\(X\\). Also stack the targets into a vector \\(\\mathbf{y} = [y_p, y_{p+1}, \\dots, y_{T-1}]^\\top\\).\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\mathbf{y} = X\\,\\beta + \\varepsilon, \\quad \\beta = [c, \\varphi_1, \\dots, \\varphi_p]^\\top\n",
    "$$\n",
    "\n",
    "### Estimation (least squares)\n",
    "We choose \\(\\hat\\beta\\) to minimize squared prediction error:\n",
    "$$\n",
    "\\hat\\beta = \\arg\\min_{\\beta} \\|\\mathbf{y} - X\\beta\\|_2^2\n",
    "$$\n",
    "\n",
    "A closed form exists if \\(X^\\top X\\) is invertible:\n",
    "$$\n",
    "\\hat\\beta = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "In practice we solve the least-squares problem with numerically stable linear algebra (still \"low-level\": just NumPy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f170f5",
   "metadata": {},
   "source": [
    "## 3) Assumptions, requirements, and stationarity\n",
    "\n",
    "### Core assumptions (what AR is \"promising\")\n",
    "- **Linearity in the past**: \\(y_t\\) is a linear function of \\(y_{t-1},\\dots,y_{t-p}\\).\n",
    "- **Equally spaced observations**: the meaning of \"one lag\" is stable.\n",
    "- **Parameter stability**: coefficients \\(\\varphi_i\\) do not change over the segment you fit.\n",
    "- **Noise behavior**: residuals \\(\\varepsilon_t\\) should be approximately **uncorrelated**, with roughly constant variance. (Gaussianity is only needed if you want exact likelihood-based inference; OLS fitting works more generally.)\n",
    "\n",
    "### Stationarity requirements (the important one)\n",
    "AR models are usually used for **covariance-stationary** series (constant mean/variance, autocovariance depends only on lag). For AR(p), this is governed by the **characteristic polynomial**:\n",
    "$$\n",
    "\\Phi(z) = 1 - \\varphi_1 z - \\varphi_2 z^2 - \\dots - \\varphi_p z^p\n",
    "$$\n",
    "The AR(p) process is stationary if **all roots** of \\(\\Phi(z)=0\\) satisfy\n",
    "$$\n",
    "|z| > 1 \\quad \\text{(roots outside the unit circle)}.\n",
    "$$\n",
    "\n",
    "Special case AR(1):\n",
    "$$\n",
    "y_t = c + \\varphi_1 y_{t-1} + \\varepsilon_t \\quad \\Rightarrow \\quad \\text{stationary iff } |\\varphi_1| < 1.\n",
    "$$\n",
    "\n",
    "### When AR is used / what patterns it fits well\n",
    "AR is a good baseline when the present depends on the recent past:\n",
    "- **Inertia / smoothing**: values move gradually (sensor readings, latency signals).\n",
    "- **Mean reversion after incidents**: a shock causes a jump that decays over a few steps.\n",
    "- **Short-memory autocorrelation**: dependence mostly captured by a small number of lags.\n",
    "- **Oscillations**: alternating/underdamped behavior can be captured by AR(2)+ (complex roots).\n",
    "\n",
    "AR is *not* a great fit (without preprocessing/extensions) for:\n",
    "- strong **trend** or **unit-root** behavior (needs differencing → ARIMA)\n",
    "- strong **seasonality** (needs seasonal terms/features)\n",
    "- structural breaks / regime changes (coefficients change)\n",
    "- heavy heteroskedasticity (e.g., volatility clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8c14c",
   "metadata": {},
   "source": [
    "## 4) How lag order \\(p\\) is chosen (intuition)\n",
    "\n",
    "### Practical ways to pick \\(p\\)\n",
    "- **Information criteria** (AIC/BIC): trade off fit vs complexity.\n",
    "- **Validation**: choose \\(p\\) that minimizes forecast error on a holdout period.\n",
    "- **(Partial) autocorrelation**: ACF/PACF heuristics can suggest a cutoff.\n",
    "- **Domain knowledge**: if physics says effects last \\~3 steps, start near \\(p=3\\).\n",
    "\n",
    "### AIC/BIC (what they do)\n",
    "If you assume Gaussian noise, the negative log-likelihood reduces to a function of the residual variance. Up to constants that do not affect the minimizer, you can use:\n",
    "$$\n",
    "\\mathrm{AIC}(p) \\propto 2k + n\\log(\\mathrm{RSS}/n)\n",
    "$$\n",
    "$$\n",
    "\\mathrm{BIC}(p) \\propto k\\log(n) + n\\log(\\mathrm{RSS}/n)\n",
    "$$\n",
    "where:\n",
    "- \\(\\mathrm{RSS} = \\sum_t \\hat\\varepsilon_t^2\\)\n",
    "- \\(n\\) is the number of fitted points (\\(T-p\\))\n",
    "- \\(k\\) is the number of parameters (\\(p+1\\) if you include an intercept)\n",
    "\n",
    "BIC penalizes complexity harder than AIC, so BIC tends to choose smaller \\(p\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4acba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ar(\n",
    "    phi: np.ndarray,\n",
    "    *,\n",
    "    c: float = 0.0,\n",
    "    sigma: float = 1.0,\n",
    "    n: int = 600,\n",
    "    burn_in: int = 200,\n",
    "    seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Simulate a univariate AR(p): y_t = c + sum_i phi_i y_{t-i} + eps_t.\"\"\"\n",
    "    phi = np.asarray(phi, dtype=float)\n",
    "    p = int(phi.size)\n",
    "    if p < 1:\n",
    "        raise ValueError(\"phi must have length >= 1\")\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive\")\n",
    "    if burn_in < 0:\n",
    "        raise ValueError(\"burn_in must be >= 0\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    eps = rng.normal(loc=0.0, scale=sigma, size=n + burn_in)\n",
    "    y = np.zeros(n + burn_in, dtype=float)\n",
    "\n",
    "    # Start at t=p so y_{t-i} exists.\n",
    "    for t in range(p, n + burn_in):\n",
    "        lags = y[t - 1 : t - p - 1 : -1]  # [y_{t-1}, ..., y_{t-p}]\n",
    "        y[t] = c + float(phi @ lags) + eps[t]\n",
    "\n",
    "    return y[burn_in:]\n",
    "\n",
    "\n",
    "def make_lagged_matrix(y: np.ndarray, p: int, *, include_intercept: bool = True):\n",
    "    \"\"\"Build (X, y_target) for AR(p) regression.\n",
    "\n",
    "    For t = p..T-1:\n",
    "      y_target[t-p] = y[t]\n",
    "      X[t-p] = [1, y[t-1], ..., y[t-p]]\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    p = int(p)\n",
    "    if p < 1:\n",
    "        raise ValueError(\"p must be >= 1\")\n",
    "    if y.ndim != 1:\n",
    "        raise ValueError(\"y must be 1D\")\n",
    "    n = y.size\n",
    "    if n <= p:\n",
    "        raise ValueError(f\"Need at least p+1 points; got n={n}, p={p}\")\n",
    "\n",
    "    # Columns are lag-1, lag-2, ..., lag-p.\n",
    "    lag_cols = [y[p - i : n - i] for i in range(1, p + 1)]\n",
    "    X = np.column_stack(lag_cols)\n",
    "    if include_intercept:\n",
    "        X = np.column_stack([np.ones(n - p, dtype=float), X])\n",
    "    y_target = y[p:]\n",
    "    return X, y_target\n",
    "\n",
    "\n",
    "def fit_ar_ols(y: np.ndarray, p: int, *, include_intercept: bool = True):\n",
    "    \"\"\"Fit AR(p) by OLS and return a small result dict.\"\"\"\n",
    "    X, y_target = make_lagged_matrix(y, p, include_intercept=include_intercept)\n",
    "    beta, *_ = np.linalg.lstsq(X, y_target, rcond=None)\n",
    "    y_hat_target = X @ beta\n",
    "    resid = y_target - y_hat_target\n",
    "    rss = float(resid @ resid)\n",
    "    n_eff = int(y_target.size)\n",
    "    k = int(X.shape[1])\n",
    "    sigma2 = rss / n_eff\n",
    "\n",
    "    y_hat = np.full_like(np.asarray(y, dtype=float), np.nan)\n",
    "    y_hat[p:] = y_hat_target\n",
    "\n",
    "    return {\n",
    "        \"p\": int(p),\n",
    "        \"include_intercept\": bool(include_intercept),\n",
    "        \"beta\": beta,\n",
    "        \"y_hat\": y_hat,\n",
    "        \"resid\": resid,\n",
    "        \"rss\": rss,\n",
    "        \"sigma2\": sigma2,\n",
    "        \"n_eff\": n_eff,\n",
    "        \"k\": k,\n",
    "    }\n",
    "\n",
    "\n",
    "def forecast_ar(beta: np.ndarray, y_history: np.ndarray, p: int, *, steps: int, include_intercept: bool = True):\n",
    "    \"\"\"Iterative multi-step forecast using the model's own predictions.\"\"\"\n",
    "    beta = np.asarray(beta, dtype=float)\n",
    "    y_hist = list(np.asarray(y_history, dtype=float).tolist())\n",
    "    p = int(p)\n",
    "    if steps < 1:\n",
    "        return np.array([], dtype=float)\n",
    "    if len(y_hist) < p:\n",
    "        raise ValueError(f\"Need at least p history points; got {len(y_hist)}\")\n",
    "\n",
    "    out = []\n",
    "    for _ in range(int(steps)):\n",
    "        lags = np.array(y_hist[-1 : -p - 1 : -1], dtype=float)\n",
    "        x = np.concatenate(([1.0], lags)) if include_intercept else lags\n",
    "        y_next = float(x @ beta)\n",
    "        y_hist.append(y_next)\n",
    "        out.append(y_next)\n",
    "    return np.array(out, dtype=float)\n",
    "\n",
    "\n",
    "def aic_bic_from_rss(rss: float, n: int, k: int):\n",
    "    \"\"\"AIC/BIC up to additive constants (sufficient for comparing p).\"\"\"\n",
    "    rss = float(rss)\n",
    "    n = int(n)\n",
    "    k = int(k)\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive\")\n",
    "    if rss <= 0:\n",
    "        rss = 1e-12\n",
    "    aic = 2 * k + n * np.log(rss / n)\n",
    "    bic = k * np.log(n) + n * np.log(rss / n)\n",
    "    return float(aic), float(bic)\n",
    "\n",
    "\n",
    "def ar_stationary(phi: np.ndarray):\n",
    "    \"\"\"Check covariance-stationarity for AR(p) via characteristic roots.\"\"\"\n",
    "    phi = np.asarray(phi, dtype=float)\n",
    "    coeffs = np.concatenate(([1.0], -phi))  # 1 - phi1 z - ... - phip z^p\n",
    "    roots = np.roots(coeffs)\n",
    "    return bool(np.all(np.abs(roots) > 1.0)), roots\n",
    "\n",
    "\n",
    "def acf(x: np.ndarray, nlags: int = 30):\n",
    "    \"\"\"Autocorrelation function for lags 0..nlags (simple, biased estimator).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x - np.mean(x)\n",
    "    denom = float(x @ x)\n",
    "    out = np.empty(int(nlags) + 1, dtype=float)\n",
    "    out[0] = 1.0\n",
    "    for k in range(1, int(nlags) + 1):\n",
    "        out[k] = float(x[:-k] @ x[k:]) / denom\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6d119",
   "metadata": {},
   "source": [
    "## 5) Synthetic example (so we know the truth)\n",
    "\n",
    "We'll simulate a stationary AR(3) series, split it into train/test, and:\n",
    "- pick \\(p\\) with AIC/BIC\n",
    "- show how forecasts change as \\(p\\) changes\n",
    "- examine residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d668fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground-truth AR(3)\n",
    "phi_true = np.array([0.65, -0.25, 0.15])\n",
    "c_true = 0.2\n",
    "sigma_true = 0.7\n",
    "\n",
    "is_stat, roots = ar_stationary(phi_true)\n",
    "print(\"Stationary (true process)?\", is_stat)\n",
    "print(\"Characteristic roots:\", np.round(roots, 3))\n",
    "\n",
    "y = simulate_ar(phi_true, c=c_true, sigma=sigma_true, n=700, burn_in=300, seed=7)\n",
    "t = np.arange(y.size)\n",
    "\n",
    "train_n = 520\n",
    "y_train = y[:train_n]\n",
    "y_test = y[train_n:]\n",
    "t_test = t[train_n:]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=y, mode=\"lines\", name=\"series\"))\n",
    "fig.add_vline(x=train_n, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(title=\"Synthetic AR(3) series (train/test split)\", xaxis_title=\"t\", yaxis_title=\"y\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3b793",
   "metadata": {},
   "source": [
    "## 6) Lag-order selection with AIC/BIC (NumPy)\n",
    "\n",
    "We'll fit AR(p) for \\(p=1..P_{max}\\) on the training set and compute AIC/BIC (up to constants). A good \\(p\\) usually:\n",
    "- makes residuals close to white noise\n",
    "- avoids unnecessary complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_MAX = 15\n",
    "\n",
    "ps = np.arange(1, P_MAX + 1)\n",
    "aics = []\n",
    "bics = []\n",
    "rss_list = []\n",
    "mse_dyn = []\n",
    "\n",
    "for p in ps:\n",
    "    fit = fit_ar_ols(y_train, p, include_intercept=True)\n",
    "    aic, bic = aic_bic_from_rss(fit[\"rss\"], fit[\"n_eff\"], fit[\"k\"])\n",
    "    aics.append(aic)\n",
    "    bics.append(bic)\n",
    "    rss_list.append(fit[\"rss\"])\n",
    "\n",
    "    # (Optional sanity metric) dynamic multi-step MSE on the test horizon\n",
    "    y_fc = forecast_ar(fit[\"beta\"], y_train, p, steps=y_test.size, include_intercept=True)\n",
    "    mse_dyn.append(float(np.mean((y_test - y_fc) ** 2)))\n",
    "\n",
    "aics = np.array(aics)\n",
    "bics = np.array(bics)\n",
    "mse_dyn = np.array(mse_dyn)\n",
    "\n",
    "p_best_aic = int(ps[np.argmin(aics)])\n",
    "p_best_bic = int(ps[np.argmin(bics)])\n",
    "print(\"Best p by AIC:\", p_best_aic)\n",
    "print(\"Best p by BIC:\", p_best_bic)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"AIC (lower is better)\", \"BIC (lower is better)\"])\n",
    "fig.add_trace(go.Scatter(x=ps, y=aics, mode=\"lines+markers\", name=\"AIC\"), row=1, col=1)\n",
    "fig.add_vline(x=p_best_aic, line_dash=\"dash\", line_color=\"#1f77b4\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=ps, y=bics, mode=\"lines+markers\", name=\"BIC\"), row=1, col=2)\n",
    "fig.add_vline(x=p_best_bic, line_dash=\"dash\", line_color=\"#1f77b4\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"p\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"p\", row=1, col=2)\n",
    "fig.update_layout(title=\"Lag-order selection on training data\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ps, y=mse_dyn, mode=\"lines+markers\", name=\"Dynamic forecast MSE\"))\n",
    "fig.update_layout(title=\"Forecast error vs lag order (dynamic multi-step)\", xaxis_title=\"p\", yaxis_title=\"MSE\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ea98a",
   "metadata": {},
   "source": [
    "## 7) Effect of different lag orders (forecast behavior)\n",
    "\n",
    "To show the impact of \\(p\\), we'll fit several AR(p) models and compare their **multi-step forecasts** over the same test window. This is where under/over-specifying \\(p\\) often becomes visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_SHOW = [1, 2, 3, 6, 12]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_test, mode=\"lines\", name=\"actual (test)\", line=dict(color=\"black\")))\n",
    "\n",
    "for p in P_SHOW:\n",
    "    fit = fit_ar_ols(y_train, p, include_intercept=True)\n",
    "    y_fc = forecast_ar(fit[\"beta\"], y_train, p, steps=y_test.size, include_intercept=True)\n",
    "    mse = float(np.mean((y_test - y_fc) ** 2))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=t_test,\n",
    "            y=y_fc,\n",
    "            mode=\"lines\",\n",
    "            name=f\"AR({p}) forecast (MSE={mse:.3f})\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Effect of lag order: multi-step forecasts on the same test window\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd6478",
   "metadata": {},
   "source": [
    "## 8) Prediction vs actual (one-step ahead)\n",
    "\n",
    "A **one-step-ahead** prediction uses the true lagged values at every time step (it does not feed predictions back in). This is often used for:\n",
    "- measuring in-sample fit\n",
    "- creating residuals for diagnostics / anomaly detection\n",
    "- comparing models fairly before doing true multi-step forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_best = p_best_bic\n",
    "fit_best = fit_ar_ols(y_train, p_best, include_intercept=True)\n",
    "\n",
    "# One-step predictions on the full series using true lags.\n",
    "X_full, y_target_full = make_lagged_matrix(y, p_best, include_intercept=True)\n",
    "y_hat_target_full = X_full @ fit_best[\"beta\"]\n",
    "y_hat_full = np.full_like(y, np.nan)\n",
    "y_hat_full[p_best:] = y_hat_target_full\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=y, mode=\"lines\", name=\"actual\", line=dict(color=\"black\")))\n",
    "fig.add_trace(go.Scatter(x=t, y=y_hat_full, mode=\"lines\", name=f\"AR({p_best}) one-step prediction\"))\n",
    "fig.add_vline(x=train_n, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(title=\"Prediction vs actual (one-step ahead)\", xaxis_title=\"t\", yaxis_title=\"y\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b8d43",
   "metadata": {},
   "source": [
    "## 9) Residual behavior (diagnostics)\n",
    "\n",
    "After fitting AR(p), residuals should ideally behave like **white noise**:\n",
    "- no obvious trend or seasonality left in residuals\n",
    "- approximately constant variance\n",
    "- residual ACF near 0 for lags > 0\n",
    "\n",
    "These diagnostics help answer:\n",
    "- Is \\(p\\) too small? (residuals still autocorrelated)\n",
    "- Is the series non-stationary / missing trend or seasonality?\n",
    "- Are there **incidents** (spikes) not explained by autoregression?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4634ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals from the training fit (one-step ahead on training window)\n",
    "resid = fit_best[\"resid\"]\n",
    "fitted = y_train[p_best:] - resid\n",
    "\n",
    "res_acf = acf(resid, nlags=30)\n",
    "lags = np.arange(res_acf.size)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Residuals over time (train)\",\n",
    "        \"Residual histogram (train)\",\n",
    "        \"Residual ACF (train)\",\n",
    "        \"Residuals vs fitted (train)\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(resid.size), y=resid, mode=\"lines\", name=\"residual\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_hline(y=0, line_color=\"gray\", line_width=1, row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=resid, nbinsx=40, name=\"residuals\"), row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(x=lags, y=res_acf, name=\"ACF\"), row=2, col=1)\n",
    "fig.add_hline(y=0, line_color=\"gray\", line_width=1, row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=fitted, y=resid, mode=\"markers\", name=\"resid vs fitted\", opacity=0.6), row=2, col=2)\n",
    "fig.add_hline(y=0, line_color=\"gray\", line_width=1, row=2, col=2)\n",
    "\n",
    "fig.update_layout(title=f\"Residual diagnostics for AR({p_best}) (fit on training)\")\n",
    "fig.update_xaxes(title_text=\"index\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"residual\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"lag\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"fitted\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"residual\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"ACF\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"residual\", row=2, col=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f97d8",
   "metadata": {},
   "source": [
    "## 10) Stationarity intuition (quick demo)\n",
    "\n",
    "A stationary AR(1) with \\(|\\varphi_1| < 1\\) tends to \"forget\" shocks (effects decay geometrically).\n",
    "\n",
    "If \\(|\\varphi_1| \\ge 1\\), the process is **not** covariance-stationary (variance explodes or becomes undefined). This is why AR is commonly paired with **differencing** (ARIMA) when the data has a trend/unit root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stat = simulate_ar(np.array([0.7]), c=0.0, sigma=1.0, n=250, burn_in=200, seed=0)\n",
    "y_nonstat = simulate_ar(np.array([1.02]), c=0.0, sigma=1.0, n=250, burn_in=200, seed=0)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=[\"Stationary AR(1): φ=0.7\", \"Non-stationary AR(1): φ=1.02\"])\n",
    "fig.add_trace(go.Scatter(y=y_stat, mode=\"lines\", name=\"φ=0.7\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=y_nonstat, mode=\"lines\", name=\"φ=1.02\"), row=2, col=1)\n",
    "fig.update_layout(title=\"Stationarity matters: |φ|<1 vs |φ|>1\", xaxis_title=\"t\", yaxis_title=\"y\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1ae0d",
   "metadata": {},
   "source": [
    "## 11) Summary (what to remember)\n",
    "\n",
    "- AR(p) is a **linear** model of \\(y_t\\) from its last \\(p\\) values.\n",
    "- Fitting AR(p) can be done as a standard **least squares regression** on lagged features.\n",
    "- **Stationarity** is crucial: for AR(p) this is a root condition on the characteristic polynomial.\n",
    "- Choose \\(p\\) using **AIC/BIC**, validation, and especially **residual diagnostics**.\n",
    "- In incident-heavy signals, AR is often used as a **baseline** and incidents show up as large residuals.\n",
    "\n",
    "## Exercises\n",
    "1. Simulate AR(2) with complex roots (oscillations). What do the series and ACF look like?\n",
    "2. Fit AR(p) with and without an intercept. How does centering the series change the interpretation of \\(c\\)?\n",
    "3. Try a trending series, apply first differencing, then refit AR(p). Compare diagnostics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}