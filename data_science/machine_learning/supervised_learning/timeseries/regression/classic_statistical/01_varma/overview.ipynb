{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0a79e5",
   "metadata": {},
   "source": [
    "# VARMA (Vector Autoregressive Moving Average)\n",
    "\n",
    "## Goals\n",
    "- Understand the difference between **VAR**, **VARMA**, and **VARMAX**.\n",
    "- Build intuition for the **moving-average (MA)** part as *shock memory / colored noise*.\n",
    "- See the **tradeoffs** (parameter count, estimation cost) and **identifiability** pitfalls.\n",
    "- Implement simulation, forecasting, and shock propagation (**impulse responses**) in **NumPy**.\n",
    "- Visualize multivariate forecasts and shock propagation with **Plotly**.\n",
    "\n",
    "## Prerequisites\n",
    "- Linear algebra (matrices, eigenvalues)\n",
    "- Basic time series: stationarity, innovations (white noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663e003",
   "metadata": {},
   "source": [
    "## 1) VAR vs VARMA vs VARMAX (precise definitions)\n",
    "\n",
    "Let \\(\\mathbf{y}_t\\in\\mathbb{R}^k\\) be a *k-variate* time series.\n",
    "\n",
    "### VAR(p)\n",
    "A **Vector Autoregression** uses only lagged values of \\(\\mathbf{y}\\):\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} A_i\\,\\mathbf{y}_{t-i} + \\boldsymbol{\\varepsilon}_t,\n",
    "\\qquad \\boldsymbol{\\varepsilon}_t \\sim (\\mathbf{0},\\Sigma)\\ \\text{i.i.d.}\n",
    "$$\n",
    "\n",
    "- \\(A_i\\in\\mathbb{R}^{k\\times k}\\) couples variables across lags.\n",
    "- Noise is **white** by assumption: no serial correlation left in \\(\\varepsilon_t\\).\n",
    "\n",
    "### VARMA(p,q)\n",
    "A **Vector ARMA** adds a moving-average part in *past innovations*:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} A_i\\,\\mathbf{y}_{t-i}\n",
    "+ \\boldsymbol{\\varepsilon}_t + \\sum_{j=1}^{q} M_j\\,\\boldsymbol{\\varepsilon}_{t-j}.\n",
    "$$\n",
    "\n",
    "- The MA matrices \\(M_j\\in\\mathbb{R}^{k\\times k}\\) mean **past shocks echo forward**.\n",
    "- Special cases:\n",
    "  - VAR(p) is VARMA(p,0)\n",
    "  - VMA(q) is VARMA(0,q)\n",
    "\n",
    "### VARMAX(p,q,r)\n",
    "A **VARMAX** further adds *exogenous inputs* \\(\\mathbf{x}_t\\in\\mathbb{R}^m\\) (covered in the next folder):\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} A_i\\,\\mathbf{y}_{t-i}\n",
    "+ \\sum_{\\ell=0}^{r} B_{\\ell}\\,\\mathbf{x}_{t-\\ell}\n",
    "+ \\boldsymbol{\\varepsilon}_t + \\sum_{j=1}^{q} M_j\\,\\boldsymbol{\\varepsilon}_{t-j}.\n",
    "$$\n",
    "\n",
    "- \\(B_\\ell\\in\\mathbb{R}^{k\\times m}\\) maps input features into the k outputs.\n",
    "\n",
    "### Lag-operator matrix form\n",
    "Define the matrix polynomials\n",
    "\n",
    "$$\n",
    "A(L)= I - A_1 L - \\cdots - A_p L^p,\\qquad\n",
    "M(L)= I + M_1 L + \\cdots + M_q L^q.\n",
    "$$\n",
    "\n",
    "Then VARMA is:\n",
    "\n",
    "$$\n",
    "A(L)\\,\\mathbf{y}_t = \\mathbf{c} + M(L)\\,\\boldsymbol{\\varepsilon}_t.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3cad2",
   "metadata": {},
   "source": [
    "## 2) What the MA extension *means*\n",
    "\n",
    "In a VAR, we assume \\(\\varepsilon_t\\) is white noise. If reality has *shock persistence* (e.g., microstructure noise, aggregation effects, measurement frictions), the residuals from a plain VAR can remain autocorrelated.\n",
    "\n",
    "A VARMA allows the innovation to be filtered before it fully “hits” \\(\\mathbf{y}\\):\n",
    "\n",
    "- **AR part** (\\(A_i\\)): feedback / propagation through *state dynamics*.\n",
    "- **MA part** (\\(M_j\\)): short-memory filter on shocks (a finite impulse response on \\(\\varepsilon\\)).\n",
    "\n",
    "A stable VARMA has an infinite moving-average (Wold) representation:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\boldsymbol{\\mu} + \\sum_{h=0}^{\\infty} \\Psi_h\\,\\boldsymbol{\\varepsilon}_{t-h}.\n",
    "$$\n",
    "\n",
    "The impulse response matrices \\(\\Psi_h\\in\\mathbb{R}^{k\\times k}\\) satisfy the recursion\n",
    "\n",
    "- \\(\\Psi_0 = I\\)\n",
    "- for \\(h\\ge 1\\):\n",
    "\n",
    "$$\n",
    "\\Psi_h = M_h + \\sum_{i=1}^{\\min(p,h)} A_i\\,\\Psi_{h-i},\n",
    "\\qquad\n",
    "M_h = \\begin{cases}\n",
    "M_h & 1\\le h\\le q\\\\\n",
    "0 & h>q\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So **shock propagation** is literally \\(\\Psi_h\\): a unit shock in component \\(j\\) produces response \\(\\Psi_h\\,\\mathbf{e}_j\\) after \\(h\\) steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020a7bd",
   "metadata": {},
   "source": [
    "## 3) Complexity tradeoffs\n",
    "\n",
    "For \\(k\\) variables:\n",
    "\n",
    "- **VAR(p)** parameters (ignoring \\(\\Sigma\\)):\n",
    "\n",
    "$$k\\ \\text{(intercept)} + k^2 p.$$\n",
    "\n",
    "- **VARMA(p,q)** parameters:\n",
    "\n",
    "$$k + k^2(p+q).$$\n",
    "\n",
    "Plus the innovation covariance \\(\\Sigma\\) with \\(k(k+1)/2\\) free parameters.\n",
    "\n",
    "Tradeoffs:\n",
    "- VAR is easy: OLS per equation (or stacked multivariate OLS).\n",
    "- VARMA is harder: estimation usually requires **state space + Kalman filter** and **maximum likelihood**.\n",
    "- VARMA can be **more parsimonious** than a high-order VAR (fewer parameters for similar autocovariance structure), but only if you can estimate it reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4e1d4",
   "metadata": {},
   "source": [
    "## 4) Identifiability (why VARMA is tricky)\n",
    "\n",
    "In multivariate ARMA, parameters are **not uniquely identified** without additional restrictions.\n",
    "\n",
    "Core issues:\n",
    "- **AR/MA cancellation**: if \\(A(L)\\) and \\(M(L)\\) share a common left factor, the same stochastic process can be represented with different \\((A,M)\\).\n",
    "- **Stability / stationarity**: require \\(\\det A(z) \\neq 0\\) for \\(|z|\\le 1\\) (no roots inside/on the unit circle).\n",
    "- **Invertibility**: require \\(\\det M(z) \\neq 0\\) for \\(|z|\\le 1\\) so innovations are recoverable from observables.\n",
    "\n",
    "Practical consequences:\n",
    "- Estimation often enforces canonical forms (e.g., **echelon form**) or uses a **minimal state-space realization**.\n",
    "- Small samples + many parameters can produce unstable or non-invertible fits.\n",
    "\n",
    "In the code below we focus on:\n",
    "- simulation (where parameters are known),\n",
    "- impulse responses and forecasts *given parameters*,\n",
    "- and VAR fitting as a baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_companion(A_list: list[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Companion matrix for VAR(p) stability checks.\n",
    "\n",
    "    For y_t = sum_{i=1}^p A_i y_{t-i} + e_t.\n",
    "\n",
    "    Returns: (k*p, k*p) companion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    p = len(A_list)\n",
    "    if p == 0:\n",
    "        raise ValueError(\"Need at least one A matrix\")\n",
    "\n",
    "    k = A_list[0].shape[0]\n",
    "    for A in A_list:\n",
    "        if A.shape != (k, k):\n",
    "            raise ValueError(\"All A_i must be (k,k)\")\n",
    "\n",
    "    top = np.concatenate(A_list, axis=1)\n",
    "    if p == 1:\n",
    "        return top\n",
    "\n",
    "    eye = np.eye(k * (p - 1))\n",
    "    bottom = np.concatenate([eye, np.zeros((k * (p - 1), k))], axis=1)\n",
    "    return np.concatenate([top, bottom], axis=0)\n",
    "\n",
    "\n",
    "def is_var_stable(A_list: list[np.ndarray]) -> bool:\n",
    "    F = var_companion(A_list)\n",
    "    eig = np.linalg.eigvals(F)\n",
    "    return np.max(np.abs(eig)) < 1.0\n",
    "\n",
    "\n",
    "def simulate_varma(\n",
    "    n: int,\n",
    "    A_list: list[np.ndarray],\n",
    "    M_list: list[np.ndarray],\n",
    "    c: np.ndarray | None = None,\n",
    "    Sigma: np.ndarray | None = None,\n",
    "    burnin: int = 200,\n",
    "    seed: int = 0,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Simulate a VARMA(p,q):\n",
    "\n",
    "    y_t = c + sum_i A_i y_{t-i} + e_t + sum_j M_j e_{t-j}\n",
    "\n",
    "    Returns:\n",
    "      y: (n, k)\n",
    "      e: (n, k) innovations used\n",
    "    \"\"\"\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    p = len(A_list)\n",
    "    q = len(M_list)\n",
    "    if p == 0 and q == 0:\n",
    "        raise ValueError(\"Need p>0 or q>0\")\n",
    "\n",
    "    k = (A_list[0].shape[0] if p > 0 else M_list[0].shape[0])\n",
    "    for A in A_list:\n",
    "        if A.shape != (k, k):\n",
    "            raise ValueError(\"All A_i must be (k,k)\")\n",
    "    for M in M_list:\n",
    "        if M.shape != (k, k):\n",
    "            raise ValueError(\"All M_j must be (k,k)\")\n",
    "\n",
    "    if c is None:\n",
    "        c = np.zeros(k)\n",
    "    c = np.asarray(c, dtype=float)\n",
    "    if c.shape != (k,):\n",
    "        raise ValueError(\"c must be (k,)\")\n",
    "\n",
    "    if Sigma is None:\n",
    "        Sigma = np.eye(k)\n",
    "    Sigma = np.asarray(Sigma, dtype=float)\n",
    "    if Sigma.shape != (k, k):\n",
    "        raise ValueError(\"Sigma must be (k,k)\")\n",
    "\n",
    "    max_lag = max(p, q)\n",
    "    T = burnin + n + max_lag\n",
    "\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    e = rng_local.standard_normal((T, k)) @ L.T\n",
    "    y = np.zeros((T, k), dtype=float)\n",
    "\n",
    "    for t in range(max_lag, T):\n",
    "        yt = c.copy()\n",
    "\n",
    "        for i, A in enumerate(A_list, start=1):\n",
    "            yt += A @ y[t - i]\n",
    "\n",
    "        yt += e[t]\n",
    "\n",
    "        for j, M in enumerate(M_list, start=1):\n",
    "            yt += M @ e[t - j]\n",
    "\n",
    "        y[t] = yt\n",
    "\n",
    "    sl = slice(burnin + max_lag, burnin + max_lag + n)\n",
    "    return y[sl], e[sl]\n",
    "\n",
    "\n",
    "def var_ols_fit(y: np.ndarray, p: int, add_intercept: bool = True) -> dict:\n",
    "    \"\"\"Fit VAR(p) by multivariate OLS: Y = X B + E.\"\"\"\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if y.ndim != 2:\n",
    "        raise ValueError(\"y must be (T,k)\")\n",
    "    T, k = y.shape\n",
    "    if T <= p:\n",
    "        raise ValueError(\"Need T > p\")\n",
    "\n",
    "    # Build design matrix X and target Y\n",
    "    Y = y[p:]\n",
    "\n",
    "    X_blocks = []\n",
    "    if add_intercept:\n",
    "        X_blocks.append(np.ones((T - p, 1)))\n",
    "\n",
    "    for lag in range(1, p + 1):\n",
    "        X_blocks.append(y[p - lag : T - lag])\n",
    "\n",
    "    X = np.concatenate(X_blocks, axis=1)\n",
    "\n",
    "    B_hat, *_ = np.linalg.lstsq(X, Y, rcond=None)\n",
    "    Y_hat = X @ B_hat\n",
    "    E = Y - Y_hat\n",
    "\n",
    "    c_hat = B_hat[0] if add_intercept else np.zeros(k)\n",
    "    A_hat = []\n",
    "    offset = 1 if add_intercept else 0\n",
    "    for i in range(p):\n",
    "        A_hat.append(B_hat[offset + i * k : offset + (i + 1) * k].T)\n",
    "\n",
    "    return {\n",
    "        'c': c_hat,\n",
    "        'A': A_hat,\n",
    "        'resid': E,\n",
    "        'Sigma': (E.T @ E) / (T - p),\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "    }\n",
    "\n",
    "\n",
    "def varma_irf(A_list: list[np.ndarray], M_list: list[np.ndarray], steps: int) -> np.ndarray:\n",
    "    \"\"\"Impulse response matrices Psi_0..Psi_steps for VARMA(p,q).\"\"\"\n",
    "\n",
    "    p = len(A_list)\n",
    "    q = len(M_list)\n",
    "    k = (A_list[0].shape[0] if p > 0 else M_list[0].shape[0])\n",
    "\n",
    "    Psi = np.zeros((steps + 1, k, k))\n",
    "    Psi[0] = np.eye(k)\n",
    "\n",
    "    for h in range(1, steps + 1):\n",
    "        Mh = M_list[h - 1] if (1 <= h <= q) else np.zeros((k, k))\n",
    "        acc = Mh.copy()\n",
    "        for i in range(1, min(p, h) + 1):\n",
    "            acc += A_list[i - 1] @ Psi[h - i]\n",
    "        Psi[h] = acc\n",
    "\n",
    "    return Psi\n",
    "\n",
    "\n",
    "def varma_forecast_paths(\n",
    "    y_hist: np.ndarray,\n",
    "    e_hist: np.ndarray,\n",
    "    A_list: list[np.ndarray],\n",
    "    M_list: list[np.ndarray],\n",
    "    c: np.ndarray,\n",
    "    Sigma: np.ndarray,\n",
    "    steps: int,\n",
    "    n_paths: int = 500,\n",
    "    seed: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Monte Carlo forecast paths for VARMA(p,q), conditional on observed history.\n",
    "\n",
    "    Notes:\n",
    "    - We treat past innovations e_hist as known (in real estimation they'd be filtered).\n",
    "    - Future innovations are sampled from N(0,Sigma).\n",
    "\n",
    "    Returns: paths (n_paths, steps, k)\n",
    "    \"\"\"\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    y_hist = np.asarray(y_hist, dtype=float)\n",
    "    e_hist = np.asarray(e_hist, dtype=float)\n",
    "\n",
    "    T, k = y_hist.shape\n",
    "    if e_hist.shape != (T, k):\n",
    "        raise ValueError(\"e_hist must match y_hist shape\")\n",
    "\n",
    "    p = len(A_list)\n",
    "    q = len(M_list)\n",
    "    max_lag = max(p, q)\n",
    "    if T < max_lag:\n",
    "        raise ValueError(\"Need enough history for max(p,q)\")\n",
    "\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "\n",
    "    paths = np.zeros((n_paths, steps, k), dtype=float)\n",
    "    for s in range(n_paths):\n",
    "        y_ext = np.zeros((T + steps, k), dtype=float)\n",
    "        e_ext = np.zeros((T + steps, k), dtype=float)\n",
    "        y_ext[:T] = y_hist\n",
    "        e_ext[:T] = e_hist\n",
    "\n",
    "        e_ext[T:] = rng_local.standard_normal((steps, k)) @ L.T\n",
    "\n",
    "        for t in range(T, T + steps):\n",
    "            yt = c.copy()\n",
    "\n",
    "            for i, A in enumerate(A_list, start=1):\n",
    "                yt += A @ y_ext[t - i]\n",
    "\n",
    "            yt += e_ext[t]\n",
    "\n",
    "            for j, M in enumerate(M_list, start=1):\n",
    "                yt += M @ e_ext[t - j]\n",
    "\n",
    "            y_ext[t] = yt\n",
    "\n",
    "        paths[s] = y_ext[T:]\n",
    "\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ab3c8",
   "metadata": {},
   "source": [
    "## 5) Synthetic example: simulate a VARMA(2,1)\n",
    "\n",
    "We will:\n",
    "- simulate a 2D VARMA process,\n",
    "- forecast multiple steps ahead via Monte Carlo,\n",
    "- compute impulse responses \\(\\Psi_h\\) (shock propagation).\n",
    "\n",
    "Because estimation of VARMA is itself a topic (state space + MLE), we treat the parameters as known in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "A1 = np.array([[0.55, 0.15],\n",
    "               [-0.10, 0.35]])\n",
    "A2 = np.array([[-0.25, 0.05],\n",
    "               [0.02, -0.15]])\n",
    "A = [A1, A2]\n",
    "\n",
    "# MA(1): past shocks echo into the next step\n",
    "M1 = np.array([[0.60, 0.00],\n",
    "               [0.20, 0.30]])\n",
    "M = [M1]\n",
    "\n",
    "c = np.array([0.05, -0.02])\n",
    "Sigma = np.array([[1.0, 0.4],\n",
    "                  [0.4, 0.8]])\n",
    "\n",
    "print(\"VAR stability (AR part):\", is_var_stable(A))\n",
    "\n",
    "T = 800\n",
    "y, e = simulate_varma(n=T, A_list=A, M_list=M, c=c, Sigma=Sigma, burnin=300, seed=SEED)\n",
    "print(y.shape, e.shape)\n",
    "\n",
    "t = np.arange(T)\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.05,\n",
    "                    subplot_titles=[\"y[0]\", \"y[1]\"])\n",
    "\n",
    "for i in range(2):\n",
    "    fig.add_trace(go.Scatter(x=t, y=y[:, i], mode=\"lines\", name=f\"y{i}\"), row=i+1, col=1)\n",
    "\n",
    "fig.update_layout(height=450, title=\"Simulated VARMA(2,1) series\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd869efc",
   "metadata": {},
   "source": [
    "## 6) Baseline: fitting a VAR(p) by OLS\n",
    "\n",
    "A common practical baseline is to fit a VAR and check residual autocorrelation. If residuals show serial correlation, a VARMA (or a higher-order VAR) may be warranted.\n",
    "\n",
    "Here we fit VAR(2) by stacked OLS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = var_ols_fit(y, p=2, add_intercept=True)\n",
    "print(\"c_hat:\", fit[\"c\"])\n",
    "print(\"A1_hat:\")\n",
    "print(fit[\"A\"][0])\n",
    "print(\"A2_hat:\")\n",
    "print(fit[\"A\"][1])\n",
    "print(\"Residual covariance (Sigma_hat):\")\n",
    "print(fit[\"Sigma\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16694d5",
   "metadata": {},
   "source": [
    "## 7) Multivariate forecasts (Monte Carlo fan charts)\n",
    "\n",
    "We forecast \\(H\\) steps ahead from a cut point \\(T_0\\). Because we **simulated** the process, we know the realized past innovations \\(\\varepsilon_t\\) up to \\(T_0\\), so we can condition on them.\n",
    "\n",
    "For each future path, we sample future innovations and roll the model forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11668001",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = 650\n",
    "H = 60\n",
    "\n",
    "y_hist = y[:T0]\n",
    "e_hist = e[:T0]\n",
    "y_true = y[T0:T0+H]\n",
    "\n",
    "paths = varma_forecast_paths(\n",
    "    y_hist=y_hist,\n",
    "    e_hist=e_hist,\n",
    "    A_list=A,\n",
    "    M_list=M,\n",
    "    c=c,\n",
    "    Sigma=Sigma,\n",
    "    steps=H,\n",
    "    n_paths=800,\n",
    "    seed=SEED + 123,\n",
    ")\n",
    "\n",
    "q_lo, q_med, q_hi = np.quantile(paths, [0.05, 0.5, 0.95], axis=0)\n",
    "\n",
    "x_past = np.arange(T0)\n",
    "x_fut = np.arange(T0, T0 + H)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.05,\n",
    "                    subplot_titles=[\"Forecast y[0]\", \"Forecast y[1]\"])\n",
    "\n",
    "for i in range(2):\n",
    "    # history\n",
    "    fig.add_trace(go.Scatter(x=x_past, y=y_hist[:, i], mode=\"lines\", name=f\"y{i} history\",\n",
    "                             line=dict(color=\"rgba(0,0,0,0.55)\")), row=i+1, col=1)\n",
    "\n",
    "    # truth\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=y_true[:, i], mode=\"lines\", name=f\"y{i} truth\",\n",
    "                             line=dict(color=\"rgba(0,0,0,1.0)\", width=2)), row=i+1, col=1)\n",
    "\n",
    "    # interval\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=q_hi[:, i], mode=\"lines\", line=dict(width=0),\n",
    "                             showlegend=False, hoverinfo=\"skip\"), row=i+1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=q_lo[:, i], mode=\"lines\", line=dict(width=0),\n",
    "                             fill=\"tonexty\", fillcolor=\"rgba(0,100,200,0.18)\",\n",
    "                             name=f\"90% PI y{i}\", hoverinfo=\"skip\"), row=i+1, col=1)\n",
    "\n",
    "    # median\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=q_med[:, i], mode=\"lines\", name=f\"median y{i}\",\n",
    "                             line=dict(color=\"rgba(0,100,200,1.0)\", dash=\"dash\")), row=i+1, col=1)\n",
    "\n",
    "    fig.add_vline(x=T0, line_width=1, line_dash=\"dot\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(height=520, title=\"VARMA multivariate forecasts (median + 90% interval)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9efda",
   "metadata": {},
   "source": [
    "## 8) Shock propagation (impulse responses)\n",
    "\n",
    "Compute \\(\\Psi_h\\) for \\(h=0,\\dots,H\\). Each \\(\\Psi_h\\) is a \\(k\\times k\\) matrix:\n",
    "- columns: *which variable is shocked*\n",
    "- rows: *which variable responds*\n",
    "\n",
    "We plot the responses to a 1-s.d. shock in each component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_irf = 25\n",
    "Psi = varma_irf(A_list=A, M_list=M, steps=H_irf)\n",
    "\n",
    "# scale shocks by marginal std\n",
    "shock_std = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.10,\n",
    "    subplot_titles=[\n",
    "        \"response y0 to shock e0\",\n",
    "        \"response y0 to shock e1\",\n",
    "        \"response y1 to shock e0\",\n",
    "        \"response y1 to shock e1\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "h = np.arange(H_irf + 1)\n",
    "for shock_j in range(2):\n",
    "    u = np.zeros(2)\n",
    "    u[shock_j] = shock_std[shock_j]\n",
    "    resp = Psi @ u  # (H+1, k)\n",
    "\n",
    "    # response variable 0\n",
    "    fig.add_trace(go.Scatter(x=h, y=resp[:, 0], mode=\"lines\", name=f\"shock {shock_j}\"), row=1, col=shock_j+1)\n",
    "    # response variable 1\n",
    "    fig.add_trace(go.Scatter(x=h, y=resp[:, 1], mode=\"lines\", showlegend=False), row=2, col=shock_j+1)\n",
    "\n",
    "fig.update_layout(height=520, title=\"VARMA impulse responses (1-s.d. innovations)\")\n",
    "fig.update_xaxes(title_text=\"horizon\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea16156",
   "metadata": {},
   "source": [
    "## 9) Exercises\n",
    "\n",
    "1) Set \\(M_1=0\\) (so VARMA becomes VAR) and compare impulse responses.\n",
    "2) Increase \\(q\\) and observe how the MA part changes early-horizon dynamics.\n",
    "3) Fit higher-order VAR models to VARMA data and compare forecast accuracy vs parameter count.\n",
    "\n",
    "## References\n",
    "- Lütkepohl, *New Introduction to Multiple Time Series Analysis*\n",
    "- Hamilton, *Time Series Analysis*\n",
    "- Tsay, *Multivariate Time Series Analysis*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
