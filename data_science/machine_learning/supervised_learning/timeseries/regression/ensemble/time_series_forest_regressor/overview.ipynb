{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeriesForestRegressor (TSF): interval features + tree ensemble\n",
    "\n",
    "The **Time-Series Forest** regressor is an interval-based ensemble:\n",
    "- Each tree samples **random intervals** from the input series.\n",
    "- On each interval it computes simple **summary features** (commonly **mean**, **std**, **slope**).\n",
    "- The tree learns a mapping from these features to the target.\n",
    "- The forest prediction is the **average** across trees.\n",
    "\n",
    "This notebook implements a practical, sklearn-style **`TimeSeriesForestRegressor(min_interval=..., ...)`**.\n",
    "\n",
    "If you want a version where you provide your own feature function list, see `../composable_time_series_forest_regressor/overview.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core intuition (with math)\n",
    "\n",
    "Let $x \\in \\mathbb{R}^m$ be one univariate series of length $m$ (e.g., a sliding window of past values).\n",
    "\n",
    "For one tree $b$:\n",
    "1. Sample $K$ random intervals $I_k = [s_k, e_k)$ with $0 \\le s_k < e_k \\le m$.\n",
    "2. For each interval, compute features (examples):\n",
    "\n",
    "$$\\mu(I) = \\frac{1}{|I|}\\sum_{t \\in I} x_t$$\n",
    "\n",
    "$$\\sigma(I) = \\sqrt{\\frac{1}{|I|}\\sum_{t \\in I} (x_t - \\mu(I))^2}$$\n",
    "\n",
    "For the slope, fit a least-squares line $x_t \\approx a + bt$ over the interval indices $t=0,\\dots,|I|-1$:\n",
    "\n",
    "$$b(I) = \\frac{\\sum_t (t-\\bar t)(x_t-\\bar x)}{\\sum_t (t-\\bar t)^2}$$\n",
    "\n",
    "3. Concatenate all features into a vector $\\phi_b(x) \\in \\mathbb{R}^{3K}$.\n",
    "4. Fit a decision tree regressor $f_b$ on $(\\phi_b(x_i), y_i)$.\n",
    "\n",
    "The forest predicts:\n",
    "\n",
    "$$\\hat y(x) = \\frac{1}{B}\\sum_{b=1}^B f_b(\\phi_b(x))$$\n",
    "\n",
    "**`min_interval`** controls the shortest allowed interval length.\n",
    "- Small `min_interval` captures spikes/local shape, but features can be noisy.\n",
    "- Large `min_interval` emphasizes slower patterns (level/trend fragments), but can miss sharp events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "import numpy, pandas, sklearn, plotly\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"pandas:\", pandas.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_3d_panel(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Accept (n, m) or (n, d, m). Return (n, d, m).\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim == 2:\n",
    "        return X[:, None, :]\n",
    "    if X.ndim == 3:\n",
    "        return X\n",
    "    raise ValueError(f\"X must be 2D or 3D, got shape={X.shape}\")\n",
    "\n",
    "\n",
    "def _random_intervals(\n",
    "    n_timepoints: int,\n",
    "    n_intervals: int,\n",
    "    *,\n",
    "    min_length: int,\n",
    "    max_length: int | None,\n",
    "    rng: np.random.Generator,\n",
    ") -> list[tuple[int, int]]:\n",
    "    n_timepoints = int(n_timepoints)\n",
    "    min_length = max(2, int(min_length))\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = n_timepoints\n",
    "    max_length = int(min(max_length, n_timepoints))\n",
    "\n",
    "    if min_length > max_length:\n",
    "        raise ValueError(\"min_length cannot exceed max_length\")\n",
    "\n",
    "    intervals: list[tuple[int, int]] = []\n",
    "    for _ in range(int(n_intervals)):\n",
    "        length = int(rng.integers(min_length, max_length + 1))\n",
    "        start = int(rng.integers(0, n_timepoints - length + 1))\n",
    "        end = start + length\n",
    "        intervals.append((start, end))\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def _f_mean(seg2d: np.ndarray) -> np.ndarray:\n",
    "    return np.mean(seg2d, axis=1)\n",
    "\n",
    "\n",
    "def _f_std(seg2d: np.ndarray) -> np.ndarray:\n",
    "    return np.std(seg2d, axis=1, ddof=0)\n",
    "\n",
    "\n",
    "def _f_slope(seg2d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Least-squares slope vs time index within the interval.\"\"\"\n",
    "    seg2d = np.asarray(seg2d, dtype=float)\n",
    "    n, L = seg2d.shape\n",
    "    t = np.arange(L, dtype=float)\n",
    "    t_mean = float(t.mean())\n",
    "    denom = float(np.sum((t - t_mean) ** 2))\n",
    "    if denom == 0.0:\n",
    "        return np.zeros(n, dtype=float)\n",
    "    x_mean = np.mean(seg2d, axis=1)\n",
    "    num = np.sum((t[None, :] - t_mean) * (seg2d - x_mean[:, None]), axis=1)\n",
    "    return num / denom\n",
    "\n",
    "\n",
    "def _transform_intervals(\n",
    "    X3: np.ndarray,\n",
    "    intervals: list[tuple[int, int]],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Turn panel (n, d, m) into tabular features for the given intervals.\"\"\"\n",
    "    X3 = _as_3d_panel(X3)\n",
    "    n, d, m = X3.shape\n",
    "\n",
    "    feats = []\n",
    "    for dim in range(d):\n",
    "        x = X3[:, dim, :]\n",
    "        for (start, end) in intervals:\n",
    "            seg = x[:, start:end]\n",
    "            feats.append(_f_mean(seg))\n",
    "            feats.append(_f_std(seg))\n",
    "            feats.append(_f_slope(seg))\n",
    "\n",
    "    return np.column_stack(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesForestRegressor:\n",
    "    \"\"\"Time-Series Forest (TSF) regressor (interval features + tree ensemble).\n",
    "\n",
    "    This is a compact educational implementation with a practical API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int\n",
    "        Number of trees.\n",
    "    n_intervals : int | \"sqrt\"\n",
    "        Intervals sampled per tree. \"sqrt\" uses int(sqrt(m)).\n",
    "    min_interval : int\n",
    "        Minimum interval length.\n",
    "    max_interval : int | None\n",
    "        Maximum interval length (default: m).\n",
    "    max_depth, min_samples_leaf : DecisionTreeRegressor params\n",
    "        Passed through to each tree.\n",
    "    random_state : int | None\n",
    "        Reproducible interval sampling + trees.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        n_estimators: int = 200,\n",
    "        n_intervals: int | str = \"sqrt\",\n",
    "        min_interval: int = 3,\n",
    "        max_interval: int | None = None,\n",
    "        max_depth: int | None = None,\n",
    "        min_samples_leaf: int = 1,\n",
    "        random_state: int | None = None,\n",
    "    ) -> None:\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.n_intervals = n_intervals\n",
    "        self.min_interval = int(min_interval)\n",
    "        self.max_interval = max_interval\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.estimators_: list[DecisionTreeRegressor] | None = None\n",
    "        self.intervals_: list[list[tuple[int, int]]] | None = None\n",
    "        self.n_timepoints_: int | None = None\n",
    "        self.n_dims_: int | None = None\n",
    "\n",
    "    def _resolve_n_intervals(self, m: int) -> int:\n",
    "        if isinstance(self.n_intervals, str):\n",
    "            if self.n_intervals != \"sqrt\":\n",
    "                raise ValueError(\"n_intervals must be int or 'sqrt'\")\n",
    "            return max(1, int(np.sqrt(m)))\n",
    "        return int(self.n_intervals)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"TimeSeriesForestRegressor\":\n",
    "        X3 = _as_3d_panel(X)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        if y.ndim != 1:\n",
    "            raise ValueError(\"y must be 1D\")\n",
    "        if X3.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"X and y must have the same number of samples\")\n",
    "\n",
    "        n, d, m = X3.shape\n",
    "        self.n_timepoints_ = int(m)\n",
    "        self.n_dims_ = int(d)\n",
    "\n",
    "        n_intervals = self._resolve_n_intervals(m)\n",
    "\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.estimators_ = []\n",
    "        self.intervals_ = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            seed = int(rng.integers(0, 2**32 - 1))\n",
    "            tree_rng = np.random.default_rng(seed)\n",
    "\n",
    "            intervals = _random_intervals(\n",
    "                n_timepoints=m,\n",
    "                n_intervals=n_intervals,\n",
    "                min_length=self.min_interval,\n",
    "                max_length=self.max_interval,\n",
    "                rng=tree_rng,\n",
    "            )\n",
    "            Phi = _transform_intervals(X3, intervals)\n",
    "\n",
    "            tree = DecisionTreeRegressor(\n",
    "                random_state=seed,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "            )\n",
    "            tree.fit(Phi, y)\n",
    "\n",
    "            self.estimators_.append(tree)\n",
    "            self.intervals_.append(intervals)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.estimators_ is None or self.intervals_ is None or self.n_timepoints_ is None:\n",
    "            raise ValueError(\"Model is not fitted\")\n",
    "\n",
    "        X3 = _as_3d_panel(X)\n",
    "        if X3.shape[1] != int(self.n_dims_):\n",
    "            raise ValueError(\"X has different n_dims than seen in fit\")\n",
    "        if X3.shape[2] != int(self.n_timepoints_):\n",
    "            raise ValueError(\"X has different n_timepoints than seen in fit\")\n",
    "\n",
    "        preds = []\n",
    "        for tree, intervals in zip(self.estimators_, self.intervals_):\n",
    "            Phi = _transform_intervals(X3, intervals)\n",
    "            preds.append(tree.predict(Phi))\n",
    "\n",
    "        return np.mean(np.vstack(preds), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Synthetic daily series (trend + weekly + monthly seasonality + autocorrelated noise) ---\n",
    "\n",
    "def simulate_ar1_noise(n: int, *, phi: float, sigma: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    eps = rng.normal(0.0, sigma, size=n)\n",
    "    u = np.zeros(n)\n",
    "    for t in range(1, n):\n",
    "        u[t] = phi * u[t - 1] + eps[t]\n",
    "    return u\n",
    "\n",
    "\n",
    "n = 700\n",
    "idx = pd.date_range(\"2022-01-01\", periods=n, freq=\"D\")\n",
    "t = np.arange(n)\n",
    "\n",
    "weekly = 2.0 * np.sin(2 * np.pi * t / 7) + 0.5 * np.cos(2 * np.pi * t / 7)\n",
    "monthly = 1.3 * np.sin(2 * np.pi * t / 30) - 0.4 * np.cos(2 * np.pi * t / 30)\n",
    "trend = 0.02 * t\n",
    "noise = simulate_ar1_noise(n, phi=0.6, sigma=0.7, rng=rng)\n",
    "\n",
    "y = 50.0 + trend + weekly + monthly + noise\n",
    "y = pd.Series(y, index=idx, name=\"y\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y.index, y=y.values, name=\"y\", line=dict(color=\"black\")))\n",
    "fig.update_layout(title=\"Synthetic series\", xaxis_title=\"date\", yaxis_title=\"value\", height=420)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sliding_windows(y: np.ndarray, window_length: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    L = int(window_length)\n",
    "    if L < 2:\n",
    "        raise ValueError(\"window_length must be >= 2\")\n",
    "    if y.size <= L:\n",
    "        raise ValueError(\"y is too short for the chosen window_length\")\n",
    "\n",
    "    X = np.column_stack([y[i : y.size - L + i] for i in range(L)])\n",
    "    y_next = y[L:]\n",
    "    return X, y_next\n",
    "\n",
    "\n",
    "L = 60\n",
    "X, y_next = make_sliding_windows(y.to_numpy(), window_length=L)\n",
    "\n",
    "# time-aware split\n",
    "h = 120\n",
    "X_train, y_train = X[:-h], y_next[:-h]\n",
    "X_test, y_test = X[-h:], y_next[-h:]\n",
    "\n",
    "t_test = y.index[-h:]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf = TimeSeriesForestRegressor(\n",
    "    n_estimators=250,\n",
    "    n_intervals=\"sqrt\",\n",
    "    min_interval=5,\n",
    "    max_interval=None,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "tsf.fit(X_train, y_train)\n",
    "y_pred = tsf.predict(X_test)\n",
    "\n",
    "mae = float(mean_absolute_error(y_test, y_pred))\n",
    "rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "r2 = float(r2_score(y_test, y_pred))\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R^2:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_test, name=\"actual\", line=dict(color=\"black\")))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred, name=\"pred (1-step)\", line=dict(color=\"#4E79A7\")))\n",
    "fig.update_layout(title=\"TimeSeriesForestRegressor one-step-ahead predictions\", xaxis_title=\"date\", yaxis_title=\"value\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual intuition: show some random intervals used by one tree over one input window\n",
    "tree_idx = 0\n",
    "intervals = tsf.intervals_[tree_idx]\n",
    "\n",
    "window = X_test[-1]\n",
    "x_axis = np.arange(window.size)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_axis, y=window, name=\"window\", line=dict(color=\"black\")))\n",
    "\n",
    "for (s, e) in intervals[:12]:\n",
    "    fig.add_vrect(x0=s, x1=e - 1, fillcolor=\"rgba(78,121,167,0.10)\", line_width=0)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Example random intervals (tree {tree_idx}) over one input window\",\n",
    "    xaxis_title=\"lag index in window\",\n",
    "    yaxis_title=\"value\",\n",
    ")\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}