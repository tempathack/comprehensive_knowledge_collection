{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComposableTimeSeriesForestRegressor (sktime-style TSF)\n",
    "\n",
    "The **Time-Series Forest** (TSF) is an interval-based ensemble: each tree trains on **summary features** computed over **random intervals** of the input series.\n",
    "\n",
    "This notebook implements a **ComposableTimeSeriesForestRegressor([...])** inspired by `sktime`'s TimeSeriesForestRegressor family:\n",
    "- You choose the **summary functions** (e.g., mean/std/slope) via a list.\n",
    "- Each tree samples random intervals, extracts features, and fits a `DecisionTreeRegressor`.\n",
    "- Predictions are the average across trees.\n",
    "\n",
    "We'll also show how to use it for **one-step-ahead forecasting** via a sliding-window reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Explain TSF intuition and why interval features work\n",
    "- Implement `ComposableTimeSeriesForestRegressor([...])` in NumPy + scikit-learn\n",
    "- Demonstrate multi-seasonal forecasting via a windowed regression dataset\n",
    "- Plot forecast behavior and residual diagnostics with Plotly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm sketch\n",
    "\n",
    "Given training pairs $(X_i, y_i)$ where each $X_i$ is a univariate (or multivariate) time series of fixed length $m$.\n",
    "\n",
    "For each tree $b=1,\\dots,B$:\n",
    "1) Sample $K$ random intervals $[s_k, e_k)$ with $0\\le s_k < e_k \\le m$.\n",
    "2) For each interval, compute a set of summary features $f_1,\\dots,f_F$ (e.g., mean/std/slope) on that segment.\n",
    "3) Concatenate all interval features into a feature vector $\\phi_b(X_i)$.\n",
    "4) Fit a decision tree regressor $T_b$ on $(\\phi_b(X_i), y_i)$.\n",
    "\n",
    "Prediction is the ensemble average:\n",
    "$$\\hat{y}(X) = \\frac{1}{B}\\sum_{b=1}^{B} T_b(\\phi_b(X)).$$\n",
    "\n",
    "**Why it works (intuition):** random intervals capture local patterns (level shifts, spikes, seasonal fragments, trend segments). Trees then learn which interval statistics matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "import numpy, pandas, sklearn, scipy, plotly\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"pandas:\", pandas.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"scipy:\", scipy.__version__)\n",
    "print(\"plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_3d_panel(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Accept (n, m) or (n, d, m). Return (n, d, m).\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim == 2:\n",
    "        return X[:, None, :]\n",
    "    if X.ndim == 3:\n",
    "        return X\n",
    "    raise ValueError(f\"X must be 2D or 3D, got shape={X.shape}\")\n",
    "\n",
    "\n",
    "def _acf(x: np.ndarray, max_lag: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x - x.mean()\n",
    "    denom = float(np.dot(x, x))\n",
    "    lags = np.arange(max_lag + 1)\n",
    "    values = np.zeros(max_lag + 1)\n",
    "    values[0] = 1.0\n",
    "    if denom == 0.0:\n",
    "        return lags, values\n",
    "    for k in range(1, max_lag + 1):\n",
    "        values[k] = float(np.dot(x[k:], x[:-k]) / denom)\n",
    "    return lags, values\n",
    "\n",
    "\n",
    "def _feature_mean(seg2d: np.ndarray) -> np.ndarray:\n",
    "    return np.mean(seg2d, axis=1)\n",
    "\n",
    "\n",
    "def _feature_std(seg2d: np.ndarray) -> np.ndarray:\n",
    "    return np.std(seg2d, axis=1, ddof=0)\n",
    "\n",
    "\n",
    "def _feature_min(seg2d: np.ndarray) -> np.ndarray:\n",
    "    return np.min(seg2d, axis=1)\n",
    "\n",
    "\n",
    "def _feature_max(seg2d: np.ndarray) -> np.ndarray:\n",
    "    return np.max(seg2d, axis=1)\n",
    "\n",
    "\n",
    "def _feature_slope(seg2d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Least-squares slope vs time index within the interval.\"\"\"\n",
    "    n = seg2d.shape[1]\n",
    "    if n < 2:\n",
    "        return np.zeros(seg2d.shape[0])\n",
    "    t = np.arange(n, dtype=float)\n",
    "    t = t - t.mean()\n",
    "    denom = float(np.dot(t, t))\n",
    "    y = seg2d - seg2d.mean(axis=1, keepdims=True)\n",
    "    return (y @ t) / denom\n",
    "\n",
    "\n",
    "_BUILTIN_FEATURES: dict[str, callable] = {\n",
    "    \"mean\": _feature_mean,\n",
    "    \"std\": _feature_std,\n",
    "    \"slope\": _feature_slope,\n",
    "    \"min\": _feature_min,\n",
    "    \"max\": _feature_max,\n",
    "}\n",
    "\n",
    "\n",
    "def _resolve_feature_functions(feature_functions) -> list[tuple[str, callable]]:\n",
    "    \"\"\"Return list of (name, f(seg2d)->(n_samples,)).\"\"\"\n",
    "    if feature_functions is None:\n",
    "        feature_functions = [\"mean\", \"std\", \"slope\"]\n",
    "\n",
    "    resolved: list[tuple[str, callable]] = []\n",
    "    for i, ff in enumerate(feature_functions):\n",
    "        if isinstance(ff, str):\n",
    "            if ff not in _BUILTIN_FEATURES:\n",
    "                raise ValueError(f\"Unknown feature '{ff}'. Available: {sorted(_BUILTIN_FEATURES)}\")\n",
    "            resolved.append((ff, _BUILTIN_FEATURES[ff]))\n",
    "            continue\n",
    "\n",
    "        if isinstance(ff, tuple) and len(ff) == 2 and isinstance(ff[0], str) and callable(ff[1]):\n",
    "            name, func = ff\n",
    "            resolved.append((name, func))\n",
    "            continue\n",
    "\n",
    "        if callable(ff):\n",
    "            name = getattr(ff, \"__name__\", f\"feature_{i}\")\n",
    "\n",
    "            def _wrapped(seg2d: np.ndarray, _f=ff) -> np.ndarray:\n",
    "                return np.apply_along_axis(_f, 1, seg2d).astype(float)\n",
    "\n",
    "            resolved.append((name, _wrapped))\n",
    "            continue\n",
    "\n",
    "        raise TypeError(\"feature_functions must contain str, callable, or (name, callable)\")\n",
    "\n",
    "    # make names unique\n",
    "    seen: dict[str, int] = {}\n",
    "    unique: list[tuple[str, callable]] = []\n",
    "    for name, func in resolved:\n",
    "        if name not in seen:\n",
    "            seen[name] = 1\n",
    "            unique.append((name, func))\n",
    "        else:\n",
    "            seen[name] += 1\n",
    "            unique.append((f\"{name}_{seen[name]}\", func))\n",
    "    return unique\n",
    "\n",
    "\n",
    "def _random_intervals(\n",
    "    n_timepoints: int,\n",
    "    n_intervals: int,\n",
    "    *,\n",
    "    min_length: int,\n",
    "    max_length: int | None,\n",
    "    rng: np.random.Generator,\n",
    ") -> list[tuple[int, int]]:\n",
    "    n_timepoints = int(n_timepoints)\n",
    "    min_length = int(min_length)\n",
    "    if min_length < 2:\n",
    "        min_length = 2\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = n_timepoints\n",
    "    max_length = int(min(max_length, n_timepoints))\n",
    "\n",
    "    if min_length > max_length:\n",
    "        raise ValueError(\"min_length cannot exceed max_length\")\n",
    "\n",
    "    intervals: list[tuple[int, int]] = []\n",
    "    for _ in range(int(n_intervals)):\n",
    "        length = int(rng.integers(min_length, max_length + 1))\n",
    "        start = int(rng.integers(0, n_timepoints - length + 1))\n",
    "        end = start + length\n",
    "        intervals.append((start, end))\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def _transform_intervals(\n",
    "    X3: np.ndarray,\n",
    "    intervals: list[tuple[int, int]],\n",
    "    feature_functions: list[tuple[str, callable]],\n",
    ") -> np.ndarray:\n",
    "    n, d, _m = X3.shape\n",
    "    features: list[np.ndarray] = []\n",
    "    for dim in range(d):\n",
    "        Xd = X3[:, dim, :]\n",
    "        for (start, end) in intervals:\n",
    "            seg = Xd[:, start:end]\n",
    "            for _name, func in feature_functions:\n",
    "                features.append(func(seg).reshape(n, 1))\n",
    "    if not features:\n",
    "        return np.zeros((n, 0), dtype=float)\n",
    "    return np.concatenate(features, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposableTimeSeriesForestRegressor:\n",
    "    \"\"\"sktime-style TSF regressor (interval features + tree ensemble).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_functions : list\n",
    "        List of features to compute per interval.\n",
    "        Each entry can be:\n",
    "        - str in {\"mean\",\"std\",\"slope\",\"min\",\"max\"}\n",
    "        - callable that maps 1D -> scalar\n",
    "        - (name, callable) where callable maps seg2d -> vector (n_samples,)\n",
    "    n_estimators : int\n",
    "        Number of trees.\n",
    "    n_intervals : int | \"sqrt\"\n",
    "        Number of random intervals per tree. \"sqrt\" uses int(sqrt(m)).\n",
    "    min_interval_length : int\n",
    "        Minimum interval length.\n",
    "    max_interval_length : int | None\n",
    "        Maximum interval length (None -> full length).\n",
    "    bootstrap : bool\n",
    "        Bootstrap samples per tree.\n",
    "    random_state : int | None\n",
    "        Seed.\n",
    "    tree_params : dict | None\n",
    "        Passed to sklearn DecisionTreeRegressor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_functions=None,\n",
    "        *,\n",
    "        n_estimators: int = 200,\n",
    "        n_intervals: int | str = \"sqrt\",\n",
    "        min_interval_length: int = 3,\n",
    "        max_interval_length: int | None = None,\n",
    "        bootstrap: bool = True,\n",
    "        random_state: int | None = None,\n",
    "        tree_params: dict | None = None,\n",
    "    ):\n",
    "        self.feature_functions = feature_functions\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.n_intervals = n_intervals\n",
    "        self.min_interval_length = int(min_interval_length)\n",
    "        self.max_interval_length = max_interval_length\n",
    "        self.bootstrap = bool(bootstrap)\n",
    "        self.random_state = random_state\n",
    "        self.tree_params = {} if tree_params is None else dict(tree_params)\n",
    "\n",
    "        self.feature_functions_: list[tuple[str, callable]] | None = None\n",
    "        self.intervals_: list[list[tuple[int, int]]] | None = None\n",
    "        self.estimators_: list[DecisionTreeRegressor] | None = None\n",
    "        self.n_timepoints_: int | None = None\n",
    "        self.n_dims_: int | None = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X3 = _as_3d_panel(X)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        if X3.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"X and y must have the same number of samples\")\n",
    "\n",
    "        n, d, m = X3.shape\n",
    "        self.n_timepoints_ = int(m)\n",
    "        self.n_dims_ = int(d)\n",
    "        self.feature_functions_ = _resolve_feature_functions(self.feature_functions)\n",
    "\n",
    "        if isinstance(self.n_intervals, str):\n",
    "            if self.n_intervals != \"sqrt\":\n",
    "                raise ValueError(\"n_intervals must be int or 'sqrt'\")\n",
    "            n_intervals = max(1, int(np.sqrt(m)))\n",
    "        else:\n",
    "            n_intervals = max(1, int(self.n_intervals))\n",
    "\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.estimators_ = []\n",
    "        self.intervals_ = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            seed = int(rng.integers(0, 2**32 - 1))\n",
    "            r = np.random.default_rng(seed)\n",
    "\n",
    "            intervals = _random_intervals(\n",
    "                n_timepoints=m,\n",
    "                n_intervals=n_intervals,\n",
    "                min_length=self.min_interval_length,\n",
    "                max_length=self.max_interval_length,\n",
    "                rng=r,\n",
    "            )\n",
    "            Phi = _transform_intervals(X3, intervals, self.feature_functions_)\n",
    "\n",
    "            if self.bootstrap:\n",
    "                idx = r.integers(0, n, size=n)\n",
    "                Phi_fit = Phi[idx]\n",
    "                y_fit = y[idx]\n",
    "            else:\n",
    "                Phi_fit = Phi\n",
    "                y_fit = y\n",
    "\n",
    "            tree = DecisionTreeRegressor(random_state=seed, **self.tree_params)\n",
    "            tree.fit(Phi_fit, y_fit)\n",
    "\n",
    "            self.estimators_.append(tree)\n",
    "            self.intervals_.append(intervals)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        if self.estimators_ is None or self.intervals_ is None or self.feature_functions_ is None:\n",
    "            raise RuntimeError(\"Call fit() before predict().\")\n",
    "\n",
    "        X3 = _as_3d_panel(X)\n",
    "        n, d, m = X3.shape\n",
    "        if m != self.n_timepoints_ or d != self.n_dims_:\n",
    "            raise ValueError(\n",
    "                f\"X must have shape (n,{self.n_dims_},{self.n_timepoints_}) (or (n,{self.n_timepoints_})), got {X3.shape}\"\n",
    "            )\n",
    "\n",
    "        preds = np.zeros(n, dtype=float)\n",
    "        for tree, intervals in zip(self.estimators_, self.intervals_):\n",
    "            Phi = _transform_intervals(X3, intervals, self.feature_functions_)\n",
    "            preds += tree.predict(Phi)\n",
    "        return preds / len(self.estimators_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: multi-seasonal one-step forecasting via sliding windows\n",
    "\n",
    "We generate a single time series with two seasonalities (weekly + ~monthly) and correlated noise.\n",
    "\n",
    "Then we build a supervised dataset:\n",
    "- input: the last $L$ observations\n",
    "- target: the next observation\n",
    "\n",
    "This reduction turns forecasting into regression on fixed-length time series windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ar1_noise(n: int, *, phi: float, sigma: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    eps = rng.normal(0.0, sigma, size=n)\n",
    "    u = np.zeros(n)\n",
    "    for t in range(1, n):\n",
    "        u[t] = phi * u[t - 1] + eps[t]\n",
    "    return u\n",
    "\n",
    "\n",
    "n = 700\n",
    "idx = pd.date_range(\"2022-01-01\", periods=n, freq=\"D\")\n",
    "t = np.arange(n)\n",
    "\n",
    "weekly = 2.0 * np.sin(2 * np.pi * t / 7) + 0.5 * np.cos(2 * np.pi * t / 7)\n",
    "monthly = 1.3 * np.sin(2 * np.pi * t / 30) - 0.4 * np.cos(2 * np.pi * t / 30)\n",
    "trend = 0.02 * t\n",
    "noise = simulate_ar1_noise(n, phi=0.6, sigma=0.7, rng=rng)\n",
    "\n",
    "y = 50.0 + trend + weekly + monthly + noise\n",
    "y = pd.Series(y, index=idx, name=\"y\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y.index, y=y, name=\"y\", line=dict(color=\"black\")))\n",
    "fig.update_layout(title=\"Synthetic multi-seasonal series\", xaxis_title=\"date\", yaxis_title=\"value\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sliding_windows(y: np.ndarray, window_length: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    L = int(window_length)\n",
    "    if L < 2:\n",
    "        raise ValueError(\"window_length must be >= 2\")\n",
    "    if y.size <= L:\n",
    "        raise ValueError(\"y is too short for the chosen window_length\")\n",
    "\n",
    "    X = np.column_stack([y[i : y.size - L + i] for i in range(L)])\n",
    "    y_next = y[L:]\n",
    "    return X, y_next\n",
    "\n",
    "\n",
    "L = 60\n",
    "X, y_next = make_sliding_windows(y.to_numpy(), window_length=L)\n",
    "\n",
    "# time-aware split\n",
    "h = 120\n",
    "X_train, y_train = X[:-h], y_next[:-h]\n",
    "X_test, y_test = X[-h:], y_next[-h:]\n",
    "t_test = y.index[-h:]\n",
    "\n",
    "tsf = ComposableTimeSeriesForestRegressor(\n",
    "    [\"mean\", \"std\", \"slope\", \"min\", \"max\"],\n",
    "    n_estimators=150,\n",
    "    n_intervals=\"sqrt\",\n",
    "    min_interval_length=5,\n",
    "    bootstrap=True,\n",
    "    random_state=7,\n",
    "    tree_params={\"max_depth\": 12, \"min_samples_leaf\": 3},\n",
    ")\n",
    "\n",
    "tsf.fit(X_train, y_train)\n",
    "y_pred = tsf.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE:  {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R^2:  {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-step-ahead predictions over the test horizon\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_test, name=\"actual\", line=dict(color=\"black\")))\n",
    "fig.add_trace(go.Scatter(x=t_test, y=y_pred, name=\"pred (1-step)\", line=dict(color=\"#4E79A7\")))\n",
    "fig.update_layout(title=\"TSF one-step-ahead predictions\", xaxis_title=\"date\", yaxis_title=\"value\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y_test, y=y_pred, mode=\"markers\", name=\"points\", marker=dict(opacity=0.7)))\n",
    "lo = min(float(np.min(y_test)), float(np.min(y_pred)))\n",
    "hi = max(float(np.max(y_test)), float(np.max(y_pred)))\n",
    "fig.add_trace(go.Scatter(x=[lo, hi], y=[lo, hi], mode=\"lines\", name=\"y=x\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "fig.update_layout(title=\"Actual vs predicted\", xaxis_title=\"actual\", yaxis_title=\"predicted\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_forecast(model, history: np.ndarray, steps: int, window_length: int) -> np.ndarray:\n",
    "    history = np.asarray(history, dtype=float).tolist()\n",
    "    preds = []\n",
    "    for _ in range(int(steps)):\n",
    "        x = np.asarray(history[-window_length:], dtype=float)[None, :]\n",
    "        y_hat = float(model.predict(x)[0])\n",
    "        preds.append(y_hat)\n",
    "        history.append(y_hat)\n",
    "    return np.asarray(preds)\n",
    "\n",
    "\n",
    "# Compare recursive multi-step vs ground-truth over a shorter horizon\n",
    "h2 = 45\n",
    "start = len(y) - h\n",
    "hist = y.to_numpy()[:start]\n",
    "truth = y.to_numpy()[start : start + h2]\n",
    "t_h2 = y.index[start : start + h2]\n",
    "\n",
    "rec = recursive_forecast(tsf, history=hist, steps=h2, window_length=L)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y.index[start - 120 : start], y=y.to_numpy()[start - 120 : start], name=\"history\", line=dict(color=\"rgba(0,0,0,0.35)\")))\n",
    "fig.add_trace(go.Scatter(x=t_h2, y=truth, name=\"truth\", line=dict(color=\"black\")))\n",
    "fig.add_trace(go.Scatter(x=t_h2, y=rec, name=\"recursive forecast\", line=dict(color=\"#E15759\")))\n",
    "fig.update_layout(title=\"Recursive multi-step forecast (reduction)\", xaxis_title=\"date\", yaxis_title=\"value\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual diagnostics (test horizon)\n",
    "resid = y_test - y_pred\n",
    "print(\"residual mean:\", float(np.mean(resid)))\n",
    "print(\"residual std:\", float(np.std(resid, ddof=1)))\n",
    "print(\"Jarque-Bera:\", stats.jarque_bera(resid))\n",
    "\n",
    "lags, acf_vals = _acf(resid, max_lag=30)\n",
    "bound = 1.96 / np.sqrt(resid.size)\n",
    "\n",
    "# QQ data\n",
    "nq = resid.size\n",
    "p = (np.arange(1, nq + 1) - 0.5) / nq\n",
    "theoretical = stats.norm.ppf(p)\n",
    "sample_q = np.sort((resid - resid.mean()) / resid.std(ddof=1))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Residuals over time\", \"Residual histogram\", \"Residual ACF\", \"QQ plot (std residuals)\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=t_test, y=resid, name=\"residuals\", line=dict(color=\"#4E79A7\")), row=1, col=1)\n",
    "fig.add_hline(y=0, line=dict(color=\"black\", dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=resid, nbinsx=25, name=\"hist\", marker_color=\"#4E79A7\"), row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(x=lags, y=acf_vals, name=\"ACF(resid)\", marker_color=\"#4E79A7\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[0, lags.max()], y=[bound, bound], mode=\"lines\", line=dict(color=\"gray\", dash=\"dash\"), showlegend=False), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[0, lags.max()], y=[-bound, -bound], mode=\"lines\", line=dict(color=\"gray\", dash=\"dash\"), showlegend=False), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=theoretical, y=sample_q, mode=\"markers\", name=\"QQ\", marker=dict(color=\"#4E79A7\")), row=2, col=2)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[theoretical.min(), theoretical.max()], y=[theoretical.min(), theoretical.max()], mode=\"lines\", line=dict(color=\"black\", dash=\"dash\"), showlegend=False),\n",
    "    row=2,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(height=750, title=\"Residual diagnostics (test horizon)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual intuition: show random intervals used by one tree on the last input window\n",
    "tree_idx = 0\n",
    "intervals = tsf.intervals_[tree_idx]\n",
    "\n",
    "window = X_test[-1]\n",
    "x_axis = np.arange(window.size)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_axis, y=window, name=\"window\", line=dict(color=\"black\")))\n",
    "\n",
    "for (s, e) in intervals[:12]:\n",
    "    fig.add_vrect(x0=s, x1=e - 1, fillcolor=\"rgba(78,121,167,0.10)\", line_width=0)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Example random intervals (tree {tree_idx}) over one input window\",\n",
    "    xaxis_title=\"lag index in window\",\n",
    "    yaxis_title=\"value\",\n",
    ")\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
