{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030f5beb",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) — Classification & Regression (From Scratch)\n",
    "\n",
    "KNN is one of the most intuitive ML algorithms:\n",
    "\n",
    "- **store** the training data\n",
    "- for a new point, **look up the K closest points**\n",
    "- **vote** (classification) or **average** (regression)\n",
    "\n",
    "It’s often described as *lazy learning*: there’s almost no “training” — the work happens at prediction time.\n",
    "\n",
    "## What you’ll learn\n",
    "- the core idea (with geometry + intuition)\n",
    "- the math for classification and regression\n",
    "- why **feature scaling** matters\n",
    "- how to implement KNN from scratch in NumPy\n",
    "- how to use `sklearn` KNN models and understand their key parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = \"notebook\"\n",
    "rng = np.random.default_rng(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df855e5",
   "metadata": {},
   "source": [
    "## 1) Intuition: “ask your closest neighbors”\n",
    "\n",
    "Imagine you move into a new city and want to predict the **rent** for an apartment.\n",
    "\n",
    "You might ask:\n",
    "\n",
    "> “What do *similar* apartments nearby cost?”\n",
    "\n",
    "That’s KNN.\n",
    "\n",
    "- Similarity is measured by a **distance** (e.g., Euclidean distance).\n",
    "- The “answer” is based on the **K nearest examples**.\n",
    "\n",
    "KNN is like a well-organized memory:\n",
    "\n",
    "- **no complicated training**\n",
    "- **excellent for weird-shaped patterns**\n",
    "- but can be **slow** at prediction time if the dataset is huge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9b62c",
   "metadata": {},
   "source": [
    "## 2) The algorithm (math + notation)\n",
    "\n",
    "Let:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{n \\times d}$ be the training features\n",
    "- $y$ be the targets\n",
    "- $x \\in \\mathbb{R}^d$ be a query point\n",
    "\n",
    "### 2.1 Distances\n",
    "We compute a distance $D(x, x_i)$ between the query point and every training point.\n",
    "\n",
    "A common family is the **Minkowski distance**:\n",
    "\n",
    "$$\n",
    "D_p(x, x_i) = \\left(\\sum_{j=1}^d |x_j - x_{i,j}|^p \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "- $p=2$ → Euclidean distance\n",
    "- $p=1$ → Manhattan distance\n",
    "\n",
    "### 2.2 Classification\n",
    "Let $\\mathcal{N}_k(x)$ be the set of indices of the $k$ nearest neighbors.\n",
    "\n",
    "**Uniform vote:**\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\arg\\max_c \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbf{1}[y_i = c]\n",
    "$$\n",
    "\n",
    "**Distance-weighted vote (common):**\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1}{D(x, x_i) + \\varepsilon}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\hat{y}(x) = \\arg\\max_c \\sum_{i \\in \\mathcal{N}_k(x)} w_i \\; \\mathbf{1}[y_i = c]\n",
    "$$\n",
    "\n",
    "### 2.3 Regression\n",
    "**Uniform average:**\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)} y_i\n",
    "$$\n",
    "\n",
    "**Distance-weighted average:**\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\frac{\\sum_{i \\in \\mathcal{N}_k(x)} w_i y_i}{\\sum_{i \\in \\mathcal{N}_k(x)} w_i}\n",
    "$$\n",
    "\n",
    "### 2.4 Big practical gotcha: scaling\n",
    "If one feature is measured in **kilometers** and another in **millimeters**, the kilometer feature dominates the distance.\n",
    "\n",
    "So KNN almost always needs feature scaling (e.g. standardization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d21ff",
   "metadata": {},
   "source": [
    "## 3) Geometry: L1 vs L2 distance “balls”\n",
    "\n",
    "Distance isn’t just a formula — it’s a **shape**.\n",
    "\n",
    "- L2 distance (Euclidean) → circles (2D)\n",
    "- L1 distance (Manhattan) → diamonds (2D)\n",
    "\n",
    "This changes which points count as “neighbors”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize iso-distance curves around the origin\n",
    "r = 1.0\n",
    "\n",
    "# L2 circle\n",
    "theta = np.linspace(0, 2*np.pi, 400)\n",
    "x_circle = r * np.cos(theta)\n",
    "y_circle = r * np.sin(theta)\n",
    "\n",
    "# L1 diamond: |x| + |y| = r\n",
    "x_diamond = np.array([0, r, 0, -r, 0])\n",
    "y_diamond = np.array([r, 0, -r, 0, r])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_circle, y=y_circle, mode=\"lines\", name=\"L2: x²+y²=1\"))\n",
    "fig.add_trace(go.Scatter(x=x_diamond, y=y_diamond, mode=\"lines\", name=\"L1: |x|+|y|=1\"))\n",
    "fig.update_layout(\n",
    "    title=\"Iso-distance curves in 2D\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    width=700,\n",
    "    height=450,\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ea759",
   "metadata": {},
   "source": [
    "## 4) A toy classification dataset (2D)\n",
    "\n",
    "We’ll use `make_moons` because it’s **not linearly separable** — perfect to show why KNN can be strong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=7)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_tr[:, 0],\n",
    "    y=X_tr[:, 1],\n",
    "    color=y_tr.astype(str),\n",
    "    title=\"Training set (make_moons)\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"class\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=7, opacity=0.8))\n",
    "fig.update_layout(width=700, height=450)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20acbe16",
   "metadata": {},
   "source": [
    "## 5) KNN from scratch (NumPy)\n",
    "\n",
    "We’ll implement:\n",
    "\n",
    "- pairwise distances\n",
    "- “k nearest” lookup\n",
    "- classifier (uniform + distance-weighted)\n",
    "- regressor (uniform + distance-weighted)\n",
    "\n",
    "We’ll keep it readable and educational.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_minkowski_distances(X_query: np.ndarray, X_train: np.ndarray, p: float = 2.0) -> np.ndarray:\n",
    "    X_query = np.asarray(X_query, dtype=float)\n",
    "    X_train = np.asarray(X_train, dtype=float)\n",
    "\n",
    "    if X_query.ndim != 2 or X_train.ndim != 2:\n",
    "        raise ValueError(\"X_query and X_train must be 2D arrays\")\n",
    "    if X_query.shape[1] != X_train.shape[1]:\n",
    "        raise ValueError(\"X_query and X_train must have the same number of features\")\n",
    "    if p <= 0:\n",
    "        raise ValueError(\"p must be > 0\")\n",
    "\n",
    "    if p == 2:\n",
    "        # (a-b)^2 = a^2 + b^2 - 2ab trick\n",
    "        q_sq = np.sum(X_query**2, axis=1, keepdims=True)  # (m,1)\n",
    "        t_sq = np.sum(X_train**2, axis=1)  # (n,)\n",
    "        d2 = q_sq + t_sq[None, :] - 2.0 * (X_query @ X_train.T)\n",
    "        d2 = np.maximum(d2, 0.0)\n",
    "        return np.sqrt(d2)\n",
    "\n",
    "    return np.sum(np.abs(X_query[:, None, :] - X_train[None, :, :]) ** p, axis=2) ** (1.0 / p)\n",
    "\n",
    "\n",
    "def kneighbors_indices_and_distances(\n",
    "    X_train: np.ndarray,\n",
    "    X_query: np.ndarray,\n",
    "    k: int,\n",
    "    p: float = 2.0,\n",
    "):\n",
    "    distances = pairwise_minkowski_distances(X_query, X_train, p=p)  # (m,n)\n",
    "\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be >= 1\")\n",
    "    if k > distances.shape[1]:\n",
    "        raise ValueError(\"k cannot exceed number of training samples\")\n",
    "\n",
    "    idx = np.argpartition(distances, kth=k - 1, axis=1)[:, :k]  # (m,k), unsorted\n",
    "    rows = np.arange(distances.shape[0])[:, None]\n",
    "    d_k = distances[rows, idx]\n",
    "\n",
    "    order = np.argsort(d_k, axis=1)\n",
    "    idx_sorted = idx[rows, order]\n",
    "    d_sorted = d_k[rows, order]\n",
    "    return idx_sorted, d_sorted\n",
    "\n",
    "\n",
    "class ScratchKNNClassifier:\n",
    "    def __init__(self, n_neighbors: int = 5, p: float = 2.0, weights: str = \"uniform\", eps: float = 1e-9):\n",
    "        self.n_neighbors = int(n_neighbors)\n",
    "        self.p = float(p)\n",
    "        self.weights = weights\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X_ = np.asarray(X, dtype=float)\n",
    "        self.y_ = np.asarray(y)\n",
    "        self.classes_, self.y_encoded_ = np.unique(self.y_, return_inverse=True)\n",
    "        return self\n",
    "\n",
    "    def _weights(self, distances: np.ndarray) -> np.ndarray:\n",
    "        if self.weights == \"uniform\":\n",
    "            return np.ones_like(distances, dtype=float)\n",
    "        if self.weights == \"distance\":\n",
    "            return 1.0 / (distances + self.eps)\n",
    "        raise ValueError(\"weights must be 'uniform' or 'distance'\")\n",
    "\n",
    "    def predict_proba(self, X_query: np.ndarray) -> np.ndarray:\n",
    "        idx, d = kneighbors_indices_and_distances(self.X_, np.asarray(X_query, dtype=float), self.n_neighbors, p=self.p)\n",
    "        w = self._weights(d)\n",
    "        neighbor_labels = self.y_encoded_[idx]\n",
    "\n",
    "        n_query = idx.shape[0]\n",
    "        n_classes = self.classes_.shape[0]\n",
    "        proba = np.zeros((n_query, n_classes), dtype=float)\n",
    "\n",
    "        w_sum = np.sum(w, axis=1, keepdims=True)\n",
    "        for c in range(n_classes):\n",
    "            proba[:, c] = np.sum(w * (neighbor_labels == c), axis=1) / w_sum[:, 0]\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X_query: np.ndarray) -> np.ndarray:\n",
    "        proba = self.predict_proba(X_query)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "\n",
    "class ScratchKNNRegressor:\n",
    "    def __init__(self, n_neighbors: int = 5, p: float = 2.0, weights: str = \"uniform\", eps: float = 1e-9):\n",
    "        self.n_neighbors = int(n_neighbors)\n",
    "        self.p = float(p)\n",
    "        self.weights = weights\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X_ = np.asarray(X, dtype=float)\n",
    "        self.y_ = np.asarray(y, dtype=float)\n",
    "        return self\n",
    "\n",
    "    def _weights(self, distances: np.ndarray) -> np.ndarray:\n",
    "        if self.weights == \"uniform\":\n",
    "            return np.ones_like(distances, dtype=float)\n",
    "        if self.weights == \"distance\":\n",
    "            return 1.0 / (distances + self.eps)\n",
    "        raise ValueError(\"weights must be 'uniform' or 'distance'\")\n",
    "\n",
    "    def predict(self, X_query: np.ndarray) -> np.ndarray:\n",
    "        idx, d = kneighbors_indices_and_distances(self.X_, np.asarray(X_query, dtype=float), self.n_neighbors, p=self.p)\n",
    "        w = self._weights(d)\n",
    "        y_neighbors = self.y_[idx]\n",
    "        return np.sum(w * y_neighbors, axis=1) / np.sum(w, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538d9f0",
   "metadata": {},
   "source": [
    "### 5.1 One query point in slow motion\n",
    "\n",
    "Let’s pick a single test point and literally see which points become its neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_knn = ScratchKNNClassifier(n_neighbors=10, p=2, weights=\"distance\").fit(X_tr, y_tr)\n",
    "\n",
    "xq = X_te[0:1]\n",
    "yq = y_te[0]\n",
    "\n",
    "idx, d = kneighbors_indices_and_distances(X_tr, xq, k=10, p=2)\n",
    "idx = idx[0]\n",
    "d = d[0]\n",
    "\n",
    "print(\"Query true class:\", yq)\n",
    "print(\"Nearest neighbor distances:\", np.round(d, 3))\n",
    "print(\"Nearest neighbor labels:\", y_tr[idx])\n",
    "\n",
    "# Plot: training points, query point, and highlight neighbors\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_tr[:, 0], y=X_tr[:, 1], mode=\"markers\",\n",
    "                         marker=dict(color=y_tr, colorscale=\"Viridis\", size=7, opacity=0.6),\n",
    "                         name=\"train\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xq[:, 0], y=xq[:, 1], mode=\"markers\",\n",
    "                         marker=dict(symbol=\"star\", size=16, color=\"red\"),\n",
    "                         name=\"query\"))\n",
    "\n",
    "neighbors = X_tr[idx]\n",
    "fig.add_trace(go.Scatter(x=neighbors[:, 0], y=neighbors[:, 1], mode=\"markers\",\n",
    "                         marker=dict(size=12, color=\"black\", symbol=\"circle-open\"),\n",
    "                         name=\"neighbors\"))\n",
    "\n",
    "# draw thin lines from query to neighbors\n",
    "for pt in neighbors:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[xq[0, 0], pt[0]],\n",
    "        y=[xq[0, 1], pt[1]],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"rgba(0,0,0,0.15)\", width=1),\n",
    "        showlegend=False,\n",
    "    ))\n",
    "\n",
    "fig.update_layout(title=\"A query point and its 10 nearest neighbors\", width=750, height=500)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76210f7d",
   "metadata": {},
   "source": [
    "### 5.2 Decision boundary (from scratch)\n",
    "\n",
    "To visualize KNN, we’ll compute predictions on a grid and color the plane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27add673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn_boundary_2d(model, X_train, y_train, title: str, grid_steps: int = 220):\n",
    "    x_min, x_max = X_train[:, 0].min() - 0.8, X_train[:, 0].max() + 0.8\n",
    "    y_min, y_max = X_train[:, 1].min() - 0.8, X_train[:, 1].max() + 0.8\n",
    "\n",
    "    xs = np.linspace(x_min, x_max, grid_steps)\n",
    "    ys = np.linspace(y_min, y_max, grid_steps)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(grid)\n",
    "        z = proba[:, 1].reshape(xx.shape)\n",
    "        z_title = \"P(class=1)\"\n",
    "    else:\n",
    "        z = model.predict(grid).reshape(xx.shape)\n",
    "        z_title = \"prediction\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Contour(\n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        z=z,\n",
    "        colorscale=\"RdBu\",\n",
    "        opacity=0.75,\n",
    "        contours=dict(showlines=False),\n",
    "        colorbar=dict(title=z_title),\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train[:, 0],\n",
    "        y=X_train[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=y_train, colorscale=\"Viridis\", size=6, line=dict(width=0.5, color=\"white\")),\n",
    "        name=\"train\",\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title=title, width=750, height=520)\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    return fig\n",
    "\n",
    "\n",
    "for k in [1, 5, 25]:\n",
    "    m = ScratchKNNClassifier(n_neighbors=k, p=2, weights=\"uniform\").fit(X_tr, y_tr)\n",
    "    fig = plot_knn_boundary_2d(m, X_tr, y_tr, title=f\"Scratch KNN boundary (k={k}, uniform)\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd91341",
   "metadata": {},
   "source": [
    "### 5.3 Bias–variance intuition: small k vs large k\n",
    "\n",
    "K is like a **zoom level**:\n",
    "\n",
    "- **k=1**: super local, very wiggly boundary → low bias, high variance\n",
    "- **large k**: very smooth boundary → higher bias, lower variance\n",
    "\n",
    "A useful mental model:\n",
    "\n",
    "> “How many neighbors should I ask for advice?”\n",
    ">\n",
    "> If you ask **one** person, you might get an extreme opinion.\n",
    "> If you ask **a hundred**, you get a stable average — but you might miss niche local context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352267b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_for_k(k: int, weights: str = \"uniform\") -> float:\n",
    "    model = ScratchKNNClassifier(n_neighbors=k, p=2, weights=weights).fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    return float(accuracy_score(y_te, y_pred))\n",
    "\n",
    "ks = list(range(1, 51, 2))\n",
    "acc_uniform = [accuracy_for_k(k, weights=\"uniform\") for k in ks]\n",
    "acc_distance = [accuracy_for_k(k, weights=\"distance\") for k in ks]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ks, y=acc_uniform, mode=\"lines+markers\", name=\"uniform\"))\n",
    "fig.add_trace(go.Scatter(x=ks, y=acc_distance, mode=\"lines+markers\", name=\"distance\"))\n",
    "fig.update_layout(\n",
    "    title=\"Scratch KNN test accuracy vs k\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"accuracy\",\n",
    "    width=750,\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "best_k = ks[int(np.argmax(acc_distance))]\n",
    "print(\"Best k (distance weights) in this sweep:\", best_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91dc6f",
   "metadata": {},
   "source": [
    "## 6) KNN regression (1D example)\n",
    "\n",
    "Regression KNN is like:\n",
    "\n",
    "> “What’s the average target value of the nearest examples?”\n",
    "\n",
    "It produces a **piecewise smooth** curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D regression dataset\n",
    "n = 120\n",
    "x = np.linspace(-3, 3, n)\n",
    "noise = rng.normal(0, 0.25, size=n)\n",
    "y = np.sin(x) + 0.2 * x + noise\n",
    "\n",
    "X1 = x.reshape(-1, 1)\n",
    "\n",
    "# Train/test split\n",
    "idx = rng.permutation(n)\n",
    "train_idx, test_idx = idx[:90], idx[90:]\n",
    "X1_tr, y_tr_reg = X1[train_idx], y[train_idx]\n",
    "X1_te, y_te_reg = X1[test_idx], y[test_idx]\n",
    "\n",
    "x_grid = np.linspace(x.min() - 0.2, x.max() + 0.2, 500).reshape(-1, 1)\n",
    "\n",
    "fig = px.scatter(x=X1_tr[:, 0], y=y_tr_reg, title=\"1D regression training data\", labels={\"x\": \"x\", \"y\": \"y\"})\n",
    "fig.update_traces(marker=dict(size=7, opacity=0.8))\n",
    "fig.update_layout(width=750, height=450)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X1_tr[:, 0], y=y_tr_reg, mode=\"markers\", name=\"train\", marker=dict(size=7, opacity=0.7)))\n",
    "\n",
    "for k in [1, 5, 15, 35]:\n",
    "    model = ScratchKNNRegressor(n_neighbors=k, p=2, weights=\"uniform\").fit(X1_tr, y_tr_reg)\n",
    "    y_grid = model.predict(x_grid)\n",
    "    y_pred_te = model.predict(X1_te)\n",
    "    fig.add_trace(go.Scatter(x=x_grid[:, 0], y=y_grid, mode=\"lines\", name=f\"k={k} (RMSE={rmse(y_te_reg, y_pred_te):.3f})\"))\n",
    "\n",
    "fig.update_layout(title=\"Scratch KNN regression: effect of k\", xaxis_title=\"x\", yaxis_title=\"y\", width=900, height=500)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bdd949",
   "metadata": {},
   "source": [
    "### 6.1 Uniform vs distance weights (regression)\n",
    "\n",
    "With distance weights, closer points count more.\n",
    "\n",
    "- helps when the “right answer” is very local\n",
    "- can be noisier when points are extremely close (hence the small $\\varepsilon$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X1_tr[:, 0], y=y_tr_reg, mode=\"markers\", name=\"train\", marker=dict(size=7, opacity=0.7)))\n",
    "\n",
    "k = 15\n",
    "m_uniform = ScratchKNNRegressor(n_neighbors=k, p=2, weights=\"uniform\").fit(X1_tr, y_tr_reg)\n",
    "m_distance = ScratchKNNRegressor(n_neighbors=k, p=2, weights=\"distance\").fit(X1_tr, y_tr_reg)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_grid[:, 0], y=m_uniform.predict(x_grid), mode=\"lines\", name=\"uniform\"))\n",
    "fig.add_trace(go.Scatter(x=x_grid[:, 0], y=m_distance.predict(x_grid), mode=\"lines\", name=\"distance\"))\n",
    "\n",
    "fig.update_layout(title=f\"Scratch KNN regression: weights (k={k})\", xaxis_title=\"x\", yaxis_title=\"y\", width=900, height=500)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0b1bd",
   "metadata": {},
   "source": [
    "## 7) Why scaling matters (a dramatic demo)\n",
    "\n",
    "We’ll make a dataset with two features:\n",
    "\n",
    "- $x_1$ in range ~[-2, 2]\n",
    "- $x_2$ in range ~[0, 2000]\n",
    "\n",
    "If we don’t scale, $x_2$ dominates the distance — and KNN behaves as if $x_1$ barely exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scaled-up second feature that should NOT dominate the decision\n",
    "X_bad = X.copy()\n",
    "X_bad[:, 1] = (X_bad[:, 1] - X_bad[:, 1].min()) / (X_bad[:, 1].ptp() + 1e-9)  # normalize to [0,1]\n",
    "X_bad[:, 1] = X_bad[:, 1] * 2000  # huge scale\n",
    "\n",
    "Xb_tr, Xb_te, yb_tr, yb_te = train_test_split(X_bad, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "# Scratch KNN without scaling\n",
    "m_raw = ScratchKNNClassifier(n_neighbors=15, weights=\"uniform\").fit(Xb_tr, yb_tr)\n",
    "acc_raw = accuracy_score(yb_te, m_raw.predict(Xb_te))\n",
    "\n",
    "# sklearn Pipeline with scaling\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=15, weights=\"uniform\")),\n",
    "])\n",
    "pipe.fit(Xb_tr, yb_tr)\n",
    "acc_scaled = accuracy_score(yb_te, pipe.predict(Xb_te))\n",
    "\n",
    "print(f\"Accuracy without scaling: {acc_raw:.3f}\")\n",
    "print(f\"Accuracy with scaling   : {acc_scaled:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boundaries (note: x2 scale is huge)\n",
    "fig1 = plot_knn_boundary_2d(m_raw, Xb_tr, yb_tr, title=\"Scratch KNN boundary (NO scaling)\")\n",
    "fig1.show()\n",
    "\n",
    "# Boundary after scaling using sklearn model: we need a wrapper with predict_proba\n",
    "class _SklearnWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict_proba(self, Xq):\n",
    "        return self.model.predict_proba(Xq)\n",
    "\n",
    "fig2 = plot_knn_boundary_2d(_SklearnWrapper(pipe), Xb_tr, yb_tr, title=\"sklearn KNN boundary (StandardScaler + KNN)\")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b191e76",
   "metadata": {},
   "source": [
    "## 8) Curse of dimensionality (why KNN can struggle in high dimensions)\n",
    "\n",
    "In high dimensions, points tend to become “equally far apart”.\n",
    "\n",
    "A classic demo:\n",
    "\n",
    "- sample points uniformly in $[0,1]^d$\n",
    "- look at the ratio: **(nearest distance) / (farthest distance)**\n",
    "\n",
    "As dimension $d$ grows, the ratio approaches 1.\n",
    "\n",
    "That means:\n",
    "\n",
    "> the nearest neighbor isn’t *that* much nearer than a random point\n",
    "\n",
    "So KNN can lose its “locality” advantage unless you reduce dimension or have a huge dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_distance_ratio(dim: int, n_points: int = 1500, seed: int = 7) -> float:\n",
    "    r = np.random.default_rng(seed + dim)\n",
    "    Xd = r.random((n_points, dim))\n",
    "    xq = r.random((1, dim))\n",
    "\n",
    "    dists = pairwise_minkowski_distances(xq, Xd, p=2)[0]\n",
    "    return float(dists.min() / dists.max())\n",
    "\n",
    "\n",
    "dims = list(range(1, 51, 2))\n",
    "ratios = [nn_distance_ratio(d) for d in dims]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=dims, y=ratios, mode=\"lines+markers\"))\n",
    "fig.update_layout(\n",
    "    title=\"Nearest/farthest distance ratio vs dimension\",\n",
    "    xaxis_title=\"dimension d\",\n",
    "    yaxis_title=\"min(dist)/max(dist)\",\n",
    "    width=750,\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df534d7",
   "metadata": {},
   "source": [
    "## 9) Practical KNN with scikit-learn\n",
    "\n",
    "`sklearn` provides optimized KNN implementations with a clean interface.\n",
    "\n",
    "### 9.1 Key parameters (Classifier / Regressor)\n",
    "- `n_neighbors`: K\n",
    "- `weights`: `'uniform'` or `'distance'` (or a custom function)\n",
    "- `p`: Minkowski distance power (`p=2` Euclidean, `p=1` Manhattan)\n",
    "- `metric`: distance metric (default `'minkowski'`)\n",
    "- `algorithm`: `'auto'`, `'ball_tree'`, `'kd_tree'`, `'brute'`\n",
    "- `leaf_size`: affects tree-based neighbor search performance\n",
    "- `n_jobs`: parallelism (when supported)\n",
    "\n",
    "A good default starting point:\n",
    "\n",
    "- scale features with `StandardScaler`\n",
    "- try a small sweep over `n_neighbors`\n",
    "- compare `weights='uniform'` vs `weights='distance'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple sklearn baseline\n",
    "sk_knn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=15, weights=\"distance\", p=2)),\n",
    "])\n",
    "\n",
    "sk_knn.fit(X_tr, y_tr)\n",
    "y_pred = sk_knn.predict(X_te)\n",
    "print(\"Test accuracy:\", accuracy_score(y_te, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e24cf",
   "metadata": {},
   "source": [
    "### 9.2 A small grid search (select K + weights)\n",
    "\n",
    "This is the “grown-up” version of our manual accuracy sweep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5dcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"knn__n_neighbors\": list(range(1, 51, 2)),\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    \"knn__p\": [1, 2],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "gs.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best CV score:\", gs.best_score_)\n",
    "print(\"Test accuracy:\", accuracy_score(y_te, gs.best_estimator_.predict(X_te)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b80131",
   "metadata": {},
   "source": [
    "## 10) Summary: when to use KNN\n",
    "\n",
    "**Great when:**\n",
    "- you want a strong baseline with minimal assumptions\n",
    "- the decision boundary is complex and you have enough data\n",
    "- interpretability as “similar examples” is valuable\n",
    "\n",
    "**Be careful when:**\n",
    "- features have different scales (always scale)\n",
    "- you’re in very high dimensions (consider PCA / feature selection)\n",
    "- you need fast predictions at huge scale (consider approximate nearest neighbors or different models)\n",
    "\n",
    "KNN is like a *good memory + a sensible notion of similarity*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5e04a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement a **custom distance** that weighs features differently.\n",
    "2. Add **tie-breaking** rules explicitly for classification.\n",
    "3. Compare `p=1` vs `p=2` on `make_moons`.\n",
    "4. For regression, compare KNN to linear regression on the same dataset.\n",
    "\n",
    "## References\n",
    "- scikit-learn docs: KNeighborsClassifier / KNeighborsRegressor\n",
    "- “The Elements of Statistical Learning” — KNN discussion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
