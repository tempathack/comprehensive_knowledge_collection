{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb98edb",
   "metadata": {},
   "source": [
    "# Generalized Linear Models (GLMs)\n",
    "\n",
    "Generalized Linear Models extend linear regression in two key ways:\n",
    "\n",
    "1) **Mean modeling via a link function**\n",
    "\n",
    "We don’t predict the mean $\\mu$ directly from a linear model. Instead we predict a *linear predictor*:\n",
    "\n",
    "$$\n",
    "\\eta = X\\beta\n",
    "$$\n",
    "\n",
    "and connect it to the mean via an **inverse link** $g^{-1}$:\n",
    "\n",
    "$$\n",
    "\\mu = \\mathbb{E}[y\\mid X] = g^{-1}(\\eta)\n",
    "$$\n",
    "\n",
    "2) **Squared loss is replaced by a distribution-aware loss**\n",
    "\n",
    "Instead of assuming Gaussian noise and minimizing squared error, we choose a distribution from the **exponential family** (more precisely: an exponential dispersion model) and minimize its **unit deviance** (plus regularization).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "- understand what GLMs are (and why they exist)\n",
    "- connect: **distribution ↔ link ↔ loss (deviance)**\n",
    "- see how common models are just GLMs:\n",
    "  - linear regression (Normal + identity)\n",
    "  - logistic regression (Bernoulli + logit)\n",
    "  - Poisson regression (Poisson + log)\n",
    "  - Gamma / Inverse Gaussian regression (positive skew + log)\n",
    "- implement **IRLS** (iteratively reweighted least squares) for logistic and Poisson regression\n",
    "- map the ideas to scikit-learn’s GLM estimators\n",
    "\n",
    "## Table of contents\n",
    "1. Why GLMs? (what linear regression can’t do)\n",
    "2. The GLM recipe (exponential family + link)\n",
    "3. Deviance: “distribution-aware squared error”\n",
    "4. PDFs/PMFs of common GLM distributions\n",
    "5. Optimization intuition: IRLS (Newton as weighted least squares)\n",
    "6. Logistic regression as a GLM (Bernoulli)\n",
    "7. Poisson regression as a GLM (Poisson)\n",
    "8. Gamma / Inverse Gaussian (via Tweedie)\n",
    "9. Multiclass (categorical) GLM intuition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy.special import expit\n",
    "from scipy.stats import norm, poisson, gamma as gamma_dist, invgauss\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, PoissonRegressor, GammaRegressor, TweedieRegressor\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3c7c4",
   "metadata": {},
   "source": [
    "## 1) Why GLMs?\n",
    "\n",
    "Linear regression predicts a real number:\n",
    "\n",
    "$$\\hat{y} = x^\\top \\beta$$\n",
    "\n",
    "That’s perfect when $y$ is continuous and (roughly) Gaussian.\n",
    "\n",
    "But what if:\n",
    "- $y$ is a **probability** (must be between 0 and 1)\n",
    "- $y$ is a **count** (must be $0,1,2,\\dots$)\n",
    "- $y$ is **positive and skewed** (e.g., time-to-failure, insurance claim size)\n",
    "\n",
    "GLMs solve this by keeping the *linear predictor* (the “score”) but changing:\n",
    "- the mapping from score → mean (the **link**)\n",
    "- the loss (via the **likelihood / deviance**)\n",
    "\n",
    "Anecdote:\n",
    "> A linear model is like a “thermostat knob” (it can turn freely). GLMs add the right *display* and *physics* so the knob controls the correct kind of quantity (probability, rate, positive mean, …).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1298a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same linear score η, different inverse links μ = g⁻¹(η)\n",
    "eta = np.linspace(-8, 8, 500)\n",
    "\n",
    "mu_identity = eta\n",
    "mu_sigmoid = expit(eta)          # Bernoulli mean via logit link\n",
    "mu_exp = np.exp(eta)             # Poisson mean via log link\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=eta, y=mu_identity, name=\"identity (Normal)\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=eta, y=mu_sigmoid, name=\"sigmoid (Bernoulli/logit)\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=eta, y=mu_exp, name=\"exp (Poisson/log)\", mode=\"lines\"))\n",
    "fig.update_layout(\n",
    "    title=\"One linear score η = Xβ, many different means μ = g⁻¹(η)\",\n",
    "    xaxis_title=\"η\",\n",
    "    yaxis_title=\"μ\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e598d",
   "metadata": {},
   "source": [
    "## 2) The GLM recipe\n",
    "\n",
    "A GLM is defined by:\n",
    "\n",
    "1) **A distribution** for $y \\mid x$ from an exponential family / EDM\n",
    "2) A **link function** $g$ connecting the mean to the linear predictor\n",
    "\n",
    "$$\n",
    "\\eta = X\\beta,\\qquad \\mu = \\mathbb{E}[y\\mid X] = g^{-1}(\\eta)\n",
    "$$\n",
    "\n",
    "### Exponential family (one common form)\n",
    "A lot of useful distributions can be written as:\n",
    "\n",
    "$$\n",
    "\\log p(y\\mid \\theta, \\phi) = \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\n",
    "$$\n",
    "\n",
    "- $\\theta$ is the **natural parameter**\n",
    "- $\\phi$ is a **dispersion** parameter\n",
    "- $b(\\theta)$ controls the mean/variance\n",
    "\n",
    "In GLMs, the link often connects $\\mu$ to $\\eta$ in a way that is convenient (the **canonical link** is a common choice).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb705b",
   "metadata": {},
   "source": [
    "## 3) Deviance: “distribution-aware squared error”\n",
    "\n",
    "For Gaussian noise, minimizing squared error is equivalent to maximizing a Gaussian likelihood.\n",
    "\n",
    "For general GLMs, we maximize likelihood for the chosen distribution. In many GLM-style estimators, this is written using the **unit deviance** $d(y, \\mu)$.\n",
    "\n",
    "A typical regularized objective is:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta}\\ \\frac{1}{2}\\,\\frac{1}{n}\\sum_{i=1}^n d\\bigl(y_i, \\mu_i\\bigr)\\; +\\; \\frac{\\alpha}{2}\\,\\lVert \\beta \\rVert_2^2\n",
    "$$\n",
    "\n",
    "If sample weights $w_i$ are provided, the average becomes a weighted average.\n",
    "\n",
    "### Unit deviance cheat sheet (common EDMs)\n",
    "Below are standard unit deviances (with conventions like $0\\log 0 = 0$).\n",
    "\n",
    "| Distribution | Target domain | Unit deviance $d(y,\\mu)$ |\n",
    "|---|---|---|\n",
    "| Normal | $\\mathbb{R}$ | $(y-\\mu)^2$ |\n",
    "| Bernoulli | $\\{0,1\\}$ | $2\\left[y\\log\\frac{y}{\\mu} + (1-y)\\log\\frac{1-y}{1-\\mu}\\right]$ |\n",
    "| Categorical (multinomial, 1 trial) | one-hot | $2\\sum_k y_k\\log\\frac{y_k}{\\mu_k}$ |\n",
    "| Poisson | $\\{0,1,2,\\dots\\}$ | $2\\left[y\\log\\frac{y}{\\mu} - (y-\\mu)\\right]$ |\n",
    "| Gamma | $\\mathbb{R}_{+}$ | $2\\left[\\frac{y-\\mu}{\\mu} - \\log\\frac{y}{\\mu}\\right]$ |\n",
    "| Inverse Gaussian | $\\mathbb{R}_{+}$ | $\\frac{(y-\\mu)^2}{\\mu^2 y}$ |\n",
    "\n",
    "Interpretation:\n",
    "- Normal deviance is just squared distance.\n",
    "- Bernoulli deviance matches **log-loss** (up to a constant).\n",
    "- Poisson deviance compares counts in a way that respects positivity and relative error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983aa170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual intuition: unit deviance as a function of μ for fixed y\n",
    "\n",
    "mus = np.linspace(1e-4, 0.9999, 500)\n",
    "\n",
    "# Bernoulli deviance for y=1 and y=0\n",
    "bern_y1 = 2.0 * np.log(1.0 / mus)\n",
    "bern_y0 = 2.0 * np.log(1.0 / (1.0 - mus))\n",
    "\n",
    "# Poisson deviance for y=5 (μ must be > 0)\n",
    "mu_pos = np.linspace(1e-3, 15, 500)\n",
    "y_p = 5.0\n",
    "pois_dev = 2.0 * (y_p * np.log(y_p / mu_pos) - (y_p - mu_pos))\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Bernoulli unit deviance\", \"Poisson unit deviance (y=5)\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=mus, y=bern_y1, mode=\"lines\", name=\"y=1\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=mus, y=bern_y0, mode=\"lines\", name=\"y=0\"), row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"μ\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"d(y,μ)\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=mu_pos, y=pois_dev, mode=\"lines\", name=\"y=5\"), row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"μ\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"d(y,μ)\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=1000, height=420, title=\"Deviance = mismatch measure tailored to the distribution\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9489f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDFs/PMFs: a quick visual gallery of common GLM distributions\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=3,\n",
    "    subplot_titles=(\n",
    "        \"Normal (PDF)\",\n",
    "        \"Bernoulli (PMF)\",\n",
    "        \"Categorical (PMF)\",\n",
    "        \"Poisson (PMF)\",\n",
    "        \"Gamma (PDF)\",\n",
    "        \"Inverse Gaussian (PDF)\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Normal\n",
    "x = np.linspace(-4, 4, 400)\n",
    "fig.add_trace(go.Scatter(x=x, y=norm.pdf(x, loc=0, scale=1), mode=\"lines\"), row=1, col=1)\n",
    "\n",
    "# Bernoulli\n",
    "p = 0.3\n",
    "fig.add_trace(go.Bar(x=[0, 1], y=[1 - p, p]), row=1, col=2)\n",
    "\n",
    "# Categorical\n",
    "probs = np.array([0.15, 0.25, 0.60])\n",
    "fig.add_trace(go.Bar(x=[\"A\", \"B\", \"C\"], y=probs), row=1, col=3)\n",
    "\n",
    "# Poisson\n",
    "lam = 4.0\n",
    "k = np.arange(0, 16)\n",
    "fig.add_trace(go.Bar(x=k, y=poisson.pmf(k, mu=lam)), row=2, col=1)\n",
    "\n",
    "# Gamma\n",
    "xg = np.linspace(1e-4, 10, 400)\n",
    "shape = 2.0\n",
    "scale = 1.0\n",
    "fig.add_trace(go.Scatter(x=xg, y=gamma_dist.pdf(xg, a=shape, scale=scale), mode=\"lines\"), row=2, col=2)\n",
    "\n",
    "# Inverse Gaussian (SciPy parameterization)\n",
    "xi = np.linspace(1e-4, 10, 400)\n",
    "mu_param = 1.5\n",
    "scale_param = 1.0\n",
    "fig.add_trace(go.Scatter(x=xi, y=invgauss.pdf(xi, mu=mu_param, scale=scale_param), mode=\"lines\"), row=2, col=3)\n",
    "\n",
    "fig.update_layout(width=1100, height=650, title=\"Distribution shapes (PDF/PMF)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dabf2",
   "metadata": {},
   "source": [
    "## 4) Optimization intuition: IRLS\n",
    "\n",
    "For many GLMs (especially with canonical links), Newton’s method / Fisher scoring yields an update that looks like:\n",
    "\n",
    "> solve a **weighted least squares** problem, update $\\beta$, repeat\n",
    "\n",
    "This is called **IRLS** (Iteratively Reweighted Least Squares).\n",
    "\n",
    "A useful mental model:\n",
    "- a GLM is “like linear regression”, but the relationship between $\\eta$ and $\\mu$ is nonlinear\n",
    "- IRLS repeatedly builds a local quadratic approximation and solves a linear-looking problem\n",
    "\n",
    "We’ll implement IRLS for:\n",
    "- Bernoulli + logit (logistic regression)\n",
    "- Poisson + log (Poisson regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba74a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def irls_logistic_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    alpha: float = 0.0,\n",
    "    max_iter: int = 50,\n",
    "    tol: float = 1e-8,\n",
    "):\n",
    "    \"\"\"Logistic regression via IRLS with optional L2 penalty (ridge).\n",
    "\n",
    "    alpha: L2 strength (penalizes coefficients except intercept).\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    n, d = X.shape\n",
    "    beta = np.zeros(d, dtype=float)\n",
    "\n",
    "    losses = []\n",
    "    reg = alpha * np.eye(d)\n",
    "    reg[0, 0] = 0.0\n",
    "\n",
    "    eps = 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        eta = X @ beta\n",
    "        mu = expit(eta)\n",
    "\n",
    "        w = mu * (1.0 - mu)\n",
    "        w = np.clip(w, 1e-9, None)\n",
    "\n",
    "        z = eta + (y - mu) / w\n",
    "\n",
    "        sqrt_w = np.sqrt(w)\n",
    "        Xw = X * sqrt_w[:, None]\n",
    "        zw = z * sqrt_w\n",
    "\n",
    "        A = Xw.T @ Xw + reg\n",
    "        b = Xw.T @ zw\n",
    "\n",
    "        beta_new = np.linalg.solve(A, b)\n",
    "\n",
    "        nll = -np.mean(y * np.log(mu + eps) + (1.0 - y) * np.log(1.0 - mu + eps))\n",
    "        penalty = 0.5 * alpha * np.sum(beta[1:] ** 2)\n",
    "        losses.append(float(nll + penalty))\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) <= tol * (np.linalg.norm(beta) + tol):\n",
    "            beta = beta_new\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return beta, np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5403601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression demo (2D)\n",
    "X, y = make_blobs(\n",
    "    n_samples=700,\n",
    "    centers=[(-2.0, -1.0), (2.0, 1.0)],\n",
    "    cluster_std=[1.6, 1.4],\n",
    "    random_state=7,\n",
    ")\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_tr[:, 0],\n",
    "    y=X_tr[:, 1],\n",
    "    color=y_tr.astype(str),\n",
    "    title=\"Binary classification dataset\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"class\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.7))\n",
    "fig.update_layout(width=720, height=470)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d912e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_i = add_intercept(X_tr)\n",
    "X_te_i = add_intercept(X_te)\n",
    "\n",
    "beta_hat, losses = irls_logistic_regression(X_tr_i, y_tr, alpha=0.5, max_iter=80)\n",
    "\n",
    "# sklearn reference\n",
    "sk = LogisticRegression(C=1/0.5, penalty=\"l2\", solver=\"lbfgs\", fit_intercept=True, max_iter=2000)\n",
    "sk.fit(X_tr, y_tr)\n",
    "\n",
    "pred_scratch = (expit(X_te_i @ beta_hat) >= 0.5).astype(int)\n",
    "pred_sklearn = sk.predict(X_te)\n",
    "\n",
    "print(\"Scratch IRLS logistic accuracy:\", accuracy_score(y_te, pred_scratch))\n",
    "print(\"sklearn LogisticRegression accuracy:\", accuracy_score(y_te, pred_sklearn))\n",
    "print(\"Scratch beta:\", beta_hat)\n",
    "print(\"sklearn  beta:\", np.r_[sk.intercept_, sk.coef_.ravel()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df18ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=losses, x=np.arange(1, len(losses) + 1), mode=\"lines+markers\"))\n",
    "fig.update_layout(title=\"IRLS convergence (avg NLL + ridge)\", xaxis_title=\"iteration\", yaxis_title=\"loss\", width=700, height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db678902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probability_surface_2d(model_proba_fn, X, y, title: str, grid_steps: int = 220):\n",
    "    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n",
    "    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n",
    "\n",
    "    xs = np.linspace(x_min, x_max, grid_steps)\n",
    "    ys = np.linspace(y_min, y_max, grid_steps)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    proba = model_proba_fn(grid).reshape(xx.shape)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Contour(\n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        z=proba,\n",
    "        colorscale=\"RdBu\",\n",
    "        opacity=0.75,\n",
    "        contours=dict(showlines=False),\n",
    "        colorbar=dict(title=\"P(class=1)\"),\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=y, colorscale=\"Viridis\", size=6, line=dict(width=0.5, color=\"white\")),\n",
    "        name=\"data\",\n",
    "    ))\n",
    "    fig.update_layout(title=title, width=760, height=520)\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig1 = plot_probability_surface_2d(lambda Z: expit(add_intercept(Z) @ beta_hat), X_te, y_te, \"Scratch logistic: P(class=1)\")\n",
    "fig1.show()\n",
    "\n",
    "fig2 = plot_probability_surface_2d(lambda Z: sk.predict_proba(Z)[:, 1], X_te, y_te, \"sklearn logistic: P(class=1)\")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd77c36",
   "metadata": {},
   "source": [
    "## 5) Poisson regression (counts) as a GLM\n",
    "\n",
    "When $y$ is a **count**, a natural model is Poisson:\n",
    "\n",
    "$$\n",
    "(y \\mid x) \\sim \\text{Poisson}(\\mu)\n",
    "$$\n",
    "\n",
    "A common link is the **log link**:\n",
    "\n",
    "$$\n",
    "\\log \\mu = \\eta = x^\\top \\beta \\quad\\Rightarrow\\quad \\mu = \\exp(x^\\top\\beta)\n",
    "$$\n",
    "\n",
    "This guarantees $\\mu>0$.\n",
    "\n",
    "IRLS exists here too and is very similar to logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irls_poisson_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    alpha: float = 0.0,\n",
    "    max_iter: int = 80,\n",
    "    tol: float = 1e-8,\n",
    "):\n",
    "    \"\"\"Poisson regression via IRLS with optional L2 penalty (ridge).\n",
    "\n",
    "    Uses log link: μ = exp(η).\n",
    "    alpha penalizes coefficients except intercept.\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    n, d = X.shape\n",
    "    beta = np.zeros(d, dtype=float)\n",
    "\n",
    "    losses = []\n",
    "    reg = alpha * np.eye(d)\n",
    "    reg[0, 0] = 0.0\n",
    "\n",
    "    eps = 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        eta = X @ beta\n",
    "        mu = np.exp(eta)\n",
    "\n",
    "        w = np.clip(mu, 1e-9, None)\n",
    "        z = eta + (y - mu) / mu\n",
    "\n",
    "        sqrt_w = np.sqrt(w)\n",
    "        Xw = X * sqrt_w[:, None]\n",
    "        zw = z * sqrt_w\n",
    "\n",
    "        A = Xw.T @ Xw + reg\n",
    "        b = Xw.T @ zw\n",
    "\n",
    "        beta_new = np.linalg.solve(A, b)\n",
    "\n",
    "        nll = float(np.mean(mu - y * np.log(mu + eps)))\n",
    "        penalty = 0.5 * alpha * float(np.sum(beta[1:] ** 2))\n",
    "        losses.append(nll + penalty)\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) <= tol * (np.linalg.norm(beta) + tol):\n",
    "            beta = beta_new\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return beta, np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dedefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Poisson regression\n",
    "n = 1500\n",
    "Xp = rng.normal(size=(n, 2))\n",
    "Xp_i = add_intercept(Xp)\n",
    "\n",
    "beta_true = np.array([0.3, 0.6, -0.4])\n",
    "rate = np.exp(Xp_i @ beta_true)\n",
    "\n",
    "yp = rng.poisson(lam=rate)\n",
    "\n",
    "Xp_tr, Xp_te, yp_tr, yp_te = train_test_split(Xp, yp, test_size=0.3, random_state=7)\n",
    "Xp_tr_i = add_intercept(Xp_tr)\n",
    "Xp_te_i = add_intercept(Xp_te)\n",
    "\n",
    "beta_p, losses_p = irls_poisson_regression(Xp_tr_i, yp_tr, alpha=1.0)\n",
    "\n",
    "sk_p = PoissonRegressor(alpha=1.0, fit_intercept=True, max_iter=5000)\n",
    "sk_p.fit(Xp_tr, yp_tr)\n",
    "\n",
    "print(\"True beta:\", beta_true)\n",
    "print(\"Scratch beta:\", beta_p)\n",
    "print(\"sklearn beta:\", np.r_[sk_p.intercept_, sk_p.coef_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and compare\n",
    "mu_hat_scratch = np.exp(Xp_te_i @ beta_p)\n",
    "mu_hat_sklearn = sk_p.predict(Xp_te)\n",
    "\n",
    "rmse_scratch = float(np.sqrt(mean_squared_error(yp_te, mu_hat_scratch)))\n",
    "rmse_sklearn = float(np.sqrt(mean_squared_error(yp_te, mu_hat_sklearn)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_hat_scratch, y=yp_te, mode=\"markers\", name=f\"scratch (RMSE={rmse_scratch:.2f})\", opacity=0.6))\n",
    "fig.add_trace(go.Scatter(x=mu_hat_sklearn, y=yp_te, mode=\"markers\", name=f\"sklearn (RMSE={rmse_sklearn:.2f})\", opacity=0.6))\n",
    "fig.update_layout(\n",
    "    title=\"Poisson regression: predicted mean μ vs observed count y\",\n",
    "    xaxis_title=\"predicted μ\",\n",
    "    yaxis_title=\"observed y\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3cd73",
   "metadata": {},
   "source": [
    "## 6) Gamma / Inverse Gaussian (positive, skewed targets)\n",
    "\n",
    "When $y>0$ and is skewed, Gaussian noise is often a poor match.\n",
    "\n",
    "Two classic EDM choices:\n",
    "- **Gamma**: variance grows like $\\mu^2$ (common for positive skew)\n",
    "- **Inverse Gaussian**: even heavier variance growth\n",
    "\n",
    "In scikit-learn, these show up via:\n",
    "- `GammaRegressor` (Gamma deviance)\n",
    "- `TweedieRegressor(power=3)` (Inverse Gaussian deviance)\n",
    "\n",
    "### Tweedie family connection\n",
    "A convenient unification is the Tweedie variance function:\n",
    "\n",
    "$$\\mathrm{Var}(y\\mid x) \\propto \\mu^{p}$$\n",
    "\n",
    "- $p=0$ → Normal\n",
    "- $p=1$ → Poisson\n",
    "- $p=2$ → Gamma\n",
    "- $p=3$ → Inverse Gaussian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86509f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A synthetic positive-skew regression dataset\n",
    "\n",
    "n = 1800\n",
    "Xg = rng.normal(size=(n, 2))\n",
    "Xg_i = add_intercept(Xg)\n",
    "\n",
    "beta_true = np.array([0.2, 0.5, -0.3])\n",
    "mu_true = np.exp(Xg_i @ beta_true)\n",
    "\n",
    "# Gamma sample with mean mu_true: choose shape k, scale=mu/k\n",
    "k_shape = 2.0\n",
    "scale = mu_true / k_shape\n",
    "\n",
    "y_gamma = rng.gamma(shape=k_shape, scale=scale)\n",
    "\n",
    "Xg_tr, Xg_te, yg_tr, yg_te = train_test_split(Xg, y_gamma, test_size=0.3, random_state=7)\n",
    "\n",
    "m_gamma = GammaRegressor(alpha=1.0, fit_intercept=True, max_iter=5000)\n",
    "m_gamma.fit(Xg_tr, yg_tr)\n",
    "\n",
    "m_ig = TweedieRegressor(power=3.0, alpha=1.0, link='log', fit_intercept=True, max_iter=5000)\n",
    "m_ig.fit(Xg_tr, yg_tr)\n",
    "\n",
    "pred_gamma = m_gamma.predict(Xg_te)\n",
    "pred_ig = m_ig.predict(Xg_te)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pred_gamma, y=yg_te, mode=\"markers\", name=\"GammaRegressor\", opacity=0.6))\n",
    "fig.add_trace(go.Scatter(x=pred_ig, y=yg_te, mode=\"markers\", name=\"Tweedie power=3 (InvGauss)\", opacity=0.6))\n",
    "fig.update_layout(title=\"Positive skew regression: predictions vs observed\", xaxis_title=\"predicted mean\", yaxis_title=\"observed y\", width=900, height=520)\n",
    "fig.show()\n",
    "\n",
    "print(\"GammaRegressor coef:\", np.r_[m_gamma.intercept_, m_gamma.coef_])\n",
    "print(\"InvGauss(Tweedie p=3) coef:\", np.r_[m_ig.intercept_, m_ig.coef_])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8d7bf",
   "metadata": {},
   "source": [
    "## 7) Multiclass (categorical) GLM intuition\n",
    "\n",
    "For $K$ classes, you can model:\n",
    "\n",
    "$$\n",
    "P(y=k\\mid x) = \\text{softmax}_k(\\eta)\\quad\\text{where}\\quad \\eta_k = x^\\top \\beta_k\n",
    "$$\n",
    "\n",
    "This is a **categorical** (multinomial) GLM. The deviance corresponds to cross-entropy.\n",
    "\n",
    "In practice, `sklearn.linear_model.LogisticRegression` with a multinomial-capable solver implements this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c68541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick multiclass demo (sklearn)\n",
    "X3, y3 = make_blobs(\n",
    "    n_samples=900,\n",
    "    centers=[(-2, 0), (2, 0), (0, 2)],\n",
    "    cluster_std=[1.1, 1.1, 1.1],\n",
    "    random_state=7,\n",
    ")\n",
    "X3_tr, X3_te, y3_tr, y3_te = train_test_split(X3, y3, test_size=0.3, random_state=7, stratify=y3)\n",
    "\n",
    "m_multi = LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "m_multi.fit(X3_tr, y3_tr)\n",
    "\n",
    "print(\"Multiclass accuracy:\", accuracy_score(y3_te, m_multi.predict(X3_te)))\n",
    "\n",
    "fig = px.scatter(x=X3_te[:, 0], y=X3_te[:, 1], color=y3_te.astype(str), title=\"3-class dataset\", labels={\"x\":\"x1\",\"y\":\"x2\",\"color\":\"class\"})\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.7))\n",
    "fig.update_layout(width=720, height=470)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e80bcb",
   "metadata": {},
   "source": [
    "## 8) Summary\n",
    "\n",
    "- GLMs keep the linear predictor $\\eta=X\\beta$, but change the mapping to the mean and the loss.\n",
    "- The distribution choice gives you the **right domain** and **right notion of error** (deviance).\n",
    "- IRLS gives an intuitive optimization story: repeatedly solve a weighted least-squares problem.\n",
    "\n",
    "## Exercises\n",
    "1. Implement IRLS for **Gamma** with log link (harder but very educational).\n",
    "2. Create a dataset with over-dispersed counts and compare Poisson vs a Tweedie model.\n",
    "3. Change `alpha` and observe coefficient shrinkage for PoissonRegressor.\n",
    "\n",
    "## References\n",
    "- scikit-learn: GLM and Tweedie regressors\n",
    "- “Generalized Linear Models” (McCullagh & Nelder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}