{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes (GP) — Regression & Probabilistic Classification\n",
    "\n",
    "Gaussian Processes (GPs) are a **Bayesian, nonparametric** way to do supervised learning.\n",
    "\n",
    "- **GP regression**: returns a *distribution* over functions that fit the data.\n",
    "- **GP classification**: returns *probabilities* (via a latent GP + a link function).\n",
    "\n",
    "A GP is best thought of as:\n",
    "\n",
    "> A probability distribution over plausible functions.\n",
    "\n",
    "---\n",
    "\n",
    "## Why people like GPs\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- **Interpolates** the observations (for many kernels when the noise is very small).\n",
    "- **Probabilistic predictions**: you get uncertainty estimates (e.g., 95% intervals).\n",
    "- **Versatile kernels**: you can encode assumptions (smoothness, periodicity, roughness).\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Not sparse by default: it uses *all* data at prediction time.\n",
    "- Training can be expensive: typically **$\\mathcal{O}(n^3)$** time and **$\\mathcal{O}(n^2)$** memory.\n",
    "- Can struggle in very high-dimensional feature spaces without careful kernel design / scaling.\n",
    "\n",
    "This notebook focuses on **intuition + statistical background** (Gaussian conditioning), uses Plotly visuals heavily, implements **GP regression from scratch**, and then connects the results to `scikit-learn` for both regression and classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain what a GP is (distribution over functions) and what a **kernel** means\n",
    "- sample functions from a GP prior and interpret how kernels change function shapes\n",
    "- derive GP regression using **conditioning a multivariate Gaussian**\n",
    "- implement GP regression in NumPy (Cholesky-based)\n",
    "- interpret the **posterior mean** and **posterior variance**\n",
    "- understand **log marginal likelihood** and why it balances fit vs complexity\n",
    "- use `GaussianProcessRegressor` and `GaussianProcessClassifier` in `scikit-learn` and explain key parameters\n",
    "\n",
    "---\n",
    "\n",
    "## Notation\n",
    "\n",
    "- Inputs: $X = [x_1,\\dots,x_n]^T$ (shape $n\\times d$)\n",
    "- Latent function values: $f = [f(x_1),\\dots,f(x_n)]^T$\n",
    "- Observations (regression): $y = f + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$\n",
    "- Kernel (covariance function): $k(x,z)$\n",
    "- Kernel matrix: $K(X,X)$ where $K_{ij} = k(x_i, x_j)$\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. The GP idea: a distribution over functions\n",
    "2. Kernels as similarity + shape priors (Plotly demos)\n",
    "3. GP regression via Gaussian conditioning (math + intuition)\n",
    "4. GP regression from scratch (NumPy)\n",
    "5. Hyperparameters + log marginal likelihood (Occam’s razor)\n",
    "6. `scikit-learn` GP regression + parameter map\n",
    "7. GP classification: latent GP + logistic link + Laplace (intuition)\n",
    "8. `scikit-learn` GP classification (probability maps)\n",
    "9. Practical tips, limitations, and scaling strategies\n",
    "10. Exercises + references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from scipy.special import expit  # sigmoid\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    RBF,\n",
    "    Matern,\n",
    "    ExpSineSquared,\n",
    "    RationalQuadratic,\n",
    "    WhiteKernel,\n",
    "    ConstantKernel,\n",
    ")\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) The GP idea: a distribution over functions\n",
    "\n",
    "A Gaussian Process is defined as:\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\;k(x,x'))\n",
    "$$\n",
    "\n",
    "meaning: **for any finite set of inputs** $X=[x_1,\\dots,x_n]$, the vector of function values\n",
    "\n",
    "$$\n",
    "f = [f(x_1),\\dots,f(x_n)]^T\n",
    "$$\n",
    "\n",
    "is multivariate Gaussian:\n",
    "\n",
    "$$\n",
    "f \\sim \\mathcal{N}(m, K)\\quad \\text{where}\\quad m_i=m(x_i),\\; K_{ij}=k(x_i,x_j).\n",
    "$$\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- The **mean function** $m(x)$ is often set to 0 (a neutral prior).\n",
    "- The **kernel** $k(x,z)$ says how much you expect two points to co-vary.\n",
    "- Sampling from a GP prior gives you *random functions* consistent with those assumptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_2d(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim == 1:\n",
    "        return X.reshape(-1, 1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def rbf_kernel(\n",
    "    X: np.ndarray,\n",
    "    Z: np.ndarray,\n",
    "    length_scale: float = 1.0,\n",
    "    variance: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    X = _as_2d(X)\n",
    "    Z = _as_2d(Z)\n",
    "    length_scale = float(length_scale)\n",
    "    variance = float(variance)\n",
    "\n",
    "    X_norm = np.sum(X**2, axis=1)[:, None]\n",
    "    Z_norm = np.sum(Z**2, axis=1)[None, :]\n",
    "    dist2 = X_norm + Z_norm - 2.0 * (X @ Z.T)\n",
    "    return variance * np.exp(-0.5 * dist2 / (length_scale**2))\n",
    "\n",
    "\n",
    "def periodic_kernel(\n",
    "    X: np.ndarray,\n",
    "    Z: np.ndarray,\n",
    "    length_scale: float = 1.0,\n",
    "    period: float = 1.0,\n",
    "    variance: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    X = _as_2d(X)\n",
    "    Z = _as_2d(Z)\n",
    "    length_scale = float(length_scale)\n",
    "    period = float(period)\n",
    "    variance = float(variance)\n",
    "\n",
    "    # For 1D this is just |x-z|; for d>1 you could generalize with norms.\n",
    "    d = np.abs(X - Z.T)\n",
    "    return variance * np.exp(-2.0 * (np.sin(np.pi * d / period) ** 2) / (length_scale**2))\n",
    "\n",
    "\n",
    "def sample_gp_prior_1d(\n",
    "    x_grid: np.ndarray,\n",
    "    kernel_fn,\n",
    "    n_samples: int = 5,\n",
    "    jitter: float = 1e-9,\n",
    ") -> np.ndarray:\n",
    "    X = _as_2d(x_grid)\n",
    "    K = kernel_fn(X, X) + jitter * np.eye(X.shape[0])\n",
    "    return rng.multivariate_normal(mean=np.zeros(X.shape[0]), cov=K, size=n_samples)\n",
    "\n",
    "\n",
    "def plot_gp_samples_1d(\n",
    "    x: np.ndarray,\n",
    "    samples: np.ndarray,\n",
    "    title: str,\n",
    "    y_mean: np.ndarray | None = None,\n",
    "    y_std: np.ndarray | None = None,\n",
    "    y_train: np.ndarray | None = None,\n",
    "    x_train: np.ndarray | None = None,\n",
    ") -> go.Figure:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if y_mean is not None and y_std is not None:\n",
    "        y_mean = np.asarray(y_mean)\n",
    "        y_std = np.asarray(y_std)\n",
    "\n",
    "        upper = y_mean + 1.96 * y_std\n",
    "        lower = y_mean - 1.96 * y_std\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=x, y=upper, mode=\"lines\", line=dict(width=0), showlegend=False))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=lower,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0),\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(31, 119, 180, 0.18)\",\n",
    "                name=\"95% interval\",\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(go.Scatter(x=x, y=y_mean, mode=\"lines\", name=\"mean\", line=dict(width=3)))\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=s,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=1),\n",
    "                name=f\"sample {i+1}\",\n",
    "                opacity=0.7,\n",
    "                showlegend=i == 0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if x_train is not None and y_train is not None:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.asarray(x_train),\n",
    "                y=np.asarray(y_train),\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=8, color=\"black\"),\n",
    "                name=\"observations\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 220)\n",
    "\n",
    "kernels = {\n",
    "    \"RBF (l=0.4)\": lambda X, Z: rbf_kernel(X, Z, length_scale=0.4, variance=1.0),\n",
    "    \"RBF (l=1.5)\": lambda X, Z: rbf_kernel(X, Z, length_scale=1.5, variance=1.0),\n",
    "    \"Periodic (p=2.5)\": lambda X, Z: periodic_kernel(X, Z, length_scale=0.6, period=2.5, variance=1.0),\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "for name, kfn in kernels.items():\n",
    "    k_slice = kfn(x.reshape(-1, 1), np.array([[0.0]])).ravel()\n",
    "    fig.add_trace(go.Scatter(x=x, y=k_slice, mode=\"lines\", name=name))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Kernel slice k(x, 0): how correlation decays with distance\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"k(x, 0)\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(-4, 4, 160)\n",
    "\n",
    "samples_rbf_short = sample_gp_prior_1d(x_grid, kernels[\"RBF (l=0.4)\"], n_samples=6)\n",
    "samples_rbf_long = sample_gp_prior_1d(x_grid, kernels[\"RBF (l=1.5)\"], n_samples=6)\n",
    "samples_per = sample_gp_prior_1d(x_grid, kernels[\"Periodic (p=2.5)\"], n_samples=6)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=[\"RBF: short length-scale\", \"RBF: long length-scale\", \"Periodic kernel\"])\n",
    "\n",
    "for s in samples_rbf_short:\n",
    "    fig.add_trace(go.Scatter(x=x_grid, y=s, mode=\"lines\", line=dict(width=1), showlegend=False), row=1, col=1)\n",
    "for s in samples_rbf_long:\n",
    "    fig.add_trace(go.Scatter(x=x_grid, y=s, mode=\"lines\", line=dict(width=1), showlegend=False), row=1, col=2)\n",
    "for s in samples_per:\n",
    "    fig.add_trace(go.Scatter(x=x_grid, y=s, mode=\"lines\", line=dict(width=1), showlegend=False), row=1, col=3)\n",
    "\n",
    "fig.update_layout(title=\"Sampling random functions from GP priors (kernels = shape assumptions)\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Kernels as similarity + shape priors\n",
    "\n",
    "Kernels do two jobs at once:\n",
    "\n",
    "1. **Similarity**: if $k(x,z)$ is large, the model believes $f(x)$ and $f(z)$ should be similar.\n",
    "2. **Shape prior**: the kernel determines what kind of functions are likely (smooth, wiggly, periodic, etc.).\n",
    "\n",
    "One useful mental picture is the **kernel matrix** $K(X,X)$:\n",
    "\n",
    "- If it has large off-diagonal values, points are strongly coupled → smoother functions.\n",
    "- If it is close to diagonal, points are weakly coupled → rougher / more local behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk = x_grid.reshape(-1, 1)\n",
    "K_short = kernels[\"RBF (l=0.4)\"](Xk, Xk)\n",
    "K_long = kernels[\"RBF (l=1.5)\"](Xk, Xk)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"RBF kernel matrix (short l)\", \"RBF kernel matrix (long l)\"])\n",
    "fig.add_trace(go.Heatmap(z=K_short, colorscale=\"Viridis\", showscale=False), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=K_long, colorscale=\"Viridis\", showscale=True), row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Kernel matrices: longer length-scale couples points more strongly\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) GP regression via Gaussian conditioning\n",
    "\n",
    "The core math trick in GP regression is: **conditioning a joint Gaussian**.\n",
    "\n",
    "Assume a zero-mean prior (for simplicity):\n",
    "\n",
    "$$\n",
    "f \\sim \\mathcal{N}(0, K(X,X)).\n",
    "$$\n",
    "\n",
    "And a Gaussian noise model:\n",
    "\n",
    "$$\n",
    "y = f + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I).\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(0,\\;K(X,X) + \\sigma^2 I).\n",
    "$$\n",
    "\n",
    "For test points $X_*$, the joint distribution of training outputs $y$ and latent test function values $f_*$ is Gaussian:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} y \\\\ f_* \\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "K(X,X)+\\sigma^2 I & K(X,X_*) \\\\\n",
    "K(X_*,X) & K(X_*,X_*)\n",
    "\\end{bmatrix}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Conditioning a Gaussian gives the posterior:\n",
    "\n",
    "$$\n",
    "\\mu_* = K(X_*,X)\\,[K(X,X)+\\sigma^2 I]^{-1} y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_* = K(X_*,X_*) - K(X_*,X)\\,[K(X,X)+\\sigma^2 I]^{-1}K(X,X_*).\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- The **posterior mean** is a kernel-weighted combination of observed outputs.\n",
    "- The **posterior variance** shrinks near observed points and grows far away.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_posterior_regression(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    kernel_fn,\n",
    "    noise_variance: float = 1e-4,\n",
    "    jitter: float = 1e-9,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    X_train = _as_2d(X_train)\n",
    "    X_test = _as_2d(X_test)\n",
    "    y_train = np.asarray(y_train, dtype=float).ravel()\n",
    "\n",
    "    n = X_train.shape[0]\n",
    "\n",
    "    K = kernel_fn(X_train, X_train) + (noise_variance + jitter) * np.eye(n)\n",
    "    L = np.linalg.cholesky(K)\n",
    "\n",
    "    # alpha = K^{-1} y via two triangular solves\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y_train))\n",
    "\n",
    "    K_s = kernel_fn(X_train, X_test)  # n x m\n",
    "    mu = K_s.T @ alpha               # m\n",
    "\n",
    "    v = np.linalg.solve(L, K_s)      # n x m\n",
    "    K_ss = kernel_fn(X_test, X_test)\n",
    "    cov = K_ss - v.T @ v\n",
    "\n",
    "    return mu, cov\n",
    "\n",
    "\n",
    "def log_marginal_likelihood_zero_mean(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    kernel_fn,\n",
    "    noise_variance: float,\n",
    "    jitter: float = 1e-9,\n",
    ") -> float:\n",
    "    X_train = _as_2d(X_train)\n",
    "    y_train = np.asarray(y_train, dtype=float).ravel()\n",
    "    n = X_train.shape[0]\n",
    "\n",
    "    K = kernel_fn(X_train, X_train) + (noise_variance + jitter) * np.eye(n)\n",
    "    L = np.linalg.cholesky(K)\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y_train))\n",
    "\n",
    "    log_det = 2.0 * float(np.sum(np.log(np.diag(L))))\n",
    "    quad = float(y_train @ alpha)\n",
    "    return -0.5 * quad - 0.5 * log_det - 0.5 * n * np.log(2.0 * np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic regression: noisy samples from a smooth function\n",
    "def f_true(x: np.ndarray) -> np.ndarray:\n",
    "    return np.sin(0.8 * x) + 0.25 * np.cos(2.0 * x)\n",
    "\n",
    "\n",
    "n_train = 18\n",
    "X_train = np.sort(rng.uniform(-4.5, 4.5, size=n_train))\n",
    "sigma = 0.15\n",
    "y_train = f_true(X_train) + sigma * rng.normal(size=n_train)\n",
    "\n",
    "X_test = np.linspace(-6, 6, 400)\n",
    "\n",
    "k = lambda X, Z: rbf_kernel(X, Z, length_scale=1.2, variance=1.0)\n",
    "mu, cov = gp_posterior_regression(X_train, y_train, X_test, k, noise_variance=sigma**2)\n",
    "std = np.sqrt(np.clip(np.diag(cov), 0.0, np.inf))\n",
    "\n",
    "posterior_samples = rng.multivariate_normal(mu, cov + 1e-8 * np.eye(len(X_test)), size=5)\n",
    "\n",
    "fig = plot_gp_samples_1d(\n",
    "    X_test,\n",
    "    posterior_samples,\n",
    "    title=\"GP regression posterior (RBF kernel): mean + uncertainty + samples\",\n",
    "    y_mean=mu,\n",
    "    y_std=std,\n",
    "    x_train=X_train,\n",
    "    y_train=y_train,\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=X_test, y=f_true(X_test), mode=\"lines\", line=dict(dash=\"dot\"), name=\"true f(x)\", opacity=0.7))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation vs smoothing: change the assumed noise\n",
    "noise_levels = [1e-6, sigma**2, 0.5**2]\n",
    "titles = [\"almost noiseless (interpolates)\", \"matched noise\", \"high noise (smoother)\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=titles)\n",
    "\n",
    "for col, nv in enumerate(noise_levels, start=1):\n",
    "    mu_c, cov_c = gp_posterior_regression(X_train, y_train, X_test, k, noise_variance=nv)\n",
    "    std_c = np.sqrt(np.clip(np.diag(cov_c), 0.0, np.inf))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=X_test, y=mu_c + 1.96 * std_c, mode=\"lines\", line=dict(width=0), showlegend=False), row=1, col=col)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_test,\n",
    "            y=mu_c - 1.96 * std_c,\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=0),\n",
    "            fill=\"tonexty\",\n",
    "            fillcolor=\"rgba(31, 119, 180, 0.18)\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(go.Scatter(x=X_test, y=mu_c, mode=\"lines\", line=dict(width=3), showlegend=False), row=1, col=col)\n",
    "    fig.add_trace(go.Scatter(x=X_train, y=y_train, mode=\"markers\", marker=dict(size=6, color=\"black\"), showlegend=False), row=1, col=col)\n",
    "\n",
    "fig.update_layout(title=\"Noise controls interpolation vs smoothing\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) GP regression from scratch: what the equations are *doing*\n",
    "\n",
    "The posterior mean can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\mu_*(x_*) = k(x_*,X)\\,[K(X,X)+\\sigma^2 I]^{-1} y.\n",
    "$$\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "- $k(x_*,X)$ measures similarity between $x_*$ and each training point.\n",
    "- $[K+\\sigma^2 I]^{-1}y$ produces weights that account for *redundancy* between training points.\n",
    "\n",
    "So GP regression isn’t just “weighted averaging” — it’s **weighted averaging corrected for correlations**.\n",
    "\n",
    "Also notice the computation bottleneck: factoring/inverting the $n\\times n$ matrix $K+\\sigma^2 I$.\n",
    "\n",
    "That’s why we used a **Cholesky factorization** above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Hyperparameters + log marginal likelihood (Occam’s razor)\n",
    "\n",
    "Kernels have hyperparameters (length-scale, amplitude, periodicity, ...). In GP regression we can fit them by maximizing the **log marginal likelihood**:\n",
    "\n",
    "$$\n",
    "\\log p(y\\mid X,\\theta)\n",
    "=-\\frac{1}{2}y^T K_y^{-1}y\n",
    "-\\frac{1}{2}\\log|K_y|\n",
    "-\\frac{n}{2}\\log(2\\pi),\n",
    "\\quad K_y = K(X,X;\\theta) + \\sigma^2 I.\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- The quadratic term ($y^T K_y^{-1}y$) rewards **data fit**.\n",
    "- The log-determinant term ($\\log|K_y|$) penalizes overly flexible models (it’s a built-in **complexity penalty**).\n",
    "\n",
    "That balance is why people say GPs implement an “Occam’s razor” automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the log marginal likelihood over a small grid of (length_scale, noise)\n",
    "length_scales = np.linspace(0.2, 2.8, 80)\n",
    "noises = np.linspace(0.02, 0.6, 80)  # noise std\n",
    "\n",
    "LML = np.zeros((len(noises), len(length_scales)))\n",
    "\n",
    "for i, ns in enumerate(noises):\n",
    "    for j, ls in enumerate(length_scales):\n",
    "        k_ij = lambda X, Z, ls=ls: rbf_kernel(X, Z, length_scale=ls, variance=1.0)\n",
    "        LML[i, j] = log_marginal_likelihood_zero_mean(X_train, y_train, k_ij, noise_variance=ns**2)\n",
    "\n",
    "best = np.unravel_index(np.argmax(LML), LML.shape)\n",
    "best_noise, best_ls = noises[best[0]], length_scales[best[1]]\n",
    "print(\"best length_scale ~\", float(best_ls), \"best noise std ~\", float(best_noise))\n",
    "\n",
    "fig = px.imshow(\n",
    "    LML,\n",
    "    x=length_scales,\n",
    "    y=noises,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    title=\"Log marginal likelihood surface (RBF): fit vs complexity tradeoff\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"length_scale\", yaxis_title=\"noise std\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[best_ls],\n",
    "        y=[best_noise],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"red\", symbol=\"x\"),\n",
    "        name=\"max\",\n",
    "    )\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick \"adaptive fitting\" demo: pick the next point where uncertainty is largest.\n",
    "\n",
    "def gp_std_on_grid(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, kernel_fn, noise_variance: float) -> np.ndarray:\n",
    "    mu_, cov_ = gp_posterior_regression(X_train, y_train, X_test, kernel_fn, noise_variance=noise_variance)\n",
    "    return np.sqrt(np.clip(np.diag(cov_), 0.0, np.inf))\n",
    "\n",
    "\n",
    "X_train_a = X_train.copy()\n",
    "y_train_a = y_train.copy()\n",
    "\n",
    "std_before = gp_std_on_grid(X_train_a, y_train_a, X_test, k, noise_variance=sigma**2)\n",
    "x_new = float(X_test[np.argmax(std_before)])\n",
    "y_new = float(f_true(np.array([x_new]))[0] + sigma * rng.normal())\n",
    "\n",
    "X_train_a = np.sort(np.r_[X_train_a, x_new])\n",
    "y_train_a = np.r_[y_train_a, y_new][np.argsort(np.r_[X_train, x_new])]\n",
    "\n",
    "std_after = gp_std_on_grid(X_train_a, y_train_a, X_test, k, noise_variance=sigma**2)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=[\"Uncertainty before adding a new point\", \"Uncertainty after adding it\"])\n",
    "fig.add_trace(go.Scatter(x=X_test, y=std_before, mode=\"lines\", name=\"std\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x_new], y=[std_before.max()], mode=\"markers\", marker=dict(size=10, color=\"red\"), name=\"chosen x\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_test, y=std_after, mode=\"lines\", showlegend=False), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x_new], y=[std_after[np.argmax(std_before)]], mode=\"markers\", marker=dict(size=10, color=\"red\"), showlegend=False), row=2, col=1)\n",
    "\n",
    "fig.update_layout(title=\"Adaptive sampling intuition: query where the GP is most uncertain\")\n",
    "fig.update_xaxes(title_text=\"x\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"posterior std\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"posterior std\", row=2, col=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"picked x_new=\", x_new, \"| y_new=\", y_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) `scikit-learn` GP regression + parameter map\n",
    "\n",
    "`scikit-learn` provides `GaussianProcessRegressor` (GPR). It optimizes kernel hyperparameters by maximizing the log marginal likelihood.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- `kernel`: the covariance function. If you include bounds, sklearn will optimize those parameters.\n",
    "- `alpha`: added to the diagonal during fitting.\n",
    "  - can represent observation noise variance (or just numerical jitter)\n",
    "- `normalize_y`: center/scale the targets internally (often helpful)\n",
    "- `n_restarts_optimizer`: tries multiple initializations (helps avoid bad local optima)\n",
    "- `random_state`: reproducibility for the optimizer restarts\n",
    "\n",
    "Common kernel pattern:\n",
    "\n",
    "- `ConstantKernel` sets overall amplitude\n",
    "- `RBF` (or `Matern`) sets smoothness/length-scale\n",
    "- `WhiteKernel` explicitly models noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = X_train.reshape(-1, 1)\n",
    "Xte = X_test.reshape(-1, 1)\n",
    "\n",
    "kernel = ConstantKernel(1.0, (1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(\n",
    "    noise_level=sigma**2,\n",
    "    noise_level_bounds=(1e-6, 1e0),\n",
    ")\n",
    "\n",
    "gpr = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=0.0,\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=5,\n",
    "    random_state=42,\n",
    ")\n",
    "gpr.fit(Xtr, y_train)\n",
    "\n",
    "print(\"Learned kernel:\")\n",
    "print(gpr.kernel_)\n",
    "print(\"log marginal likelihood:\", float(gpr.log_marginal_likelihood(gpr.kernel_.theta)))\n",
    "\n",
    "mu_sk, std_sk = gpr.predict(Xte, return_std=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_test, y=mu_sk + 1.96 * std_sk, mode=\"lines\", line=dict(width=0), showlegend=False))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_test,\n",
    "        y=mu_sk - 1.96 * std_sk,\n",
    "        mode=\"lines\",\n",
    "        line=dict(width=0),\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"rgba(0, 150, 136, 0.18)\",\n",
    "        name=\"95% interval\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=X_test, y=mu_sk, mode=\"lines\", name=\"mean\", line=dict(width=3, color=\"rgb(0,150,136)\")))\n",
    "fig.add_trace(go.Scatter(x=X_train, y=y_train, mode=\"markers\", marker=dict(size=8, color=\"black\"), name=\"observations\"))\n",
    "fig.add_trace(go.Scatter(x=X_test, y=f_true(X_test), mode=\"lines\", line=dict(dash=\"dot\"), name=\"true f(x)\", opacity=0.7))\n",
    "fig.update_layout(title=\"GaussianProcessRegressor (sklearn): posterior mean + 95% interval\", xaxis_title=\"x\", yaxis_title=\"y\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) GP classification: latent GP + logistic link + Laplace (intuition)\n",
    "\n",
    "For classification we want probabilities:\n",
    "\n",
    "$$\n",
    "p(y=1\\mid x).\n",
    "$$\n",
    "\n",
    "A common GP classification model introduces a **latent function** $f(x)$ with a GP prior, then passes it through a link function (logistic):\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(0, k),\n",
    "\\qquad\n",
    "p(y=1\\mid f(x)) = \\sigma(f(x)) = \\frac{1}{1+e^{-f(x)}}.\n",
    "$$\n",
    "\n",
    "The big difference from regression:\n",
    "\n",
    "- The likelihood is **not Gaussian** anymore.\n",
    "- So the posterior $p(f\\mid y)$ is **not exactly Gaussian**.\n",
    "\n",
    "A standard approach is the **Laplace approximation**:\n",
    "\n",
    "1. Find the mode $\\hat{f}$ of the log posterior.\n",
    "2. Approximate the posterior with a Gaussian around that mode.\n",
    "\n",
    "`scikit-learn`'s `GaussianProcessClassifier` implements this (logistic link + Laplace).\n",
    "\n",
    "Below we focus on building intuition via probability maps and uncertainty regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D probabilistic classification demo with sklearn\n",
    "X, y = make_moons(n_samples=220, noise=0.25, random_state=42)\n",
    "\n",
    "# GPs are sensitive to feature scales, so standardize.\n",
    "kernel_rbf = 1.0 * RBF(length_scale=1.0)\n",
    "gpc_rbf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    GaussianProcessClassifier(\n",
    "        kernel=kernel_rbf,\n",
    "        n_restarts_optimizer=2,\n",
    "        max_iter_predict=100,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "gpc_rbf.fit(X, y)\n",
    "\n",
    "# build a grid for visualization\n",
    "pad = 0.6\n",
    "x_min, x_max = float(X[:, 0].min() - pad), float(X[:, 0].max() + pad)\n",
    "y_min, y_max = float(X[:, 1].min() - pad), float(X[:, 1].max() + pad)\n",
    "\n",
    "xs = np.linspace(x_min, x_max, 260)\n",
    "ys = np.linspace(y_min, y_max, 260)\n",
    "xx, yy = np.meshgrid(xs, ys)\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "proba = gpc_rbf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        z=proba,\n",
    "        colorscale=\"Viridis\",\n",
    "        contours=dict(coloring=\"fill\"),\n",
    "        colorbar=dict(title=\"P(y=1)\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# decision boundary at 0.5\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        z=proba,\n",
    "        showscale=False,\n",
    "        contours=dict(coloring=\"none\", start=0.5, end=0.5, size=1),\n",
    "        line=dict(color=\"black\", width=2),\n",
    "        hoverinfo=\"skip\",\n",
    "    )\n",
    ")\n",
    "\n",
    "colors = np.where(y == 1, \"#1f77b4\", \"#d62728\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=7, color=colors, line=dict(color=\"white\", width=0.5)),\n",
    "        name=\"data\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"GaussianProcessClassifier (RBF): probability map + decision boundary\",\n",
    "    xaxis_title=\"x1\",\n",
    "    yaxis_title=\"x2\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"Learned kernel (classifier):\")\n",
    "print(gpc_rbf.named_steps[\"gaussianprocessclassifier\"].kernel_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare kernels for classification: RBF vs Matern (different smoothness assumptions)\n",
    "\n",
    "gpc_matern = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    GaussianProcessClassifier(\n",
    "        kernel=1.0 * Matern(length_scale=1.0, nu=1.5),\n",
    "        n_restarts_optimizer=2,\n",
    "        max_iter_predict=100,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "gpc_matern.fit(X, y)\n",
    "\n",
    "proba_rbf = gpc_rbf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "proba_mat = gpc_matern.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"RBF kernel\", \"Matern (nu=1.5)\"])\n",
    "\n",
    "for col, P in enumerate([proba_rbf, proba_mat], start=1):\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=xs,\n",
    "            y=ys,\n",
    "            z=P,\n",
    "            colorscale=\"Viridis\",\n",
    "            contours=dict(coloring=\"fill\"),\n",
    "            showscale=(col == 2),\n",
    "            colorbar=dict(title=\"P(y=1)\"),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=xs,\n",
    "            y=ys,\n",
    "            z=P,\n",
    "            showscale=False,\n",
    "            contours=dict(coloring=\"none\", start=0.5, end=0.5, size=1),\n",
    "            line=dict(color=\"black\", width=2),\n",
    "            hoverinfo=\"skip\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X[:, 0],\n",
    "            y=X[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=6, color=colors, line=dict(color=\"white\", width=0.5)),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "fig.update_layout(title=\"Kernel choice changes the function prior → changes the classifier\")\n",
    "fig.show()\n",
    "\n",
    "print(\"RBF kernel learned:\")\n",
    "print(gpc_rbf.named_steps[\"gaussianprocessclassifier\"].kernel_)\n",
    "print(\"Matern kernel learned:\")\n",
    "print(gpc_matern.named_steps[\"gaussianprocessclassifier\"].kernel_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Practical tips, limitations, and scaling strategies\n",
    "\n",
    "### Practical tips\n",
    "\n",
    "- **Scale inputs**: kernels are distance-based; feature scaling changes everything.\n",
    "- Start with **RBF** (very smooth) or **Matern** (rougher) and let the optimizer tune length-scales.\n",
    "- For regression, include a **WhiteKernel** (explicit noise) or use `alpha`.\n",
    "- Use `n_restarts_optimizer` if hyperparameters are sensitive.\n",
    "\n",
    "### Complexity (why GPs don’t scale by default)\n",
    "\n",
    "- Training typically needs a Cholesky of an $n\\times n$ matrix: **$\\mathcal{O}(n^3)$**.\n",
    "- Storing the kernel matrix costs **$\\mathcal{O}(n^2)$** memory.\n",
    "\n",
    "### High-dimensional inputs\n",
    "\n",
    "In high dimensions, distances become less informative and kernels can behave poorly.\n",
    "\n",
    "Common fixes:\n",
    "\n",
    "- dimensionality reduction / feature engineering\n",
    "- ARD kernels (separate length-scale per feature)\n",
    "- sparse/approximate GP methods (inducing points, random Fourier features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Build a GP regression model with a **periodic** kernel and fit data from a periodic function. How does the posterior behave outside the observed range?\n",
    "2. For regression, compare `RBF` vs `Matern(nu=0.5, 1.5, 2.5)`. Which priors look most realistic for noisy data?\n",
    "3. On the moons dataset, sweep over `length_scale` (fix it instead of optimizing) and visualize under/over-fitting.\n",
    "4. Explain (in words) why the log marginal likelihood includes a $\\log|K_y|$ term and how it relates to model complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Rasmussen, Williams (2006): *Gaussian Processes for Machine Learning* (the classic)\n",
    "- `scikit-learn` docs: `GaussianProcessRegressor`, `GaussianProcessClassifier`, `sklearn.gaussian_process.kernels`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}