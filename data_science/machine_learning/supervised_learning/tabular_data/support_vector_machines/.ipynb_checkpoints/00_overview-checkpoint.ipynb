{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Machines (SVC + SVR)\n",
        "\n",
        "Support Vector Machines (SVMs) are margin-based models: they don\u2019t just separate classes or fit a curve \u2014 they try to do it with the **widest possible safety buffer**.\n",
        "\n",
        "- **SVC (classification)**: finds a decision boundary with a maximum margin.\n",
        "- **SVR (regression)**: fits a function while ignoring small errors inside an **\u03b5-tube**.\n",
        "\n",
        "This notebook builds intuition with Plotly visuals, derives the key equations (including where **Lagrange multipliers** appear), implements simplified versions from scratch, and then connects everything to `scikit-learn`.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning goals\n",
        "\n",
        "By the end you should be able to:\n",
        "\n",
        "- explain the geometry of the **maximum margin** idea (support vectors + margin)\n",
        "- derive the **primal** and **dual** forms and interpret the **Lagrange multipliers**\n",
        "- explain the **kernel trick** and common kernels (linear / polynomial / RBF)\n",
        "- train and visualize SVM decision boundaries (Plotly)\n",
        "- implement a simplified kernel SVM classifier (SMO-style, NumPy)\n",
        "- implement a simplified kernel SVR (dual QP setup)\n",
        "- use `sklearn.svm.SVC` and `sklearn.svm.SVR` and explain key parameters (`C`, `gamma`, `epsilon`, ...)\n",
        "\n",
        "---\n",
        "\n",
        "## Notation\n",
        "\n",
        "- Dataset: $(x_i, y_i)$ for $i=1..n$\n",
        "- Feature matrix: $X \\in \\mathbb{R}^{n \\times d}$\n",
        "- Binary labels: $y_i \\in \\{-1, +1\\}$\n",
        "- Hyperplane: $f(x) = w^T x + b$\n",
        "- Kernel: $K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle$\n",
        "\n",
        "---\n",
        "\n",
        "## Table of contents\n",
        "\n",
        "1. Maximum margin intuition (classification)\n",
        "2. Lagrange multipliers + KKT (why they show up)\n",
        "3. Hard-margin SVM (primal \u2194 dual)\n",
        "4. Soft margin + the role of `C`\n",
        "5. Kernels + the kernel trick (RBF / poly / ...)\n",
        "6. SVM classification from scratch (simplified SMO)\n",
        "7. Support Vector Regression (SVR): \u03b5-tube + dual\n",
        "8. `scikit-learn` SVMs: practical parameter map\n",
        "9. Exercises + references\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Literal\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "from sklearn.datasets import make_blobs, make_circles\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC, SVR\n",
        "\n",
        "\n",
        "pio.templates.default = \"plotly_white\"\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "\n",
        "rng = np.random.default_rng(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Maximum margin intuition (classification)\n",
        "\n",
        "A linear classifier uses a hyperplane:\n",
        "\n",
        "$$\n",
        "f(x) = w^T x + b\n",
        "$$\n",
        "\n",
        "and predicts:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\mathrm{sign}(f(x)).\n",
        "$$\n",
        "\n",
        "For a point $x_i$, the signed distance to the hyperplane is:\n",
        "\n",
        "$$\n",
        "\\frac{w^T x_i + b}{\\lVert w \\rVert}.\n",
        "$$\n",
        "\n",
        "SVMs choose $(w,b)$ to make the *smallest* (signed) distance as large as possible \u2014 i.e. **maximize the margin**.\n",
        "\n",
        "In the hard-margin (perfectly separable) case, we can scale $(w,b)$ so that:\n",
        "\n",
        "$$\n",
        "y_i (w^T x_i + b) \\ge 1\n",
        "$$\n",
        "\n",
        "and the geometric margin becomes:\n",
        "\n",
        "$$\n",
        "\\text{margin} = \\frac{2}{\\lVert w \\rVert}.\n",
        "$$\n",
        "\n",
        "The points that touch the margin are the **support vectors**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A simple 2D, almost-separable dataset\n",
        "X, y01 = make_blobs(n_samples=90, centers=2, cluster_std=1.4, random_state=42)\n",
        "y = np.where(y01 == 0, -1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Xs = scaler.fit_transform(X)\n",
        "\n",
        "fig = px.scatter(\n",
        "    x=Xs[:, 0],\n",
        "    y=Xs[:, 1],\n",
        "    color=y.astype(str),\n",
        "    title=\"A simple 2D binary dataset (features scaled)\",\n",
        ")\n",
        "fig.update_layout(xaxis_title=\"x1 (scaled)\", yaxis_title=\"x2 (scaled)\", legend_title=\"y\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_svm_decision_2d(\n",
        "    model,\n",
        "    X2: np.ndarray,\n",
        "    y2: np.ndarray,\n",
        "    title: str,\n",
        "    show_margin: bool = True,\n",
        "    grid_res: int = 300,\n",
        ") -> go.Figure:\n",
        "    x_min, x_max = float(X2[:, 0].min() - 1.0), float(X2[:, 0].max() + 1.0)\n",
        "    y_min, y_max = float(X2[:, 1].min() - 1.0), float(X2[:, 1].max() + 1.0)\n",
        "\n",
        "    xs = np.linspace(x_min, x_max, grid_res)\n",
        "    ys = np.linspace(y_min, y_max, grid_res)\n",
        "    xx, yy = np.meshgrid(xs, ys)\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    zz = model.decision_function(grid).reshape(xx.shape)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Contour(\n",
        "            x=xs,\n",
        "            y=ys,\n",
        "            z=zz,\n",
        "            colorscale=\"RdBu\",\n",
        "            opacity=0.25,\n",
        "            showscale=False,\n",
        "            contours=dict(coloring=\"fill\", showlines=False),\n",
        "            hoverinfo=\"skip\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    levels = [-1.0, 0.0, 1.0] if show_margin else [0.0]\n",
        "    colors = [\"#444\", \"black\", \"#444\"] if show_margin else [\"black\"]\n",
        "    widths = [1, 2, 1] if show_margin else [2]\n",
        "    for level, color, width in zip(levels, colors, widths):\n",
        "        fig.add_trace(\n",
        "            go.Contour(\n",
        "                x=xs,\n",
        "                y=ys,\n",
        "                z=zz,\n",
        "                showscale=False,\n",
        "                contours=dict(coloring=\"none\", start=level, end=level, size=1),\n",
        "                line=dict(color=color, width=width),\n",
        "                hoverinfo=\"skip\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    colors = np.where(y2 == 1, \"#1f77b4\", \"#d62728\")\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=X2[:, 0],\n",
        "            y=X2[:, 1],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(size=8, color=colors, line=dict(color=\"white\", width=0.5)),\n",
        "            name=\"points\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if hasattr(model, \"support_vectors_\"):\n",
        "        sv = getattr(model, \"support_vectors_\")\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=sv[:, 0],\n",
        "                y=sv[:, 1],\n",
        "                mode=\"markers\",\n",
        "                marker=dict(size=14, symbol=\"circle-open\", color=\"black\", line=dict(width=2)),\n",
        "                name=\"support vectors\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"x1 (scaled)\",\n",
        "        yaxis_title=\"x2 (scaled)\",\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "\n",
        "clf_linear = SVC(kernel=\"linear\", C=1.0)\n",
        "clf_linear.fit(Xs, y)\n",
        "\n",
        "fig = plot_svm_decision_2d(\n",
        "    clf_linear,\n",
        "    Xs,\n",
        "    y,\n",
        "    title=\"Linear SVM: decision boundary (0) and margins (\u00b11)\",\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Lagrange multipliers + KKT (why they show up)\n",
        "\n",
        "SVMs are built from a **constrained optimization** problem.\n",
        "\n",
        "### Lagrange multipliers (big idea)\n",
        "\n",
        "- If you want to minimize $f(\\theta)$ subject to an **equality constraint** $g(\\theta)=0$, you build a Lagrangian:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta,\\lambda) = f(\\theta) + \\lambda\\, g(\\theta).\n",
        "$$\n",
        "\n",
        "At an optimum you get (informally):\n",
        "\n",
        "- **stationarity**: $\\nabla_\\theta \\mathcal{L}=0$\n",
        "- **feasibility**: $g(\\theta)=0$\n",
        "\n",
        "### Inequality constraints and KKT\n",
        "\n",
        "For **inequality constraints** $h(\\theta) \\le 0$, you introduce multipliers $\\alpha \\ge 0$:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta,\\alpha)=f(\\theta)+\\alpha\\,h(\\theta).\n",
        "$$\n",
        "\n",
        "The KKT conditions include an extra rule:\n",
        "\n",
        "$$\n",
        "\\alpha\\,h(\\theta)=0 \\quad \\text{(complementary slackness)}.\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- If a constraint is **inactive** ($h(\\theta)<0$), then it must have **zero weight** ($\\alpha=0$).\n",
        "- Only the **active** constraints can have non-zero multipliers.\n",
        "\n",
        "This is the heart of the *support vector* story: only a small subset of points (the active constraints) end up with non-zero multipliers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Hard-margin SVM (primal \u2194 dual)\n",
        "\n",
        "### Primal (hard margin)\n",
        "\n",
        "If the data are perfectly separable, the *hard-margin* SVM is:\n",
        "\n",
        "$$\n",
        "\\min_{w,b}\\; \\frac{1}{2}\\lVert w \\rVert^2\n",
        "\\quad \\text{s.t.} \\quad\n",
        "y_i (w^T x_i + b) \\ge 1\\;\\;\\forall i.\n",
        "$$\n",
        "\n",
        "### Lagrangian\n",
        "\n",
        "Introduce multipliers $\\alpha_i \\ge 0$ for the constraints:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\alpha)\n",
        "=\\frac{1}{2}\\lVert w \\rVert^2 - \\sum_i \\alpha_i \\big(y_i(w^T x_i + b)-1\\big).\n",
        "$$\n",
        "\n",
        "Stationarity gives:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w}=0 \\Rightarrow w = \\sum_i \\alpha_i y_i x_i,\n",
        "\\qquad\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b}=0 \\Rightarrow \\sum_i \\alpha_i y_i=0.\n",
        "$$\n",
        "\n",
        "### Dual (hard margin)\n",
        "\n",
        "Plugging back yields the dual problem:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha}\\; \\sum_i \\alpha_i - \\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_i y_j \\langle x_i,x_j\\rangle\n",
        "\\quad\\text{s.t.}\\quad\n",
        "\\alpha_i\\ge 0,\\; \\sum_i \\alpha_i y_i=0.\n",
        "$$\n",
        "\n",
        "Key takeaway:\n",
        "\n",
        "- The solution is expressed in terms of **inner products** $\\langle x_i, x_j\\rangle$.\n",
        "- Only points with $\\alpha_i>0$ matter \u2192 those are the **support vectors**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The dual coefficients in sklearn's linear SVC let us *reconstruct* w as a weighted sum of support vectors.\n",
        "# This is a concrete view of what the Lagrange multipliers are doing.\n",
        "\n",
        "clf_almost_hard = SVC(kernel=\"linear\", C=1e6)\n",
        "clf_almost_hard.fit(Xs, y)\n",
        "\n",
        "w_sklearn = clf_almost_hard.coef_[0]\n",
        "b_sklearn = float(clf_almost_hard.intercept_[0])\n",
        "\n",
        "y_alpha = clf_almost_hard.dual_coef_[0]  # shape (n_support,), equals (y_i * alpha_i) for support vectors\n",
        "sv = clf_almost_hard.support_vectors_\n",
        "w_from_dual = y_alpha @ sv\n",
        "\n",
        "print(\"||w_sklearn - w_from_dual||:\", float(np.linalg.norm(w_sklearn - w_from_dual)))\n",
        "print(\"b_sklearn:\", b_sklearn)\n",
        "print(\"n_support:\", sv.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Soft margin + the role of `C`\n",
        "\n",
        "Real data often aren\u2019t perfectly separable. The soft-margin SVM adds slack variables (or equivalently, hinge loss):\n",
        "\n",
        "$$\n",
        "\\min_{w,b}\\; \\frac{1}{2}\\lVert w \\rVert^2 + C\\sum_i \\max(0, 1 - y_i(w^T x_i + b)).\n",
        "$$\n",
        "\n",
        "Mental model:\n",
        "\n",
        "- **Large `C`**: cares a lot about misclassifications \u2192 tries to fit the training data harder (narrower margin).\n",
        "- **Small `C`**: tolerates training errors \u2192 prefers a wider margin (more regularization).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# A noisier dataset to see the C tradeoff\n",
        "X2, y2_01 = make_blobs(n_samples=130, centers=2, cluster_std=2.3, random_state=0)\n",
        "y2 = np.where(y2_01 == 0, -1, 1)\n",
        "Xs2 = StandardScaler().fit_transform(X2)\n",
        "\n",
        "Cs = [0.2, 10.0]\n",
        "models = [SVC(kernel=\"linear\", C=C).fit(Xs2, y2) for C in Cs]\n",
        "\n",
        "for C, m in zip(Cs, models):\n",
        "    acc = accuracy_score(y2, m.predict(Xs2))\n",
        "    print(f\"C={C:>5}: train acc={acc:.3f} | support vectors={m.support_vectors_.shape[0]}\")\n",
        "\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=[f\"C={C}\" for C in Cs])\n",
        "for col, m in enumerate(models, start=1):\n",
        "    x_min, x_max = float(Xs2[:, 0].min() - 1.0), float(Xs2[:, 0].max() + 1.0)\n",
        "    y_min, y_max = float(Xs2[:, 1].min() - 1.0), float(Xs2[:, 1].max() + 1.0)\n",
        "    xs = np.linspace(x_min, x_max, 250)\n",
        "    ys = np.linspace(y_min, y_max, 250)\n",
        "    xx, yy = np.meshgrid(xs, ys)\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    zz = m.decision_function(grid).reshape(xx.shape)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Contour(\n",
        "            x=xs,\n",
        "            y=ys,\n",
        "            z=zz,\n",
        "            colorscale=\"RdBu\",\n",
        "            opacity=0.25,\n",
        "            showscale=False,\n",
        "            contours=dict(coloring=\"fill\", showlines=False),\n",
        "            hoverinfo=\"skip\",\n",
        "        ),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "\n",
        "    for level, width in [(-1.0, 1), (0.0, 2), (1.0, 1)]:\n",
        "        fig.add_trace(\n",
        "            go.Contour(\n",
        "                x=xs,\n",
        "                y=ys,\n",
        "                z=zz,\n",
        "                showscale=False,\n",
        "                contours=dict(coloring=\"none\", start=level, end=level, size=1),\n",
        "                line=dict(color=\"black\", width=width),\n",
        "                hoverinfo=\"skip\",\n",
        "            ),\n",
        "            row=1,\n",
        "            col=col,\n",
        "        )\n",
        "\n",
        "    colors = np.where(y2 == 1, \"#1f77b4\", \"#d62728\")\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=Xs2[:, 0],\n",
        "            y=Xs2[:, 1],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(size=7, color=colors, line=dict(color=\"white\", width=0.5)),\n",
        "            showlegend=False,\n",
        "        ),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "    sv = m.support_vectors_\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=sv[:, 0],\n",
        "            y=sv[:, 1],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(size=13, symbol=\"circle-open\", color=\"black\", line=dict(width=2)),\n",
        "            showlegend=False,\n",
        "        ),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Soft margin tradeoff: smaller C widens the margin (more regularization)\",\n",
        "    xaxis_title=\"x1 (scaled)\",\n",
        "    yaxis_title=\"x2 (scaled)\",\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Kernels + the kernel trick (RBF / poly / ...)\n",
        "\n",
        "The dual SVM objective depends on the data only through **inner products**:\n",
        "\n",
        "$$\n",
        "\\langle x_i, x_j \\rangle.\n",
        "$$\n",
        "\n",
        "If we first map inputs to a feature space $\\phi(x)$, the inner product becomes:\n",
        "\n",
        "$$\n",
        "\\langle \\phi(x_i), \\phi(x_j) \\rangle.\n",
        "$$\n",
        "\n",
        "A **kernel** is any function $K$ that *acts like* that inner product:\n",
        "\n",
        "$$\n",
        "K(x,z) = \\langle \\phi(x), \\phi(z) \\rangle.\n",
        "$$\n",
        "\n",
        "This is the kernel trick: we get non-linear decision boundaries without ever explicitly building $\\phi(x)$.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "- Linear: $K(x,z) = x^T z$\n",
        "- Polynomial: $K(x,z) = (\\gamma x^T z + c_0)^{d}$\n",
        "- RBF / Gaussian: $K(x,z)=\\exp(-\\gamma\\lVert x-z \\rVert^2)$\n",
        "- Sigmoid: $K(x,z)=\\tanh(\\gamma x^T z + c_0)$\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "- `gamma`: sets the length-scale (larger \\(\\gamma\\) means \u201cmore local\u201d influence)\n",
        "- `degree`: polynomial complexity\n",
        "- `coef0`: shifts polynomial/sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KernelName = Literal[\"linear\", \"poly\", \"rbf\"]\n",
        "\n",
        "\n",
        "def _gamma_scale(X: np.ndarray) -> float:\n",
        "    # sklearn's default for RBF/poly/sigmoid is gamma='scale': 1 / (n_features * X.var())\n",
        "    return 1.0 / (X.shape[1] * float(X.var()))\n",
        "\n",
        "\n",
        "def linear_kernel(X: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "    return X @ Z.T\n",
        "\n",
        "\n",
        "def polynomial_kernel(\n",
        "    X: np.ndarray,\n",
        "    Z: np.ndarray,\n",
        "    degree: int = 3,\n",
        "    gamma: float | None = None,\n",
        "    coef0: float = 1.0,\n",
        ") -> np.ndarray:\n",
        "    gamma = _gamma_scale(X) if gamma is None else float(gamma)\n",
        "    return (gamma * (X @ Z.T) + coef0) ** degree\n",
        "\n",
        "\n",
        "def rbf_kernel(X: np.ndarray, Z: np.ndarray, gamma: float | None = None) -> np.ndarray:\n",
        "    gamma = _gamma_scale(X) if gamma is None else float(gamma)\n",
        "    X_norm = np.sum(X**2, axis=1)[:, None]\n",
        "    Z_norm = np.sum(Z**2, axis=1)[None, :]\n",
        "    dist2 = X_norm + Z_norm - 2.0 * (X @ Z.T)\n",
        "    return np.exp(-gamma * dist2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Non-linearly separable dataset\n",
        "Xc, yc01 = make_circles(n_samples=220, factor=0.45, noise=0.08, random_state=42)\n",
        "yc = np.where(yc01 == 0, -1, 1)\n",
        "Xcs = StandardScaler().fit_transform(Xc)\n",
        "\n",
        "fig = px.scatter(\n",
        "    x=Xcs[:, 0],\n",
        "    y=Xcs[:, 1],\n",
        "    color=yc.astype(str),\n",
        "    title=\"A classic non-linear dataset: circles (scaled)\",\n",
        ")\n",
        "fig.update_layout(xaxis_title=\"x1 (scaled)\", yaxis_title=\"x2 (scaled)\", legend_title=\"y\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "configs = [\n",
        "    (\"linear\", dict(kernel=\"linear\")),\n",
        "    (\"poly (deg=3)\", dict(kernel=\"poly\", degree=3, gamma=\"scale\", coef0=1.0)),\n",
        "    (\"rbf\", dict(kernel=\"rbf\", gamma=\"scale\")),\n",
        "]\n",
        "\n",
        "fig = make_subplots(rows=1, cols=3, subplot_titles=[c[0] for c in configs])\n",
        "\n",
        "for col, (name, params) in enumerate(configs, start=1):\n",
        "    m = SVC(C=2.0, **params)\n",
        "    m.fit(Xcs, yc)\n",
        "\n",
        "    x_min, x_max = float(Xcs[:, 0].min() - 1.0), float(Xcs[:, 0].max() + 1.0)\n",
        "    y_min, y_max = float(Xcs[:, 1].min() - 1.0), float(Xcs[:, 1].max() + 1.0)\n",
        "    xs = np.linspace(x_min, x_max, 260)\n",
        "    ys = np.linspace(y_min, y_max, 260)\n",
        "    xx, yy = np.meshgrid(xs, ys)\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    zz = m.decision_function(grid).reshape(xx.shape)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Contour(\n",
        "            x=xs,\n",
        "            y=ys,\n",
        "            z=zz,\n",
        "            colorscale=\"RdBu\",\n",
        "            opacity=0.25,\n",
        "            showscale=False,\n",
        "            contours=dict(coloring=\"fill\", showlines=False),\n",
        "            hoverinfo=\"skip\",\n",
        "        ),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Contour(\n",
        "            x=xs,\n",
        "            y=ys,\n",
        "            z=zz,\n",
        "            showscale=False,\n",
        "            contours=dict(coloring=\"none\", start=0.0, end=0.0, size=1),\n",
        "            line=dict(color=\"black\", width=2),\n",
        "            hoverinfo=\"skip\",\n",
        "        ),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "\n",
        "    colors = np.where(yc == 1, \"#1f77b4\", \"#d62728\")\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=Xcs[:, 0],\n",
        "            y=Xcs[:, 1],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(size=6, color=colors, line=dict(color=\"white\", width=0.5)),\n",
        "            showlegend=False,\n",
        "        ),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "\n",
        "fig.update_layout(title=\"Kernel effect: non-linear boundaries via the kernel trick\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RBF gamma controls locality. Here's what the kernel similarity matrix looks like.\n",
        "\n",
        "Xk = Xcs[:120]\n",
        "gammas = [0.2, 1.0, 5.0]\n",
        "\n",
        "fig = make_subplots(rows=1, cols=3, subplot_titles=[f\"gamma={g}\" for g in gammas])\n",
        "for col, g in enumerate(gammas, start=1):\n",
        "    K = rbf_kernel(Xk, Xk, gamma=g)\n",
        "    fig.add_trace(go.Heatmap(z=K, colorscale=\"Viridis\", showscale=(col == 3)), row=1, col=col)\n",
        "\n",
        "fig.update_layout(title=\"RBF kernel matrix: larger gamma \u2192 more local similarity\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) SVM classification from scratch (simplified SMO)\n",
        "\n",
        "The soft-margin dual (with kernels) is a quadratic program:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha}\\; \\sum_i \\alpha_i - \\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_i y_j K(x_i,x_j)\n",
        "\\quad\\text{s.t.}\\quad\n",
        "0 \\le \\alpha_i \\le C,\\; \\sum_i \\alpha_i y_i=0.\n",
        "$$\n",
        "\n",
        "A classic way to solve this efficiently is **SMO (Sequential Minimal Optimization)**: repeatedly update two multipliers at a time while keeping constraints satisfied.\n",
        "\n",
        "Below is a **learning-oriented** (not production-optimized) SMO-style implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class KernelSVMClassifierSMO:\n",
        "    C: float = 1.0\n",
        "    kernel: KernelName = \"rbf\"\n",
        "    degree: int = 3\n",
        "    gamma: float | None = None\n",
        "    coef0: float = 1.0\n",
        "    tol: float = 1e-3\n",
        "    max_passes: int = 10\n",
        "    max_iter: int = 10_000\n",
        "    random_state: int = 42\n",
        "\n",
        "    def _kernel_matrix(self, X: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        if self.kernel == \"linear\":\n",
        "            return linear_kernel(X, Z)\n",
        "        if self.kernel == \"poly\":\n",
        "            return polynomial_kernel(X, Z, degree=self.degree, gamma=self.gamma, coef0=self.coef0)\n",
        "        if self.kernel == \"rbf\":\n",
        "            return rbf_kernel(X, Z, gamma=self.gamma)\n",
        "        raise ValueError(f\"Unknown kernel: {self.kernel}\")\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"KernelSVMClassifierSMO\":\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float)\n",
        "        y = np.where(y <= 0, -1.0, 1.0)\n",
        "\n",
        "        n = X.shape[0]\n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "        self.alphas_ = np.zeros(n)\n",
        "        self.b_ = 0.0\n",
        "        self.K_ = self._kernel_matrix(X, X)\n",
        "\n",
        "        rng_local = np.random.default_rng(self.random_state)\n",
        "\n",
        "        def f_i(i: int) -> float:\n",
        "            return float((self.alphas_ * self.y_) @ self.K_[:, i] + self.b_)\n",
        "\n",
        "        passes = 0\n",
        "        it = 0\n",
        "\n",
        "        while passes < self.max_passes and it < self.max_iter:\n",
        "            num_changed = 0\n",
        "            for i in range(n):\n",
        "                Ei = f_i(i) - float(y[i])\n",
        "                ai = self.alphas_[i]\n",
        "\n",
        "                violates = (y[i] * Ei < -self.tol and ai < self.C) or (y[i] * Ei > self.tol and ai > 0)\n",
        "                if not violates:\n",
        "                    continue\n",
        "\n",
        "                j = int(rng_local.integers(0, n - 1))\n",
        "                if j >= i:\n",
        "                    j += 1\n",
        "\n",
        "                Ej = f_i(j) - float(y[j])\n",
        "                aj = self.alphas_[j]\n",
        "\n",
        "                ai_old, aj_old = ai, aj\n",
        "\n",
        "                if y[i] != y[j]:\n",
        "                    L = max(0.0, aj_old - ai_old)\n",
        "                    H = min(self.C, self.C + aj_old - ai_old)\n",
        "                else:\n",
        "                    L = max(0.0, ai_old + aj_old - self.C)\n",
        "                    H = min(self.C, ai_old + aj_old)\n",
        "                if L == H:\n",
        "                    continue\n",
        "\n",
        "                eta = 2.0 * self.K_[i, j] - self.K_[i, i] - self.K_[j, j]\n",
        "                if eta >= 0:\n",
        "                    continue\n",
        "\n",
        "                aj_new = aj_old - y[j] * (Ei - Ej) / eta\n",
        "                aj_new = float(np.clip(aj_new, L, H))\n",
        "                if abs(aj_new - aj_old) < 1e-5:\n",
        "                    continue\n",
        "\n",
        "                ai_new = ai_old + y[i] * y[j] * (aj_old - aj_new)\n",
        "\n",
        "                b1 = (\n",
        "                    self.b_\n",
        "                    - Ei\n",
        "                    - y[i] * (ai_new - ai_old) * self.K_[i, i]\n",
        "                    - y[j] * (aj_new - aj_old) * self.K_[i, j]\n",
        "                )\n",
        "                b2 = (\n",
        "                    self.b_\n",
        "                    - Ej\n",
        "                    - y[i] * (ai_new - ai_old) * self.K_[i, j]\n",
        "                    - y[j] * (aj_new - aj_old) * self.K_[j, j]\n",
        "                )\n",
        "\n",
        "                if 0 < ai_new < self.C:\n",
        "                    b_new = b1\n",
        "                elif 0 < aj_new < self.C:\n",
        "                    b_new = b2\n",
        "                else:\n",
        "                    b_new = 0.5 * (b1 + b2)\n",
        "\n",
        "                self.alphas_[i] = ai_new\n",
        "                self.alphas_[j] = aj_new\n",
        "                self.b_ = float(b_new)\n",
        "                num_changed += 1\n",
        "\n",
        "            passes = passes + 1 if num_changed == 0 else 0\n",
        "            it += 1\n",
        "\n",
        "        self.support_ = np.where(self.alphas_ > 1e-8)[0]\n",
        "        self.support_vectors_ = self.X_[self.support_]\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        K = self._kernel_matrix(self.X_, X)\n",
        "        return (self.alphas_ * self.y_) @ K + self.b_\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        scores = self.decision_function(X)\n",
        "        return np.where(scores >= 0.0, 1, -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_scratch = KernelSVMClassifierSMO(C=2.0, kernel=\"rbf\", gamma=_gamma_scale(Xcs), max_passes=15)\n",
        "svm_scratch.fit(Xcs, yc)\n",
        "\n",
        "svm_sklearn = SVC(C=2.0, kernel=\"rbf\", gamma=_gamma_scale(Xcs))\n",
        "svm_sklearn.fit(Xcs, yc)\n",
        "\n",
        "acc_scratch = accuracy_score(yc, svm_scratch.predict(Xcs))\n",
        "acc_sklearn = accuracy_score(yc, svm_sklearn.predict(Xcs))\n",
        "\n",
        "print(f\"scratch acc: {acc_scratch:.3f} | support vectors: {svm_scratch.support_vectors_.shape[0]}\")\n",
        "print(f\"sklearn acc: {acc_sklearn:.3f} | support vectors: {svm_sklearn.support_vectors_.shape[0]}\")\n",
        "\n",
        "fig = plot_svm_decision_2d(\n",
        "    svm_scratch,\n",
        "    Xcs,\n",
        "    yc,\n",
        "    title=\"From scratch (SMO-style) kernel SVM on circles (RBF)\",\n",
        "    show_margin=False,\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Support Vector Regression (SVR): \u03b5-tube + dual\n",
        "\n",
        "SVR is the regression sibling of SVC. Instead of separating classes, it fits a function while ignoring small errors.\n",
        "\n",
        "### Intuition: the \u03b5-insensitive tube\n",
        "\n",
        "SVR tries to find a function $f(x)$ such that most points lie within an $\u03b5$-tube:\n",
        "\n",
        "$$\n",
        "|y_i - f(x_i)| \\le \\epsilon.\n",
        "$$\n",
        "\n",
        "Errors smaller than $\\epsilon$ are treated as **free**. Points that touch or break the tube become support vectors.\n",
        "\n",
        "### Primal (soft margin)\n",
        "\n",
        "One common form is:\n",
        "\n",
        "$$\n",
        "\\min_{w,b}\\; \\frac{1}{2}\\lVert w \\rVert^2 + C\\sum_i \\max\\big(0,\\;|y_i-(w^T x_i+b)|-\\epsilon\\big).\n",
        "$$\n",
        "\n",
        "### Dual (why SVR also has support vectors)\n",
        "\n",
        "The dual uses two sets of multipliers $(\\alpha_i, \\alpha_i^*)$ (for the upper and lower tube constraints). The learned function becomes:\n",
        "\n",
        "$$\n",
        "f(x)=\\sum_i (\\alpha_i-\\alpha_i^*)\\,K(x_i,x) + b.\n",
        "$$\n",
        "\n",
        "Below we set up and solve this dual for a small dataset using `scipy.optimize` (again: learning-oriented, not production-optimized).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class KernelSVRQP:\n",
        "    C: float = 1.0\n",
        "    epsilon: float = 0.1\n",
        "    kernel: KernelName = \"rbf\"\n",
        "    degree: int = 3\n",
        "    gamma: float | None = None\n",
        "    coef0: float = 1.0\n",
        "    tol: float = 1e-8\n",
        "    max_iter: int = 2000\n",
        "\n",
        "    def _kernel_matrix(self, X: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        if self.kernel == \"linear\":\n",
        "            return linear_kernel(X, Z)\n",
        "        if self.kernel == \"poly\":\n",
        "            return polynomial_kernel(X, Z, degree=self.degree, gamma=self.gamma, coef0=self.coef0)\n",
        "        if self.kernel == \"rbf\":\n",
        "            return rbf_kernel(X, Z, gamma=self.gamma)\n",
        "        raise ValueError(f\"Unknown kernel: {self.kernel}\")\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"KernelSVRQP\":\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float)\n",
        "\n",
        "        n = X.shape[0]\n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "        K = self._kernel_matrix(X, X)\n",
        "\n",
        "        # variables are a = [alpha, alpha*] (length 2n)\n",
        "        def objective(a: np.ndarray) -> float:\n",
        "            alpha = a[:n]\n",
        "            alpha_star = a[n:]\n",
        "            beta = alpha - alpha_star\n",
        "            # We *minimize* the negative dual (a convex QP in this form):\n",
        "            # 0.5 * beta^T K beta + epsilon * sum(alpha + alpha*) - y^T beta\n",
        "            return 0.5 * float(beta @ K @ beta) + self.epsilon * float(np.sum(alpha + alpha_star)) - float(y @ beta)\n",
        "\n",
        "        def grad(a: np.ndarray) -> np.ndarray:\n",
        "            alpha = a[:n]\n",
        "            alpha_star = a[n:]\n",
        "            beta = alpha - alpha_star\n",
        "            g_beta = K @ beta\n",
        "            g_alpha = g_beta + self.epsilon - y\n",
        "            g_alpha_star = -g_beta + self.epsilon + y\n",
        "            return np.concatenate([g_alpha, g_alpha_star])\n",
        "\n",
        "        cons = {\n",
        "            \"type\": \"eq\",\n",
        "            \"fun\": lambda a: float(np.sum(a[:n] - a[n:])),\n",
        "            \"jac\": lambda a: np.concatenate([np.ones(n), -np.ones(n)]),\n",
        "        }\n",
        "        bounds = [(0.0, self.C) for _ in range(2 * n)]\n",
        "        x0 = np.zeros(2 * n)\n",
        "\n",
        "        res = minimize(\n",
        "            objective,\n",
        "            x0,\n",
        "            method=\"SLSQP\",\n",
        "            jac=grad,\n",
        "            bounds=bounds,\n",
        "            constraints=[cons],\n",
        "            options={\"ftol\": self.tol, \"maxiter\": self.max_iter, \"disp\": False},\n",
        "        )\n",
        "        if not res.success:\n",
        "            print(\"Warning: optimizer did not fully converge:\", res.message)\n",
        "\n",
        "        a = np.asarray(res.x)\n",
        "        self.alpha_ = a[:n]\n",
        "        self.alpha_star_ = a[n:]\n",
        "        self.beta_ = self.alpha_ - self.alpha_star_\n",
        "\n",
        "        # Compute intercept b using KKT conditions:\n",
        "        # - if 0 < alpha_i < C: f(x_i) = y_i - epsilon\n",
        "        # - if 0 < alpha*_i < C: f(x_i) = y_i + epsilon\n",
        "        f_no_b = self.beta_ @ K  # shape (n,)\n",
        "\n",
        "        tol = 1e-6\n",
        "        mask_up = (self.alpha_ > tol) & (self.alpha_ < self.C - tol)\n",
        "        mask_lo = (self.alpha_star_ > tol) & (self.alpha_star_ < self.C - tol)\n",
        "\n",
        "        b_vals = []\n",
        "        for i in np.where(mask_up)[0]:\n",
        "            b_vals.append(y[i] - self.epsilon - f_no_b[i])\n",
        "        for i in np.where(mask_lo)[0]:\n",
        "            b_vals.append(y[i] + self.epsilon - f_no_b[i])\n",
        "\n",
        "        self.b_ = float(np.mean(b_vals)) if b_vals else float(np.mean(y - f_no_b))\n",
        "        self.support_ = np.where(np.abs(self.beta_) > tol)[0]\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        K = self._kernel_matrix(self.X_, X)\n",
        "        return self.beta_ @ K + self.b_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A 1D regression dataset\n",
        "n = 60\n",
        "Xr = np.sort(rng.uniform(-3.0, 3.0, size=n)).reshape(-1, 1)\n",
        "yr = np.sin(Xr[:, 0]) + 0.15 * rng.normal(size=n)\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "Xrs = x_scaler.fit_transform(Xr)\n",
        "\n",
        "x_grid = np.linspace(Xr.min() - 0.5, Xr.max() + 0.5, 400).reshape(-1, 1)\n",
        "x_grid_s = x_scaler.transform(x_grid)\n",
        "\n",
        "C = 6.0\n",
        "epsilon = 0.12\n",
        "gamma = _gamma_scale(Xrs)\n",
        "\n",
        "svr_qp = KernelSVRQP(C=C, epsilon=epsilon, kernel=\"rbf\", gamma=gamma)\n",
        "svr_qp.fit(Xrs, yr)\n",
        "\n",
        "svr_sk = SVR(C=C, epsilon=epsilon, kernel=\"rbf\", gamma=gamma)\n",
        "svr_sk.fit(Xrs, yr)\n",
        "\n",
        "pred_qp = svr_qp.predict(x_grid_s)\n",
        "pred_sk = svr_sk.predict(x_grid_s)\n",
        "\n",
        "train_pred_qp = svr_qp.predict(Xrs)\n",
        "train_pred_sk = svr_sk.predict(Xrs)\n",
        "\n",
        "print(\"scratch SVR: MSE=\", mean_squared_error(yr, train_pred_qp), \"R2=\", r2_score(yr, train_pred_qp))\n",
        "print(\"sklearn  SVR: MSE=\", mean_squared_error(yr, train_pred_sk), \"R2=\", r2_score(yr, train_pred_sk))\n",
        "print(\"scratch support vectors:\", len(svr_qp.support_))\n",
        "print(\"sklearn support vectors:\", len(svr_sk.support_))\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=Xr[:, 0], y=yr, mode=\"markers\", name=\"data\"))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=x_grid[:, 0], y=pred_qp, mode=\"lines\", name=\"scratch SVR (QP)\"))\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=x_grid[:, 0],\n",
        "        y=pred_qp + epsilon,\n",
        "        mode=\"lines\",\n",
        "        line=dict(dash=\"dash\"),\n",
        "        name=\"+epsilon\",\n",
        "    )\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=x_grid[:, 0],\n",
        "        y=pred_qp - epsilon,\n",
        "        mode=\"lines\",\n",
        "        line=dict(dash=\"dash\"),\n",
        "        name=\"-epsilon\",\n",
        "    )\n",
        ")\n",
        "\n",
        "sv_idx = svr_qp.support_\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=Xr[sv_idx, 0],\n",
        "        y=yr[sv_idx],\n",
        "        mode=\"markers\",\n",
        "        marker=dict(size=12, symbol=\"circle-open\", color=\"black\", line=dict(width=2)),\n",
        "        name=\"support vectors (scratch)\",\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"SVR intuition: epsilon-tube and support vectors\",\n",
        "    xaxis_title=\"x\",\n",
        "    yaxis_title=\"y\",\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Epsilon controls tube width; gamma controls smoothness (for RBF).\n",
        "\n",
        "epsilons = [0.05, 0.2]\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=[f\"epsilon={e}\" for e in epsilons])\n",
        "\n",
        "for col, e in enumerate(epsilons, start=1):\n",
        "    m = SVR(C=6.0, epsilon=e, kernel=\"rbf\", gamma=gamma)\n",
        "    m.fit(Xrs, yr)\n",
        "    pred = m.predict(x_grid_s)\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=Xr[:, 0], y=yr, mode=\"markers\", showlegend=False), row=1, col=col)\n",
        "    fig.add_trace(go.Scatter(x=x_grid[:, 0], y=pred, mode=\"lines\", showlegend=False), row=1, col=col)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=x_grid[:, 0], y=pred + e, mode=\"lines\", line=dict(dash=\"dash\"), showlegend=False),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=x_grid[:, 0], y=pred - e, mode=\"lines\", line=dict(dash=\"dash\"), showlegend=False),\n",
        "        row=1,\n",
        "        col=col,\n",
        "    )\n",
        "\n",
        "fig.update_layout(title=\"Effect of epsilon: wider tube \u2192 fewer support vectors (usually)\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) `scikit-learn` SVMs: practical parameter map\n",
        "\n",
        "### Picking the estimator\n",
        "\n",
        "- `SVC`: kernel SVM classification (powerful, but training scales poorly with very large datasets)\n",
        "- `LinearSVC`: linear SVM classification (faster for large/high-dimensional problems)\n",
        "- `SVR`: kernel SVR regression\n",
        "- `LinearSVR`: linear SVR regression\n",
        "\n",
        "### Key parameters (mental model)\n",
        "\n",
        "- `C` (SVC/SVR): regularization strength\n",
        "  - small `C` \u2192 wider margin / more regularization\n",
        "  - large `C` \u2192 tighter fit to training data\n",
        "- `kernel`:\n",
        "  - `linear` for linear problems or very high-dimensional sparse features\n",
        "  - `rbf` is a strong default for non-linear problems\n",
        "- `gamma` (rbf/poly/sigmoid): locality / smoothness\n",
        "  - in sklearn, default is `gamma='scale'` meaning $1/(d\\,\\mathrm{Var}(X))$\n",
        "- `degree` (poly): polynomial complexity\n",
        "- `coef0` (poly/sigmoid): shifts the kernel\n",
        "- `epsilon` (SVR): width of the \u03b5-tube\n",
        "- `class_weight` (SVC): handle class imbalance\n",
        "- `probability=True` (SVC): enables probabilities via extra calibration work (slower)\n",
        "\n",
        "### Always scale your features\n",
        "\n",
        "SVMs are very sensitive to feature scales. Use `StandardScaler` (or domain-appropriate scaling) inside a pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Canonical patterns\n",
        "\n",
        "svc = make_pipeline(StandardScaler(), SVC(C=1.0, kernel=\"rbf\", gamma=\"scale\"))\n",
        "svr = make_pipeline(StandardScaler(), SVR(C=5.0, kernel=\"rbf\", gamma=\"scale\", epsilon=0.1))\n",
        "\n",
        "print(svc)\n",
        "print(svr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. On the circles dataset, sweep over `gamma` (small \u2192 large) and plot how the number of support vectors changes.\n",
        "2. Make a dataset with outliers and compare SVR with different `epsilon` values. Which settings treat the outliers as support vectors?\n",
        "3. Implement a one-vs-rest wrapper around the scratch SVM classifier and test it on a 3-class dataset.\n",
        "4. Explain in your own words how complementary slackness connects to the idea of \u201conly support vectors matter\u201d.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- Cortes, Vapnik (1995): *Support-vector networks*\n",
        "- Smola, Sch\u00f6lkopf (2004): *A tutorial on support vector regression*\n",
        "- Platt (1998): *Sequential Minimal Optimization: A Fast Algorithm for Training SVMs*\n",
        "- `scikit-learn` docs: `sklearn.svm.SVC`, `sklearn.svm.SVR`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
