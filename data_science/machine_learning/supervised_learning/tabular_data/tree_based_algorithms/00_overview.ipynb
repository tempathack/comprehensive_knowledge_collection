{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Algorithms (Decision Trees, Random Forests, Gradient Boosting)\n",
    "\n",
    "Tree models are some of the most practical “workhorse” algorithms in machine learning:\n",
    "\n",
    "- **Decision trees** are interpretable if-else rules learned from data.\n",
    "- **Random forests** reduce variance by averaging many trees.\n",
    "- **Gradient-boosted trees** reduce bias by adding trees sequentially to fix mistakes.\n",
    "\n",
    "A simple way to remember the core idea:\n",
    "\n",
    "> A tree is like playing **20 Questions** with your data.\n",
    ">\n",
    "> Each question splits the world into two parts. You want questions that quickly reduce uncertainty.\n",
    "\n",
    "This notebook builds intuition with visual, Plotly-based demos, then implements simplified versions from scratch in NumPy, and finally connects everything to `scikit-learn` (and the modern boosting libraries).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain how decision trees work for **classification** and **regression**\n",
    "- use and compare **Gini impurity** vs **entropy / information gain**\n",
    "- implement a small **CART-style tree** from scratch (NumPy)\n",
    "- explain and implement **bagging** and a simplified **random forest**\n",
    "- understand what gradient boosting is doing (and how it relates to **XGBoost / LightGBM / CatBoost**)\n",
    "\n",
    "## Notation\n",
    "\n",
    "- Dataset: $(x_i, y_i)$ for $i=1..n$\n",
    "- Feature matrix: $X \\in \\mathbb{R}^{n \\times d}$\n",
    "- Targets:\n",
    "  - classification: $y \\in \\{1,\\dots,K\\}$\n",
    "  - regression: $y \\in \\mathbb{R}$\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Decision trees: intuition\n",
    "2. Impurity measures (Gini, entropy, variance)\n",
    "3. A split in slow motion (Plotly)\n",
    "4. Decision trees from scratch (classification + regression)\n",
    "5. `scikit-learn` trees + key hyperparameters\n",
    "6. Ensembles: bagging and random forests (from scratch + sklearn)\n",
    "7. Gradient boosting and modern libraries (XGBoost / LightGBM / CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Decision trees: intuition\n",
    "\n",
    "A decision tree is a sequence of questions.\n",
    "\n",
    "Example (classification):\n",
    "\n",
    "- “Is `x1 ≤ 0.7`?”\n",
    "  - yes → “Is `x2 ≤ -0.2`?” → class A\n",
    "  - no  → class B\n",
    "\n",
    "Example (regression):\n",
    "\n",
    "- “Is `area ≤ 80 m²`?” → predict \\$220k\n",
    "- else “Is `area ≤ 120 m²`?” → predict \\$310k\n",
    "- else → predict \\$420k\n",
    "\n",
    "A tree doesn’t learn a smooth curve. It learns a set of **regions**, and predicts a **constant** value per region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Impurity measures (how to pick a good question)\n",
    "\n",
    "At any node, the data is a “bag” of samples.\n",
    "\n",
    "A good split makes the children bags **purer**.\n",
    "\n",
    "### Classification\n",
    "\n",
    "Let $p_k$ be the fraction of class $k$ in the node.\n",
    "\n",
    "- **Gini impurity**:\n",
    "\n",
    "$$\n",
    "G = 1 - \\sum_{k=1}^K p_k^2\n",
    "$$\n",
    "\n",
    "- **Entropy**:\n",
    "\n",
    "$$\n",
    "H = -\\sum_{k=1}^K p_k\\log_2(p_k)\n",
    "$$\n",
    "\n",
    "Both measure “messiness”. Both are 0 when the node is perfectly pure.\n",
    "\n",
    "A common split score is **information gain**:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = I(\\text{parent}) - \\frac{n_L}{n}I(L) - \\frac{n_R}{n}I(R)\n",
    "$$\n",
    "\n",
    "where $I$ is either Gini or entropy.\n",
    "\n",
    "### Regression\n",
    "\n",
    "A common impurity is **variance** (equivalently MSE around the mean):\n",
    "\n",
    "$$\n",
    "V = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "Splits are chosen to reduce the weighted variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(y: np.ndarray) -> np.ndarray:\n",
    "    y = np.asarray(y)\n",
    "    if y.size == 0:\n",
    "        return np.array([], dtype=int)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    return counts\n",
    "\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    counts = class_counts(y)\n",
    "    if counts.size == 0:\n",
    "        return 0.0\n",
    "    p = counts / counts.sum()\n",
    "    return float(1.0 - np.sum(p ** 2))\n",
    "\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    counts = class_counts(y)\n",
    "    if counts.size == 0:\n",
    "        return 0.0\n",
    "    p = counts / counts.sum()\n",
    "    p = p[p > 0]\n",
    "    return float(-np.sum(p * np.log2(p)))\n",
    "\n",
    "\n",
    "def variance(y: np.ndarray) -> float:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if y.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.mean((y - y.mean()) ** 2))\n",
    "\n",
    "\n",
    "def information_gain(y: np.ndarray, y_left: np.ndarray, y_right: np.ndarray, impurity_fn) -> float:\n",
    "    n = y.size\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    n_left = y_left.size\n",
    "    n_right = y_right.size\n",
    "    if n_left == 0 or n_right == 0:\n",
    "        return 0.0\n",
    "\n",
    "    parent = impurity_fn(y)\n",
    "    child = (n_left / n) * impurity_fn(y_left) + (n_right / n) * impurity_fn(y_right)\n",
    "    return float(parent - child)\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "print('gini([A,A,B,B]) =', gini_impurity(np.array([0, 0, 1, 1])))\n",
    "print('entropy([A,A,B,B]) =', entropy(np.array([0, 0, 1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) A split in slow motion (1D classification)\n",
    "\n",
    "We’ll create a 1D dataset and evaluate “How good is the split at threshold $t$?”\n",
    "\n",
    "Think of the threshold as a question:\n",
    "\n",
    "> “Is $x \\le t$?”\n",
    "\n",
    "We’ll plot information gain for both Gini and entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D binary classification dataset\n",
    "n = 140\n",
    "x_1d = np.concatenate([\n",
    "    rng.normal(loc=-1.2, scale=0.7, size=n // 2),\n",
    "    rng.normal(loc=1.0, scale=0.8, size=n // 2),\n",
    "])\n",
    "y_1d = np.array([0] * (n // 2) + [1] * (n // 2))\n",
    "\n",
    "# Shuffle\n",
    "perm = rng.permutation(n)\n",
    "x_1d = x_1d[perm]\n",
    "y_1d = y_1d[perm]\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=x_1d,\n",
    "    y=y_1d,\n",
    "    title=\"1D classification dataset (points are classes)\",\n",
    "    labels={\"x\": \"x\", \"y\": \"class\"},\n",
    ")\n",
    "fig.update_yaxes(tickmode=\"array\", tickvals=[0, 1])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate thresholds: midpoints between sorted unique x values\n",
    "x_sorted = np.sort(np.unique(x_1d))\n",
    "thresholds = (x_sorted[:-1] + x_sorted[1:]) / 2.0\n",
    "\n",
    "ig_gini = []\n",
    "ig_entropy = []\n",
    "\n",
    "for t in thresholds:\n",
    "    left = x_1d <= t\n",
    "    y_left = y_1d[left]\n",
    "    y_right = y_1d[~left]\n",
    "\n",
    "    ig_gini.append(information_gain(y_1d, y_left, y_right, gini_impurity))\n",
    "    ig_entropy.append(information_gain(y_1d, y_left, y_right, entropy))\n",
    "\n",
    "ig_gini = np.array(ig_gini)\n",
    "ig_entropy = np.array(ig_entropy)\n",
    "\n",
    "best_t_gini = float(thresholds[np.argmax(ig_gini)])\n",
    "best_t_entropy = float(thresholds[np.argmax(ig_entropy)])\n",
    "\n",
    "best_t_gini, best_t_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=ig_gini, mode=\"lines\", name=\"Information gain (Gini)\"))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=ig_entropy, mode=\"lines\", name=\"Information gain (Entropy)\"))\n",
    "fig.add_vline(x=best_t_gini, line_dash=\"dash\", line_color=\"blue\", annotation_text=\"best (Gini)\")\n",
    "fig.add_vline(x=best_t_entropy, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"best (Entropy)\")\n",
    "fig.update_layout(\n",
    "    title=\"How good is a split? Information gain vs threshold\",\n",
    "    xaxis_title=\"threshold t\",\n",
    "    yaxis_title=\"information gain\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did the “best split” actually do?\n",
    "\n",
    "Let’s look at the before/after impurity numbers at the best Gini threshold.\n",
    "\n",
    "This makes the abstract formula feel more concrete:\n",
    "\n",
    "- parent node: one mixed bag\n",
    "- left/right nodes: two less-mixed bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect impurity before/after the best split (Gini)\n",
    "left = x_1d <= best_t_gini\n",
    "\n",
    "g_parent = gini_impurity(y_1d)\n",
    "g_left = gini_impurity(y_1d[left])\n",
    "g_right = gini_impurity(y_1d[~left])\n",
    "\n",
    "gain = information_gain(y_1d, y_1d[left], y_1d[~left], gini_impurity)\n",
    "\n",
    "counts_parent = np.bincount(y_1d, minlength=2)\n",
    "counts_left = np.bincount(y_1d[left], minlength=2)\n",
    "counts_right = np.bincount(y_1d[~left], minlength=2)\n",
    "\n",
    "print('counts parent:', counts_parent, 'gini:', round(g_parent, 4))\n",
    "print('counts left  :', counts_left, 'gini:', round(g_left, 4))\n",
    "print('counts right :', counts_right, 'gini:', round(g_right, 4))\n",
    "print('information gain:', round(gain, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the split on the 1D data\n",
    "jitter = rng.normal(0, 0.03, size=y_1d.size)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_1d, y=y_1d + jitter, mode='markers', name='data'))\n",
    "fig.add_vline(x=best_t_gini, line_dash='dash', line_color='black', annotation_text='best split')\n",
    "fig.update_layout(\n",
    "    title='The best split threshold on the 1D dataset',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='class (jittered)'\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Decision trees from scratch (NumPy)\n",
    "\n",
    "Below we implement a simplified, educational version of a CART-style tree:\n",
    "\n",
    "- continuous features\n",
    "- binary splits of the form $x_j \\le t$\n",
    "- greedy choice: pick the best (feature, threshold) at each node\n",
    "\n",
    "This is not optimized for speed; it’s meant to be readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Decision tree classifier (Gini or entropy)\n",
    "\n",
    "Stopping rules (common “pre-pruning” controls):\n",
    "\n",
    "- `max_depth`\n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "\n",
    "Random forest also needs `max_features`: only consider a random subset of features at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    feature_index: int | None = None\n",
    "    threshold: float | None = None\n",
    "    left: \"Node | None\" = None\n",
    "    right: \"Node | None\" = None\n",
    "    value: object | None = None  # predicted class (classifier) or mean value (regressor)\n",
    "    proba: np.ndarray | None = None  # class probabilities for classifier\n",
    "\n",
    "\n",
    "def _resolve_max_features(max_features, n_features: int) -> int:\n",
    "    if max_features is None:\n",
    "        return n_features\n",
    "    if isinstance(max_features, int):\n",
    "        return max(1, min(n_features, max_features))\n",
    "    if isinstance(max_features, float):\n",
    "        return max(1, min(n_features, int(np.ceil(max_features * n_features))))\n",
    "    if isinstance(max_features, str):\n",
    "        key = max_features.lower()\n",
    "        if key == \"sqrt\":\n",
    "            return max(1, int(np.sqrt(n_features)))\n",
    "        if key == \"log2\":\n",
    "            return max(1, int(np.log2(n_features)))\n",
    "    raise ValueError(f\"Unsupported max_features={max_features!r}\")\n",
    "\n",
    "\n",
    "class DecisionTreeClassifierScratch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        criterion: str = \"gini\",\n",
    "        max_depth: int | None = None,\n",
    "        min_samples_split: int = 2,\n",
    "        min_samples_leaf: int = 1,\n",
    "        max_features=None,\n",
    "        random_state: int = 0,\n",
    "    ):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.root_: Node | None = None\n",
    "        self.classes_: np.ndarray | None = None\n",
    "        self._rng = np.random.default_rng(random_state)\n",
    "\n",
    "    def _impurity(self, y: np.ndarray) -> float:\n",
    "        if self.criterion == \"gini\":\n",
    "            return gini_impurity(y)\n",
    "        if self.criterion in {\"entropy\", \"log_loss\"}:\n",
    "            return entropy(y)\n",
    "        raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int | None, float | None, float]:\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples < self.min_samples_split:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        parent_impurity = self._impurity(y)\n",
    "        if parent_impurity == 0.0:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        k = _resolve_max_features(self.max_features, n_features)\n",
    "        feature_indices = self._rng.choice(n_features, size=k, replace=False)\n",
    "\n",
    "        best_gain = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            x_col = X[:, feature_index]\n",
    "            uniq = np.unique(x_col)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "\n",
    "            thresholds = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "            for t in thresholds:\n",
    "                left = x_col <= t\n",
    "                n_left = int(left.sum())\n",
    "                n_right = n_samples - n_left\n",
    "\n",
    "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                gain = information_gain(y, y[left], y[~left], self._impurity)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = int(feature_index)\n",
    "                    best_threshold = float(t)\n",
    "\n",
    "        return best_feature, best_threshold, float(best_gain)\n",
    "\n",
    "    def _build(self, X: np.ndarray, y: np.ndarray, depth: int) -> Node:\n",
    "        counts = np.bincount(y, minlength=len(self.classes_))\n",
    "        proba = counts / counts.sum() if counts.sum() else np.zeros_like(counts, dtype=float)\n",
    "        predicted_class = int(np.argmax(counts)) if counts.size else 0\n",
    "\n",
    "        node = Node(value=self.classes_[predicted_class], proba=proba)\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return node\n",
    "        if y.size < self.min_samples_split:\n",
    "            return node\n",
    "        if np.unique(y).size == 1:\n",
    "            return node\n",
    "\n",
    "        feature, threshold, gain = self._best_split(X, y)\n",
    "        if feature is None or threshold is None or gain <= 0.0:\n",
    "            return node\n",
    "\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        node.feature_index = feature\n",
    "        node.threshold = threshold\n",
    "        node.left = self._build(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self._build(X[~left_mask], y[~left_mask], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y_raw = np.asarray(y)\n",
    "        self.classes_, y_enc = np.unique(y_raw, return_inverse=True)\n",
    "        self.root_ = self._build(X, y_enc, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _predict_one(self, x_row: np.ndarray) -> object:\n",
    "        node = self.root_\n",
    "        while node is not None and node.feature_index is not None:\n",
    "            node = node.left if x_row[node.feature_index] <= node.threshold else node.right\n",
    "        return node.value\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return np.array([self._predict_one(row) for row in X])\n",
    "\n",
    "\n",
    "print('Scratch tree ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual demo: scratch tree vs sklearn tree (classification)\n",
    "\n",
    "We’ll use `make_moons` (a classic non-linear dataset), and compare:\n",
    "\n",
    "- a shallow `DecisionTreeClassifierScratch`\n",
    "- `sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "We’ll plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X: np.ndarray, y: np.ndarray, title: str, grid_steps: int = 250):\n",
    "    x_min, x_max = X[:, 0].min() - 0.6, X[:, 0].max() + 0.6\n",
    "    y_min, y_max = X[:, 1].min() - 0.6, X[:, 1].max() + 0.6\n",
    "\n",
    "    xs = np.linspace(x_min, x_max, grid_steps)\n",
    "    ys = np.linspace(y_min, y_max, grid_steps)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    grid = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "    z = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=xs,\n",
    "            y=ys,\n",
    "            z=z,\n",
    "            showscale=False,\n",
    "            opacity=0.35,\n",
    "            contours=dict(showlines=False),\n",
    "            colorscale=[[0, \"#7aa6ff\"], [1, \"#ff8c7a\"]],\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X[:, 0],\n",
    "            y=X[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=6, color=y, colorscale=[[0, \"#1f77b4\"], [1, \"#d62728\"]], line=dict(width=0)),\n",
    "            name=\"data\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(title=title, xaxis_title=\"x1\", yaxis_title=\"x2\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=450, noise=0.25, random_state=42)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_moons, y_moons, test_size=0.25, random_state=42)\n",
    "\n",
    "scratch_tree = DecisionTreeClassifierScratch(max_depth=4, min_samples_leaf=5, criterion=\"gini\", random_state=42)\n",
    "scratch_tree.fit(X_tr, y_tr)\n",
    "\n",
    "sk_tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=5, criterion=\"gini\", random_state=42)\n",
    "sk_tree.fit(X_tr, y_tr)\n",
    "\n",
    "acc_scratch = accuracy_score(y_te, scratch_tree.predict(X_te))\n",
    "acc_sklearn = accuracy_score(y_te, sk_tree.predict(X_te))\n",
    "\n",
    "acc_scratch, acc_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did the tree learn?\n",
    "\n",
    "A nice sanity check is to **print** the learned rules.\n",
    "\n",
    "- `sklearn` provides `export_text(...)`\n",
    "- for our scratch tree, we’ll print a tiny readable representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scratch_tree(node, feature_names=(\"x1\", \"x2\"), indent=\"\"):\n",
    "    if node.feature_index is None:\n",
    "        if node.proba is None:\n",
    "            print(f\"{indent}Predict: {node.value}\")\n",
    "        else:\n",
    "            p = np.array2string(node.proba, precision=2, floatmode='fixed')\n",
    "            print(f\"{indent}Predict: {node.value}  (proba={p})\")\n",
    "        return\n",
    "\n",
    "    name = feature_names[node.feature_index] if feature_names else f\"x[{node.feature_index}]\"\n",
    "    print(f\"{indent}if {name} <= {node.threshold:.3f}:\")\n",
    "    print_scratch_tree(node.left, feature_names=feature_names, indent=indent + \"  \")\n",
    "    print(f\"{indent}else:\")\n",
    "    print_scratch_tree(node.right, feature_names=feature_names, indent=indent + \"  \")\n",
    "\n",
    "\n",
    "print(\"Scratch tree rules:\")\n",
    "print()\n",
    "print_scratch_tree(scratch_tree.root_)\n",
    "\n",
    "print()\n",
    "print(\"---\")\n",
    "print()\n",
    "print(\"sklearn tree rules:\")\n",
    "print()\n",
    "print(export_text(sk_tree, feature_names=[\"x1\", \"x2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plot_decision_boundary(scratch_tree, X_te, y_te, f\"Scratch DecisionTreeClassifier (acc={acc_scratch:.3f})\")\n",
    "fig1.show()\n",
    "\n",
    "fig2 = plot_decision_boundary(sk_tree, X_te, y_te, f\"sklearn DecisionTreeClassifier (acc={acc_sklearn:.3f})\")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decision tree regressor (variance reduction)\n",
    "\n",
    "For regression, the “purity” is about how tightly clustered the target values are.\n",
    "\n",
    "- leaf prediction is usually the **mean** of $y$ in that leaf\n",
    "- the split tries to reduce weighted **variance**\n",
    "\n",
    "A tree regressor produces a **piecewise constant** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressorScratch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        max_depth: int | None = None,\n",
    "        min_samples_split: int = 2,\n",
    "        min_samples_leaf: int = 1,\n",
    "        max_features=None,\n",
    "        random_state: int = 0,\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.root_: Node | None = None\n",
    "        self._rng = np.random.default_rng(random_state)\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int | None, float | None, float]:\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples < self.min_samples_split:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        parent_var = variance(y)\n",
    "        if parent_var == 0.0:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        k = _resolve_max_features(self.max_features, n_features)\n",
    "        feature_indices = self._rng.choice(n_features, size=k, replace=False)\n",
    "\n",
    "        best_reduction = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            x_col = X[:, feature_index]\n",
    "            uniq = np.unique(x_col)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "\n",
    "            thresholds = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "            for t in thresholds:\n",
    "                left = x_col <= t\n",
    "                n_left = int(left.sum())\n",
    "                n_right = n_samples - n_left\n",
    "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                child_var = (n_left / n_samples) * variance(y[left]) + (n_right / n_samples) * variance(y[~left])\n",
    "                reduction = parent_var - child_var\n",
    "\n",
    "                if reduction > best_reduction:\n",
    "                    best_reduction = float(reduction)\n",
    "                    best_feature = int(feature_index)\n",
    "                    best_threshold = float(t)\n",
    "\n",
    "        return best_feature, best_threshold, float(best_reduction)\n",
    "\n",
    "    def _build(self, X: np.ndarray, y: np.ndarray, depth: int) -> Node:\n",
    "        node = Node(value=float(np.mean(y)) if y.size else 0.0)\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return node\n",
    "        if y.size < self.min_samples_split:\n",
    "            return node\n",
    "        if variance(y) == 0.0:\n",
    "            return node\n",
    "\n",
    "        feature, threshold, reduction = self._best_split(X, y)\n",
    "        if feature is None or threshold is None or reduction <= 0.0:\n",
    "            return node\n",
    "\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        node.feature_index = feature\n",
    "        node.threshold = threshold\n",
    "        node.left = self._build(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self._build(X[~left_mask], y[~left_mask], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        self.root_ = self._build(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _predict_one(self, x_row: np.ndarray) -> float:\n",
    "        node = self.root_\n",
    "        while node is not None and node.feature_index is not None:\n",
    "            node = node.left if x_row[node.feature_index] <= node.threshold else node.right\n",
    "        return float(node.value)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return np.array([self._predict_one(row) for row in X], dtype=float)\n",
    "\n",
    "\n",
    "print('Scratch regressor ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D regression dataset: nonlinear signal\n",
    "n = 260\n",
    "x_reg = np.sort(rng.uniform(-3, 3, size=n))\n",
    "y_reg = np.sin(x_reg) + 0.25 * rng.normal(size=n)\n",
    "\n",
    "X_reg = x_reg.reshape(-1, 1)\n",
    "\n",
    "fig = px.scatter(x=x_reg, y=y_reg, title=\"Regression dataset: y = sin(x) + noise\", labels={\"x\": \"x\", \"y\": \"y\"})\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit trees with different depths to show piecewise-constant behavior\n",
    "x_grid = np.linspace(x_reg.min(), x_reg.max(), 600)\n",
    "X_grid = x_grid.reshape(-1, 1)\n",
    "\n",
    "depths = [1, 2, 3, 5, 8]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_reg, y=y_reg, mode=\"markers\", name=\"data\", marker=dict(size=5)))\n",
    "\n",
    "for d in depths:\n",
    "    tree = DecisionTreeRegressorScratch(max_depth=d, min_samples_leaf=6, random_state=42)\n",
    "    tree.fit(X_reg, y_reg)\n",
    "    y_hat = tree.predict(X_grid)\n",
    "    fig.add_trace(go.Scatter(x=x_grid, y=y_hat, mode=\"lines\", name=f\"depth={d}\"))\n",
    "\n",
    "fig.update_layout(title=\"Scratch regression trees: deeper = more flexible\", xaxis_title=\"x\", yaxis_title=\"prediction\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with `sklearn` (regression)\n",
    "\n",
    "The overall shape should look similar: deeper trees fit more wiggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn decision tree regressor on the same 1D regression data\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_reg, y=y_reg, mode=\"markers\", name=\"data\", marker=dict(size=5)))\n",
    "\n",
    "for d in [1, 2, 3, 5, 8]:\n",
    "    m = DecisionTreeRegressor(max_depth=d, min_samples_leaf=6, random_state=42)\n",
    "    m.fit(X_reg, y_reg)\n",
    "    fig.add_trace(go.Scatter(x=x_grid, y=m.predict(X_grid), mode=\"lines\", name=f\"sk depth={d}\"))\n",
    "\n",
    "fig.update_layout(title=\"sklearn regression trees\", xaxis_title=\"x\", yaxis_title=\"prediction\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor (variance reduction in regression)\n",
    "\n",
    "A single regression tree can be very “blocky”. A random forest regressor averages many trees and often produces a smoother approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for the regression task\n",
    "Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(X_reg, y_reg, test_size=0.25, random_state=42)\n",
    "\n",
    "single_reg = DecisionTreeRegressor(max_depth=8, min_samples_leaf=3, random_state=42)\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    max_features=1.0,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "single_reg.fit(Xr_tr, yr_tr)\n",
    "rf_reg.fit(Xr_tr, yr_tr)\n",
    "\n",
    "mse_single = mean_squared_error(yr_te, single_reg.predict(Xr_te))\n",
    "mse_rf = mean_squared_error(yr_te, rf_reg.predict(Xr_te))\n",
    "\n",
    "mse_single, mse_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_reg, y=y_reg, mode=\"markers\", name=\"data\", marker=dict(size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=single_reg.predict(X_grid), mode=\"lines\", name=f\"single tree (MSE={mse_single:.3f})\"))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=rf_reg.predict(X_grid), mode=\"lines\", name=f\"random forest (MSE={mse_rf:.3f})\"))\n",
    "\n",
    "fig.update_layout(title=\"Regression: single tree vs random forest\", xaxis_title=\"x\", yaxis_title=\"prediction\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) `scikit-learn` trees + key hyperparameters\n",
    "\n",
    "`scikit-learn` uses an optimized CART implementation.\n",
    "\n",
    "Common parameters:\n",
    "\n",
    "- `criterion`:\n",
    "  - classifier: `\"gini\"` or `\"entropy\"` (also `\"log_loss\"` in newer versions)\n",
    "  - regressor: `\"squared_error\"` (variance reduction)\n",
    "- `max_depth`: maximum depth (prevents overfitting)\n",
    "- `min_samples_split`, `min_samples_leaf`: prevent overly small leaves\n",
    "- `max_features`: random subset of features per split (key for random forests)\n",
    "- `ccp_alpha`: cost-complexity pruning (post-pruning)\n",
    "\n",
    "Below is a quick look at a trained sklearn tree in text form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_tree_small = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, criterion=\"gini\", random_state=42)\n",
    "sk_tree_small.fit(X_tr, y_tr)\n",
    "\n",
    "print(export_text(sk_tree_small, feature_names=[\"x1\", \"x2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Ensembles: bagging and random forests\n",
    "\n",
    "A single decision tree is a **high-variance** learner:\n",
    "\n",
    "- small data changes can produce a different tree\n",
    "\n",
    "Bagging (“bootstrap aggregating”) reduces variance:\n",
    "\n",
    "1. sample many bootstrap datasets\n",
    "2. train a model on each\n",
    "3. average (regression) or majority vote (classification)\n",
    "\n",
    "Random forests add another trick:\n",
    "\n",
    "- at each split, only consider a random subset of features (`max_features`)\n",
    "\n",
    "This decorrelates the trees, making the average stronger.\n",
    "\n",
    "Analogy:\n",
    "\n",
    "- One expert can be inconsistent.\n",
    "- A panel of experts who *don’t all think the same way* tends to be more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifierScratch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        n_estimators: int = 50,\n",
    "        max_depth: int | None = None,\n",
    "        min_samples_split: int = 2,\n",
    "        min_samples_leaf: int = 1,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state: int = 0,\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.trees_: list[DecisionTreeClassifierScratch] = []\n",
    "        self._rng = np.random.default_rng(random_state)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        self.trees_ = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            idx = self._rng.integers(0, n_samples, size=n_samples)\n",
    "            tree = DecisionTreeClassifierScratch(\n",
    "                criterion=\"gini\",\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features,\n",
    "                random_state=int(self._rng.integers(0, 1_000_000_000)),\n",
    "            )\n",
    "            tree.fit(X[idx], y[idx])\n",
    "            self.trees_.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        preds = np.vstack([tree.predict(X) for tree in self.trees_])\n",
    "\n",
    "        # Majority vote per column\n",
    "        out = []\n",
    "        for j in range(preds.shape[1]):\n",
    "            values, counts = np.unique(preds[:, j], return_counts=True)\n",
    "            out.append(values[int(np.argmax(counts))])\n",
    "        return np.array(out)\n",
    "\n",
    "\n",
    "# Compare: single tree vs scratch forest\n",
    "single = DecisionTreeClassifierScratch(max_depth=6, min_samples_leaf=2, random_state=42)\n",
    "single.fit(X_tr, y_tr)\n",
    "\n",
    "forest = RandomForestClassifierScratch(n_estimators=120, max_depth=8, min_samples_leaf=2, max_features=\"sqrt\", random_state=42)\n",
    "forest.fit(X_tr, y_tr)\n",
    "\n",
    "acc_single = accuracy_score(y_te, single.predict(X_te))\n",
    "acc_forest = accuracy_score(y_te, forest.predict(X_te))\n",
    "\n",
    "acc_single, acc_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_decision_boundary(single, X_te, y_te, f\"Scratch single tree (acc={acc_single:.3f})\")\n",
    "fig.show()\n",
    "\n",
    "fig = plot_decision_boundary(forest, X_te, y_te, f\"Scratch random forest (acc={acc_forest:.3f})\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests in `scikit-learn`\n",
    "\n",
    "Key parameters you’ll actually tune:\n",
    "\n",
    "- `n_estimators`: number of trees\n",
    "- `max_depth`: tree depth\n",
    "- `max_features`: features per split (`\"sqrt\"` is a common default)\n",
    "- `min_samples_leaf`: makes trees less “spiky”\n",
    "- `bootstrap`: whether to bootstrap samples\n",
    "- `oob_score`: out-of-bag estimate (free-ish validation when bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    max_features=\"sqrt\",\n",
    "    min_samples_leaf=2,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    ")\n",
    "rf.fit(X_tr, y_tr)\n",
    "\n",
    "acc_rf = accuracy_score(y_te, rf.predict(X_te))\n",
    "\n",
    "acc_rf, rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances (on this synthetic dataset they're not very meaningful, but the API is)\n",
    "fig = px.bar(x=[\"x1\", \"x2\"], y=rf.feature_importances_, title=\"RandomForest feature_importances_\")\n",
    "fig.update_layout(xaxis_title=\"feature\", yaxis_title=\"importance\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Gradient boosting and modern libraries\n",
    "\n",
    "Bagging/forests mainly reduce **variance**.\n",
    "\n",
    "Boosting mainly reduces **bias**.\n",
    "\n",
    "### The tutoring analogy\n",
    "\n",
    "Imagine a student learning math:\n",
    "\n",
    "- Tutor #1 explains the basics.\n",
    "- Tutor #2 focuses on what the student still gets wrong.\n",
    "- Tutor #3 focuses on the remaining mistakes.\n",
    "\n",
    "Boosting is like that: it builds an additive model\n",
    "\n",
    "$$\n",
    "F_M(x) = \\sum_{m=1}^M \\eta\\, f_m(x)\n",
    "$$\n",
    "\n",
    "where each new tree $f_m$ is trained to reduce the current errors.\n",
    "\n",
    "Modern boosting libraries:\n",
    "\n",
    "- **XGBoost**: strong regularization, second-order optimization, robust defaults, great for tabular\n",
    "- **LightGBM**: histogram + leaf-wise growth, very fast on large data\n",
    "- **CatBoost**: excellent categorical handling (ordered target statistics), strong out-of-the-box performance\n",
    "\n",
    "We’ll demonstrate boosting with `sklearn` and then map the ideas to the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting demo (sklearn HistGradientBoostingClassifier)\n",
    "# Note: this model can work without scaling; we keep features as-is.\n",
    "\n",
    "X_tr2, X_val2, y_tr2, y_val2 = train_test_split(X_tr, y_tr, test_size=0.25, random_state=7)\n",
    "\n",
    "gb = HistGradientBoostingClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=250,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "gb.fit(X_tr2, y_tr2)\n",
    "\n",
    "val_losses = []\n",
    "for proba in gb.staged_predict_proba(X_val2):\n",
    "    val_losses.append(log_loss(y_val2, proba))\n",
    "\n",
    "best_iter = int(np.argmin(val_losses) + 1)\n",
    "\n",
    "fig = px.line(x=np.arange(1, len(val_losses) + 1), y=val_losses, title=f\"Boosting: validation log loss (best_iter={best_iter})\")\n",
    "fig.update_layout(xaxis_title=\"# trees (iterations)\", yaxis_title=\"log loss\")\n",
    "fig.show()\n",
    "\n",
    "best_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost vs LightGBM vs CatBoost (high-level)\n",
    "\n",
    "All three are gradient-boosted decision tree (GBDT) systems, but they differ in *engineering* choices.\n",
    "\n",
    "#### Tree growth strategy\n",
    "\n",
    "- XGBoost (often): **level-wise** growth (more balanced)\n",
    "- LightGBM: typically **leaf-wise** growth (can be faster/more accurate, can overfit if unconstrained)\n",
    "- CatBoost: commonly uses **symmetric / oblivious trees** (same split per depth level)\n",
    "\n",
    "#### Categorical features\n",
    "\n",
    "- CatBoost: built-in strong categorical handling (ordered target statistics)\n",
    "- XGBoost / LightGBM: can handle categoricals in newer versions/modes, but many workflows still rely on encoding\n",
    "\n",
    "#### Practical hyperparameter map (mental model)\n",
    "\n",
    "| Concept | XGBoost | LightGBM | CatBoost |\n",
    "|---|---|---|---|\n",
    "| # trees | `n_estimators` / `num_boost_round` | `num_iterations` | `iterations` |\n",
    "| step size | `learning_rate` (`eta`) | `learning_rate` | `learning_rate` |\n",
    "| tree size | `max_depth`, `min_child_weight` | `num_leaves`, `min_data_in_leaf` | `depth`, `min_data_in_leaf` |\n",
    "| row sampling | `subsample` | `bagging_fraction` | `subsample` |\n",
    "| col sampling | `colsample_bytree` | `feature_fraction` | `rsm` |\n",
    "| L2 reg | `reg_lambda` | `lambda_l2` | `l2_leaf_reg` |\n",
    "| L1 reg | `reg_alpha` | `lambda_l1` | (limited / different) |\n",
    "\n",
    "Rule of thumb:\n",
    "\n",
    "- If you increase tree complexity, counterbalance with stronger regularization and/or smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: check whether the big boosting libs are installed in this environment.\n",
    "# (This notebook still makes sense if they're not installed.)\n",
    "\n",
    "libs = {\n",
    "    \"xgboost\": \"xgboost\",\n",
    "    \"lightgbm\": \"lightgbm\",\n",
    "    \"catboost\": \"catboost\",\n",
    "}\n",
    "\n",
    "for name, mod in libs.items():\n",
    "    try:\n",
    "        m = __import__(mod)\n",
    "        ver = getattr(m, \"__version__\", \"(unknown version)\")\n",
    "        print(f\"{name}: installed ({ver})\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: not installed ({type(e).__name__}: {e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Change `max_depth` of a tree and visualize how the decision boundary changes.\n",
    "2. Increase noise in `make_moons` and compare single tree vs forest.\n",
    "3. For boosting, try `learning_rate=0.05` with more `max_iter` and see the validation curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- ISLR (James, Witten, Hastie, Tibshirani)\n",
    "- ESL (Hastie, Tibshirani, Friedman)\n",
    "- `sklearn` documentation: DecisionTree, RandomForest, HistGradientBoosting\n",
    "- XGBoost / LightGBM / CatBoost official docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}