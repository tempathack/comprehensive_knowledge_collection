{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE: Visualizing High-Dimensional Data (Preserve Friendships, Not Geography)\n",
    "\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) is a **visualization** tool: it tries to place points in 2D/3D so that **nearby points in the original space stay nearby**.\n",
    "\n",
    "A useful mental model:\n",
    "\n",
    "- **People standing close at a party**: you can’t keep everyone’s exact GPS position when you redraw the room, but you *can* keep friend groups clustered.\n",
    "- **Preserving friendships, not geography**: t-SNE cares about who counts as a neighbor, not about the global map scale.\n",
    "\n",
    "This notebook builds intuition first, then shows the probabilistic objective, then runs a small t-SNE optimizer (from scratch) so we can **see** optimization happening.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain t-SNE as “probabilities over neighbors”\n",
    "- interpret **perplexity** as a knob for “how many neighbors matter”\n",
    "- explain why t-SNE uses a **Student-t** distribution in low dimensions (the crowding problem)\n",
    "- recognize strengths/pitfalls (beautiful clusters, dishonest distances)\n",
    "- compare t-SNE vs PCA vs UMAP at a high level\n",
    "\n",
    "## Notation (quick)\n",
    "\n",
    "- High-dimensional points: $x_i \\in \\mathbb{R}^D$\n",
    "- Low-dimensional embedding: $y_i \\in \\mathbb{R}^d$ (usually $d=2$)\n",
    "- Similarities as probabilities: $p_{ij}$ in high-D, $q_{ij}$ in low-D\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Intuition\n",
    "2. Probabilistic explanation\n",
    "3. Algorithm mechanics (perplexity, optimization, early exaggeration)\n",
    "4. Plotly visualizations\n",
    "   - effect of perplexity\n",
    "   - optimization animation (from scratch)\n",
    "   - crowding problem illustration\n",
    "5. Strengths & pitfalls\n",
    "6. Comparison: t-SNE vs UMAP vs PCA\n",
    "7. Exercises\n",
    "8. References\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Intuition: people at a party\n",
    "\n",
    "Imagine each data point is a person, and their features are things like music taste, hobbies, and politics.\n",
    "\n",
    "In the real feature space ($D$ dimensions), you can measure who is “close” to whom — but you **can’t draw it**.\n",
    "\n",
    "t-SNE asks a very specific question:\n",
    "\n",
    "> “For each person, who are their closest friends?”\n",
    "\n",
    "Then it tries to place the people in 2D so that:\n",
    "\n",
    "- close friends stay close (**preserving friendships**)\n",
    "- everyone else is allowed to move around as long as they don’t break those friendships (**not preserving geography**)\n",
    "\n",
    "That’s why t-SNE plots look like convincing clusters: they’re optimized to make neighborhoods look good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used throughout: handwritten digits (64 features per sample)\n",
    "digits = load_digits()\n",
    "\n",
    "X_all = digits.data.astype(float)\n",
    "y_all = digits.target.astype(int)\n",
    "images_all = digits.images\n",
    "\n",
    "# Distance-based methods are sensitive to feature scaling\n",
    "X_all = StandardScaler().fit_transform(X_all)\n",
    "\n",
    "# Keep things fast for visualization\n",
    "n_samples = 900\n",
    "idx = rng.choice(X_all.shape[0], size=n_samples, replace=False)\n",
    "X = X_all[idx]\n",
    "y = y_all[idx]\n",
    "images = images_all[idx]\n",
    "\n",
    "# A common practical trick: PCA before t-SNE (denoise + speed)\n",
    "X_pca = PCA(n_components=50).fit_transform(X)\n",
    "\n",
    "i = int(rng.integers(0, n_samples))\n",
    "fig = px.imshow(\n",
    "    images[i],\n",
    "    color_continuous_scale=\"gray\",\n",
    "    title=f\"One sample from the dataset (label={y[i]})\",\n",
    ")\n",
    "fig.update_layout(coloraxis_showscale=False)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()\n",
    "\n",
    "X.shape, X_pca.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Probabilistic explanation\n",
    "\n",
    "### High-dimensional similarities as probabilities\n",
    "\n",
    "Instead of trying to preserve raw distances, t-SNE turns “closeness” into probabilities.\n",
    "\n",
    "For each point $x_i$, define the probability that $x_j$ is a neighbor:\n",
    "\n",
    "$$\n",
    "p_{j\\mid i} = \\frac{\\exp\\left(-\\|x_i - x_j\\|^2 / (2\\sigma_i^2)\\right)}{\\sum_{k\\neq i} \\exp\\left(-\\|x_i - x_k\\|^2 / (2\\sigma_i^2)\\right)}\n",
    "$$\n",
    "\n",
    "- This is a **Gaussian** kernel around $x_i$.\n",
    "- Each point gets its own bandwidth $\\sigma_i$ (chosen via **perplexity**).\n",
    "\n",
    "Then we symmetrize into a single joint distribution:\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j\\mid i} + p_{i\\mid j}}{2n}\n",
    "$$\n",
    "\n",
    "### Low-dimensional similarities: Gaussian vs Student-t\n",
    "\n",
    "In the embedding space, we also define neighbor probabilities $q_{ij}$.\n",
    "\n",
    "If we used a Gaussian here too, moderately-distant points would get *tiny* probability mass. In 2D that causes the **crowding problem** (everything wants to sit on top of everything else).\n",
    "\n",
    "t-SNE fixes this by using a heavy-tailed **Student-t** distribution with 1 degree of freedom:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{\\left(1 + \\|y_i - y_j\\|^2\\right)^{-1}}{\\sum_{k\\neq l} \\left(1 + \\|y_k - y_l\\|^2\\right)^{-1}}\n",
    "$$\n",
    "\n",
    "### Objective: KL divergence\n",
    "\n",
    "t-SNE chooses the embedding $Y$ by minimizing the mismatch between these two distributions:\n",
    "\n",
    "$$\n",
    "C(Y) = \\mathrm{KL}(P\\,\\|\\,Q) = \\sum_{i\\neq j} p_{ij}\\,\\log\\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "Key intuition about this KL direction:\n",
    "\n",
    "- If two points are neighbors in high-D ($p_{ij}$ is big), but far in 2D ($q_{ij}$ is small), you pay a **big penalty**.\n",
    "- If two points are far in high-D (tiny $p_{ij}$) but close in 2D, the penalty is smaller.\n",
    "\n",
    "This “asymmetry” is part of why t-SNE creates separated, clean-looking islands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Algorithm mechanics\n",
    "\n",
    "### Perplexity (the “friend-circle size”)\n",
    "\n",
    "Perplexity is defined from the entropy of the neighbor distribution:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(P_i) = \\exp\\left( -\\sum_j p_{j\\mid i} \\log p_{j\\mid i} \\right)\n",
    "$$\n",
    "\n",
    "Interpretation: **effective number of neighbors**.\n",
    "\n",
    "- small perplexity (e.g. 5): focus on very local “best friends”\n",
    "- large perplexity (e.g. 50): wider social circle\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "The objective is **non-convex**, so:\n",
    "\n",
    "- different random seeds can give different looking layouts\n",
    "- you can trust local neighborhoods more than global geometry\n",
    "\n",
    "### Early exaggeration\n",
    "\n",
    "During the first phase, t-SNE temporarily multiplies $P$ by a constant (often 12). A story:\n",
    "\n",
    "> “For a while, we pretend friendships are stronger than they really are.”\n",
    "\n",
    "This makes local attraction stronger early on, helping clusters form before the layout settles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_squared_distances(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    sum_sq = np.sum(X**2, axis=1)\n",
    "    D = sum_sq[:, None] + sum_sq[None, :] - 2 * (X @ X.T)\n",
    "    return np.maximum(D, 0.0)\n",
    "\n",
    "\n",
    "def _Hbeta(dist_sq: np.ndarray, beta: float) -> tuple[float, np.ndarray]:\n",
    "    \"\"\"Shannon entropy H and probabilities for a distance vector at precision beta.\"\"\"\n",
    "    P = np.exp(-dist_sq * beta)\n",
    "    sumP = float(np.sum(P))\n",
    "    if sumP == 0.0:\n",
    "        P = np.full_like(P, 1.0 / P.size)\n",
    "        H = float(np.log(P.size))\n",
    "        return H, P\n",
    "\n",
    "    H = float(np.log(sumP) + beta * np.sum(dist_sq * P) / sumP)\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "\n",
    "def conditional_probabilities(\n",
    "    dist_sq_row: np.ndarray,\n",
    "    perplexity: float,\n",
    "    tol: float = 1e-5,\n",
    "    max_iter: int = 60,\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"Binary-search beta so that exp(H) ~= perplexity.\"\"\"\n",
    "    dist_sq_row = np.asarray(dist_sq_row, dtype=float)\n",
    "    logU = float(np.log(perplexity))\n",
    "\n",
    "    beta = 1.0\n",
    "    beta_min = -np.inf\n",
    "    beta_max = np.inf\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        H, P = _Hbeta(dist_sq_row, beta)\n",
    "        Hdiff = H - logU\n",
    "\n",
    "        if abs(Hdiff) < tol:\n",
    "            break\n",
    "\n",
    "        if Hdiff > 0:\n",
    "            beta_min = beta\n",
    "            beta = beta * 2.0 if np.isinf(beta_max) else 0.5 * (beta + beta_max)\n",
    "        else:\n",
    "            beta_max = beta\n",
    "            beta = beta / 2.0 if np.isinf(beta_min) else 0.5 * (beta + beta_min)\n",
    "\n",
    "    return P, float(beta)\n",
    "\n",
    "\n",
    "def perplexity_of(p: np.ndarray) -> float:\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = p[p > 0]\n",
    "    H = -np.sum(p * np.log(p))\n",
    "    return float(np.exp(H))\n",
    "\n",
    "\n",
    "# Demonstration: how perplexity changes the neighbor distribution for ONE point\n",
    "X_demo = X_pca[:400]\n",
    "y_demo = y[:400]\n",
    "\n",
    "D_demo = pairwise_squared_distances(X_demo)\n",
    "\n",
    "i0 = 0\n",
    "mask = np.arange(D_demo.shape[0]) != i0\n",
    "dist_row = D_demo[i0, mask]\n",
    "\n",
    "perplexities = [5, 30, 80]\n",
    "\n",
    "fig = go.Figure()\n",
    "for perp in perplexities:\n",
    "    p_row, beta = conditional_probabilities(dist_row, perplexity=perp)\n",
    "    p_sorted = np.sort(p_row)[::-1]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(1, p_sorted.size + 1),\n",
    "            y=p_sorted,\n",
    "            mode=\"lines\",\n",
    "            name=f\"target={perp} (actual={perplexity_of(p_row):.1f})\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Perplexity as 'friend-circle size' (one point, label={y_demo[i0]})\",\n",
    "    xaxis_title=\"Neighbor rank (sorted)\",\n",
    "    yaxis_title=\"p(j | i)\",\n",
    ")\n",
    "fig.update_yaxes(type=\"log\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Plotly visualizations\n",
    "\n",
    "We’ll focus on three things you can *see*:\n",
    "\n",
    "1. how embeddings change with **perplexity**\n",
    "2. how optimization “pulls neighborhoods together” over time\n",
    "3. what the **crowding problem** is and why Student-t helps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Effect of different perplexities (sklearn t-SNE)\n",
    "perplexities = [5, 30, 50]\n",
    "embeddings = {}\n",
    "\n",
    "for perp in perplexities:\n",
    "    model = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perp,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "        max_iter=750,\n",
    "        random_state=42,\n",
    "    )\n",
    "    embeddings[perp] = model.fit_transform(X_pca)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(perplexities),\n",
    "    subplot_titles=[f\"perplexity={p}\" for p in perplexities],\n",
    ")\n",
    "\n",
    "for col, perp in enumerate(perplexities, start=1):\n",
    "    Z = embeddings[perp]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=Z[:, 0],\n",
    "            y=Z[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=y,\n",
    "                colorscale=\"Turbo\",\n",
    "                size=6,\n",
    "                opacity=0.85,\n",
    "                line=dict(width=0.2, color=\"white\"),\n",
    "                showscale=(col == len(perplexities)),\n",
    "                colorbar=dict(title=\"digit\") if col == len(perplexities) else None,\n",
    "            ),\n",
    "            text=[f\"digit={d}\" for d in y],\n",
    "            hovertemplate=\"%{text}<br>x=%{x:.2f}<br>y=%{y:.2f}<extra></extra>\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"t-SNE 1\", row=1, col=col)\n",
    "    fig.update_yaxes(title_text=\"t-SNE 2\" if col == 1 else \"\", row=1, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE changes with perplexity (same data, different 'friend-circle size')\",\n",
    "    height=370,\n",
    "    width=1100,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Animation of optimization (from scratch)\n",
    "\n",
    "To animate optimization, we’ll run a small **from-scratch** t-SNE loop on a subset. This is not meant to be the fastest implementation; it’s meant to make the mechanics visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_probabilities(\n",
    "    X: np.ndarray,\n",
    "    perplexity: float,\n",
    "    tol: float = 1e-5,\n",
    "    max_iter: int = 60,\n",
    "    eps: float = 1e-12,\n",
    ") -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    D = pairwise_squared_distances(X)\n",
    "    P = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    for i in range(n):\n",
    "        mask = np.arange(n) != i\n",
    "        p_row, _ = conditional_probabilities(D[i, mask], perplexity=perplexity, tol=tol, max_iter=max_iter)\n",
    "        P[i, mask] = p_row\n",
    "\n",
    "    P = (P + P.T) / (2.0 * n)\n",
    "    np.fill_diagonal(P, 0.0)\n",
    "    P = np.maximum(P, eps)\n",
    "    P = P / np.sum(P)\n",
    "    return P\n",
    "\n",
    "\n",
    "def kl_divergence(P: np.ndarray, Q: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    return float(np.sum(P * (np.log(P + eps) - np.log(Q + eps))))\n",
    "\n",
    "\n",
    "def optimize_embedding(\n",
    "    P: np.ndarray,\n",
    "    *,\n",
    "    Y_init: np.ndarray | None = None,\n",
    "    n_iter: int = 350,\n",
    "    learning_rate: float = 200.0,\n",
    "    momentum_early: float = 0.5,\n",
    "    momentum_late: float = 0.8,\n",
    "    momentum_switch_iter: int = 250,\n",
    "    early_exaggeration: float = 12.0,\n",
    "    exaggeration_iters: int = 120,\n",
    "    save_every: int = 10,\n",
    "    kernel: str = \"student_t\",\n",
    "    seed: int = 42,\n",
    "    eps: float = 1e-12,\n",
    ") -> tuple[list[tuple[int, np.ndarray]], np.ndarray]:\n",
    "    \"\"\"Optimize Y with either t-SNE (Student-t) or symmetric SNE (Gaussian).\"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    if kernel not in {\"student_t\", \"gaussian\"}:\n",
    "        raise ValueError(\"kernel must be 'student_t' or 'gaussian'\")\n",
    "\n",
    "    if Y_init is None:\n",
    "        local_rng = np.random.default_rng(seed)\n",
    "        Y = local_rng.normal(0.0, 1e-4, size=(n, 2))\n",
    "    else:\n",
    "        Y = np.asarray(Y_init, dtype=float).copy()\n",
    "\n",
    "    iY = np.zeros_like(Y)\n",
    "    gains = np.ones_like(Y)\n",
    "    min_gain = 0.01\n",
    "\n",
    "    history: list[tuple[int, np.ndarray]] = []\n",
    "    kls = np.zeros(n_iter, dtype=float)\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        P_use = P * (early_exaggeration if it < exaggeration_iters else 1.0)\n",
    "\n",
    "        sum_Y = np.sum(Y**2, axis=1)\n",
    "        D = np.maximum(sum_Y[:, None] + sum_Y[None, :] - 2.0 * (Y @ Y.T), 0.0)\n",
    "\n",
    "        if kernel == \"student_t\":\n",
    "            num = 1.0 / (1.0 + D)\n",
    "        else:\n",
    "            num = np.exp(-D)\n",
    "\n",
    "        np.fill_diagonal(num, 0.0)\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, eps)\n",
    "\n",
    "        if kernel == \"student_t\":\n",
    "            PQ = (P_use - Q) * num\n",
    "        else:\n",
    "            PQ = P_use - Q\n",
    "\n",
    "        dY = 4.0 * (Y * PQ.sum(axis=1)[:, None] - PQ @ Y)\n",
    "\n",
    "        gains = (gains + 0.2) * ((dY > 0) != (iY > 0)) + (gains * 0.8) * ((dY > 0) == (iY > 0))\n",
    "        gains = np.maximum(gains, min_gain)\n",
    "\n",
    "        momentum = momentum_early if it < momentum_switch_iter else momentum_late\n",
    "        iY = momentum * iY - learning_rate * gains * dY\n",
    "        Y = Y + iY\n",
    "        Y = Y - np.mean(Y, axis=0)\n",
    "\n",
    "        kls[it] = kl_divergence(P, Q, eps=eps)\n",
    "\n",
    "        if it % save_every == 0 or it == n_iter - 1:\n",
    "            history.append((it, Y.copy()))\n",
    "\n",
    "    return history, kls\n",
    "\n",
    "\n",
    "n_anim = 320\n",
    "idx_anim = rng.choice(X_pca.shape[0], size=n_anim, replace=False)\n",
    "X_anim = X_pca[idx_anim]\n",
    "y_anim = y[idx_anim]\n",
    "\n",
    "perplexity_anim = 30\n",
    "P_anim = compute_joint_probabilities(X_anim, perplexity=perplexity_anim)\n",
    "\n",
    "Y0 = rng.normal(0.0, 1e-4, size=(n_anim, 2))\n",
    "history_tsne, kl_tsne = optimize_embedding(\n",
    "    P_anim,\n",
    "    Y_init=Y0,\n",
    "    n_iter=360,\n",
    "    save_every=10,\n",
    "    exaggeration_iters=120,\n",
    "    early_exaggeration=12.0,\n",
    "    kernel=\"student_t\",\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    x=np.arange(kl_tsne.size),\n",
    "    y=kl_tsne,\n",
    "    title=\"From-scratch t-SNE: KL divergence over iterations\",\n",
    "    labels={\"x\": \"iteration\", \"y\": \"KL(P || Q)\"},\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=120,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=\"early exaggeration ends\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation: embedding positions as optimization progresses\n",
    "frames = []\n",
    "steps = []\n",
    "\n",
    "marker = dict(\n",
    "    color=y_anim,\n",
    "    colorscale=\"Turbo\",\n",
    "    size=7,\n",
    "    opacity=0.85,\n",
    "    line=dict(width=0.25, color=\"white\"),\n",
    "    showscale=True,\n",
    "    colorbar=dict(title=\"digit\"),\n",
    ")\n",
    "\n",
    "for it, Y in history_tsne:\n",
    "    frames.append(\n",
    "        go.Frame(\n",
    "            data=[\n",
    "                go.Scatter(\n",
    "                    x=Y[:, 0],\n",
    "                    y=Y[:, 1],\n",
    "                    mode=\"markers\",\n",
    "                    marker=marker,\n",
    "                    text=[f\"digit={d}\" for d in y_anim],\n",
    "                    hovertemplate=\"%{text}<br>x=%{x:.2f}<br>y=%{y:.2f}<extra></extra>\",\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            ],\n",
    "            name=str(it),\n",
    "            layout=go.Layout(title_text=f\"t-SNE optimization (from scratch) — iteration {it}\"),\n",
    "        )\n",
    "    )\n",
    "    steps.append(\n",
    "        dict(\n",
    "            method=\"animate\",\n",
    "            args=[[str(it)], {\"mode\": \"immediate\", \"frame\": {\"duration\": 300, \"redraw\": True}, \"transition\": {\"duration\": 0}}],\n",
    "            label=str(it),\n",
    "        )\n",
    "    )\n",
    "\n",
    "it0, Y_first = history_tsne[0]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=Y_first[:, 0],\n",
    "            y=Y_first[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=marker,\n",
    "            text=[f\"digit={d}\" for d in y_anim],\n",
    "            hovertemplate=\"%{text}<br>x=%{x:.2f}<br>y=%{y:.2f}<extra></extra>\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    ],\n",
    "    layout=go.Layout(\n",
    "        title=f\"t-SNE optimization (from scratch) — iteration {it0}\",\n",
    "        xaxis=dict(title=\"y1\", zeroline=False),\n",
    "        yaxis=dict(title=\"y2\", zeroline=False, scaleanchor=\"x\", scaleratio=1),\n",
    "        height=520,\n",
    "        width=720,\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type=\"buttons\",\n",
    "                showactive=False,\n",
    "                y=1.05,\n",
    "                x=1.0,\n",
    "                xanchor=\"right\",\n",
    "                buttons=[\n",
    "                    dict(\n",
    "                        label=\"Play\",\n",
    "                        method=\"animate\",\n",
    "                        args=[None, {\"fromcurrent\": True, \"frame\": {\"duration\": 300, \"redraw\": True}, \"transition\": {\"duration\": 0}}],\n",
    "                    ),\n",
    "                    dict(\n",
    "                        label=\"Pause\",\n",
    "                        method=\"animate\",\n",
    "                        args=[[None], {\"mode\": \"immediate\", \"frame\": {\"duration\": 0, \"redraw\": False}, \"transition\": {\"duration\": 0}}],\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        sliders=[\n",
    "            dict(\n",
    "                active=0,\n",
    "                currentvalue={\"prefix\": \"iteration=\"},\n",
    "                pad={\"t\": 50},\n",
    "                steps=steps,\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    "    frames=frames,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Crowding problem illustration\n",
    "\n",
    "Why does t-SNE use Student-t in 2D?\n",
    "\n",
    "A geometric story:\n",
    "\n",
    "- In high dimensions, there’s “room” for many points to be at *moderate* distances.\n",
    "- In 2D, if you try to keep all those moderate distances, points run out of room and get **crowded**.\n",
    "\n",
    "Using a **heavy tail** makes it easier to push moderately-distant points further apart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian vs Student-t tails (shape comparison)\n",
    "d = np.linspace(0, 10, 400)\n",
    "sigma = 1.0\n",
    "\n",
    "gaussian = np.exp(-(d**2) / (2 * sigma**2))\n",
    "student_t = 1.0 / (1.0 + d**2)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Linear scale\", \"Log scale (tails matter)\"])\n",
    "fig.add_trace(go.Scatter(x=d, y=gaussian, mode=\"lines\", name=\"Gaussian\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=d, y=student_t, mode=\"lines\", name=\"Student-t (df=1)\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=d, y=gaussian, mode=\"lines\", showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=d, y=student_t, mode=\"lines\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"distance\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"unnormalized similarity\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"distance\", row=1, col=2)\n",
    "fig.update_yaxes(type=\"log\", title_text=\"unnormalized similarity (log)\", row=1, col=2)\n",
    "fig.update_layout(title=\"Heavy tails give distant points more 'influence' (less crowding)\")\n",
    "fig.show()\n",
    "\n",
    "# Same P, same init, different low-D kernel: symmetric SNE (Gaussian) vs t-SNE (Student-t)\n",
    "history_gauss, kl_gauss = optimize_embedding(\n",
    "    P_anim,\n",
    "    Y_init=Y0,\n",
    "    n_iter=360,\n",
    "    save_every=360,\n",
    "    exaggeration_iters=120,\n",
    "    early_exaggeration=12.0,\n",
    "    kernel=\"gaussian\",\n",
    ")\n",
    "\n",
    "Y_gauss = history_gauss[-1][1]\n",
    "Y_tsne = history_tsne[-1][1]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Gaussian low-D kernel (crowding)\", \"Student-t low-D kernel (t-SNE)\"])\n",
    "for col, Z in enumerate([Y_gauss, Y_tsne], start=1):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=Z[:, 0],\n",
    "            y=Z[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=y_anim, colorscale=\"Turbo\", size=6, opacity=0.85, line=dict(width=0.2, color=\"white\"), showscale=(col == 2), colorbar=dict(title=\"digit\") if col == 2 else None),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"dim 1\", row=1, col=col)\n",
    "    fig.update_yaxes(title_text=\"dim 2\" if col == 1 else \"\", row=1, col=col, scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.update_layout(title=\"Crowding: Gaussian vs Student-t in the low-dimensional space\", height=420, width=1000)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Strengths & pitfalls\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- **Great for local structure**: who is similar to whom.\n",
    "- **Great for exploration**: clusters, subclusters, outliers.\n",
    "- Often reveals nonlinear manifolds that PCA can’t.\n",
    "\n",
    "### Pitfalls (the important ones)\n",
    "\n",
    "- **Clusters look convincing but distances lie**: the spacing *between* clusters is not trustworthy.\n",
    "- **Non-parametric**: standard t-SNE doesn’t learn a function you can apply to new points; you typically rerun it.\n",
    "- **Sensitive knobs**: perplexity, learning rate, early exaggeration, random seed.\n",
    "- **Not a modeling tool**: it’s for visualization/diagnostics, not for predictive performance.\n",
    "\n",
    "A practical checklist:\n",
    "\n",
    "- try multiple seeds and perplexities\n",
    "- don’t interpret global distances\n",
    "- validate clusters with labels/metadata when you can\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Comparison: t-SNE vs UMAP vs PCA\n",
    "\n",
    "| Method | What it tries to preserve | Strengths | Typical pitfalls |\n",
    "|---|---|---|---|\n",
    "| **PCA** | global variance (linear) | fast, stable, interpretable | misses nonlinear structure |\n",
    "| **t-SNE** | local neighborhoods (probabilistic) | excellent cluster visualization | global geometry not meaningful; slow; non-parametric |\n",
    "| **UMAP** | local neighborhoods + more global structure (graph) | fast; often preserves more global structure; can transform new points | hyperparameters matter; can also make “convincing” clusters |\n",
    "\n",
    "Rule of thumb:\n",
    "\n",
    "- Start with **PCA** (baseline, sanity check).\n",
    "- Use **t-SNE** when you want the cleanest view of local neighborhoods.\n",
    "- Use **UMAP** when you want speed + a bit more global structure (and often an easier path to mapping new points).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison on the same dataset\n",
    "Z_pca2 = PCA(n_components=2).fit_transform(X_pca)\n",
    "\n",
    "if \"embeddings\" in globals() and 30 in embeddings:\n",
    "    Z_tsne30 = embeddings[30]\n",
    "else:\n",
    "    Z_tsne30 = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=30,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "        max_iter=750,\n",
    "        random_state=42,\n",
    "    ).fit_transform(X_pca)\n",
    "\n",
    "Z_umap = None\n",
    "try:\n",
    "    import umap\n",
    "\n",
    "    Z_umap = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(X_pca)\n",
    "except Exception as e:\n",
    "    print(\"UMAP is optional. Install with: pip install umap-learn\")\n",
    "    print(f\"Reason: {e}\")\n",
    "\n",
    "embeds = [(\"PCA\", Z_pca2), (\"t-SNE (perp=30)\", Z_tsne30)]\n",
    "if Z_umap is not None:\n",
    "    embeds.append((\"UMAP\", Z_umap))\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(embeds), subplot_titles=[name for name, _ in embeds])\n",
    "for col, (name, Z) in enumerate(embeds, start=1):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=Z[:, 0],\n",
    "            y=Z[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=y,\n",
    "                colorscale=\"Turbo\",\n",
    "                size=6,\n",
    "                opacity=0.85,\n",
    "                line=dict(width=0.2, color=\"white\"),\n",
    "                showscale=(col == len(embeds)),\n",
    "                colorbar=dict(title=\"digit\") if col == len(embeds) else None,\n",
    "            ),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"dim 1\", row=1, col=col)\n",
    "    fig.update_yaxes(title_text=\"dim 2\" if col == 1 else \"\", row=1, col=col)\n",
    "\n",
    "fig.update_layout(title=\"PCA vs t-SNE vs (optional) UMAP on digits\", height=380, width=1100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Run the perplexity sweep with values 2, 10, 100. What changes? What stays stable?\n",
    "2. Pick a single point and look at its nearest neighbors in the original space (by Euclidean distance). Do they stay neighbors in t-SNE?\n",
    "3. Change the random seed and rerun the from-scratch optimizer. Which conclusions remain valid?\n",
    "4. Try initializing with PCA for the from-scratch implementation. Does it converge faster?\n",
    "5. Compare t-SNE and UMAP on a different dataset (e.g. `make_moons`, `make_swiss_roll`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- van der Maaten & Hinton (2008): *Visualizing Data using t-SNE*\n",
    "- van der Maaten (2014): *Accelerating t-SNE using Tree-Based Algorithms* (Barnes-Hut t-SNE)\n",
    "- McInnes et al. (2018): *UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction*\n",
    "- scikit-learn documentation: `sklearn.manifold.TSNE`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}