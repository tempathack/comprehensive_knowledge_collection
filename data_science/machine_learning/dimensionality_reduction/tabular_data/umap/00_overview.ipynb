{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "UMAP is a dimensionality reduction algorithm that builds a **2D/3D map** where *nearby points in high dimensions tend to stay nearby*.\n",
    "It’s commonly used for:\n",
    "\n",
    "- visualization (like t-SNE)\n",
    "- building features for clustering / downstream models\n",
    "- fast, scalable “manifold-ish” embeddings\n",
    "\n",
    "This notebook keeps the style consistent with the supervised notebooks: **intuition first, math second**, with Plotly experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain UMAP with two mental models: **“Stretching a rubber sheet”** and **“Local neighborhoods with global awareness”**\n",
    "- describe the math pipeline: manifold assumption → fuzzy simplicial set → graph → cross-entropy optimization\n",
    "- tune the key hyperparameters: `n_neighbors`, `min_dist`, and `metric`\n",
    "- compare UMAP vs t-SNE vs Isomap (strengths, weaknesses, when to use what)\n",
    "\n",
    "## Prerequisites (light)\n",
    "\n",
    "- nearest neighbors / distance metrics\n",
    "- graphs (weighted edges) + the idea of optimizing a loss\n",
    "\n",
    "## Notation\n",
    "\n",
    "- Data: $X \\in \\mathbb{R}^{n\\times d}$ (rows are samples)\n",
    "- High-dim distance: $d(x_i, x_j)$\n",
    "- Low-dim embedding: $Y \\in \\mathbb{R}^{n\\times m}$ (usually $m=2$)\n",
    "- Neighborhood size: $k$ (UMAP’s `n_neighbors`)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Intuition (rubber sheet + neighborhoods)\n",
    "2. Mathematical foundation\n",
    "3. Hyperparameters explained\n",
    "4. Plotly experiments\n",
    "5. Practical guidance\n",
    "6. Comparison table (UMAP vs t-SNE vs Isomap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import load_digits, make_swiss_roll\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, TSNE, trustworthiness\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "try:\n",
    "    from umap import UMAP\n",
    "except ModuleNotFoundError:\n",
    "    UMAP = None\n",
    "    print(\"Optional dependency missing: `umap-learn` (UMAP).\")\n",
    "    print(\"Install with: pip install umap-learn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Intuition: “Stretching a rubber sheet”\n",
    "\n",
    "UMAP is easiest to remember as a **two-part story**:\n",
    "\n",
    "### Metaphor A: “Stretching a rubber sheet”\n",
    "\n",
    "Imagine the data lie on a **wrinkled surface** embedded in a high-dimensional space. If you could grab that surface and carefully flatten it onto a table, you’d want to:\n",
    "\n",
    "- keep **nearby** points nearby (don’t tear local patches)\n",
    "- allow the sheet to **stretch** where needed (some distortion is inevitable)\n",
    "\n",
    "That’s the manifold idea: the data are high-dimensional, but the *degrees of freedom* are lower.\n",
    "\n",
    "### Metaphor B: “Local neighborhoods with global awareness”\n",
    "\n",
    "UMAP starts by asking, for each point:\n",
    "\n",
    "> “Who are your closest friends (neighbors), and how strongly do you belong together?”\n",
    "\n",
    "Then it places points in 2D so that **all those local friendships can be satisfied at once**. That second step is where the “global awareness” comes from: *the global layout emerges as a compromise between many local constraints*.\n",
    "\n",
    "---\n",
    "\n",
    "A classic manifold toy problem is the **swiss roll**: it’s 3D data that’s really a 2D sheet rolled up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A classic \"manifold\" toy dataset: the swiss roll\n",
    "X_swiss, t_swiss = make_swiss_roll(n_samples=1400, noise=0.06, random_state=42)\n",
    "t_swiss = t_swiss.astype(np.float64)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    specs=[[{\"type\": \"scene\"}, {\"type\": \"xy\"}]],\n",
    "    subplot_titles=(\"Observed data in 3D\", \"Intrinsic coordinates (t vs height)\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=X_swiss[:, 0],\n",
    "        y=X_swiss[:, 1],\n",
    "        z=X_swiss[:, 2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=3,\n",
    "            color=t_swiss,\n",
    "            colorscale=\"Viridis\",\n",
    "            opacity=0.85,\n",
    "            colorbar=dict(title=\"t\"),\n",
    "        ),\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# A \"ground truth\" 2D view: swiss roll is basically (t, height)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=t_swiss,\n",
    "        y=X_swiss[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=3, color=t_swiss, colorscale=\"Viridis\", opacity=0.85, showscale=False),\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(title=\"Manifold intuition: the swiss roll is 2D in disguise\", height=460)\n",
    "fig.update_scenes(xaxis_title=\"x\", yaxis_title=\"y (height)\", zaxis_title=\"z\")\n",
    "fig.update_xaxes(title_text=\"t (roll position)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"y (height)\", row=1, col=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mathematical foundation (intuition → math)\n",
    "\n",
    "UMAP’s full paper is deep, but the “working mental model” can be built from four pieces.\n",
    "\n",
    "### 2.1 Manifold assumption\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "- The data live on (or near) a low-dimensional manifold.\n",
    "- **Locally**, the manifold looks approximately flat, so nearest neighbors are meaningful.\n",
    "- A good embedding stitches many local patches together.\n",
    "\n",
    "### 2.2 Fuzzy simplicial sets\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "Instead of saying “edge or no edge” between points, UMAP uses **fuzzy membership strengths** in $[0,1]$.\n",
    "\n",
    "- “1” means “very confidently connected”\n",
    "- “0” means “not connected”\n",
    "\n",
    "**Math (core idea):**\n",
    "\n",
    "UMAP builds a $k$-nearest-neighbor graph and assigns a directed membership from $i$ to neighbor $j$:\n",
    "\n",
    "$$\\mu_{i\\mid j} = \\exp\\left(-\\frac{\\max(0,\\; d(x_i, x_j) - \\rho_i)}{\\sigma_i}\\right)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\rho_i$ is a small local “connectivity” offset (often near the distance to the closest neighbor)\n",
    "- $\\sigma_i$ is a *local scale* chosen so that each point has a comparable neighborhood “mass”\n",
    "\n",
    "Then it symmetrizes directed memberships with a fuzzy-OR:\n",
    "\n",
    "$$\\mu_{ij} = \\mu_{i\\mid j} + \\mu_{j\\mid i} - \\mu_{i\\mid j}\\mu_{j\\mid i}$$\n",
    "\n",
    "### 2.3 Graph construction\n",
    "\n",
    "After symmetrization, you can think of UMAP as building a **weighted graph** whose edge weights are $\\mu_{ij}$.\n",
    "That graph is the data’s “fuzzy topology” in a form we can optimize.\n",
    "\n",
    "### 2.4 Cross-entropy optimization\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "We want a low-dimensional layout where:\n",
    "\n",
    "- edges with high $\\mu_{ij}$ become **close**\n",
    "- pairs with low $\\mu_{ij}$ are encouraged to be **far**\n",
    "\n",
    "**Math (core objective):**\n",
    "\n",
    "UMAP defines a low-dimensional similarity (a smooth “attraction curve”), often written as:\n",
    "\n",
    "$$\\nu_{ij} = \\frac{1}{1 + a\\,\\lVert y_i - y_j \\rVert^{2b}}$$\n",
    "\n",
    "and minimizes a cross-entropy between $\\mu$ (high-dim graph) and $\\nu$ (low-dim layout):\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{i<j} \\Big[\\mu_{ij}\\log(\\nu_{ij}) + (1-\\mu_{ij})\\log(1-\\nu_{ij})\\Big]$$\n",
    "\n",
    "In practice, UMAP uses **negative sampling**: it doesn’t sum over all non-edges explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small, simplified demo of the \"fuzzy edge\" idea (not a full UMAP implementation).\n",
    "# The goal is to visualize what “fuzzy membership strengths” look like.\n",
    "\n",
    "\n",
    "def _binary_search_sigma(distances: np.ndarray, rho: float, target: float, n_iters: int = 64) -> float:\n",
    "    lo, hi = 1e-4, 1000.0\n",
    "    for _ in range(n_iters):\n",
    "        mid = (lo + hi) / 2.0\n",
    "        weights = np.exp(-(np.maximum(0.0, distances - rho)) / max(mid, 1e-12))\n",
    "        s = float(weights.sum())\n",
    "        if s > target:\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "def simplified_fuzzy_graph(\n",
    "    X: np.ndarray,\n",
    "    n_neighbors: int = 15,\n",
    "    metric: str = \"euclidean\",\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors + 1, metric=metric).fit(X)\n",
    "    dists, idx = nn.kneighbors(X)\n",
    "    dists = dists[:, 1:]\n",
    "    idx = idx[:, 1:]\n",
    "\n",
    "    rho = dists[:, 0].copy()\n",
    "    target = float(np.log2(n_neighbors))\n",
    "    sigmas = np.array([_binary_search_sigma(dists[i], float(rho[i]), target) for i in range(X.shape[0])])\n",
    "    sigmas = np.maximum(sigmas, 1e-12)\n",
    "\n",
    "    directed = np.exp(-(np.maximum(0.0, dists - rho[:, None])) / sigmas[:, None])\n",
    "\n",
    "    # Dense adjacency for demo-sized X\n",
    "    n = X.shape[0]\n",
    "    A = np.zeros((n, n), dtype=np.float64)\n",
    "    rows = np.repeat(np.arange(n), n_neighbors)\n",
    "    cols = idx.ravel()\n",
    "    A[rows, cols] = directed.ravel()\n",
    "\n",
    "    sym = A + A.T - A * A.T\n",
    "    return sym, idx, dists, rho, sigmas\n",
    "\n",
    "\n",
    "# Run the demo on a small subset of the swiss roll\n",
    "demo_idx = rng.choice(np.arange(X_swiss.shape[0]), size=240, replace=False)\n",
    "X_demo = X_swiss[demo_idx]\n",
    "\n",
    "W, knn_idx, knn_dists, rho, sigma = simplified_fuzzy_graph(X_demo, n_neighbors=15)\n",
    "\n",
    "i = 0\n",
    "weights_i = np.exp(-(np.maximum(0.0, knn_dists[i] - rho[i])) / sigma[i])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=knn_dists[i], y=weights_i, mode=\"markers+lines\"))\n",
    "fig.update_layout(\n",
    "    title=\"Fuzzy edge strengths: distance → membership (one point)\",\n",
    "    xaxis_title=\"distance to neighbor\",\n",
    "    yaxis_title=\"membership strength\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.imshow(\n",
    "    W,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    aspect=\"auto\",\n",
    "    title=\"(Demo) Symmetrized fuzzy graph weights μ_ij\",\n",
    ")\n",
    "fig.update_layout(height=440)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Hyperparameters explained (the ones that matter)\n",
    "\n",
    "### `n_neighbors`\n",
    "\n",
    "**Intuition:** how many “friends” each point tries to keep close.\n",
    "\n",
    "- smaller values (e.g., 5–15) emphasize **very local** structure (tight clusters, fine details)\n",
    "- larger values (e.g., 30–100+) encourage more **global coherence** (how clusters relate to each other)\n",
    "\n",
    "If you’re getting a “spidery” map with lots of tiny islands, try **increasing** `n_neighbors`.\n",
    "\n",
    "### `min_dist`\n",
    "\n",
    "**Intuition:** how tightly points are allowed to pack together in the embedding.\n",
    "\n",
    "- smaller values (e.g., 0.0–0.1) allow **dense clumps** (clear cluster separation)\n",
    "- larger values (e.g., 0.3–0.8) produce a **more even** spread (less clumping)\n",
    "\n",
    "A good way to remember it: `min_dist` is mostly about **visual density**, not “truth.”\n",
    "\n",
    "### `metric` choice\n",
    "\n",
    "**Intuition:** defines what “nearby” means before UMAP ever starts.\n",
    "\n",
    "- `euclidean`: a solid default for continuous/tabular features (after scaling)\n",
    "- `cosine`: common for text embeddings / high-dimensional sparse-ish vectors\n",
    "- `manhattan`, `correlation`, etc.: sometimes better when features have specific meaning\n",
    "\n",
    "If the metric doesn’t match your notion of similarity, *no dimensionality reduction method can save it*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for hyperparameter/stability experiments: handwritten digits (64D)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Standardize + a bit of PCA to speed non-linear methods\n",
    "X_digits = StandardScaler().fit_transform(X_digits)\n",
    "X_digits = PCA(n_components=30, random_state=42).fit_transform(X_digits)\n",
    "\n",
    "# Keep a moderately sized subset for interactive speed\n",
    "n_digits = 900\n",
    "sub_idx = rng.choice(np.arange(X_digits.shape[0]), size=n_digits, replace=False)\n",
    "X_d = X_digits[sub_idx]\n",
    "y_d = y_digits[sub_idx]\n",
    "\n",
    "\n",
    "def fit_umap(\n",
    "    X: np.ndarray,\n",
    "    n_neighbors: int = 15,\n",
    "    min_dist: float = 0.1,\n",
    "    metric: str = \"euclidean\",\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray | None:\n",
    "    if UMAP is None:\n",
    "        return None\n",
    "    model = UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=int(n_neighbors),\n",
    "        min_dist=float(min_dist),\n",
    "        metric=metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return model.fit_transform(X)\n",
    "\n",
    "\n",
    "def fit_tsne(\n",
    "    X: np.ndarray,\n",
    "    perplexity: float = 30.0,\n",
    "    metric: str = \"euclidean\",\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    return TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=float(perplexity),\n",
    "        metric=metric,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "        max_iter=1000,\n",
    "        random_state=random_state,\n",
    "    ).fit_transform(X)\n",
    "\n",
    "\n",
    "def fit_isomap(X: np.ndarray, n_neighbors: int = 10) -> np.ndarray:\n",
    "    return Isomap(n_neighbors=int(n_neighbors), n_components=2).fit_transform(X)\n",
    "\n",
    "\n",
    "def _digit_colorbar() -> dict:\n",
    "    return dict(\n",
    "        title=\"digit\",\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(range(10)),\n",
    "        ticktext=[str(i) for i in range(10)],\n",
    "    )\n",
    "\n",
    "\n",
    "def scatter_digits(emb: np.ndarray, title: str, show_scale: bool = False) -> go.Scatter:\n",
    "    return go.Scatter(\n",
    "        x=emb[:, 0],\n",
    "        y=emb[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            opacity=0.85,\n",
    "            color=y_d,\n",
    "            colorscale=\"Turbo\",\n",
    "            showscale=show_scale,\n",
    "            colorbar=_digit_colorbar() if show_scale else None,\n",
    "            line=dict(color=\"white\", width=0.5),\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        name=title,\n",
    "        hovertemplate=\"x=%{x:.2f}<br>y=%{y:.2f}<br>digit=%{marker.color}<extra></extra>\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Plotly experiments\n",
    "\n",
    "We’ll focus on three visual questions:\n",
    "\n",
    "1. How do `n_neighbors` and `min_dist` change the map?\n",
    "2. How stable are UMAP and t-SNE across random seeds?\n",
    "3. What do we mean by “local vs global” structure preservation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Effect of hyperparameters: n_neighbors × min_dist (digits)\n",
    "\n",
    "if UMAP is None:\n",
    "    print(\"Skipping UMAP hyperparameter sweep (install umap-learn to run this section).\")\n",
    "else:\n",
    "    n_neighbors_grid = [5, 15, 50]\n",
    "    min_dist_grid = [0.0, 0.1, 0.5]\n",
    "\n",
    "    titles = [\n",
    "        f\"k={k}, min_dist={md}\" for md in min_dist_grid for k in n_neighbors_grid\n",
    "    ]\n",
    "    fig = make_subplots(\n",
    "        rows=len(min_dist_grid),\n",
    "        cols=len(n_neighbors_grid),\n",
    "        subplot_titles=titles,\n",
    "        horizontal_spacing=0.03,\n",
    "        vertical_spacing=0.06,\n",
    "    )\n",
    "\n",
    "    for r, md in enumerate(min_dist_grid, start=1):\n",
    "        for c, k in enumerate(n_neighbors_grid, start=1):\n",
    "            emb = fit_umap(X_d, n_neighbors=k, min_dist=md, metric=\"euclidean\", random_state=42)\n",
    "            assert emb is not None\n",
    "            fig.add_trace(scatter_digits(emb, title=f\"k={k}, md={md}\", show_scale=(r == 1 and c == 1)), row=r, col=c)\n",
    "            fig.update_xaxes(showticklabels=False, row=r, col=c)\n",
    "            fig.update_yaxes(showticklabels=False, row=r, col=c)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"UMAP hyperparameters: `n_neighbors` (k) vs `min_dist` (digits)\",\n",
    "        height=760,\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric choice demo: euclidean vs cosine (same k and min_dist)\n",
    "\n",
    "if UMAP is None:\n",
    "    print(\"Skipping UMAP metric comparison (install umap-learn to run this section).\")\n",
    "else:\n",
    "    metrics = [\"euclidean\", \"cosine\"]\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[f\"metric={m}\" for m in metrics])\n",
    "    for col, m in enumerate(metrics, start=1):\n",
    "        emb = fit_umap(X_d, n_neighbors=15, min_dist=0.1, metric=m, random_state=42)\n",
    "        assert emb is not None\n",
    "        fig.add_trace(scatter_digits(emb, title=m, show_scale=(col == 1)), row=1, col=col)\n",
    "        fig.update_xaxes(showticklabels=False, row=1, col=col)\n",
    "        fig.update_yaxes(showticklabels=False, row=1, col=col)\n",
    "\n",
    "    fig.update_layout(title=\"UMAP: metric choice changes what “nearby” means\", height=420)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 UMAP vs t-SNE stability (run-to-run)\n",
    "\n",
    "Both UMAP and t-SNE are **stochastic optimizations**. Two important takeaways:\n",
    "\n",
    "- always set `random_state` when you want reproducibility\n",
    "- don’t over-interpret global rotations/flips (a “good” map can be rotated and still be the same)\n",
    "\n",
    "A simple rotation-invariant sanity check is: **do nearest neighbors in the 2D map stay similar across runs?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_knn_overlap(a: np.ndarray, b: np.ndarray, k: int = 10) -> float:\n",
    "    nn_a = NearestNeighbors(n_neighbors=k + 1).fit(a)\n",
    "    nn_b = NearestNeighbors(n_neighbors=k + 1).fit(b)\n",
    "    na = nn_a.kneighbors(return_distance=False)[:, 1:]\n",
    "    nb = nn_b.kneighbors(return_distance=False)[:, 1:]\n",
    "    overlaps = [len(set(ra) & set(rb)) / k for ra, rb in zip(na, nb)]\n",
    "    return float(np.mean(overlaps))\n",
    "\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "# Compute embeddings\n",
    "umap_embeds = []\n",
    "if UMAP is not None:\n",
    "    umap_embeds = [fit_umap(X_d, n_neighbors=15, min_dist=0.1, metric=\"euclidean\", random_state=s) for s in seeds]\n",
    "    assert all(e is not None for e in umap_embeds)\n",
    "\n",
    "tsne_embeds = [fit_tsne(X_d, perplexity=30.0, metric=\"euclidean\", random_state=s) for s in seeds]\n",
    "\n",
    "# Visual: small multiples\n",
    "rows = 2 if UMAP is not None else 1\n",
    "row_titles = [\"UMAP\" if UMAP is not None else \"t-SNE\", \"t-SNE\"]\n",
    "fig = make_subplots(\n",
    "    rows=rows,\n",
    "    cols=len(seeds),\n",
    "    subplot_titles=[f\"seed={s}\" for s in seeds] * rows,\n",
    "    vertical_spacing=0.08,\n",
    ")\n",
    "\n",
    "if UMAP is not None:\n",
    "    for col, (s, emb) in enumerate(zip(seeds, umap_embeds), start=1):\n",
    "        fig.add_trace(scatter_digits(emb, title=f\"UMAP seed={s}\", show_scale=(col == 1)), row=1, col=col)\n",
    "        fig.update_xaxes(showticklabels=False, row=1, col=col)\n",
    "        fig.update_yaxes(showticklabels=False, row=1, col=col)\n",
    "\n",
    "tsne_row = 2 if UMAP is not None else 1\n",
    "for col, (s, emb) in enumerate(zip(seeds, tsne_embeds), start=1):\n",
    "    fig.add_trace(scatter_digits(emb, title=f\"t-SNE seed={s}\", show_scale=(UMAP is None and col == 1)), row=tsne_row, col=col)\n",
    "    fig.update_xaxes(showticklabels=False, row=tsne_row, col=col)\n",
    "    fig.update_yaxes(showticklabels=False, row=tsne_row, col=col)\n",
    "\n",
    "fig.update_layout(title=\"Run-to-run variability (digits): UMAP vs t-SNE\", height=520 if rows == 2 else 300)\n",
    "fig.show()\n",
    "\n",
    "# Quantify: average overlap of kNN neighborhoods in the embedding\n",
    "rows = []\n",
    "k = 10\n",
    "\n",
    "if UMAP is not None:\n",
    "    base = umap_embeds[0]\n",
    "    for s, emb in zip(seeds, umap_embeds):\n",
    "        rows.append({\"method\": \"UMAP\", \"seed\": s, \"avg_knn_overlap_vs_seed0\": avg_knn_overlap(base, emb, k=k)})\n",
    "\n",
    "base = tsne_embeds[0]\n",
    "for s, emb in zip(seeds, tsne_embeds):\n",
    "    rows.append({\"method\": \"t-SNE\", \"seed\": s, \"avg_knn_overlap_vs_seed0\": avg_knn_overlap(base, emb, k=k)})\n",
    "\n",
    "df_stability = pd.DataFrame(rows)\n",
    "fig = px.bar(\n",
    "    df_stability,\n",
    "    x=\"seed\",\n",
    "    y=\"avg_knn_overlap_vs_seed0\",\n",
    "    color=\"method\",\n",
    "    barmode=\"group\",\n",
    "    title=f\"Stability metric: avg kNN overlap in embedding (k={k}; higher = more stable)\",\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1.0], title=\"avg overlap\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Local vs global structure preservation\n",
    "\n",
    "A useful mental split:\n",
    "\n",
    "- **Local structure**: do you keep the same nearest neighbors?\n",
    "- **Global structure**: do long-range relationships (\"how far apart\") make sense?\n",
    "\n",
    "No 2D map can perfectly preserve all distances, so methods pick tradeoffs.\n",
    "\n",
    "We’ll compare on the swiss roll:\n",
    "\n",
    "- **Isomap**: tries to preserve **geodesic** (on-manifold) distances → strong global unfolding when the manifold is clean.\n",
    "- **t-SNE**: heavily emphasizes local neighborhoods → great cluster visualization, weaker global geometry.\n",
    "- **UMAP**: also neighborhood-based, but often shows *more coherent global structure* than t-SNE (still not a guarantee).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiss roll embeddings + a simple local/global metric comparison\n",
    "\n",
    "X_sr, t_sr = make_swiss_roll(n_samples=1100, noise=0.08, random_state=7)\n",
    "t_sr = t_sr.astype(np.float64)\n",
    "X_sr = StandardScaler().fit_transform(X_sr)\n",
    "\n",
    "embeddings: dict[str, np.ndarray] = {}\n",
    "embeddings[\"Isomap (k=10)\"] = fit_isomap(X_sr, n_neighbors=10)\n",
    "embeddings[\"t-SNE (perp=30)\"] = fit_tsne(X_sr, perplexity=30.0, random_state=42)\n",
    "\n",
    "if UMAP is not None:\n",
    "    embeddings[\"UMAP (k=10)\"] = fit_umap(X_sr, n_neighbors=10, min_dist=0.1, random_state=42)\n",
    "    embeddings[\"UMAP (k=80)\"] = fit_umap(X_sr, n_neighbors=80, min_dist=0.1, random_state=42)\n",
    "\n",
    "\n",
    "def geodesic_spearman(X: np.ndarray, emb: np.ndarray, n_neighbors: int = 10) -> float:\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors + 1).fit(X)\n",
    "    dists, idx = nn.kneighbors(X)\n",
    "    dists = dists[:, 1:]\n",
    "    idx = idx[:, 1:]\n",
    "\n",
    "    n = X.shape[0]\n",
    "    rows = np.repeat(np.arange(n), n_neighbors)\n",
    "    cols = idx.ravel()\n",
    "    data = dists.ravel()\n",
    "\n",
    "    graph = csr_matrix((data, (rows, cols)), shape=(n, n))\n",
    "    graph = graph.maximum(graph.T)\n",
    "    geo = shortest_path(graph, directed=False)\n",
    "\n",
    "    iu = np.triu_indices(n, k=1)\n",
    "    geo_vec = geo[iu]\n",
    "    emb_vec = pdist(emb)\n",
    "    mask = np.isfinite(geo_vec)\n",
    "\n",
    "    corr = spearmanr(geo_vec[mask], emb_vec[mask]).correlation\n",
    "    return float(corr)\n",
    "\n",
    "\n",
    "# Visual comparison\n",
    "names = list(embeddings.keys())\n",
    "cols = 2\n",
    "rows = (len(names) + cols - 1) // cols\n",
    "\n",
    "fig = make_subplots(rows=rows, cols=cols, subplot_titles=names, vertical_spacing=0.10)\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    emb = embeddings[name]\n",
    "    if emb is None:\n",
    "        continue\n",
    "    r, c = i // cols + 1, i % cols + 1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=emb[:, 0],\n",
    "            y=emb[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                opacity=0.85,\n",
    "                color=t_sr,\n",
    "                colorscale=\"Viridis\",\n",
    "                showscale=(i == 0),\n",
    "                colorbar=dict(title=\"t\") if i == 0 else None,\n",
    "            ),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "    fig.update_xaxes(showticklabels=False, row=r, col=c)\n",
    "    fig.update_yaxes(showticklabels=False, row=r, col=c)\n",
    "\n",
    "fig.update_layout(title=\"Swiss roll embeddings (color = intrinsic roll position t)\", height=600)\n",
    "fig.show()\n",
    "\n",
    "# Metrics (computed on a subset to keep it interactive)\n",
    "n_metric = 420\n",
    "metric_idx = rng.choice(np.arange(X_sr.shape[0]), size=n_metric, replace=False)\n",
    "\n",
    "X_m = X_sr[metric_idx]\n",
    "metrics = []\n",
    "for name, emb in embeddings.items():\n",
    "    if emb is None:\n",
    "        continue\n",
    "    emb_m = emb[metric_idx]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"method\": name,\n",
    "            \"trustworthiness@10\": trustworthiness(X_m, emb_m, n_neighbors=10),\n",
    "            \"geodesic_spearman\": geodesic_spearman(X_m, emb_m, n_neighbors=10),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Local: trustworthiness@10 (higher = better)\", \"Global: geodesic Spearman (higher = better)\"),\n",
    ")\n",
    "fig.add_trace(go.Bar(x=df_metrics[\"method\"], y=df_metrics[\"trustworthiness@10\"], name=\"trustworthiness\"), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=df_metrics[\"method\"], y=df_metrics[\"geodesic_spearman\"], name=\"geodesic Spearman\"), row=1, col=2)\n",
    "fig.update_xaxes(tickangle=-25)\n",
    "fig.update_yaxes(range=[0.0, 1.0], row=1, col=1)\n",
    "fig.update_yaxes(range=[-0.1, 1.0], row=1, col=2)\n",
    "fig.update_layout(title=\"Local vs global preservation (swiss roll; subset for metrics)\", height=420, showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Practical guidance\n",
    "\n",
    "### When UMAP often beats t-SNE\n",
    "\n",
    "- you need **speed + scale** (UMAP is typically faster on larger datasets)\n",
    "- you want an **out-of-sample transform** (`UMAP(...).fit(X_train).transform(X_new)`)\n",
    "- you care about a bit more **global coherence** than typical t-SNE visualizations\n",
    "- you want flexible distance **metrics** (e.g., cosine) and reasonable results\n",
    "\n",
    "### When it doesn’t\n",
    "\n",
    "- you want the most “separated-looking” clusters for visualization; t-SNE can be more dramatic\n",
    "- you’re in a regime where the neighborhood graph is unreliable (very noisy data, wrong metric, not scaled)\n",
    "- you need faithful **global distances** (2D can’t do that; consider PCA/MDS for distance-faithful goals)\n",
    "\n",
    "### A simple workflow\n",
    "\n",
    "1. Start with `metric=\"euclidean\"` (after scaling) and `n_neighbors=15`, `min_dist=0.1`.\n",
    "2. If the map is too “fragmented”: increase `n_neighbors`.\n",
    "3. If clusters look too smeared: decrease `min_dist`.\n",
    "4. If your notion of similarity isn’t Euclidean: change `metric` *before* tuning the other knobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Comparison table: UMAP vs t-SNE vs Isomap\n",
    "\n",
    "The best method depends on what you mean by “structure.” Here’s a high-level comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Method\": \"UMAP\",\n",
    "            \"Main idea\": \"Match a fuzzy neighbor graph via cross-entropy\",\n",
    "            \"Strength\": \"Fast, scalable, often decent global layout\",\n",
    "            \"Weakness\": \"Can still distort global distances; sensitive to metric\",\n",
    "            \"Key knobs\": \"n_neighbors, min_dist, metric\",\n",
    "            \"Out-of-sample?\": \"Yes (transform)\",\n",
    "        },\n",
    "        {\n",
    "            \"Method\": \"t-SNE\",\n",
    "            \"Main idea\": \"Match neighbor probabilities (KL divergence)\",\n",
    "            \"Strength\": \"Great local cluster visualization\",\n",
    "            \"Weakness\": \"Global geometry unreliable; typically no native transform\",\n",
    "            \"Key knobs\": \"perplexity, learning_rate, init\",\n",
    "            \"Out-of-sample?\": \"Not standard\",\n",
    "        },\n",
    "        {\n",
    "            \"Method\": \"Isomap\",\n",
    "            \"Main idea\": \"Preserve geodesic distances via shortest paths\",\n",
    "            \"Strength\": \"Can unfold clean manifolds (strong global)\",\n",
    "            \"Weakness\": \"Sensitive to noise/holes; graph must be well-connected\",\n",
    "            \"Key knobs\": \"n_neighbors\",\n",
    "            \"Out-of-sample?\": \"Limited\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=list(comparison.columns),\n",
    "                fill_color=\"#f2f2f2\",\n",
    "                align=\"left\",\n",
    "                font=dict(size=12),\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[comparison[c] for c in comparison.columns],\n",
    "                align=\"left\",\n",
    "                font=dict(size=11),\n",
    "                height=28,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(title=\"UMAP vs t-SNE vs Isomap (high-level)\", height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Pick a dataset you know (tabular, images, embeddings). Try UMAP with:\n",
    "   - `n_neighbors ∈ {5, 15, 50}`\n",
    "   - `min_dist ∈ {0.0, 0.1, 0.5}`\n",
    "   Write down: what changes visually? what do you think changed *conceptually*?\n",
    "2. Measure local structure: compute `trustworthiness(X, Y, n_neighbors=10)` for several settings.\n",
    "3. For swiss-roll-like data, compare Isomap vs UMAP by increasing noise and seeing when Isomap breaks first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- McInnes, Healy, Melville (2018). *UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.*\n",
    "- van der Maaten, Hinton (2008). *Visualizing Data using t-SNE.*\n",
    "- Tenenbaum, de Silva, Langford (2000). *A Global Geometric Framework for Nonlinear Dimensionality Reduction (Isomap).*\n",
    "- scikit-learn docs: `TSNE`, `Isomap`, and `trustworthiness`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
