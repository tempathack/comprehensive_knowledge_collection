{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2ff74e",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) + Kernel PCA\n",
    "\n",
    "PCA is one of the most useful “first moves” in data science:\n",
    "\n",
    "- **compress** data while keeping what varies the most\n",
    "- **denoise** (sometimes) by dropping low-variance directions\n",
    "- **visualize** high-dimensional data in 2D/3D\n",
    "- understand how ideas like **eigenvectors** show up in real ML\n",
    "\n",
    "This notebook mirrors the style of the supervised-learning notebooks: intuition first, then the math, then a from-scratch implementation + visuals.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain PCA in plain language (“rotate the camera to the widest view”)\n",
    "- derive PCA from **variance maximization** and the **covariance matrix**\n",
    "- implement PCA from scratch using NumPy (center → covariance → eigendecompose → project)\n",
    "- understand Kernel PCA and the **kernel trick** at a high level\n",
    "- interpret **explained variance** and know when PCA can mislead you\n",
    "\n",
    "## Notation (quick)\n",
    "\n",
    "- Data matrix: $X \\in \\mathbb{R}^{n\\times d}$ (rows = samples, columns = features)\n",
    "- Centered data: $X_c = X - \\mathbf{1}\\mu^T$ where $\\mu$ is the feature-wise mean\n",
    "- Covariance matrix: $\\Sigma = \\frac{1}{n-1} X_c^T X_c \\in \\mathbb{R}^{d\\times d}$\n",
    "- Principal axes (directions): columns of $V$ (or rows of `components_`)\n",
    "- Scores (coordinates in PC space): $Z = X_c V_k$\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Intuition (very simple)\n",
    "2. Statistical & linear algebra explanation\n",
    "3. Step-by-step algorithm (from scratch)\n",
    "4. Kernel PCA extension (why + how)\n",
    "5. Visual explanations (Plotly)\n",
    "6. When PCA works / fails\n",
    "7. Comparison table (PCA vs Kernel PCA vs ICA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0093e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468f0b4",
   "metadata": {},
   "source": [
    "## 1) Intuition (very simple)\n",
    "\n",
    "Explain PCA like you’re 10:\n",
    "\n",
    "Imagine you have a handful of **dots** on the floor and you want to take a picture that shows the dots as “spread out” as possible.\n",
    "\n",
    "### Metaphor 1: “rotate the camera to see the widest view”\n",
    "\n",
    "- If you point your camera the wrong way, the dots might look **squished**.\n",
    "- If you rotate the camera to the best angle, the dots look **most spread out**.\n",
    "\n",
    "PCA is the math version of:\n",
    "\n",
    "> “Rotate the axes until the data looks widest in one direction.”\n",
    "\n",
    "That “widest direction” is the **first principal component**.\n",
    "\n",
    "### Metaphor 2: “shadow on the wall”\n",
    "\n",
    "Hold a 3D object in front of a lamp. The object casts a **shadow** on the wall.\n",
    "\n",
    "- Different angles give different shadows.\n",
    "- PCA chooses the angle where the shadow is **most spread out**.\n",
    "\n",
    "Then PCA can also choose a second direction (at a right angle) that’s the next most spread out shadow.\n",
    "\n",
    "### What PCA is *not*\n",
    "\n",
    "- It doesn’t find “the best features” in a semantic sense.\n",
    "- It finds **directions of maximum variance** (biggest spread), which is a statistical idea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa5c97",
   "metadata": {},
   "source": [
    "## 2) Statistical & linear algebra explanation\n",
    "\n",
    "### 2.1 Covariance matrix (what PCA is built from)\n",
    "\n",
    "After centering, the covariance matrix is:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X_c^T X_c.\n",
    "$$\n",
    "\n",
    "- Diagonal entries: variances of each feature\n",
    "- Off-diagonal entries: how two features vary **together**\n",
    "\n",
    "PCA looks for directions (unit vectors) $w$ where the projected variance is large.\n",
    "\n",
    "### 2.2 Variance maximization → eigenvectors\n",
    "\n",
    "Project data onto a unit direction $w \\in \\mathbb{R}^d$:\n",
    "\n",
    "$$\n",
    "z = X_c w.\n",
    "$$\n",
    "\n",
    "The variance of $z$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(z) = \\mathrm{Var}(X_c w) = w^T \\Sigma w.\n",
    "$$\n",
    "\n",
    "PCA’s first component solves:\n",
    "\n",
    "$$\n",
    "\\max_{\\|w\\|=1} \\; w^T \\Sigma w.\n",
    "$$\n",
    "\n",
    "Using a Lagrange multiplier (constraint $\\|w\\|^2 = 1$), you get:\n",
    "\n",
    "$$\n",
    "\\Sigma w = \\lambda w.\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "- **eigenvectors** of $\\Sigma$ are candidate directions\n",
    "- **eigenvalues** tell you how much variance is along that direction\n",
    "\n",
    "The largest eigenvalue’s eigenvector is the **first principal component**.\n",
    "\n",
    "### 2.3 Orthogonality (why PCs are at right angles)\n",
    "\n",
    "The covariance matrix $\\Sigma$ is symmetric, so it has an orthonormal eigenbasis:\n",
    "\n",
    "- eigenvectors are **orthogonal** (perpendicular)\n",
    "- we can choose them to have unit length\n",
    "\n",
    "This gives you a clean story:\n",
    "\n",
    "- PC1 captures the most variance\n",
    "- PC2 captures the most remaining variance **subject to being orthogonal to PC1**\n",
    "- and so on\n",
    "\n",
    "### 2.4 Relationship to SVD (another way to compute PCA)\n",
    "\n",
    "Take the SVD of the centered data:\n",
    "\n",
    "$$\n",
    "X_c = U S V^T.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X_c^T X_c\n",
    "= \\frac{1}{n-1} V S^2 V^T.\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "- the columns of $V$ are the PCA directions (principal axes)\n",
    "- the eigenvalues of $\\Sigma$ are $\\lambda_i = \\frac{S_i^2}{n-1}$\n",
    "\n",
    "In practice, SVD is often the numerically stable way to compute PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc97174",
   "metadata": {},
   "source": [
    "## 3) Step-by-step algorithm (from scratch)\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n\\times d}$:\n",
    "\n",
    "1. **Center** the data: $X_c = X - \\mu$\n",
    "2. Compute covariance: $\\Sigma = \\frac{1}{n-1} X_c^T X_c$\n",
    "3. **Eigen-decompose**: $\\Sigma v_i = \\lambda_i v_i$\n",
    "4. Sort eigenvalues descending, keep the top $k$\n",
    "5. **Project**: $Z = X_c V_k$\n",
    "\n",
    "Where $V_k$ is the matrix of the top-$k$ eigenvectors.\n",
    "\n",
    "Below we’ll implement PCA in NumPy and sanity-check it against `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 2D dataset with strong correlation (nice for PCA visuals)\n",
    "n = 350\n",
    "Z = rng.normal(size=(n, 2))\n",
    "\n",
    "A = np.array(\n",
    "    [\n",
    "        [3.0, 1.2],\n",
    "        [0.0, 0.7],\n",
    "    ]\n",
    ")\n",
    "X2 = Z @ A.T\n",
    "X2 = X2 + np.array([2.0, -1.0])\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X2[:, 0],\n",
    "    y=X2[:, 1],\n",
    "    title=\"Correlated 2D data (we'll find the best rotation)\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\"},\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PCAFit:\n",
    "    mean_: np.ndarray\n",
    "    components_: np.ndarray  # shape: (k, d)\n",
    "    explained_variance_: np.ndarray  # shape: (k,)\n",
    "    explained_variance_ratio_: np.ndarray  # shape: (k,)\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        Xc = X - self.mean_\n",
    "        return Xc @ self.components_.T\n",
    "\n",
    "    def inverse_transform(self, Z: np.ndarray) -> np.ndarray:\n",
    "        return Z @ self.components_ + self.mean_\n",
    "\n",
    "\n",
    "def pca_fit(X: np.ndarray, n_components: int | None = None) -> PCAFit:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    mean = X.mean(axis=0)\n",
    "    Xc = X - mean\n",
    "\n",
    "    cov = (Xc.T @ Xc) / (n_samples - 1)\n",
    "\n",
    "    # Symmetric matrix => use eigh (stable, returns ascending eigenvalues)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "\n",
    "    order = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[order]\n",
    "    eigvecs = eigvecs[:, order]\n",
    "\n",
    "    total_var = eigvals.sum()\n",
    "\n",
    "    if n_components is None:\n",
    "        k = n_features\n",
    "    else:\n",
    "        k = int(n_components)\n",
    "\n",
    "    components = eigvecs[:, :k].T\n",
    "    explained_variance = eigvals[:k]\n",
    "    explained_variance_ratio = explained_variance / total_var\n",
    "\n",
    "    return PCAFit(\n",
    "        mean_=mean,\n",
    "        components_=components,\n",
    "        explained_variance_=explained_variance,\n",
    "        explained_variance_ratio_=explained_variance_ratio,\n",
    "    )\n",
    "\n",
    "\n",
    "pca2 = pca_fit(X2, n_components=2)\n",
    "pca2.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_pca2 = PCA(n_components=2)\n",
    "Z_sklearn = sk_pca2.fit_transform(X2)\n",
    "\n",
    "print(\"from scratch explained variance ratio:\", pca2.explained_variance_ratio_)\n",
    "print(\"sklearn explained variance ratio:     \", sk_pca2.explained_variance_ratio_)\n",
    "\n",
    "# Compare directions up to sign (PCA axes can flip sign and still be 'the same')\n",
    "print()\n",
    "print(\"from scratch components_ (rows):\")\n",
    "print(pca2.components_)\n",
    "print()\n",
    "print(\"sklearn components_ (rows):\")\n",
    "print(sk_pca2.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f834b4f",
   "metadata": {},
   "source": [
    "## 4) Kernel PCA extension (non-linear structure)\n",
    "\n",
    "### 4.1 Motivation: when linear PCA is the wrong shape\n",
    "\n",
    "Linear PCA can only:\n",
    "\n",
    "- **rotate** your coordinate system\n",
    "- then optionally drop dimensions\n",
    "\n",
    "So if your data lives on a curved shape (like two moons), linear PCA can’t “unbend” it.\n",
    "\n",
    "Kernel PCA keeps the PCA idea (maximize variance) but does it in a **feature space** where the data may become more linear.\n",
    "\n",
    "### 4.2 Kernel trick intuition\n",
    "\n",
    "PCA needs dot products like $\\langle x, z \\rangle$.\n",
    "\n",
    "Kernel trick idea:\n",
    "\n",
    "- pretend we mapped points into a huge feature space: $\\phi(x)$\n",
    "- but instead of computing $\\phi(x)$ explicitly, we compute:\n",
    "\n",
    "$$\n",
    "k(x, z) = \\langle \\phi(x), \\phi(z) \\rangle.\n",
    "$$\n",
    "\n",
    "Common kernels:\n",
    "\n",
    "- RBF (Gaussian): $k(x,z)=\\exp(-\\gamma\\|x-z\\|^2)$\n",
    "- Polynomial: $k(x,z)=(x^T z + c)^p$\n",
    "\n",
    "### 4.3 Mathematical formulation (high level)\n",
    "\n",
    "Let $\\Phi$ be the centered feature-space data matrix (rows are $\\phi(x_i)$).\n",
    "Kernel PCA avoids forming $\\Phi$ and works with the **kernel matrix**:\n",
    "\n",
    "$$\n",
    "K_{ij} = k(x_i, x_j).\n",
    "$$\n",
    "\n",
    "We must center it (equivalent to centering in feature space):\n",
    "\n",
    "$$\n",
    "K_c = K - \\mathbf{1}K - K\\mathbf{1} + \\mathbf{1}K\\mathbf{1},\n",
    "\\quad \\text{where } \\mathbf{1} = \\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n^T.\n",
    "$$\n",
    "\n",
    "Then we eigendecompose:\n",
    "\n",
    "$$\n",
    "K_c \\alpha = \\lambda \\alpha.\n",
    "$$\n",
    "\n",
    "A point $x$ is projected using kernel similarities to training points:\n",
    "\n",
    "$$\n",
    "z_m(x) = \\sum_{i=1}^n \\alpha_{m,i}\\, k_c(x_i, x).\n",
    "$$\n",
    "\n",
    "(Details vary by normalization convention, but the core idea is: **projection = weighted sum of kernel evaluations**.)\n",
    "\n",
    "### 4.4 Linear PCA vs Kernel PCA (conceptually)\n",
    "\n",
    "- **Linear PCA**: pick a straight line / plane that captures the most spread.\n",
    "- **Kernel PCA**: first “bend” space using a kernel (implicitly), then do PCA there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c205a84",
   "metadata": {},
   "source": [
    "## 5) Visual explanations (Plotly)\n",
    "\n",
    "We’ll build four interactive visuals:\n",
    "\n",
    "1. Original data vs principal axes\n",
    "2. Explained variance bar chart\n",
    "3. 2D → 1D projection animation\n",
    "4. Linear vs Kernel PCA comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Original data + principal axes (PC1, PC2)\n",
    "mu = pca2.mean_\n",
    "pc1, pc2 = pca2.components_[0], pca2.components_[1]\n",
    "\n",
    "# Scale arrows by a few standard deviations along each PC\n",
    "s1 = 3.0 * float(np.sqrt(pca2.explained_variance_[0]))\n",
    "s2 = 3.0 * float(np.sqrt(pca2.explained_variance_[1]))\n",
    "\n",
    "p1a, p1b = mu - s1 * pc1, mu + s1 * pc1\n",
    "p2a, p2b = mu - s2 * pc2, mu + s2 * pc2\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X2[:, 0],\n",
    "        y=X2[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=7, opacity=0.75, line=dict(width=0.5, color=\"white\")),\n",
    "        name=\"data\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=[p1a[0], p1b[0]], y=[p1a[1], p1b[1]], mode=\"lines\", name=\"PC1\"))\n",
    "fig.add_trace(go.Scatter(x=[p2a[0], p2b[0]], y=[p2a[1], p2b[1]], mode=\"lines\", name=\"PC2\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[mu[0]],\n",
    "        y=[mu[1]],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10, symbol=\"x\", color=\"black\"),\n",
    "        name=\"mean\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"PCA on 2D data: principal axes are the best 'camera rotation'\",\n",
    "    xaxis_title=\"x1\",\n",
    "    yaxis_title=\"x2\",\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Explained variance (a slightly higher-dimensional example)\n",
    "\n",
    "n = 600\n",
    "latent = rng.normal(size=(n, 2))\n",
    "latent[:, 0] *= 3.0  # big variance direction\n",
    "latent[:, 1] *= 1.0\n",
    "\n",
    "W = rng.normal(size=(2, 6))\n",
    "X6 = latent @ W + 0.25 * rng.normal(size=(n, 6))\n",
    "\n",
    "pca6 = pca_fit(X6, n_components=6)\n",
    "ratios = pca6.explained_variance_ratio_\n",
    "cum = np.cumsum(ratios)\n",
    "\n",
    "pcs = [f\"PC{i+1}\" for i in range(len(ratios))]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=pcs, y=ratios, name=\"explained variance\"))\n",
    "fig.add_trace(go.Scatter(x=pcs, y=cum, mode=\"lines+markers\", name=\"cumulative\"))\n",
    "fig.update_layout(\n",
    "    title=\"Explained variance ratio\",\n",
    "    yaxis_title=\"fraction of total variance\",\n",
    ")\n",
    "fig.update_yaxes(tickformat=\".0%\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 2D → 1D projection animation (onto PC1)\n",
    "\n",
    "Xc = X2 - pca2.mean_\n",
    "z1 = Xc @ pca2.components_[0]\n",
    "X_proj = pca2.mean_ + np.outer(z1, pca2.components_[0])\n",
    "\n",
    "# A line representing PC1\n",
    "pc1_dir = pca2.components_[0]\n",
    "span = 4.0 * float(np.sqrt(pca2.explained_variance_[0]))\n",
    "line_a, line_b = pca2.mean_ - span * pc1_dir, pca2.mean_ + span * pc1_dir\n",
    "\n",
    "x_min = float(min(X2[:, 0].min(), X_proj[:, 0].min()) - 0.5)\n",
    "x_max = float(max(X2[:, 0].max(), X_proj[:, 0].max()) + 0.5)\n",
    "y_min = float(min(X2[:, 1].min(), X_proj[:, 1].min()) - 0.5)\n",
    "y_max = float(max(X2[:, 1].max(), X_proj[:, 1].max()) + 0.5)\n",
    "\n",
    "ts = np.linspace(0.0, 1.0, 21)\n",
    "frames = []\n",
    "for t in ts:\n",
    "    Xt = (1 - t) * X2 + t * X_proj\n",
    "    frames.append(\n",
    "        go.Frame(\n",
    "            data=[go.Scatter(x=Xt[:, 0], y=Xt[:, 1])],\n",
    "            name=f\"t={t:.2f}\",\n",
    "            traces=[0],\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=X2[:, 0],\n",
    "            y=X2[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=7, opacity=0.75, line=dict(width=0.5, color=\"white\")),\n",
    "            name=\"points\",\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=[line_a[0], line_b[0]],\n",
    "            y=[line_a[1], line_b[1]],\n",
    "            mode=\"lines\",\n",
    "            name=\"PC1 axis\",\n",
    "        ),\n",
    "    ],\n",
    "    frames=frames,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Projecting 2D points down to 1D (onto PC1)\",\n",
    "    xaxis_title=\"x1\",\n",
    "    yaxis_title=\"x2\",\n",
    "    updatemenus=[\n",
    "        {\n",
    "            \"type\": \"buttons\",\n",
    "            \"showactive\": False,\n",
    "            \"buttons\": [\n",
    "                {\n",
    "                    \"label\": \"Play\",\n",
    "                    \"method\": \"animate\",\n",
    "                    \"args\": [None, {\"frame\": {\"duration\": 80, \"redraw\": True}, \"fromcurrent\": True}],\n",
    "                },\n",
    "                {\n",
    "                    \"label\": \"Pause\",\n",
    "                    \"method\": \"animate\",\n",
    "                    \"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    sliders=[\n",
    "        {\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"method\": \"animate\",\n",
    "                    \"args\": [[f\"t={t:.2f}\"], {\"mode\": \"immediate\", \"frame\": {\"duration\": 0, \"redraw\": True}}],\n",
    "                    \"label\": f\"{t:.2f}\",\n",
    "                }\n",
    "                for t in ts\n",
    "            ],\n",
    "            \"currentvalue\": {\"prefix\": \"t = \"},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[x_min, x_max])\n",
    "fig.update_yaxes(range=[y_min, y_max], scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ea3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Linear PCA vs Kernel PCA on a non-linear dataset (two moons)\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.12, random_state=7)\n",
    "Xs = StandardScaler().fit_transform(X_moons)\n",
    "\n",
    "Z_lin = PCA(n_components=2).fit_transform(Xs)\n",
    "Z_k = KernelPCA(n_components=2, kernel=\"rbf\", gamma=10.0).fit_transform(Xs)\n",
    "\n",
    "colors = {0: \"#1f77b4\", 1: \"#d62728\"}\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=[\n",
    "        \"Original space (2D)\",\n",
    "        \"Linear PCA embedding\",\n",
    "        \"Kernel PCA embedding (RBF)\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "for label in [0, 1]:\n",
    "    m = y_moons == label\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=Xs[m, 0],\n",
    "            y=Xs[m, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=6, color=colors[label], opacity=0.75),\n",
    "            name=f\"class {label}\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=Z_lin[m, 0],\n",
    "            y=Z_lin[m, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=6, color=colors[label], opacity=0.75),\n",
    "            name=f\"class {label}\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=Z_k[m, 0],\n",
    "            y=Z_k[m, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=6, color=colors[label], opacity=0.75),\n",
    "            name=f\"class {label}\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=3,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"x1 (scaled)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"x2 (scaled)\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"PC1\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"PC2\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"KPCA1\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"KPCA2\", row=1, col=3)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Linear vs Kernel PCA: kernel can 'unfold' non-linear structure\",\n",
    "    legend_title_text=\"\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e0349",
   "metadata": {},
   "source": [
    "## 6) When PCA works / fails\n",
    "\n",
    "### When PCA tends to work well\n",
    "\n",
    "- You believe the “signal” is in **large-variance directions**.\n",
    "- Features are roughly on the **same scale** (or you standardize).\n",
    "- You want a quick baseline for compression/visualization.\n",
    "\n",
    "### Common failure modes\n",
    "\n",
    "#### Noise sensitivity\n",
    "\n",
    "- PCA is variance-hungry: if noise creates variance, PCA may chase it.\n",
    "- In very noisy settings, PC1 can become “the noisiest direction”, not “the most meaningful direction”.\n",
    "\n",
    "#### Scaling issues\n",
    "\n",
    "- PCA is not unitless — it depends on variance.\n",
    "- If one feature is measured in dollars and another in cents, the dollar feature can dominate.\n",
    "- Fix: standardize (`StandardScaler`) or use a domain-appropriate scaling.\n",
    "\n",
    "#### Linear limitation\n",
    "\n",
    "- PCA can only find **linear** directions.\n",
    "- If the data lies on a curve/manifold (moons, circles), linear PCA won’t separate the structure.\n",
    "- Fix: Kernel PCA (or other non-linear methods like UMAP/t-SNE for visualization).\n",
    "\n",
    "---\n",
    "\n",
    "### Practical checklist\n",
    "\n",
    "- Always center your data.\n",
    "- Decide whether you need scaling:\n",
    "  - yes for mixed units / mixed scales\n",
    "  - sometimes no if scale itself is meaningful\n",
    "- Pick `n_components` using explained variance *and* downstream performance.\n",
    "- Don’t over-interpret PCs as “real factors” without domain validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e9dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick scaling demo: one feature has a much larger numeric scale\n",
    "\n",
    "n = 400\n",
    "x1 = rng.normal(0, 1.0, size=n)\n",
    "# Same underlying signal, but scaled by 50x\n",
    "x2 = 50.0 * (0.8 * x1 + rng.normal(0, 0.4, size=n))\n",
    "X_scale = np.c_[x1, x2]\n",
    "\n",
    "p_unscaled = pca_fit(X_scale, n_components=2)\n",
    "\n",
    "Xs_scaled = StandardScaler().fit_transform(X_scale)\n",
    "p_scaled = pca_fit(Xs_scaled, n_components=2)\n",
    "\n",
    "print(\"Explained variance ratio (unscaled):\", p_unscaled.explained_variance_ratio_)\n",
    "print(\"Explained variance ratio (scaled):  \", p_scaled.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfc40d",
   "metadata": {},
   "source": [
    "## 7) Comparison table: PCA vs Kernel PCA vs ICA\n",
    "\n",
    "| Method | What it tries to do | Linear? | What “components” mean | Typical uses | Key gotchas |\n",
    "|---|---|---:|---|---|---|\n",
    "| **PCA** | Maximize variance with orthogonal directions | ✅ | Orthogonal axes capturing decreasing variance | Compression, denoising, visualization, preprocessing | Sensitive to scaling; can chase noise; linear only |\n",
    "| **Kernel PCA** | PCA in an implicit feature space via a kernel | ❌ (can be non-linear) | Non-linear directions in feature space (via kernel eigenvectors) | Unfold curved structure; non-linear embeddings | Kernel/`gamma` choice matters; scaling still matters; can be expensive |\n",
    "| **ICA** | Separate statistically independent sources | ✅ (typically) | Directions that make components as independent as possible | Blind source separation (signals), artifact removal | More assumptions; components not ordered by variance; sign/scale ambiguities |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Implement PCA via **SVD** and verify it matches the covariance-eigendecomposition approach.\n",
    "2. On the moons dataset, sweep `gamma` in Kernel PCA and observe when the embedding “unfolds” vs when it becomes noisy.\n",
    "3. Create a dataset where the highest variance direction is pure noise — see PCA fail on purpose.\n",
    "\n",
    "## References\n",
    "\n",
    "- ESL (Hastie, Tibshirani, Friedman): *The Elements of Statistical Learning*\n",
    "- Bishop: *Pattern Recognition and Machine Learning*\n",
    "- Schölkopf, Smola, Müller (1998): *Nonlinear Component Analysis as a Kernel Eigenvalue Problem* (Kernel PCA)\n",
    "- `sklearn` docs: `PCA`, `KernelPCA`, `FastICA`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
