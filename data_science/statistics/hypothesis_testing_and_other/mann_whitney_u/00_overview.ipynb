{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann–Whitney U Test (Wilcoxon Rank-Sum) — From Scratch with NumPy\n",
    "\n",
    "The Mann–Whitney U test is a **nonparametric** test for comparing two **independent** samples.\n",
    "Instead of comparing means (like a two-sample t-test), it compares **ranks** — which makes it useful when data is skewed, heavy-tailed, or ordinal.\n",
    "\n",
    "## What you’ll learn\n",
    "- when to use Mann–Whitney U (and when not to)\n",
    "- the hypotheses it tests (what “significant” actually means)\n",
    "- how $U$ is computed (wins + ranks)\n",
    "- a low-level implementation in NumPy (including ties)\n",
    "- how to visualize the null distribution via permutations\n",
    "- how to interpret results + effect sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from math import erf, sqrt\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) When to use it\n",
    "\n",
    "Use Mann–Whitney U when you have:\n",
    "- two **independent** groups (different people, different devices, etc.)\n",
    "- a numeric or at least **ordinal** outcome\n",
    "- a reason to avoid a normality assumption (skew, heavy tails, small samples)\n",
    "\n",
    "Common situations:\n",
    "- A/B testing for skewed metrics (latency, time-on-task)\n",
    "- small samples where “normal enough” is questionable\n",
    "- ordinal ratings (Likert) treated as ordered\n",
    "\n",
    "Not appropriate when:\n",
    "- observations are **paired** / repeated measures (use the *Wilcoxon signed-rank* test instead)\n",
    "- you have **more than two groups** (use *Kruskal–Wallis*, then post-hoc tests)\n",
    "- data are not independent (clustered/longitudinal without modeling that structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) What hypothesis does it test?\n",
    "\n",
    "Let $X$ be a random draw from group A and $Y$ from group B.\n",
    "\n",
    "A convenient way to express the null hypothesis is:\n",
    "\n",
    "$$\n",
    "H_0: \\; P(X > Y) + \\tfrac12 P(X = Y) = 0.5\n",
    "$$\n",
    "\n",
    "So under $H_0$, a value from A is equally likely to be larger than a value from B.\n",
    "\n",
    "Alternatives:\n",
    "- `two-sided`: the distributions differ in either direction\n",
    "- `greater`: A tends to have larger values than B\n",
    "- `less`: A tends to have smaller values than B\n",
    "\n",
    "Important nuance:\n",
    "- Mann–Whitney detects **differences in distributions**.\n",
    "- If the two groups have similar shapes/spreads and mostly differ by a shift, a significant result is often interpreted as a **median shift**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Example data (skewed metric)\n",
    "\n",
    "We’ll simulate an A/B test on a skewed metric (think “response time”).\n",
    "Group B will be slightly smaller (faster) while keeping a similar shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(7)\n",
    "\n",
    "n_a = 25\n",
    "n_b = 28\n",
    "\n",
    "group_a = rng.lognormal(mean=0.10, sigma=0.55, size=n_a)\n",
    "group_b = rng.lognormal(mean=-0.30, sigma=0.55, size=n_b)\n",
    "\n",
    "{\n",
    "    \"n\": (n_a, n_b),\n",
    "    \"mean\": (group_a.mean(), group_b.mean()),\n",
    "    \"median\": (np.median(group_a), np.median(group_b)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        y=group_a,\n",
    "        name=\"A (control)\",\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        points=\"all\",\n",
    "        jitter=0.25,\n",
    "        scalemode=\"width\",\n",
    "        marker=dict(size=6, opacity=0.7, color=\"#636EFA\"),\n",
    "        line=dict(color=\"#636EFA\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        y=group_b,\n",
    "        name=\"B (treatment)\",\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        points=\"all\",\n",
    "        jitter=0.25,\n",
    "        scalemode=\"width\",\n",
    "        marker=dict(size=6, opacity=0.7, color=\"#EF553B\"),\n",
    "        line=dict(color=\"#EF553B\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Skewed metric in two independent groups\",\n",
    "    yaxis_title=\"Response time (arbitrary units)\",\n",
    "    width=760,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Core idea: replace values by ranks\n",
    "\n",
    "The test works on **ranks**:\n",
    "1. Pool the two samples.\n",
    "2. Sort them.\n",
    "3. Assign ranks $1,2,\\ldots,n$.\n",
    "4. Add up the ranks inside each group.\n",
    "\n",
    "If there are ties, we assign the tied values their **average rank**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankdata_average(values):\n",
    "    values = np.asarray(values)\n",
    "    order = np.argsort(values, kind=\"mergesort\")\n",
    "    sorted_vals = values[order]\n",
    "\n",
    "    ranks_sorted = np.empty_like(sorted_vals, dtype=float)\n",
    "    _, first, counts = np.unique(sorted_vals, return_index=True, return_counts=True)\n",
    "    for start, count in zip(first, counts):\n",
    "        end = start + count\n",
    "        avg_rank = 0.5 * ((start + 1) + end)\n",
    "        ranks_sorted[start:end] = avg_rank\n",
    "\n",
    "    ranks = np.empty_like(ranks_sorted)\n",
    "    ranks[order] = ranks_sorted\n",
    "    return ranks\n",
    "\n",
    "\n",
    "demo = np.array([10, 10, 20, 15])\n",
    "rankdata_average(demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) From rank sums to $U$ (the “wins” statistic)\n",
    "\n",
    "Let:\n",
    "- $n_1$ = size of group A\n",
    "- $n_2$ = size of group B\n",
    "- $R_1$ = sum of ranks for group A (after pooling + ranking)\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "U_1 = R_1 - \\frac{n_1(n_1+1)}{2}\n",
    "$$\n",
    "\n",
    "$U_1$ can be interpreted as:\n",
    "- the number of (A,B) pairs where $A > B$\n",
    "- plus half the number of ties\n",
    "\n",
    "And:\n",
    "\n",
    "$$\n",
    "U_2 = n_1 n_2 - U_1\n",
    "$$\n",
    "\n",
    "For a two-sided test, many references report $U = \\min(U_1, U_2)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_from_pairs(x, y):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    gt = (x[:, None] > y[None, :]).sum()\n",
    "    eq = (x[:, None] == y[None, :]).sum()\n",
    "    return gt + 0.5 * eq\n",
    "\n",
    "\n",
    "def mann_whitney_u_from_ranks(x, y):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    if x.size == 0 or y.size == 0:\n",
    "        raise ValueError(\"Both samples must be non-empty\")\n",
    "\n",
    "    n1 = x.size\n",
    "    n2 = y.size\n",
    "\n",
    "    pooled = np.concatenate([x, y])\n",
    "    ranks = rankdata_average(pooled)\n",
    "\n",
    "    R1 = ranks[:n1].sum()\n",
    "    U1 = R1 - n1 * (n1 + 1) / 2\n",
    "    U2 = n1 * n2 - U1\n",
    "    return U1, U2, ranks\n",
    "\n",
    "\n",
    "x_small = np.array([3, 1, 5])\n",
    "y_small = np.array([2, 4, 6])\n",
    "\n",
    "U1_pair = u_from_pairs(x_small, y_small)\n",
    "U1_rank, U2_rank, _ = mann_whitney_u_from_ranks(x_small, y_small)\n",
    "\n",
    "(U1_pair, U1_rank, U2_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, U2, ranks = mann_whitney_u_from_ranks(group_a, group_b)\n",
    "U = min(U1, U2)\n",
    "(U1, U2, U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled = np.concatenate([group_a, group_b])\n",
    "labels = np.array([\"A\"] * len(group_a) + [\"B\"] * len(group_b))\n",
    "ranks = rankdata_average(pooled)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ranks[labels == \"A\"],\n",
    "        y=pooled[labels == \"A\"],\n",
    "        mode=\"markers\",\n",
    "        name=\"A\",\n",
    "        marker=dict(size=8, opacity=0.85, color=\"#636EFA\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ranks[labels == \"B\"],\n",
    "        y=pooled[labels == \"B\"],\n",
    "        mode=\"markers\",\n",
    "        name=\"B\",\n",
    "        marker=dict(size=8, opacity=0.85, color=\"#EF553B\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Pooled ranks: lower values get lower ranks\",\n",
    "    xaxis_title=\"Rank (1 = smallest)\",\n",
    "    yaxis_title=\"Value\",\n",
    "    width=760,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) The p-value: a null distribution for $U$\n",
    "\n",
    "Under $H_0$, the two groups are **exchangeable**: the labels “A” and “B” shouldn’t matter.\n",
    "\n",
    "So we can build a null distribution by:\n",
    "1. pooling the observed values\n",
    "2. randomly re-assigning $n_1$ of them to “A” (the rest to “B”)\n",
    "3. computing $U_1$ each time\n",
    "\n",
    "The p-value is the proportion of permutations with a statistic at least as extreme as what we observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mw_u_permutation_test(x, y, alternative=\"two-sided\", n_permutations=20_000, rng=None):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    if x.size == 0 or y.size == 0:\n",
    "        raise ValueError(\"Both samples must be non-empty\")\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    n1 = x.size\n",
    "    n2 = y.size\n",
    "    n = n1 + n2\n",
    "\n",
    "    pooled = np.concatenate([x, y])\n",
    "    ranks = rankdata_average(pooled)\n",
    "\n",
    "    obs_R1 = ranks[:n1].sum()\n",
    "    obs_U1 = obs_R1 - n1 * (n1 + 1) / 2\n",
    "\n",
    "    keys = rng.random((n_permutations, n))\n",
    "    idx = np.argpartition(keys, kth=n1 - 1, axis=1)[:, :n1]\n",
    "    perm_R1 = ranks[idx].sum(axis=1)\n",
    "    perm_U1 = perm_R1 - n1 * (n1 + 1) / 2\n",
    "\n",
    "    if alternative == \"greater\":\n",
    "        p = (np.sum(perm_U1 >= obs_U1) + 1) / (n_permutations + 1)\n",
    "    elif alternative == \"less\":\n",
    "        p = (np.sum(perm_U1 <= obs_U1) + 1) / (n_permutations + 1)\n",
    "    elif alternative == \"two-sided\":\n",
    "        lo = (np.sum(perm_U1 <= obs_U1) + 1) / (n_permutations + 1)\n",
    "        hi = (np.sum(perm_U1 >= obs_U1) + 1) / (n_permutations + 1)\n",
    "        p = min(1.0, 2 * min(lo, hi))\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "\n",
    "    return obs_U1, perm_U1, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_perm = np.random.default_rng(7)\n",
    "\n",
    "obs_U1, perm_U1, p_perm = mw_u_permutation_test(\n",
    "    group_a,\n",
    "    group_b,\n",
    "    alternative=\"two-sided\",\n",
    "    n_permutations=20_000,\n",
    "    rng=rng_perm,\n",
    ")\n",
    "\n",
    "{\n",
    "    \"U1\": obs_U1,\n",
    "    \"U2\": len(group_a) * len(group_b) - obs_U1,\n",
    "    \"p_value (permutation)\": p_perm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_null = len(group_a) * len(group_b) / 2\n",
    "d = abs(obs_U1 - mu_null)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=perm_U1, nbinsx=40, name=\"permuted U1\", marker=dict(color=\"rgba(99,110,250,0.65)\")))\n",
    "fig.add_vline(\n",
    "    x=obs_U1,\n",
    "    line_width=3,\n",
    "    line_color=\"black\",\n",
    "    annotation_text=f\"observed U1 = {obs_U1:.1f}\",\n",
    "    annotation_position=\"top\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=mu_null,\n",
    "    line_width=2,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"gray\",\n",
    "    annotation_text=\"null mean\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "fig.add_vrect(\n",
    "    x0=min(perm_U1),\n",
    "    x1=mu_null - d,\n",
    "    fillcolor=\"rgba(239,85,59,0.18)\",\n",
    "    line_width=0,\n",
    ")\n",
    "fig.add_vrect(\n",
    "    x0=mu_null + d,\n",
    "    x1=max(perm_U1),\n",
    "    fillcolor=\"rgba(239,85,59,0.18)\",\n",
    "    line_width=0,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Null distribution of U1 under H0 (permutations), p ≈ {p_perm:.4f}\",\n",
    "    xaxis_title=\"U1\",\n",
    "    yaxis_title=\"Count\",\n",
    "    width=760,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Normal approximation (large samples)\n",
    "\n",
    "For larger samples, $U_1$ is well-approximated by a normal distribution.\n",
    "\n",
    "Mean under $H_0$:\n",
    "\n",
    "$$\n",
    "\\mu_U = \\frac{n_1 n_2}{2}\n",
    "$$\n",
    "\n",
    "Variance under $H_0$ (no ties):\n",
    "\n",
    "$$\n",
    "\\sigma_U^2 = \\frac{n_1 n_2 (n_1 + n_2 + 1)}{12}\n",
    "$$\n",
    "\n",
    "With ties, we apply a standard **tie correction** to the variance.\n",
    "\n",
    "Then we compute a z-score (often with a continuity correction) and get a p-value from the normal CDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cdf(z):\n",
    "    return 0.5 * (1 + erf(z / sqrt(2)))\n",
    "\n",
    "\n",
    "def mw_u_normal_approx(x, y, alternative=\"two-sided\", continuity=True):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    if x.size == 0 or y.size == 0:\n",
    "        raise ValueError(\"Both samples must be non-empty\")\n",
    "\n",
    "    n1 = x.size\n",
    "    n2 = y.size\n",
    "    n = n1 + n2\n",
    "\n",
    "    pooled = np.concatenate([x, y])\n",
    "    ranks = rankdata_average(pooled)\n",
    "    R1 = ranks[:n1].sum()\n",
    "    U1 = R1 - n1 * (n1 + 1) / 2\n",
    "\n",
    "    mu = n1 * n2 / 2\n",
    "    _, counts = np.unique(pooled, return_counts=True)\n",
    "    tie_term = np.sum(counts**3 - counts)\n",
    "    var = n1 * n2 / 12 * ((n + 1) - tie_term / (n * (n - 1)))\n",
    "    sigma = sqrt(var)\n",
    "\n",
    "    if sigma == 0:\n",
    "        return U1, 0.0, 1.0\n",
    "\n",
    "    cc = 0.0\n",
    "    if continuity:\n",
    "        if alternative == \"greater\":\n",
    "            cc = -0.5\n",
    "        elif alternative == \"less\":\n",
    "            cc = 0.5\n",
    "        elif alternative == \"two-sided\":\n",
    "            cc = -0.5 if U1 > mu else 0.5 if U1 < mu else 0.0\n",
    "        else:\n",
    "            raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "\n",
    "    z = (U1 - mu + cc) / sigma\n",
    "    cdf = normal_cdf(z)\n",
    "\n",
    "    if alternative == \"greater\":\n",
    "        p = 1 - cdf\n",
    "    elif alternative == \"less\":\n",
    "        p = cdf\n",
    "    else:\n",
    "        p = 2 * min(cdf, 1 - cdf)\n",
    "\n",
    "    return U1, z, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1_norm, z, p_norm = mw_u_normal_approx(group_a, group_b, alternative=\"two-sided\", continuity=True)\n",
    "\n",
    "{\n",
    "    \"U1\": U1_norm,\n",
    "    \"z\": z,\n",
    "    \"p_value (normal approx)\": p_norm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Interpreting the result\n",
    "\n",
    "What a small p-value means:\n",
    "- If $H_0$ were true (no tendency for A to be larger than B), then observing a $U$ at least this extreme would be rare.\n",
    "- So a small p-value is evidence against $H_0$.\n",
    "\n",
    "What it does **not** mean:\n",
    "- It does **not** tell you the probability that $H_0$ is true.\n",
    "- It does **not** tell you how large the difference is.\n",
    "\n",
    "A simple, interpretable effect size is the **probability of superiority**:\n",
    "\n",
    "$$\n",
    "A = P(X > Y) + \\tfrac12 P(X = Y) = \\frac{U_1}{n_1 n_2}\n",
    "$$\n",
    "\n",
    "- $A = 0.5$ suggests no tendency for A to be larger than B.\n",
    "- $A = 0.65$ means a random A observation exceeds a random B observation about 65% of the time (counting ties as half).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = len(group_a)\n",
    "n2 = len(group_b)\n",
    "\n",
    "A = obs_U1 / (n1 * n2)\n",
    "cliffs_delta = 2 * A - 1\n",
    "r_effect = abs(z) / sqrt(n1 + n2)\n",
    "\n",
    "{\n",
    "    \"probability_of_superiority (A)\": A,\n",
    "    \"Cliff's delta (2A-1)\": cliffs_delta,\n",
    "    \"r (|z|/sqrt(n))\": r_effect,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=[A], y=[\"A\"], orientation=\"h\", marker=dict(color=\"#00CC96\")))\n",
    "fig.add_vline(x=0.5, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.add_annotation(x=A, y=0, text=f\"{A:.2f}\", showarrow=False, yshift=14)\n",
    "fig.update_layout(\n",
    "    title=\"Common-language effect size: probability of superiority\",\n",
    "    xaxis=dict(range=[0, 1], title=\"P(A > B) + 0.5·P(tie)\"),\n",
    "    yaxis=dict(showticklabels=False),\n",
    "    width=760,\n",
    "    height=260,\n",
    "    showlegend=False,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) A cautionary example: “same median” does not imply “no difference”\n",
    "\n",
    "Mann–Whitney U is often described as a test for a median shift.\n",
    "That interpretation is most defensible when the two distributions have similar shapes.\n",
    "\n",
    "Here we build two samples with **exactly the same sample median**, but different shapes.\n",
    "It’s still possible for Mann–Whitney U to be significant, because the test is about **relative ordering**, not “median equality”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_shape = np.random.default_rng(7)\n",
    "\n",
    "n = 200\n",
    "x = rng_shape.normal(0, 1, size=n)\n",
    "y = rng_shape.exponential(scale=1.0, size=n) - np.log(2)\n",
    "y = y - np.median(y) + np.median(x)\n",
    "\n",
    "U1_xy, z_xy, p_xy = mw_u_normal_approx(x, y, alternative=\"two-sided\", continuity=True)\n",
    "\n",
    "{\n",
    "    \"sample_median_x\": float(np.median(x)),\n",
    "    \"sample_median_y\": float(np.median(y)),\n",
    "    \"p_value (normal approx)\": p_xy,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        y=x,\n",
    "        name=\"x (Normal)\",\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        points=False,\n",
    "        line=dict(color=\"#636EFA\"),\n",
    "        fillcolor=\"rgba(99,110,250,0.35)\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        y=y,\n",
    "        name=\"y (Shifted Exponential)\",\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        points=False,\n",
    "        line=dict(color=\"#EF553B\"),\n",
    "        fillcolor=\"rgba(239,85,59,0.35)\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Different shapes with the same sample median (p ≈ {p_xy:.4f})\",\n",
    "    yaxis_title=\"Value\",\n",
    "    width=760,\n",
    "    height=450,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Pitfalls + diagnostics\n",
    "\n",
    "- **Independence is crucial**: if measurements are paired/repeated, switch tests.\n",
    "- **Ties**: common in discrete/rounded data. Average ranks + tie-corrected variance help, but consider a permutation test.\n",
    "- **Report an effect size**: a significant p-value can still correspond to a small $A$ (small practical difference).\n",
    "- **Distribution shape matters**: if spreads/skews differ a lot, interpret the result as “distribution difference”, not “median difference”.\n",
    "- **Power**: like any test, Mann–Whitney can miss small effects with small samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Practical usage (SciPy)\n",
    "\n",
    "In real projects, you’ll usually call a library implementation.\n",
    "SciPy’s `mannwhitneyu` returns the U statistic (for the first sample you pass) and a p-value.\n",
    "\n",
    "Below is an optional sanity check against SciPy’s asymptotic (normal-approx) result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from scipy.stats import mannwhitneyu\n",
    "\n",
    "    res = mannwhitneyu(group_a, group_b, alternative=\"two-sided\", method=\"asymptotic\")\n",
    "    {\n",
    "        \"ours_U1\": float(U1_norm),\n",
    "        \"ours_p_value\": float(p_norm),\n",
    "        \"scipy_U1\": float(res.statistic),\n",
    "        \"scipy_p_value\": float(res.pvalue),\n",
    "    }\n",
    "except Exception as e:\n",
    "    f\"SciPy check skipped: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement $U_1$ using pairwise comparisons and show it matches the rank-based formula (including ties).\n",
    "2. Use the permutation approach to approximate a p-value for different sample sizes and see how it stabilizes as `n_permutations` increases.\n",
    "3. Simulate power: pick a small shift in location and estimate how often Mann–Whitney rejects at $\\alpha=0.05$.\n",
    "\n",
    "## References\n",
    "- SciPy documentation: `scipy.stats.mannwhitneyu`\n",
    "- Conover, *Practical Nonparametric Statistics*\n",
    "- Lehmann, *Nonparametrics: Statistical Methods Based on Ranks*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}