{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "cells": [
  {
   "id": "b0c9189e",
   "cell_type": "markdown",
   "source": [
    "# Kurtosis Test (Anscombe–Glynn)\n",
    "\n",
    "The **kurtosis test** asks a narrow but useful question:\n",
    "\n",
    "- *Are the tails of this distribution consistent with a Normal distribution?*\n",
    "\n",
    "It is often used as a **diagnostic** for:\n",
    "\n",
    "- spotting **heavy tails / outliers** (more extreme values than Normal),\n",
    "- spotting **light tails** (bounded / overly thin-tailed data),\n",
    "- checking Normality assumptions in **residuals** (regression, time series, etc.).\n",
    "\n",
    "This notebook explains what kurtosis means, how the test works, how to interpret it, and implements the full **Anscombe–Glynn kurtosis test** *from scratch using only NumPy* (Plotly is used only for visuals).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Understand **Pearson kurtosis** vs **excess kurtosis**\n",
    "- See how kurtosis reacts to **tail events / outliers**\n",
    "- Implement the kurtosis test (test statistic + p-value) with NumPy\n",
    "- Interpret the **z-score sign**, **p-value**, and common pitfalls\n"
   ],
   "metadata": {}
  },
  {
   "id": "f04c7632",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ],
   "outputs": []
  },
  {
   "id": "358d4176",
   "cell_type": "markdown",
   "source": [
    "## 1) What is kurtosis? (intuition)\n",
    "\n",
    "Kurtosis is a **shape** summary of a distribution that is most usefully interpreted as **tail heaviness / outlier propensity**.\n",
    "\n",
    "Two common conventions:\n",
    "\n",
    "- **Pearson kurtosis**:\n",
    "\n",
    "  $$b_2 = \\frac{\\mathbb{E}[(X-\\mu)^4]}{(\\mathbb{E}[(X-\\mu)^2])^2}$$\n",
    "\n",
    "  For a Normal distribution, $b_2 = 3$.\n",
    "\n",
    "- **Excess kurtosis** (a.k.a. Fisher kurtosis):\n",
    "\n",
    "  $$g_2 = b_2 - 3$$\n",
    "\n",
    "  For a Normal distribution, $g_2 = 0$.\n",
    "\n",
    "**Rule of thumb:**\n",
    "\n",
    "- $g_2 > 0$ (**leptokurtic**): heavier tails / more extremes than Normal\n",
    "- $g_2 < 0$ (**platykurtic**): lighter tails / bounded behavior\n",
    "\n",
    "Important: kurtosis is sometimes described as \"peakedness\", but that can be misleading — the **tails** are the core story.\n"
   ],
   "metadata": {}
  },
  {
   "id": "33360a00",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "def sample_kurtosis_pearson(x: np.ndarray) -> float:\n",
    "    \"\"\"Biased Pearson kurtosis b2 = m4 / m2^2 (Normal -> 3).\n",
    "\n",
    "    Uses central moments with denominator n (bias=True style).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    n = x.size\n",
    "    if n < 4:\n",
    "        raise ValueError(f\"Need >=4 finite observations, got {n}.\")\n",
    "\n",
    "    mean = x.mean()\n",
    "    dev = x - mean\n",
    "    m2 = np.mean(dev**2)\n",
    "    if m2 == 0.0:\n",
    "        return np.nan\n",
    "    m4 = np.mean(dev**4)\n",
    "    return float(m4 / (m2**2))\n",
    "\n",
    "\n",
    "def standardize(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return (x - x.mean()) / x.std(ddof=0)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "id": "1e53fab1",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare several distributions after standardizing to mean=0, std=1\n",
    "n_big = 50_000\n",
    "samples = {\n",
    "    \"Normal\": rng.normal(size=n_big),\n",
    "    \"Laplace (heavy-ish tails)\": rng.laplace(size=n_big),\n",
    "    \"t(df=3) (very heavy tails)\": rng.standard_t(df=3, size=n_big),\n",
    "    \"Uniform (bounded)\": rng.uniform(low=-1, high=1, size=n_big),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, x in samples.items():\n",
    "    xs = standardize(x)\n",
    "    b2 = sample_kurtosis_pearson(xs)\n",
    "    rows.append((name, b2, b2 - 3.0))\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Table(\n",
    "            header=dict(values=[\"Distribution\", \"Pearson kurtosis b2\", \"Excess kurtosis g2\"]),\n",
    "            cells=dict(values=[\n",
    "                [r[0] for r in rows],\n",
    "                [f\"{r[1]:.3f}\" for r in rows],\n",
    "                [f\"{r[2]:.3f}\" for r in rows],\n",
    "            ]),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(title=\"Kurtosis changes a lot across tail shapes (all standardized to mean=0, std=1)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "id": "c599aa87",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Tail probability curves: P(|X| > t) highlights tail heaviness clearly\n",
    "thresholds = np.linspace(0, 5, 101)\n",
    "\n",
    "fig = go.Figure()\n",
    "for name, x in samples.items():\n",
    "    xs = standardize(x)\n",
    "    probs = [(np.abs(xs) > t).mean() for t in thresholds]\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=probs, mode=\"lines\", name=name))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Tail heaviness via exceedance probability (log scale)\",\n",
    "    xaxis_title=\"threshold t (in standard deviations)\",\n",
    "    yaxis_title=\"P(|X| > t)\",\n",
    "    yaxis_type=\"log\",\n",
    ")\n",
    "fig.show()\n"
   ],
   "outputs": []
  },
  {
   "id": "5b4a74e8",
   "cell_type": "markdown",
   "source": [
    "## 2) What does the kurtosis test do?\n",
    "\n",
    "The kurtosis test is a hypothesis test of whether the *population* kurtosis equals the Normal distribution's kurtosis.\n",
    "\n",
    "Let $b_2$ be the **sample Pearson kurtosis**.\n",
    "\n",
    "- **Null hypothesis**: the data come from a population with Normal kurtosis.\n",
    "\n",
    "  $$H_0: \\; b_2 = 3$$\n",
    "\n",
    "- **Alternative hypotheses**:\n",
    "\n",
    "  - two-sided: $b_2 \\neq 3$\n",
    "  - greater: $b_2 > 3$ (heavier tails than Normal)\n",
    "  - less: $b_2 < 3$ (lighter tails than Normal)\n",
    "\n",
    "The result is a **z-score** and a **p-value**:\n",
    "\n",
    "- The **z-score sign** indicates direction (heavy vs light tails).\n",
    "- The **p-value** is computed under $H_0$; it is the probability of seeing a z-score at least as extreme as the observed one if the underlying distribution had Normal kurtosis.\n",
    "\n",
    "⚠️ This is *not* a full Normality test: many non-Normal distributions can still have $b_2 \\approx 3$.\n"
   ],
   "metadata": {}
  },
  {
   "id": "c226d423",
   "cell_type": "markdown",
   "source": [
    "## 3) The Anscombe–Glynn test statistic (what we implement)\n",
    "\n",
    "A naive large-sample approximation is that excess kurtosis $g_2$ is approximately Normal with variance $24/n$, but the sampling distribution of kurtosis is **skewed** for realistic sample sizes.\n",
    "\n",
    "The **Anscombe–Glynn** method transforms $b_2$ into a quantity $Z$ that is approximately $\\mathcal{N}(0,1)$ under $H_0$.\n",
    "\n",
    "For sample size $n$ and sample Pearson kurtosis $b_2$:\n",
    "\n",
    "1) Under Normality, compute the expected value and variance of $b_2$:\n",
    "\n",
    "$$E[b_2] = 3 \\frac{n-1}{n+1}$$\n",
    "\n",
    "$$\\mathrm{Var}(b_2) = \\frac{24n(n-2)(n-3)}{(n+1)^2(n+3)(n+5)}$$\n",
    "\n",
    "2) Standardize:\n",
    "\n",
    "$$x = \\frac{b_2 - E[b_2]}{\\sqrt{\\mathrm{Var}(b_2)}}$$\n",
    "\n",
    "3) Apply the Anscombe–Glynn transform to get $Z$.\n",
    "\n",
    "Then the p-value is computed from the standard Normal distribution.\n",
    "\n",
    "Rule of thumb: p-values are most reliable for **n > 20**.\n"
   ],
   "metadata": {}
  },
  {
   "id": "cd9c68d9",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "Alternative = Literal[\"two-sided\", \"less\", \"greater\"]\n",
    "\n",
    "\n",
    "def erf_approx(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Fast erf approximation (Abramowitz & Stegun 7.1.26).\n",
    "\n",
    "    Vectorized NumPy implementation; max error ~1.5e-7.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    sign = np.sign(x)\n",
    "    ax = np.abs(x)\n",
    "\n",
    "    p = 0.3275911\n",
    "    a1 = 0.254829592\n",
    "    a2 = -0.284496736\n",
    "    a3 = 1.421413741\n",
    "    a4 = -1.453152027\n",
    "    a5 = 1.061405429\n",
    "\n",
    "    t = 1.0 / (1.0 + p * ax)\n",
    "    poly = (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t\n",
    "    y = 1.0 - poly * np.exp(-(ax * ax))\n",
    "    return sign * y\n",
    "\n",
    "\n",
    "def normal_cdf(z: np.ndarray) -> np.ndarray:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return 0.5 * (1.0 + erf_approx(z / np.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def normal_sf(z: np.ndarray) -> np.ndarray:\n",
    "    return 1.0 - normal_cdf(z)\n",
    "\n",
    "\n",
    "def pvalue_from_z(z: np.ndarray, alternative: Alternative) -> np.ndarray:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    if alternative == \"two-sided\":\n",
    "        return 2.0 * normal_sf(np.abs(z))\n",
    "    if alternative == \"greater\":\n",
    "        return normal_sf(z)\n",
    "    if alternative == \"less\":\n",
    "        return normal_cdf(z)\n",
    "    raise ValueError(f\"Unknown alternative: {alternative!r}\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class KurtosisTestResult:\n",
    "    statistic: float\n",
    "    pvalue: float\n",
    "    n: int\n",
    "    b2: float\n",
    "    expected_b2: float\n",
    "    var_b2: float\n",
    "    alternative: str\n",
    "\n",
    "\n",
    "def kurtosis_test_anscombe_glynn(\n",
    "    x: np.ndarray,\n",
    "    alternative: Alternative = \"two-sided\",\n",
    ") -> KurtosisTestResult:\n",
    "    \"\"\"Anscombe–Glynn test of Normal kurtosis (NumPy-only).\n",
    "\n",
    "    H0: population kurtosis equals Normal kurtosis (b2 = 3).\n",
    "\n",
    "    Notes:\n",
    "    - Requires at least 5 observations.\n",
    "    - p-values may be inaccurate for n < 20 (rule of thumb).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    n = int(x.size)\n",
    "    if n < 5:\n",
    "        raise ValueError(f\"Need >=5 finite observations for the kurtosis test, got {n}.\")\n",
    "\n",
    "    b2 = sample_kurtosis_pearson(x)\n",
    "\n",
    "    E = 3.0 * (n - 1.0) / (n + 1.0)\n",
    "    varb2 = 24.0 * n * (n - 2.0) * (n - 3.0) / ((n + 1.0) ** 2 * (n + 3.0) * (n + 5.0))\n",
    "    x_std = (b2 - E) / np.sqrt(varb2)\n",
    "\n",
    "    sqrtbeta1 = (\n",
    "        6.0 * (n * n - 5.0 * n + 2.0) / ((n + 7.0) * (n + 9.0))\n",
    "        * np.sqrt(6.0 * (n + 3.0) * (n + 5.0) / (n * (n - 2.0) * (n - 3.0)))\n",
    "    )\n",
    "    A = 6.0 + 8.0 / sqrtbeta1 * (2.0 / sqrtbeta1 + np.sqrt(1.0 + 4.0 / (sqrtbeta1**2)))\n",
    "\n",
    "    term1 = 1.0 - 2.0 / (9.0 * A)\n",
    "    denom = 1.0 + x_std * np.sqrt(2.0 / (A - 4.0))\n",
    "\n",
    "    if denom == 0.0:\n",
    "        z = np.nan\n",
    "    else:\n",
    "        term2 = np.sign(denom) * ((1.0 - 2.0 / A) / np.abs(denom)) ** (1.0 / 3.0)\n",
    "        z = (term1 - term2) / np.sqrt(2.0 / (9.0 * A))\n",
    "\n",
    "    p = float(pvalue_from_z(z, alternative=alternative))\n",
    "\n",
    "    return KurtosisTestResult(\n",
    "        statistic=float(z),\n",
    "        pvalue=p,\n",
    "        n=n,\n",
    "        b2=float(b2),\n",
    "        expected_b2=float(E),\n",
    "        var_b2=float(varb2),\n",
    "        alternative=alternative,\n",
    "    )\n"
   ],
   "outputs": []
  },
  {
   "id": "31066c03",
   "cell_type": "markdown",
   "source": [
    "### What the z-score means\n",
    "\n",
    "- **Z > 0**: sample kurtosis is larger than expected under Normality → heavier tails / more extremes\n",
    "- **Z < 0**: sample kurtosis is smaller than expected under Normality → lighter tails / bounded data\n",
    "\n",
    "The p-value turns this into a decision rule via your chosen $\\alpha$ (e.g. 0.05).\n"
   ],
   "metadata": {}
  },
  {
   "id": "c3b54a13",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Optional: compare our implementation to SciPy (should match closely)\n",
    "try:\n",
    "    from scipy.stats import kurtosistest as scipy_kurtosistest\n",
    "\n",
    "    x_demo = rng.normal(size=200)\n",
    "    ours = kurtosis_test_anscombe_glynn(x_demo)\n",
    "    theirs = scipy_kurtosistest(x_demo)\n",
    "\n",
    "    print('ours :', ours.statistic, ours.pvalue)\n",
    "    print('scipy:', float(theirs.statistic), float(theirs.pvalue))\n",
    "except Exception as e:\n",
    "    print('SciPy comparison skipped:', e)\n"
   ],
   "outputs": []
  },
  {
   "id": "78534949",
   "cell_type": "markdown",
   "source": [
    "## 4) Worked examples + interpretation\n",
    "\n",
    "We'll test two samples:\n",
    "\n",
    "1) A truly Normal sample (we expect *no* strong evidence against $H_0$ most of the time).\n",
    "2) A heavy-tailed sample (we expect evidence for $b_2 > 3$).\n"
   ],
   "metadata": {}
  },
  {
   "id": "77c98b1f",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "def standard_normal_pdf(z: np.ndarray) -> np.ndarray:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return np.exp(-0.5 * z * z) / np.sqrt(2.0 * np.pi)\n",
    "\n",
    "\n",
    "def plot_pvalue_shading(\n",
    "    z: float,\n",
    "    alternative: Alternative = \"two-sided\",\n",
    "    title: str | None = None,\n",
    ") -> go.Figure:\n",
    "    xs = np.linspace(-4.5, 4.5, 800)\n",
    "    ys = standard_normal_pdf(xs)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", name=\"N(0,1) pdf\"))\n",
    "\n",
    "    if alternative == \"two-sided\":\n",
    "        z0 = float(abs(z))\n",
    "        mask_left = xs <= -z0\n",
    "        mask_right = xs >= z0\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=xs[mask_left],\n",
    "                y=ys[mask_left],\n",
    "                fill=\"tozeroy\",\n",
    "                mode=\"lines\",\n",
    "                name=\"left tail\",\n",
    "                line=dict(width=0),\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=xs[mask_right],\n",
    "                y=ys[mask_right],\n",
    "                fill=\"tozeroy\",\n",
    "                mode=\"lines\",\n",
    "                name=\"right tail\",\n",
    "                line=dict(width=0),\n",
    "            )\n",
    "        )\n",
    "    elif alternative == \"greater\":\n",
    "        mask = xs >= z\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=xs[mask], y=ys[mask], fill=\"tozeroy\", mode=\"lines\", name=\"upper tail\", line=dict(width=0))\n",
    "        )\n",
    "    elif alternative == \"less\":\n",
    "        mask = xs <= z\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=xs[mask], y=ys[mask], fill=\"tozeroy\", mode=\"lines\", name=\"lower tail\", line=dict(width=0))\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    fig.add_vline(x=z, line_width=2, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "    p = float(pvalue_from_z(z, alternative=alternative))\n",
    "    fig.update_layout(\n",
    "        title=title or f\"p-value shading for z={z:.3f} ({alternative}), p={p:.4g}\",\n",
    "        xaxis_title=\"z\",\n",
    "        yaxis_title=\"density\",\n",
    "        showlegend=True,\n",
    "    )\n",
    "    return fig\n"
   ],
   "outputs": []
  },
  {
   "id": "5b22d06f",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Example A: Normal sample\n",
    "x_norm = rng.normal(size=200)\n",
    "res_norm = kurtosis_test_anscombe_glynn(x_norm, alternative=\"two-sided\")\n",
    "res_norm\n"
   ],
   "outputs": []
  },
  {
   "id": "15c95cba",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "fig = px.histogram(\n",
    "    x_norm,\n",
    "    nbins=40,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Example A: sample histogram (Normal)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"x\", yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "plot_pvalue_shading(res_norm.statistic, alternative=res_norm.alternative).show()\n"
   ],
   "outputs": []
  },
  {
   "id": "cf6b1a5b",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# Example B: heavy tails\n",
    "x_heavy = rng.standard_t(df=3, size=200)\n",
    "x_heavy = standardize(x_heavy)  # compare tail shape, not scale\n",
    "res_heavy = kurtosis_test_anscombe_glynn(x_heavy, alternative=\"greater\")\n",
    "res_heavy\n"
   ],
   "outputs": []
  },
  {
   "id": "197809f7",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "fig = px.histogram(\n",
    "    x_heavy,\n",
    "    nbins=40,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Example B: sample histogram (heavy tails, standardized)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"x\", yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "plot_pvalue_shading(res_heavy.statistic, alternative=res_heavy.alternative).show()\n"
   ],
   "outputs": []
  },
  {
   "id": "49a96738",
   "cell_type": "markdown",
   "source": [
    "### Interpreting the p-value (and what it does *not* mean)\n",
    "\n",
    "- If **p ≤ α** (e.g. 0.05): reject $H_0$ → the kurtosis is inconsistent with Normal kurtosis.\n",
    "- If **p > α**: *fail to reject* $H_0$ → you do not have evidence that kurtosis differs from Normal at this sample size.\n",
    "\n",
    "The p-value is **not** the probability that $H_0$ is true. It is a statement about what would happen **if** $H_0$ were true.\n"
   ],
   "metadata": {}
  },
  {
   "id": "63df8377",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# How sensitive is kurtosis to a few outliers?\n",
    "base = rng.normal(size=200)\n",
    "ks = list(range(0, 11))\n",
    "b2s = []\n",
    "for k in ks:\n",
    "    x = base.copy()\n",
    "    if k > 0:\n",
    "        x[:k] = 8.0  # inject k identical extreme outliers\n",
    "    b2s.append(sample_kurtosis_pearson(x))\n",
    "\n",
    "fig = px.line(x=ks, y=b2s, markers=True, title=\"A few outliers can dominate kurtosis\")\n",
    "fig.update_layout(xaxis_title=\"number of injected outliers\", yaxis_title=\"sample Pearson kurtosis b2\")\n",
    "fig.add_hline(y=3.0, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.show()\n"
   ],
   "outputs": []
  },
  {
   "id": "6647c463",
   "cell_type": "markdown",
   "source": [
    "## 5) Diagnostics: does Z look Normal under H0? (simulation)\n",
    "\n",
    "Under $H_0$ and with decent sample sizes, the transformed statistic $Z$ should be roughly $\\mathcal{N}(0,1)$.\n",
    "\n",
    "We'll simulate many Normal samples, compute $Z$, and compare histograms to the standard Normal pdf.\n"
   ],
   "metadata": {}
  },
  {
   "id": "448dcbbd",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def kurtosis_test_statistic_vectorized(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vectorized Z computation for many samples at once.\n",
    "\n",
    "    x: shape (reps, n)\n",
    "    returns: shape (reps,)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    reps, n = x.shape\n",
    "\n",
    "    mean = x.mean(axis=1, keepdims=True)\n",
    "    dev = x - mean\n",
    "    m2 = np.mean(dev**2, axis=1)\n",
    "    m4 = np.mean(dev**4, axis=1)\n",
    "    b2 = m4 / (m2**2)\n",
    "\n",
    "    E = 3.0 * (n - 1.0) / (n + 1.0)\n",
    "    varb2 = 24.0 * n * (n - 2.0) * (n - 3.0) / ((n + 1.0) ** 2 * (n + 3.0) * (n + 5.0))\n",
    "    x_std = (b2 - E) / np.sqrt(varb2)\n",
    "\n",
    "    sqrtbeta1 = (\n",
    "        6.0 * (n * n - 5.0 * n + 2.0) / ((n + 7.0) * (n + 9.0))\n",
    "        * np.sqrt(6.0 * (n + 3.0) * (n + 5.0) / (n * (n - 2.0) * (n - 3.0)))\n",
    "    )\n",
    "    A = 6.0 + 8.0 / sqrtbeta1 * (2.0 / sqrtbeta1 + np.sqrt(1.0 + 4.0 / (sqrtbeta1**2)))\n",
    "\n",
    "    term1 = 1.0 - 2.0 / (9.0 * A)\n",
    "    denom = 1.0 + x_std * np.sqrt(2.0 / (A - 4.0))\n",
    "    term2 = np.sign(denom) * ((1.0 - 2.0 / A) / np.abs(denom)) ** (1.0 / 3.0)\n",
    "\n",
    "    z = (term1 - term2) / np.sqrt(2.0 / (9.0 * A))\n",
    "    return z\n",
    "\n",
    "\n",
    "reps = 6000\n",
    "ns = [10, 20, 50, 200]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=[f\"n={n}\" for n in ns])\n",
    "xs = np.linspace(-4, 4, 400)\n",
    "pdf = standard_normal_pdf(xs)\n",
    "\n",
    "for i, n in enumerate(ns):\n",
    "    x = rng.normal(size=(reps, n))\n",
    "    z = kurtosis_test_statistic_vectorized(x)\n",
    "\n",
    "    r = i // 2 + 1\n",
    "    c = i % 2 + 1\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=z,\n",
    "            nbinsx=60,\n",
    "            histnorm=\"probability density\",\n",
    "            opacity=0.7,\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=xs,\n",
    "            y=pdf,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "\n",
    "fig.update_layout(title=\"Sampling distribution of Z under H0 (Normal data)\")\n",
    "fig.update_xaxes(title_text=\"Z\")\n",
    "fig.update_yaxes(title_text=\"density\")\n",
    "fig.show()\n"
   ],
   "outputs": []
  },
  {
   "id": "6a6e4f44",
   "cell_type": "markdown",
   "source": [
    "## 6) Practical guidance + pitfalls\n",
    "\n",
    "- The test is most trustworthy for **n > 20**. With small samples, kurtosis is noisy and the approximation can be off.\n",
    "- The test is **very sensitive to outliers**. A single bad data point can flip the conclusion — sometimes that's exactly what you want, sometimes it's a data quality issue.\n",
    "- The test only targets **kurtosis**, not skewness. Combine with a skewness test or a broader normality test if you need overall Normality.\n",
    "- With very large $n$, tiny deviations in kurtosis can be statistically significant; always check the estimated $b_2$ / $g_2$ and plots.\n"
   ],
   "metadata": {}
  },
  {
   "id": "9fb2f13b",
   "cell_type": "markdown",
   "source": [
    "## Exercises\n",
    "\n",
    "1) Modify `kurtosis_test_anscombe_glynn` to support `axis=` for 2D arrays.\n",
    "2) Create a mixture distribution with $b_2 \\approx 3$ but visibly non-Normal; verify that the kurtosis test often fails to reject.\n",
    "3) Compare the kurtosis test to D'Agostino's $K^2$ test (which combines skewness and kurtosis).\n"
   ],
   "metadata": {}
  },
  {
   "id": "f950eb9a",
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "- F. J. Anscombe, W. J. Glynn (1983). *Distribution of the kurtosis statistic b2 for normal samples*. **Biometrika** 70(1): 227–234.\n",
    "- SciPy: `scipy.stats.kurtosistest` (implementation follows Anscombe–Glynn).\n"
   ],
   "metadata": {}
  }
 ]
}
