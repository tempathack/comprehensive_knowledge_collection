{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov–Smirnov (KS) test (one-sample + two-sample)\n",
    "\n",
    "The Kolmogorov–Smirnov (KS) test is a **nonparametric** test about **distributions**:\n",
    "\n",
    "- **One-sample KS**: does one sample look like it came from a *fully specified* continuous distribution $F_0$?\n",
    "- **Two-sample KS**: do two samples look like they came from the *same* continuous distribution?\n",
    "\n",
    "It does this by looking at the **largest vertical gap** between cumulative distribution functions (CDFs).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "After this notebook you should be able to:\n",
    "\n",
    "- explain what the KS statistic $D$ measures (and what it does *not* measure)\n",
    "- compute $D$ from scratch using only NumPy (no SciPy)\n",
    "- approximate p-values using Monte Carlo (one-sample) and permutations (two-sample)\n",
    "- interpret results in practice and spot common pitfalls\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- CDF vs PDF, and the idea of an empirical CDF (ECDF)\n",
    "- basic hypothesis testing (null, alternative, p-value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Record versions for reproducibility (numerical details can vary across versions).\n",
    "VERSIONS = {\"numpy\": np.__version__, \"plotly\": plotly.__version__}\n",
    "VERSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the KS test is (and isn't)\n",
    "\n",
    "The KS test is about the **entire distribution**, not a single summary like the mean.\n",
    "\n",
    "- If two distributions differ in **location** (mean/median), **scale** (variance), or **shape** (skew/heavy tails), KS can detect it.\n",
    "- The KS statistic $D$ is an **effect size**: a maximum vertical distance between two CDF curves.\n",
    "- A small p-value means: *\"under the null hypothesis, a gap this large (or larger) would be rare.\"*\n",
    "\n",
    "Typical uses:\n",
    "\n",
    "- **Goodness-of-fit**: Does a simulator/assumption match a known distribution $F_0$?\n",
    "- **Distribution shift**: Did a feature distribution change between two time periods?\n",
    "- **A/B comparisons**: Do two groups have different outcome distributions?\n",
    "\n",
    "Key assumptions for the classic KS p-values:\n",
    "\n",
    "- observations are **independent**\n",
    "- the null distribution is **continuous** (ties / discrete support changes the calibration)\n",
    "- for the one-sample test, $F_0$ is **fully specified** (if you estimate parameters from the same data, you need a correction like Lilliefors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core object: the CDF (and the ECDF)\n",
    "\n",
    "For a random variable $X$, the cumulative distribution function (CDF) is\n",
    "\n",
    "$$F(x) = \\Pr(X \\le x).$$\n",
    "\n",
    "Given data $x_1,\\dots,x_n$, the **empirical CDF** (ECDF) is\n",
    "\n",
    "$$\\hat F_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{x_i \\le x\\}.$$\n",
    "\n",
    "The ECDF is a step function: it jumps by $1/n$ at each observed value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf_step(x):\n",
    "    \"\"\"Return sorted data and step-plot points for the ECDF.\"\"\"\n",
    "    x_sorted = np.sort(np.asarray(x))\n",
    "    n = x_sorted.size\n",
    "    if n == 0:\n",
    "        raise ValueError(\"x must contain at least one observation.\")\n",
    "\n",
    "    # With line_shape=\"hv\", repeating the first x gives the initial vertical jump from 0.\n",
    "    x_step = np.r_[x_sorted[0], x_sorted]\n",
    "    y_step = np.r_[0.0, np.arange(1, n + 1) / n]\n",
    "    return x_sorted, x_step, y_step\n",
    "\n",
    "\n",
    "def padded_linspace(x, n=500, pad=0.05):\n",
    "    x = np.asarray(x)\n",
    "    x_min = float(np.min(x))\n",
    "    x_max = float(np.max(x))\n",
    "    span = x_max - x_min\n",
    "    if span == 0:\n",
    "        span = 1.0\n",
    "    return np.linspace(x_min - pad * span, x_max + pad * span, n)\n",
    "\n",
    "\n",
    "def cdf_uniform_01(u):\n",
    "    \"\"\"CDF of Uniform(0,1) evaluated at u.\"\"\"\n",
    "    return np.clip(np.asarray(u), 0.0, 1.0)\n",
    "\n",
    "\n",
    "def ks_1samp_statistic(sample, cdf):\n",
    "    \"\"\"One-sample KS statistic against a continuous, fully specified CDF.\n",
    "\n",
    "    D is the max vertical gap between the ECDF and F0:\n",
    "        D = sup_x |F_n(x) - F0(x)|\n",
    "\n",
    "    For a sorted sample x_(1) <= ... <= x_(n), the maximum occurs at the order statistics.\n",
    "    A convenient computation is via:\n",
    "        D+ = max_i (i/n - F0(x_(i)))\n",
    "        D- = max_i (F0(x_(i)) - (i-1)/n)\n",
    "        D = max(D+, D-)\n",
    "    \"\"\"\n",
    "    x = np.sort(np.asarray(sample))\n",
    "    n = x.size\n",
    "    if n == 0:\n",
    "        raise ValueError(\"sample must contain at least one observation.\")\n",
    "\n",
    "    F = np.asarray(cdf(x), dtype=float)\n",
    "    F = np.clip(F, 0.0, 1.0)\n",
    "\n",
    "    i = np.arange(1, n + 1)\n",
    "    d_plus_vals = i / n - F\n",
    "    d_minus_vals = F - (i - 1) / n\n",
    "\n",
    "    D_plus = float(np.max(d_plus_vals))\n",
    "    D_minus = float(np.max(d_minus_vals))\n",
    "\n",
    "    if D_plus >= D_minus:\n",
    "        idx = int(np.argmax(d_plus_vals))\n",
    "        direction = \"+\"  # ECDF is above F0 at the max gap\n",
    "        y_emp = float(i[idx] / n)  # right-limit ECDF at x_(i)\n",
    "        y_cdf = float(F[idx])\n",
    "        D = D_plus\n",
    "    else:\n",
    "        idx = int(np.argmax(d_minus_vals))\n",
    "        direction = \"-\"  # ECDF is below F0 at the max gap\n",
    "        y_emp = float((i[idx] - 1) / n)  # left-limit ECDF at x_(i)\n",
    "        y_cdf = float(F[idx])\n",
    "        D = D_minus\n",
    "\n",
    "    return {\n",
    "        \"D\": float(D),\n",
    "        \"D_plus\": D_plus,\n",
    "        \"D_minus\": D_minus,\n",
    "        \"x_star\": float(x[idx]),\n",
    "        \"y_emp\": y_emp,\n",
    "        \"y_cdf\": y_cdf,\n",
    "        \"direction\": direction,\n",
    "    }\n",
    "\n",
    "\n",
    "def ks_2samp_statistic(x, y):\n",
    "    \"\"\"Two-sample KS statistic D = sup_x |F_n(x) - G_m(x)|.\"\"\"\n",
    "    x = np.sort(np.asarray(x))\n",
    "    y = np.sort(np.asarray(y))\n",
    "    n = x.size\n",
    "    m = y.size\n",
    "    if n == 0 or m == 0:\n",
    "        raise ValueError(\"Both samples must contain at least one observation.\")\n",
    "\n",
    "    grid = np.sort(np.concatenate([x, y]))\n",
    "    F_n = np.searchsorted(x, grid, side=\"right\") / n\n",
    "    G_m = np.searchsorted(y, grid, side=\"right\") / m\n",
    "    diff = np.abs(F_n - G_m)\n",
    "    idx = int(np.argmax(diff))\n",
    "\n",
    "    return {\"D\": float(diff[idx]), \"x_star\": float(grid[idx]), \"F_n\": float(F_n[idx]), \"G_m\": float(G_m[idx])}\n",
    "\n",
    "\n",
    "def ks_pvalue_asymptotic_two_sided(D, n_eff, tol=1e-12, max_terms=10_000):\n",
    "    \"\"\"Asymptotic two-sided KS p-value via the Kolmogorov distribution.\n",
    "\n",
    "    Uses a common Stephens-type finite-sample correction:\n",
    "        lambda = (sqrt(n_eff) + 0.12 + 0.11/sqrt(n_eff)) * D\n",
    "\n",
    "    This is great for intuition; for production, prefer a vetted library implementation.\n",
    "    \"\"\"\n",
    "    D = float(D)\n",
    "    n_eff = float(n_eff)\n",
    "    if D <= 0:\n",
    "        return 1.0\n",
    "    if n_eff <= 0:\n",
    "        raise ValueError(\"n_eff must be positive.\")\n",
    "\n",
    "    en = np.sqrt(n_eff)\n",
    "    lam = (en + 0.12 + 0.11 / en) * D\n",
    "\n",
    "    total = 0.0\n",
    "    for k in range(1, int(max_terms) + 1):\n",
    "        term = (-1.0) ** (k - 1) * np.exp(-2.0 * (k * k) * (lam * lam))\n",
    "        total += term\n",
    "        if abs(term) < tol:\n",
    "            break\n",
    "\n",
    "    p = 2.0 * total\n",
    "    return float(np.clip(p, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def ks_1samp_null_ds(n, n_sim=5000, rng=None):\n",
    "    \"\"\"Simulate the null distribution of D for the one-sample KS test (continuous null).\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    ds = np.empty(n_sim, dtype=float)\n",
    "    for b in range(n_sim):\n",
    "        u = rng.uniform(0.0, 1.0, size=n)\n",
    "        ds[b] = ks_1samp_statistic(u, cdf_uniform_01)[\"D\"]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def mc_pvalue_right_tail(stat_obs, stats_null):\n",
    "    \"\"\"Add-one smoothed right-tail Monte Carlo p-value.\"\"\"\n",
    "    stats_null = np.asarray(stats_null)\n",
    "    return float((np.sum(stats_null >= stat_obs) + 1.0) / (stats_null.size + 1.0))\n",
    "\n",
    "\n",
    "def ks_2samp_pvalue_perm(x, y, n_perm=5000, rng=None):\n",
    "    \"\"\"Permutation p-value for the two-sample KS test (two-sided).\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    n = x.size\n",
    "    D_obs = ks_2samp_statistic(x, y)[\"D\"]\n",
    "\n",
    "    pooled = np.concatenate([x, y])\n",
    "    ds = np.empty(n_perm, dtype=float)\n",
    "\n",
    "    for b in range(n_perm):\n",
    "        perm = rng.permutation(pooled.size)\n",
    "        x_b = pooled[perm[:n]]\n",
    "        y_b = pooled[perm[n:]]\n",
    "        ds[b] = ks_2samp_statistic(x_b, y_b)[\"D\"]\n",
    "\n",
    "    return mc_pvalue_right_tail(D_obs, ds), ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) One-sample KS test (goodness-of-fit)\n",
    "\n",
    "**Question:** *Does a sample look like it came from a given continuous distribution $F_0$?*\n",
    "\n",
    "### Hypotheses (two-sided)\n",
    "\n",
    "- $H_0$: the data are i.i.d. from $F_0$\n",
    "- $H_1$: the data are not from $F_0$ (their CDF differs somewhere)\n",
    "\n",
    "### Test statistic\n",
    "\n",
    "$$D_n = \\sup_x \\left|\\hat F_n(x) - F_0(x)\\right|.$$\n",
    "\n",
    "Intuition: imagine drawing the ECDF and the theoretical CDF. $D_n$ is the **largest vertical gap** between those two curves.\n",
    "\n",
    "Because the ECDF is a step function, you only need to check the **order statistics** $x_{(1)} \\le \\dots \\le x_{(n)}$:\n",
    "\n",
    "$$D^+ = \\max_i \\left(\\frac{i}{n} - F_0(x_{(i)})\\right), \\qquad D^- = \\max_i \\left(F_0(x_{(i)}) - \\frac{i-1}{n}\\right),$$\n",
    "\n",
    "and $D = \\max(D^+, D^-)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_exponential(x, rate):\n",
    "    \"\"\"CDF of Exp(rate) with support x>=0.\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    return np.where(x < 0.0, 0.0, 1.0 - np.exp(-rate * x))\n",
    "\n",
    "\n",
    "# Synthetic data: true distribution is Exp(rate=1.0)\n",
    "n = 80\n",
    "rng_data = np.random.default_rng(1)\n",
    "x = rng_data.exponential(scale=1.0, size=n)\n",
    "\n",
    "models = {\n",
    "    \"rate=1.0 (correct)\": 1.0,\n",
    "    \"rate=0.5 (too heavy tail)\": 0.5,\n",
    "}\n",
    "\n",
    "# Under a continuous fully specified null, D is distribution-free, so we simulate it once.\n",
    "n_sim = 5000\n",
    "rng_null = np.random.default_rng(2)\n",
    "ds_null_1samp = ks_1samp_null_ds(n=n, n_sim=n_sim, rng=rng_null)\n",
    "\n",
    "one_sample_results = {}\n",
    "for name, rate in models.items():\n",
    "    res = ks_1samp_statistic(x, lambda t, r=rate: cdf_exponential(t, r))\n",
    "    one_sample_results[name] = {\n",
    "        **res,\n",
    "        \"p_mc\": mc_pvalue_right_tail(res[\"D\"], ds_null_1samp),\n",
    "        \"p_asym\": ks_pvalue_asymptotic_two_sided(res[\"D\"], n_eff=n),\n",
    "    }\n",
    "\n",
    "one_sample_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1samp = one_sample_results[\"rate=1.0 (correct)\"]\n",
    "rate = models[\"rate=1.0 (correct)\"]\n",
    "\n",
    "x_sorted, x_step, y_step = ecdf_step(x)\n",
    "x_grid = padded_linspace(x, n=600)\n",
    "\n",
    "cdf_grid = cdf_exponential(x_grid, rate=rate)\n",
    "ecdf_grid = np.searchsorted(x_sorted, x_grid, side=\"right\") / x_sorted.size\n",
    "abs_gap = np.abs(ecdf_grid - cdf_grid)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.12,\n",
    "    row_heights=[0.65, 0.35],\n",
    ")\n",
    "\n",
    "# Top panel: ECDF vs theoretical CDF\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_step, y=y_step, mode=\"lines\", name=\"ECDF\", line_shape=\"hv\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_grid, y=cdf_grid, mode=\"lines\", name=f\"Exp CDF (rate={rate})\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Highlight the max gap D at x_star\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[res_1samp[\"x_star\"], res_1samp[\"x_star\"]],\n",
    "        y=[res_1samp[\"y_emp\"], res_1samp[\"y_cdf\"]],\n",
    "        mode=\"lines\",\n",
    "        name=f\"D = {res_1samp['D']:.3f}\",\n",
    "        line=dict(color=\"crimson\", width=4),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[res_1samp[\"x_star\"], res_1samp[\"x_star\"]],\n",
    "        y=[res_1samp[\"y_emp\"], res_1samp[\"y_cdf\"]],\n",
    "        mode=\"markers\",\n",
    "        showlegend=False,\n",
    "        marker=dict(color=\"crimson\", size=9),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Bottom panel: absolute gap as a function of x\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_grid, y=abs_gap, mode=\"lines\", name=\"|ECDF - CDF|\", line=dict(color=\"gray\")),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_vline(x=res_1samp[\"x_star\"], line=dict(color=\"crimson\", dash=\"dot\"))\n",
    "fig.add_hline(y=res_1samp[\"D\"], line=dict(color=\"crimson\", dash=\"dash\"), row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"CDF value\", row=1, col=1, range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"Absolute gap\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"One-sample KS (Exp rate=1.0): ECDF vs CDF and the max gap D\",\n",
    "    height=720,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the null distribution of D and compare the two observed statistics\n",
    "D_crit_05 = float(np.quantile(ds_null_1samp, 0.95))\n",
    "\n",
    "D_correct = one_sample_results[\"rate=1.0 (correct)\"][\"D\"]\n",
    "D_wrong = one_sample_results[\"rate=0.5 (too heavy tail)\"][\"D\"]\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=ds_null_1samp,\n",
    "    nbins=60,\n",
    "    title=\"One-sample KS: null distribution of D (simulated under H0)\",\n",
    "    labels={\"x\": \"D\"},\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=D_crit_05,\n",
    "    line_color=\"black\",\n",
    "    line_dash=\"dash\",\n",
    "    annotation_text=f\"~5% critical value: {D_crit_05:.3f}\",\n",
    "    annotation_position=\"top left\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=D_correct,\n",
    "    line_color=\"seagreen\",\n",
    "    annotation_text=f\"Observed D (correct): {D_correct:.3f}<br>p≈{one_sample_results['rate=1.0 (correct)']['p_mc']:.3f}\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=D_wrong,\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"Observed D (wrong): {D_wrong:.3f}<br>p≈{one_sample_results['rate=0.5 (too heavy tail)']['p_mc']:.3f}\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret the one-sample KS test\n",
    "\n",
    "- $D$ is the **largest absolute CDF difference**. Example: $D=0.12$ means there exists a threshold $x$ where the empirical probability mass below $x$ differs from $F_0(x)$ by about **12 percentage points**.\n",
    "- The **p-value** is about how surprising that gap is *under $H_0$*.\n",
    "- Decision rule at significance level $\\alpha$:\n",
    "  - reject $H_0$ if p-value $< \\alpha$\n",
    "  - otherwise: you **fail to reject** (you don't \"prove\" $H_0$; you just lack evidence against it)\n",
    "\n",
    "Also note the sign:\n",
    "\n",
    "- If the maximum comes from $D^+$ (direction `+` in the helper), the ECDF is *above* $F_0$ at $x_\\*$. That typically means the sample is **more concentrated on smaller values** than $F_0$ at that threshold.\n",
    "- If it comes from $D^-$ (direction `-`), the ECDF is *below* $F_0$ at $x_\\*$ (the sample puts **less mass** below that threshold).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Two-sample KS test (distribution comparison)\n",
    "\n",
    "**Question:** *Do two independent samples look like they came from the same continuous distribution?*\n",
    "\n",
    "### Hypotheses (two-sided)\n",
    "\n",
    "- $H_0$: both samples are i.i.d. from the same distribution (their CDFs are equal)\n",
    "- $H_1$: the distributions differ\n",
    "\n",
    "### Test statistic\n",
    "\n",
    "$$D_{n,m} = \\sup_x \\left|\\hat F_n(x) - \\hat G_m(x)\\right|.$$\n",
    "\n",
    "Same intuition: draw both ECDFs; $D_{n,m}$ is the biggest vertical gap.\n",
    "\n",
    "For p-values here we’ll use a **permutation test**:\n",
    "\n",
    "- under $H_0$, the combined sample is exchangeable; labels \"A\" vs \"B\" are arbitrary\n",
    "- so we repeatedly shuffle labels and recompute $D$\n",
    "- the p-value is the fraction of shuffled datasets with $D_{\\text{perm}} \\ge D_{\\text{obs}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic two-sample example\n",
    "n1, n2 = 80, 90\n",
    "\n",
    "rng_a = np.random.default_rng(3)\n",
    "rng_b = np.random.default_rng(4)\n",
    "\n",
    "x_a = rng_a.normal(loc=0.0, scale=1.0, size=n1)\n",
    "x_b = rng_b.normal(loc=0.4, scale=1.0, size=n2)  # shifted mean\n",
    "\n",
    "res_2samp = ks_2samp_statistic(x_a, x_b)\n",
    "\n",
    "n_perm = 4000\n",
    "rng_perm = np.random.default_rng(5)\n",
    "p_perm, ds_perm_2samp = ks_2samp_pvalue_perm(x_a, x_b, n_perm=n_perm, rng=rng_perm)\n",
    "\n",
    "n_eff = n1 * n2 / (n1 + n2)\n",
    "p_asym = ks_pvalue_asymptotic_two_sided(res_2samp[\"D\"], n_eff=n_eff)\n",
    "\n",
    "{**res_2samp, \"p_perm\": p_perm, \"p_asym\": p_asym}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both ECDFs and the location of the max gap\n",
    "x_sorted_a, x_step_a, y_step_a = ecdf_step(x_a)\n",
    "x_sorted_b, x_step_b, y_step_b = ecdf_step(x_b)\n",
    "\n",
    "grid = padded_linspace(np.concatenate([x_a, x_b]), n=700)\n",
    "Fa = np.searchsorted(x_sorted_a, grid, side=\"right\") / x_sorted_a.size\n",
    "Fb = np.searchsorted(x_sorted_b, grid, side=\"right\") / x_sorted_b.size\n",
    "abs_gap = np.abs(Fa - Fb)\n",
    "\n",
    "x_star = res_2samp[\"x_star\"]\n",
    "Fa_star = res_2samp[\"F_n\"]\n",
    "Fb_star = res_2samp[\"G_m\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.12,\n",
    "    row_heights=[0.65, 0.35],\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_step_a, y=y_step_a, mode=\"lines\", name=\"ECDF A\", line_shape=\"hv\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_step_b, y=y_step_b, mode=\"lines\", name=\"ECDF B\", line_shape=\"hv\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Highlight max gap D\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[x_star, x_star],\n",
    "        y=[Fa_star, Fb_star],\n",
    "        mode=\"lines\",\n",
    "        name=f\"D = {res_2samp['D']:.3f}\",\n",
    "        line=dict(color=\"crimson\", width=4),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[x_star, x_star],\n",
    "        y=[Fa_star, Fb_star],\n",
    "        mode=\"markers\",\n",
    "        showlegend=False,\n",
    "        marker=dict(color=\"crimson\", size=9),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=grid, y=abs_gap, mode=\"lines\", name=\"|ECDF A - ECDF B|\", line=dict(color=\"gray\")),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_vline(x=x_star, line=dict(color=\"crimson\", dash=\"dot\"))\n",
    "fig.add_hline(y=res_2samp[\"D\"], line=dict(color=\"crimson\", dash=\"dash\"), row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"CDF value\", row=1, col=1, range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"Absolute gap\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Two-sample KS: ECDFs and the max gap D\",\n",
    "    height=720,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation null distribution for the two-sample test\n",
    "D_crit_05 = float(np.quantile(ds_perm_2samp, 0.95))\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=ds_perm_2samp,\n",
    "    nbins=60,\n",
    "    title=\"Two-sample KS: permutation null distribution of D\",\n",
    "    labels={\"x\": \"D\"},\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=D_crit_05,\n",
    "    line_color=\"black\",\n",
    "    line_dash=\"dash\",\n",
    "    annotation_text=f\"~5% critical value: {D_crit_05:.3f}\",\n",
    "    annotation_position=\"top left\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=res_2samp[\"D\"],\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"Observed D: {res_2samp['D']:.3f}<br>p≈{p_perm:.3f}\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does a KS result *mean*?\n",
    "\n",
    "### 1) The statistic $D$ (effect size)\n",
    "\n",
    "- One-sample: $D$ is the maximum difference between the sample ECDF and $F_0$.\n",
    "- Two-sample: $D$ is the maximum difference between the two ECDFs.\n",
    "\n",
    "A useful way to say it in plain language:\n",
    "\n",
    "> There exists a threshold $x$ such that the two distributions disagree about the probability of being \\(\\le x\\) by about $D$.\n",
    "\n",
    "### 2) The p-value (evidence)\n",
    "\n",
    "- p-value is computed **under $H_0$**.\n",
    "- small p-value: the observed max gap is unlikely if $H_0$ were true → evidence against $H_0$.\n",
    "- large p-value: the observed gap is plausible under $H_0$ → not enough evidence to reject.\n",
    "\n",
    "### 3) What the test does *not* tell you\n",
    "\n",
    "- It does not tell you *why* distributions differ (mean shift? variance? tails?)\n",
    "- It does not tell you whether the difference is practically important (that’s your domain call; $D$ helps)\n",
    "- With very large samples, tiny differences can become statistically significant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls and diagnostics\n",
    "\n",
    "- **Discrete data / ties**: the classic KS calibration assumes continuity. With many ties (e.g., integer counts), p-values can be off (often conservative). Consider alternatives (e.g., chi-square / exact tests) or permutation/bootstrap calibration.\n",
    "- **Estimated parameters (one-sample)**: if you fit parameters using the same sample (e.g., test normality after fitting mean/std), the standard KS p-value is not valid. Look into **Lilliefors**-type corrections or use a parametric bootstrap.\n",
    "- **Sensitivity profile**: KS is typically more sensitive around the center of the distribution than the far tails. If tails matter, consider Anderson–Darling or Cramér–von Mises.\n",
    "- **Independence**: dependence (time series, clustered data) breaks the null distribution. Consider block bootstrap / time-series aware tests.\n",
    "- **Multiple testing**: if you test many features, control false positives (Bonferroni, BH/FDR, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Modify the two-sample example so that the groups differ only in **variance**, not mean. How does $D$ change?\n",
    "2. Simulate discrete data (e.g., Poisson) and see how ties affect the ECDF plot and the test calibration.\n",
    "3. For the one-sample setting, estimate the rate parameter from the sample (MLE) and compare:\n",
    "   - naive KS p-value (treating the fitted distribution as fixed)\n",
    "   - parametric bootstrap p-value (simulate from fitted distribution, refit, recompute D)\n",
    "4. Implement a one-sided KS test using $D^+$ or $D^-$ and interpret what each side means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Kolmogorov–Smirnov test (general overview): https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n",
    "- Stephens, M. A. (1970). Use of the Kolmogorov–Smirnov, Cramér–von Mises and related statistics without extensive tables.\n",
    "- For parameter-estimation caveat (Lilliefors test): https://en.wikipedia.org/wiki/Lilliefors_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}