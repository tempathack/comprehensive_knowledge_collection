{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9f2941",
   "metadata": {},
   "source": [
    "# Jarque–Bera normality test (`jarque_bera`)\n",
    "\n",
    "The **Jarque–Bera (JB) test** is a classic *normality test* built around two facts that must hold if data are normally distributed:\n",
    "\n",
    "- **Skewness** should be 0 (symmetry)\n",
    "- **Kurtosis** should be 3 (tail thickness; equivalently **excess kurtosis** should be 0)\n",
    "\n",
    "It’s often used as a quick check (especially on **model residuals**) to see whether the **Gaussian assumption** is plausible.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Understand what the JB statistic measures (skewness + kurtosis departures from normal)\n",
    "- Implement the test **from scratch with NumPy only**\n",
    "- Interpret the statistic and p-value correctly\n",
    "- Build intuition via **Plotly** visuals (histograms, Q–Q plots, and the null distribution)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Sample mean/variance and central moments\n",
    "- Basic hypothesis testing (null vs alternative, p-values)\n",
    "- The chi-square distribution (JB is asymptotically χ² with 2 degrees of freedom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "VERSIONS = {\"numpy\": np.__version__, \"plotly\": plotly.__version__}\n",
    "VERSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b04518",
   "metadata": {},
   "source": [
    "## 1) What the test is for (and when to use it)\n",
    "\n",
    "### What it tests\n",
    "\n",
    "The JB test is a hypothesis test of **normality**:\n",
    "\n",
    "- **H₀ (null)**: the data are generated by a normal distribution\n",
    "- **H₁ (alternative)**: the data are *not* normal\n",
    "\n",
    "The test doesn’t try to match the *entire* shape of the distribution. It focuses on two features:\n",
    "\n",
    "- asymmetry (**skewness**)\n",
    "- tail heaviness / peakedness (**kurtosis**)\n",
    "\n",
    "So the JB test is most informative when the key deviations you care about are skewness and/or tail behavior.\n",
    "\n",
    "### Where it shows up in practice\n",
    "\n",
    "Common use cases:\n",
    "\n",
    "- **Residual diagnostics** (linear regression, ARIMA, etc.)\n",
    "- **Sanity checks** before using methods that assume normality (or normal residuals)\n",
    "- Quick comparisons of “how non-normal” different samples look in terms of skewness and kurtosis\n",
    "\n",
    "### What it is *not*\n",
    "\n",
    "- It is not a proof of normality. A large p-value means **“insufficient evidence to reject normality”**, not “the data are normal”.\n",
    "- It is not a comprehensive shape test: some non-normal distributions can have skewness near 0 and kurtosis near 3 (JB may miss them).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ed85f",
   "metadata": {},
   "source": [
    "## 2) The Jarque–Bera statistic\n",
    "\n",
    "Let $x_1,\\dots,x_n$ be a sample.\n",
    "\n",
    "Define the **sample central moments**\n",
    "\n",
    "$$\n",
    " m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^k.\n",
    "$$\n",
    "\n",
    "From these, define the (moment-based) **sample skewness** $S$ and **sample kurtosis** $K$:\n",
    "\n",
    "$$\n",
    "S = \\frac{m_3}{m_2^{3/2}},\n",
    "\\qquad\n",
    "K = \\frac{m_4}{m_2^2}.\n",
    "$$\n",
    "\n",
    "For a normal distribution, $S=0$ and $K=3$.\n",
    "\n",
    "The **Jarque–Bera statistic** combines deviations in both quantities:\n",
    "\n",
    "$$\n",
    "\\mathrm{JB}\n",
    "= \\frac{n}{6}\\left(S^2 + \\frac{(K-3)^2}{4}\\right)\n",
    "= \\underbrace{\\frac{n}{6}S^2}_{\\text{skew contribution}}\n",
    "+ \\underbrace{\\frac{n}{24}(K-3)^2}_{\\text{kurtosis contribution}}.\n",
    "$$\n",
    "\n",
    "### Asymptotic null distribution and p-value\n",
    "\n",
    "Under $H_0$ (normality), as $n \\to \\infty$:\n",
    "\n",
    "$$\n",
    "\\mathrm{JB} \\;\\xrightarrow[]{d}\\; \\chi^2(2).\n",
    "$$\n",
    "\n",
    "So the p-value is\n",
    "\n",
    "$$\n",
    "\\text{p-value} = \\mathbb{P}(\\chi^2(2) \\ge \\mathrm{JB}).\n",
    "$$\n",
    "\n",
    "For **2 degrees of freedom**, the survival function has a simple closed form:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\chi^2(2) \\ge x) = e^{-x/2}.\n",
    "$$\n",
    "\n",
    "That means for JB we can compute the p-value with just `np.exp` (no SciPy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed456b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_1d_finite(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    return x[np.isfinite(x)]\n",
    "\n",
    "\n",
    "def central_moment(x: np.ndarray, k: int) -> float:\n",
    "    # Central moment m_k = mean((x - mean(x))^k)\n",
    "    if k < 0:\n",
    "        raise ValueError(\"k must be non-negative\")\n",
    "\n",
    "    x = _to_1d_finite(x)\n",
    "    if x.size == 0:\n",
    "        return float(\"nan\")\n",
    "    if k == 0:\n",
    "        return 1.0\n",
    "\n",
    "    mean = float(x.mean())\n",
    "    return float(np.mean((x - mean) ** k))\n",
    "\n",
    "\n",
    "def sample_skewness_kurtosis(x: np.ndarray) -> tuple[float, float]:\n",
    "    # Moment-based sample skewness S and kurtosis K (normal => K=3).\n",
    "    x = _to_1d_finite(x)\n",
    "    if x.size < 3:\n",
    "        raise ValueError(\"Need at least 3 finite values\")\n",
    "\n",
    "    m2 = central_moment(x, 2)\n",
    "    if not np.isfinite(m2) or m2 <= 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    m3 = central_moment(x, 3)\n",
    "    m4 = central_moment(x, 4)\n",
    "\n",
    "    s = m3 / (m2 ** 1.5)\n",
    "    k = m4 / (m2**2)\n",
    "    return float(s), float(k)\n",
    "\n",
    "\n",
    "def jarque_bera_test(x: np.ndarray) -> dict:\n",
    "    # Jarque–Bera normality test (asymptotic χ² with df=2).\n",
    "    x = _to_1d_finite(x)\n",
    "    n = int(x.size)\n",
    "    if n < 3:\n",
    "        raise ValueError(\"Need at least 3 finite values\")\n",
    "\n",
    "    s, k = sample_skewness_kurtosis(x)\n",
    "    if not (np.isfinite(s) and np.isfinite(k)):\n",
    "        return {\n",
    "            \"n\": n,\n",
    "            \"statistic\": float(\"nan\"),\n",
    "            \"p_value\": float(\"nan\"),\n",
    "            \"skewness\": float(\"nan\"),\n",
    "            \"kurtosis\": float(\"nan\"),\n",
    "            \"excess_kurtosis\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    jb = (n / 6.0) * (s**2 + ((k - 3.0) ** 2) / 4.0)\n",
    "\n",
    "    # Under H0: JB ~ chi-square(df=2) asymptotically.\n",
    "    # For df=2, survival function is exp(-x/2).\n",
    "    p_value = float(np.exp(-0.5 * jb))\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"statistic\": float(jb),\n",
    "        \"p_value\": p_value,\n",
    "        \"skewness\": float(s),\n",
    "        \"kurtosis\": float(k),\n",
    "        \"excess_kurtosis\": float(k - 3.0),\n",
    "        \"skew_component\": float((n / 6.0) * (s**2)),\n",
    "        \"kurtosis_component\": float((n / 24.0) * ((k - 3.0) ** 2)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04418103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on a few synthetic samples\n",
    "\n",
    "samples = {\n",
    "    \"normal\": rng.standard_normal(500),\n",
    "    \"t(df=3)\": rng.standard_t(df=3, size=500),\n",
    "    \"exponential\": rng.exponential(scale=1.0, size=500),\n",
    "}\n",
    "\n",
    "for name, x in samples.items():\n",
    "    res = jarque_bera_test(x)\n",
    "    print(\n",
    "        f\"{name:>12s} | JB={res['statistic']:.3f} | p={res['p_value']:.4f} | \"\n",
    "        f\"skew={res['skewness']:.3f} | excess_kurt={res['excess_kurtosis']:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4efc61",
   "metadata": {},
   "source": [
    "## 3) How to interpret the result\n",
    "\n",
    "Pick a significance level $\\alpha$ (often 0.05).\n",
    "\n",
    "- If **p-value ≤ α**: reject $H_0$ → the sample shows statistically significant evidence of **non-normality** (in skewness and/or kurtosis).\n",
    "- If **p-value > α**: fail to reject $H_0$ → you do *not* have enough evidence (from skewness/kurtosis) to say it’s non-normal.\n",
    "\n",
    "Two important interpretation details:\n",
    "\n",
    "1. **JB is an asymptotic test.** For small samples, the χ² approximation can be rough.\n",
    "2. **With large n, tiny deviations become significant.** In big datasets it’s common to reject normality even when the deviation is practically irrelevant.\n",
    "\n",
    "So: always pair the p-value with **effect size diagnostics** (skewness, excess kurtosis) and plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_df2_pdf(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.zeros_like(x)\n",
    "    mask = x >= 0\n",
    "    out[mask] = 0.5 * np.exp(-0.5 * x[mask])\n",
    "    return out\n",
    "\n",
    "\n",
    "def chi2_df2_sf(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.ones_like(x)\n",
    "    mask = x >= 0\n",
    "    out[mask] = np.exp(-0.5 * x[mask])\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_jb_pvalue(jb: float, *, title: str = \"JB vs χ²(2) (right-tail p-value)\") -> go.Figure:\n",
    "    xmax = float(max(20.0, jb * 1.5))\n",
    "    xs = np.linspace(0.0, xmax, 600)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=xs, y=chi2_df2_pdf(xs), mode=\"lines\", name=\"χ²(2) PDF\"))\n",
    "\n",
    "    xs_tail = xs[xs >= jb]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.r_[xs_tail, xs_tail[::-1]],\n",
    "            y=np.r_[chi2_df2_pdf(xs_tail), np.zeros_like(xs_tail)],\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(31, 119, 180, 0.2)\",\n",
    "            line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "            hoverinfo=\"skip\",\n",
    "            name=\"p-value area\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    p = float(chi2_df2_sf(jb))\n",
    "    fig.add_vline(x=jb, line_width=2, line_dash=\"dash\", line_color=\"firebrick\")\n",
    "    fig.update_layout(\n",
    "        title=f\"{title}<br><sup>JB={jb:.3f}, p-value={p:.4f}</sup>\",\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"density\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0.0),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "res_norm = jarque_bera_test(samples[\"normal\"])\n",
    "plot_jb_pvalue(res_norm[\"statistic\"], title=\"Normal sample: JB null distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) The null distribution: simulated JB values under true normality\n",
    "\n",
    "def simulate_jb_under_normal(*, n: int, m: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    jb_vals = np.empty(m, dtype=float)\n",
    "    for i in range(m):\n",
    "        x = rng.standard_normal(n)\n",
    "        jb_vals[i] = jarque_bera_test(x)[\"statistic\"]\n",
    "    return jb_vals\n",
    "\n",
    "n = 200\n",
    "m = 5000\n",
    "jb_vals = simulate_jb_under_normal(n=n, m=m, rng=rng)\n",
    "\n",
    "xs = np.linspace(0.0, np.quantile(jb_vals, 0.995), 600)\n",
    "\n",
    "alpha = 0.05\n",
    "crit = float(-2.0 * np.log(alpha))  # df=2\n",
    "rejection_rate = float(np.mean(jb_vals >= crit))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=jb_vals, nbinsx=60, histnorm=\"probability density\", name=\"simulated JB\"))\n",
    "fig.add_trace(go.Scatter(x=xs, y=chi2_df2_pdf(xs), mode=\"lines\", name=\"χ²(2) PDF\"))\n",
    "fig.add_vline(x=crit, line_dash=\"dash\", line_color=\"firebrick\")\n",
    "fig.update_layout(\n",
    "    title=(\n",
    "        \"JB under H₀ (true normality)\" +\n",
    "        f\"<br><sup>n={n}, simulations={m}, α={alpha} ⇒ critical={crit:.3f}, empirical rejection≈{rejection_rate:.3f}</sup>\"\n",
    "    ),\n",
    "    xaxis_title=\"JB statistic\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154df3d",
   "metadata": {},
   "source": [
    "## 5) What JB is “measuring” (geometry in skewness/kurtosis space)\n",
    "\n",
    "Because\n",
    "\n",
    "$$\n",
    "\\mathrm{JB} = \\frac{n}{6}S^2 + \\frac{n}{24}(K-3)^2,\n",
    "$$\n",
    "\n",
    "JB is basically a **scaled squared distance** from the normal point $(S, K-3) = (0, 0)$.\n",
    "\n",
    "- Moving away from 0 skewness increases JB.\n",
    "- Moving away from 0 excess kurtosis increases JB.\n",
    "- The same p-value corresponds (approximately) to an **ellipse** in $(S, K-3)$ space.\n",
    "\n",
    "The plot below shows this idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A contour view of JB as a function of (skewness, excess kurtosis)\n",
    "\n",
    "n_for_geometry = 200\n",
    "s_grid = np.linspace(-1.2, 1.2, 241)\n",
    "e_grid = np.linspace(-2.5, 2.5, 241)  # excess kurtosis\n",
    "S, E = np.meshgrid(s_grid, e_grid)\n",
    "JB_grid = (n_for_geometry / 6.0) * (S**2 + (E**2) / 4.0)\n",
    "\n",
    "alpha_levels = [0.10, 0.05, 0.01]\n",
    "crit_levels = {a: float(-2.0 * np.log(a)) for a in alpha_levels}  # df=2\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=s_grid,\n",
    "        y=e_grid,\n",
    "        z=JB_grid,\n",
    "        contours=dict(coloring=\"lines\", showlabels=True),\n",
    "        line=dict(width=1),\n",
    "        showscale=False,\n",
    "        hovertemplate=\"skew=%{x:.3f}<br>excess kurt=%{y:.3f}<br>JB≈%{z:.3f}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "for a, q in crit_levels.items():\n",
    "    rhs = (6.0 * q) / n_for_geometry\n",
    "    s_line = np.linspace(-np.sqrt(rhs), np.sqrt(rhs), 400)\n",
    "    e_line = 2.0 * np.sqrt(np.maximum(0.0, rhs - s_line**2))\n",
    "    fig.add_trace(go.Scatter(x=s_line, y=e_line, mode=\"lines\", name=f\"α={a} boundary\"))\n",
    "    fig.add_trace(go.Scatter(x=s_line, y=-e_line, mode=\"lines\", showlegend=False))\n",
    "\n",
    "pts = []\n",
    "for name, x in samples.items():\n",
    "    r = jarque_bera_test(x)\n",
    "    pts.append((name, r[\"skewness\"], r[\"excess_kurtosis\"]))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[p[1] for p in pts],\n",
    "        y=[p[2] for p in pts],\n",
    "        mode=\"markers+text\",\n",
    "        text=[p[0] for p in pts],\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(size=10, color=\"firebrick\"),\n",
    "        name=\"example samples\",\n",
    "        hovertemplate=\"%{text}<br>skew=%{x:.3f}<br>excess kurt=%{y:.3f}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=(\n",
    "        \"JB statistic as a function of skewness and excess kurtosis\"\n",
    "        + f\"<br><sup>n={n_for_geometry}; JB contours + approximate rejection boundaries</sup>\"\n",
    "    ),\n",
    "    xaxis_title=\"skewness S\",\n",
    "    yaxis_title=\"excess kurtosis (K − 3)\",\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddac65d",
   "metadata": {},
   "source": [
    "## 6) Visual intuition: histogram + Q–Q plot\n",
    "\n",
    "A good workflow is:\n",
    "\n",
    "1. Run JB (get **p-value**, **skewness**, **excess kurtosis**)\n",
    "2. Inspect a **standardized histogram** with a normal overlay\n",
    "3. Inspect a **Q–Q plot** (systematic curvature indicates non-normal tails; S-shaped patterns indicate skew)\n",
    "\n",
    "Below we compare three distributions after z-scoring (mean 0, std 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_pdf(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-0.5 * x**2)\n",
    "\n",
    "\n",
    "def norm_ppf(p: np.ndarray) -> np.ndarray:\n",
    "    # Approximate inverse CDF of the standard normal distribution.\n",
    "    # Vectorized rational approximation (Acklam). Returns NaN for p outside (0, 1).\n",
    "\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    x = np.full_like(p, np.nan, dtype=float)\n",
    "\n",
    "    a = np.array(\n",
    "        [\n",
    "            -39.69683028665376,\n",
    "            220.9460984245205,\n",
    "            -275.9285104469687,\n",
    "            138.3577518672690,\n",
    "            -30.66479806614716,\n",
    "            2.506628277459239,\n",
    "        ]\n",
    "    )\n",
    "    b = np.array(\n",
    "        [\n",
    "            -54.47609879822406,\n",
    "            161.5858368580409,\n",
    "            -155.6989798598866,\n",
    "            66.80131188771972,\n",
    "            -13.28068155288572,\n",
    "        ]\n",
    "    )\n",
    "    c = np.array(\n",
    "        [\n",
    "            -0.007784894002430293,\n",
    "            -0.3223964580411365,\n",
    "            -2.400758277161838,\n",
    "            -2.549732539343734,\n",
    "            4.374664141464968,\n",
    "            2.938163982698783,\n",
    "        ]\n",
    "    )\n",
    "    d = np.array(\n",
    "        [\n",
    "            0.007784695709041462,\n",
    "            0.3224671290700398,\n",
    "            2.445134137142996,\n",
    "            3.754408661907416,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plow = 0.02425\n",
    "    phigh = 1.0 - plow\n",
    "\n",
    "    valid = (p > 0.0) & (p < 1.0)\n",
    "\n",
    "    mask = valid & (p < plow)\n",
    "    if np.any(mask):\n",
    "        q = np.sqrt(-2.0 * np.log(p[mask]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1.0)\n",
    "        x[mask] = -(num / den)\n",
    "\n",
    "    mask = valid & (p >= plow) & (p <= phigh)\n",
    "    if np.any(mask):\n",
    "        q = p[mask] - 0.5\n",
    "        r = q * q\n",
    "        num = (((((a[0] * r + a[1]) * r + a[2]) * r + a[3]) * r + a[4]) * r + a[5]) * q\n",
    "        den = (((((b[0] * r + b[1]) * r + b[2]) * r + b[3]) * r + b[4]) * r + 1.0)\n",
    "        x[mask] = num / den\n",
    "\n",
    "    mask = valid & (p > phigh)\n",
    "    if np.any(mask):\n",
    "        q = np.sqrt(-2.0 * np.log(1.0 - p[mask]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1.0)\n",
    "        x[mask] = num / den\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def zscore(x: np.ndarray) -> np.ndarray:\n",
    "    x = _to_1d_finite(x)\n",
    "    mu = x.mean()\n",
    "    sigma = x.std(ddof=0)\n",
    "    return (x - mu) / sigma\n",
    "\n",
    "\n",
    "labels = list(samples.keys())\n",
    "z_samples = {k: zscore(v) for k, v in samples.items()}\n",
    "results = {k: jarque_bera_test(v) for k, v in z_samples.items()}\n",
    "\n",
    "subplot_titles = [\n",
    "    f\"{k}<br><sup>JB={results[k]['statistic']:.2f}, p={results[k]['p_value']:.4f}</sup>\" for k in labels\n",
    "]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=3,\n",
    "    subplot_titles=subplot_titles * 2,\n",
    "    vertical_spacing=0.12,\n",
    "    row_heights=[0.55, 0.45],\n",
    ")\n",
    "\n",
    "x_grid = np.linspace(-4.0, 4.0, 500)\n",
    "\n",
    "for j, k in enumerate(labels, start=1):\n",
    "    z = z_samples[k]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=z, nbinsx=45, histnorm=\"probability density\", name=f\"{k} hist\", showlegend=False),\n",
    "        row=1,\n",
    "        col=j,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_grid,\n",
    "            y=norm_pdf(x_grid),\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\"),\n",
    "            name=\"N(0,1)\",\n",
    "            showlegend=(j == 1),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=j,\n",
    "    )\n",
    "\n",
    "    z_sorted = np.sort(z)\n",
    "    n = z_sorted.size\n",
    "    p = (np.arange(1, n + 1) - 0.5) / n\n",
    "    q = norm_ppf(p)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=q, y=z_sorted, mode=\"markers\", marker=dict(size=4), name=f\"{k} QQ\", showlegend=False),\n",
    "        row=2,\n",
    "        col=j,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[q.min(), q.max()],\n",
    "            y=[q.min(), q.max()],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"firebrick\", dash=\"dash\"),\n",
    "            name=\"y=x\",\n",
    "            showlegend=(j == 1),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=j,\n",
    "    )\n",
    "\n",
    "for j in range(1, 4):\n",
    "    fig.update_xaxes(title_text=\"z\", row=1, col=j)\n",
    "    fig.update_yaxes(title_text=\"density\", row=1, col=j)\n",
    "\n",
    "    fig.update_xaxes(title_text=\"theoretical quantiles (N(0,1))\", row=2, col=j)\n",
    "    fig.update_yaxes(title_text=\"sample quantiles (z-scored)\", row=2, col=j)\n",
    "\n",
    "fig.update_layout(title=\"Distributions (z-scored): histogram + Q–Q plot\", height=850)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4615aa",
   "metadata": {},
   "source": [
    "## 7) Practical notes & pitfalls\n",
    "\n",
    "- **Asymptotic nature**: JB relies on a large-sample χ² approximation. For small $n$, use JB as a *hint* and rely more on plots or small-sample tests.\n",
    "- **Outliers**: skewness and kurtosis are highly sensitive to outliers → a single extreme value can drive rejection.\n",
    "- **Power and misspecification**: JB is tuned to skewness/kurtosis. It can have low power against other deviations (e.g., some symmetric mixtures).\n",
    "- **Big n**: with enough data, *any* small deviation becomes statistically significant. Always check whether non-normality is *practically* important.\n",
    "\n",
    "A good “diagnostics bundle” is: **JB p-value + (skewness, excess kurtosis) + histogram + Q–Q plot**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74a8f1",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Simulate JB rejection rates under $H_0$ for different sample sizes ($n=20,50,200,1000$). How quickly does the χ² approximation settle?\n",
    "2. Build two non-normal samples with skewness near 0 and kurtosis near 3 (e.g. symmetric mixtures) and see whether JB detects them.\n",
    "3. Apply JB to regression residuals from a simple linear model with (a) Gaussian noise and (b) heavy-tailed noise. Compare with Q–Q plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c591f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Jarque, C. M., & Bera, A. K. (1980). *Efficient tests for normality, homoscedasticity and serial independence of regression residuals.*\n",
    "- Jarque, C. M., & Bera, A. K. (1987). *A test for normality of observations and regression residuals.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
