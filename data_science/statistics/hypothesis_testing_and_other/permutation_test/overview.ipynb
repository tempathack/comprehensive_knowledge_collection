{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Test (Randomization Test)\n",
    "\n",
    "Permutation tests are **nonparametric** hypothesis tests that build a null distribution directly from your data by **shuffling labels**.\n",
    "They’re especially useful when you don’t trust parametric assumptions (normality, equal variances), but you *can* assume the data are **exchangeable under the null**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain what a permutation test is (and what it is not)\n",
    "- choose a sensible test statistic for your question\n",
    "- implement a two-sample permutation test from scratch with NumPy\n",
    "- interpret the permutation distribution and the p-value\n",
    "- adapt the randomization scheme for **paired** data (sign-flip)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. What problem does it solve?\n",
    "2. The core assumption: exchangeability\n",
    "3. Two-sample permutation test (NumPy from scratch)\n",
    "4. Visualizing the permutation distribution + p-value\n",
    "5. Choosing the statistic (mean vs median)\n",
    "6. Paired designs (sign-flip permutation test)\n",
    "7. Interpretation, pitfalls, and diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) What problem does it solve?\n",
    "\n",
    "You often have data like:\n",
    "\n",
    "- **A/B tests**: did variant B increase average revenue vs A?\n",
    "- **two conditions**: did a new model reduce latency vs the old one?\n",
    "- **two groups**: do group 1 and group 2 differ in some outcome?\n",
    "\n",
    "A permutation test answers:\n",
    "\n",
    "> **If the null hypothesis were true**, how surprising is the difference (or association) we observed?\n",
    "\n",
    "Instead of assuming a theoretical sampling distribution (like Student’s t), we **simulate the null** by repeatedly re-labeling the observed data in a way that would be valid under $H_0$.\n",
    "\n",
    "This makes permutation tests a great fit when:\n",
    "\n",
    "- sample sizes are small\n",
    "- distributions are skewed / heavy-tailed\n",
    "- you want a test for a custom statistic (median difference, trimmed mean, correlation, accuracy, …)\n",
    "\n",
    "Permutation tests are related to **randomization tests**: if treatment assignment was truly random, the permutation test is (conditionally) exact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) The core assumption: exchangeability\n",
    "\n",
    "A permutation test is valid when the data are **exchangeable under the null**.\n",
    "\n",
    "Two-sample case (independent groups):\n",
    "\n",
    "- You observe two sets: $x = (x_1, \\dots, x_{n_x})$ and $y = (y_1, \\dots, y_{n_y})$.\n",
    "- Null hypothesis (common version):\n",
    "\n",
    "$$H_0: x \\text{ and } y \\text{ come from the same distribution}$$\n",
    "\n",
    "If $H_0$ is true, then the labels “x-group” and “y-group” don’t matter: any reshuffling of the labels is just as plausible.\n",
    "\n",
    "What can break exchangeability?\n",
    "\n",
    "- dependence between observations (time series, clustered data)\n",
    "- confounding in observational data (labels carry information not explained by chance)\n",
    "- a design with blocks/strata where only certain permutations are allowed\n",
    "\n",
    "The *randomization scheme* must match the *data collection design*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Two-sample permutation test: the recipe\n",
    "\n",
    "### Choose a test statistic\n",
    "\n",
    "Pick a scalar statistic that answers your question, for example:\n",
    "\n",
    "- difference in means: $T(x, y) = \\bar{x} - \\bar{y}$\n",
    "- difference in medians\n",
    "- difference in trimmed means\n",
    "- KS distance, correlation, etc.\n",
    "\n",
    "Compute the observed statistic:\n",
    "\n",
    "$$T_\\text{obs} = T(x, y)$$\n",
    "\n",
    "### Build the permutation (null) distribution\n",
    "\n",
    "1. Pool the values: $z = (x, y)$\n",
    "2. Repeatedly **permute** $z$ and split back into two groups of sizes $n_x$ and $n_y$\n",
    "3. Recompute the statistic each time: $T_1, \\dots, T_B$\n",
    "\n",
    "This yields a Monte Carlo approximation to the null distribution $T \\mid H_0$.\n",
    "\n",
    "### Turn it into a p-value\n",
    "\n",
    "- two-sided: $p \\approx P(|T| \\ge |T_\\text{obs}| \\mid H_0)$\n",
    "- greater: $p \\approx P(T \\ge T_\\text{obs} \\mid H_0)$\n",
    "- less: $p \\approx P(T \\le T_\\text{obs} \\mid H_0)$\n",
    "\n",
    "A common finite-sample correction (avoids returning exactly 0):\n",
    "\n",
    "$$\\hat p = \\frac{\\#\\{\\text{as-extreme permutations}\\} + 1}{B + 1}$$\n",
    "\n",
    "Note: with $B$ permutations, the smallest possible p-value is $1/(B+1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_1d_float_array(x, name):\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(f\"{name} is empty\")\n",
    "    if np.isnan(x).any():\n",
    "        raise ValueError(f\"{name} contains NaN\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def _permutation_p_value(perm_stats, observed, alternative=\"two-sided\"):\n",
    "    perm_stats = np.asarray(perm_stats, dtype=float).reshape(-1)\n",
    "    observed = float(observed)\n",
    "\n",
    "    if alternative == \"two-sided\":\n",
    "        extreme = np.sum(np.abs(perm_stats) >= abs(observed))\n",
    "    elif alternative == \"greater\":\n",
    "        extreme = np.sum(perm_stats >= observed)\n",
    "    elif alternative == \"less\":\n",
    "        extreme = np.sum(perm_stats <= observed)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be one of: 'two-sided', 'greater', 'less'\")\n",
    "\n",
    "    # +1 correction to avoid returning exactly 0\n",
    "    return (extreme + 1) / (perm_stats.size + 1)\n",
    "\n",
    "\n",
    "def permutation_test_two_sample(\n",
    "    x,\n",
    "    y,\n",
    "    statistic_fn=None,\n",
    "    alternative=\"two-sided\",\n",
    "    n_permutations=10_000,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"Two-sample permutation test (independent samples).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like\n",
    "        The two groups.\n",
    "    statistic_fn : callable or None\n",
    "        Function with signature statistic_fn(x, y) -> float.\n",
    "        Defaults to difference in means (x.mean() - y.mean()).\n",
    "    alternative : {'two-sided', 'greater', 'less'}\n",
    "        Tail(s) used for the p-value.\n",
    "    n_permutations : int\n",
    "        Number of Monte Carlo permutations.\n",
    "    seed : int\n",
    "        RNG seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : dict\n",
    "        Keys: 'stat_obs', 'p_value', 'perm_stats', 'alternative', 'n_permutations'.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _as_1d_float_array(x, \"x\")\n",
    "    y = _as_1d_float_array(y, \"y\")\n",
    "\n",
    "    if not isinstance(n_permutations, int) or n_permutations <= 0:\n",
    "        raise ValueError(\"n_permutations must be a positive integer\")\n",
    "\n",
    "    if statistic_fn is None:\n",
    "        statistic_fn = lambda a, b: a.mean() - b.mean()\n",
    "\n",
    "    stat_obs = float(statistic_fn(x, y))\n",
    "\n",
    "    pooled = np.concatenate([x, y])\n",
    "    n_x = x.size\n",
    "    n_total = pooled.size\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    perm_stats = np.empty(n_permutations, dtype=float)\n",
    "    for i in range(n_permutations):\n",
    "        idx = rng_local.permutation(n_total)\n",
    "        x_star = pooled[idx[:n_x]]\n",
    "        y_star = pooled[idx[n_x:]]\n",
    "        perm_stats[i] = statistic_fn(x_star, y_star)\n",
    "\n",
    "    p_value = _permutation_p_value(perm_stats, stat_obs, alternative=alternative)\n",
    "\n",
    "    return {\n",
    "        \"stat_obs\": stat_obs,\n",
    "        \"p_value\": p_value,\n",
    "        \"perm_stats\": perm_stats,\n",
    "        \"alternative\": alternative,\n",
    "        \"n_permutations\": n_permutations,\n",
    "    }\n",
    "\n",
    "\n",
    "def permutation_test_paired_sign_flip(\n",
    "    before,\n",
    "    after,\n",
    "    statistic_fn=None,\n",
    "    alternative=\"two-sided\",\n",
    "    n_permutations=10_000,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"Paired permutation test using sign flips on within-pair differences.\n",
    "\n",
    "    Under H0 (no systematic effect), the sign of each pairwise difference is arbitrary.\n",
    "\n",
    "    Default statistic: mean(after - before).\n",
    "    \"\"\"\n",
    "\n",
    "    before = _as_1d_float_array(before, \"before\")\n",
    "    after = _as_1d_float_array(after, \"after\")\n",
    "\n",
    "    if before.size != after.size:\n",
    "        raise ValueError(\"before and after must have the same length\")\n",
    "\n",
    "    if not isinstance(n_permutations, int) or n_permutations <= 0:\n",
    "        raise ValueError(\"n_permutations must be a positive integer\")\n",
    "\n",
    "    d = after - before\n",
    "\n",
    "    if statistic_fn is None:\n",
    "        statistic_fn = np.mean\n",
    "\n",
    "    stat_obs = float(statistic_fn(d))\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    perm_stats = np.empty(n_permutations, dtype=float)\n",
    "    for i in range(n_permutations):\n",
    "        signs = rng_local.choice(np.array([-1.0, 1.0]), size=d.size)\n",
    "        perm_stats[i] = statistic_fn(signs * d)\n",
    "\n",
    "    p_value = _permutation_p_value(perm_stats, stat_obs, alternative=alternative)\n",
    "\n",
    "    return {\n",
    "        \"stat_obs\": stat_obs,\n",
    "        \"p_value\": p_value,\n",
    "        \"perm_stats\": perm_stats,\n",
    "        \"alternative\": alternative,\n",
    "        \"n_permutations\": n_permutations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Example: A/B test on skewed data\n",
    "\n",
    "Imagine an A/B test where the metric is **user spend**.\n",
    "\n",
    "- Spend is usually **right-skewed** (many small values, a few huge ones).\n",
    "- You might still care about **average** spend, but you may not want to assume normality.\n",
    "\n",
    "We’ll simulate two groups and test whether treatment increases the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_control = 35\n",
    "n_treatment = 35\n",
    "\n",
    "control = rng.lognormal(mean=0.0, sigma=0.8, size=n_control)\n",
    "treatment = rng.lognormal(mean=0.4, sigma=0.8, size=n_treatment)\n",
    "\n",
    "{\n",
    "    \"control_mean\": control.mean(),\n",
    "    \"treatment_mean\": treatment.mean(),\n",
    "    \"control_median\": np.median(control),\n",
    "    \"treatment_median\": np.median(treatment),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Raw values (skewed)\", \"log1p(values)\"),\n",
    ")\n",
    "\n",
    "for name, values, color in [\n",
    "    (\"Control\", control, \"#1f77b4\"),\n",
    "    (\"Treatment\", treatment, \"#ff7f0e\"),\n",
    "]:\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=values,\n",
    "            name=name,\n",
    "            box_visible=True,\n",
    "            meanline_visible=True,\n",
    "            points=\"all\",\n",
    "            jitter=0.25,\n",
    "            marker_color=color,\n",
    "            legendgroup=name,\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=np.log1p(values),\n",
    "            name=name,\n",
    "            box_visible=True,\n",
    "            meanline_visible=True,\n",
    "            points=\"all\",\n",
    "            jitter=0.25,\n",
    "            marker_color=color,\n",
    "            legendgroup=name,\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"A/B data: distribution of outcomes\",\n",
    "    violingap=0.25,\n",
    "    violinmode=\"group\",\n",
    ")\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"log1p(value)\", row=1, col=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the permutation test (difference in means)\n",
    "\n",
    "We’ll use:\n",
    "\n",
    "$$T(x, y) = \\bar{x} - \\bar{y}$$\n",
    "\n",
    "Here we interpret $x$ as **treatment** and $y$ as **control**, so a positive statistic means treatment has a higher mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_mean_diff = lambda x, y: x.mean() - y.mean()\n",
    "\n",
    "result_mean = permutation_test_two_sample(\n",
    "    treatment,\n",
    "    control,\n",
    "    statistic_fn=stat_mean_diff,\n",
    "    alternative=\"two-sided\",\n",
    "    n_permutations=20_000,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "result_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_obs = result_mean[\"stat_obs\"]\n",
    "p_value = result_mean[\"p_value\"]\n",
    "\n",
    "alpha = 0.05\n",
    "{\n",
    "    \"observed_mean_diff\": t_obs,\n",
    "    \"p_value\": p_value,\n",
    "    \"reject_at_0.05\": p_value <= alpha,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_perm = result_mean[\"perm_stats\"]\n",
    "mask_extreme = np.abs(t_perm) >= abs(t_obs)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=t_perm[~mask_extreme],\n",
    "        nbinsx=60,\n",
    "        name=\"not as extreme\",\n",
    "        marker_color=\"lightgray\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=t_perm[mask_extreme],\n",
    "        nbinsx=60,\n",
    "        name=\"as / more extreme\",\n",
    "        marker_color=\"#d62728\",\n",
    "        opacity=0.85,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=t_obs,\n",
    "    line_width=3,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Permutation (null) distribution of Δ mean\",\n",
    "    xaxis_title=\"Δ mean (treatment - control)\",\n",
    "    yaxis_title=\"count\",\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=t_obs,\n",
    "    y=0.98,\n",
    "    xref=\"x\",\n",
    "    yref=\"paper\",\n",
    "    text=f\"observed Δ={t_obs:.3f}<br>p={p_value:.4f}\",\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    ax=40,\n",
    "    ay=-30,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme = np.abs(t_perm) >= abs(t_obs)\n",
    "\n",
    "k = np.arange(1, t_perm.size + 1)\n",
    "# +1 / (k+1) is the same finite-sample correction used for the final p-value\n",
    "p_running = (np.cumsum(extreme) + 1) / (k + 1)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=k, y=p_running, mode=\"lines\", name=\"running p-value\"))\n",
    "fig.add_hline(y=p_value, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Monte Carlo p-value convergence\",\n",
    "    xaxis_title=\"# permutations used\",\n",
    "    yaxis_title=\"p-value estimate\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Choosing the statistic: mean vs median\n",
    "\n",
    "Permutation tests let you choose *any* statistic — which is powerful, but also a responsibility.\n",
    "\n",
    "- The **mean** is sensitive to outliers (which might be exactly what you care about for revenue).\n",
    "- The **median** is more robust (focuses on the “typical” user).\n",
    "\n",
    "Different questions → different statistics.\n",
    "\n",
    "Below we compute both, using the same permutation idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_median_diff = lambda x, y: np.median(x) - np.median(y)\n",
    "\n",
    "result_median = permutation_test_two_sample(\n",
    "    treatment,\n",
    "    control,\n",
    "    statistic_fn=stat_median_diff,\n",
    "    alternative=\"two-sided\",\n",
    "    n_permutations=20_000,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "{\n",
    "    \"mean_diff\": {\"stat_obs\": result_mean[\"stat_obs\"], \"p_value\": result_mean[\"p_value\"]},\n",
    "    \"median_diff\": {\"stat_obs\": result_median[\"stat_obs\"], \"p_value\": result_median[\"p_value\"]},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"mean diff\", \"median diff\"]\n",
    "p_vals = [result_mean[\"p_value\"], result_median[\"p_value\"]]\n",
    "\n",
    "fig = go.Figure(\n",
    "    go.Bar(\n",
    "        x=labels,\n",
    "        y=p_vals,\n",
    "        text=[f\"{p:.4f}\" for p in p_vals],\n",
    "        textposition=\"outside\",\n",
    "    )\n",
    ")\n",
    "fig.add_hline(y=0.05, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(\n",
    "    title=\"p-values depend on the chosen statistic\",\n",
    "    yaxis_title=\"p-value\",\n",
    ")\n",
    "fig.update_yaxes(range=[0, max(0.06, max(p_vals) * 1.2)])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_obs_med = result_median[\"stat_obs\"]\n",
    "p_med = result_median[\"p_value\"]\n",
    "t_perm_med = result_median[\"perm_stats\"]\n",
    "mask_extreme_med = np.abs(t_perm_med) >= abs(t_obs_med)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=t_perm_med[~mask_extreme_med],\n",
    "        nbinsx=60,\n",
    "        name=\"not as extreme\",\n",
    "        marker_color=\"lightgray\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=t_perm_med[mask_extreme_med],\n",
    "        nbinsx=60,\n",
    "        name=\"as / more extreme\",\n",
    "        marker_color=\"#d62728\",\n",
    "        opacity=0.85,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(x=t_obs_med, line_width=3, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Permutation (null) distribution of Δ median\",\n",
    "    xaxis_title=\"Δ median (treatment - control)\",\n",
    "    yaxis_title=\"count\",\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=t_obs_med,\n",
    "    y=0.98,\n",
    "    xref=\"x\",\n",
    "    yref=\"paper\",\n",
    "    text=f\"observed Δ={t_obs_med:.3f}<br>p={p_med:.4f}\",\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    ax=40,\n",
    "    ay=-30,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Paired designs: sign-flip permutation test\n",
    "\n",
    "If your data are **paired** (same user before/after, matched pairs, repeated measures), you can’t shuffle labels across all observations.\n",
    "\n",
    "A common paired null is:\n",
    "\n",
    "$$H_0: \\text{the treatment has no systematic effect}$$\n",
    "\n",
    "Let differences be $d_i = \\text{after}_i - \\text{before}_i$.\n",
    "\n",
    "Under $H_0$, the sign of each $d_i$ is arbitrary (positive or negative is equally likely), so we create the null distribution by **randomly flipping signs**:\n",
    "\n",
    "- draw random signs $s_i \\in \\{-1, +1\\}$\n",
    "- compute $T(s \\odot d)$, e.g. the mean\n",
    "\n",
    "This is the right permutation scheme for paired data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "before = rng.normal(50, 10, size=n)\n",
    "# Treatment tends to increase the metric by ~3 on average, but with noise\n",
    "after = before + rng.normal(3.0, 8.0, size=n)\n",
    "\n",
    "paired_result = permutation_test_paired_sign_flip(\n",
    "    before,\n",
    "    after,\n",
    "    alternative=\"two-sided\",\n",
    "    n_permutations=20_000,\n",
    "    seed=321,\n",
    ")\n",
    "\n",
    "paired_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = after - before\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Paired observations\", \"Within-pair differences\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=before, y=after, mode=\"markers\", name=\"pairs\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "lo = min(before.min(), after.min())\n",
    "hi = max(before.max(), after.max())\n",
    "line = np.linspace(lo, hi, 100)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=line, y=line, mode=\"lines\", line=dict(dash=\"dash\"), name=\"y=x\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Histogram(x=diff, nbinsx=30, name=\"after - before\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"before\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"after\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"after - before\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Paired data view\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_obs_p = paired_result[\"stat_obs\"]\n",
    "p_p = paired_result[\"p_value\"]\n",
    "t_perm_p = paired_result[\"perm_stats\"]\n",
    "mask_extreme_p = np.abs(t_perm_p) >= abs(t_obs_p)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=t_perm_p[~mask_extreme_p],\n",
    "        nbinsx=60,\n",
    "        name=\"not as extreme\",\n",
    "        marker_color=\"lightgray\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=t_perm_p[mask_extreme_p],\n",
    "        nbinsx=60,\n",
    "        name=\"as / more extreme\",\n",
    "        marker_color=\"#d62728\",\n",
    "        opacity=0.85,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(x=t_obs_p, line_width=3, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Sign-flip permutation distribution (paired mean difference)\",\n",
    "    xaxis_title=\"mean(after - before)\",\n",
    "    yaxis_title=\"count\",\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=t_obs_p,\n",
    "    y=0.98,\n",
    "    xref=\"x\",\n",
    "    yref=\"paper\",\n",
    "    text=f\"observed={t_obs_p:.3f}<br>p={p_p:.4f}\",\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    ax=40,\n",
    "    ay=-30,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Interpretation: what the result means\n",
    "\n",
    "A permutation test gives you:\n",
    "\n",
    "- an **observed statistic** (e.g., $\\bar{x} - \\bar{y}$)\n",
    "- a **null distribution** of that statistic (generated by re-labeling)\n",
    "- a **p-value**: how often a null world produces a statistic at least as extreme\n",
    "\n",
    "### What the p-value means\n",
    "\n",
    "If $p = 0.02$ (two-sided), a precise reading is:\n",
    "\n",
    "> *Assuming the null hypothesis and the permutation scheme are valid*, only about **2%** of random labelings would produce a difference at least as extreme as the one we observed.\n",
    "\n",
    "### What the p-value does NOT mean\n",
    "\n",
    "- it is **not** $P(H_0 \\mid \\text{data})$\n",
    "- it is **not** the probability the result happened “by chance” in some vague sense\n",
    "- it does **not** measure effect size (always report the effect itself)\n",
    "\n",
    "### Decision rule\n",
    "\n",
    "Choose a significance level $\\alpha$ (commonly 0.05).\n",
    "\n",
    "- if $p \\le \\alpha$: reject $H_0$ (evidence against the null)\n",
    "- if $p > \\alpha$: fail to reject $H_0$ (not enough evidence)\n",
    "\n",
    "Failing to reject is not the same as “proving no effect” — it may just mean the test is underpowered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls + diagnostics\n",
    "\n",
    "- **Match the permutation to the design**: paired data needs sign-flips (or within-pair swaps); blocked experiments need restricted permutations.\n",
    "- **Exchangeability is the real assumption**: permutation tests aren’t automatically valid for observational data with confounding.\n",
    "- **Pick the statistic before peeking**: changing the statistic after seeing the data is p-hacking.\n",
    "- **Monte Carlo error**: two runs with different seeds can give slightly different p-values; increase `n_permutations` to reduce noise.\n",
    "- **p-value resolution**: with `B` permutations, the smallest possible p-value is `1/(B+1)`.\n",
    "- **Report more than p**: include the observed effect (and ideally uncertainty / practical significance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement a **permutation test for correlation**: keep `x` fixed and permute `y`, using $T=\\mathrm{corr}(x,y)$.\n",
    "2. Implement a **stratified** two-sample permutation test where labels are only permuted *within* strata.\n",
    "3. Compare mean-difference permutation tests on:\n",
    "   - normal data\n",
    "   - heavy-tailed data\n",
    "   What changes?\n",
    "4. Increase `n_permutations` and plot how the running p-value stabilizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Fisher: *The Design of Experiments* (randomization tests)\n",
    "- Good: *Permutation, Parametric and Bootstrap Tests of Hypotheses*\n",
    "- Ernst (2004): “Permutation Methods: A Basis for Exact Inference”\n",
    "- Manly: *Randomization, Bootstrap and Monte Carlo Methods in Biology*\n",
    "- Efron & Tibshirani: *An Introduction to the Bootstrap* (related resampling perspective)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}