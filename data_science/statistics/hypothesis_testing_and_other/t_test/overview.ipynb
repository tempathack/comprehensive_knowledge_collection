{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student's t-test (one-sample, paired, two-sample)\n",
    "\n",
    "A *t-test* answers a very specific question:\n",
    "\n",
    "> **Is a mean (or mean difference) far enough from a reference value that it would be surprising if the true mean were actually that reference value?**\n",
    "\n",
    "It’s the default tool for mean comparisons when:\n",
    "\n",
    "- you have **numeric** data (measurements, scores, times, amounts)\n",
    "- the population standard deviation $\\sigma$ is **unknown** (almost always)\n",
    "- sample sizes are **small to medium**, so estimating uncertainty matters\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- pick the right t-test variant (one-sample vs paired vs two-sample)\n",
    "- compute the **t statistic** and **degrees of freedom** by hand\n",
    "- understand and interpret **p-values** and **confidence intervals**\n",
    "- implement the test end-to-end using **NumPy only** (no SciPy)\n",
    "- use interactive Plotly visuals to build intuition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) When do you use a t-test?\n",
    "\n",
    "Use a t-test when you want to compare **means**:\n",
    "\n",
    "- **One-sample**: “Is the mean different from a baseline?” (e.g., average delivery time vs SLA)\n",
    "- **Paired**: “Did the mean change within the same units?” (e.g., before/after intervention on the same users)\n",
    "- **Two-sample**: “Are the group means different?” (e.g., A/B test: variant B vs A)\n",
    "\n",
    "A t-test is *not* a general “difference detector”. It’s not ideal when:\n",
    "\n",
    "- the data are **extremely skewed** or dominated by **outliers**\n",
    "- you care about **medians** or **quantiles**, not means\n",
    "- observations are **dependent** (e.g., time series without adjustments)\n",
    "\n",
    "If those issues apply, consider robust or nonparametric alternatives (permutation tests, Wilcoxon/Mann–Whitney, trimmed-mean tests).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) The three t-tests (same idea, different setup)\n",
    "\n",
    "All t-tests have the same structure:\n",
    "\n",
    "1. define a **null hypothesis** $H_0$ about a mean (or mean difference)\n",
    "2. compute a **t statistic** = (estimate − null value) / (estimated standard error)\n",
    "3. use the **Student t distribution** to measure how “extreme” that statistic is\n",
    "\n",
    "### Quick reference\n",
    "\n",
    "| Test | What is the parameter? | Typical $H_0$ |\n",
    "|---|---|---|\n",
    "| One-sample | mean $\\mu$ | $\\mu = \\mu_0$ |\n",
    "| Paired | mean difference $\\mu_d$ | $\\mu_d = 0$ |\n",
    "| Two-sample (Welch) | mean difference $\\mu_1 - \\mu_2$ | $\\mu_1 - \\mu_2 = 0$ |\n",
    "\n",
    "**Rule of thumb**: for two independent groups, prefer **Welch’s t-test** (it does *not* assume equal variances).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Hypothesis testing vocabulary (and what it *actually* means)\n",
    "\n",
    "- $H_0$ (**null**): the “no effect / no difference” statement you test.\n",
    "- $H_1$ (**alternative**): what you’ll believe if the data are inconsistent with $H_0$.\n",
    "- $\\alpha$ (**significance level**): a chosen false-positive rate (often 0.05).\n",
    "- **p-value**: under $H_0$, the probability of observing a test statistic **at least as extreme** as what you got.\n",
    "\n",
    "Important interpretation points:\n",
    "\n",
    "- A p-value is **not** “the probability $H_0$ is true”.\n",
    "- A small p-value means the data are **unlikely under $H_0$**.\n",
    "- A large p-value does **not** prove “no effect”; it may mean “not enough evidence”.\n",
    "\n",
    "A good habit: always pair the p-value with a **confidence interval (CI)** and an **effect size**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Why the t distribution appears\n",
    "\n",
    "If the population standard deviation $\\sigma$ were known, the standardized mean\n",
    "\n",
    "$$\n",
    "Z = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "would follow a standard normal distribution under $H_0$ (for normal data).\n",
    "\n",
    "In reality, $\\sigma$ is unknown, so we estimate it with the sample standard deviation $s$.\n",
    "Replacing $\\sigma$ with $s$ injects extra uncertainty, and the distribution becomes **heavier-tailed**.\n",
    "\n",
    "That heavier-tailed distribution is **Student’s t**, controlled by the **degrees of freedom** (df).\n",
    "\n",
    "- small df → heavier tails (more uncertainty)\n",
    "- large df → approaches Normal(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student t density (no SciPy)\n",
    "\n",
    "def t_pdf(t: np.ndarray, df: float) -> np.ndarray:\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    df = float(df)\n",
    "\n",
    "    # log form for numerical stability\n",
    "    log_coeff = (\n",
    "        math.lgamma((df + 1) / 2)\n",
    "        - 0.5 * math.log(df * math.pi)\n",
    "        - math.lgamma(df / 2)\n",
    "    )\n",
    "    return np.exp(log_coeff) * (1 + (t**2) / df) ** (-(df + 1) / 2)\n",
    "\n",
    "\n",
    "x = np.linspace(-6, 6, 2000)\n",
    "\n",
    "fig = go.Figure()\n",
    "for df in [1, 2, 5, 10, 30, 100]:\n",
    "    fig.add_trace(go.Scatter(x=x, y=t_pdf(x, df), mode=\"lines\", name=f\"t(df={df})\"))\n",
    "\n",
    "normal_pdf = (1 / np.sqrt(2 * np.pi)) * np.exp(-(x**2) / 2)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=normal_pdf,\n",
    "        mode=\"lines\",\n",
    "        name=\"Normal(0,1)\",\n",
    "        line=dict(color=\"black\", dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Student's t distribution: heavier tails for small df\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) The one-sample t-test (the core idea)\n",
    "\n",
    "Given observations $x_1, \\dots, x_n$:\n",
    "\n",
    "- sample mean: $\\bar{x}$\n",
    "- sample standard deviation:\n",
    "\n",
    "$$\n",
    " s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "The standard error (estimated std of the sample mean) is:\n",
    "\n",
    "$$\n",
    "\\text{SE}(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "The t statistic for testing $H_0: \\mu = \\mu_0$ is:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "Under $H_0$ (and normal data),\n",
    "\n",
    "$$\n",
    "t \\sim t_{\\text{df}=n-1}\n",
    "$$\n",
    "\n",
    "Interpretation: $t$ is “how many estimated standard errors away from $\\mu_0$ the sample mean is”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Implementation from scratch (NumPy-only)\n",
    "\n",
    "We’ll implement:\n",
    "\n",
    "- one-sample t-test\n",
    "- two-sample t-test (pooled-variance and Welch)\n",
    "- paired t-test (as a one-sample test on differences)\n",
    "\n",
    "To keep everything transparent, we’ll also compute:\n",
    "\n",
    "- p-values via **numerical integration** of the t density\n",
    "- critical values / confidence intervals via **inverting** the CDF with bisection\n",
    "\n",
    "This is for learning. In production, use a dedicated stats library for speed and edge cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_cdf_numeric(t: float, df: float) -> float:\n",
    "    # Numerical CDF via symmetry + trapezoidal integration (no SciPy).\n",
    "    t = float(t)\n",
    "    df = float(df)\n",
    "\n",
    "    if t == 0.0:\n",
    "        return 0.5\n",
    "\n",
    "    sign = 1.0 if t > 0 else -1.0\n",
    "    t_abs = abs(t)\n",
    "\n",
    "    # Adaptive resolution: more points for larger |t|.\n",
    "    n = int(np.clip(np.ceil(5000 * t_abs), 4000, 200000))\n",
    "    grid = np.linspace(0.0, t_abs, n)\n",
    "    area = np.trapz(t_pdf(grid, df), grid)\n",
    "\n",
    "    return float(0.5 + sign * area)\n",
    "\n",
    "\n",
    "def t_p_value(t_stat: float, df: float, alternative: str = \"two-sided\") -> float:\n",
    "    cdf = t_cdf_numeric(t_stat, df)\n",
    "\n",
    "    if alternative == \"two-sided\":\n",
    "        return float(2 * min(cdf, 1 - cdf))\n",
    "    if alternative == \"greater\":\n",
    "        return float(1 - cdf)\n",
    "    if alternative == \"less\":\n",
    "        return float(cdf)\n",
    "\n",
    "    raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "\n",
    "\n",
    "def t_ppf_numeric(p: float, df: float, *, tol: float = 1e-6, max_iter: int = 80) -> float:\n",
    "    # Inverse CDF via bisection (sufficient for CI/critical values).\n",
    "    p = float(p)\n",
    "    df = float(df)\n",
    "\n",
    "    if not (0.0 < p < 1.0):\n",
    "        raise ValueError(\"p must be in (0, 1)\")\n",
    "    if p == 0.5:\n",
    "        return 0.0\n",
    "\n",
    "    # Symmetry: solve on positive side.\n",
    "    sign = 1.0 if p > 0.5 else -1.0\n",
    "    p_target = p if p > 0.5 else 1.0 - p\n",
    "\n",
    "    lo, hi = 0.0, 50.0\n",
    "    while t_cdf_numeric(hi, df) < p_target:\n",
    "        hi *= 2\n",
    "        if hi > 1e6:\n",
    "            raise RuntimeError(\"Failed to bracket quantile\")\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        if t_cdf_numeric(mid, df) < p_target:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "        if (hi - lo) < tol:\n",
    "            break\n",
    "\n",
    "    return sign * 0.5 * (lo + hi)\n",
    "\n",
    "\n",
    "def ci_two_sided(estimate: float, se: float, df: float, alpha: float = 0.05) -> tuple[float, float]:\n",
    "    t_crit = t_ppf_numeric(1 - alpha / 2, df)\n",
    "    return (estimate - t_crit * se, estimate + t_crit * se)\n",
    "\n",
    "\n",
    "def one_sample_t_test(\n",
    "    x: np.ndarray,\n",
    "    *,\n",
    "    mu0: float = 0.0,\n",
    "    alternative: str = \"two-sided\",\n",
    "    alpha: float = 0.05,\n",
    ") -> dict:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim != 1 or x.size < 2:\n",
    "        raise ValueError(\"x must be a 1D array with at least 2 values\")\n",
    "\n",
    "    n = x.size\n",
    "    mean = float(x.mean())\n",
    "    s = float(x.std(ddof=1))\n",
    "    se = s / np.sqrt(n)\n",
    "\n",
    "    estimate = mean - mu0\n",
    "    t_stat = estimate / se\n",
    "    df = float(n - 1)\n",
    "\n",
    "    p = t_p_value(t_stat, df, alternative=alternative)\n",
    "    ci = ci_two_sided(estimate, se, df, alpha=alpha)\n",
    "\n",
    "    return {\n",
    "        \"test\": \"one-sample t-test\",\n",
    "        \"n\": int(n),\n",
    "        \"df\": df,\n",
    "        \"mu0\": float(mu0),\n",
    "        \"estimate\": float(estimate),\n",
    "        \"t_stat\": float(t_stat),\n",
    "        \"p_value\": float(p),\n",
    "        \"ci\": tuple(map(float, ci)),\n",
    "        \"reject_at_alpha\": bool(p <= alpha),\n",
    "    }\n",
    "\n",
    "\n",
    "def two_sample_t_test(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    diff0: float = 0.0,\n",
    "    equal_var: bool = False,\n",
    "    alternative: str = \"two-sided\",\n",
    "    alpha: float = 0.05,\n",
    ") -> dict:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if x.ndim != 1 or y.ndim != 1 or x.size < 2 or y.size < 2:\n",
    "        raise ValueError(\"x and y must be 1D arrays with at least 2 values each\")\n",
    "\n",
    "    n1, n2 = x.size, y.size\n",
    "    m1, m2 = float(x.mean()), float(y.mean())\n",
    "    s1, s2 = float(x.std(ddof=1)), float(y.std(ddof=1))\n",
    "\n",
    "    raw_diff = m1 - m2\n",
    "    estimate = raw_diff - diff0\n",
    "\n",
    "    if equal_var:\n",
    "        sp2 = ((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2)\n",
    "        se = float(np.sqrt(sp2 * (1 / n1 + 1 / n2)))\n",
    "        df = float(n1 + n2 - 2)\n",
    "        variant = \"two-sample t-test (pooled variance)\"\n",
    "    else:\n",
    "        se2 = s1**2 / n1 + s2**2 / n2\n",
    "        se = float(np.sqrt(se2))\n",
    "        df = float(se2**2 / ((s1**2 / n1) ** 2 / (n1 - 1) + (s2**2 / n2) ** 2 / (n2 - 1)))\n",
    "        variant = \"two-sample t-test (Welch)\"\n",
    "\n",
    "    t_stat = estimate / se\n",
    "    p = t_p_value(t_stat, df, alternative=alternative)\n",
    "    ci = ci_two_sided(estimate, se, df, alpha=alpha)\n",
    "\n",
    "    return {\n",
    "        \"test\": variant,\n",
    "        \"n1\": int(n1),\n",
    "        \"n2\": int(n2),\n",
    "        \"df\": float(df),\n",
    "        \"diff0\": float(diff0),\n",
    "        \"estimate\": float(estimate),\n",
    "        \"t_stat\": float(t_stat),\n",
    "        \"p_value\": float(p),\n",
    "        \"ci\": tuple(map(float, ci)),\n",
    "        \"reject_at_alpha\": bool(p <= alpha),\n",
    "        \"means\": (m1, m2),\n",
    "        \"stds\": (s1, s2),\n",
    "    }\n",
    "\n",
    "\n",
    "def paired_t_test(\n",
    "    before: np.ndarray,\n",
    "    after: np.ndarray,\n",
    "    *,\n",
    "    diff0: float = 0.0,\n",
    "    alternative: str = \"two-sided\",\n",
    "    alpha: float = 0.05,\n",
    ") -> dict:\n",
    "    before = np.asarray(before, dtype=float)\n",
    "    after = np.asarray(after, dtype=float)\n",
    "\n",
    "    if before.shape != after.shape or before.ndim != 1 or before.size < 2:\n",
    "        raise ValueError(\"before and after must be 1D arrays of the same length (>= 2)\")\n",
    "\n",
    "    d = after - before\n",
    "    res = one_sample_t_test(d, mu0=diff0, alternative=alternative, alpha=alpha)\n",
    "    res[\"test\"] = \"paired t-test (one-sample on differences)\"\n",
    "    res[\"mean_before\"] = float(before.mean())\n",
    "    res[\"mean_after\"] = float(after.mean())\n",
    "    return res\n",
    "\n",
    "\n",
    "def plot_t_p_value(\n",
    "    t_stat: float,\n",
    "    df: float,\n",
    "    *,\n",
    "    alternative: str = \"two-sided\",\n",
    "    title: str | None = None,\n",
    "):\n",
    "    x_max = max(6.0, abs(float(t_stat)) * 1.6 + 1.0)\n",
    "    x = np.linspace(-x_max, x_max, 3000)\n",
    "    y = t_pdf(x, df)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=f\"t(df={df:.2f})\"))\n",
    "\n",
    "    fill_color = \"rgba(200, 30, 30, 0.25)\"\n",
    "\n",
    "    if alternative == \"two-sided\":\n",
    "        cut = abs(float(t_stat))\n",
    "        masks = [x <= -cut, x >= cut]\n",
    "        for i, mask in enumerate(masks):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=np.concatenate([x[mask], x[mask][::-1]]),\n",
    "                    y=np.concatenate([y[mask], np.zeros_like(y[mask])]),\n",
    "                    fill=\"toself\",\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "                    fillcolor=fill_color,\n",
    "                    name=\"p-value area\" if i == 0 else None,\n",
    "                    showlegend=(i == 0),\n",
    "                )\n",
    "            )\n",
    "        fig.add_vline(x=cut, line_dash=\"dash\", line_color=\"rgba(200, 30, 30, 0.9)\")\n",
    "        fig.add_vline(x=-cut, line_dash=\"dash\", line_color=\"rgba(200, 30, 30, 0.9)\")\n",
    "\n",
    "    elif alternative == \"greater\":\n",
    "        cut = float(t_stat)\n",
    "        mask = x >= cut\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.concatenate([x[mask], x[mask][::-1]]),\n",
    "                y=np.concatenate([y[mask], np.zeros_like(y[mask])]),\n",
    "                fill=\"toself\",\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "                fillcolor=fill_color,\n",
    "                name=\"p-value area\",\n",
    "            )\n",
    "        )\n",
    "        fig.add_vline(x=cut, line_dash=\"dash\", line_color=\"rgba(200, 30, 30, 0.9)\")\n",
    "\n",
    "    elif alternative == \"less\":\n",
    "        cut = float(t_stat)\n",
    "        mask = x <= cut\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.concatenate([x[mask], x[mask][::-1]]),\n",
    "                y=np.concatenate([y[mask], np.zeros_like(y[mask])]),\n",
    "                fill=\"toself\",\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "                fillcolor=fill_color,\n",
    "                name=\"p-value area\",\n",
    "            )\n",
    "        )\n",
    "        fig.add_vline(x=cut, line_dash=\"dash\", line_color=\"rgba(200, 30, 30, 0.9)\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "\n",
    "    p = t_p_value(t_stat, df, alternative=alternative)\n",
    "    fig.update_layout(\n",
    "        title=title or f\"Tail area = p-value (t={t_stat:.3f}, df={df:.2f}, p≈{p:.4f})\",\n",
    "        xaxis_title=\"t\",\n",
    "        yaxis_title=\"density\",\n",
    "    )\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Example: one-sample t-test\n",
    "\n",
    "Scenario: you measure a metric (say, average task completion time) and want to test whether the mean differs from a baseline $\\mu_0$.\n",
    "\n",
    "We’ll generate a small synthetic sample and test $H_0: \\mu = \\mu_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data: mean slightly above baseline\n",
    "mu0 = 10.0\n",
    "n = 18\n",
    "x = rng.normal(loc=10.8, scale=1.5, size=n)\n",
    "\n",
    "res_1s = one_sample_t_test(x, mu0=mu0, alternative=\"two-sided\", alpha=0.05)\n",
    "res_1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_t_p_value(res_1s[\"t_stat\"], res_1s[\"df\"], title=\"One-sample t-test: p-value as tail area\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float(np.mean(x))\n",
    "ci_low, ci_high = res_1s[\"ci\"]\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=x,\n",
    "    nbins=12,\n",
    "    title=\"Raw observations (with baseline and estimated mean)\",\n",
    "    labels={\"x\": \"value\"},\n",
    ")\n",
    "fig.add_vline(x=mu0, line_dash=\"dash\", line_color=\"black\", annotation_text=\"μ₀\")\n",
    "fig.add_vline(x=mean, line_dash=\"solid\", line_color=\"rgba(200,30,30,0.9)\", annotation_text=\"x̄\")\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[res_1s[\"estimate\"]],\n",
    "        y=[\"mean − μ₀\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10),\n",
    "        error_x=dict(\n",
    "            type=\"data\",\n",
    "            symmetric=False,\n",
    "            array=[ci_high - res_1s[\"estimate\"]],\n",
    "            arrayminus=[res_1s[\"estimate\"] - ci_low],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "fig2.add_vline(x=0.0, line_dash=\"dash\", line_color=\"black\")\n",
    "fig2.update_layout(\n",
    "    title=\"Estimated mean difference with 95% CI\",\n",
    "    xaxis_title=\"difference\",\n",
    "    yaxis_title=\"\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the one-sample result\n",
    "\n",
    "- If `p_value <= α`, you **reject** $H_0$ at level $\\alpha$.\n",
    "- The CI tells you a **range of plausible mean differences** (mean − $\\mu_0$).\n",
    "\n",
    "A useful equivalence (two-sided tests):\n",
    "\n",
    "> **Rejecting $H_0: \\mu=\\mu_0$ at level $\\alpha$ is the same as the $(1-\\alpha)$ CI for $\\mu$ not containing $\\mu_0$.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Example: two-sample t-test (Welch vs pooled)\n",
    "\n",
    "Two independent groups (A and B). We want to test:\n",
    "\n",
    "$$\n",
    "H_0: \\mu_A - \\mu_B = 0\n",
    "$$\n",
    "\n",
    "### Welch vs pooled-variance (Student) t-test\n",
    "\n",
    "- **Welch** (recommended default): does **not** assume equal variances.\n",
    "- **Pooled variance**: assumes the two populations have the **same variance**.\n",
    "\n",
    "If you’re unsure, use **Welch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic A/B data with different variances (Welch is appropriate)\n",
    "n_a, n_b = 24, 20\n",
    "A = rng.normal(loc=0.0, scale=1.0, size=n_a)\n",
    "B = rng.normal(loc=0.55, scale=1.5, size=n_b)\n",
    "\n",
    "res_welch = two_sample_t_test(A, B, equal_var=False)\n",
    "res_pooled = two_sample_t_test(A, B, equal_var=True)\n",
    "\n",
    "res_welch, res_pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.concatenate([A, B])\n",
    "groups = np.array([\"A\"] * A.size + [\"B\"] * B.size)\n",
    "\n",
    "fig = px.violin(\n",
    "    x=groups,\n",
    "    y=values,\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"Two groups: distribution view (A vs B)\",\n",
    "    labels={\"x\": \"group\", \"y\": \"value\"},\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "ci_low, ci_high = res_welch[\"ci\"]\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[res_welch[\"estimate\"]],\n",
    "        y=[\"A − B\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10),\n",
    "        error_x=dict(\n",
    "            type=\"data\",\n",
    "            symmetric=False,\n",
    "            array=[ci_high - res_welch[\"estimate\"]],\n",
    "            arrayminus=[res_welch[\"estimate\"] - ci_low],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "fig2.add_vline(x=0.0, line_dash=\"dash\", line_color=\"black\")\n",
    "fig2.update_layout(\n",
    "    title=\"Estimated mean difference (Welch) with 95% CI\",\n",
    "    xaxis_title=\"difference\",\n",
    "    yaxis_title=\"\",\n",
    ")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_t_p_value(res_welch[\"t_stat\"], res_welch[\"df\"], title=\"Welch t-test: p-value as tail area\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Example: paired t-test\n",
    "\n",
    "Paired data means each “before” is linked to an “after” for the same unit (person, device, country, …).\n",
    "\n",
    "The trick: compute the **difference per pair** and run a one-sample t-test on those differences.\n",
    "\n",
    "$$\n",
    "d_i = \\text{after}_i - \\text{before}_i\n",
    "$$\n",
    "\n",
    "Then test $H_0: \\mu_d = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic paired data: the same units measured twice\n",
    "n = 18\n",
    "before = rng.normal(loc=50, scale=8, size=n)\n",
    "# after is correlated with before + an average improvement\n",
    "after = before + rng.normal(loc=2.0, scale=3.0, size=n)\n",
    "\n",
    "res_paired = paired_t_test(before, after)\n",
    "res_paired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(n):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[\"before\", \"after\"],\n",
    "            y=[before[i], after[i]],\n",
    "            mode=\"lines+markers\",\n",
    "            line=dict(color=\"rgba(0,0,0,0.25)\"),\n",
    "            marker=dict(size=6),\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Paired measurements: each line is one unit\",\n",
    "    xaxis_title=\"\",\n",
    "    yaxis_title=\"value\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "diffs = after - before\n",
    "fig2 = px.histogram(\n",
    "    x=diffs,\n",
    "    nbins=12,\n",
    "    title=\"Paired differences (after − before)\",\n",
    "    labels={\"x\": \"difference\"},\n",
    ")\n",
    "fig2.add_vline(x=0.0, line_dash=\"dash\", line_color=\"black\")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_t_p_value(res_paired[\"t_stat\"], res_paired[\"df\"], title=\"Paired t-test: p-value as tail area\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Interpreting the output (t, df, p, CI)\n",
    "\n",
    "### t statistic\n",
    "\n",
    "$$\n",
    " t = \\frac{\\text{estimate} - \\text{null value}}{\\text{estimated standard error}}\n",
    "$$\n",
    "\n",
    "- large $|t|$ → estimate is many standard errors away from the null\n",
    "\n",
    "### degrees of freedom (df)\n",
    "\n",
    "df controls tail heaviness. Smaller df means more uncertainty because you estimated more from limited data.\n",
    "\n",
    "### p-value\n",
    "\n",
    "- **two-sided**: “How likely is a value with $|T| \\ge |t_{obs}|$ under $H_0$?”\n",
    "- **one-sided**: tail probability only on the direction you care about\n",
    "\n",
    "### confidence interval\n",
    "\n",
    "For two-sided tests, the $(1-\\alpha)$ CI and the test are consistent:\n",
    "\n",
    "- if the CI excludes 0 (or $\\mu_0$), you reject at level $\\alpha$\n",
    "- if the CI includes 0 (or $\\mu_0$), you don’t reject\n",
    "\n",
    "### practical vs statistical significance\n",
    "\n",
    "A tiny effect can be statistically significant with a large sample; a meaningful effect can be non-significant with a small sample.\n",
    "Always look at the **effect size + CI**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "def ci_excludes_zero(ci: tuple[float, float]) -> bool:\n",
    "    lo, hi = ci\n",
    "    return (lo > 0.0) or (hi < 0.0)\n",
    "\n",
    "checks = {\n",
    "    \"one_sample\": {\n",
    "        \"p<=alpha\": res_1s[\"p_value\"] <= alpha,\n",
    "        \"ci_excludes_0\": ci_excludes_zero(res_1s[\"ci\"]),\n",
    "    },\n",
    "    \"welch\": {\n",
    "        \"p<=alpha\": res_welch[\"p_value\"] <= alpha,\n",
    "        \"ci_excludes_0\": ci_excludes_zero(res_welch[\"ci\"]),\n",
    "    },\n",
    "    \"paired\": {\n",
    "        \"p<=alpha\": res_paired[\"p_value\"] <= alpha,\n",
    "        \"ci_excludes_0\": ci_excludes_zero(res_paired[\"ci\"]),\n",
    "    },\n",
    "}\n",
    "checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under a true null, p-values should be ~Uniform(0,1).\n",
    "# We'll simulate many one-sample t-tests where H0 is true.\n",
    "\n",
    "n = 12\n",
    "n_sim = 400\n",
    "mu0 = 0.0\n",
    "\n",
    "pvals = []\n",
    "for _ in range(n_sim):\n",
    "    sample = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "    mean = float(sample.mean())\n",
    "    s = float(sample.std(ddof=1))\n",
    "    se = s / np.sqrt(n)\n",
    "    t_stat = (mean - mu0) / se\n",
    "    pvals.append(t_p_value(t_stat, n - 1, alternative=\"two-sided\"))\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=pvals,\n",
    "    nbins=20,\n",
    "    title=\"p-values under a true null are (approximately) uniform\",\n",
    "    labels={\"x\": \"p-value\"},\n",
    ")\n",
    "fig.add_hline(y=n_sim / 20, line_dash=\"dash\", line_color=\"black\", annotation_text=\"uniform baseline\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Assumptions, diagnostics, and pitfalls\n",
    "\n",
    "### Assumptions (typical)\n",
    "\n",
    "- **Independence** of observations (or of paired differences)\n",
    "- The sampling distribution of the mean is close enough to normal:\n",
    "  - data are approximately normal, **or**\n",
    "  - sample size is large enough for the CLT to kick in\n",
    "\n",
    "### Common pitfalls\n",
    "\n",
    "- choosing one-sided vs two-sided **after** seeing the data\n",
    "- stopping data collection when `p < 0.05` (optional stopping) without correction\n",
    "- multiple testing without adjustment (p-hacking)\n",
    "- ignoring effect size and uncertainty (CI)\n",
    "\n",
    "### Alternatives when assumptions fail\n",
    "\n",
    "- **Permutation test** on the mean difference (very interpretable)\n",
    "- **Bootstrap** CI for the mean / mean difference\n",
    "- **Wilcoxon signed-rank** (paired) or **Mann–Whitney U** (two-sample) for ordinal / heavy-tailed cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight: as n grows, the same effect becomes easier to detect.\n",
    "\n",
    "alpha = 0.05\n",
    "true_effect = 0.3  # mean shift in SD units\n",
    "n_grid = np.array([5, 8, 12, 20, 30, 50, 80])\n",
    "\n",
    "n_sim = 3000\n",
    "\n",
    "powers = []\n",
    "false_pos = []\n",
    "\n",
    "for n in n_grid:\n",
    "    df = n - 1\n",
    "    t_crit = t_ppf_numeric(1 - alpha / 2, df)\n",
    "\n",
    "    # Under H0 (no effect)\n",
    "    x0 = rng.normal(loc=0.0, scale=1.0, size=(n_sim, n))\n",
    "    m0 = x0.mean(axis=1)\n",
    "    s0 = x0.std(axis=1, ddof=1)\n",
    "    t0 = m0 / (s0 / np.sqrt(n))\n",
    "    false_pos.append(float(np.mean(np.abs(t0) >= t_crit)))\n",
    "\n",
    "    # Under H1 (effect)\n",
    "    x1 = rng.normal(loc=true_effect, scale=1.0, size=(n_sim, n))\n",
    "    m1 = x1.mean(axis=1)\n",
    "    s1 = x1.std(axis=1, ddof=1)\n",
    "    t1 = m1 / (s1 / np.sqrt(n))\n",
    "    powers.append(float(np.mean(np.abs(t1) >= t_crit)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=n_grid, y=false_pos, mode=\"lines+markers\", name=\"Type I error (H0)\"))\n",
    "fig.add_trace(go.Scatter(x=n_grid, y=powers, mode=\"lines+markers\", name=f\"Power (effect={true_effect})\"))\n",
    "\n",
    "fig.add_hline(y=alpha, line_dash=\"dash\", line_color=\"black\", annotation_text=\"α\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Sample size vs false-positive rate and power (two-sided t-test)\",\n",
    "    xaxis_title=\"sample size n\",\n",
    "    yaxis_title=\"probability\",\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Exercises\n",
    "\n",
    "1. Implement a **permutation test** for the two-sample mean difference and compare its p-value to Welch’s t-test.\n",
    "2. Create data with one huge outlier and see how the t-test changes. Try a trimmed mean instead.\n",
    "3. For the paired case, simulate scenarios where pairing helps a lot vs not at all (correlation between before/after).\n",
    "\n",
    "## References\n",
    "\n",
    "- Student (1908), *The probable error of a mean* (original t-test paper)\n",
    "- Any intro stats text: sections on Student’s t distribution and t-tests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}