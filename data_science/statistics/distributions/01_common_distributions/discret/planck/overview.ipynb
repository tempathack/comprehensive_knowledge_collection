{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cd7b24",
   "metadata": {},
   "source": [
    "# Planck distribution (`planck`)\n",
    "\n",
    "SciPy's `planck` distribution is a **discrete exponential** distribution on the non‑negative integers.\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X=k)=(1-e^{-\\lambda})e^{-\\lambda k},\\qquad k=0,1,2,\\dots\\quad (\\lambda>0)\n",
    "\\]\n",
    "\n",
    "It is mathematically the same as a **geometric** distribution (counting *failures before the first success*) under the reparameterization\n",
    "\\(p = 1-e^{-\\lambda}\\).\n",
    "\n",
    "> This is *not* the continuous “Planck's law” distribution of photon wavelengths/energies; it is the discrete distribution of **occupation numbers** (counts) for a single mode.\n",
    "\n",
    "## Learning goals\n",
    "- Classify the distribution and state its support/parameter space.\n",
    "- Derive the PMF/CDF and connect them to geometric / truncated Boltzmann.\n",
    "- Compute mean/variance/skewness/kurtosis, MGF/CF, and entropy.\n",
    "- Derive the likelihood and the MLE for \\(\\lambda\\).\n",
    "- Implement **NumPy-only** sampling and validate it with Monte Carlo.\n",
    "- Use `scipy.stats.planck` for PMF/CDF/RVS and fitting via `scipy.stats.fit`.\n",
    "\n",
    "## Prerequisites\n",
    "- Comfort with geometric series and basic calculus.\n",
    "- Familiarity with `log`, `exp`, and numerical-stability helpers like `expm1` and `log1p`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b72f8",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a201fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import plotly\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"SciPy:\", scipy.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"Seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc5e43",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `planck` (Planck *discrete exponential* distribution)\n",
    "- **Type**: **Discrete**\n",
    "- **Support** (base distribution): \\(k \\in \\{0,1,2,\\dots\\}\\)\n",
    "- **Parameter space** (SciPy shape): \\(\\lambda>0\\)\n",
    "\n",
    "SciPy also allows a location shift `loc` (an integer shift for discrete distributions). With `loc`:\n",
    "\n",
    "\\[\n",
    "X = \\texttt{loc} + K,\\qquad K\\in\\{0,1,2,\\dots\\}.\n",
    "\\]\n",
    "\n",
    "We will mostly work with the base distribution \\(K\\) (i.e. `loc=0`).\n",
    "\n",
    "Notation and convenient reparameterizations:\n",
    "\n",
    "- \\(K \\sim \\mathrm{Planck}(\\lambda)\\)\n",
    "- Define \\(q = e^{-\\lambda} \\in (0,1)\\) and \\(p = 1-q = 1-e^{-\\lambda}\\in(0,1)\\)\n",
    "\n",
    "Then the PMF is \\(\\mathbb{P}(K=k)=p\\,q^k\\), i.e. a **geometric** distribution on \\(\\{0,1,2,\\dots\\}\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4deac",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "The Planck distribution models a **count** with an **exponentially decaying tail**:\n",
    "\n",
    "- large counts are possible, but their probability shrinks like \\(e^{-\\lambda k}\\)\n",
    "- the distribution is **right-skewed** (often strongly)\n",
    "\n",
    "A key interpretation comes from physics:\n",
    "\n",
    "- Consider a single quantum harmonic oscillator mode in thermal equilibrium.\n",
    "- Each additional excitation (“photon”, “quantum”) costs energy proportional to the count.\n",
    "- The probability of seeing \\(k\\) excitations is proportional to \\(e^{-\\lambda k}\\), and normalization yields the Planck PMF.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Occupation numbers** (counts) of bosonic modes in statistical physics.\n",
    "- Any **count process** where the probability of additional units decays exponentially.\n",
    "- A simple discrete alternative to exponential/gamma when data are integer-valued (e.g. binned waiting times).\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Geometric distribution**: exactly the same distribution under \\(p = 1-e^{-\\lambda}\\).\n",
    "- **Boltzmann distribution** (`scipy.stats.boltzmann`): a **truncated** discrete exponential on \\(\\{0,\\dots,N-1\\}\\). As \\(N\\to\\infty\\), it approaches the Planck distribution.\n",
    "- **Exponential distribution**: if \\(E\\sim\\mathrm{Exp}(\\text{rate}=\\lambda)\\), then \\(\\lfloor E\\rfloor\\sim\\mathrm{Planck}(\\lambda)\\).\n",
    "- **Negative binomial**: sums of independent geometric/Planck variables produce negative binomial counts.\n",
    "\n",
    "A useful qualitative property: **memorylessness**.\n",
    "For \\(q=e^{-\\lambda}\\),\n",
    "\\[\n",
    "\\mathbb{P}(K\\ge k+n\\mid K\\ge k)=q^n.\n",
    "\\]\n",
    "Among discrete distributions on \\(\\{0,1,2,\\dots\\}\\), this is characteristic of the geometric/Planck family.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55cd0e",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(K \\sim \\mathrm{Planck}(\\lambda)\\) with \\(\\lambda>0\\). Define \\(q=e^{-\\lambda}\\in(0,1)\\) and \\(p=1-q\\).\n",
    "\n",
    "### PMF\n",
    "\\[\n",
    "\\mathbb{P}(K=k) = (1-e^{-\\lambda})e^{-\\lambda k} = p\\,q^k,\\qquad k\\in\\{0,1,2,\\dots\\}\n",
    "\\]\n",
    "and \\(\\mathbb{P}(K=k)=0\\) otherwise.\n",
    "\n",
    "A numerically stable way to compute \\(1-e^{-\\lambda}\\) for small \\(\\lambda\\) is\n",
    "\\(1-e^{-\\lambda} = -\\mathrm{expm1}(-\\lambda)\\).\n",
    "\n",
    "### CDF\n",
    "Because this is a discrete distribution, the CDF is step-like.\n",
    "For real \\(x\\):\n",
    "\\[\n",
    "F(x)=\\mathbb{P}(K\\le x)=\n",
    "\\begin{cases}\n",
    "0, & x<0\\\\\n",
    "1 - e^{-\\lambda(\\lfloor x\\rfloor+1)}, & x\\ge 0.\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Equivalently, for an integer \\(k\\ge 0\\):\n",
    "\\[\n",
    "F(k) = 1 - q^{k+1}.\n",
    "\\]\n",
    "\n",
    "### Survival function\n",
    "For an integer \\(k\\ge 0\\):\n",
    "\\[\n",
    "\\mathbb{P}(K>k) = q^{k+1} = e^{-\\lambda(k+1)}.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lambda(lambda_: float) -> float:\n",
    "    lambda_ = float(lambda_)\n",
    "    if not (np.isfinite(lambda_) and lambda_ > 0.0):\n",
    "        raise ValueError(f\"lambda_ must be finite and > 0, got {lambda_!r}\")\n",
    "    return lambda_\n",
    "\n",
    "\n",
    "def _is_integer_array(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"True for integer dtype and for floats that are exact integers.\"\"\"\n",
    "    return np.equal(x, np.floor(x))\n",
    "\n",
    "\n",
    "def planck_logpmf(k, lambda_: float):\n",
    "    \"\"\"Log-PMF of the Planck distribution at integer k >= 0.\"\"\"\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    k = np.asarray(k)\n",
    "\n",
    "    out = np.full_like(k, -np.inf, dtype=float)\n",
    "    mask = (k >= 0) & _is_integer_array(k)\n",
    "\n",
    "    # log(1 - exp(-lambda_)) computed stably\n",
    "    log_p = np.log(-np.expm1(-lambda_))\n",
    "    out[mask] = log_p - lambda_ * k[mask]\n",
    "    return out\n",
    "\n",
    "\n",
    "def planck_pmf(k, lambda_: float):\n",
    "    \"\"\"PMF of the Planck distribution at integer k >= 0.\"\"\"\n",
    "    return np.exp(planck_logpmf(k, lambda_))\n",
    "\n",
    "\n",
    "def planck_cdf(x, lambda_: float):\n",
    "    \"\"\"CDF of the Planck distribution for real x.\"\"\"\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x >= 0\n",
    "    k = np.floor(x[mask])\n",
    "\n",
    "    # F(x) = 1 - exp(-lambda_ * (floor(x) + 1))\n",
    "    out[mask] = -np.expm1(-lambda_ * (k + 1.0))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks vs SciPy\n",
    "\n",
    "lambda_ = 1.3\n",
    "ks = np.arange(0, 15)\n",
    "\n",
    "pmf_np = planck_pmf(ks, lambda_)\n",
    "pmf_sp = stats.planck.pmf(ks, lambda_)\n",
    "\n",
    "print(\"max |pmf numpy - pmf scipy|:\", float(np.max(np.abs(pmf_np - pmf_sp))))\n",
    "\n",
    "K = 200\n",
    "cdf_tail = planck_cdf(K, lambda_)\n",
    "print(f\"P(K <= {K}) = {cdf_tail:.12f} (so remaining tail mass ~ {1-cdf_tail:.2e})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35d089",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let \\(q=e^{-\\lambda}\\in(0,1)\\) and \\(p=1-q\\).\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "For \\(K\\sim\\mathrm{Planck}(\\lambda)\\):\n",
    "\n",
    "| Quantity | Closed form |\n",
    "|---|---|\n",
    "| Mean | \\(\\mathbb{E}[K]=\\dfrac{q}{p}=\\dfrac{1}{e^{\\lambda}-1}\\) |\n",
    "| Variance | \\(\\mathrm{Var}(K)=\\dfrac{q}{p^2}=\\dfrac{e^{\\lambda}}{(e^{\\lambda}-1)^2}=\\mu(1+\\mu)\\) |\n",
    "| Skewness | \\(\\gamma_1 = \\dfrac{1+q}{\\sqrt{q}} = 2\\cosh(\\lambda/2)\\) |\n",
    "| Excess kurtosis | \\(\\gamma_2 = q + q^{-1} + 4 = 2\\cosh(\\lambda) + 4\\) |\n",
    "\n",
    "(Full kurtosis is \\(3+\\gamma_2\\). SciPy’s `stats(..., moments='k')` returns **excess** kurtosis.)\n",
    "\n",
    "### MGF / characteristic function\n",
    "The probability generating function (PGF) is\n",
    "\\[\n",
    "G(z)=\\mathbb{E}[z^K] = \\frac{p}{1-qz},\\qquad |qz|<1.\n",
    "\\]\n",
    "\n",
    "- **MGF** \\(M(t)=\\mathbb{E}[e^{tK}]\\) exists when \\(t<\\lambda\\) and equals\n",
    "\\[\n",
    "M(t)=\\frac{p}{1-qe^{t}} = \\frac{1-e^{-\\lambda}}{1-e^{t-\\lambda}}\\,.\n",
    "\\]\n",
    "\n",
    "- **Characteristic function** \\(\\varphi(\\omega)=\\mathbb{E}[e^{i\\omega K}]\\) exists for all real \\(\\omega\\):\n",
    "\\[\n",
    "\\varphi(\\omega)=\\frac{p}{1-qe^{i\\omega}}.\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "The (Shannon) entropy is\n",
    "\\[\n",
    "H(K) = -\\sum_{k\\ge 0} \\mathbb{P}(K=k)\\log \\mathbb{P}(K=k)\n",
    "= -\\log(1-e^{-\\lambda}) + \\frac{\\lambda}{e^{\\lambda}-1}.\n",
    "\\]\n",
    "\n",
    "### Memorylessness\n",
    "\\[\n",
    "\\mathbb{P}(K\\ge k+n\\mid K\\ge k)=q^n\\quad\\text{for integers }k,n\\ge 0.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planck_mean(lambda_: float) -> float:\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    return float(1.0 / np.expm1(lambda_))\n",
    "\n",
    "\n",
    "def planck_variance(lambda_: float) -> float:\n",
    "    mu = planck_mean(lambda_)\n",
    "    return float(mu * (1.0 + mu))\n",
    "\n",
    "\n",
    "def planck_skew(lambda_: float) -> float:\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    q = np.exp(-lambda_)\n",
    "    return float((1.0 + q) / np.sqrt(q))\n",
    "\n",
    "\n",
    "def planck_excess_kurtosis(lambda_: float) -> float:\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    q = np.exp(-lambda_)\n",
    "    return float(q + 1.0 / q + 4.0)\n",
    "\n",
    "\n",
    "def planck_entropy(lambda_: float) -> float:\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    mu = planck_mean(lambda_)\n",
    "    return float(-np.log(-np.expm1(-lambda_)) + mu * lambda_)\n",
    "\n",
    "\n",
    "def planck_mgf(t, lambda_: float):\n",
    "    \"\"\"MGF M(t) = E[e^{tK}] for t < lambda_.\"\"\"\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    if np.any(t >= lambda_):\n",
    "        raise ValueError(\"MGF exists only for t < lambda_\")\n",
    "\n",
    "    q = np.exp(-lambda_)\n",
    "    p = -np.expm1(-lambda_)\n",
    "    return p / (1.0 - q * np.exp(t))\n",
    "\n",
    "\n",
    "def planck_cf(w, lambda_: float):\n",
    "    \"\"\"Characteristic function phi(w) = E[e^{i w K}] for real w.\"\"\"\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    q = np.exp(-lambda_)\n",
    "    p = -np.expm1(-lambda_)\n",
    "    return p / (1.0 - q * np.exp(1j * w))\n",
    "\n",
    "\n",
    "lambda_ = 1.3\n",
    "\n",
    "mu = planck_mean(lambda_)\n",
    "var = planck_variance(lambda_)\n",
    "skew = planck_skew(lambda_)\n",
    "exkurt = planck_excess_kurtosis(lambda_)\n",
    "ent = planck_entropy(lambda_)\n",
    "\n",
    "mean_sp, var_sp, skew_sp, exkurt_sp = stats.planck.stats(lambda_, moments='mvsk')\n",
    "ent_sp = stats.planck.entropy(lambda_)\n",
    "\n",
    "print(\"mean:\", mu, \"| scipy:\", float(mean_sp))\n",
    "print(\"var:\", var, \"| scipy:\", float(var_sp))\n",
    "print(\"skew:\", skew, \"| scipy:\", float(skew_sp))\n",
    "print(\"excess kurt:\", exkurt, \"| scipy:\", float(exkurt_sp))\n",
    "print(\"entropy:\", ent, \"| scipy:\", float(ent_sp))\n",
    "\n",
    "# MGF spot-check (finite sum vs closed form)\n",
    "t = 0.5\n",
    "K_max = 2000\n",
    "ks = np.arange(0, K_max + 1)\n",
    "mgf_sum = np.sum(np.exp(t * ks) * planck_pmf(ks, lambda_))\n",
    "mgf_closed = float(planck_mgf(t, lambda_))\n",
    "print(f\"MGF(t={t}) sum ~ {mgf_sum:.8f} | closed {mgf_closed:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e31b3d",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "\\(\\lambda\\) controls **how quickly the tail decays**.\n",
    "\n",
    "- Larger \\(\\lambda\\) \\(\\Rightarrow\\) faster decay \\(\\Rightarrow\\) most mass near 0.\n",
    "- Smaller \\(\\lambda\\) \\(\\Rightarrow\\) heavier tail (mean and variance increase like \\(\\approx 1/\\lambda\\) and \\(\\approx 1/\\lambda^2\\) for small \\(\\lambda\\)).\n",
    "\n",
    "Useful equivalences:\n",
    "\n",
    "- \\(q=e^{-\\lambda}\\) is the **tail ratio**: \\(\\mathbb{P}(K\\ge k) = q^k\\).\n",
    "- \\(p=1-e^{-\\lambda}\\) is the **geometric success probability**.\n",
    "\n",
    "So you can interpret \\(\\lambda\\) as a *discrete decay rate* (analogous to the rate of an exponential distribution).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape changes: PMF curves for different lambda_\n",
    "\n",
    "lambdas = [0.2, 0.5, 1.0, 2.0]\n",
    "ks = np.arange(0, 35)\n",
    "\n",
    "fig = go.Figure()\n",
    "for lam in lambdas:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=ks,\n",
    "            y=planck_pmf(ks, lam),\n",
    "            mode=\"lines+markers\",\n",
    "            name=f\"lambda_={lam}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Planck PMF for different lambda_\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(K = k)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Mean / variance as a function of lambda_\n",
    "lam_grid = np.linspace(0.05, 3.0, 200)\n",
    "mu_grid = np.array([planck_mean(l) for l in lam_grid])\n",
    "var_grid = np.array([planck_variance(l) for l in lam_grid])\n",
    "\n",
    "fig2 = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(x=lam_grid, y=mu_grid, mode=\"lines\", name=\"mean\"),\n",
    "        go.Scatter(x=lam_grid, y=var_grid, mode=\"lines\", name=\"variance\"),\n",
    "    ],\n",
    "    layout=go.Layout(\n",
    "        title=\"Mean and variance vs lambda_\",\n",
    "        xaxis_title=\"lambda_\",\n",
    "        yaxis_title=\"value\",\n",
    "        legend_title=\"moment\",\n",
    "    ),\n",
    ")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015e9bf",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "Throughout, let \\(q=e^{-\\lambda}\\in(0,1)\\) and \\(p=1-q\\).\n",
    "\n",
    "### Expectation\n",
    "Start from the PMF \\(\\mathbb{P}(K=k)=p q^k\\):\n",
    "\\[\n",
    "\\mathbb{E}[K] = \\sum_{k=0}^{\\infty} k\\,p q^k = p\\sum_{k=0}^{\\infty} k q^k.\n",
    "\\]\n",
    "Using the standard geometric-series identity\n",
    "\\(\\sum_{k\\ge 0} k q^k = \\frac{q}{(1-q)^2}\\) (for \\(|q|<1\\)):\n",
    "\\[\n",
    "\\mathbb{E}[K] = p\\frac{q}{(1-q)^2} = \\frac{q}{1-q}.\n",
    "\\]\n",
    "Substituting \\(q=e^{-\\lambda}\\) gives\n",
    "\\(\\mathbb{E}[K] = \\frac{1}{e^{\\lambda}-1}\\).\n",
    "\n",
    "### Variance\n",
    "Compute \\(\\mathbb{E}[K^2]\\) using\n",
    "\\(\\sum_{k\\ge 0} k^2 q^k = \\frac{q(1+q)}{(1-q)^3}\\):\n",
    "\\[\n",
    "\\mathbb{E}[K^2] = p\\frac{q(1+q)}{(1-q)^3} = \\frac{q(1+q)}{(1-q)^2}.\n",
    "\\]\n",
    "Then\n",
    "\\[\n",
    "\\mathrm{Var}(K)=\\mathbb{E}[K^2]-\\mathbb{E}[K]^2\n",
    "= \\frac{q}{(1-q)^2}.\n",
    "\\]\n",
    "\n",
    "### Likelihood and MLE\n",
    "For observations \\(k_1,\\dots,k_n\\in\\{0,1,2,\\dots\\}\\), the log-likelihood is\n",
    "\\[\n",
    "\\ell(\\lambda)\n",
    "= \\sum_{i=1}^n \\log\\big((1-e^{-\\lambda})e^{-\\lambda k_i}\\big)\n",
    "= n\\log(1-e^{-\\lambda}) - \\lambda\\sum_{i=1}^n k_i.\n",
    "\\]\n",
    "Differentiate and set to zero:\n",
    "\\[\n",
    "\\ell'(\\lambda) = \\frac{n}{e^{\\lambda}-1} - \\sum_{i=1}^n k_i = 0\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\widehat{\\lambda} = \\log\\Big(1+\\frac{n}{\\sum_i k_i}\\Big)=\\log\\Big(1+\\frac{1}{\\bar{k}}\\Big).\n",
    "\\]\n",
    "\n",
    "If \\(\\bar{k}=0\\) (all observations are zero), the likelihood increases as \\(\\lambda\\to\\infty\\), reflecting a near-degenerate distribution at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sample(k):\n",
    "    k = np.asarray(k)\n",
    "    if k.ndim != 1:\n",
    "        raise ValueError(\"sample must be 1D\")\n",
    "    if np.any(~np.isfinite(k)):\n",
    "        raise ValueError(\"sample must be finite\")\n",
    "    if np.any(k < 0) or np.any(~_is_integer_array(k)):\n",
    "        raise ValueError(\"sample must contain integers >= 0 (use loc-shifted model if needed)\")\n",
    "    return k.astype(int)\n",
    "\n",
    "\n",
    "def planck_loglik(lambda_: float, sample) -> float:\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    k = validate_sample(sample)\n",
    "\n",
    "    n = k.size\n",
    "    s = int(np.sum(k))\n",
    "\n",
    "    # log L = n log(1 - exp(-lambda_)) - lambda_ * sum k_i\n",
    "    return float(n * np.log(-np.expm1(-lambda_)) - lambda_ * s)\n",
    "\n",
    "\n",
    "def planck_mle_lambda(sample) -> float:\n",
    "    k = validate_sample(sample)\n",
    "    m = float(np.mean(k))\n",
    "    if m == 0.0:\n",
    "        return float(\"inf\")\n",
    "    return float(np.log1p(1.0 / m))\n",
    "\n",
    "\n",
    "# Example: recover lambda_ from simulated data\n",
    "true_lambda = 1.1\n",
    "sample = stats.planck.rvs(true_lambda, size=2000, random_state=rng)\n",
    "\n",
    "lam_hat_closed = planck_mle_lambda(sample)\n",
    "print(\"true lambda_:\", true_lambda)\n",
    "print(\"closed-form MLE:\", lam_hat_closed)\n",
    "\n",
    "# Compare to generic optimizer-based fit\n",
    "fit_res = stats.fit(stats.planck, sample, bounds={\"lambda\": (1e-6, 10.0)})\n",
    "print(\"scipy.stats.fit params:\", fit_res.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab167d8",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only sampler (inverse transform via tail)\n",
    "The survival function is\n",
    "\\(\\mathbb{P}(K\\ge k)=q^k=e^{-\\lambda k}\\).\n",
    "\n",
    "If \\(U\\sim\\mathrm{Unif}(0,1)\\), define\n",
    "\\[\n",
    "K = \\Big\\lfloor \\frac{-\\log U}{\\lambda} \\Big\\rfloor.\n",
    "\\]\n",
    "Then for integer \\(k\\ge 0\\):\n",
    "\\[\n",
    "\\mathbb{P}(K\\ge k)\n",
    "= \\mathbb{P}\\left(\\frac{-\\log U}{\\lambda}\\ge k\\right)\n",
    "= \\mathbb{P}(U\\le e^{-\\lambda k})\n",
    "= e^{-\\lambda k},\n",
    "\\]\n",
    "so \\(K\\) has the desired Planck PMF.\n",
    "\n",
    "This is also a nice conceptual bridge:\n",
    "\n",
    "- \\(E=-\\log U/\\lambda\\) is an \\(\\mathrm{Exp}(\\text{rate}=\\lambda)\\) random variable\n",
    "- and \\(K=\\lfloor E\\rfloor\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6676ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planck_rvs_numpy(lambda_: float, size: int, *, rng: np.random.Generator) -> np.ndarray:\n",
    "    lambda_ = validate_lambda(lambda_)\n",
    "    size = int(size)\n",
    "\n",
    "    u = rng.random(size)\n",
    "    # rng.random can (rarely) return exactly 0, which would map to +inf.\n",
    "    u = np.maximum(u, np.nextafter(0.0, 1.0))\n",
    "\n",
    "    return np.floor(-np.log(u) / lambda_).astype(int)\n",
    "\n",
    "\n",
    "# Monte Carlo check\n",
    "lambda_ = 0.9\n",
    "n = 200_000\n",
    "s = planck_rvs_numpy(lambda_, n, rng=rng)\n",
    "\n",
    "print(\"MC mean:\", float(np.mean(s)), \"| theory:\", planck_mean(lambda_))\n",
    "print(\"MC var :\", float(np.var(s)), \"| theory:\", planck_variance(lambda_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06080f",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** (bars)\n",
    "- the **CDF** (step curve)\n",
    "- a **Monte Carlo** histogram compared to the theoretical PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad548873",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.8\n",
    "ks = np.arange(0, 30)\n",
    "\n",
    "pmf = planck_pmf(ks, lambda_)\n",
    "cdf = planck_cdf(ks, lambda_)\n",
    "\n",
    "fig_pmf = go.Figure(\n",
    "    data=[go.Bar(x=ks, y=pmf, name=\"PMF\")],\n",
    "    layout=go.Layout(\n",
    "        title=f\"Planck PMF (lambda_={lambda_})\",\n",
    "        xaxis_title=\"k\",\n",
    "        yaxis_title=\"P(K = k)\",\n",
    "    ),\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure(\n",
    "    data=[go.Scatter(x=ks, y=cdf, mode=\"lines+markers\", name=\"CDF\")],\n",
    "    layout=go.Layout(\n",
    "        title=f\"Planck CDF (lambda_={lambda_})\",\n",
    "        xaxis_title=\"k\",\n",
    "        yaxis_title=\"P(K ≤ k)\",\n",
    "    ),\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo vs theory\n",
    "n = 80_000\n",
    "samples = planck_rvs_numpy(lambda_, n, rng=rng)\n",
    "\n",
    "# Empirical frequencies for 0..K_max\n",
    "K_max = 25\n",
    "counts = np.bincount(samples[samples <= K_max], minlength=K_max + 1)\n",
    "emp_pmf = counts / n\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=np.arange(K_max + 1), y=emp_pmf, name=\"Empirical\", opacity=0.6))\n",
    "fig_mc.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(K_max + 1),\n",
    "        y=planck_pmf(np.arange(K_max + 1), lambda_),\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Theory\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo PMF vs theory (n={n:,}, lambda_={lambda_})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3276136",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.planck` as an `rv_discrete` distribution.\n",
    "\n",
    "Common methods:\n",
    "- `pmf`, `logpmf`\n",
    "- `cdf`, `sf`\n",
    "- `rvs`\n",
    "- `stats` (mean/var/skew/**excess** kurtosis)\n",
    "- `entropy`\n",
    "\n",
    "### Fitting\n",
    "Many `rv_discrete` distributions (including `planck`) do **not** implement a `.fit(...)` method.\n",
    "In modern SciPy, you can fit discrete or continuous distributions with `scipy.stats.fit`.\n",
    "\n",
    "For `planck`, the MLE for `lambda_` also has a closed form (derived above), which is often preferable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1.0\n",
    "rv = stats.planck(lambda_)  # frozen distribution (loc=0)\n",
    "\n",
    "ks = np.arange(0, 6)\n",
    "print(\"pmf:\", rv.pmf(ks))\n",
    "print(\"cdf:\", rv.cdf(ks))\n",
    "print(\"rvs:\", rv.rvs(size=10, random_state=rng))\n",
    "\n",
    "mean_sp, var_sp, skew_sp, exkurt_sp = rv.stats(moments='mvsk')\n",
    "print(\"mean/var/skew/excess kurt:\", float(mean_sp), float(var_sp), float(skew_sp), float(exkurt_sp))\n",
    "print(\"entropy:\", float(rv.entropy()))\n",
    "\n",
    "# Fit lambda_ from data using scipy.stats.fit (fix loc=0 implicitly)\n",
    "data = rv.rvs(size=2000, random_state=rng)\n",
    "fit_res = stats.fit(stats.planck, data, bounds={\"lambda\": (1e-6, 10.0)})\n",
    "print(\"fit params:\", fit_res.params)\n",
    "\n",
    "lam_hat_closed = planck_mle_lambda(data)\n",
    "print(\"closed-form MLE:\", lam_hat_closed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744f4d2",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing (likelihood-ratio test)\n",
    "A simple test for\n",
    "\\(H_0: \\lambda=\\lambda_0\\)\n",
    "uses the likelihood ratio statistic\n",
    "\\[\n",
    "\\Lambda = 2\\big(\\ell(\\widehat{\\lambda}) - \\ell(\\lambda_0)\\big).\n",
    "\\]\n",
    "Under regularity conditions and large \\(n\\), \\(\\Lambda\\) is approximately \\(\\chi^2_1\\) under \\(H_0\\).\n",
    "\n",
    "### Bayesian modeling (Beta–Geometric, then transform)\n",
    "Using the geometric parameterization\n",
    "\\(\\mathbb{P}(K=k)=p(1-p)^k\\),\n",
    "with a Beta prior \\(p\\sim\\mathrm{Beta}(\\alpha,\\beta)\\), the posterior is conjugate:\n",
    "\\[\n",
    " p\\mid k_{1:n} \\sim \\mathrm{Beta}\\Big(\\alpha+n,\\; \\beta+\\sum_i k_i\\Big).\n",
    "\\]\n",
    "Then \\(\\lambda\\) is a deterministic transform: \\(\\lambda = -\\log(1-p)\\).\n",
    "\n",
    "### Generative modeling\n",
    "Because Planck is geometric:\n",
    "- it is a natural count likelihood for **exponentially-tailed** discrete data\n",
    "- sums and mixtures connect it to common families (e.g. negative binomial)\n",
    "- it can serve as a simple component in hierarchical count models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planck_lrt(sample, lambda0: float):\n",
    "    \"\"\"Likelihood ratio test for H0: lambda_ = lambda0.\"\"\"\n",
    "    lambda0 = validate_lambda(lambda0)\n",
    "    sample = validate_sample(sample)\n",
    "\n",
    "    lam_hat = planck_mle_lambda(sample)\n",
    "    ll_hat = planck_loglik(lam_hat, sample) if np.isfinite(lam_hat) else 0.0\n",
    "    ll_0 = planck_loglik(lambda0, sample)\n",
    "\n",
    "    stat = 2.0 * (ll_hat - ll_0)\n",
    "    p_value = float(stats.chi2.sf(stat, df=1))\n",
    "    return stat, p_value, lam_hat\n",
    "\n",
    "\n",
    "# Hypothesis test example\n",
    "true_lambda = 1.2\n",
    "sample = stats.planck.rvs(true_lambda, size=1500, random_state=rng)\n",
    "\n",
    "lambda0 = 0.9\n",
    "stat, p_value, lam_hat = planck_lrt(sample, lambda0=lambda0)\n",
    "\n",
    "print(\"true lambda_:\", true_lambda)\n",
    "print(\"H0 lambda_:\", lambda0)\n",
    "print(\"MLE lambda_:\", lam_hat)\n",
    "print(\"LRT stat:\", stat)\n",
    "print(\"p-value :\", p_value)\n",
    "\n",
    "\n",
    "# Bayesian example: Beta prior on p = 1 - exp(-lambda_)\n",
    "alpha0, beta0 = 2.0, 2.0\n",
    "k = validate_sample(sample)\n",
    "\n",
    "n = k.size\n",
    "s = int(np.sum(k))\n",
    "\n",
    "alpha_post = alpha0 + n\n",
    "beta_post = beta0 + s\n",
    "\n",
    "n_draws = 20_000\n",
    "p_draws = rng.beta(alpha_post, beta_post, size=n_draws)\n",
    "\n",
    "# Transform to lambda = -log(1 - p)\n",
    "lambda_draws = -np.log1p(-p_draws)\n",
    "\n",
    "fig = px.histogram(\n",
    "    lambda_draws,\n",
    "    nbins=60,\n",
    "    title=f\"Posterior over lambda_ (Beta prior on p, alpha={alpha0}, beta={beta0})\",\n",
    "    labels={\"value\": \"lambda_\"},\n",
    ")\n",
    "fig.add_vline(x=true_lambda, line_dash=\"dash\", line_color=\"black\", annotation_text=\"true\")\n",
    "fig.add_vline(x=lam_hat, line_dash=\"dash\", line_color=\"red\", annotation_text=\"MLE\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703d8a4",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: \\(\\lambda\\le 0\\) is not valid.\n",
    "- **Numerical cancellation for small \\(\\lambda\\)**: computing \\(1-e^{-\\lambda}\\) as `1 - np.exp(-lambda_)` loses precision when \\(\\lambda\\) is small.\n",
    "  - Prefer `-np.expm1(-lambda_)` and `np.log(-np.expm1(-lambda_))`.\n",
    "- **Huge means for small \\(\\lambda\\)**: \\(\\mathbb{E}[K]=1/(e^{\\lambda}-1)\\approx 1/\\lambda\\) blows up as \\(\\lambda\\to 0\\).\n",
    "  - Monte Carlo estimates may need very large sample sizes to stabilize.\n",
    "- **Degeneracy as \\(\\lambda\\to\\infty\\)**: the distribution collapses toward 0; higher standardized moments become ill-conditioned.\n",
    "- **Fitting with location shifts**: if your data are supported on \\(\\{c,c+1,\\dots\\}\\), you need a `loc=c` shift (or subtract \\(c\\)) before using the base formulas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84538af0",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `planck` is a **discrete exponential** distribution on \\(\\{0,1,2,\\dots\\}\\) with PMF \\(\\mathbb{P}(K=k)=(1-e^{-\\lambda})e^{-\\lambda k}\\).\n",
    "- It is exactly a **geometric** distribution under \\(p=1-e^{-\\lambda}\\), and has the memoryless tail \\(\\mathbb{P}(K\\ge k)=e^{-\\lambda k}\\).\n",
    "- Closed forms are available for moments, MGF/CF, entropy, and the MLE \\(\\widehat{\\lambda}=\\log(1+1/\\bar{k})\\).\n",
    "- A simple NumPy-only sampler is \\(K=\\lfloor -\\log(U)/\\lambda\\rfloor\\) with \\(U\\sim\\mathrm{Unif}(0,1)\\).\n",
    "- In SciPy, use `scipy.stats.planck` for evaluation/sampling and `scipy.stats.fit` (or the closed-form MLE) for fitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}