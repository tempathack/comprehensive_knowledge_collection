{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46801606",
   "metadata": {},
   "source": [
    "# `binom` (Binomial distribution)\n",
    "\n",
    "The **binomial** distribution models the number of successes in a fixed number of independent yes/no trials.\n",
    "\n",
    "This notebook uses the same parameterization as `scipy.stats.binom`:\n",
    "- `n` = number of trials (a non-negative integer)\n",
    "- `p` = success probability per trial (a number in \\([0, 1]\\))\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- recognize when a binomial model is appropriate (and when it isn’t)\n",
    "- write down the PMF/CDF and key properties\n",
    "- derive the mean, variance, and likelihood\n",
    "- implement binomial sampling using **NumPy only**\n",
    "- visualize PMF/CDF and validate with Monte Carlo simulation\n",
    "- use `scipy.stats.binom` for computation and estimation workflows\n",
    "\n",
    "## Table of contents\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56372712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dceeb2",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name**: `binom` (Binomial distribution)  \n",
    "**Type**: **Discrete**  \n",
    "\n",
    "**Support**:\n",
    "\\[\n",
    "k \\in \\{0, 1, 2, \\dots, n\\}\n",
    "\\]\n",
    "\n",
    "**Parameter space**:\n",
    "\\[\n",
    "n \\in \\{0, 1, 2, \\dots\\},\\qquad p \\in [0, 1]\n",
    "\\]\n",
    "\n",
    "Interpretation:\n",
    "- `n` is the number of independent trials\n",
    "- `p` is the probability of “success” on each trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b90069",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "If you repeat the same yes/no experiment `n` times, and each trial succeeds with probability `p`, then\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathrm{Binomial}(n, p)\n",
    "\\]\n",
    "\n",
    "means:\n",
    "\n",
    "> **`X` counts how many successes you got out of `n`.**\n",
    "\n",
    "### What this distribution models\n",
    "- counting successes in a **fixed** number of independent trials\n",
    "- each trial has the **same** success probability\n",
    "\n",
    "### Typical real-world use cases\n",
    "- A/B tests: number of conversions out of `n` visitors\n",
    "- quality control: number of defective items in a batch (with independence as an approximation)\n",
    "- reliability: number of working components out of `n`\n",
    "- epidemiology: number of positive tests out of `n` (when tests are conditionally independent)\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Bernoulli**: if `n = 1`, then `Binomial(1, p)` is `Bernoulli(p)`.\n",
    "- **Poisson approximation**: if `n` is large and `p` is small with \\(\\lambda = np\\) fixed, then\n",
    "  \\(\\mathrm{Binomial}(n,p) \\approx \\mathrm{Poisson}(\\lambda)\\).\n",
    "- **Normal approximation (CLT)**: for large `n` with \\(np(1-p)\\) not tiny,\n",
    "  \\(X \\approx \\mathcal{N}(np,\\, np(1-p))\\) (often with a continuity correction).\n",
    "- **Beta–Binomial**: if `p` itself is random with a Beta prior, the marginal count distribution becomes beta–binomial.\n",
    "- **Negative binomial**: counts trials needed to reach a fixed number of successes (\"fixed successes\" vs \"fixed trials\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888dd9e4",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(X\\) be the number of successes in \\(n\\) independent Bernoulli trials with success probability \\(p\\).\n",
    "\n",
    "### PMF\n",
    "For \\(k \\in \\{0,1,\\dots,n\\}\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X = k)\n",
    "= \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "\\]\n",
    "\n",
    "and \\(\\mathbb{P}(X=k)=0\\) for integers \\(k\\notin\\{0,\\dots,n\\}\\).\n",
    "\n",
    "### CDF\n",
    "For a real number \\(x\\), the CDF is\n",
    "\n",
    "\\[\n",
    "F(x) = \\mathbb{P}(X \\le x)\n",
    "= \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "\\]\n",
    "\n",
    "with the conventions \\(F(x)=0\\) for \\(x<0\\) and \\(F(x)=1\\) for \\(x\\ge n\\).\n",
    "\n",
    "A useful special-function identity (often used for numerical evaluation) is:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X \\le k) = I_{1-p}(n-k,\\, k+1)\n",
    "\\]\n",
    "\n",
    "where \\(I\\) is the regularized incomplete beta function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62640c",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Mean and variance\n",
    "\\[\n",
    "\\mathbb{E}[X] = np,\\qquad \\mathrm{Var}(X) = np(1-p)\n",
    "\\]\n",
    "\n",
    "### Skewness and kurtosis\n",
    "Let \\(\\sigma^2 = np(1-p)\\). Then\n",
    "\n",
    "\\[\n",
    "\\text{skew}(X) = \\frac{1-2p}{\\sqrt{np(1-p)}}\n",
    "\\]\n",
    "\n",
    "The **excess** kurtosis is\n",
    "\n",
    "\\[\n",
    "\\text{excess kurt}(X) = \\frac{1 - 6p(1-p)}{np(1-p)}\n",
    "\\]\n",
    "\n",
    "(so the full kurtosis is \\(3 + \\text{excess kurt}(X)\\)).\n",
    "\n",
    "### MGF and characteristic function\n",
    "With \\(q = 1-p\\),\n",
    "\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = (q + p e^t)^n\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = (q + p e^{it})^n\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "The (Shannon) entropy is\n",
    "\n",
    "\\[\n",
    "H(X) = -\\sum_{k=0}^{n} \\mathbb{P}(X=k)\\,\\log \\mathbb{P}(X=k)\n",
    "\\]\n",
    "\n",
    "There is no simple closed form in general, but it is easy to compute numerically for moderate `n`.\n",
    "\n",
    "### Other useful properties\n",
    "- **Mode**: \\(\\lfloor (n+1)p \\rfloor\\), with a tie when \\((n+1)p\\) is an integer.\n",
    "- **Additivity (same `p`)**: if \\(X\\sim\\text{Bin}(n_1,p)\\) and \\(Y\\sim\\text{Bin}(n_2,p)\\) independent,\n",
    "  then \\(X+Y\\sim\\text{Bin}(n_1+n_2,p)\\).\n",
    "- **Complement symmetry**: if \\(X\\sim\\text{Bin}(n,p)\\), then \\(n-X\\sim\\text{Bin}(n,1-p)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c786c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_n_p(n, p):\n",
    "    if isinstance(n, bool) or not isinstance(n, (int, np.integer)):\n",
    "        raise TypeError(\"n must be an integer\")\n",
    "    n_int = int(n)\n",
    "    if n_int < 0:\n",
    "        raise ValueError(\"n must be >= 0\")\n",
    "\n",
    "    p_float = float(p)\n",
    "    if not (0.0 <= p_float <= 1.0):\n",
    "        raise ValueError(\"p must be in [0, 1]\")\n",
    "    return n_int, p_float\n",
    "\n",
    "\n",
    "def binom_logpmf(k, n, p):\n",
    "    # Log PMF for Bin(n,p) for integer k. Returns -inf outside support.\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    k_arr = np.asarray(k)\n",
    "\n",
    "    out = np.full(k_arr.shape, -np.inf, dtype=float)\n",
    "\n",
    "    k_int = k_arr.astype(int)\n",
    "    valid = (k_int == k_arr) & (k_int >= 0) & (k_int <= n)\n",
    "    if not np.any(valid):\n",
    "        return out\n",
    "\n",
    "    if p == 0.0:\n",
    "        out[valid & (k_int == 0)] = 0.0\n",
    "        return out\n",
    "    if p == 1.0:\n",
    "        out[valid & (k_int == n)] = 0.0\n",
    "        return out\n",
    "\n",
    "    kv = k_int[valid]\n",
    "    log_binom_coeff = (\n",
    "        math.lgamma(n + 1)\n",
    "        - np.vectorize(math.lgamma)(kv + 1)\n",
    "        - np.vectorize(math.lgamma)(n - kv + 1)\n",
    "    )\n",
    "\n",
    "    out[valid] = log_binom_coeff + kv * math.log(p) + (n - kv) * math.log1p(-p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def binom_pmf(k, n, p):\n",
    "    return np.exp(binom_logpmf(k, n, p))\n",
    "\n",
    "\n",
    "def binom_pmf_support(n):\n",
    "    n, _ = _validate_n_p(n, 0.5)\n",
    "    return np.arange(n + 1)\n",
    "\n",
    "\n",
    "def binom_pmf_array(n, p):\n",
    "    ks = binom_pmf_support(n)\n",
    "    return ks, binom_pmf(ks, n, p)\n",
    "\n",
    "\n",
    "def binom_cdf(x, n, p):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    x_arr = np.asarray(x)\n",
    "\n",
    "    _, pmf = binom_pmf_array(n, p)\n",
    "    cdf = np.cumsum(pmf)\n",
    "    cdf = np.clip(cdf, 0.0, 1.0)\n",
    "\n",
    "    k = np.floor(x_arr).astype(int)\n",
    "    out = np.zeros_like(x_arr, dtype=float)\n",
    "    out[x_arr >= n] = 1.0\n",
    "\n",
    "    inside = (x_arr >= 0) & (x_arr < n)\n",
    "    if np.any(inside):\n",
    "        out[inside] = cdf[k[inside]]\n",
    "    return out\n",
    "\n",
    "\n",
    "def binom_moments(n, p):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    mean = n * p\n",
    "    var = n * p * (1.0 - p)\n",
    "\n",
    "    if var == 0.0:\n",
    "        skew = float(\"nan\")\n",
    "        excess_kurt = float(\"nan\")\n",
    "    else:\n",
    "        skew = (1.0 - 2.0 * p) / math.sqrt(var)\n",
    "        excess_kurt = (1.0 - 6.0 * p * (1.0 - p)) / var\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"var\": var,\n",
    "        \"skew\": skew,\n",
    "        \"excess_kurt\": excess_kurt,\n",
    "    }\n",
    "\n",
    "\n",
    "def binom_entropy(n, p, *, base=math.e):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    _, pmf = binom_pmf_array(n, p)\n",
    "\n",
    "    mask = pmf > 0\n",
    "    H_nats = -np.sum(pmf[mask] * np.log(pmf[mask]))\n",
    "\n",
    "    if base == math.e:\n",
    "        return float(H_nats)\n",
    "    return float(H_nats / math.log(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 40, 0.25\n",
    "\n",
    "moments = binom_moments(n, p)\n",
    "moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check (matches formulas up to sampling error)\n",
    "samples = rng.binomial(n=n, p=p, size=200_000)\n",
    "\n",
    "est_mean = samples.mean()\n",
    "est_var = samples.var(ddof=0)\n",
    "\n",
    "{\n",
    "    \"formula_mean\": moments[\"mean\"],\n",
    "    \"mc_mean\": float(est_mean),\n",
    "    \"formula_var\": moments[\"var\"],\n",
    "    \"mc_var\": float(est_var),\n",
    "    \"entropy_nats\": binom_entropy(n, p),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dbb487",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "- **`n` (number of trials)** controls the scale and granularity.\n",
    "  Larger `n` tends to make the distribution wider and (often) more bell-shaped.\n",
    "- **`p` (success probability)** controls the location and skew:\n",
    "  - if `p < 0.5`, most mass is near 0 with right-skew\n",
    "  - if `p = 0.5`, the distribution is symmetric around `n/2`\n",
    "  - if `p > 0.5`, most mass is near `n` with left-skew\n",
    "\n",
    "Two helpful identities:\n",
    "\\[\n",
    "\\mathbb{E}[X]=np,\\qquad \\mathrm{Var}(X)=np(1-p)\n",
    "\\]\n",
    "\n",
    "so increasing `p` shifts the distribution right, and increasing `n` typically increases both the mean and the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c92ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "n_fixed = 20\n",
    "p_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "p_fixed = 0.3\n",
    "n_values = [5, 20, 100]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        f\"PMF vs p (n={n_fixed})\",\n",
    "        f\"PMF vs n (p={p_fixed})\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "ks = binom_pmf_support(n_fixed)\n",
    "for p_ in p_values:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=ks,\n",
    "            y=binom_pmf(ks, n_fixed, p_),\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"p={p_}\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "for n_ in n_values:\n",
    "    ks = binom_pmf_support(n_)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=ks,\n",
    "            y=binom_pmf(ks, n_, p_fixed),\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"n={n_}\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"P(X=k)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"P(X=k)\", row=1, col=2)\n",
    "fig.update_layout(height=420, legend_title_text=\"\")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621bb8f",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "A standard way to derive binomial moments is to write \\(X\\) as a sum of indicator variables.\n",
    "\n",
    "### Expectation\n",
    "Let \\(I_i\\) indicate success on trial \\(i\\): \\(I_i\\in\\{0,1\\}\\) with \\(\\mathbb{P}(I_i=1)=p\\).\n",
    "\n",
    "Then\n",
    "\\[\n",
    "X = \\sum_{i=1}^{n} I_i\n",
    "\\]\n",
    "\n",
    "and by linearity of expectation:\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\sum_{i=1}^n \\mathbb{E}[I_i] = \\sum_{i=1}^n p = np\n",
    "\\]\n",
    "\n",
    "### Variance\n",
    "If trials are independent, \\(\\mathrm{Cov}(I_i,I_j)=0\\) for \\(i\\ne j\\). So:\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\sum_{i=1}^{n} \\mathrm{Var}(I_i)\n",
    "= \\sum_{i=1}^n p(1-p)\n",
    "= np(1-p)\n",
    "\\]\n",
    "\n",
    "### Likelihood (single observation)\n",
    "If you observe \\(X=k\\) with known `n`, the likelihood of `p` is\n",
    "\n",
    "\\[\n",
    "L(p\\mid k) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "\\]\n",
    "\n",
    "Taking logs (and dropping constants that do not depend on `p`):\n",
    "\n",
    "\\[\n",
    "\\ell(p) = k\\log p + (n-k)\\log(1-p)\n",
    "\\]\n",
    "\n",
    "Differentiate and set to zero:\n",
    "\n",
    "\\[\n",
    "\\ell'(p) = \\frac{k}{p} - \\frac{n-k}{1-p} = 0\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\hat p = \\frac{k}{n}\n",
    "\\]\n",
    "\n",
    "So the MLE is the observed success fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the likelihood for p (single observation)\n",
    "n = 30\n",
    "k_obs = 9\n",
    "\n",
    "p_grid = np.linspace(1e-6, 1 - 1e-6, 600)\n",
    "log_binom_coeff = math.lgamma(n + 1) - math.lgamma(k_obs + 1) - math.lgamma(n - k_obs + 1)\n",
    "logL = log_binom_coeff + k_obs * np.log(p_grid) + (n - k_obs) * np.log1p(-p_grid)\n",
    "\n",
    "p_hat = k_obs / n\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=p_grid, y=logL, mode=\"lines\", name=\"log-likelihood\"))\n",
    "fig.add_vline(x=p_hat, line_dash=\"dash\", line_color=\"black\", annotation_text=f\"MLE p̂={p_hat:.3f}\")\n",
    "fig.update_layout(title=\"Binomial log-likelihood for p (n fixed)\", xaxis_title=\"p\", yaxis_title=\"log L(p)\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5332ec1",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "Below are two **NumPy-only** sampling strategies.\n",
    "\n",
    "### A) Sum of Bernoulli trials\n",
    "Directly simulate `n` Bernoulli trials and count successes.\n",
    "\n",
    "- Correct by construction\n",
    "- Cost: \\(O(n\\cdot \\text{size})\\)\n",
    "\n",
    "### B) Inverse CDF (inverse transform sampling)\n",
    "1. Compute probabilities \\(\\mathbb{P}(X=k)\\) for \\(k=0,\\dots,n\\)\n",
    "2. Compute the cumulative sum to get the CDF on the support\n",
    "3. Draw \\(U\\sim\\text{Uniform}(0,1)\\) and return the smallest \\(k\\) with \\(F(k)\\ge U\\)\n",
    "\n",
    "- Cost: \\(O(n + \\text{size}\\log n)\\) with binary search (`np.searchsorted`)\n",
    "- Useful for didactic purposes; production implementations use more specialized algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d05bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_binom_bernoulli_sum(n, p, size=1, *, rng: np.random.Generator):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    u = rng.random(size=np.atleast_1d(size).tolist() + [n])\n",
    "    return (u < p).sum(axis=-1)\n",
    "\n",
    "\n",
    "def sample_binom_inverse_cdf(n, p, size=1, *, rng: np.random.Generator):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "\n",
    "    if p == 0.0:\n",
    "        return np.zeros(size, dtype=int)\n",
    "    if p == 1.0:\n",
    "        return np.full(size, n, dtype=int)\n",
    "\n",
    "    ks = np.arange(n + 1)\n",
    "\n",
    "    # Compute PMF stably via log-space, then normalize.\n",
    "    logp = binom_logpmf(ks, n, p)\n",
    "    logp = logp - np.max(logp)\n",
    "    pmf = np.exp(logp)\n",
    "    pmf = pmf / pmf.sum()\n",
    "\n",
    "    cdf = np.cumsum(pmf)\n",
    "    cdf[-1] = 1.0\n",
    "\n",
    "    u = rng.random(size=size)\n",
    "    return np.searchsorted(cdf, u, side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4aafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 25, 0.35\n",
    "size = 50_000\n",
    "\n",
    "x1 = sample_binom_bernoulli_sum(n, p, size=size, rng=rng)\n",
    "x2 = sample_binom_inverse_cdf(n, p, size=size, rng=rng)\n",
    "\n",
    "{\n",
    "    \"bernoulli_sum_mean\": float(x1.mean()),\n",
    "    \"inverse_cdf_mean\": float(x2.mean()),\n",
    "    \"theory_mean\": n * p,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f63dbe",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** on \\(\\{0,\\dots,n\\}\\)\n",
    "- the **CDF** (as a step function)\n",
    "- Monte Carlo samples vs the exact PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afed9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 30, 0.4\n",
    "ks, pmf = binom_pmf_array(n, p)\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=ks, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(title=f\"Binomial PMF (n={n}, p={p})\", xaxis_title=\"k\", yaxis_title=\"P(X=k)\")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=ks, y=cdf, mode=\"lines\", line_shape=\"hv\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(title=f\"Binomial CDF (n={n}, p={p})\", xaxis_title=\"k\", yaxis_title=\"P(X≤k)\")\n",
    "fig_cdf.show()\n",
    "\n",
    "mc = sample_binom_inverse_cdf(n, p, size=200_000, rng=rng)\n",
    "counts = np.bincount(mc, minlength=n + 1)\n",
    "pmf_hat = counts / counts.sum()\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=ks, y=pmf_hat, name=\"Monte Carlo\", opacity=0.6))\n",
    "fig_mc.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers+lines\", name=\"Exact PMF\"))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo vs exact PMF (n={n}, p={p})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9319fd1",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides a fast, numerically robust implementation via `scipy.stats.binom`.\n",
    "\n",
    "- Use `pmf`, `cdf`, `sf` (survival function), `rvs`, `logpmf`, `logcdf`, …\n",
    "- For parameter estimation, `binom` does **not** currently expose a `.fit()` method (SciPy 1.15).\n",
    "  For `n` known, the MLE for `p` is closed-form; otherwise you can do custom optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.stats import binom\n",
    "\n",
    "n, p = 12, 0.2\n",
    "ks = np.arange(n + 1)\n",
    "\n",
    "pmf_scipy = binom.pmf(ks, n, p)\n",
    "cdf_scipy = binom.cdf(ks, n, p)\n",
    "samples_scipy = binom.rvs(n, p, size=10_000, random_state=rng)\n",
    "\n",
    "{\n",
    "    \"pmf_sum\": float(pmf_scipy.sum()),\n",
    "    \"cdf_last\": float(cdf_scipy[-1]),\n",
    "    \"sample_mean\": float(samples_scipy.mean()),\n",
    "    \"theory_mean\": n * p,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Fit\" p with n known (closed-form MLE + SciPy optimization check)\n",
    "n_true, p_true = 20, 0.35\n",
    "data = binom.rvs(n_true, p_true, size=2_000, random_state=rng)\n",
    "\n",
    "p_mle_closed = data.mean() / n_true\n",
    "\n",
    "def nll(p):\n",
    "    if not (0.0 < p < 1.0):\n",
    "        return float(\"inf\")\n",
    "    return -binom.logpmf(data, n_true, p).sum()\n",
    "\n",
    "res = minimize_scalar(nll, bounds=(1e-9, 1 - 1e-9), method=\"bounded\")\n",
    "\n",
    "{\n",
    "    \"p_true\": p_true,\n",
    "    \"p_mle_closed\": float(p_mle_closed),\n",
    "    \"p_mle_opt\": float(res.x),\n",
    "    \"opt_success\": bool(res.success),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc8657",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (exact binomial test)\n",
    "Test whether a success probability equals a reference value \\(p_0\\).\n",
    "\n",
    "### B) Bayesian modeling (Beta–Binomial conjugacy)\n",
    "If \\(p\\sim\\text{Beta}(\\alpha,\\beta)\\) and \\(X\\mid p \\sim\\text{Bin}(n,p)\\), then the posterior is\n",
    "\n",
    "\\[\n",
    "p\\mid X=k \\sim \\text{Beta}(\\alpha + k,\\, \\beta + n - k)\n",
    "\\]\n",
    "\n",
    "This is one of the cleanest examples of conjugacy.\n",
    "\n",
    "### C) Generative modeling (counts conditioned on probabilities)\n",
    "Binomial observations show up whenever you generate **counts** given probabilities,\n",
    "e.g. modeling conversions, click-throughs, and success/failure outcomes at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ff7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta, betabinom, binomtest\n",
    "\n",
    "# A) Hypothesis test\n",
    "n = 50\n",
    "k = 18\n",
    "p0 = 0.25\n",
    "\n",
    "test = binomtest(k=k, n=n, p=p0, alternative=\"two-sided\")\n",
    "ci = test.proportion_ci(confidence_level=0.95)\n",
    "\n",
    "{\n",
    "    \"k\": k,\n",
    "    \"n\": n,\n",
    "    \"p0\": p0,\n",
    "    \"p_value\": float(test.pvalue),\n",
    "    \"95%_CI\": (float(ci.low), float(ci.high)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Bayesian update with Beta prior\n",
    "alpha0, beta0 = 2.0, 2.0\n",
    "n, k = 50, 18\n",
    "\n",
    "alpha_post = alpha0 + k\n",
    "beta_post = beta0 + (n - k)\n",
    "\n",
    "posterior_mean = alpha_post / (alpha_post + beta_post)\n",
    "posterior_ci = beta.ppf([0.025, 0.975], alpha_post, beta_post)\n",
    "\n",
    "x = np.linspace(0, 1, 400)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=beta.pdf(x, alpha0, beta0), mode=\"lines\", name=\"prior Beta(2,2)\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=beta.pdf(x, alpha_post, beta_post),\n",
    "        mode=\"lines\",\n",
    "        name=f\"posterior Beta({alpha_post:.0f},{beta_post:.0f})\",\n",
    "    )\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=posterior_mean,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=f\"posterior mean={posterior_mean:.3f}\",\n",
    ")\n",
    "fig.update_layout(title=\"Beta prior → Beta posterior for p\", xaxis_title=\"p\", yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"posterior_mean\": float(posterior_mean),\n",
    "    \"posterior_95%_CI\": (float(posterior_ci[0]), float(posterior_ci[1])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive: future successes out of m trials\n",
    "m = 20\n",
    "k_new = np.arange(m + 1)\n",
    "\n",
    "pred_pmf = betabinom.pmf(k_new, m, alpha_post, beta_post)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=k_new, y=pred_pmf, name=\"posterior predictive\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Posterior predictive for X_new | data (m={m})\",\n",
    "    xaxis_title=\"k_new\",\n",
    "    yaxis_title=\"P(X_new=k_new)\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32690a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Simple generative example: daily conversions\n",
    "days = 60\n",
    "visitors = rng.integers(200, 1200, size=days)\n",
    "\n",
    "# latent daily conversion rate (captures extra variability beyond pure binomial)\n",
    "p_day = rng.beta(30, 70, size=days)\n",
    "conversions = np.array([rng.binomial(n=v, p=p) for v, p in zip(visitors, p_day)])\n",
    "\n",
    "df = {\n",
    "    \"day\": np.arange(days),\n",
    "    \"visitors\": visitors,\n",
    "    \"conversions\": conversions,\n",
    "    \"rate\": conversions / visitors,\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df[\"day\"], y=df[\"rate\"], mode=\"markers+lines\", name=\"observed rate\"))\n",
    "fig.update_layout(title=\"Simulated conversion rates\", xaxis_title=\"day\", yaxis_title=\"conversions / visitors\")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"avg_visitors\": float(np.mean(visitors)),\n",
    "    \"avg_rate\": float(np.mean(df[\"rate\"])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ef3a3",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "### Invalid parameters\n",
    "- `n` must be a **non-negative integer**.\n",
    "- `p` must lie in **\\([0,1]\\)**.\n",
    "\n",
    "### Numerical issues\n",
    "- For large `n`, the PMF can underflow in floating point. Prefer **log-space** computations:\n",
    "  `scipy.stats.binom.logpmf`, `logcdf`, etc.\n",
    "- For tail probabilities, prefer `sf` (survival function) over `1 - cdf` to avoid catastrophic cancellation.\n",
    "\n",
    "### Modeling issues\n",
    "- **Independence** can be violated (e.g. clustered trials, repeated users). A beta–binomial model can capture over-dispersion.\n",
    "- **Constant p** across trials may be unrealistic (mixture models / hierarchical models often fit better).\n",
    "- For sampling **without replacement**, a **hypergeometric** distribution is more appropriate.\n",
    "\n",
    "### Approximation gotchas\n",
    "- Poisson approximation requires `n` large, `p` small, and \\(np\\) moderate.\n",
    "- Normal approximation is best when \\(np(1-p)\\) is reasonably large (rule-of-thumb: \\(\\ge 10\\))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20387bd3",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `binom` is a **discrete** distribution on \\(\\{0,\\dots,n\\}\\) counting successes in `n` Bernoulli trials.\n",
    "- PMF: \\(\\binom{n}{k}p^k(1-p)^{n-k}\\), mean \\(np\\), variance \\(np(1-p)\\).\n",
    "- Derivations are clean via the indicator-sum representation \\(X=\\sum I_i\\).\n",
    "- For computation and tail probabilities, prefer SciPy’s `binom` (especially in log-space).\n",
    "- When `p` varies across trials or trials aren’t independent, consider richer models (e.g. beta–binomial).\n",
    "\n",
    "**References**\n",
    "- SciPy: `scipy.stats.binom` and `scipy.stats.binomtest`\n",
    "- Any standard probability text (e.g., Casella & Berger, *Statistical Inference*)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}