{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d9b4a",
   "metadata": {},
   "source": [
    "# Bernoulli distribution (`bernoulli`)\n",
    "\n",
    "The Bernoulli distribution models a single binary outcome: **success (1)** with probability $p$ and **failure (0)** with probability $1-p$.\n",
    "\n",
    "## Learning goals\n",
    "- Recognize Bernoulli data and common modeling patterns.\n",
    "- Write the PMF and CDF in closed form.\n",
    "- Compute and interpret mean/variance and other moments.\n",
    "- Derive the likelihood and the MLE for $p$.\n",
    "- Simulate Bernoulli trials (NumPy-only) and visualize behavior.\n",
    "- Use `scipy.stats.bernoulli` and `scipy.stats.fit`.\n",
    "\n",
    "## Prerequisites\n",
    "- Basic probability (PMF/CDF), expectation, and variance\n",
    "- Comfort with logs and derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18f4f6",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations (Expectation, Variance, Likelihood)\n",
    "7. Sampling & Simulation (NumPy-only)\n",
    "8. Visualization (PMF, CDF, Monte Carlo)\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.special import xlogy, xlog1py\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fda98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import plotly\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"SciPy:\", scipy.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"Seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2d06f",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `bernoulli` (Bernoulli distribution)\n",
    "- **Type**: **Discrete**\n",
    "- **Support**: $x \\in \\{0, 1\\}$\n",
    "- **Parameter space**: $p \\in [0, 1]$ (often $p \\in (0,1)$ for a non-degenerate distribution)\n",
    "\n",
    "Notation:\n",
    "- $X \\sim \\mathrm{Bernoulli}(p)$ or $X \\sim \\mathrm{Ber}(p)$.\n",
    "- You will also see the parameter written as $\\theta$ instead of $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9c3ac",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "A Bernoulli random variable is the mathematical abstraction of a *single yes/no trial*.\n",
    "\n",
    "- **What it models**: a single occurrence/non-occurrence event.\n",
    "  - Examples: click/no-click, fraud/not, defect/not, recovered/not at a fixed time, coin toss.\n",
    "- **Typical use cases**:\n",
    "  - Binary labels in classification (Bernoulli likelihood underlying logistic regression).\n",
    "  - A/B testing and conversion-rate estimation.\n",
    "  - Random masks (e.g., dropout in neural networks).\n",
    "  - Reliability: component works/fails.\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Binomial**: if $X_1,\\dots,X_n$ are i.i.d. Bernoulli$(p)$, then $\\sum_{i=1}^n X_i \\sim \\mathrm{Binomial}(n,p)$.\n",
    "- **Categorical**: Bernoulli is a categorical distribution with two categories.\n",
    "- **Beta–Bernoulli**: a Beta prior on $p$ is conjugate and yields a closed-form posterior.\n",
    "- **Geometric**: the number of trials until the first success in repeated Bernoulli trials is geometric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d2b8e",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $X \\sim \\mathrm{Bernoulli}(p)$ with $p \\in [0,1]$.\n",
    "\n",
    "### PMF\n",
    "$$\n",
    "\\mathbb{P}(X = x) =\n",
    "\\begin{cases}\n",
    "1-p & x=0 \\\\\n",
    "p   & x=1 \\\\\n",
    "0   & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A compact form (valid for $x \\in \\{0,1\\}$) is:\n",
    "$$\n",
    "\\mathbb{P}(X=x) = p^x (1-p)^{1-x}.\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "Because this is a discrete distribution, the CDF is a step function:\n",
    "$$\n",
    "F(x)=\\mathbb{P}(X \\le x)=\n",
    "\\begin{cases}\n",
    "0 & x < 0 \\\\\n",
    "1-p & 0 \\le x < 1 \\\\\n",
    "1 & x \\ge 1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Indicator view\n",
    "If $A$ is an event with $\\mathbb{P}(A)=p$, then the indicator $X=\\mathbf{1}\\{A\\}$ is Bernoulli$(p)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_p(p: float) -> float:\n",
    "    p = float(p)\n",
    "    if not (0.0 <= p <= 1.0):\n",
    "        raise ValueError(f\"p must be in [0, 1], got {p!r}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def bernoulli_pmf(x, p: float):\n",
    "    \"\"\"PMF of Bernoulli(p) evaluated at x.\"\"\"\n",
    "    p = validate_p(p)\n",
    "    x = np.asarray(x)\n",
    "    return np.where(x == 1, p, np.where(x == 0, 1.0 - p, 0.0)).astype(float)\n",
    "\n",
    "\n",
    "def bernoulli_cdf(x, p: float):\n",
    "    \"\"\"CDF of Bernoulli(p) evaluated at x (step function).\"\"\"\n",
    "    p = validate_p(p)\n",
    "    x = np.asarray(x)\n",
    "    return np.where(x < 0, 0.0, np.where(x < 1, 1.0 - p, 1.0)).astype(float)\n",
    "\n",
    "\n",
    "p_demo = 0.3\n",
    "x_demo = np.array([-1, 0, 1, 2])\n",
    "print(\"x:\", x_demo)\n",
    "print(\"pmf:\", bernoulli_pmf(x_demo, p_demo))\n",
    "print(\"cdf:\", bernoulli_cdf(x_demo, p_demo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f8b8b",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "For $X \\sim \\mathrm{Bernoulli}(p)$, define $q = 1-p$.\n",
    "\n",
    "- **Mean**: $\\mathbb{E}[X] = p$\n",
    "- **Variance**: $\\mathrm{Var}(X) = pq$\n",
    "- **Skewness** (for $p \\in (0,1)$):\n",
    "$$\n",
    "\\gamma_1 = \\frac{\\mathbb{E}[(X-\\mu)^3]}{\\sigma^3} = \\frac{1-2p}{\\sqrt{pq}}\n",
    "$$\n",
    "- **Kurtosis**:\n",
    "  - Central 4th moment: $\\mu_4 = pq(1-3pq)$\n",
    "  - **Excess kurtosis** (kurtosis minus 3):\n",
    "$$\n",
    "\\gamma_2 = \\frac{\\mu_4}{(pq)^2} - 3 = \\frac{1}{pq} - 6\n",
    "$$\n",
    "  (Undefined at $p \\in \\{0,1\\}$ because $\\sigma^2=0$.)\n",
    "\n",
    "### MGF and characteristic function\n",
    "- **MGF**:\n",
    "$$\n",
    "M_X(t)=\\mathbb{E}[e^{tX}] = q + p e^t = 1-p + p e^t\n",
    "$$\n",
    "- **Characteristic function**:\n",
    "$$\n",
    "\\varphi_X(t)=\\mathbb{E}[e^{itX}] = 1-p + p e^{it}\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "Using natural logarithms (nats):\n",
    "$$\n",
    "H(X) = -p\\log p - q \\log q.\n",
    "$$\n",
    "This is maximized at $p=1/2$ and equals 0 at $p=0$ or $p=1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_mean(p: float) -> float:\n",
    "    return validate_p(p)\n",
    "\n",
    "\n",
    "def bernoulli_var(p: float) -> float:\n",
    "    p = validate_p(p)\n",
    "    return p * (1.0 - p)\n",
    "\n",
    "\n",
    "def bernoulli_skewness(p: float) -> float:\n",
    "    p = validate_p(p)\n",
    "    v = bernoulli_var(p)\n",
    "    if v == 0.0:\n",
    "        return np.nan\n",
    "    return (1.0 - 2.0 * p) / np.sqrt(v)\n",
    "\n",
    "\n",
    "def bernoulli_excess_kurtosis(p: float) -> float:\n",
    "    p = validate_p(p)\n",
    "    v = bernoulli_var(p)\n",
    "    if v == 0.0:\n",
    "        return np.nan\n",
    "    return 1.0 / v - 6.0\n",
    "\n",
    "\n",
    "def bernoulli_mgf(t, p: float):\n",
    "    p = validate_p(p)\n",
    "    t = np.asarray(t)\n",
    "    return (1.0 - p) + p * np.exp(t)\n",
    "\n",
    "\n",
    "def bernoulli_cf(t, p: float):\n",
    "    p = validate_p(p)\n",
    "    t = np.asarray(t)\n",
    "    return (1.0 - p) + p * np.exp(1j * t)\n",
    "\n",
    "\n",
    "def bernoulli_entropy(p: float, base=np.e):\n",
    "    p = validate_p(p)\n",
    "    p_arr = np.asarray(p, dtype=float)\n",
    "    h = -(xlogy(p_arr, p_arr) + xlog1py(1.0 - p_arr, -p_arr))  # 0*log(0) -> 0\n",
    "    if base == 2:\n",
    "        return h / np.log(2)\n",
    "    if base != np.e:\n",
    "        return h / np.log(base)\n",
    "    return h\n",
    "\n",
    "\n",
    "# Monte Carlo sanity checks\n",
    "p = 0.27\n",
    "n = 200_000\n",
    "x = (rng.random(n) < p).astype(int)\n",
    "\n",
    "mu_hat = x.mean()\n",
    "var_hat = x.var(ddof=0)\n",
    "\n",
    "mu = bernoulli_mean(p)\n",
    "var = bernoulli_var(p)\n",
    "\n",
    "mu3 = ((x - mu_hat) ** 3).mean()\n",
    "mu4 = ((x - mu_hat) ** 4).mean()\n",
    "skew_hat = mu3 / (var_hat ** 1.5)\n",
    "excess_kurt_hat = mu4 / (var_hat**2) - 3.0\n",
    "\n",
    "t = 1.2\n",
    "mgf_hat = np.exp(t * x).mean()\n",
    "cf_hat = np.exp(1j * t * x).mean()\n",
    "\n",
    "print(f\"p = {p}\")\n",
    "print(f\"E[X] theory={mu:.4f} | MC={mu_hat:.4f}\")\n",
    "print(f\"Var(X) theory={var:.4f} | MC={var_hat:.4f}\")\n",
    "print(f\"skewness theory={bernoulli_skewness(p):.4f} | MC={skew_hat:.4f}\")\n",
    "print(f\"excess kurtosis theory={bernoulli_excess_kurtosis(p):.4f} | MC={excess_kurt_hat:.4f}\")\n",
    "print(f\"MGF(t) theory={bernoulli_mgf(t, p):.4f} | MC={mgf_hat:.4f}\")\n",
    "print(f\"CF(t)  theory={bernoulli_cf(t, p):.4f} | MC={cf_hat:.4f}\")\n",
    "print(f\"Entropy (nats) theory={bernoulli_entropy(p):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05c1e2",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The single parameter $p$ is the probability of observing a 1 (“success”).\n",
    "\n",
    "- As $p \\to 0$, almost all mass is at 0.\n",
    "- As $p \\to 1$, almost all mass is at 1.\n",
    "- At $p = 1/2$, the distribution is balanced.\n",
    "\n",
    "A common reparameterization is the **log-odds**:\n",
    "$$\n",
    "\\operatorname{logit}(p) = \\log\\frac{p}{1-p},\n",
    "$$\n",
    "which is the natural parameter in logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ed0cf",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation\n",
    "Because $X\\in\\{0,1\\}$:\n",
    "$$\n",
    "\\mathbb{E}[X] = 0\\cdot(1-p) + 1\\cdot p = p.\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "Use $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ and note $X^2 = X$ for $X\\in\\{0,1\\}$:\n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\mathbb{E}[X] = p\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\mathrm{Var}(X)=p-p^2=p(1-p).\n",
    "$$\n",
    "\n",
    "### Likelihood (i.i.d. sample)\n",
    "Let $x_1,\\dots,x_n \\in \\{0,1\\}$ be i.i.d. Bernoulli$(p)$. The likelihood is\n",
    "$$\n",
    "L(p; x_{1:n}) = \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}.\n",
    "$$\n",
    "Let $k=\\sum_i x_i$ be the number of successes. Then\n",
    "$$\n",
    "L(p; x_{1:n}) = p^k (1-p)^{n-k}.\n",
    "$$\n",
    "\n",
    "The log-likelihood is\n",
    "$$\n",
    "\\ell(p) = k \\log p + (n-k)\\log(1-p).\n",
    "$$\n",
    "\n",
    "Differentiate and set to zero:\n",
    "$$\n",
    "\\ell'(p)=\\frac{k}{p}-\\frac{n-k}{1-p}=0\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\hat{p}_{\\text{MLE}}=\\frac{k}{n}=\\bar{x}.\n",
    "$$\n",
    "\n",
    "The second derivative $\\ell''(p)=-k/p^2-(n-k)/(1-p)^2<0$ for $p\\in(0,1)$, so this is a maximum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_log_likelihood(p, x) -> float:\n",
    "    \"\"\"Bernoulli log-likelihood for i.i.d. data x in {0,1}.\"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    x = np.asarray(x)\n",
    "    if not np.all((x == 0) | (x == 1)):\n",
    "        raise ValueError(\"x must contain only 0/1 values\")\n",
    "\n",
    "    k = x.sum()\n",
    "    n = x.size\n",
    "\n",
    "    eps = 1e-12\n",
    "    p = np.clip(p, eps, 1.0 - eps)\n",
    "    return k * np.log(p) + (n - k) * np.log1p(-p)\n",
    "\n",
    "\n",
    "# Visualize the log-likelihood and the MLE\n",
    "p_true = 0.35\n",
    "n = 40\n",
    "x = (rng.random(n) < p_true).astype(int)\n",
    "k = int(x.sum())\n",
    "\n",
    "p_hat = k / n\n",
    "grid = np.linspace(1e-4, 1.0 - 1e-4, 600)\n",
    "ll = bernoulli_log_likelihood(grid, x)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=grid, y=ll, mode=\"lines\", name=\"log-likelihood\"))\n",
    "fig.add_vline(x=p_true, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"p_true\")\n",
    "fig.add_vline(x=p_hat, line_dash=\"dash\", line_color=\"red\", annotation_text=\"p_hat (MLE)\")\n",
    "fig.update_layout(\n",
    "    title=f\"Bernoulli log-likelihood (n={n}, k={k})\",\n",
    "    xaxis_title=\"p\",\n",
    "    yaxis_title=\"log L(p)\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56ce7c",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "A simple sampler uses inverse transform sampling with a uniform random variable:\n",
    "\n",
    "1. Draw $U \\sim \\mathrm{Uniform}(0,1)$.\n",
    "2. Return $X = 1$ if $U < p$, else return $X = 0$.\n",
    "\n",
    "This works because $\\mathbb{P}(U < p)=p$.\n",
    "\n",
    "Vectorized implementation:\n",
    "- Generate `U = rng.random(size)`\n",
    "- Compute `(U < p).astype(int)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_bernoulli_numpy(p: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    p = validate_p(p)\n",
    "    u = rng.random(size)\n",
    "    return (u < p).astype(int)\n",
    "\n",
    "\n",
    "p = 0.4\n",
    "n = 5_000\n",
    "x = sample_bernoulli_numpy(p, n, rng=rng)\n",
    "\n",
    "# Running mean to illustrate the law of large numbers\n",
    "running_mean = np.cumsum(x) / (np.arange(n) + 1)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=running_mean, mode=\"lines\", name=\"running mean\"))\n",
    "fig.add_hline(y=p, line_dash=\"dash\", line_color=\"red\", annotation_text=\"p\")\n",
    "fig.update_layout(\n",
    "    title=\"Law of Large Numbers: running mean of Bernoulli samples\",\n",
    "    xaxis_title=\"n\",\n",
    "    yaxis_title=\"cumulative mean\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2231cb",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** for several $p$ values\n",
    "- the **CDF** (step function)\n",
    "- Monte Carlo samples: the **empirical PMF** compared to the theoretical PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = [0.1, 0.5, 0.9]\n",
    "\n",
    "# PMF\n",
    "fig_pmf = go.Figure()\n",
    "for p in p_values:\n",
    "    fig_pmf.add_trace(\n",
    "        go.Bar(\n",
    "            name=f\"p={p}\",\n",
    "            x=[0, 1],\n",
    "            y=[1 - p, p],\n",
    "        )\n",
    "    )\n",
    "fig_pmf.update_layout(\n",
    "    title=\"Bernoulli PMF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X = x)\",\n",
    "    barmode=\"group\",\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "# CDF\n",
    "x_grid = np.linspace(-0.5, 1.5, 400)\n",
    "fig_cdf = go.Figure()\n",
    "for p in p_values:\n",
    "    fig_cdf.add_trace(\n",
    "        go.Scatter(\n",
    "            name=f\"p={p}\",\n",
    "            x=x_grid,\n",
    "            y=bernoulli_cdf(x_grid, p),\n",
    "            mode=\"lines\",\n",
    "            line_shape=\"hv\",\n",
    "        )\n",
    "    )\n",
    "fig_cdf.update_layout(\n",
    "    title=\"Bernoulli CDF (step function)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x)\",\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo: empirical PMF vs theoretical PMF\n",
    "p = 0.3\n",
    "n = 10_000\n",
    "x = sample_bernoulli_numpy(p, n, rng=rng)\n",
    "emp = np.array([(x == 0).mean(), (x == 1).mean()])\n",
    "theo = np.array([1 - p, p])\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(name=\"empirical\", x=[0, 1], y=emp))\n",
    "fig_mc.add_trace(go.Bar(name=\"theoretical\", x=[0, 1], y=theo))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Empirical vs theoretical PMF (p={p}, n={n})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"probability\",\n",
    "    barmode=\"group\",\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d4b4f",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides the Bernoulli distribution as `scipy.stats.bernoulli`.\n",
    "\n",
    "- PMF / CDF: `stats.bernoulli.pmf`, `stats.bernoulli.cdf`\n",
    "- Sampling: `stats.bernoulli.rvs`\n",
    "- Fitting: use `scipy.stats.fit(dist, data, ...)` (many discrete `rv_discrete` objects don’t expose a `.fit` method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = stats.bernoulli\n",
    "\n",
    "p = 0.25\n",
    "print(\"pmf(0), pmf(1):\", bernoulli.pmf([0, 1], p))\n",
    "print(\"cdf(0), cdf(1):\", bernoulli.cdf([0, 1], p))\n",
    "\n",
    "samples = bernoulli.rvs(p, size=10, random_state=rng)\n",
    "print(\"rvs:\", samples)\n",
    "\n",
    "# Fit p with scipy.stats.fit\n",
    "data = bernoulli.rvs(0.32, size=2_000, random_state=rng)\n",
    "fit_res = stats.fit(bernoulli, data)  # MLE by default\n",
    "print(fit_res)\n",
    "print(\"p_hat:\", fit_res.params.p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f0f88",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing (proportion / coin fairness)\n",
    "Testing a Bernoulli parameter typically reduces to testing a **binomial** count $K=\\sum_i X_i$.\n",
    "SciPy’s `stats.binomtest` provides an exact test for $H_0: p=p_0$.\n",
    "\n",
    "### Bayesian modeling (Beta–Bernoulli)\n",
    "With a Beta prior $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$ and data $x_{1:n}$, the posterior is:\n",
    "$$\n",
    "p \\mid x_{1:n} \\sim \\mathrm{Beta}(\\alpha + k,\\ \\beta + n-k),\n",
    "$$\n",
    "where $k=\\sum_i x_i$.\n",
    "\n",
    "### Generative modeling\n",
    "Bernoulli likelihood is a workhorse for binary observations:\n",
    "- logistic regression / GLMs: $p_i = \\sigma(\\eta_i)$, $x_i\\sim \\mathrm{Bernoulli}(p_i)$\n",
    "- naïve Bayes with binary features (BernoulliNB)\n",
    "- neural nets for binary outputs; binary cross-entropy is (negative) Bernoulli log-likelihood\n",
    "- dropout masks are Bernoulli draws applied to activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing: is a coin fair?\n",
    "n = 20\n",
    "k = 15\n",
    "p0 = 0.5\n",
    "\n",
    "test = stats.binomtest(k=k, n=n, p=p0, alternative=\"two-sided\")\n",
    "ci = test.proportion_ci(confidence_level=0.95)\n",
    "\n",
    "print(test)\n",
    "print(\"95% CI for p:\", (ci.low, ci.high))\n",
    "\n",
    "\n",
    "# Bayesian modeling: Beta prior + Bernoulli data\n",
    "alpha, beta = 2.0, 2.0  # prior strength and prior mean = 0.5\n",
    "\n",
    "alpha_post = alpha + k\n",
    "beta_post = beta + (n - k)\n",
    "\n",
    "posterior_mean = alpha_post / (alpha_post + beta_post)\n",
    "print(\"Posterior mean of p:\", posterior_mean)\n",
    "\n",
    "x_grid = np.linspace(0, 1, 400)\n",
    "prior_pdf = stats.beta.pdf(x_grid, alpha, beta)\n",
    "post_pdf = stats.beta.pdf(x_grid, alpha_post, beta_post)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=prior_pdf, mode=\"lines\", name=f\"prior Beta({alpha:.0f},{beta:.0f})\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_grid,\n",
    "        y=post_pdf,\n",
    "        mode=\"lines\",\n",
    "        name=f\"posterior Beta({alpha_post:.0f},{beta_post:.0f})\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Beta–Bernoulli: prior vs posterior for p\",\n",
    "    xaxis_title=\"p\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63f1a4",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: $p\\notin[0,1]$ is not a valid Bernoulli distribution.\n",
    "- **Degenerate boundaries**: at $p=0$ or $p=1$, variance is 0 and quantities like skewness/kurtosis are undefined.\n",
    "- **Log-likelihood at the boundaries**: `log(p)` or `log(1-p)` can produce `-inf`.\n",
    "  - Use `np.log1p(-p)` for stability when $p$ is close to 1.\n",
    "  - When optimizing numerically, clip: `p = np.clip(p, eps, 1-eps)`.\n",
    "- **Data validation**: fitting assumes observations are exactly 0/1 (or booleans). If you have probabilities or varying success rates, you want a different model (e.g., Bernoulli with varying $p_i$ or a Beta-Binomial).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical edge cases: log-likelihood at p=0 or p=1\n",
    "x = np.array([0, 1, 1, 0, 1])\n",
    "\n",
    "for p in [0.0, 1e-12, 0.5, 1 - 1e-12, 1.0]:\n",
    "    ll = bernoulli_log_likelihood(p, x)\n",
    "    print(f\"p={p: .12f} -> log-likelihood={ll: .3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b8c77",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- Bernoulli is the canonical **binary** distribution: $X\\in\\{0,1\\}$ and $\\mathbb{P}(X=1)=p$.\n",
    "- PMF: $\\mathbb{P}(X=x)=p^x(1-p)^{1-x}$ for $x\\in\\{0,1\\}$; the CDF is a step function.\n",
    "- Mean/variance: $\\mathbb{E}[X]=p$, $\\mathrm{Var}(X)=p(1-p)$.\n",
    "- Likelihood for i.i.d. data yields the MLE $\\hat{p}=\\bar{x}$.\n",
    "- Sum of Bernoullis gives Binomial; Beta prior gives conjugate Beta posterior.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}