{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e35b06",
   "metadata": {},
   "source": [
    "# `multinomial` (Multinomial distribution)\n",
    "\n",
    "The **multinomial** distribution models **counts across `K` categories** when you run **`n` independent categorical trials** with fixed category probabilities.\n",
    "\n",
    "This notebook uses the same parameterization as `scipy.stats.multinomial`:\n",
    "\n",
    "- `n` = total number of trials (a non-negative integer)\n",
    "- `p` = probability vector of length `K` with entries in \\([0,1]\\) that sums to 1\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- recognize when a multinomial model is appropriate (and when it isn’t)\n",
    "- write down the PMF and a useful notion of a multivariate CDF\n",
    "- compute and interpret the mean vector, covariance, and key properties\n",
    "- derive the likelihood and the MLE for `p`\n",
    "- sample from a multinomial using **NumPy-only** algorithms\n",
    "- visualize the PMF/CDF for `K=3` on the simplex (triangle)\n",
    "- use `scipy.stats.multinomial` for PMF/moments/sampling and implement missing pieces (CDF/fit) yourself\n",
    "\n",
    "## Prerequisites\n",
    "- Basic probability (PMF/CDF), expectation, variance, covariance\n",
    "- Multinomial coefficients / factorials\n",
    "- Comfort with logs and derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9df69",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d82170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "np.set_printoptions(precision=6, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267c1a5",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name**: `multinomial` (Multinomial distribution)  \n",
    "**Type**: **Discrete** (multivariate counts)\n",
    "\n",
    "**Support** (for `K` categories and total count `n`):\n",
    "\n",
    "$$\n",
    "\\mathcal{S}_{n,K} = \\left\\{ x \\in \\{0,1,2,\\dots\\}^K : \\sum_{i=1}^K x_i = n \\right\\}\n",
    "$$\n",
    "\n",
    "**Parameter space**:\n",
    "\n",
    "- $n \\in \\{0,1,2,\\dots\\}$\n",
    "- $p=(p_1,\\dots,p_K)$ with $p_i\\ge 0$ and $\\sum_{i=1}^K p_i = 1$\n",
    "- typically $K\\ge 2$\n",
    "\n",
    "We write:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathrm{Multinomial}(n, p),\\qquad X=(X_1,\\dots,X_K)\n",
    "$$\n",
    "\n",
    "where $X_i$ counts how many times category $i$ occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce2d82",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "A clean mental model is **rolling a `K`-sided die `n` times**.\n",
    "If the probability of face $i$ is $p_i$, and $X_i$ counts how many times you saw face $i$, then\n",
    "\n",
    "$$\n",
    "X = (X_1,\\dots,X_K) \\sim \\mathrm{Multinomial}(n,p).\n",
    "$$\n",
    "\n",
    "Key idea:\n",
    "\n",
    "> **The multinomial is a multivariate generalization of the binomial: it counts how many times each category occurs.**\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Survey / A-B-n experiments**: how many users chose each option.\n",
    "- **NLP (bag-of-words)**: word counts in a document given a fixed word-probability vector.\n",
    "- **Quality control**: counts of defect types.\n",
    "- **Genetics**: allele counts across categories.\n",
    "- **Marketing / ads**: impressions or clicks split across multiple creatives.\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Categorical (a.k.a. multinoulli)**: if $n=1$, then $X$ is one-hot and you recover a categorical draw.\n",
    "- **Binomial**: if $K=2$, then $X_1\\sim\\mathrm{Bin}(n,p_1)$ and $X_2=n-X_1$.\n",
    "- **Binomial marginals**: for any $i$, $X_i\\sim\\mathrm{Bin}(n,p_i)$ (but the components are dependent).\n",
    "- **Poisson splitting**: if $Y_i\\stackrel{\\text{ind}}{\\sim}\\mathrm{Poisson}(\\lambda p_i)$, then\n",
    "  $(Y_1,\\dots,Y_K)\\mid \\sum_i Y_i=n$ is multinomial $\\mathrm{Multinomial}(n,p)$.\n",
    "- **Dirichlet–multinomial**: if $p$ is random with a Dirichlet prior and then $X\\mid p$ is multinomial, you get overdispersion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfa78e",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $X=(X_1,\\dots,X_K)$ be a count vector with $\\sum_i X_i = n$.\n",
    "\n",
    "### PMF\n",
    "For $x\\in\\mathcal{S}_{n,K}$:\n",
    "\n",
    "$$\n",
    "\\Pr(X=x\\mid n,p)\n",
    "= \\frac{n!}{\\prod_{i=1}^K x_i!}\\,\\prod_{i=1}^K p_i^{x_i}.\n",
    "$$\n",
    "\n",
    "If $x\\notin\\mathcal{S}_{n,K}$, the probability is 0.\n",
    "\n",
    "A common shorthand is the **multinomial coefficient**:\n",
    "\n",
    "$$\n",
    "\\binom{n}{x_1,\\dots,x_K} = \\frac{n!}{\\prod_{i=1}^K x_i!}.\n",
    "$$\n",
    "\n",
    "### CDF (lower-orthant CDF)\n",
    "A standard multivariate analogue of a CDF is the **lower-orthant** probability:\n",
    "\n",
    "$$\n",
    "F(x) = \\Pr(X_1\\le x_1,\\dots,X_K\\le x_K)\n",
    "= \\sum_{y\\in\\mathcal{S}_{n,K}:\\; y\\le x} \\Pr(X=y),\n",
    "$$\n",
    "\n",
    "where $y\\le x$ means componentwise inequality.\n",
    "\n",
    "Notes:\n",
    "- There is **no simple closed form** for this CDF in general.\n",
    "- Because $\\sum_i X_i=n$ almost surely, you need $\\sum_i x_i\\ge n$ to get a nonzero CDF.\n",
    "- For visualization, it’s often more informative to leave some components unconstrained, e.g.\n",
    "\n",
    "$$\n",
    "\\Pr(X_1\\le a,\\;X_2\\le b)\n",
    "= F(a,b,n,\\dots,n).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_n(n):\n",
    "    if isinstance(n, bool) or not isinstance(n, (int, np.integer)):\n",
    "        raise TypeError(\"n must be an integer\")\n",
    "    n_int = int(n)\n",
    "    if n_int < 0:\n",
    "        raise ValueError(\"n must be >= 0\")\n",
    "    return n_int\n",
    "\n",
    "\n",
    "def _validate_p(p):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if p.ndim != 1:\n",
    "        raise ValueError(\"p must be a 1D probability vector\")\n",
    "    if p.size < 2:\n",
    "        raise ValueError(\"p must have length K>=2\")\n",
    "    if not np.all(np.isfinite(p)):\n",
    "        raise ValueError(\"p must be finite\")\n",
    "    if np.any(p < 0):\n",
    "        raise ValueError(\"p must be non-negative\")\n",
    "\n",
    "    s = float(p.sum())\n",
    "    if not np.isclose(s, 1.0, atol=1e-12, rtol=0.0):\n",
    "        raise ValueError(f\"p must sum to 1 (got {s})\")\n",
    "\n",
    "    if s != 1.0:\n",
    "        p = p / s\n",
    "    return p\n",
    "\n",
    "\n",
    "def _validate_counts(x, k, *, require_sum_n=None):\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    if x.ndim != 2 or x.shape[1] != k:\n",
    "        raise ValueError(f\"x must have shape (k,) or (m,k) with k={k}\")\n",
    "\n",
    "    if not np.issubdtype(x.dtype, np.integer):\n",
    "        if np.any(np.abs(x - np.round(x)) > 0):\n",
    "            raise ValueError(\"x must contain integers\")\n",
    "        x = np.round(x).astype(int)\n",
    "    else:\n",
    "        x = x.astype(int)\n",
    "\n",
    "    if np.any(x < 0):\n",
    "        raise ValueError(\"x must be nonnegative\")\n",
    "\n",
    "    if require_sum_n is not None:\n",
    "        row_sums = x.sum(axis=1)\n",
    "        if np.any(row_sums != require_sum_n):\n",
    "            raise ValueError(\"Each row of x must sum to n\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "_lgamma_vec = np.vectorize(math.lgamma)\n",
    "\n",
    "\n",
    "def multinomial_logpmf(x, n, p):\n",
    "    n = _validate_n(n)\n",
    "    p = _validate_p(p)\n",
    "    x = _validate_counts(x, k=p.size, require_sum_n=n)\n",
    "\n",
    "    if n == 0:\n",
    "        out = np.zeros(x.shape[0], dtype=float)\n",
    "        return out[0] if out.size == 1 else out\n",
    "\n",
    "    log_coeff = math.lgamma(n + 1.0) - np.sum(_lgamma_vec(x + 1.0), axis=1)\n",
    "\n",
    "    log_p = np.where(p > 0, np.log(p), -np.inf)\n",
    "    # Interpret 0 * log(0) = 0 (limit).\n",
    "    x_log_p = np.where(x == 0, 0.0, x * log_p[None, :])\n",
    "    log_prob = np.sum(x_log_p, axis=1)\n",
    "\n",
    "    out = log_coeff + log_prob\n",
    "    return out[0] if out.size == 1 else out\n",
    "\n",
    "\n",
    "def multinomial_pmf(x, n, p):\n",
    "    return np.exp(multinomial_logpmf(x, n=n, p=p))\n",
    "\n",
    "\n",
    "def compositions(n, k):\n",
    "    # Generate all k-tuples of nonnegative integers summing to n (stars and bars).\n",
    "    if k == 1:\n",
    "        yield (n,)\n",
    "        return\n",
    "    for i in range(n + 1):\n",
    "        for tail in compositions(n - i, k - 1):\n",
    "            yield (i,) + tail\n",
    "\n",
    "\n",
    "def enumerate_support(n, k):\n",
    "    n = _validate_n(n)\n",
    "    if k < 1:\n",
    "        raise ValueError(\"k must be >= 1\")\n",
    "    return np.array(list(compositions(n, k)), dtype=int)\n",
    "\n",
    "\n",
    "def multinomial_cdf_small_n(x, n, p):\n",
    "    # Lower-orthant CDF by brute-force summation (only feasible for small n,k).\n",
    "    n = _validate_n(n)\n",
    "    p = _validate_p(p)\n",
    "    x = _validate_counts(x, k=p.size)[0]\n",
    "\n",
    "    ys = enumerate_support(n=n, k=p.size)\n",
    "    mask = np.all(ys <= x[None, :], axis=1)\n",
    "    return float(np.sum(multinomial_pmf(ys[mask], n=n, p=p)))\n",
    "\n",
    "\n",
    "def simplex_xy_3(counts):\n",
    "    # Map 3-category compositions to 2D barycentric coordinates for plotting.\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    counts = np.atleast_2d(counts)\n",
    "    if counts.shape[1] != 3:\n",
    "        raise ValueError(\"simplex_xy_3 expects shape (m,3)\")\n",
    "\n",
    "    n = counts.sum(axis=1)\n",
    "    if np.any(n <= 0):\n",
    "        raise ValueError(\"All rows must sum to a positive n\")\n",
    "\n",
    "    p = counts / n[:, None]\n",
    "    x = p[:, 1] + 0.5 * p[:, 2]\n",
    "    y = (np.sqrt(3) / 2.0) * p[:, 2]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007fd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: PMF sums to 1 on the support\n",
    "n = 6\n",
    "p = np.array([0.2, 0.3, 0.5])\n",
    "\n",
    "support = enumerate_support(n=n, k=p.size)\n",
    "pmf = multinomial_pmf(support, n=n, p=p)\n",
    "\n",
    "{\n",
    "    \"support_size\": int(support.shape[0]),\n",
    "    \"pmf_sum\": float(pmf.sum()),\n",
    "    \"min_pmf\": float(pmf.min()),\n",
    "    \"max_pmf\": float(pmf.max()),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbb378",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let $X\\sim\\mathrm{Multinomial}(n,p)$ with $p\\in\\mathbb{R}^K$ and $\\sum_i p_i=1$.\n",
    "\n",
    "### Mean and covariance\n",
    "For each component:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X_i] = n p_i.\n",
    "$$\n",
    "\n",
    "The variance and covariance are:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X_i) = n p_i(1-p_i),\\qquad\n",
    "\\mathrm{Cov}(X_i,X_j) = -n p_i p_j\\quad (i\\ne j).\n",
    "$$\n",
    "\n",
    "Equivalently, the covariance matrix is\n",
    "\n",
    "$$\n",
    "\\Sigma = n\\,(\\mathrm{diag}(p) - p p^\\top).\n",
    "$$\n",
    "\n",
    "A key qualitative property is **negative dependence**: if one category count is high, others must (on average) be lower because the total is fixed.\n",
    "\n",
    "### Skewness and kurtosis (marginals)\n",
    "Each component $X_i$ is marginally binomial: $X_i\\sim\\mathrm{Bin}(n,p_i)$. Therefore, for each $i$:\n",
    "\n",
    "$$\n",
    "\\text{skew}(X_i) = \\frac{1-2p_i}{\\sqrt{n p_i(1-p_i)}},\n",
    "\\qquad\n",
    "\\text{excess kurt}(X_i) = \\frac{1-6p_i(1-p_i)}{n p_i(1-p_i)}.\n",
    "$$\n",
    "\n",
    "(These are **univariate** skewness/kurtosis of the marginals; multivariate notions also exist.)\n",
    "\n",
    "### MGF and characteristic function\n",
    "For a vector $t\\in\\mathbb{R}^K$, the multivariate moment generating function is\n",
    "\n",
    "$$\n",
    "M_X(t) = \\mathbb{E}[e^{t^\\top X}] = \\left(\\sum_{i=1}^K p_i e^{t_i}\\right)^n.\n",
    "$$\n",
    "\n",
    "The characteristic function ($\\omega\\in\\mathbb{R}^K$) is\n",
    "\n",
    "$$\n",
    "\\varphi_X(\\omega) = \\mathbb{E}[e^{i\\,\\omega^\\top X}] = \\left(\\sum_{i=1}^K p_i e^{i\\,\\omega_i}\\right)^n.\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "The entropy is\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{x\\in\\mathcal{S}_{n,K}} \\Pr(X=x)\\,\\log\\Pr(X=x),\n",
    "$$\n",
    "\n",
    "which generally has no simple closed form.\n",
    "\n",
    "For **large** $n$ (and all $p_i>0$), a useful approximation comes from a multivariate normal approximation on the $(K-1)$-dimensional simplex:\n",
    "\n",
    "$$\n",
    "H(X)\\;\\approx\\;\\tfrac12\\,\\log\\Bigl((2\\pi e)^{K-1}\\,n^{K-1}\\,\\prod_{i=1}^K p_i\\Bigr).\n",
    "$$\n",
    "\n",
    "### Other useful properties\n",
    "- **Sum constraint**: $\\sum_i X_i = n$ almost surely, so the covariance matrix has rank $K-1$.\n",
    "- **Merging categories**: if you merge two categories, you get another multinomial with merged probability.\n",
    "- **Conditionals**: given some counts, the remaining counts are multinomial (or binomial in the sequential decomposition).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify mean/covariance with Monte Carlo (using NumPy's built-in multinomial sampler)\n",
    "\n",
    "def mean_cov_multinomial(n, p):\n",
    "    n = _validate_n(n)\n",
    "    p = _validate_p(p)\n",
    "    mean = n * p\n",
    "    cov = n * (np.diag(p) - np.outer(p, p))\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "n = 40\n",
    "p = np.array([0.15, 0.35, 0.20, 0.30])\n",
    "\n",
    "mean_theory, cov_theory = mean_cov_multinomial(n, p)\n",
    "\n",
    "samples = rng.multinomial(n, p, size=200_000)\n",
    "mean_mc = samples.mean(axis=0)\n",
    "cov_mc = np.cov(samples.T, ddof=0)\n",
    "\n",
    "{\n",
    "    \"mean_theory\": mean_theory,\n",
    "    \"mean_mc\": mean_mc,\n",
    "    \"max_abs_mean_err\": float(np.max(np.abs(mean_mc - mean_theory))),\n",
    "    \"max_abs_cov_err\": float(np.max(np.abs(cov_mc - cov_theory))),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7fba3",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### `n` (total count)\n",
    "- Controls the **scale** of the counts.\n",
    "- As `n` increases with fixed `p`, the distribution concentrates around its mean $np$ and (after centering/scaling) becomes close to multivariate normal.\n",
    "\n",
    "### `p` (category probabilities)\n",
    "- Controls the **direction** of the mean and where the mass sits on the simplex.\n",
    "- If one $p_i$ is large, most mass is near the corresponding **vertex** (most trials land in that category).\n",
    "- If `p` is close to uniform, mass is concentrated near the **center** (counts are balanced).\n",
    "\n",
    "For `K=3`: points $x=(x_1,x_2,x_3)$ lie on a triangle (the 2-simplex) because $x_1+x_2+x_3=n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How p changes the shape (K=3): PMF on the simplex\n",
    "\n",
    "def plot_simplex_pmf(n, p, *, title):\n",
    "    p = _validate_p(p)\n",
    "    if p.size != 3:\n",
    "        raise ValueError(\"This visualization expects K=3\")\n",
    "\n",
    "    support = enumerate_support(n=n, k=3)\n",
    "    pmf = multinomial_pmf(support, n=n, p=p)\n",
    "    sx, sy = simplex_xy_3(support)\n",
    "\n",
    "    tri_x = [0.0, 1.0, 0.5, 0.0]\n",
    "    tri_y = [0.0, 0.0, float(np.sqrt(3) / 2.0), 0.0]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=tri_x, y=tri_y, mode=\"lines\", line=dict(color=\"black\"), showlegend=False))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sx,\n",
    "            y=sy,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=pmf,\n",
    "                colorscale=\"Viridis\",\n",
    "                colorbar=dict(title=\"PMF\"),\n",
    "                line=dict(width=0.2, color=\"rgba(0,0,0,0.2)\"),\n",
    "            ),\n",
    "            text=[f\"x={tuple(row)}, pmf={val:.3e}\" for row, val in zip(support, pmf)],\n",
    "            hoverinfo=\"text\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"barycentric x\",\n",
    "        yaxis_title=\"barycentric y\",\n",
    "        xaxis=dict(range=[-0.05, 1.05], zeroline=False),\n",
    "        yaxis=dict(\n",
    "            scaleanchor=\"x\",\n",
    "            scaleratio=1,\n",
    "            range=[-0.05, float(np.sqrt(3) / 2.0) + 0.05],\n",
    "            zeroline=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_annotation(x=0.0, y=0.0, text=\"cat 1\", showarrow=False, yshift=-12)\n",
    "    fig.add_annotation(x=1.0, y=0.0, text=\"cat 2\", showarrow=False, yshift=-12)\n",
    "    fig.add_annotation(x=0.5, y=float(np.sqrt(3) / 2.0), text=\"cat 3\", showarrow=False, yshift=12)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "n = 18\n",
    "plot_simplex_pmf(n, p=[1 / 3, 1 / 3, 1 / 3], title=\"PMF on simplex (n=18, p uniform)\")\n",
    "plot_simplex_pmf(n, p=[0.70, 0.20, 0.10], title=\"PMF on simplex (n=18, p favors cat 1)\")\n",
    "plot_simplex_pmf(n, p=[0.10, 0.15, 0.75], title=\"PMF on simplex (n=18, p favors cat 3)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ed445",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "A convenient derivation uses **one-hot indicator variables**.\n",
    "\n",
    "Let $Z_t\\in\\{e_1,\\dots,e_K\\}$ be the one-hot vector for trial $t$, with\n",
    "\n",
    "$$\n",
    "\\Pr(Z_t=e_i)=p_i,\n",
    "$$\n",
    "\n",
    "and define counts as\n",
    "\n",
    "$$\n",
    "X = \\sum_{t=1}^n Z_t,\\qquad X_i = \\sum_{t=1}^n Z_{t,i}.\n",
    "$$\n",
    "\n",
    "### Expectation\n",
    "By linearity of expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X_i] = \\sum_{t=1}^n \\mathbb{E}[Z_{t,i}] = \\sum_{t=1}^n p_i = n p_i.\n",
    "$$\n",
    "\n",
    "### Variance and covariance\n",
    "For a fixed trial $t$, the indicators satisfy:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Z_{t,i}] = p_i,\n",
    "\\qquad\n",
    "\\mathrm{Var}(Z_{t,i}) = p_i(1-p_i),\n",
    "\\qquad\n",
    "\\mathrm{Cov}(Z_{t,i}, Z_{t,j}) = -p_i p_j\\;(i\\ne j)\n",
    "$$\n",
    "\n",
    "because $Z_{t,i}Z_{t,j}=0$ when $i\\ne j$.\n",
    "\n",
    "Across trials $t\\ne s$, independence gives zero covariance. Therefore:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X_i) = \\sum_{t=1}^n \\mathrm{Var}(Z_{t,i}) = n p_i(1-p_i),\n",
    "$$\n",
    "\n",
    "and for $i\\ne j$:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X_i,X_j) = \\sum_{t=1}^n \\mathrm{Cov}(Z_{t,i}, Z_{t,j}) = -n p_i p_j.\n",
    "$$\n",
    "\n",
    "### Likelihood and MLE\n",
    "For one observed count vector $x\\in\\mathcal{S}_{n,K}$, the likelihood (as a function of $p$) is\n",
    "\n",
    "$$\n",
    "L(p\\mid x) = \\frac{n!}{\\prod_i x_i!}\\,\\prod_{i=1}^K p_i^{x_i}.\n",
    "$$\n",
    "\n",
    "The log-likelihood (dropping constants independent of $p$) is\n",
    "\n",
    "$$\n",
    "\\ell(p) = \\sum_{i=1}^K x_i\\log p_i\n",
    "\\quad\\text{subject to}\\quad \\sum_i p_i = 1,\\;p_i\\ge 0.\n",
    "$$\n",
    "\n",
    "Using a Lagrange multiplier for the constraint gives the MLE:\n",
    "\n",
    "$$\n",
    "\\hat p_i = \\frac{x_i}{n}.\n",
    "$$\n",
    "\n",
    "For multiple independent observations $x^{(1)},\\dots,x^{(m)}$ (possibly with different totals $n_j$), the MLE becomes\n",
    "\n",
    "$$\n",
    "\\hat p_i = \\frac{\\sum_{j=1}^m x_i^{(j)}}{\\sum_{j=1}^m n_j}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE for p from multinomial samples\n",
    "\n",
    "def fit_multinomial_mle(counts):\n",
    "    counts = np.asarray(counts)\n",
    "    if counts.ndim == 1:\n",
    "        counts = counts[None, :]\n",
    "    if counts.ndim != 2:\n",
    "        raise ValueError(\"counts must have shape (k,) or (m,k)\")\n",
    "    if np.any(counts < 0):\n",
    "        raise ValueError(\"counts must be nonnegative\")\n",
    "\n",
    "    totals = counts.sum(axis=1)\n",
    "    if np.any(totals == 0):\n",
    "        raise ValueError(\"each observation must have positive total count\")\n",
    "\n",
    "    total_counts = counts.sum(axis=0)\n",
    "    return total_counts / total_counts.sum()\n",
    "\n",
    "\n",
    "n = 25\n",
    "p_true = np.array([0.10, 0.25, 0.05, 0.20, 0.40])\n",
    "\n",
    "data = rng.multinomial(n, p_true, size=5_000)\n",
    "p_hat = fit_multinomial_mle(data)\n",
    "\n",
    "{\n",
    "    \"p_true\": p_true,\n",
    "    \"p_hat\": p_hat,\n",
    "    \"L1_error\": float(np.sum(np.abs(p_hat - p_true))),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b27b25",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "Below are two **NumPy-only** strategies.\n",
    "\n",
    "### A) Count categorical draws\n",
    "1. Draw $n$ iid categorical outcomes with probabilities $p$.\n",
    "2. Count how many times each category appears.\n",
    "\n",
    "This is the definition of the model, but it costs $O(n)$ work per sample.\n",
    "\n",
    "### B) Sequential binomials (conditional decomposition)\n",
    "A very useful identity is that a multinomial can be generated as a sequence of conditional binomials:\n",
    "\n",
    "$$\n",
    "X_1\\sim\\mathrm{Bin}(n, p_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_2\\mid X_1\\sim\\mathrm{Bin}\\left(n-X_1, \\frac{p_2}{1-p_1}\\right)\n",
    "$$\n",
    "\n",
    "…and so on, with the last component determined by the remaining count.\n",
    "\n",
    "This is often faster than simulating all $n$ categorical trials when `n` is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087fce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_rvs_categorical_counts(n, p, size=1, *, rng: np.random.Generator):\n",
    "    n = _validate_n(n)\n",
    "    p = _validate_p(p)\n",
    "    k = p.size\n",
    "\n",
    "    if n == 0:\n",
    "        out = np.zeros((size, k), dtype=int)\n",
    "        return out[0] if size == 1 else out\n",
    "\n",
    "    draws = rng.choice(k, size=(size, n), p=p)\n",
    "    out = np.empty((size, k), dtype=int)\n",
    "    for i in range(size):\n",
    "        out[i] = np.bincount(draws[i], minlength=k)\n",
    "    return out[0] if size == 1 else out\n",
    "\n",
    "\n",
    "def multinomial_rvs_sequential_binomial(n, p, size=1, *, rng: np.random.Generator):\n",
    "    n = _validate_n(n)\n",
    "    p = _validate_p(p)\n",
    "    k = p.size\n",
    "\n",
    "    if n == 0:\n",
    "        out = np.zeros((size, k), dtype=int)\n",
    "        return out[0] if size == 1 else out\n",
    "\n",
    "    out = np.zeros((size, k), dtype=int)\n",
    "    remaining_n = np.full(size, n, dtype=int)\n",
    "\n",
    "    remaining_prob = 1.0\n",
    "    for i in range(k - 1):\n",
    "        if p[i] == 0.0:\n",
    "            xi = np.zeros(size, dtype=int)\n",
    "        elif remaining_prob == p[i]:\n",
    "            xi = remaining_n.copy()\n",
    "        else:\n",
    "            pi_cond = p[i] / remaining_prob\n",
    "            xi = rng.binomial(remaining_n, pi_cond)\n",
    "\n",
    "        out[:, i] = xi\n",
    "        remaining_n = remaining_n - xi\n",
    "        remaining_prob = remaining_prob - p[i]\n",
    "\n",
    "    out[:, -1] = remaining_n\n",
    "    return out[0] if size == 1 else out\n",
    "\n",
    "\n",
    "# Compare the two NumPy-only samplers\n",
    "n = 30\n",
    "p = np.array([0.2, 0.1, 0.3, 0.4])\n",
    "size = 100_000\n",
    "\n",
    "s1 = multinomial_rvs_categorical_counts(n, p, size=size, rng=rng)\n",
    "s2 = multinomial_rvs_sequential_binomial(n, p, size=size, rng=rng)\n",
    "\n",
    "mean_theory, cov_theory = mean_cov_multinomial(n, p)\n",
    "\n",
    "summary = {\n",
    "    \"max_abs_mean_err_categorical\": float(np.max(np.abs(s1.mean(axis=0) - mean_theory))),\n",
    "    \"max_abs_mean_err_sequential\": float(np.max(np.abs(s2.mean(axis=0) - mean_theory))),\n",
    "    \"max_abs_cov_err_categorical\": float(np.max(np.abs(np.cov(s1.T, ddof=0) - cov_theory))),\n",
    "    \"max_abs_cov_err_sequential\": float(np.max(np.abs(np.cov(s2.T, ddof=0) - cov_theory))),\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c90230",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** for `K=3` on the simplex (triangle)\n",
    "- a **useful 2D CDF slice**: $\\Pr(X_1\\le a, X_2\\le b)$ (leaving the third count unconstrained)\n",
    "- **Monte Carlo samples** on the simplex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PMF on the simplex (K=3)\n",
    "n = 20\n",
    "p = np.array([0.25, 0.50, 0.25])\n",
    "\n",
    "support = enumerate_support(n=n, k=3)\n",
    "pmf = multinomial_pmf(support, n=n, p=p)\n",
    "\n",
    "sx, sy = simplex_xy_3(support)\n",
    "\n",
    "tri_x = [0.0, 1.0, 0.5, 0.0]\n",
    "tri_y = [0.0, 0.0, float(np.sqrt(3) / 2.0), 0.0]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tri_x, y=tri_y, mode=\"lines\", line=dict(color=\"black\"), showlegend=False))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sx,\n",
    "        y=sy,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10, color=pmf, colorscale=\"Viridis\", colorbar=dict(title=\"PMF\")),\n",
    "        text=[f\"x={tuple(row)}, pmf={val:.3e}\" for row, val in zip(support, pmf)],\n",
    "        hoverinfo=\"text\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Multinomial PMF on the simplex (n={n}, p={p.tolist()})\",\n",
    "    xaxis_title=\"barycentric x\",\n",
    "    yaxis_title=\"barycentric y\",\n",
    "    xaxis=dict(range=[-0.05, 1.05], zeroline=False),\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1, range=[-0.05, float(np.sqrt(3) / 2.0) + 0.05], zeroline=False),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D CDF slice: P(X1 <= a, X2 <= b) for K=3\n",
    "# This equals the lower-orthant CDF evaluated at (a, b, n).\n",
    "\n",
    "n = 20\n",
    "p = np.array([0.25, 0.50, 0.25])\n",
    "\n",
    "support = enumerate_support(n=n, k=3)\n",
    "pmf = multinomial_pmf(support, n=n, p=p)\n",
    "\n",
    "cdf = np.zeros((n + 1, n + 1), dtype=float)\n",
    "for a in range(n + 1):\n",
    "    for b in range(n + 1):\n",
    "        mask = (support[:, 0] <= a) & (support[:, 1] <= b)\n",
    "        cdf[a, b] = pmf[mask].sum()\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cdf,\n",
    "        x=np.arange(n + 1),\n",
    "        y=np.arange(n + 1),\n",
    "        colorscale=\"Blues\",\n",
    "        colorbar=dict(title=\"P(X1≤a, X2≤b)\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"2D CDF slice: P(X1≤a, X2≤b) (n={n}, p={p.tolist()})\",\n",
    "    xaxis_title=\"b (upper bound for X2)\",\n",
    "    yaxis_title=\"a (upper bound for X1)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81801ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo samples on the simplex (K=3)\n",
    "\n",
    "n = 20\n",
    "p = np.array([0.25, 0.50, 0.25])\n",
    "\n",
    "samples = multinomial_rvs_sequential_binomial(n, p, size=25_000, rng=rng)\n",
    "x, y = simplex_xy_3(samples)\n",
    "\n",
    "tri_x = [0.0, 1.0, 0.5, 0.0]\n",
    "tri_y = [0.0, 0.0, float(np.sqrt(3) / 2.0), 0.0]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tri_x, y=tri_y, mode=\"lines\", line=dict(color=\"black\"), showlegend=False))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=4, opacity=0.15, color=\"rgba(31,119,180,1)\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Theoretical mean location\n",
    "mean = n * p\n",
    "mx, my = simplex_xy_3(mean)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=mx,\n",
    "        y=my,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"red\"),\n",
    "        name=\"mean (theory)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Monte Carlo samples on simplex (n={n}, p={p.tolist()})\",\n",
    "    xaxis_title=\"barycentric x\",\n",
    "    yaxis_title=\"barycentric y\",\n",
    "    xaxis=dict(range=[-0.05, 1.05], zeroline=False),\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1, range=[-0.05, float(np.sqrt(3) / 2.0) + 0.05], zeroline=False),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94815db",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides a numerically robust implementation via `scipy.stats.multinomial`.\n",
    "\n",
    "- Supports `pmf` / `logpmf`, `rvs`, `mean`, `cov`, and `entropy`.\n",
    "- As of SciPy 1.15, `multinomial` does **not** expose:\n",
    "  - a `cdf` method (multivariate CDFs are expensive)\n",
    "  - a `.fit()` method\n",
    "\n",
    "For CDFs and fitting, you typically implement problem-specific utilities:\n",
    "- CDFs by enumeration for small `n,K` or by approximation.\n",
    "- `p` estimation via the closed-form MLE $\\hat p_i = \\tfrac{\\sum_j x_i^{(j)}}{\\sum_j n_j}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "n = 12\n",
    "p = np.array([0.2, 0.3, 0.5])\n",
    "\n",
    "support = enumerate_support(n=n, k=p.size)\n",
    "\n",
    "pmf_scipy = multinomial.pmf(support, n=n, p=p)\n",
    "pmf_ours = multinomial_pmf(support, n=n, p=p)\n",
    "\n",
    "samples_scipy = multinomial.rvs(n=n, p=p, size=50_000, random_state=rng)\n",
    "\n",
    "# SciPy doesn't have multinomial.cdf, but we can compute it for small n with enumeration.\n",
    "example_cdf = multinomial_cdf_small_n([4, 4, 12], n=n, p=p)\n",
    "\n",
    "{\n",
    "    \"pmf_sum_scipy\": float(pmf_scipy.sum()),\n",
    "    \"pmf_sum_ours\": float(pmf_ours.sum()),\n",
    "    \"max_abs_pmf_diff\": float(np.max(np.abs(pmf_scipy - pmf_ours))),\n",
    "    \"mean_empirical\": samples_scipy.mean(axis=0),\n",
    "    \"mean_theory\": multinomial.mean(n=n, p=p),\n",
    "    \"entropy_scipy\": float(multinomial.entropy(n=n, p=p)),\n",
    "    \"example_cdf_small_n\": float(example_cdf),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Fit\" p: closed-form MLE on SciPy-generated data\n",
    "n = 30\n",
    "p_true = np.array([0.10, 0.25, 0.05, 0.20, 0.40])\n",
    "\n",
    "data = multinomial.rvs(n=n, p=p_true, size=5_000, random_state=rng)\n",
    "p_hat = fit_multinomial_mle(data)\n",
    "\n",
    "{\n",
    "    \"p_true\": p_true,\n",
    "    \"p_hat\": p_hat,\n",
    "    \"L1_error\": float(np.sum(np.abs(p_hat - p_true))),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c206e",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (goodness-of-fit)\n",
    "Given observed counts $x$ and a null probability vector $p^{(0)}$, a common test is a **chi-square goodness-of-fit** test.\n",
    "\n",
    "- Null: data comes from $\\mathrm{Multinomial}(n, p^{(0)})$\n",
    "- Expected counts under the null: $n p^{(0)}$\n",
    "\n",
    "Rule of thumb: chi-square approximations are best when expected counts aren’t too small.\n",
    "\n",
    "### B) Bayesian modeling (Dirichlet conjugacy)\n",
    "If\n",
    "\n",
    "$$\n",
    "p \\sim \\mathrm{Dirichlet}(\\alpha),\\qquad X\\mid p \\sim \\mathrm{Multinomial}(n,p),\n",
    "$$\n",
    "\n",
    "then the posterior is\n",
    "\n",
    "$$\n",
    "p\\mid X=x \\sim \\mathrm{Dirichlet}(\\alpha + x).\n",
    "$$\n",
    "\n",
    "### C) Generative modeling\n",
    "Multinomial likelihoods appear whenever you model **count vectors** given probabilities, e.g.\n",
    "- bag-of-words document models\n",
    "- discrete emissions in mixture models\n",
    "- naive Bayes classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare, power_divergence\n",
    "\n",
    "# A) Chi-square goodness-of-fit for one multinomial observation\n",
    "n = 200\n",
    "p0 = np.array([0.2, 0.5, 0.3])\n",
    "\n",
    "# simulate an observation under an alternative\n",
    "x_obs = multinomial.rvs(n=n, p=[0.25, 0.45, 0.30], size=1, random_state=rng)\n",
    "\n",
    "expected = n * p0\n",
    "chi2 = chisquare(f_obs=x_obs, f_exp=expected)\n",
    "\n",
    "# Likelihood ratio (G-test): lambda_=0 gives the log-likelihood ratio statistic\n",
    "# (Different approximations behave differently when expected counts are small.)\n",
    "gtest = power_divergence(f_obs=x_obs, f_exp=expected, lambda_=0)\n",
    "\n",
    "{\n",
    "    \"x_obs\": x_obs,\n",
    "    \"p0\": p0,\n",
    "    \"chi2_stat\": float(chi2.statistic),\n",
    "    \"chi2_pvalue\": float(chi2.pvalue),\n",
    "    \"g_stat\": float(gtest.statistic),\n",
    "    \"g_pvalue\": float(gtest.pvalue),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Dirichlet conjugacy: posterior update and posterior mean\n",
    "\n",
    "def dirichlet_rvs_numpy(alpha, size, *, rng: np.random.Generator):\n",
    "    alpha = np.asarray(alpha, dtype=float)\n",
    "    if alpha.ndim != 1 or alpha.size < 2:\n",
    "        raise ValueError(\"alpha must be a 1D array with length >= 2\")\n",
    "    if np.any(alpha <= 0) or not np.all(np.isfinite(alpha)):\n",
    "        raise ValueError(\"alpha must be positive and finite\")\n",
    "\n",
    "    g = rng.gamma(shape=alpha, scale=1.0, size=(size, alpha.size))\n",
    "    return g / g.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "alpha_prior = np.array([1.0, 1.0, 1.0])  # uniform prior on 3-simplex\n",
    "n = 50\n",
    "p_true = np.array([0.2, 0.5, 0.3])\n",
    "\n",
    "x = multinomial.rvs(n=n, p=p_true, size=1, random_state=rng)\n",
    "alpha_post = alpha_prior + x\n",
    "\n",
    "posterior_mean = alpha_post / alpha_post.sum()\n",
    "\n",
    "{\n",
    "    \"x\": x,\n",
    "    \"alpha_prior\": alpha_prior,\n",
    "    \"alpha_post\": alpha_post,\n",
    "    \"posterior_mean\": posterior_mean,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e105bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Generative modeling example: bag-of-words counts\n",
    "\n",
    "vocab = [\"cat\", \"dog\", \"fish\", \"tree\", \"car\"]\n",
    "p_words = np.array([0.30, 0.25, 0.05, 0.20, 0.20])\n",
    "\n",
    "n_words = 80\n",
    "counts = multinomial.rvs(n=n_words, p=p_words, size=1, random_state=rng)\n",
    "\n",
    "{w: int(c) for w, c in zip(vocab, counts)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c6546",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "### Invalid parameters\n",
    "- `n` must be a **non-negative integer**.\n",
    "- `p` must be non-negative and **sum to 1**.\n",
    "- For the PMF, `x` must be a non-negative integer vector with **sum exactly `n`**.\n",
    "\n",
    "### Numerical issues\n",
    "- Factorials explode quickly; compute PMFs in **log-space** when `n` is moderate/large.\n",
    "- Probabilities can underflow for rare events; prefer `logpmf` when comparing likelihoods.\n",
    "\n",
    "### Modeling issues\n",
    "- The multinomial assumes **independent trials with a fixed probability vector** `p`.\n",
    "  Overdispersion (extra variability across replicates) is common; a Dirichlet–multinomial can help.\n",
    "- Categories must be well-defined and mutually exclusive; if observations can belong to multiple labels, the model changes.\n",
    "\n",
    "### CDF gotchas\n",
    "- Multivariate CDFs are not unique in the same way as 1D CDFs (different “orders”/definitions exist).\n",
    "- Even for the standard lower-orthant CDF, computation is usually expensive beyond small `n,K`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1597a",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `multinomial` is a **discrete multivariate** distribution over count vectors $x\\in\\mathbb{N}_0^K$ with $\\sum_i x_i=n$.\n",
    "- PMF: $\\frac{n!}{\\prod_i x_i!}\\prod_i p_i^{x_i}$; mean $np$; covariance $n(\\mathrm{diag}(p)-pp^\\top)$.\n",
    "- Each component is marginally **binomial**, but components are negatively correlated.\n",
    "- The MLE for `p` is the empirical frequency: $\\hat p_i = x_i/n$ (or pooled across observations).\n",
    "- For computation and simulation, prefer `scipy.stats.multinomial` and log-space methods; for CDFs you usually need custom code or approximations.\n",
    "\n",
    "**References**\n",
    "- SciPy: `scipy.stats.multinomial`\n",
    "- Standard probability texts covering multinomial coefficients and multinomial sampling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
