{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85ce3db",
   "metadata": {},
   "source": [
    "# `nbinom` (Negative binomial distribution)\n",
    "\n",
    "The **negative binomial** distribution models a *count* (discrete) outcome. In the `scipy.stats.nbinom` parameterization, it is:\n",
    "\n",
    "> the number of **failures** \\(K\\) observed before achieving **`n` successes**,\n",
    "> when each trial succeeds with probability **`p`**.\n",
    "\n",
    "Two common interpretations depending on `n`:\n",
    "- If `n` is a **positive integer**, this is literally a “repeat Bernoulli trials until `n` successes” model.\n",
    "- If `n` is a **positive real** (as allowed by SciPy), `n` is best thought of as a **dispersion/shape** parameter via the **Gamma–Poisson mixture** view.\n",
    "\n",
    "This notebook uses the same parameterization as `scipy.stats.nbinom`:\n",
    "- `n` > 0 (integer in the classic counting story; real-valued in many statistical models)\n",
    "- `p` ∈ (0, 1] (success probability)\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- write the PMF/CDF and map between common parameterizations\n",
    "- derive mean/variance and a usable log-likelihood\n",
    "- sample using a **NumPy-only** algorithm (Gamma–Poisson mixture)\n",
    "- visualize PMF/CDF and validate by Monte Carlo\n",
    "- use `scipy.stats.nbinom` for `pmf`, `cdf`, `rvs`, and fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f16712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c22d8c",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name**: `nbinom` (Negative binomial distribution)  \n",
    "**Type**: **Discrete**  \n",
    "\n",
    "**Support**:\n",
    "\\[\n",
    "k \\in \\{0, 1, 2, \\dots\\}\n",
    "\\]\n",
    "\n",
    "**Parameter space** (SciPy):\n",
    "\\[\n",
    "n > 0,\\qquad 0 < p \\le 1\n",
    "\\]\n",
    "\n",
    "Interpretation (classic Bernoulli-trial story):\n",
    "- `n` is the number of **successes** we wait for\n",
    "- `p` is the **success probability** per trial\n",
    "- the random variable \\(K\\) counts **failures before the `n`-th success**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709eeab",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "The negative binomial answers questions like:\n",
    "\n",
    "- “How many **misses** happen before we see `n` **hits**?”\n",
    "- “How many **non-events** occur before the `n`-th **event**?”\n",
    "\n",
    "If `n=1`, you get the **geometric** distribution (failures before the first success).\n",
    "\n",
    "### Typical real-world use cases\n",
    "The negative binomial is a workhorse for **count data** when the Poisson model is too “tight”.\n",
    "Common examples:\n",
    "- number of defects before a fixed number of passes\n",
    "- number of insurance claims, incidents, arrivals in a time window\n",
    "- word counts in documents (topic models / language)\n",
    "- RNA-seq / gene expression counts (overdispersed relative to Poisson)\n",
    "\n",
    "### Relations to other distributions\n",
    "Two especially important relationships:\n",
    "\n",
    "1) **Sum of geometrics (integer `n`)**\n",
    "\\[\n",
    "K = G_1 + \\cdots + G_n,\\qquad G_i\\sim\text{Geom}(p)\text{ on }\\{0,1,2,\\dots\\}\n",
    "\\]\n",
    "\n",
    "2) **Gamma–Poisson mixture (any real `n>0`)**\n",
    "\\[\n",
    "\\Lambda \\sim \text{Gamma}(\text{shape}=n,\\ \text{scale}=\tfrac{1-p}{p}),\\qquad K\\mid\\Lambda\\sim\text{Poisson}(\\Lambda)\n",
    "\\]\n",
    "Then marginally \\(K\\sim\text{NB}(n,p)\\). This view explains why negative binomial is used for **overdispersed** counts.\n",
    "\n",
    "A convenient re-parameterization in terms of the mean \\(\\mu\\) and dispersion \\(n\\) is:\n",
    "\\[\n",
    "\\mu = \\mathbb{E}[K],\\qquad p = \f",
    "rac{n}{n+\\mu},\\qquad \\mathrm{Var}(K)=\\mu+\f",
    "rac{\\mu^2}{n}\n",
    "\\]\n",
    "So smaller `n` means *more* overdispersion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf276764",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(K\\sim\text{NB}(n,p)\\) denote the number of failures before the `n`-th success.\n",
    "\n",
    "### PMF\n",
    "For integers \\(k\\ge 0\\):\n",
    "\\[\n",
    "\\mathbb{P}(K=k)\n",
    "= \binom{k+n-1}{k}(1-p)^k p^n\n",
    "\\]\n",
    "\n",
    "A numerically convenient equivalent form (valid for real \\(n>0\\)) uses gamma functions:\n",
    "\\[\n",
    "\binom{k+n-1}{k} = \f",
    "rac{\\Gamma(k+n)}{\\Gamma(n)\\,\\Gamma(k+1)}\n",
    "\\]\n",
    "so\n",
    "\\[\n",
    "\\mathbb{P}(K=k)\n",
    "= \f",
    "rac{\\Gamma(k+n)}{\\Gamma(n)\\,\\Gamma(k+1)}\\,(1-p)^k\\,p^n.\n",
    "\\]\n",
    "\n",
    "### CDF\n",
    "For real \\(x\\), define \\(\\lfloor x\r",
    "floor\\) as the floor. Then\n",
    "\\[\n",
    "F(x)=\\mathbb{P}(K\\le x)=\\sum_{k=0}^{\\lfloor x\r",
    "floor}\\mathbb{P}(K=k).\n",
    "\\]\n",
    "\n",
    "A standard special-function identity is\n",
    "\\[\n",
    "\\mathbb{P}(K\\le k) = I_{p}(n,\\ k+1),\\qquad k\\in\\{0,1,2,\\dots\\}\n",
    "\\]\n",
    "where \\(I\\) is the regularized incomplete beta function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4c794",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let \\(q = 1-p\\).\n",
    "\n",
    "### Mean and variance\n",
    "\\[\n",
    "\\mathbb{E}[K] = \f",
    "rac{nq}{p},\\qquad \\mathrm{Var}(K)=\f",
    "rac{nq}{p^2}.\n",
    "\\]\n",
    "\n",
    "Equivalently, if you use the mean \\(\\mu\\): \\(\\mu=\tfrac{nq}{p}\\), then\n",
    "\\[\n",
    "\\mathrm{Var}(K)=\\mu+\f",
    "rac{\\mu^2}{n}.\n",
    "\\]\n",
    "\n",
    "### Skewness and kurtosis\n",
    "\\[\n",
    "\text{skew}(K)=\f",
    "rac{2-p}{\\sqrt{n(1-p)}}\n",
    "\\]\n",
    "\n",
    "The **excess** kurtosis is\n",
    "\\[\n",
    "\text{excess kurt}(K)=\f",
    "rac{6 + \f",
    "rac{p^2}{1-p}}{n}\n",
    "\\]\n",
    "(so the full kurtosis is \\(3+\text{excess kurt}\\)).\n",
    "\n",
    "### MGF and characteristic function\n",
    "For \\(t < -\\log(1-p)\\):\n",
    "\\[\n",
    "M_K(t)=\\mathbb{E}[e^{tK}] = \\left(\f",
    "rac{p}{1-(1-p)e^t}\r",
    "ight)^{n}.\n",
    "\\]\n",
    "\n",
    "For real \\(t\\):\n",
    "\\[\n",
    "\u000b",
    "arphi_K(t)=\\mathbb{E}[e^{itK}] = \\left(\f",
    "rac{p}{1-(1-p)e^{it}}\r",
    "ight)^{n}.\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "The (Shannon) entropy is\n",
    "\\[\n",
    "H(K) = -\\sum_{k=0}^{\\infty} \\mathbb{P}(K=k)\\,\\log \\mathbb{P}(K=k).\n",
    "\\]\n",
    "There is no simple elementary closed form; in practice you compute it numerically\n",
    "(e.g., via truncation or via SciPy).\n",
    "\n",
    "### Other useful properties\n",
    "- **Mode**: for \\(n>1\\), \\(\\left\\lfloor \tfrac{(n-1)(1-p)}{p}\r",
    "ight\r",
    "floor\\). For \\(0<n\\le 1\\), the mode is 0.\n",
    "- **Additivity (same `p`)**: if \\(K_1\\sim\text{NB}(n_1,p)\\) and \\(K_2\\sim\text{NB}(n_2,p)\\) independent, then \\(K_1+K_2\\sim\text{NB}(n_1+n_2,p)\\).\n",
    "- **Poisson limit**: with mean \\(\\mu\\) fixed and \\(n\to\\infty\\), the distribution approaches \\(\text{Poisson}(\\mu)\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab51f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_n_p(n, p):\n",
    "    if isinstance(n, bool):\n",
    "        raise TypeError(\"n must be a positive number\")\n",
    "    n_float = float(n)\n",
    "    if not (n_float > 0.0):\n",
    "        raise ValueError(\"n must be > 0\")\n",
    "\n",
    "    p_float = float(p)\n",
    "    if not (0.0 < p_float <= 1.0):\n",
    "        raise ValueError(\"p must be in (0, 1]\")\n",
    "\n",
    "    return n_float, p_float\n",
    "\n",
    "\n",
    "def nbinom_logpmf(k, n, p):\n",
    "    # Log PMF for NB(n, p) on k=0,1,2,... (SciPy parameterization).\n",
    "    n, p = _validate_n_p(n, p)\n",
    "\n",
    "    k_arr = np.asarray(k)\n",
    "    out = np.full(k_arr.shape, -np.inf, dtype=float)\n",
    "\n",
    "    k_int = k_arr.astype(int)\n",
    "    valid = (k_int == k_arr) & (k_int >= 0)\n",
    "    if not np.any(valid):\n",
    "        return out\n",
    "\n",
    "    if p == 1.0:\n",
    "        out[valid & (k_int == 0)] = 0.0\n",
    "        return out\n",
    "\n",
    "    kv = k_int[valid]\n",
    "\n",
    "    # log Γ(k+n) - log Γ(n) - log(k!)\n",
    "    log_coeff = (\n",
    "        np.vectorize(math.lgamma)(kv + n)\n",
    "        - math.lgamma(n)\n",
    "        - np.vectorize(math.lgamma)(kv + 1)\n",
    "    )\n",
    "\n",
    "    out[valid] = log_coeff + kv * math.log1p(-p) + n * math.log(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def nbinom_pmf(k, n, p):\n",
    "    return np.exp(nbinom_logpmf(k, n, p))\n",
    "\n",
    "\n",
    "def nbinom_mean(n, p):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    return n * (1 - p) / p\n",
    "\n",
    "\n",
    "def nbinom_var(n, p):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    return n * (1 - p) / (p * p)\n",
    "\n",
    "\n",
    "def nbinom_skew(n, p):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    q = 1 - p\n",
    "    return (2 - p) / math.sqrt(n * q)\n",
    "\n",
    "\n",
    "def nbinom_excess_kurt(n, p):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    q = 1 - p\n",
    "    return (6 + (p * p) / q) / n\n",
    "\n",
    "\n",
    "def nbinom_pmf_cdf_trunc(n, p, *, q=0.999, max_k=200_000):\n",
    "    # Return (ks, pmf, cdf) for k=0..K capturing ~q mass via recurrence.\n",
    "    n, p = _validate_n_p(n, p)\n",
    "\n",
    "    if not (0.0 < q <= 1.0):\n",
    "        raise ValueError(\"q must be in (0, 1]\")\n",
    "\n",
    "    if p == 1.0:\n",
    "        return np.array([0]), np.array([1.0]), np.array([1.0])\n",
    "\n",
    "    pmf0 = math.exp(n * math.log(p))\n",
    "    pmf_vals = [pmf0]\n",
    "    cdf_vals = [pmf0]\n",
    "\n",
    "    k = 0\n",
    "    while cdf_vals[-1] < q and k < max_k:\n",
    "        k += 1\n",
    "        # f(k)/f(k-1) = ((k-1+n)/k) * (1-p)\n",
    "        pmf_k = pmf_vals[-1] * ((k - 1 + n) / k) * (1 - p)\n",
    "        pmf_vals.append(pmf_k)\n",
    "        cdf_vals.append(cdf_vals[-1] + pmf_k)\n",
    "\n",
    "        if pmf_k == 0.0:\n",
    "            break\n",
    "\n",
    "    ks = np.arange(len(pmf_vals))\n",
    "    pmf = np.array(pmf_vals, dtype=float)\n",
    "    cdf = np.minimum(1.0, np.array(cdf_vals, dtype=float))\n",
    "    cdf[-1] = min(1.0, cdf[-1])\n",
    "    return ks, pmf, cdf\n",
    "\n",
    "\n",
    "def nbinom_entropy_trunc(n, p, *, q=0.999999, max_k=400_000):\n",
    "    # Approximate entropy in nats via truncation at mass q.\n",
    "    ks, pmf, _ = nbinom_pmf_cdf_trunc(n, p, q=q, max_k=max_k)\n",
    "\n",
    "    # Lower bound (ignores tail beyond the truncation point).\n",
    "    pmf = pmf[pmf > 0]\n",
    "    return float(-(pmf * np.log(pmf)).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 8, 0.35\n",
    "\n",
    "moments = {\n",
    "    \"mean\": nbinom_mean(n, p),\n",
    "    \"var\": nbinom_var(n, p),\n",
    "    \"skew\": nbinom_skew(n, p),\n",
    "    \"excess_kurt\": nbinom_excess_kurt(n, p),\n",
    "    \"entropy_trunc_nats\": nbinom_entropy_trunc(n, p),\n",
    "}\n",
    "\n",
    "moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check (matches formulas up to sampling error)\n",
    "samples = rng.negative_binomial(n=n, p=p, size=200_000)\n",
    "\n",
    "est_mean = samples.mean()\n",
    "est_var = samples.var(ddof=0)\n",
    "\n",
    "{\n",
    "    \"formula_mean\": moments[\"mean\"],\n",
    "    \"mc_mean\": float(est_mean),\n",
    "    \"formula_var\": moments[\"var\"],\n",
    "    \"mc_var\": float(est_var),\n",
    "    \"entropy_trunc_nats\": moments[\"entropy_trunc_nats\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5d494",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "In the SciPy parameterization:\n",
    "\n",
    "- **`p` (success probability)** controls how quickly successes arrive.\n",
    "  Larger `p` puts more mass near 0 failures.\n",
    "- **`n` (shape / number of successes)** controls both scale and dispersion.\n",
    "  For integer `n`, it is literally the number of successes you wait for.\n",
    "\n",
    "Two identities are especially useful:\n",
    "\\[\n",
    "\\mathbb{E}[K]=\f",
    "rac{n(1-p)}{p},\\qquad \\mathrm{Var}(K)=\f",
    "rac{n(1-p)}{p^2}.\n",
    "\\]\n",
    "\n",
    "Re-parameterization by mean \\(\\mu\\) and dispersion \\(n\\):\n",
    "\\[\n",
    "p = \f",
    "rac{n}{n+\\mu},\\qquad \\mathrm{Var}(K)=\\mu+\f",
    "rac{\\mu^2}{n}.\n",
    "\\]\n",
    "So with \\(\\mu\\) fixed:\n",
    "- small `n` ⇒ large variance ⇒ heavy right tail (strong overdispersion)\n",
    "- large `n` ⇒ variance close to \\(\\mu\\) ⇒ close to Poisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8642783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Left: vary p with fixed n\n",
    "n_fixed = 10\n",
    "p_values = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# Right: fix mean mu and vary dispersion n\n",
    "mu_fixed = 20\n",
    "n_values = [1, 3, 10, 50]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        f\"PMF vs p (n={n_fixed})\",\n",
    "        f\"PMF vs dispersion n (mean μ={mu_fixed})\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for p_ in p_values:\n",
    "    ks, pmf, _ = nbinom_pmf_cdf_trunc(n_fixed, p_, q=0.995)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ks, y=pmf, mode=\"markers+lines\", name=f\"p={p_}\"),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "for n_ in n_values:\n",
    "    p_ = n_ / (n_ + mu_fixed)\n",
    "    ks, pmf, _ = nbinom_pmf_cdf_trunc(n_, p_, q=0.995)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=ks,\n",
    "            y=pmf,\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"n={n_} (p={p_:.3f})\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"P(K=k)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"P(K=k)\", row=1, col=2)\n",
    "fig.update_layout(height=430, legend_title_text=\"\")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88522b20",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "Using the **Gamma–Poisson mixture** representation:\n",
    "\\(\n",
    "\\Lambda\\sim\text{Gamma}(n,\\ \text{scale}=\tfrac{1-p}{p})\n",
    "\\) and \\(K\\mid\\Lambda\\sim\text{Poisson}(\\Lambda)\\).\n",
    "\n",
    "By iterated expectation:\n",
    "\\[\n",
    "\\mathbb{E}[K] = \\mathbb{E}[\\,\\mathbb{E}[K\\mid\\Lambda] \\,] = \\mathbb{E}[\\Lambda] = n\\,\f",
    "rac{1-p}{p}.\n",
    "\\]\n",
    "\n",
    "### 6.2 Variance\n",
    "By the law of total variance:\n",
    "\\[\n",
    "\\mathrm{Var}(K)\n",
    "= \\mathbb{E}[\\mathrm{Var}(K\\mid\\Lambda)] + \\mathrm{Var}(\\mathbb{E}[K\\mid\\Lambda]).\n",
    "\\]\n",
    "For a Poisson, \\(\\mathrm{Var}(K\\mid\\Lambda)=\\Lambda\\) and \\(\\mathbb{E}[K\\mid\\Lambda]=\\Lambda\\). Hence\n",
    "\\[\n",
    "\\mathrm{Var}(K)=\\mathbb{E}[\\Lambda]+\\mathrm{Var}(\\Lambda).\n",
    "\\]\n",
    "For \\(\\Lambda\\sim\text{Gamma}(n,\text{scale}=\tfrac{1-p}{p})\\):\n",
    "\\(\n",
    "\\mathbb{E}[\\Lambda]=n\tfrac{1-p}{p}\n",
    "\\) and\n",
    "\\(\n",
    "\\mathrm{Var}(\\Lambda)=n(\tfrac{1-p}{p})^2\n",
    "\\).\n",
    "So\n",
    "\\[\n",
    "\\mathrm{Var}(K)=\f",
    "rac{n(1-p)}{p}+\f",
    "rac{n(1-p)^2}{p^2}=\f",
    "rac{n(1-p)}{p^2}.\n",
    "\\]\n",
    "\n",
    "### 6.3 Likelihood\n",
    "For i.i.d. observations \\(k_1,\\dots,k_m\\), the log-likelihood is\n",
    "\\[\n",
    "\\ell(n,p) = \\sum_{i=1}^m\\Big[\n",
    "\\log\\Gamma(k_i+n)-\\log\\Gamma(n)-\\log\\Gamma(k_i+1)\n",
    "+ k_i\\log(1-p) + n\\log p\n",
    "\\Big].\n",
    "\\]\n",
    "\n",
    "If `n` is known, the MLE for `p` has a closed form. Differentiate with respect to `p` and set to zero:\n",
    "\\[\n",
    "\\hat p = \f",
    "rac{n}{n+\bar k},\\qquad \bar k = \f",
    "rac{1}{m}\\sum_i k_i.\n",
    "\\]\n",
    "Estimating both `n` and `p` requires numerical optimization (or the mean/dispersion re-parameterization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74622b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the likelihood for p with n fixed\n",
    "\n",
    "def nbinom_loglik(n, p, data):\n",
    "    return float(nbinom_logpmf(data, n, p).sum())\n",
    "\n",
    "n_fixed = 8\n",
    "# Synthetic data (overdispersed counts)\n",
    "data = rng.negative_binomial(n=n_fixed, p=0.35, size=400)\n",
    "\n",
    "kbar = data.mean()\n",
    "p_hat_closed = n_fixed / (n_fixed + kbar)\n",
    "\n",
    "p_grid = np.linspace(1e-4, 1 - 1e-4, 400)\n",
    "ll = np.array([nbinom_loglik(n_fixed, p, data) for p in p_grid])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=p_grid, y=ll, mode=\"lines\", name=\"log-likelihood\"))\n",
    "fig.add_vline(x=p_hat_closed, line_dash=\"dash\", annotation_text=f\"MLE p≈{p_hat_closed:.3f}\")\n",
    "fig.update_layout(\n",
    "    title=f\"Log-likelihood for p (n fixed at {n_fixed})\",\n",
    "    xaxis_title=\"p\",\n",
    "    yaxis_title=\"log L(p)\",\n",
    "    width=850,\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "{\"kbar\": float(kbar), \"p_hat_closed\": float(p_hat_closed)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f4ef7",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only sampling via the Gamma–Poisson mixture\n",
    "Use the hierarchical model:\n",
    "\n",
    "1) Sample \\(\\Lambda\\sim\text{Gamma}(n,\text{scale}=\tfrac{1-p}{p})\\)\n",
    "2) Sample \\(K\\mid\\Lambda\\sim\text{Poisson}(\\Lambda)\\)\n",
    "\n",
    "This produces exact samples from \\(\text{NB}(n,p)\\), works for **any** real `n>0`, and uses only NumPy RNGs.\n",
    "\n",
    "### Alternative (integer `n`): sum of geometrics\n",
    "If `n` is an integer, you can sample `n` independent geometric “failures before success” variables and sum them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nbinom_poisson_gamma(n, p, size=1, *, rng: np.random.Generator):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    if p == 1.0:\n",
    "        return np.zeros(size, dtype=int)\n",
    "\n",
    "    # Lambda ~ Gamma(shape=n, scale=(1-p)/p)\n",
    "    lam = rng.gamma(shape=n, scale=(1 - p) / p, size=size)\n",
    "    return rng.poisson(lam)\n",
    "\n",
    "\n",
    "def sample_nbinom_geometric_sum(n, p, size=1, *, rng: np.random.Generator):\n",
    "    n, p = _validate_n_p(n, p)\n",
    "    if not float(n).is_integer():\n",
    "        raise ValueError(\"geometric-sum sampler requires integer n\")\n",
    "    n_int = int(n)\n",
    "\n",
    "    if p == 1.0:\n",
    "        return np.zeros(size, dtype=int)\n",
    "\n",
    "    size_tuple = (size,) if isinstance(size, (int, np.integer)) else tuple(size)\n",
    "\n",
    "    # NumPy's geometric returns the number of trials until first success (>=1).\n",
    "    # Failures before success = geometric - 1.\n",
    "    g = rng.geometric(p, size=size_tuple + (n_int,)) - 1\n",
    "    return g.sum(axis=-1)\n",
    "\n",
    "\n",
    "n, p = 7.5, 0.35\n",
    "x = sample_nbinom_poisson_gamma(n, p, size=200_000, rng=rng)\n",
    "\n",
    "{\n",
    "    \"theory_mean\": nbinom_mean(n, p),\n",
    "    \"mc_mean\": float(x.mean()),\n",
    "    \"theory_var\": nbinom_var(n, p),\n",
    "    \"mc_var\": float(x.var(ddof=0)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ff34f",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** (truncated to cover most probability mass)\n",
    "- the **CDF** (step function)\n",
    "- Monte Carlo samples vs the theoretical PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 10, 0.3\n",
    "\n",
    "ks, pmf, cdf = nbinom_pmf_cdf_trunc(n, p, q=0.999)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=ks, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(\n",
    "    title=f\"Negative binomial PMF (n={n}, p={p}) — truncated at CDF≈{cdf[-1]:.4f}\",\n",
    "    xaxis_title=\"k (failures)\",\n",
    "    yaxis_title=\"P(K=k)\",\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=ks, y=cdf, mode=\"lines\", line_shape=\"hv\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(\n",
    "    title=f\"Negative binomial CDF (n={n}, p={p})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(K≤k)\",\n",
    "    yaxis=dict(range=[0, 1.02]),\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "mc = sample_nbinom_poisson_gamma(n, p, size=200_000, rng=rng)\n",
    "counts = np.bincount(mc, minlength=int(ks[-1]) + 1)[: len(ks)]\n",
    "pmf_hat = counts / counts.sum()\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=ks, y=pmf_hat, name=\"Monte Carlo\", opacity=0.65))\n",
    "fig_mc.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers+lines\", name=\"Theory\"))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo vs theory (n={len(mc):,} samples)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77dddc7",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.nbinom` as an `rv_discrete` distribution.\n",
    "\n",
    "Common methods:\n",
    "- `pmf`, `logpmf`\n",
    "- `cdf`, `sf`, `ppf`\n",
    "- `rvs`\n",
    "- `stats` (mean/var/skew/excess kurtosis)\n",
    "- `entropy`\n",
    "\n",
    "### Fitting\n",
    "`rv_discrete` objects typically do **not** expose a `.fit(...)` method.\n",
    "In modern SciPy, you can fit *discrete or continuous* distributions with `scipy.stats.fit`.\n",
    "\n",
    "Note: for modeling overdispersed counts, many libraries use the mean/dispersion parameterization; always check which convention you are using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bca992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "n, p = 12, 0.4\n",
    "rv = stats.nbinom(n, p)  # frozen distribution (loc=0)\n",
    "\n",
    "ks = np.arange(0, 8)\n",
    "print(\"pmf:\", rv.pmf(ks))\n",
    "print(\"cdf:\", rv.cdf(ks))\n",
    "\n",
    "s = rv.rvs(size=10, random_state=rng)\n",
    "print(\"rvs:\", s)\n",
    "\n",
    "mean_sp, var_sp, skew_sp, kurt_sp = rv.stats(moments=\"mvsk\")\n",
    "print(\"mean/var/skew/excess_kurt:\", float(mean_sp), float(var_sp), float(skew_sp), float(kurt_sp))\n",
    "print(\"entropy (nats):\", float(rv.entropy()))\n",
    "\n",
    "# Compare SciPy's skew/kurt to formulas\n",
    "print(\"formula skew:\", nbinom_skew(n, p))\n",
    "print(\"formula excess kurt:\", nbinom_excess_kurt(n, p))\n",
    "\n",
    "# Fit with scipy.stats.fit (estimate n and p, keep loc fixed)\n",
    "true_n, true_p = 7.5, 0.35\n",
    "x = stats.nbinom.rvs(true_n, true_p, size=3000, random_state=rng)\n",
    "\n",
    "fit_res = stats.fit(\n",
    "    stats.nbinom,\n",
    "    x,\n",
    "    bounds={\n",
    "        \"n\": (1e-6, 200.0),\n",
    "        \"p\": (1e-6, 1 - 1e-6),\n",
    "        \"loc\": (0, 0),\n",
    "    },\n",
    ")\n",
    "\n",
    "fit_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35ced7",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing: detecting overdispersion vs a Poisson model\n",
    "A standard diagnostic for Poisson counts is the **dispersion** ratio:\n",
    "\\[\n",
    "\\hat\\phi = \f",
    "rac{s^2}{\bar y}\n",
    "\\]\n",
    "where \\(\bar y\\) is the sample mean and \\(s^2\\) is the sample variance.\n",
    "For a Poisson, \\(\\mathbb{E}[s^2]\u0007pprox \bar y\\), so \\(\\hat\\phi\u0007pprox 1\\).\n",
    "Values significantly larger than 1 indicate overdispersion (a common motivation for a negative binomial model).\n",
    "\n",
    "### B) Bayesian modeling: Gamma–Poisson predictive is negative binomial\n",
    "If \\(Y\\mid\\lambda\\sim\text{Poisson}(\\lambda)\\) and \\(\\lambda\\sim\text{Gamma}(\u0007lpha,\text{rate}=\beta)\\),\n",
    "then the **prior predictive** for \\(Y\\) is negative binomial.\n",
    "After observing data, the **posterior predictive** for a new count is also negative binomial.\n",
    "\n",
    "### C) Generative modeling: heterogeneous rates\n",
    "If each observation has its own random rate \\(\\Lambda_i\\) (Gamma-distributed heterogeneity), and counts are Poisson given rates,\n",
    "then the marginal distribution of counts is negative binomial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a4c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "# Synthetic example: data generated from a negative binomial (overdispersed)\n",
    "true_n, true_p = 5.0, 0.25\n",
    "m = 300\n",
    "\n",
    "y = stats.nbinom.rvs(true_n, true_p, size=m, random_state=rng)\n",
    "\n",
    "ybar = y.mean()\n",
    "s2 = y.var(ddof=1)\n",
    "\n",
    "# Dispersion test statistic (approximate): (m-1)*s^2 / ybar ~ Chi^2_{m-1} under Poisson\n",
    "D = (m - 1) * s2 / ybar\n",
    "p_value_over = 1 - chi2.cdf(D, df=m - 1)\n",
    "\n",
    "{\n",
    "    \"sample_mean\": float(ybar),\n",
    "    \"sample_var\": float(s2),\n",
    "    \"dispersion_ratio_var_over_mean\": float(s2 / ybar),\n",
    "    \"D\": float(D),\n",
    "    \"p_value_overdispersion\": float(p_value_over),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian modeling: Gamma-Poisson posterior predictive\n",
    "# Prior: lambda ~ Gamma(alpha, rate=beta)\n",
    "alpha, beta = 2.0, 1.0\n",
    "\n",
    "# Observe Poisson counts (synthetic)\n",
    "lambda_true = 3.0\n",
    "obs = stats.poisson.rvs(lambda_true, size=50, random_state=rng)\n",
    "\n",
    "alpha_post = alpha + obs.sum()\n",
    "beta_post = beta + len(obs)\n",
    "\n",
    "# Posterior predictive for a new count:\n",
    "# If lambda|data ~ Gamma(alpha_post, rate=beta_post), then\n",
    "# Y_new ~ NB(n=alpha_post, p=beta_post/(beta_post+1)) in SciPy's parameterization.\n",
    "\n",
    "n_pred = alpha_post\n",
    "p_pred = beta_post / (beta_post + 1.0)\n",
    "\n",
    "rv_pred = stats.nbinom(n_pred, p_pred)\n",
    "\n",
    "ks, pmf, cdf = nbinom_pmf_cdf_trunc(n_pred, p_pred, q=0.999)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ks, y=pmf, name=\"Posterior predictive PMF\"))\n",
    "fig.update_layout(\n",
    "    title=\"Posterior predictive for next count (Gamma–Poisson → negative binomial)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(Y_new=k | data)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"alpha_post\": float(alpha_post),\n",
    "    \"beta_post\": float(beta_post),\n",
    "    \"predictive_n\": float(n_pred),\n",
    "    \"predictive_p\": float(p_pred),\n",
    "    \"predictive_mean\": float(rv_pred.mean()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240de219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling: heterogeneous Poisson rates\n",
    "# Lambda_i ~ Gamma(shape=n, scale=(1-p)/p), then Y_i|Lambda_i ~ Poisson(Lambda_i).\n",
    "# Marginally Y_i ~ NB(n, p).\n",
    "\n",
    "n, p = 3.0, 0.4\n",
    "m = 200_000\n",
    "\n",
    "lam = rng.gamma(shape=n, scale=(1 - p) / p, size=m)\n",
    "y_mix = rng.poisson(lam)\n",
    "\n",
    "# Compare empirical histogram to the NB PMF on a truncation grid\n",
    "ks, pmf, _ = nbinom_pmf_cdf_trunc(n, p, q=0.999)\n",
    "counts = np.bincount(y_mix, minlength=int(ks[-1]) + 1)[: len(ks)]\n",
    "pmf_hat = counts / counts.sum()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ks, y=pmf_hat, name=\"Empirical (Gamma–Poisson)\", opacity=0.65))\n",
    "fig.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers+lines\", name=\"NB theory\"))\n",
    "fig.update_layout(\n",
    "    title=\"Gamma–Poisson mixture produces a negative binomial count distribution\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e6336",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameterization confusion**: some sources count *successes* before failures, or count *trials* instead of failures.\n",
    "  Always confirm what the random variable represents.\n",
    "- **`n` as real vs integer**: the “wait for `n` successes” story needs integer `n`; for real `n`, interpret via the Gamma–Poisson mixture.\n",
    "- **Degenerate boundary**: `p=1` collapses to \\(K\\equiv 0\\). Values extremely close to 0 or 1 can create numerical headaches.\n",
    "- **Numerical stability**: direct factorial / binomial-coefficient computations overflow quickly.\n",
    "  Prefer `logpmf` with gamma functions (as done here and in SciPy).\n",
    "- **Truncation**: visualizations and numerical sums over \\(k\\in\\{0,1,2,\\dots\\}\\) require truncating the tail; check captured mass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2f0fe",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `nbinom` is a **discrete** distribution on \\(\\{0,1,2,\\dots\\}\\) modeling failures before `n` successes.\n",
    "- PMF: \\(\binom{k+n-1}{k}(1-p)^k p^n\\) (gamma-form extends to real `n>0`).\n",
    "- Mean/variance: \\(\\mathbb{E}[K]=n(1-p)/p\\), \\(\\mathrm{Var}(K)=n(1-p)/p^2\\) ⇒ overdispersion vs Poisson.\n",
    "- Key relationship: **Gamma–Poisson mixture** ⇔ negative binomial.\n",
    "- For computation, prefer **log-PMF**; for simulation, the **Gamma–Poisson** sampler is simple and exact.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}