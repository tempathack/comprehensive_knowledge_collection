{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad762cc",
   "metadata": {},
   "source": [
    "# `betanbinom` (Beta-Negative Binomial)\n",
    "\n",
    "The **beta-negative binomial** distribution models a *count* (discrete) outcome:\n",
    "\n",
    "> the number of **failures** you see before achieving **`n` successes**,\n",
    "> when the success probability **`p` is unknown** and is modeled as a **Beta(`a`,`b`)** random variable.\n",
    "\n",
    "It is a natural choice when you want a negative binomial model, but you also want to account for **heterogeneity / uncertainty in the success probability**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- write the PMF and recognize it as a **Beta mixture of a negative binomial**\n",
    "- derive the **mean** and **variance** using the law of total expectation/variance\n",
    "- implement a **NumPy-only sampler** via the hierarchical definition\n",
    "- visualize the PMF/CDF and check Monte Carlo simulations\n",
    "- use `scipy.stats.betanbinom` (and fit parameters via a custom MLE routine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e40641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import betaln, gammaln, hyp2f1\n",
    "from scipy.stats import betanbinom\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49802eb0",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `betanbinom` (Beta-negative binomial)\n",
    "- **Type**: **Discrete** distribution\n",
    "- **Support** (SciPy standardized form):\n",
    "  - \\(k \\in \\{0,1,2,\\dots\\}\\)\n",
    "  - with shift: \\(X = K + \\text{loc}\\)\n",
    "- **Parameter space** (SciPy):\n",
    "  - \\(n \\ge 0\\) (often an integer “number of successes”, but the PMF can be extended via Gamma functions)\n",
    "  - \\(a > 0\\), \\(b > 0\\) (Beta shape parameters)\n",
    "\n",
    "**Moment existence (important for interpretation):**\n",
    "\n",
    "- \\(\\mathbb{E}[X]\\) exists if \\(a > 1\\)\n",
    "- \\(\\mathrm{Var}(X)\\) exists if \\(a > 2\\)\n",
    "- higher moments require larger \\(a\\) (heavy tail when \\(a\\) is near 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1beb1",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "You run Bernoulli trials (success/failure). You stop once you have seen **`n` successes**.\n",
    "Let \\(X\\) be the number of **failures** observed *before* the \\(n\\)-th success.\n",
    "\n",
    "If the success probability \\(p\\) were known and fixed, \\(X\\) would follow a **negative binomial** distribution.\n",
    "\n",
    "In many real settings, \\(p\\) varies across people, items, time periods, or is simply uncertain. A Beta distribution is a standard way to encode that uncertainty:\n",
    "\n",
    "- \\(p \\sim \\mathrm{Beta}(a,b)\\)\n",
    "- \\(X \\mid p \\sim \\mathrm{NegBin}(n, p)\\)  (failures before \\(n\\) successes)\n",
    "\n",
    "Marginalizing out \\(p\\) gives `betanbinom`.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Conversion funnels / retries**: number of failed attempts before achieving `n` successes (signups, purchases), when conversion rate varies by user.\n",
    "- **Reliability / quality control**: how many failed tests occur before `n` passes, with heterogeneous pass rates.\n",
    "- **Overdispersed counts**: compared to a plain negative binomial, the extra randomness in \\(p\\) produces **heavier tails**.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Negative binomial**: recovered when the Beta distribution concentrates at a fixed \\(p\\) (e.g., \\(a+b\\to\\infty\\) with \\(a/(a+b)=p\\)).\n",
    "- **Beta-geometric**: when \\(n=1\\), you count failures before the first success.\n",
    "- **Beta-binomial**: finite-trial analogue (random \\(p\\), then a Binomial count)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c2e2e",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### Hierarchical (mixture) definition\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "P &\\sim \\mathrm{Beta}(a,b), \\\\\n",
    "X \\mid P=p &\\sim \\mathrm{NegBin}(n,p), \\qquad X \\in \\{0,1,2,\\dots\\}.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Here \\(X\\) counts failures before \\(n\\) successes, so the conditional PMF is\n",
    "\n",
    "\\[\n",
    "\\Pr(X=k\\mid p) = \\binom{n+k-1}{k} (1-p)^k p^n, \\qquad k\\ge 0.\n",
    "\\]\n",
    "\n",
    "### PMF\n",
    "\n",
    "After integrating out \\(p\\), the (standardized) PMF is\n",
    "\n",
    "\\[\n",
    "\\Pr(X=k) = \\binom{n+k-1}{k} \\frac{B(a+n,\\, b+k)}{B(a,b)}, \\qquad k=0,1,2,\\dots\n",
    "\\]\n",
    "\n",
    "where \\(B(\\cdot,\\cdot)\\) is the Beta function:\n",
    "\n",
    "\\[\n",
    "B(x,y) = \\int_0^1 t^{x-1}(1-t)^{y-1}\\,dt = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}.\n",
    "\\]\n",
    "\n",
    "### CDF\n",
    "\n",
    "Because the support is discrete/infinite, a standard definition is\n",
    "\n",
    "\\[\n",
    "F(k) = \\Pr(X\\le k) = \\sum_{j=0}^{\\lfloor k \\rfloor} \\Pr(X=j).\n",
    "\\]\n",
    "\n",
    "In practice, we compute this sum numerically or use SciPy’s `cdf`/`sf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logpmf_betanbinom(k, n, a, b):\n",
    "    # Stable log-PMF using log-gamma and log-beta.\n",
    "    # PMF: C(n+k-1, k) * B(a+n, b+k) / B(a,b),  k=0,1,2,...\n",
    "    k = np.asarray(k)\n",
    "    if np.any(k < 0):\n",
    "        out = np.full_like(k, fill_value=-np.inf, dtype=float)\n",
    "        out[k < 0] = -np.inf\n",
    "        return out\n",
    "\n",
    "    k_int = k.astype(int)\n",
    "    if np.any(k_int != k):\n",
    "        raise ValueError(\"k must be integer-valued for the discrete PMF.\")\n",
    "\n",
    "    # log binomial coefficient with Gamma functions\n",
    "    log_choose = gammaln(n + k_int) - gammaln(n) - gammaln(k_int + 1)\n",
    "\n",
    "    return log_choose + betaln(a + n, b + k_int) - betaln(a, b)\n",
    "\n",
    "\n",
    "def pmf_betanbinom(k, n, a, b):\n",
    "    return np.exp(logpmf_betanbinom(k, n, a, b))\n",
    "\n",
    "\n",
    "def cdf_betanbinom(k, n, a, b):\n",
    "    k = np.asarray(k)\n",
    "    k_floor = np.floor(k).astype(int)\n",
    "    out = np.zeros_like(k_floor, dtype=float)\n",
    "\n",
    "    for idx, kk in np.ndenumerate(k_floor):\n",
    "        if kk < 0:\n",
    "            out[idx] = 0.0\n",
    "            continue\n",
    "        ks = np.arange(0, kk + 1)\n",
    "        out[idx] = pmf_betanbinom(ks, n, a, b).sum()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def support_for_plotting(rv, q=0.999, fallback_max=2000):\n",
    "    # Choose a reasonable finite support grid for plotting.\n",
    "    try:\n",
    "        k_max = rv.ppf(q)\n",
    "    except Exception:\n",
    "        k_max = np.nan\n",
    "\n",
    "    try:\n",
    "        k_max = np.asarray(k_max).item()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if np.isfinite(k_max):\n",
    "        k_max = int(k_max)\n",
    "        return np.arange(0, max(k_max, 20) + 1)\n",
    "\n",
    "    # Fallback: mean + a few std devs (if finite)\n",
    "    try:\n",
    "        mean, var = rv.stats(moments=\"mv\")\n",
    "    except Exception:\n",
    "        mean, var = np.nan, np.nan\n",
    "\n",
    "    if np.isfinite(mean) and np.isfinite(var) and var >= 0:\n",
    "        k_max = int(np.ceil(mean + 8 * np.sqrt(var)))\n",
    "        k_max = min(k_max, fallback_max)\n",
    "        return np.arange(0, max(k_max, 20) + 1)\n",
    "\n",
    "    # Last resort: fixed cap\n",
    "    return np.arange(0, 200 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick correctness check vs SciPy\n",
    "n, a, b = 8, 4.5, 2.0\n",
    "rv = betanbinom(n, a, b)\n",
    "ks = np.arange(0, 80)\n",
    "\n",
    "max_abs_err = np.max(np.abs(pmf_betanbinom(ks, n, a, b) - rv.pmf(ks)))\n",
    "max_abs_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66236a50",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### A useful general result: factorial moments\n",
    "\n",
    "For discrete counts, **falling factorial moments** often simplify:\n",
    "\n",
    "\\[\n",
    "(X)_m = X(X-1)\\cdots(X-m+1).\n",
    "\\]\n",
    "\n",
    "For `betanbinom`, using the Poisson–Gamma representation of the negative binomial and then mixing over \\(p\\), one obtains (for \\(a>m\\)):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[(X)_m] = (n)_{m}\\; \\frac{B(a-m,\\, b+m)}{B(a,b)},\n",
    "\\]\n",
    "\n",
    "where \\((n)_m = \\frac{\\Gamma(n+m)}{\\Gamma(n)}\\) is the **rising** Pochhammer symbol.\n",
    "\n",
    "From factorial moments you can recover raw moments using Stirling numbers of the second kind.\n",
    "\n",
    "### Mean and variance (closed form)\n",
    "\n",
    "Provided \\(a>2\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\frac{n\\,b}{a-1},\n",
    "\\qquad\n",
    "\\mathrm{Var}(X) = \\frac{n\\,b\\,(a+b-1)\\,(n+a-1)}{(a-1)^2\\,(a-2)}.\n",
    "\\]\n",
    "\n",
    "### Skewness and kurtosis\n",
    "\n",
    "Closed forms exist but are algebraically messy; a clean route is:\n",
    "\n",
    "1) compute \\(\\mathbb{E}[(X)_m]\\) for \\(m=1,2,3,4\\)\n",
    "2) convert to \\(\\mathbb{E}[X^r]\\) for \\(r\\le 4\\)\n",
    "3) convert to central moments and then to skewness/kurtosis\n",
    "\n",
    "We’ll do this in code (and verify against SciPy’s `stats`).\n",
    "\n",
    "### PGF / MGF / characteristic function\n",
    "\n",
    "A probability generating function (for \\(|z|<1\\)) is\n",
    "\n",
    "\\[\n",
    "G(z)=\\mathbb{E}[z^X]\n",
    "= (1-z)^{-n}\\,\\frac{B(a+n,b)}{B(a,b)}\\; {}_2F_1\\!\\left(n, a+n; a+b+n; -\\frac{z}{1-z}\\right).\n",
    "\\]\n",
    "\n",
    "The MGF is \\(M(t)=G(e^t)\\), which typically exists only for **\\(t<0\\)** because the distribution can have heavy tails.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "There is no simple closed-form expression in general; numerically:\n",
    "\n",
    "\\[\n",
    "H(X) = -\\sum_{k=0}^{\\infty} \\Pr(X=k)\\,\\log \\Pr(X=k),\n",
    "\\]\n",
    "\n",
    "approximated by truncating the sum where the remaining tail probability is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f294788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorial moments and conversion to mean/var/skew/kurt\n",
    "\n",
    "def factorial_moment(m, n, a, b):\n",
    "    # E[(X)_m] for m=0,1,2,... (requires a>m)\n",
    "    if m == 0:\n",
    "        return 1.0\n",
    "    if a <= m:\n",
    "        return np.nan\n",
    "\n",
    "    poch = np.exp(gammaln(n + m) - gammaln(n))  # (n)_m rising\n",
    "    return poch * np.exp(betaln(a - m, b + m) - betaln(a, b))\n",
    "\n",
    "\n",
    "def raw_moments_up_to_4(n, a, b):\n",
    "    f1 = factorial_moment(1, n, a, b)\n",
    "    f2 = factorial_moment(2, n, a, b)\n",
    "    f3 = factorial_moment(3, n, a, b)\n",
    "    f4 = factorial_moment(4, n, a, b)\n",
    "\n",
    "    # x^r = sum_{j=0}^r S(r,j) (x)_j, for r<=4\n",
    "    e1 = f1\n",
    "    e2 = f2 + f1\n",
    "    e3 = f3 + 3 * f2 + f1\n",
    "    e4 = f4 + 6 * f3 + 7 * f2 + f1\n",
    "    return e1, e2, e3, e4\n",
    "\n",
    "\n",
    "def mean_var_skew_kurt(n, a, b):\n",
    "    mu1, m2, m3, m4 = raw_moments_up_to_4(n, a, b)\n",
    "\n",
    "    var = m2 - mu1**2\n",
    "    mu3 = m3 - 3 * mu1 * m2 + 2 * mu1**3\n",
    "    mu4 = m4 - 4 * mu1 * m3 + 6 * mu1**2 * m2 - 3 * mu1**4\n",
    "\n",
    "    skew = mu3 / (var ** 1.5)\n",
    "    ex_kurt = mu4 / (var ** 2) - 3\n",
    "    return mu1, var, skew, ex_kurt\n",
    "\n",
    "\n",
    "n, a, b = 8, 4.5, 2.0\n",
    "mu, var, skew, ex_kurt = mean_var_skew_kurt(n, a, b)\n",
    "mu, var, skew, ex_kurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to SciPy\n",
    "n, a, b = 8, 4.5, 2.0\n",
    "rv = betanbinom(n, a, b)\n",
    "scipy_mu, scipy_var, scipy_skew, scipy_ex_kurt = rv.stats(moments=\"mvsk\")\n",
    "\n",
    "np.array([mu, var, skew, ex_kurt]), np.array([scipy_mu, scipy_var, scipy_skew, scipy_ex_kurt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8b191",
   "metadata": {},
   "source": [
    "### Entropy (numerical) and generating functions\n",
    "\n",
    "A good way to build intuition is to check that:\n",
    "\n",
    "- a truncated entropy sum agrees with SciPy’s `entropy()` (up to truncation error)\n",
    "- the closed-form PGF matches the definition \\(G(z)=\\sum_k z^k\\Pr(X=k)\\) for \\(|z|<1\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f1782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "\n",
    "def pgf_betanbinom(z, n, a, b):\n",
    "    # Probability generating function G(z)=E[z^X] for |z|<1.\n",
    "    z = np.asarray(z)\n",
    "\n",
    "    # log prefactor: (1-z)^(-n) * B(a+n,b)/B(a,b)\n",
    "    log_pref = -n * np.log(1 - z) + (betaln(a + n, b) - betaln(a, b))\n",
    "    x = -z / (1 - z)\n",
    "\n",
    "    return np.exp(log_pref) * hyp2f1(n, a + n, a + b + n, x)\n",
    "\n",
    "\n",
    "n, a, b = 10, 6.0, 3.0\n",
    "rv = betanbinom(n, a, b)\n",
    "\n",
    "# Entropy: SciPy vs truncation\n",
    "ks_H = support_for_plotting(rv, q=0.9999)\n",
    "pmf_H = rv.pmf(ks_H)\n",
    "H_trunc = -np.sum(pmf_H * np.log(pmf_H + 1e-300))\n",
    "tail_mass = 1.0 - rv.cdf(ks_H[-1])\n",
    "\n",
    "H_scipy = rv.entropy()\n",
    "\n",
    "print(f\"Entropy (SciPy)    : {H_scipy:.6f}\")\n",
    "print(f\"Entropy (truncated): {H_trunc:.6f}\")\n",
    "print(f\"Truncation tail mass ~ {tail_mass:.2e}\")\n",
    "\n",
    "# PGF sanity check\n",
    "z = 0.4\n",
    "ks = support_for_plotting(rv, q=0.9999)\n",
    "pgf_sum = np.sum(rv.pmf(ks) * (z ** ks))\n",
    "pgf_closed = pgf_betanbinom(z, n, a, b)\n",
    "\n",
    "print(f\"PGF via sum      : {pgf_sum:.12f}\")\n",
    "print(f\"PGF closed-form  : {pgf_closed:.12f}\")\n",
    "print(f\"abs diff         : {abs(pgf_sum - pgf_closed):.2e}\")\n",
    "\n",
    "# MGF exists at least for t<0 (since z=e^t<1)\n",
    "t = -0.25\n",
    "print(f\"MGF(t) with t={t}: {pgf_betanbinom(exp(t), n, a, b):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d73435",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### `n` (number of successes)\n",
    "\n",
    "- In the “stop after `n` successes” story, `n` is how many successes you require.\n",
    "- Larger `n` typically increases the scale (more opportunities to accumulate failures).\n",
    "\n",
    "### `a, b` (Beta prior on the success probability)\n",
    "\n",
    "For \\(P\\sim\\mathrm{Beta}(a,b)\\):\n",
    "\n",
    "- \\(\\mathbb{E}[P] = \\frac{a}{a+b}\\) (prior mean success probability)\n",
    "- \\(a+b\\) controls **concentration** (how variable \\(P\\) is)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- **Large `a`** (relative to `b`) pushes \\(P\\) toward 1 → fewer failures.\n",
    "- **Large `b`** pushes \\(P\\) toward 0 → more failures and heavier tail.\n",
    "- **Small `a+b`** means \\(P\\) varies a lot → `betanbinom` is more overdispersed than a plain negative binomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeadfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pmf(params_list, q=0.995, title=\"PMF comparison\"):\n",
    "    fig = go.Figure()\n",
    "    max_k = 0\n",
    "    for (n, a, b, name) in params_list:\n",
    "        rv = betanbinom(n, a, b)\n",
    "        ks = support_for_plotting(rv, q=q)\n",
    "        max_k = max(max_k, ks.max())\n",
    "        fig.add_trace(go.Bar(x=ks, y=rv.pmf(ks), name=name, opacity=0.6))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"k (failures before n successes)\",\n",
    "        yaxis_title=\"PMF\",\n",
    "        barmode=\"overlay\",\n",
    "        width=850,\n",
    "        height=450,\n",
    "    )\n",
    "    fig.update_xaxes(range=[0, min(max_k, 120)])\n",
    "    return fig\n",
    "\n",
    "\n",
    "params = [\n",
    "    (10, 2.0, 2.0, \"n=10, a=2, b=2 (mean p=0.5, high var)\"),\n",
    "    (10, 20.0, 20.0, \"n=10, a=20, b=20 (mean p=0.5, low var)\"),\n",
    "    (10, 2.0, 8.0, \"n=10, a=2, b=8 (mean p=0.2)\"),\n",
    "]\n",
    "plot_pmf(params, title=\"How (a,b) shapes the PMF\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40038d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_n = [\n",
    "    (1, 6.0, 3.0, \"n=1\"),\n",
    "    (5, 6.0, 3.0, \"n=5\"),\n",
    "    (20, 6.0, 3.0, \"n=20\"),\n",
    "]\n",
    "plot_pmf(params_n, title=\"Effect of n (Beta prior fixed)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07072edc",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "\n",
    "Using the hierarchical model and the law of total expectation:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\mathbb{E}[\\,\\mathbb{E}[X\\mid P]\\,].\n",
    "\\]\n",
    "\n",
    "For \\(X\\mid P=p\\sim\\mathrm{NegBin}(n,p)\\) (failures before \\(n\\) successes):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X\\mid p] = \\frac{n(1-p)}{p}.\n",
    "\\]\n",
    "\n",
    "So\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = n\\,\\mathbb{E}\\!\\left[\\frac{1-p}{p}\\right]\n",
    "= n\\,(\\mathbb{E}[p^{-1}] - 1).\n",
    "\\]\n",
    "\n",
    "For \\(P\\sim\\mathrm{Beta}(a,b)\\), \\(\\mathbb{E}[p^{-1}]\\) exists only if \\(a>1\\), and\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[p^{-1}] = \\frac{B(a-1,b)}{B(a,b)} = \\frac{a+b-1}{a-1}.\n",
    "\\]\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\[\n",
    "\\boxed{\\mathbb{E}[X] = \\frac{n\\,b}{a-1}} \\quad (a>1).\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Variance\n",
    "\n",
    "Law of total variance:\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X\\mid P)] + \\mathrm{Var}(\\mathbb{E}[X\\mid P]).\n",
    "\\]\n",
    "\n",
    "For the negative binomial (failures before \\(n\\) successes):\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X\\mid p) = \\frac{n(1-p)}{p^2}.\n",
    "\\]\n",
    "\n",
    "Both terms require \\(\\mathbb{E}[p^{-2}]\\), which exists only if \\(a>2\\). After algebra you get:\n",
    "\n",
    "\\[\n",
    "\\boxed{\\mathrm{Var}(X) = \\frac{n\\,b\\,(a+b-1)\\,(n+a-1)}{(a-1)^2\\,(a-2)}} \\quad (a>2).\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Likelihood (for data)\n",
    "\n",
    "Given i.i.d. observations \\(k_1,\\dots,k_m\\) from `betanbinom(n,a,b)`, the likelihood is\n",
    "\n",
    "\\[\n",
    "L(n,a,b) = \\prod_{i=1}^m \\binom{n+k_i-1}{k_i}\\,\\frac{B(a+n, b+k_i)}{B(a,b)}.\n",
    "\\]\n",
    "\n",
    "A numerically stable log-likelihood uses `gammaln` and `betaln` (log-Beta).\n",
    "\n",
    "In many applications `n` is known (you decide how many successes to wait for), and you fit `a,b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d353e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify mean/variance by Monte Carlo\n",
    "n, a, b = 10, 6.0, 3.0\n",
    "rv = betanbinom(n, a, b)\n",
    "\n",
    "mu_theory, var_theory = rv.stats(moments=\"mv\")\n",
    "\n",
    "samples = rv.rvs(size=200_000, random_state=rng)\n",
    "\n",
    "mu_mc = samples.mean()\n",
    "var_mc = samples.var(ddof=0)\n",
    "\n",
    "(mu_theory, var_theory), (mu_mc, var_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cff7c",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "### Algorithm (hierarchical sampling)\n",
    "\n",
    "The definition already gives a sampler:\n",
    "\n",
    "1) Sample \\(p\\sim\\mathrm{Beta}(a,b)\\)\n",
    "2) Sample \\(X\\mid p\\sim\\mathrm{NegBin}(n,p)\\) (failures before `n` successes)\n",
    "\n",
    "This is efficient and easy to vectorize.\n",
    "\n",
    "Below is a **NumPy-only** implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rvs_betanbinom_numpy(n, a, b, size, rng=None):\n",
    "    # NumPy-only sampler using the hierarchical definition.\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    p = rng.beta(a, b, size=size)\n",
    "    # NumPy's negative_binomial returns failures before n successes (success prob = p)\n",
    "    x = rng.negative_binomial(n, p)\n",
    "    return x\n",
    "\n",
    "\n",
    "n, a, b = 10, 6.0, 3.0\n",
    "x_np = rvs_betanbinom_numpy(n, a, b, size=10_000, rng=rng)\n",
    "x_np[:10], x_np.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467f8d1",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll plot:\n",
    "\n",
    "- PMF (exact)\n",
    "- CDF (exact)\n",
    "- Monte Carlo histogram compared to the PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, a, b = 10, 6.0, 3.0\n",
    "rv = betanbinom(n, a, b)\n",
    "ks = support_for_plotting(rv, q=0.995)\n",
    "\n",
    "pmf = rv.pmf(ks)\n",
    "cdf = rv.cdf(ks)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=ks, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(\n",
    "    title=\"betanbinom PMF\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(X=k)\",\n",
    "    width=850,\n",
    "    height=420,\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=ks, y=cdf, mode=\"lines+markers\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(\n",
    "    title=\"betanbinom CDF\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(X ≤ k)\",\n",
    "    width=850,\n",
    "    height=420,\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo vs PMF\n",
    "samples = rv.rvs(size=50_000, random_state=rng)\n",
    "\n",
    "fig_mc = px.histogram(\n",
    "    samples,\n",
    "    nbins=int(ks.max() + 1),\n",
    "    histnorm=\"probability\",\n",
    "    title=\"Monte Carlo histogram vs exact PMF\",\n",
    ")\n",
    "fig_mc.update_layout(xaxis_title=\"k\", yaxis_title=\"empirical probability\", width=850, height=420)\n",
    "fig_mc.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers\", name=\"exact PMF\"))\n",
    "fig_mc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbdc4a",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.betanbinom` as a discrete distribution.\n",
    "\n",
    "Common methods:\n",
    "\n",
    "- `pmf(k, n, a, b)` / `logpmf(...)`\n",
    "- `cdf(k, n, a, b)` / `sf(...)`\n",
    "- `rvs(n, a, b, size=..., random_state=...)`\n",
    "- `stats(..., moments='mvsk')`, `entropy(...)`\n",
    "\n",
    "### About `fit`\n",
    "\n",
    "As of SciPy **1.15.0**, `rv_discrete` distributions like `betanbinom` **do not provide a `.fit()` method**.\n",
    "\n",
    "A standard replacement is to implement **maximum likelihood** yourself using `scipy.optimize`.\n",
    "Below we fit `a,b` assuming `n` is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SciPy usage\n",
    "n, a, b = 10, 6.0, 3.0\n",
    "rv = betanbinom(n, a, b)\n",
    "\n",
    "rv.pmf([0, 1, 2, 3]), rv.cdf([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb37db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_loglik_ab(log_ab, data, n, eps=1e-12):\n",
    "    # Negative log-likelihood for a,b (n fixed), parameterized in log-space.\n",
    "    log_a, log_b = log_ab\n",
    "    a = np.exp(log_a)\n",
    "    b = np.exp(log_b)\n",
    "\n",
    "    if not np.isfinite(a) or not np.isfinite(b) or a <= eps or b <= eps:\n",
    "        return np.inf\n",
    "\n",
    "    data = np.asarray(data)\n",
    "    if np.any(data < 0) or np.any(np.floor(data) != data):\n",
    "        return np.inf\n",
    "\n",
    "    ll = logpmf_betanbinom(data, n, a, b).sum()\n",
    "    return -ll\n",
    "\n",
    "\n",
    "def fit_ab_mle(data, n, a0=2.0, b0=2.0):\n",
    "    x0 = np.log([a0, b0])\n",
    "    res = minimize(neg_loglik_ab, x0=x0, args=(data, n), method=\"Nelder-Mead\")\n",
    "    a_hat, b_hat = np.exp(res.x)\n",
    "    return a_hat, b_hat, res\n",
    "\n",
    "\n",
    "# Simulate data from known parameters, then fit a,b\n",
    "n_true, a_true, b_true = 10, 6.0, 3.0\n",
    "rv_true = betanbinom(n_true, a_true, b_true)\n",
    "\n",
    "data = rv_true.rvs(size=2_000, random_state=rng)\n",
    "\n",
    "a_hat, b_hat, res = fit_ab_mle(data, n=n_true, a0=3.0, b0=3.0)\n",
    "(a_true, b_true), (a_hat, b_hat), res.success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42161aec",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing / anomaly detection\n",
    "\n",
    "If you have a baseline `betanbinom(n,a,b)` model for failures-before-successes, you can test whether an observed count \\(k_\\text{obs}\\) is unusually large:\n",
    "\n",
    "\\[\n",
    "\\text{p-value} = \\Pr(X \\ge k_\\text{obs}) = \\mathrm{sf}(k_\\text{obs}-1).\n",
    "\\]\n",
    "\n",
    "This is a one-sided “too many failures” test.\n",
    "\n",
    "### 10.2 Bayesian modeling (posterior + posterior predictive)\n",
    "\n",
    "If \\(P\\sim\\mathrm{Beta}(a,b)\\) and you observe **`n` successes** and **`k` failures** (i.e., you stopped at the \\(n\\)-th success), then\n",
    "\n",
    "\\[\n",
    "P \\mid (k,n) \\sim \\mathrm{Beta}(a+n, b+k).\n",
    "\\]\n",
    "\n",
    "The **posterior predictive** distribution for *future failures* before `n_future` successes is again beta-negative binomial:\n",
    "\n",
    "\\[\n",
    "X_\\text{future} \\mid (k,n) \\sim \\mathrm{betanbinom}(n_\\text{future},\\; a+n,\\; b+k).\n",
    "\\]\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "\n",
    "In a hierarchical generative model, you might sample \\(p_i\\) per individual/group from a Beta distribution, then generate counts via a negative binomial. The resulting marginal distribution across individuals is `betanbinom`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Hypothesis testing example: is k_obs unusually large?\n",
    "\n",
    "n, a, b = 10, 6.0, 3.0\n",
    "rv = betanbinom(n, a, b)\n",
    "\n",
    "k_obs = 40\n",
    "p_value = rv.sf(k_obs - 1)  # P(X >= k_obs)\n",
    "\n",
    "k_obs, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347dad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian update + posterior predictive example\n",
    "\n",
    "# Prior on p\n",
    "a0, b0 = 6.0, 3.0\n",
    "\n",
    "# Observe: stop after n successes, saw k failures\n",
    "n_obs = 10\n",
    "k_obs = 40\n",
    "\n",
    "# Posterior on p\n",
    "a_post = a0 + n_obs\n",
    "b_post = b0 + k_obs\n",
    "\n",
    "# Posterior mean of p\n",
    "p_post_mean = a_post / (a_post + b_post)\n",
    "\n",
    "# Posterior predictive: failures before n_future successes\n",
    "n_future = 10\n",
    "rv_pred = betanbinom(n_future, a_post, b_post)\n",
    "\n",
    "ks = support_for_plotting(rv_pred, q=0.99)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ks, y=rv_pred.pmf(ks), name=\"posterior predictive PMF\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Posterior predictive (n_future={n_future}) after observing k={k_obs} failures, n={n_obs} successes\",\n",
    "    xaxis_title=\"future failures\",\n",
    "    yaxis_title=\"probability\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "p_post_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059ce46",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Moment conditions**: mean requires \\(a>1\\), variance requires \\(a>2\\), etc. If `a` is near 1 the tail can be extremely heavy.\n",
    "- **Parameter interpretation**: `n` is often an integer in the “wait for `n` successes” story; extending `n` to non-integers is mathematically possible via Gamma functions but may not match your data-generating process.\n",
    "- **Numerical stability**: computing \\(\\binom{n+k-1}{k}\\) directly will overflow; use `gammaln`/`betaln` (log-space).\n",
    "- **Infinite support**: CDF/entropy computations require truncation; use `sf` for tail probabilities and choose cutoffs via quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2abfd",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `betanbinom` is a **Beta mixture** of a **negative binomial** (failures before `n` successes).\n",
    "- It is useful when the success probability is **uncertain or heterogeneous**, producing **overdispersion**.\n",
    "- The PMF has a compact Beta-function form, and mean/variance are available in closed form (with conditions on `a`).\n",
    "- Sampling is straightforward via the hierarchical model using **NumPy-only** random generators.\n",
    "- SciPy implements the distribution as `scipy.stats.betanbinom` (but you fit parameters via custom MLE routines).\n",
    "\n",
    "### References\n",
    "\n",
    "- SciPy docstring: `scipy.stats.betanbinom`\n",
    "- Wikipedia: “Beta negative binomial distribution”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import plotly\n",
    "\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
