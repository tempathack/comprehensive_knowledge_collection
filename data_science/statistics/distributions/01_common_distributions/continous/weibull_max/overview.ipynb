{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9e7883",
   "metadata": {},
   "source": [
    "# Weibull maximum distribution (`weibull_max`)\n",
    "\n",
    "The **Weibull maximum** distribution (also called the **reversed Weibull** or **Type III extreme value** law) is a **continuous** distribution with a **finite upper endpoint**. It’s a natural model when outcomes can be *arbitrarily small*, but cannot exceed some maximum (a hard cap).\n",
    "\n",
    "A very useful way to think about it:\n",
    "\n",
    "- If $Y\\ge 0$ is a standard Weibull (minimum) random variable, then $X=\\mu-Y$ has a Weibull-maximum distribution with upper endpoint $\\mu$.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Title & classification\n",
    "\n",
    "| Item | Value |\n",
    "|---|---|\n",
    "| Name | Weibull maximum (`weibull_max`) |\n",
    "| Type | Continuous |\n",
    "| Support | $x \\in (-\\infty,\\mu]$ |\n",
    "| Parameter space | shape $c>0$, location $\\mu\\in\\mathbb{R}$, scale $\\lambda>0$ |\n",
    "\n",
    "We’ll use SciPy’s convention: shape is `c`, and `loc=\\mu`, `scale=\\lambda`.\n",
    "\n",
    "### What you’ll be able to do after this notebook\n",
    "\n",
    "- write down the PDF/CDF/quantile in a clean parameterization\n",
    "- interpret how $(c,\\lambda,\\mu)$ change the shape and support\n",
    "- derive $\\mathbb{E}[X]$, $\\mathrm{Var}(X)$, and the log-likelihood\n",
    "- sample from `weibull_max` using **NumPy only** (inverse transform)\n",
    "- use `scipy.stats.weibull_max` for simulation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.special import gamma\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ae1b9",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "The key structural feature is the **finite upper endpoint** $\\mu$:\n",
    "\n",
    "- $X$ can take very negative values, but it cannot exceed $\\mu$.\n",
    "- Equivalently, the **distance to the cap**\n",
    "\n",
    "$$\n",
    "Y = \\mu - X \\ge 0\n",
    "$$\n",
    "\n",
    "has a standard **Weibull (minimum)** distribution.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Extreme value modeling (block maxima)** when the underlying phenomenon has a **hard upper bound** (material strengths, physical limits, bounded scores).\n",
    "- **“Only-negative” noise** models: $\\varepsilon\\le 0$ with a tunable left tail, useful when observations are systematically below a theoretical maximum.\n",
    "- **Time-to-failure style modeling** via $Y=\\mu-X$ (a standard Weibull) while keeping the original variable $X$ bounded above.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Reflection/shift of `weibull_min`**: if $Y\\sim\\texttt{weibull\\_min}(c,\\,\\text{scale}=\\lambda)$ then $X=\\mu-Y\\sim\\texttt{weibull\\_max}(c,\\,\\text{loc}=\\mu,\\,\\text{scale}=\\lambda)$.\n",
    "- **Generalized extreme value (GEV)**: the reversed Weibull is the **Type III** domain (finite upper endpoint). In GEV terms, it corresponds to a **negative** shape parameter $\\xi<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668293a8",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "We parameterize with shape $c>0$, location (upper endpoint) $\\mu\\in\\mathbb{R}$, and scale $\\lambda>0$.\n",
    "\n",
    "Define the nonnegative “distance to the endpoint”\n",
    "\n",
    "$$\n",
    "z(x) = \\frac{\\mu-x}{\\lambda}.\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "$$\n",
    "F(x) = \\mathbb{P}(X\\le x) =\n",
    "\\begin{cases}\n",
    "\\exp\\!\\big(-z(x)^c\\big), & x\\le \\mu,\\\\\n",
    "1, & x>\\mu.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### PDF\n",
    "\n",
    "Differentiate the CDF (for $x\\le \\mu$):\n",
    "\n",
    "$$\n",
    " f(x) = \\frac{c}{\\lambda}\\,z(x)^{c-1}\\,\\exp\\!\\big(-z(x)^c\\big),\\qquad x\\le\\mu,\n",
    "$$\n",
    "\n",
    "and $f(x)=0$ for $x>\\mu$.\n",
    "\n",
    "### Quantile function (inverse CDF)\n",
    "\n",
    "For $p\\in(0,1)$,\n",
    "\n",
    "$$\n",
    "Q(p)=\\mu-\\lambda\\,(-\\ln p)^{1/c}.\n",
    "$$\n",
    "\n",
    "This makes sampling straightforward (inverse transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ad7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_max_logpdf(x, c, loc=0.0, scale=1.0):\n",
    "    '''Log-PDF of weibull_max(c, loc, scale) with support (-inf, loc].'''\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "\n",
    "    if c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"c must be > 0 and scale must be > 0\")\n",
    "\n",
    "    y = (loc - x) / scale  # y >= 0 on the support\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "\n",
    "    # Special-case c=1 to avoid 0 * log(0) at the endpoint.\n",
    "    if np.isclose(c, 1.0):\n",
    "        mask = y >= 0\n",
    "        out[mask] = -np.log(scale) - y[mask]\n",
    "        return out\n",
    "\n",
    "    mask_pos = y > 0\n",
    "    out[mask_pos] = np.log(c / scale) + (c - 1) * np.log(y[mask_pos]) - y[mask_pos] ** c\n",
    "\n",
    "    mask_zero = y == 0\n",
    "    if np.any(mask_zero):\n",
    "        out[mask_zero] = np.inf if c < 1 else -np.inf\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_max_pdf(x, c, loc=0.0, scale=1.0):\n",
    "    return np.exp(weibull_max_logpdf(x, c, loc=loc, scale=scale))\n",
    "\n",
    "\n",
    "def weibull_max_cdf(x, c, loc=0.0, scale=1.0):\n",
    "    '''CDF of weibull_max(c, loc, scale).'''\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "\n",
    "    if c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"c must be > 0 and scale must be > 0\")\n",
    "\n",
    "    y = (loc - x) / scale\n",
    "    out = np.ones_like(x, dtype=float)\n",
    "    mask = y >= 0\n",
    "    out[mask] = np.exp(-(y[mask] ** c))\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_max_ppf(p, c, loc=0.0, scale=1.0):\n",
    "    '''Quantile function (inverse CDF).'''\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "\n",
    "    if c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"c must be > 0 and scale must be > 0\")\n",
    "    if np.any((p < 0) | (p > 1)):\n",
    "        raise ValueError(\"p must be in [0, 1]\")\n",
    "\n",
    "    out = np.full_like(p, np.nan, dtype=float)\n",
    "    out[p == 0] = -np.inf\n",
    "    out[p == 1] = loc\n",
    "\n",
    "    mask = (p > 0) & (p < 1)\n",
    "    out[mask] = loc - scale * (-np.log(p[mask])) ** (1.0 / c)\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_max_entropy(c, scale=1.0):\n",
    "    '''Differential entropy H(X) (independent of loc).'''\n",
    "    c = float(c)\n",
    "    scale = float(scale)\n",
    "    if c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"c must be > 0 and scale must be > 0\")\n",
    "\n",
    "    return 1.0 + np.log(scale / c) + np.euler_gamma * (1.0 - 1.0 / c)\n",
    "\n",
    "\n",
    "def weibull_max_moments(c, loc=0.0, scale=1.0):\n",
    "    '''Return (mean, variance, skewness, excess_kurtosis).'''\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "    if c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"c must be > 0 and scale must be > 0\")\n",
    "\n",
    "    g1 = gamma(1.0 + 1.0 / c)\n",
    "    g2 = gamma(1.0 + 2.0 / c)\n",
    "    g3 = gamma(1.0 + 3.0 / c)\n",
    "    g4 = gamma(1.0 + 4.0 / c)\n",
    "\n",
    "    mean = loc - scale * g1\n",
    "    var = scale**2 * (g2 - g1**2)\n",
    "\n",
    "    mu3 = g3 - 3 * g2 * g1 + 2 * g1**3\n",
    "    skew = -mu3 / (g2 - g1**2) ** 1.5\n",
    "\n",
    "    mu4 = g4 - 4 * g3 * g1 + 6 * g2 * g1**2 - 3 * g1**4\n",
    "    excess_kurt = mu4 / (g2 - g1**2) ** 2 - 3\n",
    "\n",
    "    return mean, var, skew, excess_kurt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dacdad5",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "A convenient trick is to work with\n",
    "\n",
    "$$\n",
    "Y = \\mu - X \\ge 0.\n",
    "$$\n",
    "\n",
    "Then $Y$ is a **standard Weibull (minimum)** random variable with the same shape $c$ and scale $\\lambda$.\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "\n",
    "Let $g_r = \\Gamma\\!\\left(1+\\frac{r}{c}\\right)$.\n",
    "\n",
    "| Quantity | Value |\n",
    "|---|---|\n",
    "| Mean | $\\mathbb{E}[X]=\\mu-\\lambda\\,g_1$ |\n",
    "| Variance | $\\mathrm{Var}(X)=\\lambda^2\\,(g_2-g_1^2)$ |\n",
    "| Skewness | $\\gamma_1(X)=-\\dfrac{g_3-3g_2g_1+2g_1^3}{(g_2-g_1^2)^{3/2}}$ |\n",
    "| Excess kurtosis | $\\gamma_2(X)=\\dfrac{g_4-4g_3g_1+6g_2g_1^2-3g_1^4}{(g_2-g_1^2)^2}-3$ |\n",
    "\n",
    "Because $X$ is an affine transform of $Y$, all moments exist for any $c>0$.\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "There is generally **no simple closed-form** MGF/CF for arbitrary $c$, but we can still write them cleanly.\n",
    "\n",
    "Write $X=\\mu-\\lambda Z$ where $Z\\ge 0$ has the **standard** Weibull PDF $c z^{c-1}e^{-z^c}$.\n",
    "\n",
    "- The **MGF** (when finite) is a Laplace transform:\n",
    "\n",
    "$$\n",
    "M_X(t)=\\mathbb{E}[e^{tX}]=e^{\\mu t}\\,\\mathbb{E}[e^{-\\lambda t Z}],\n",
    "$$\n",
    "\n",
    "which always exists for $t\\ge 0$ (since $X\\le\\mu$ implies $e^{tX}\\le e^{\\mu t}$).\n",
    "\n",
    "- The **existence for $t<0$** depends on how fast the left tail decays:\n",
    "  - $c>1$: MGF exists for all real $t$.\n",
    "  - $c=1$: exists only for $t>-1/\\lambda$ (shifted negative exponential).\n",
    "  - $0<c<1$: diverges for any $t<0$.\n",
    "\n",
    "A useful series expansion around $t=0$ (when it converges) comes from moments:\n",
    "\n",
    "$$\n",
    "M_X(t)=e^{\\mu t}\\sum_{n=0}^{\\infty}\\frac{(-\\lambda t)^n}{n!}\\,\\Gamma\\!\\left(1+\\frac{n}{c}\\right).\n",
    "$$\n",
    "\n",
    "The **characteristic function** always exists:\n",
    "\n",
    "$$\n",
    "\\varphi_X(\\omega)=\\mathbb{E}[e^{i\\omega X}]=e^{i\\mu\\omega}\\sum_{n=0}^{\\infty}\\frac{(-i\\lambda\\omega)^n}{n!}\\,\\Gamma\\!\\left(1+\\frac{n}{c}\\right).\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy (independent of $\\mu$) is\n",
    "\n",
    "$$\n",
    "H(X)=1+\\ln\\Big(\\frac{\\lambda}{c}\\Big)+\\gamma\\Big(1-\\frac{1}{c}\\Big),\n",
    "$$\n",
    "\n",
    "where $\\gamma\\approx 0.57721$ is the Euler–Mascheroni constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc24d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ex, loc_ex, scale_ex = 1.7, 2.0, 1.5\n",
    "\n",
    "mean_th, var_th, skew_th, exkurt_th = weibull_max_moments(c_ex, loc=loc_ex, scale=scale_ex)\n",
    "entropy_th = weibull_max_entropy(c_ex, scale=scale_ex)\n",
    "\n",
    "dist = stats.weibull_max(c_ex, loc=loc_ex, scale=scale_ex)\n",
    "mean_s, var_s, skew_s, exkurt_s = dist.stats(moments=\"mvsk\")\n",
    "entropy_s = dist.entropy()\n",
    "\n",
    "{\n",
    "    \"mean (theory)\": mean_th,\n",
    "    \"mean (scipy)\": float(mean_s),\n",
    "    \"var (theory)\": var_th,\n",
    "    \"var (scipy)\": float(var_s),\n",
    "    \"skew (theory)\": skew_th,\n",
    "    \"skew (scipy)\": float(skew_s),\n",
    "    \"excess_kurtosis (theory)\": exkurt_th,\n",
    "    \"excess_kurtosis (scipy)\": float(exkurt_s),\n",
    "    \"entropy (theory)\": entropy_th,\n",
    "    \"entropy (scipy)\": float(entropy_s),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e811087",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "We’ll use SciPy’s parameters: `c` (shape), `loc=\\mu` (upper endpoint), `scale=\\lambda`.\n",
    "\n",
    "### Location $\\mu$ (`loc`)\n",
    "\n",
    "- Sets the **finite upper endpoint**.\n",
    "- The support is $(-\\infty,\\mu]$.\n",
    "- Shifting $\\mu$ shifts the entire distribution.\n",
    "\n",
    "### Scale $\\lambda$ (`scale`)\n",
    "\n",
    "- Controls typical **distance below the endpoint**.\n",
    "- In the representation $X=\\mu-Y$, we have $Y\\sim\\text{Weibull}(c,\\lambda)$.\n",
    "- Larger $\\lambda$ produces heavier spread to the left (more mass far below $\\mu$).\n",
    "\n",
    "### Shape $c$ (`c`)\n",
    "\n",
    "- Controls **tail decay** and behavior near the endpoint $\\mu$.\n",
    "- For $0<c<1$, the density **blows up** at $x\\uparrow\\mu$ (very sharp pile-up near the cap) and the left tail is relatively heavy.\n",
    "- For $c=1$, $\\mu-X$ is **exponential** (memoryless in the distance-to-cap).\n",
    "- For $c>1$, the density goes to **0** at $x=\\mu$ and the left tail decays faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9653e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape changes (standardized loc=0, scale=1)\n",
    "c_values = [0.6, 1.0, 1.8, 4.0]\n",
    "x = np.linspace(-6, -1e-6, 800)\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in c_values:\n",
    "    fig.add_trace(go.Scatter(x=x, y=weibull_max_pdf(x, c), mode=\"lines\", name=f\"c={c}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"weibull_max PDF shape for different c (loc=0, scale=1)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"pdf(x)\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49d3fd",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "A standard and very reusable move is to transform to the *distance from the endpoint*:\n",
    "\n",
    "$$\n",
    "Y = \\mu - X\\quad\\Rightarrow\\quad Y\\ge 0.\n",
    "$$\n",
    "\n",
    "Then $Y$ has the usual Weibull (minimum) PDF\n",
    "\n",
    "$$\n",
    "f_Y(y)=\\frac{c}{\\lambda}\\left(\\frac{y}{\\lambda}\\right)^{c-1}\\exp\\!\\left(-\\left(\\frac{y}{\\lambda}\\right)^c\\right),\\qquad y\\ge 0.\n",
    "$$\n",
    "\n",
    "### Expectation\n",
    "\n",
    "Compute $\\mathbb{E}[Y]$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Y]=\\int_0^\\infty y\\,f_Y(y)\\,dy.\n",
    "$$\n",
    "\n",
    "Substitute $u=(y/\\lambda)^c$ so $y=\\lambda u^{1/c}$ and $dy=\\lambda\\,\\frac{1}{c}u^{1/c-1}du$.\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Y]=\\lambda\\int_0^\\infty u^{1/c}e^{-u}\\,du\n",
    "=\\lambda\\,\\Gamma\\!\\left(1+\\frac{1}{c}\\right).\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X]=\\mu-\\mathbb{E}[Y]=\\mu-\\lambda\\,\\Gamma\\!\\left(1+\\frac{1}{c}\\right).\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Y^2]=\\lambda^2\\,\\Gamma\\!\\left(1+\\frac{2}{c}\\right).\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X)=\\mathrm{Var}(Y)=\\lambda^2\\left(\\Gamma\\!\\left(1+\\frac{2}{c}\\right)-\\Gamma\\!\\left(1+\\frac{1}{c}\\right)^2\\right).\n",
    "$$\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "For i.i.d. data $x_1,\\dots,x_n$ with $x_i\\le \\mu$, the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(c,\\lambda,\\mu) = \\sum_{i=1}^n \\log f(x_i)\n",
    "= n\\log\\frac{c}{\\lambda} + (c-1)\\sum_{i=1}^n\\log\\left(\\frac{\\mu-x_i}{\\lambda}\\right)\n",
    "-\\sum_{i=1}^n\\left(\\frac{\\mu-x_i}{\\lambda}\\right)^c.\n",
    "$$\n",
    "\n",
    "A practical consequence: **if $\\mu$ is known**, you can fit $(c,\\lambda)$ by fitting a standard Weibull to $y_i=\\mu-x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_max_loglik(x, c, loc=0.0, scale=1.0):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return float(np.sum(weibull_max_logpdf(x, c, loc=loc, scale=scale)))\n",
    "\n",
    "\n",
    "# Sanity check: log-likelihood matches SciPy's logpdf sum\n",
    "c_true, loc_true, scale_true = 1.8, 2.0, 1.2\n",
    "n = 5000\n",
    "\n",
    "x_synth = stats.weibull_max(c_true, loc=loc_true, scale=scale_true).rvs(\n",
    "    size=n, random_state=rng\n",
    ")\n",
    "ll_manual = weibull_max_loglik(x_synth, c_true, loc=loc_true, scale=scale_true)\n",
    "ll_scipy = float(\n",
    "    np.sum(stats.weibull_max(c_true, loc=loc_true, scale=scale_true).logpdf(x_synth))\n",
    ")\n",
    "ll_manual, ll_scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac3ef9",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation (NumPy-only)\n",
    "\n",
    "Use the inverse CDF:\n",
    "\n",
    "$$\n",
    "X = \\mu - \\lambda\\,(-\\ln U)^{1/c},\\qquad U\\sim\\mathrm{Uniform}(0,1).\n",
    "$$\n",
    "\n",
    "A numerically convenient version uses the fact that $E=-\\ln U\\sim\\mathrm{Exp}(1)$:\n",
    "\n",
    "$$\n",
    "X = \\mu - \\lambda\\,E^{1/c},\\qquad E\\sim\\mathrm{Exp}(1).\n",
    "$$\n",
    "\n",
    "This is fast, vectorized, and uses only NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_max_rvs_numpy(c, loc=0.0, scale=1.0, size=1, rng=None):\n",
    "    '''NumPy-only sampler via E ~ Exp(1), X = loc - scale * E**(1/c).'''\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "    if c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"c must be > 0 and scale must be > 0\")\n",
    "\n",
    "    e = rng.exponential(scale=1.0, size=size)  # Exp(1)\n",
    "    return loc - scale * e ** (1.0 / c)\n",
    "\n",
    "\n",
    "# Monte Carlo check\n",
    "c_mc, loc_mc, scale_mc = 0.7, 1.5, 0.8\n",
    "n_mc = 200_000\n",
    "\n",
    "x_mc = weibull_max_rvs_numpy(c_mc, loc=loc_mc, scale=scale_mc, size=n_mc, rng=rng)\n",
    "\n",
    "mean_th, var_th, *_ = weibull_max_moments(c_mc, loc=loc_mc, scale=scale_mc)\n",
    "mean_mc = float(np.mean(x_mc))\n",
    "var_mc = float(np.var(x_mc, ddof=0))\n",
    "\n",
    "{\n",
    "    \"mean (MC)\": mean_mc,\n",
    "    \"mean (theory)\": mean_th,\n",
    "    \"var (MC)\": var_mc,\n",
    "    \"var (theory)\": var_th,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4fb74b",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "\n",
    "- the **PDF** and a Monte Carlo histogram\n",
    "- the **CDF** and an empirical CDF\n",
    "\n",
    "using the NumPy-only sampler from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb401d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(samples):\n",
    "    x = np.sort(np.asarray(samples))\n",
    "    y = np.arange(1, x.size + 1) / x.size\n",
    "    return x, y\n",
    "\n",
    "\n",
    "c_viz, loc_viz, scale_viz = 1.2, 2.0, 1.0\n",
    "n_viz = 80_000\n",
    "\n",
    "samples = weibull_max_rvs_numpy(c_viz, loc=loc_viz, scale=scale_viz, size=n_viz, rng=rng)\n",
    "\n",
    "# pick a finite plotting window using quantiles (support extends to -inf)\n",
    "x_lo = float(np.quantile(samples, 0.001))\n",
    "x_hi = float(np.quantile(samples, 0.999))\n",
    "x_grid = np.linspace(x_lo, min(x_hi, loc_viz - 1e-6), 700)\n",
    "\n",
    "pdf_grid = weibull_max_pdf(x_grid, c_viz, loc=loc_viz, scale=scale_viz)\n",
    "cdf_grid = weibull_max_cdf(x_grid, c_viz, loc=loc_viz, scale=scale_viz)\n",
    "\n",
    "# PDF + histogram\n",
    "fig_pdf = go.Figure()\n",
    "fig_pdf.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples,\n",
    "        histnorm=\"probability density\",\n",
    "        nbinsx=80,\n",
    "        name=\"samples\",\n",
    "        opacity=0.55,\n",
    "    )\n",
    ")\n",
    "fig_pdf.add_trace(go.Scatter(x=x_grid, y=pdf_grid, mode=\"lines\", name=\"theory pdf\"))\n",
    "fig_pdf.update_layout(\n",
    "    title=\"weibull_max: histogram vs PDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig_pdf.show()\n",
    "\n",
    "# CDF + ECDF\n",
    "x_ecdf, y_ecdf = ecdf(samples)\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=x_grid, y=cdf_grid, mode=\"lines\", name=\"theory cdf\"))\n",
    "fig_cdf.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_ecdf[::200],\n",
    "        y=y_ecdf[::200],\n",
    "        mode=\"markers\",\n",
    "        name=\"empirical cdf\",\n",
    "        marker=dict(size=4),\n",
    "    )\n",
    ")\n",
    "fig_cdf.update_layout(\n",
    "    title=\"weibull_max: empirical CDF vs CDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x)\",\n",
    ")\n",
    "fig_cdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed12808",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.weibull_max`)\n",
    "\n",
    "SciPy’s distribution is `scipy.stats.weibull_max`.\n",
    "\n",
    "- Shape is passed as `c`.\n",
    "- Shift and scale are handled by `loc` and `scale`.\n",
    "\n",
    "Mapping to our notation:\n",
    "\n",
    "$$\n",
    "X\\sim\\texttt{weibull\\_max}(c,\\text{loc}=\\mu,\\text{scale}=\\lambda).\n",
    "$$\n",
    "\n",
    "We’ll use it to evaluate `pdf`, `cdf`, sample via `rvs`, and fit via `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b471d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_true, loc_true, scale_true = 1.5, 3.0, 0.9\n",
    "\n",
    "dist = stats.weibull_max(c_true, loc=loc_true, scale=scale_true)\n",
    "\n",
    "x = np.linspace(loc_true - 5 * scale_true, loc_true - 1e-6, 500)\n",
    "pdf = dist.pdf(x)\n",
    "cdf = dist.cdf(x)\n",
    "\n",
    "samples_scipy = dist.rvs(size=5000, random_state=rng)\n",
    "\n",
    "# MLE fit (free loc)\n",
    "c_hat, loc_hat, scale_hat = stats.weibull_max.fit(samples_scipy)\n",
    "\n",
    "# If you know the upper endpoint in advance, fixing loc can be much more stable.\n",
    "c_hat_fix, loc_hat_fix, scale_hat_fix = stats.weibull_max.fit(samples_scipy, floc=loc_true)\n",
    "\n",
    "{\n",
    "    \"true\": (c_true, loc_true, scale_true),\n",
    "    \"fit (free loc)\": (c_hat, loc_hat, scale_hat),\n",
    "    \"fit (fixed loc=true loc)\": (c_hat_fix, loc_hat_fix, scale_hat_fix),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9570cf",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### Hypothesis testing\n",
    "\n",
    "A common question: “Do these data plausibly come from a `weibull_max` model?”\n",
    "\n",
    "- If parameters are known a priori, a one-sample **KS test** against the specified CDF is straightforward.\n",
    "- If parameters are estimated from the data, the standard KS p-value is not calibrated; a **parametric bootstrap** is a typical fix.\n",
    "\n",
    "### Bayesian modeling\n",
    "\n",
    "In a Bayesian setting, you put priors on $(c,\\lambda,\\mu)$ and update with the likelihood. A simple educational starting point is a **grid posterior** over $(c,\\lambda)$ when $\\mu$ is known.\n",
    "\n",
    "### Generative modeling\n",
    "\n",
    "Because it has a hard upper endpoint, `weibull_max` is useful as a building block in generative stories where observations are **bounded above**:\n",
    "\n",
    "- one-sided error models ($\\varepsilon\\le 0$)\n",
    "- simulation of capped phenomena with tunable tail thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) KS test + parametric bootstrap (illustration)\n",
    "\n",
    "c0, loc0, scale0 = 1.4, 2.0, 1.0\n",
    "n0 = 400\n",
    "\n",
    "d0 = stats.weibull_max(c0, loc=loc0, scale=scale0)\n",
    "data0 = d0.rvs(size=n0, random_state=rng)\n",
    "\n",
    "# KS test when the null distribution is fully specified\n",
    "ks_stat, ks_p = stats.kstest(data0, d0.cdf)\n",
    "\n",
    "# Now pretend loc is known but (c, scale) are estimated and calibrate with a bootstrap\n",
    "c_hat, loc_hat, scale_hat = stats.weibull_max.fit(data0, floc=loc0)\n",
    "d_hat = stats.weibull_max(c_hat, loc=loc0, scale=scale_hat)\n",
    "ks_obs, _ = stats.kstest(data0, d_hat.cdf)\n",
    "\n",
    "B = 200\n",
    "ks_boot = np.empty(B)\n",
    "for b in range(B):\n",
    "    sim = d_hat.rvs(size=n0, random_state=rng)\n",
    "    c_b, _, scale_b = stats.weibull_max.fit(sim, floc=loc0)\n",
    "    d_b = stats.weibull_max(c_b, loc=loc0, scale=scale_b)\n",
    "    ks_boot[b] = stats.kstest(sim, d_b.cdf).statistic\n",
    "\n",
    "p_boot = float(np.mean(ks_boot >= ks_obs))\n",
    "\n",
    "{\n",
    "    \"KS (known params) statistic\": float(ks_stat),\n",
    "    \"KS (known params) pvalue\": float(ks_p),\n",
    "    \"KS (fit params) statistic\": float(ks_obs),\n",
    "    \"bootstrap pvalue (approx)\": p_boot,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B) Simple grid posterior for (c, scale) when loc is known\n",
    "\n",
    "loc_known = 2.0\n",
    "x = data0\n",
    "\n",
    "y = loc_known - x\n",
    "if np.any(y < 0):\n",
    "    raise ValueError(\"Data must satisfy x <= loc_known\")\n",
    "\n",
    "\n",
    "# Log-likelihood for y ~ Weibull_min(c, scale)\n",
    "def weibull_min_loglik(y, c, scale):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    c = float(c)\n",
    "    scale = float(scale)\n",
    "    if c <= 0 or scale <= 0:\n",
    "        return -np.inf\n",
    "    if np.any(y < 0):\n",
    "        return -np.inf\n",
    "\n",
    "    # sum log(c/scale) + (c-1) log(y/scale) - (y/scale)^c\n",
    "    # handle y=0 safely (log 0); in continuous data this is usually not hit.\n",
    "    logy = np.where(y > 0, np.log(y), -np.inf)\n",
    "    return float(\n",
    "        y.size * (np.log(c) - np.log(scale))\n",
    "        + (c - 1) * np.sum(logy - np.log(scale))\n",
    "        - np.sum((y / scale) ** c)\n",
    "    )\n",
    "\n",
    "\n",
    "# Priors: independent log-normal on c and scale (mildly informative)\n",
    "def lognormal_logpdf(z, mu, sigma):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    if np.any(z <= 0):\n",
    "        return -np.inf\n",
    "    return float(\n",
    "        -np.sum(np.log(z))\n",
    "        - z.size * np.log(sigma * np.sqrt(2 * np.pi))\n",
    "        - 0.5 * np.sum(((np.log(z) - mu) / sigma) ** 2)\n",
    "    )\n",
    "\n",
    "\n",
    "c_grid = np.linspace(0.3, 4.0, 220)\n",
    "scale_grid = np.linspace(0.2, 2.5, 220)\n",
    "\n",
    "log_post = np.empty((c_grid.size, scale_grid.size))\n",
    "for i, c_val in enumerate(c_grid):\n",
    "    for j, s_val in enumerate(scale_grid):\n",
    "        ll = weibull_min_loglik(y, c_val, s_val)\n",
    "        lp = lognormal_logpdf(np.array([c_val]), mu=np.log(1.2), sigma=0.7) + lognormal_logpdf(\n",
    "            np.array([s_val]), mu=np.log(1.0), sigma=0.7\n",
    "        )\n",
    "        log_post[i, j] = ll + lp\n",
    "\n",
    "# Stabilize and normalize\n",
    "log_post -= np.max(log_post)\n",
    "post = np.exp(log_post)\n",
    "post /= np.sum(post)\n",
    "\n",
    "# MAP estimate\n",
    "idx = np.unravel_index(np.argmax(post), post.shape)\n",
    "c_map = float(c_grid[idx[0]])\n",
    "scale_map = float(scale_grid[idx[1]])\n",
    "\n",
    "# Posterior means (grid approximation)\n",
    "c_mean = float(np.sum(c_grid[:, None] * post))\n",
    "scale_mean = float(np.sum(scale_grid[None, :] * post))\n",
    "\n",
    "fig = px.imshow(\n",
    "    post,\n",
    "    x=scale_grid,\n",
    "    y=c_grid,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    labels={\"x\": \"scale\", \"y\": \"c\", \"color\": \"posterior mass\"},\n",
    "    title=\"Grid posterior over (c, scale) with known loc\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"MAP (c, scale)\": (c_map, scale_map),\n",
    "    \"posterior mean (c, scale)\": (c_mean, scale_mean),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Generative example: one-sided errors bounded above\n",
    "\n",
    "x = np.linspace(0, 10, 250)\n",
    "true_curve = 2.0 + np.sin(x)\n",
    "\n",
    "# noise <= 0 with tunable left tail\n",
    "noise = weibull_max_rvs_numpy(c=1.3, loc=0.0, scale=0.35, size=x.size, rng=rng)\n",
    "obs = true_curve + noise\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=true_curve, mode=\"lines\", name=\"true curve\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=obs,\n",
    "        mode=\"markers\",\n",
    "        name=\"observations\",\n",
    "        marker=dict(size=5, opacity=0.7),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"One-sided (upper-bounded) noise via weibull_max\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737afcad",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter constraints**: `c>0` and `scale>0` are required. The support is $x\\le\\mu$; if any data exceed $\\mu$, the likelihood is zero.\n",
    "- **Confusing `weibull_min` vs `weibull_max`**: `weibull_min` lives on $[0,\\infty)$ (after shifting), while `weibull_max` has a **finite upper endpoint**.\n",
    "- **Endpoint behavior for $c<1$**: the PDF diverges as $x\\uparrow\\mu$. This is mathematically fine (integrable), but can look alarming in plots and can create very large values if you evaluate exactly at $x=\\mu$.\n",
    "- **Numerical under/overflow**: for very negative $x$, $((\\mu-x)/\\lambda)^c$ can be huge; prefer `logpdf` when doing inference and use quantiles to choose plotting ranges.\n",
    "- **Fitting `loc`**: when `loc` is free, MLE can be sensitive. If you know the physical cap/endpoint, fixing `loc` (via `floc=...`) can stabilize estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9c498",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `weibull_max` is a continuous distribution supported on $(-\\infty,\\mu]$ with a finite upper endpoint.\n",
    "- The transform $Y=\\mu-X$ turns it into a standard Weibull (minimum) model on $[0,\\infty)$.\n",
    "- Mean/variance and higher standardized moments are cleanly expressed using Gamma functions $\\Gamma(1+r/c)$.\n",
    "- Sampling is easy via inverse CDF: $X=\\mu-\\lambda(-\\ln U)^{1/c}$.\n",
    "- SciPy’s `scipy.stats.weibull_max` provides `pdf`, `cdf`, `rvs`, and `fit` (often more stable if `loc` is fixed when known)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}