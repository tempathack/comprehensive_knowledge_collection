{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932392f8",
   "metadata": {},
   "source": [
    "# Student's t distribution (`t`)\n",
    "\n",
    "The **Student's t distribution** is a continuous distribution on \\(\\mathbb{R}\\) that looks normal near the center but has **heavier tails**.\n",
    "\n",
    "It shows up in two complementary ways:\n",
    "\n",
    "- as the sampling distribution of the **t-statistic** when the variance is unknown\n",
    "- as a practical **robust noise model** (a heavy-tailed alternative to a Gaussian)\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end, you should be able to:\n",
    "\n",
    "- write down the PDF/CDF and understand the **role of degrees of freedom**\n",
    "- explain where the t distribution comes from (normal + chi-square)\n",
    "- compute key properties (when moments exist, entropy, characteristic function)\n",
    "- sample from it using **NumPy only**\n",
    "- use `scipy.stats.t` for evaluation, simulation, and fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize, special, stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t as t_dist\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "VERSIONS = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"numpy\": np.__version__,\n",
    "    \"scipy\": scipy.__version__,\n",
    "    \"plotly\": plotly.__version__,\n",
    "}\n",
    "\n",
    "VERSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65b385",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `t` (Student's t distribution; SciPy: `scipy.stats.t`)\n",
    "- **Type**: **Continuous**\n",
    "- **Support (standard form)**: \\(x \\in (-\\infty, \\infty)\\)\n",
    "- **Parameter space (standard form)**: degrees of freedom \\(\\nu > 0\\)\n",
    "- **SciPy location/scale**: `loc \\in \\mathbb{R}`, `scale > 0` with\n",
    "\n",
    "\\[\n",
    "X = \\mathrm{loc} + \\mathrm{scale}\\,T, \\qquad T \\sim t_{\\nu}.\n",
    "\\]\n",
    "\n",
    "Unless stated otherwise, we work with the **standard** t distribution (`loc=0`, `scale=1`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d4b1f",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "A t distribution behaves like a normal distribution near its center, but its tails decay only polynomially.\n",
    "This makes it a good model for **occasional large deviations**.\n",
    "\n",
    "A canonical generative story is:\n",
    "\n",
    "1. Draw a standard normal \\(Z \\sim \\mathcal{N}(0,1)\\)\n",
    "2. Draw an independent chi-square \\(V \\sim \\chi^2_{\\nu}\\)\n",
    "3. Form the ratio\n",
    "\n",
    "\\[\n",
    "T = \\frac{Z}{\\sqrt{V/\\nu}} \\sim t_{\\nu}.\n",
    "\\]\n",
    "\n",
    "The random denominator inflates or deflates the normal draw, producing heavier tails.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Small-sample inference**: the t-statistic \\((\\bar X-\\mu_0)/(S/\\sqrt{n})\\) follows a t distribution under normality.\n",
    "- **Robust modeling**: t likelihoods reduce the influence of outliers (common in finance, sensor data, web metrics).\n",
    "- **Bayesian modeling**: t priors/likelihoods appear as **normal–gamma mixtures**; posterior predictive distributions are often t.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- As \\(\\nu \\to \\infty\\), \\(t_{\\nu}\\) converges to \\(\\mathcal{N}(0,1)\\).\n",
    "- \\(t_1\\) is exactly the **standard Cauchy** distribution.\n",
    "- If \\(T \\sim t_{\\nu}\\), then \\(T^2 \\sim F_{1,\\nu}\\) (F distribution).\n",
    "- The t distribution is a **scale mixture of normals**: if \\(\\lambda \\sim \\mathrm{Gamma}(\\nu/2,\\ \\nu/2)\\) (shape–rate), then\n",
    "\n",
    "\\[\n",
    "T\\mid\\lambda \\sim \\mathcal{N}(0, 1/\\lambda),\n",
    "\\quad\\Rightarrow\\quad\n",
    "T \\sim t_{\\nu}.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba66863",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(T \\sim t_{\\nu}\\) be a standard Student's t random variable with degrees of freedom \\(\\nu>0\\).\n",
    "\n",
    "### PDF\n",
    "\n",
    "\\[\n",
    " f(t;\\nu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\left(\\frac{\\nu}{2}\\right)}\n",
    " \\left(1 + \\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\n",
    " \\qquad t \\in \\mathbb{R}.\n",
    "\\]\n",
    "\n",
    "For the location–scale family \\(X = \\mathrm{loc} + \\mathrm{scale}\\,T\\) with \\(\\mathrm{scale}>0\\), the density is\n",
    "\n",
    "\\[\n",
    " f_X(x;\\nu,\\mathrm{loc},\\mathrm{scale})\n",
    " = \\frac{1}{\\mathrm{scale}}\\,f\\!\\left(\\frac{x-\\mathrm{loc}}{\\mathrm{scale}};\\nu\\right).\n",
    "\\]\n",
    "\n",
    "### CDF\n",
    "\n",
    "The CDF can be written using the **regularized incomplete beta function** \\(I_x(a,b)\\).\n",
    "Let\n",
    "\n",
    "\\[\n",
    " w(t) = \\frac{\\nu}{\\nu + t^2} \\in (0,1].\n",
    "\\]\n",
    "\n",
    "Then for \\(t\\ge 0\\):\n",
    "\n",
    "\\[\n",
    "F(t;\\nu) = 1 - \\tfrac{1}{2} I_{w(t)}\\!\\left(\\tfrac{\\nu}{2}, \\tfrac{1}{2}\\right),\n",
    "\\]\n",
    "\n",
    "and by symmetry for \\(t<0\\):\n",
    "\n",
    "\\[\n",
    "F(t;\\nu) = \\tfrac{1}{2} I_{w(t)}\\!\\left(\\tfrac{\\nu}{2}, \\tfrac{1}{2}\\right).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72639a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_df_loc_scale(df: float, loc: float, scale: float) -> tuple[float, float, float]:\n",
    "    df = float(df)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "    if not (df > 0.0):\n",
    "        raise ValueError(\"df (ν) must be > 0\")\n",
    "    if not (scale > 0.0):\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "    return df, loc, scale\n",
    "\n",
    "\n",
    "def t_logpdf(x: np.ndarray, df: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    '''Log-PDF of the Student t distribution with df, loc, scale.\n",
    "\n",
    "    Uses stable log-gamma expressions.\n",
    "    '''\n",
    "\n",
    "    df, loc, scale = _validate_df_loc_scale(df, loc, scale)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    z = (x - loc) / scale\n",
    "\n",
    "    log_norm = (\n",
    "        special.gammaln((df + 1.0) / 2.0)\n",
    "        - special.gammaln(df / 2.0)\n",
    "        - 0.5 * np.log(df * np.pi)\n",
    "        - np.log(scale)\n",
    "    )\n",
    "\n",
    "    return log_norm - ((df + 1.0) / 2.0) * np.log1p((z * z) / df)\n",
    "\n",
    "\n",
    "def t_pdf(x: np.ndarray, df: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    return np.exp(t_logpdf(x, df=df, loc=loc, scale=scale))\n",
    "\n",
    "\n",
    "def t_cdf(x: np.ndarray, df: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    '''CDF via the regularized incomplete beta representation.'''\n",
    "\n",
    "    df, loc, scale = _validate_df_loc_scale(df, loc, scale)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    z = (x - loc) / scale\n",
    "\n",
    "    w = df / (df + z * z)\n",
    "    ib = special.betainc(df / 2.0, 0.5, w)\n",
    "\n",
    "    return np.where(z >= 0.0, 1.0 - 0.5 * ib, 0.5 * ib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30378a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks against SciPy\n",
    "\n",
    "df = 7.5\n",
    "loc = -0.3\n",
    "scale = 1.7\n",
    "\n",
    "x = np.linspace(-6, 6, 501)\n",
    "\n",
    "assert np.allclose(t_pdf(x, df, loc=loc, scale=scale), t_dist.pdf(x, df, loc=loc, scale=scale), rtol=1e-10, atol=0)\n",
    "assert np.allclose(t_cdf(x, df, loc=loc, scale=scale), t_dist.cdf(x, df, loc=loc, scale=scale), rtol=1e-10, atol=0)\n",
    "\n",
    "\"ok\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f9a96",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Mean / variance / skewness / kurtosis\n",
    "\n",
    "Let \\(T\\sim t_{\\nu}\\) (standard, centered).\n",
    "\n",
    "- **Mean**: \\(\\mathbb{E}[T]=0\\) if \\(\\nu>1\\); undefined for \\(\\nu\\le 1\\).\n",
    "- **Variance**: \\(\\mathrm{Var}(T)=\\dfrac{\\nu}{\\nu-2}\\) if \\(\\nu>2\\); infinite for \\(1<\\nu\\le 2\\).\n",
    "- **Skewness**: 0 if \\(\\nu>3\\); undefined otherwise.\n",
    "- **Excess kurtosis**: \\(\\dfrac{6}{\\nu-4}\\) if \\(\\nu>4\\); infinite for \\(2<\\nu\\le 4\\).\n",
    "\n",
    "For the location–scale family \\(X = \\mathrm{loc} + \\mathrm{scale}\\,T\\), when the mean/variance exist:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X]=\\mathrm{loc},\n",
    "\\qquad\n",
    "\\mathrm{Var}(X)=\\mathrm{scale}^2\\,\\frac{\\nu}{\\nu-2}.\n",
    "\\]\n",
    "\n",
    "### MGF and characteristic function\n",
    "\n",
    "- The **moment generating function** \\(M_T(t)=\\mathbb{E}[e^{tT}]\\) **does not exist** (is infinite) for any nonzero \\(t\\).\n",
    "  Intuition: t tails behave like \\(|x|^{-(\\nu+1)}\\), and \\(e^{t x}\\) dominates any polynomial as \\(x\\to\\infty\\).\n",
    "\n",
    "- The **characteristic function** exists for all \\(t\\) and can be written using the modified Bessel function \\(K_{\\alpha}\\):\n",
    "\n",
    "\\[\n",
    "\\varphi_T(t)\n",
    "= \\mathbb{E}[e^{i t T}]\n",
    "= \\frac{(\\sqrt{\\nu}|t|)^{\\nu/2}\\,K_{\\nu/2}(\\sqrt{\\nu}|t|)}{2^{\\nu/2-1}\\,\\Gamma(\\nu/2)}.\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy of the standard t distribution is\n",
    "\n",
    "\\[\n",
    "H(T) = \\log\\bigl(\\sqrt{\\nu}\\,B(\\nu/2, 1/2)\\bigr)\n",
    "+ \\frac{\\nu+1}{2}\\left[\\psi\\left(\\frac{\\nu+1}{2}\\right) - \\psi\\left(\\frac{\\nu}{2}\\right)\\right],\n",
    "\\]\n",
    "\n",
    "where \\(B\\) is the beta function and \\(\\psi\\) is the digamma function.\n",
    "For \\(X=\\mathrm{loc}+\\mathrm{scale}\\,T\\),\n",
    "\n",
    "\\[\n",
    "H(X)=H(T)+\\log(\\mathrm{scale}).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe90442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_moments(df: float, loc: float = 0.0, scale: float = 1.0) -> dict:\n",
    "    df, loc, scale = _validate_df_loc_scale(df, loc, scale)\n",
    "\n",
    "    mean = np.nan\n",
    "    var = np.nan\n",
    "    skew = np.nan\n",
    "    excess_kurt = np.nan\n",
    "\n",
    "    if df > 1:\n",
    "        mean = loc\n",
    "    if df > 2:\n",
    "        var = (scale * scale) * df / (df - 2.0)\n",
    "    elif df > 1:\n",
    "        var = np.inf\n",
    "\n",
    "    if df > 3:\n",
    "        skew = 0.0\n",
    "\n",
    "    if df > 4:\n",
    "        excess_kurt = 6.0 / (df - 4.0)\n",
    "    elif df > 2:\n",
    "        excess_kurt = np.inf\n",
    "\n",
    "    return {\n",
    "        \"df\": df,\n",
    "        \"loc\": loc,\n",
    "        \"scale\": scale,\n",
    "        \"mean\": mean,\n",
    "        \"var\": var,\n",
    "        \"skew\": skew,\n",
    "        \"excess_kurt\": excess_kurt,\n",
    "        \"kurt\": excess_kurt + 3.0 if np.isfinite(excess_kurt) else excess_kurt,\n",
    "    }\n",
    "\n",
    "\n",
    "def t_entropy(df: float, scale: float = 1.0) -> float:\n",
    "    df, _, scale = _validate_df_loc_scale(df, 0.0, scale)\n",
    "    a = df / 2.0\n",
    "    term1 = 0.5 * np.log(df) + special.betaln(a, 0.5)\n",
    "    term2 = (df + 1.0) / 2.0 * (special.digamma((df + 1.0) / 2.0) - special.digamma(a))\n",
    "    return float(term1 + term2 + np.log(scale))\n",
    "\n",
    "\n",
    "def t_charfun(t: np.ndarray, df: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    '''Characteristic function of Student-t (df, loc, scale).'''\n",
    "\n",
    "    df, loc, scale = _validate_df_loc_scale(df, loc, scale)\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    a = df / 2.0\n",
    "    x = np.sqrt(df) * np.abs(scale * t)\n",
    "\n",
    "    out = np.empty_like(x, dtype=float)\n",
    "    mask0 = x == 0.0\n",
    "    out[mask0] = 1.0\n",
    "\n",
    "    xm = x[~mask0]\n",
    "    log_const = -(a - 1.0) * np.log(2.0) - special.gammaln(a)\n",
    "    out[~mask0] = np.exp(log_const + a * np.log(xm) + np.log(special.kv(a, xm)))\n",
    "\n",
    "    return np.exp(1j * loc * t) * out\n",
    "\n",
    "\n",
    "df_demo = 5\n",
    "{\n",
    "    \"moments\": t_moments(df_demo),\n",
    "    \"entropy\": t_entropy(df_demo),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09af2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic function: closed form vs Monte Carlo estimate (memory-safe)\n",
    "\n",
    "df = 5\n",
    "n = 80_000\n",
    "samples = t_dist.rvs(df, size=n, random_state=rng)\n",
    "\n",
    "t_grid = np.linspace(0, 15, 300)\n",
    "phi_formula = np.real(t_charfun(t_grid, df=df))\n",
    "phi_mc = np.array([np.real(np.mean(np.exp(1j * t0 * samples))) for t0 in t_grid])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_grid, y=phi_formula, mode=\"lines\", name=\"closed form\"))\n",
    "fig.add_trace(go.Scatter(x=t_grid, y=phi_mc, mode=\"lines\", name=\"Monte Carlo\", line=dict(dash=\"dot\")))\n",
    "fig.update_layout(\n",
    "    title=\"Characteristic function of t distribution (df=5)\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"Re φ(t)\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693dcb6e",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The t distribution has three knobs in SciPy: `(df, loc, scale)`.\n",
    "\n",
    "- **`df` (\\(\\nu\\), degrees of freedom)** controls **tail heaviness**.\n",
    "  - small \\(\\nu\\) ⇒ very heavy tails and unstable sample means\n",
    "  - \\(\\nu\\to\\infty\\) ⇒ approaches the standard normal\n",
    "\n",
    "- **`loc`** shifts the distribution (it is the center of symmetry, and the mean when it exists).\n",
    "\n",
    "- **`scale`** stretches the distribution.\n",
    "\n",
    "A practical way to compare tails is via the decay rate:\n",
    "\n",
    "\\[\n",
    "f(t;\\nu) \\propto |t|^{-(\\nu+1)}\\quad\\text{for large }|t|.\n",
    "\\]\n",
    "\n",
    "So each additional degree of freedom makes the tail a little lighter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 2000)\n",
    "\n",
    "dfs = [1, 2, 5, 10, 30]\n",
    "\n",
    "fig = go.Figure()\n",
    "for df in dfs:\n",
    "    fig.add_trace(go.Scatter(x=x, y=t_pdf(x, df=df), mode=\"lines\", name=f\"t(df={df})\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=norm.pdf(x), mode=\"lines\", name=\"N(0,1)\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PDF shape changes: degrees of freedom ν\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd042efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of loc and scale\n",
    "x = np.linspace(-10, 10, 2500)\n",
    "params = [\n",
    "    (5, 0.0, 1.0),\n",
    "    (5, 2.0, 1.0),\n",
    "    (5, 0.0, 2.0),\n",
    "    (30, 0.0, 1.0),\n",
    "]\n",
    "\n",
    "fig = go.Figure()\n",
    "for df, loc, scale in params:\n",
    "    fig.add_trace(go.Scatter(x=x, y=t_pdf(x, df=df, loc=loc, scale=scale), mode=\"lines\", name=f\"df={df}, loc={loc}, scale={scale}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Location/scale effects\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f0662",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "\n",
    "For the standard t distribution, the PDF is symmetric: \\(f(t;\\nu)=f(-t;\\nu)\\).\n",
    "So the integral of \\(t f(t;\\nu)\\) over symmetric limits is 0.\n",
    "\n",
    "However, a mean exists only if the integral of \\(|t| f(t;\\nu)\\) is finite.\n",
    "Because the tails behave like \\(|t|^{-(\\nu+1)}\\),\n",
    "\n",
    "\\[\n",
    "\\int^{\\infty} |t| f(t;\\nu)\\,dt\\ \\text{converges iff}\\ \\nu>1.\n",
    "\\]\n",
    "\n",
    "Thus:\n",
    "\n",
    "- if \\(\\nu>1\\), \\(\\mathbb{E}[T]=0\\)\n",
    "- if \\(\\nu\\le 1\\), the mean is undefined\n",
    "\n",
    "### 6.2 Variance\n",
    "\n",
    "Use the ratio representation \\(T = Z/\\sqrt{V/\\nu}\\) with \\(Z\\sim\\mathcal{N}(0,1)\\), \\(V\\sim\\chi^2_{\\nu}\\), independent.\n",
    "Then\n",
    "\n",
    "\\[\n",
    "T^2 = \\frac{Z^2}{V/\\nu} = \\nu\\,\\frac{Z^2}{V}.\n",
    "\\]\n",
    "\n",
    "If \\(\\nu>2\\), the variance exists and\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[T^2] = \\nu\\,\\mathbb{E}[Z^2] \\mathbb{E}[1/V]\n",
    "= \\nu\\cdot 1 \\cdot \\frac{1}{\\nu-2}\n",
    "= \\frac{\\nu}{\\nu-2}.\n",
    "\\]\n",
    "\n",
    "(The identity \\(\\mathbb{E}[1/V]=1/(\\nu-2)\\) holds for \\(V\\sim\\chi^2_{\\nu}\\) and \\(\\nu>2\\).)\n",
    "\n",
    "### 6.3 Likelihood\n",
    "\n",
    "Given observations \\(x_1,\\dots,x_n\\) modeled as i.i.d.\n",
    "\n",
    "\\[\n",
    "X_i \\sim t_{\\nu}(\\mathrm{loc},\\mathrm{scale}),\n",
    "\\]\n",
    "\n",
    "the log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell(\\nu,\\mathrm{loc},\\mathrm{scale})\n",
    "= \\sum_{i=1}^n \\log f_X(x_i;\\nu,\\mathrm{loc},\\mathrm{scale}),\n",
    "\\]\n",
    "\n",
    "with\n",
    "\n",
    "\\[\n",
    "\\log f_X(x_i)\n",
    "= \\log\\Gamma\\left(\\frac{\\nu+1}{2}\\right)\n",
    "- \\log\\Gamma\\left(\\frac{\\nu}{2}\\right)\n",
    "- \\tfrac12\\log(\\nu\\pi)\n",
    "- \\log(\\mathrm{scale})\n",
    "- \\frac{\\nu+1}{2}\\log\\left(1 + \\frac{1}{\\nu}\\left(\\frac{x_i-\\mathrm{loc}}{\\mathrm{scale}}\\right)^2\\right).\n",
    "\\]\n",
    "\n",
    "This is what MLE routines optimize (including `scipy.stats.t.fit`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f003033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_loglik(x: np.ndarray, df: float, loc: float, scale: float) -> float:\n",
    "    return float(np.sum(t_logpdf(x, df=df, loc=loc, scale=scale)))\n",
    "\n",
    "\n",
    "def t_nll(theta: np.ndarray, x: np.ndarray) -> float:\n",
    "    df, loc, log_scale = float(theta[0]), float(theta[1]), float(theta[2])\n",
    "    scale = float(np.exp(log_scale))\n",
    "    # Penalize invalid df directly (optimizer-friendly).\n",
    "    if df <= 0:\n",
    "        return np.inf\n",
    "    return -t_loglik(x, df=df, loc=loc, scale=scale)\n",
    "\n",
    "\n",
    "# Quick demo: optimize (df, loc, scale) on simulated data\n",
    "true = {\"df\": 6.0, \"loc\": 1.5, \"scale\": 0.8}\n",
    "x = t_dist.rvs(true[\"df\"], loc=true[\"loc\"], scale=true[\"scale\"], size=3_000, random_state=rng)\n",
    "\n",
    "theta0 = np.array([10.0, np.median(x), np.log(np.std(x) + 1e-9)])\n",
    "res = optimize.minimize(t_nll, theta0, args=(x,), method=\"Nelder-Mead\")\n",
    "\n",
    "df_hat, loc_hat, scale_hat = float(res.x[0]), float(res.x[1]), float(np.exp(res.x[2]))\n",
    "{ \"true\": true, \"mle_hat\": {\"df\": df_hat, \"loc\": loc_hat, \"scale\": scale_hat} }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dec62",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Use the defining ratio:\n",
    "\n",
    "\\[\n",
    "T = \\frac{Z}{\\sqrt{V/\\nu}},\n",
    "\\qquad Z\\sim\\mathcal{N}(0,1),\\ \\ V\\sim\\chi^2_{\\nu},\\ \\ Z\\perp V.\n",
    "\\]\n",
    "\n",
    "To sample \\(V \\sim \\chi^2_{\\nu}\\) for general (non-integer) \\(\\nu\\), use the gamma representation:\n",
    "\n",
    "\\[\n",
    "V \\sim \\chi^2_{\\nu} \\iff V \\sim \\mathrm{Gamma}(k=\\nu/2,\\ \\theta=2)\n",
    "\\]\n",
    "\n",
    "(shape \\(k\\), scale \\(\\theta\\)).\n",
    "\n",
    "Then return \\(X=\\mathrm{loc}+\\mathrm{scale}\\,T\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafee90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_rvs_numpy(\n",
    "    df: float,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    "    size: int | tuple[int, ...] = 1,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    '''NumPy-only sampler for Student-t via Normal / Chi-square ratio.'''\n",
    "\n",
    "    df, loc, scale = _validate_df_loc_scale(df, loc, scale)\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    z = rng.standard_normal(size=size)\n",
    "    v = rng.gamma(shape=df / 2.0, scale=2.0, size=size)  # Chi-square(df)\n",
    "\n",
    "    t = z / np.sqrt(v / df)\n",
    "    return loc + scale * t\n",
    "\n",
    "\n",
    "# Monte Carlo check\n",
    "\n",
    "df = 8\n",
    "n = 400_000\n",
    "samples = t_rvs_numpy(df, size=n, rng=rng)\n",
    "\n",
    "mc_mean = float(np.mean(samples))\n",
    "mc_var = float(np.var(samples))\n",
    "\n",
    "theory = t_moments(df)\n",
    "(mc_mean, theory[\"mean\"], mc_var, theory[\"var\"]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b03f0",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We'll visualize:\n",
    "\n",
    "- PDFs for multiple degrees of freedom (tail heaviness)\n",
    "- CDFs (how quickly probability accumulates)\n",
    "- Monte Carlo samples vs the theoretical PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865baff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF + CDF side-by-side\n",
    "x = np.linspace(-6, 6, 2000)\n",
    "dfs = [1, 2, 5, 10, 30]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"PDF\", \"CDF\"))\n",
    "\n",
    "for df in dfs:\n",
    "    fig.add_trace(go.Scatter(x=x, y=t_pdf(x, df=df), mode=\"lines\", name=f\"df={df}\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=t_cdf(x, df=df), mode=\"lines\", name=f\"df={df}\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"density\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"F(x)\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Student-t: PDF and CDF for different ν\", width=1000, height=420)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo histogram vs theoretical PDF\n",
    "\n",
    "df = 4\n",
    "n = 200_000\n",
    "samples = t_rvs_numpy(df, size=n, rng=rng)\n",
    "\n",
    "x = np.linspace(-8, 8, 2500)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=samples, nbinsx=120, histnorm=\"probability density\", name=\"samples\", opacity=0.55))\n",
    "fig.add_trace(go.Scatter(x=x, y=t_pdf(x, df=df), mode=\"lines\", name=\"theoretical PDF\", line=dict(color=\"black\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Monte Carlo samples vs PDF (df={df})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a27c3",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.t`)\n",
    "\n",
    "SciPy provides a full implementation of the t distribution:\n",
    "\n",
    "- `t_dist.pdf(x, df, loc=..., scale=...)`\n",
    "- `t_dist.cdf(x, df, loc=..., scale=...)`\n",
    "- `t_dist.rvs(df, loc=..., scale=..., size=..., random_state=...)`\n",
    "- `t_dist.fit(data)` for MLE estimation of `(df, loc, scale)`\n",
    "\n",
    "We'll use it both as a reference implementation and for fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db096a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, loc, scale = 6.0, 1.2, 0.7\n",
    "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "pdf_vals = t_dist.pdf(x, df, loc=loc, scale=scale)\n",
    "cdf_vals = t_dist.cdf(x, df, loc=loc, scale=scale)\n",
    "\n",
    "samples = t_dist.rvs(df, loc=loc, scale=scale, size=20_000, random_state=rng)\n",
    "fit_df, fit_loc, fit_scale = t_dist.fit(samples)\n",
    "\n",
    "{\n",
    "    \"pdf\": pdf_vals,\n",
    "    \"cdf\": cdf_vals,\n",
    "    \"fit\": {\"df\": float(fit_df), \"loc\": float(fit_loc), \"scale\": float(fit_scale)},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c73db7",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing (why the t distribution exists)\n",
    "\n",
    "If \\(X_1,\\dots,X_n\\) are i.i.d. \\(\\mathcal{N}(\\mu,\\sigma^2)\\), then the **t statistic**\n",
    "\n",
    "\\[\n",
    "T = \\frac{\\bar X - \\mu}{S/\\sqrt{n}}\n",
    "\\]\n",
    "\n",
    "follows \\(t_{n-1}\\), where \\(S\\) is the sample standard deviation.\n",
    "That is the mathematical justification for **t tests** and **t confidence intervals**.\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "\n",
    "Two common appearances:\n",
    "\n",
    "1. **Posterior predictive**: with a Normal–Inverse-Gamma prior for \\((\\mu,\\sigma^2)\\), the posterior predictive distribution for a new observation is Student-t.\n",
    "2. **Robust likelihood**: using a t likelihood instead of Gaussian makes Bayesian inference less sensitive to outliers.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "\n",
    "A Student-t noise model is a simple way to generate data with occasional big shocks while remaining symmetric and unimodal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the t-statistic under the null and compare to t_{n-1}\n",
    "\n",
    "n = 12\n",
    "n_rep = 200_000\n",
    "mu0 = 0.0\n",
    "sigma = 2.0\n",
    "\n",
    "x = rng.normal(loc=mu0, scale=sigma, size=(n_rep, n))\n",
    "x_bar = x.mean(axis=1)\n",
    "s = x.std(axis=1, ddof=1)\n",
    "\n",
    "t_stat = (x_bar - mu0) / (s / np.sqrt(n))\n",
    "\n",
    "df = n - 1\n",
    "\n",
    "grid = np.linspace(-6, 6, 2000)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=t_stat, nbinsx=140, histnorm=\"probability density\", name=\"simulated t-stat\", opacity=0.55))\n",
    "fig.add_trace(go.Scatter(x=grid, y=t_dist.pdf(grid, df), mode=\"lines\", name=f\"t(df={df})\", line=dict(color=\"black\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"t-statistic distribution under Normality (n={n} ⇒ df={df})\",\n",
    "    xaxis_title=\"t-stat\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian posterior predictive (Normal data, unknown mean & variance)\n",
    "# Prior: Normal-Inverse-Gamma (μ, σ^2)\n",
    "\n",
    "# Synthetic data (with a couple outliers to make things interesting)\n",
    "rng2 = np.random.default_rng(123)\n",
    "y = rng2.normal(loc=1.0, scale=1.2, size=25)\n",
    "y[[3, 17]] += np.array([5.0, -4.0])\n",
    "\n",
    "# Prior hyperparameters (weakly informative)\n",
    "mu0 = 0.0\n",
    "kappa0 = 0.01\n",
    "alpha0 = 2.0\n",
    "beta0 = 2.0\n",
    "\n",
    "n = y.size\n",
    "y_bar = float(y.mean())\n",
    "ssq = float(np.sum((y - y_bar) ** 2))\n",
    "\n",
    "kappa_n = kappa0 + n\n",
    "mu_n = (kappa0 * mu0 + n * y_bar) / kappa_n\n",
    "alpha_n = alpha0 + n / 2.0\n",
    "beta_n = beta0 + 0.5 * ssq + 0.5 * (kappa0 * n / kappa_n) * (y_bar - mu0) ** 2\n",
    "\n",
    "# Posterior predictive is Student-t with:\n",
    "# df = 2*alpha_n\n",
    "# loc = mu_n\n",
    "# scale^2 = beta_n * (kappa_n + 1) / (alpha_n * kappa_n)\n",
    "\n",
    "df_pred = 2.0 * alpha_n\n",
    "loc_pred = mu_n\n",
    "scale_pred = float(np.sqrt(beta_n * (kappa_n + 1.0) / (alpha_n * kappa_n)))\n",
    "\n",
    "x = np.linspace(loc_pred - 6 * scale_pred, loc_pred + 6 * scale_pred, 2500)\n",
    "\n",
    "pred_pdf = t_dist.pdf(x, df_pred, loc=loc_pred, scale=scale_pred)\n",
    "plug_in_pdf = norm.pdf(x, loc=y_bar, scale=float(np.std(y, ddof=1)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=pred_pdf, mode=\"lines\", name=f\"posterior predictive t (df={df_pred:.1f})\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=plug_in_pdf, mode=\"lines\", name=\"plug-in Normal\", line=dict(dash=\"dash\")))\n",
    "fig.add_trace(go.Histogram(x=y, nbinsx=20, histnorm=\"probability density\", name=\"data\", opacity=0.4))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Posterior predictive: Student-t vs plug-in Normal\",\n",
    "    xaxis_title=\"y\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc2dd9",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: `df` must be \\(>0\\), `scale` must be \\(>0\\).\n",
    "- **Nonexistent moments**: the mean/variance/kurtosis require \\(\n",
    "u>1\\), \\(\n",
    "u>2\\), \\(\n",
    "u>4\\) respectively.\n",
    "  With small \\(\n",
    "u\\), sample means and variances can be extremely unstable.\n",
    "- **Numerical issues**:\n",
    "  - for extreme \\(|x|\\) or small \\(\n",
    "u\\), computing the PDF directly can underflow; prefer `logpdf`.\n",
    "  - CDF tails are best computed by special functions (SciPy does this well).\n",
    "- **Parameterization confusion**: many texts define the t distribution only via \\(\n",
    "u\\), but SciPy uses a location–scale family (`df`, `loc`, `scale`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dd842",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- The Student's t distribution \\(t_{\\nu}\\) is a **continuous**, symmetric, heavy-tailed distribution on \\(\\mathbb{R}\\).\n",
    "- It arises as \\(Z/\\sqrt{V/\\nu}\\) (normal divided by chi-square scale), explaining its role in **t tests**.\n",
    "- `df=1` gives the Cauchy; `df\\to\\infty` approaches the normal.\n",
    "- Moments exist only above thresholds: mean (\\(\\nu>1\\)), variance (\\(\\nu>2\\)), kurtosis (\\(\\nu>4\\)).\n",
    "- A NumPy-only sampler is easy via the ratio representation; SciPy provides a full-featured implementation and MLE fitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
