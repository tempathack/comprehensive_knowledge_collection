{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponentiated Weibull distribution (`exponweib`)\n",
    "\n",
    "The **exponentiated Weibull** distribution (SciPy: `scipy.stats.exponweib`) is a flexible *continuous* distribution on $[0,\\infty)$ (in its standard form) obtained by **raising the Weibull CDF to a positive power**.\n",
    "\n",
    "It is popular in **reliability** and **survival analysis** because—with two shape parameters—it can represent a wide range of hazard-rate behaviors (increasing, decreasing, unimodal, and bathtub-like).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Write down the PDF/CDF/PPF and connect them to the Weibull distribution.\n",
    "- Understand how the parameters change shape and hazard behavior.\n",
    "- Derive moments using a clean change-of-variables + series expansion.\n",
    "- Implement inverse-CDF sampling using only NumPy.\n",
    "- Use SciPy (`exponweib`) for evaluation, simulation, and MLE fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import integrate, optimize, special\n",
    "from scipy.stats import exponweib as exponweib_dist\n",
    "from scipy.stats import weibull_min, chi2, kstest\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "# Record versions for reproducibility (useful when numerical details matter)\n",
    "VERSIONS = {\"numpy\": np.__version__, \"scipy\": scipy.__version__, \"plotly\": plotly.__version__}\n",
    "VERSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `exponweib` (Exponentiated Weibull; SciPy: `scipy.stats.exponweib`)\n",
    "- **Type**: Continuous\n",
    "- **Support (standard form)**: $x \\in [0,\\infty)$\n",
    "- **Parameter space (standard form)**: shape parameters $a>0$, $c>0$\n",
    "- **SciPy location/scale**: `loc \\in \\mathbb{R}`, `scale > 0` with\n",
    "\n",
    "  $$X = \\text{loc} + \\text{scale}\\,Y, \\qquad Y \\sim \\mathrm{ExponWeib}(a,c).$$\n",
    "\n",
    "Unless stated otherwise, we work with the **standard form** (`loc=0`, `scale=1`).\n",
    "\n",
    "> Notation note: many texts write the parameters as $(\\alpha, k, \\lambda)$, where $\\alpha$ is the exponentiation parameter, $k$ is the Weibull shape, and $\\lambda$ is the scale. SciPy uses $(a, c)$ for the two shape parameters and `scale` for $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "Start with a **Weibull** random variable $W$ (shape $c$, scale $\\lambda$):\n",
    "\n",
    "$$F_W(x) = 1 - \\exp\\left(-(x/\\lambda)^c\\right), \\qquad x \\ge 0.$$\n",
    "\n",
    "The exponentiated Weibull distribution raises this CDF to a power $a>0$:\n",
    "\n",
    "$$F_X(x) = \\bigl(F_W(x)\\bigr)^a = \\left[1 - \\exp\\left(-(x/\\lambda)^c\\right)\\right]^a.$$\n",
    "\n",
    "So it is literally the **Weibull CDF, exponentiated**.\n",
    "\n",
    "A useful interpretation when $a$ is an integer ($a=m\\in\\mathbb{N}$):\n",
    "\n",
    "- If $W_1,\\dots,W_m$ are i.i.d. Weibull$(c,\\lambda)$, then\n",
    "  $$\\max\\{W_1,\\dots,W_m\\} \\sim \\mathrm{ExponWeib}(a=m, c, \\lambda).$$\n",
    "\n",
    "This “maximum-of-$m$ Weibulls” story helps build intuition: larger $a$ pushes mass to the right (stochastically larger lifetimes).\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Reliability engineering**: modeling lifetimes with *non-monotone* hazard (e.g. bathtub-shaped failure rates).\n",
    "- **Survival analysis**: flexible parametric baseline hazard.\n",
    "- **Hydrology / environmental extremes**: positive-valued quantities with skew and flexible tails.\n",
    "- **Manufacturing / materials**: time-to-failure with early-life and wear-out effects.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- $a=1$ gives the standard **Weibull** distribution (`scipy.stats.weibull_min`).\n",
    "- $c=1$ gives the **exponentiated exponential** distribution.\n",
    "- $a=1$ and $c=1$ gives the **exponential** distribution.\n",
    "- Location/scale transform recovers different units and origins: $X=\\text{loc}+\\text{scale}\\,Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "We present the **standardized** form first (`loc=0`, `scale=1`).\n",
    "\n",
    "### CDF\n",
    "\n",
    "For $x \\ge 0$, $a>0$, $c>0$:\n",
    "\n",
    "$$\n",
    "F(x; a,c) = \\left[1-\\exp\\left(-x^c\\right)\\right]^a.\n",
    "$$\n",
    "\n",
    "### PDF\n",
    "\n",
    "For $x>0$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(x; a,c)\n",
    " &= \\frac{d}{dx}F(x;a,c) \\\\\n",
    " &= a\\,c\\,x^{c-1}\\,\\exp(-x^c)\\,\\left[1-\\exp(-x^c)\\right]^{a-1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Quantile function (PPF)\n",
    "\n",
    "For $q\\in(0,1)$:\n",
    "\n",
    "$$\n",
    "F^{-1}(q;a,c) = \\left[-\\log\\left(1-q^{1/a}\\right)\\right]^{1/c}.\n",
    "$$\n",
    "\n",
    "This inverse-CDF is the key to fast sampling.\n",
    "\n",
    "### Location / scale\n",
    "\n",
    "With `loc` and `scale>0`, define $z = (x-\\text{loc})/\\text{scale}$. Then for $x\\ge \\text{loc}$:\n",
    "\n",
    "$$\n",
    "F_X(x) = F(z;a,c),\n",
    "\\qquad\n",
    "f_X(x) = \\frac{1}{\\text{scale}}\\,f(z;a,c).\n",
    "$$\n",
    "\n",
    "### Hazard function (survival analysis)\n",
    "\n",
    "The **hazard rate** is\n",
    "\n",
    "$$h(x) = \\frac{f(x)}{1-F(x)}.$$\n",
    "\n",
    "For the exponentiated Weibull, the hazard can be increasing, decreasing, unimodal, or bathtub-shaped depending on $(a,c)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponweib_cdf(x: np.ndarray, a: float, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"CDF of the exponentiated Weibull (NumPy implementation).\n",
    "\n",
    "    Matches SciPy's `scipy.stats.exponweib.cdf` for the same (a, c, loc, scale).\n",
    "    \"\"\"\n",
    "    if a <= 0 or c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"Require a>0, c>0, scale>0\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    z = (x - loc) / scale\n",
    "\n",
    "    out = np.zeros_like(z, dtype=float)\n",
    "    mask = z >= 0\n",
    "    zm = z[mask]\n",
    "\n",
    "    # exm1c = 1 - exp(-zm**c), computed stably for small zm**c\n",
    "    exm1c = -np.expm1(-(zm**c))\n",
    "    out[mask] = exm1c**a\n",
    "    return out\n",
    "\n",
    "\n",
    "def exponweib_logpdf(x: np.ndarray, a: float, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of the exponentiated Weibull (stable for small/large x).\"\"\"\n",
    "    if a <= 0 or c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"Require a>0, c>0, scale>0\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    z = (x - loc) / scale\n",
    "\n",
    "    out = np.full_like(z, -np.inf, dtype=float)\n",
    "    mask = z > 0\n",
    "    zm = z[mask]\n",
    "\n",
    "    neg_zc = -(zm**c)\n",
    "    exm1c = -np.expm1(neg_zc)  # = 1 - exp(-zm**c) in (0,1)\n",
    "\n",
    "    logp = (\n",
    "        np.log(a)\n",
    "        + np.log(c)\n",
    "        - np.log(scale)\n",
    "        + (a - 1.0) * np.log(exm1c)\n",
    "        + neg_zc\n",
    "        + (c - 1.0) * np.log(zm)\n",
    "    )\n",
    "    out[mask] = logp\n",
    "    return out\n",
    "\n",
    "\n",
    "def exponweib_pdf(x: np.ndarray, a: float, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"PDF computed from the log-PDF.\"\"\"\n",
    "    return np.exp(exponweib_logpdf(x, a, c, loc=loc, scale=scale))\n",
    "\n",
    "\n",
    "def exponweib_ppf(q: np.ndarray, a: float, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Percent-point function (inverse CDF).\"\"\"\n",
    "    if a <= 0 or c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"Require a>0, c>0, scale>0\")\n",
    "\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    if np.any((q < 0) | (q > 1)):\n",
    "        raise ValueError(\"q must be in [0,1]\")\n",
    "\n",
    "    out = np.empty_like(q, dtype=float)\n",
    "    out[q == 0] = loc\n",
    "    out[q == 1] = np.inf\n",
    "\n",
    "    mask = (q > 0) & (q < 1)\n",
    "    qm = q[mask]\n",
    "    out[mask] = loc + scale * (-np.log1p(-(qm ** (1.0 / a)))) ** (1.0 / c)\n",
    "    return out\n",
    "\n",
    "\n",
    "def exponweib_rvs_numpy(\n",
    "    a: float,\n",
    "    c: float,\n",
    "    *,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    "    size=1,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Draw samples from ExponWeib(a,c) using **only NumPy**.\n",
    "\n",
    "    Inverse transform sampling:\n",
    "      U ~ Uniform(0,1)\n",
    "      X = loc + scale * (-log(1 - U^{1/a}))^{1/c}\n",
    "    \"\"\"\n",
    "    if a <= 0 or c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"Require a>0, c>0, scale>0\")\n",
    "\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    u = rng.uniform(size=size)\n",
    "\n",
    "    # Avoid log(0) at the endpoints.\n",
    "    eps = np.finfo(float).eps\n",
    "    u = np.clip(u, eps, 1.0 - eps)\n",
    "\n",
    "    return loc + scale * (-np.log1p(-(u ** (1.0 / a)))) ** (1.0 / c)\n",
    "\n",
    "\n",
    "def exponweib_sf(x: np.ndarray, a: float, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Survival function S(x)=1-CDF(x), computed stably via log1p.\"\"\"\n",
    "    cdf = exponweib_cdf(x, a, c, loc=loc, scale=scale)\n",
    "    return np.exp(np.log1p(-cdf))\n",
    "\n",
    "\n",
    "def exponweib_hazard(x: np.ndarray, a: float, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Hazard h(x)=f(x)/S(x), computed in log-space for stability.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    logf = exponweib_logpdf(x, a, c, loc=loc, scale=scale)\n",
    "    sf = exponweib_sf(x, a, c, loc=loc, scale=scale)\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "\n",
    "    mask = sf > 0\n",
    "    out[mask] = np.exp(logf[mask] - np.log(sf[mask]))\n",
    "    out[~mask] = np.inf\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: our formulas match SciPy (prefer logpdf to avoid overflow near 0).\n",
    "\n",
    "a, c, loc, scale = 1.7, 1.3, 0.4, 2.2\n",
    "x = np.logspace(-5, 2, 40) * scale + loc\n",
    "qs = np.linspace(0.01, 0.99, 9)\n",
    "\n",
    "assert np.allclose(exponweib_logpdf(x, a, c, loc=loc, scale=scale), exponweib_dist.logpdf(x, a, c, loc=loc, scale=scale))\n",
    "assert np.allclose(exponweib_cdf(x, a, c, loc=loc, scale=scale), exponweib_dist.cdf(x, a, c, loc=loc, scale=scale))\n",
    "assert np.allclose(exponweib_ppf(qs, a, c, loc=loc, scale=scale), exponweib_dist.ppf(qs, a, c, loc=loc, scale=scale))\n",
    "\n",
    "# Check that inverse-CDF sampling produces the right distribution (quick KS test).\n",
    "x_samp = exponweib_rvs_numpy(a, c, loc=loc, scale=scale, size=3_000, rng=rng)\n",
    "D, p = kstest(x_samp, lambda t: exponweib_dist.cdf(t, a, c, loc=loc, scale=scale))\n",
    "print(f\"KS statistic={D:.4f}, p-value={p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Raw moments\n",
    "\n",
    "Assume the standard form with `loc=0` and `scale=\\lambda`.\n",
    "\n",
    "Using the substitution $y = (x/\\lambda)^c$ (so $x=\\lambda y^{1/c}$), the exponentiated Weibull density simplifies nicely and we obtain a series expression for the raw moments:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^r]\n",
    "= \\lambda^r\\,a\\,\\Gamma\\left(1+\\frac{r}{c}\\right)\n",
    "\\sum_{j=0}^{\\infty} (-1)^j\\,\\binom{a-1}{j}\\,\\frac{1}{(j+1)^{1+r/c}}.\n",
    "$$\n",
    "\n",
    "- For integer $a=m\\in\\mathbb{N}$, the binomial series terminates at $j=m-1$.\n",
    "- For non-integer $a$, the series is infinite but often converges quickly.\n",
    "\n",
    "From the raw moments we get:\n",
    "\n",
    "- mean: $\\mu = \\mathbb{E}[X]$\n",
    "- variance: $\\sigma^2 = \\mathbb{E}[X^2] - \\mu^2$\n",
    "- skewness and kurtosis from the central moments $\\mu_3,\\mu_4$.\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "There is no simple elementary closed form for the MGF in general. Existence depends on the tail:\n",
    "\n",
    "- If $c>1$, the tail is *super-exponential* and $M_X(t)=\\mathbb{E}[e^{tX}]$ exists for all real $t$.\n",
    "- If $c=1$ (exponentiated exponential tail), $M_X(t)$ exists for $t < 1/\\lambda$.\n",
    "- If $0<c<1$, $M_X(t)$ diverges for any $t>0$ (but the Laplace transform exists for $t\\le 0$).\n",
    "\n",
    "The characteristic function $\\varphi_X(t)=\\mathbb{E}[e^{itX}]$ always exists and can be computed numerically.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy is\n",
    "\n",
    "$$H(X) = -\\mathbb{E}[\\log f(X)].$$\n",
    "\n",
    "SciPy provides `exponweib.entropy(...)` (numerical integration). A Monte Carlo estimate using `-mean(logpdf(samples))` is also useful for sanity checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponweib_raw_moment_series(\n",
    "    r: float,\n",
    "    a: float,\n",
    "    c: float,\n",
    "    *,\n",
    "    scale: float = 1.0,\n",
    "    max_terms: int = 50_000,\n",
    "    tol: float = 1e-12,\n",
    ") -> float:\n",
    "    \"\"\"Compute E[X^r] via the binomial/Gamma series (loc=0).\n",
    "\n",
    "    Uses the identity:\n",
    "      E[X^r] = scale^r * a * Gamma(1 + r/c) * sum_{j>=0} (-1)^j * binom(a-1, j) / (j+1)^{1 + r/c}\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Requires a>0, c>0, scale>0.\n",
    "    - For integer a, the sum is finite (j=0,...,a-1).\n",
    "    - For non-integer a, the sum is infinite; we truncate by `tol`.\n",
    "    \"\"\"\n",
    "    if a <= 0 or c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"Require a>0, c>0, scale>0\")\n",
    "    if r <= -c:\n",
    "        raise ValueError(\"Moment diverges for r <= -c in the standard form\")\n",
    "\n",
    "    p = 1.0 + r / c\n",
    "    prefactor = (scale**r) * a * special.gamma(1.0 + r / c)\n",
    "\n",
    "    s = 0.0\n",
    "    for j in range(max_terms):\n",
    "        coeff = special.binom(a - 1.0, j)\n",
    "        term = ((-1.0) ** j) * coeff / (j + 1.0) ** p\n",
    "        s_new = s + term\n",
    "\n",
    "        # Stop once the marginal contribution becomes negligible.\n",
    "        if j > 0 and abs(term) < tol * max(1.0, abs(s_new)):\n",
    "            s = s_new\n",
    "            break\n",
    "        s = s_new\n",
    "\n",
    "    return float(prefactor * s)\n",
    "\n",
    "\n",
    "def exponweib_mvsk_from_raw_moments(a: float, c: float, *, scale: float = 1.0):\n",
    "    \"\"\"Return mean, variance, skewness, excess kurtosis via raw moments.\"\"\"\n",
    "    m1 = exponweib_raw_moment_series(1.0, a, c, scale=scale)\n",
    "    m2 = exponweib_raw_moment_series(2.0, a, c, scale=scale)\n",
    "    m3 = exponweib_raw_moment_series(3.0, a, c, scale=scale)\n",
    "    m4 = exponweib_raw_moment_series(4.0, a, c, scale=scale)\n",
    "\n",
    "    mean = m1\n",
    "    var = m2 - m1**2\n",
    "\n",
    "    mu3 = m3 - 3 * m2 * mean + 2 * mean**3\n",
    "    mu4 = m4 - 4 * m3 * mean + 6 * m2 * mean**2 - 3 * mean**4\n",
    "\n",
    "    skew = mu3 / (var ** 1.5)\n",
    "    kurt_excess = mu4 / (var**2) - 3.0\n",
    "\n",
    "    return mean, var, skew, kurt_excess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the series-based moments to SciPy's numerical stats.\n",
    "\n",
    "a, c, scale = 1.6, 1.2, 2.0\n",
    "\n",
    "mean_s, var_s, skew_s, kurt_s = exponweib_mvsk_from_raw_moments(a, c, scale=scale)\n",
    "mean_sp, var_sp, skew_sp, kurt_sp = exponweib_dist.stats(a, c, scale=scale, moments=\"mvsk\")\n",
    "\n",
    "print(\"Series moments:\")\n",
    "print(\"  mean=\", mean_s)\n",
    "print(\"  var =\", var_s)\n",
    "print(\"  skew=\", skew_s)\n",
    "print(\"  kurt(excess)=\", kurt_s)\n",
    "\n",
    "print(\"\\nSciPy stats (numerical):\")\n",
    "print(\"  mean=\", float(mean_sp))\n",
    "print(\"  var =\", float(var_sp))\n",
    "print(\"  skew=\", float(skew_sp))\n",
    "print(\"  kurt(excess)=\", float(kurt_sp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy: SciPy (numerical integration) vs Monte Carlo.\n",
    "\n",
    "a, c, scale = 1.6, 1.2, 2.0\n",
    "rv = exponweib_dist(a, c, scale=scale)\n",
    "\n",
    "H_scipy = rv.entropy()\n",
    "\n",
    "x = rv.rvs(size=60_000, random_state=rng)\n",
    "H_mc = -np.mean(rv.logpdf(x))\n",
    "\n",
    "print(f\"Entropy (SciPy)      = {H_scipy:.6f}\")\n",
    "print(f\"Entropy (Monte Carlo)= {H_mc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic function (CF) example via Monte Carlo\n",
    "\n",
    "a, c, scale = 1.2, 0.9, 1.5\n",
    "rv = exponweib_dist(a, c, scale=scale)\n",
    "\n",
    "x = rv.rvs(size=120_000, random_state=rng)\n",
    "\n",
    "def cf_mc(t: float) -> complex:\n",
    "    return complex(np.mean(np.exp(1j * t * x)))\n",
    "\n",
    "for t in [0.5, 1.0, 2.0, 4.0]:\n",
    "    print(f\"t={t:>3}: phi(t)≈ {cf_mc(t)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "Think of the parameters as controlling **(i)** the underlying Weibull shape and **(ii)** how strongly we “exponentiate” its CDF.\n",
    "\n",
    "### Shape parameter $c$ (Weibull shape)\n",
    "\n",
    "- $c<1$: heavy mass near 0, **decreasing hazard** (infant mortality)\n",
    "- $c=1$: constant hazard (exponential-like tail)\n",
    "- $c>1$: **increasing hazard** (wear-out)\n",
    "\n",
    "### Exponentiation parameter $a$\n",
    "\n",
    "Because $F(x) = F_W(x)^a$:\n",
    "\n",
    "- $a>1$ makes the distribution **stochastically larger** (shifts probability mass to the right).\n",
    "- $0<a<1$ makes it **stochastically smaller** (more mass near 0).\n",
    "- For integer $a=m$, $X$ is the maximum of $m$ i.i.d. Weibull lifetimes.\n",
    "\n",
    "### Scale and location\n",
    "\n",
    "- `scale` rescales the x-axis (units of time/length/etc.).\n",
    "- `loc` shifts the support to start at `loc`.\n",
    "\n",
    "Below we visualize how the PDF, CDF, and hazard change with parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf_grid(param_sets, *, title: str):\n",
    "    fig = go.Figure()\n",
    "    for (a, c, scale) in param_sets:\n",
    "        rv = exponweib_dist(a, c, scale=scale)\n",
    "        x_max = rv.ppf(0.995)\n",
    "        x = np.linspace(1e-6, x_max, 500)\n",
    "        fig.add_trace(go.Scatter(x=x, y=rv.pdf(x), mode=\"lines\", name=f\"a={a}, c={c}, scale={scale}\"))\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=\"x\", yaxis_title=\"pdf\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_cdf_grid(param_sets, *, title: str):\n",
    "    fig = go.Figure()\n",
    "    for (a, c, scale) in param_sets:\n",
    "        rv = exponweib_dist(a, c, scale=scale)\n",
    "        x_max = rv.ppf(0.995)\n",
    "        x = np.linspace(0.0, x_max, 500)\n",
    "        fig.add_trace(go.Scatter(x=x, y=rv.cdf(x), mode=\"lines\", name=f\"a={a}, c={c}, scale={scale}\"))\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=\"x\", yaxis_title=\"cdf\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_hazard_grid(param_sets, *, title: str):\n",
    "    fig = go.Figure()\n",
    "    for (a, c, scale) in param_sets:\n",
    "        rv = exponweib_dist(a, c, scale=scale)\n",
    "        x_max = rv.ppf(0.995)\n",
    "        x = np.linspace(1e-6, x_max, 600)\n",
    "        h = rv.pdf(x) / rv.sf(x)\n",
    "        fig.add_trace(go.Scatter(x=x, y=h, mode=\"lines\", name=f\"a={a}, c={c}, scale={scale}\"))\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=\"x\", yaxis_title=\"hazard h(x)\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Vary 'a' at fixed c\n",
    "plot_pdf_grid([(0.6, 1.5, 1.0), (1.0, 1.5, 1.0), (2.0, 1.5, 1.0)], title=\"PDF: effect of a (fixed c=1.5, scale=1)\")\n",
    "plot_cdf_grid([(0.6, 1.5, 1.0), (1.0, 1.5, 1.0), (2.0, 1.5, 1.0)], title=\"CDF: effect of a (fixed c=1.5, scale=1)\")\n",
    "\n",
    "# Vary 'c' at fixed a\n",
    "plot_pdf_grid([(1.5, 0.7, 1.0), (1.5, 1.0, 1.0), (1.5, 2.0, 1.0)], title=\"PDF: effect of c (fixed a=1.5, scale=1)\")\n",
    "plot_hazard_grid([(1.2, 0.7, 1.0), (1.2, 1.0, 1.0), (1.2, 2.0, 1.0)], title=\"Hazard: effect of c (fixed a=1.2, scale=1)\")\n",
    "\n",
    "# A few parameter combinations that show different hazard shapes\n",
    "plot_hazard_grid(\n",
    "    [(0.6, 0.8, 1.0), (1.0, 1.0, 1.0), (2.0, 0.9, 1.0), (2.0, 2.0, 1.0)],\n",
    "    title=\"Hazard: different shapes across (a, c)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "We sketch derivations for the expectation/variance and the likelihood.\n",
    "\n",
    "### 6.1 Expectation (raw moments)\n",
    "\n",
    "Work with `loc=0`, `scale=\\lambda` and consider the $r$-th raw moment:\n",
    "\n",
    "$$\\mathbb{E}[X^r] = \\int_0^{\\infty} x^r\\,f(x;a,c,\\lambda)\\,dx.$$\n",
    "\n",
    "With\n",
    "\n",
    "$$f(x) = \\frac{a c}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{c-1}\\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^c\\right)\\left[1-\\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^c\\right)\\right]^{a-1},$$\n",
    "\n",
    "use the substitution $y=(x/\\lambda)^c$ (so $x=\\lambda y^{1/c}$ and $dx = (\\lambda/c) y^{1/c-1}\\,dy$). The Jacobian cancels the Weibull power term and the integral becomes\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^r]\n",
    "= \\lambda^r\\,a\\int_0^{\\infty} y^{r/c}\\,e^{-y}\\,\\bigl(1-e^{-y}\\bigr)^{a-1}\\,dy.\n",
    "$$\n",
    "\n",
    "Now expand using the binomial series (valid for $0<e^{-y}<1$):\n",
    "\n",
    "$$\n",
    "(1-e^{-y})^{a-1} = \\sum_{j=0}^{\\infty} (-1)^j\\binom{a-1}{j}\\,e^{-jy}.\n",
    "$$\n",
    "\n",
    "Swap sum and integral (justified under standard conditions) to get\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^r]\n",
    "= \\lambda^r\\,a\\sum_{j=0}^{\\infty} (-1)^j\\binom{a-1}{j}\n",
    "\\int_0^{\\infty} y^{r/c} e^{-(j+1)y}\\,dy.\n",
    "$$\n",
    "\n",
    "Finally, recognize a Gamma integral:\n",
    "\n",
    "$$\n",
    "\\int_0^{\\infty} y^{r/c} e^{-(j+1)y}\\,dy\n",
    "= \\frac{\\Gamma(1+r/c)}{(j+1)^{1+r/c}},\n",
    "$$\n",
    "\n",
    "which yields the moment formula used earlier.\n",
    "\n",
    "### 6.2 Variance\n",
    "\n",
    "Compute $\\mathbb{E}[X]$ and $\\mathbb{E}[X^2]$ from the raw-moment expression and combine:\n",
    "\n",
    "$$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2.$$\n",
    "\n",
    "### 6.3 Likelihood (iid sample)\n",
    "\n",
    "Given iid data $x_1,\\dots,x_n$ from the *standard* model with scale $\\lambda$ and shapes $(a,c)$, the likelihood is\n",
    "\n",
    "$$L(a,c,\\lambda) = \\prod_{i=1}^n f(x_i; a,c,\\lambda).$$\n",
    "\n",
    "The log-likelihood (often optimized numerically) is\n",
    "\n",
    "$$\n",
    "\\ell(a,c,\\lambda)\n",
    "= n(\\log a + \\log c - \\log \\lambda)\n",
    "+ (c-1)\\sum_{i=1}^n \\log(x_i/\\lambda)\n",
    "- \\sum_{i=1}^n (x_i/\\lambda)^c\n",
    "+ (a-1)\\sum_{i=1}^n \\log\\bigl(1-\\exp(-(x_i/\\lambda)^c)\\bigr).\n",
    "$$\n",
    "\n",
    "SciPy’s `fit` routine maximizes this (with `loc`/`scale` included) via numerical optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponweib_loglik(x: np.ndarray, a: float, c: float, *, loc: float = 0.0, scale: float = 1.0) -> float:\n",
    "    \"\"\"Total log-likelihood using our NumPy logpdf.\"\"\"\n",
    "    return float(np.sum(exponweib_logpdf(x, a, c, loc=loc, scale=scale)))\n",
    "\n",
    "\n",
    "def fit_exponweib_mle_via_minimize(x: np.ndarray, *, loc_fixed: float = 0.0):\n",
    "    \"\"\"Simple MLE demo using SciPy optimize on transformed parameters.\n",
    "\n",
    "    We optimize over (log a, log c, log scale) with loc fixed.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    def nll(theta):\n",
    "        log_a, log_c, log_scale = theta\n",
    "        a = np.exp(log_a)\n",
    "        c = np.exp(log_c)\n",
    "        scale = np.exp(log_scale)\n",
    "        return -exponweib_loglik(x, a, c, loc=loc_fixed, scale=scale)\n",
    "\n",
    "    # crude initial guess from a Weibull fit\n",
    "    c0, loc0, scale0 = weibull_min.fit(x, floc=loc_fixed)\n",
    "    theta0 = np.log([1.0, c0, scale0])\n",
    "\n",
    "    res = optimize.minimize(nll, theta0, method=\"Nelder-Mead\")\n",
    "    a_hat, c_hat, scale_hat = np.exp(res.x)\n",
    "    return a_hat, c_hat, loc_fixed, scale_hat, res\n",
    "\n",
    "\n",
    "# Quick demo on synthetic data\n",
    "true = dict(a=1.8, c=1.1, loc=0.0, scale=2.5)\n",
    "x = exponweib_dist.rvs(true[\"a\"], true[\"c\"], loc=true[\"loc\"], scale=true[\"scale\"], size=900, random_state=rng)\n",
    "\n",
    "(a_hat, c_hat, loc_hat, scale_hat, res) = fit_exponweib_mle_via_minimize(x, loc_fixed=0.0)\n",
    "print(\"True params:\", true)\n",
    "print(\"Minimize MLE:\", {\"a\": a_hat, \"c\": c_hat, \"loc\": loc_hat, \"scale\": scale_hat})\n",
    "print(\"Converged:\", res.success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only algorithm (inverse transform)\n",
    "\n",
    "Because the CDF has a closed-form inverse, sampling is straightforward.\n",
    "\n",
    "Let $U\\sim\\mathrm{Uniform}(0,1)$. Set\n",
    "\n",
    "$$\n",
    "X = \\text{loc} + \\text{scale}\\,\\left[-\\log\\left(1-U^{1/a}\\right)\\right]^{1/c}.\n",
    "$$\n",
    "\n",
    "Then $X \\sim \\mathrm{ExponWeib}(a,c,\\text{loc},\\text{scale})$.\n",
    "\n",
    "Implementation detail: for numerical stability,\n",
    "\n",
    "- compute $1-\\exp(-t)$ as `-expm1(-t)`\n",
    "- compute $\\log(1-u)$ as `log1p(-u)`\n",
    "- clip $U$ away from exactly 0 or 1 to avoid `log(0)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with the NumPy-only sampler\n",
    "\n",
    "a, c, scale = 1.6, 1.2, 2.0\n",
    "x = exponweib_rvs_numpy(a, c, scale=scale, size=60_000, rng=rng)\n",
    "\n",
    "# Compare empirical moments to theory\n",
    "mean_emp = x.mean()\n",
    "var_emp = x.var()\n",
    "mean_theory, var_theory = exponweib_dist.stats(a, c, scale=scale, moments=\"mv\")\n",
    "\n",
    "print(f\"Empirical mean={mean_emp:.4f}, variance={var_emp:.4f}\")\n",
    "print(f\"Theory    mean={float(mean_theory):.4f}, variance={float(var_theory):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We visualize:\n",
    "\n",
    "- the PDF for different parameter settings\n",
    "- the CDF (as a sanity check for probability mass)\n",
    "- Monte Carlo samples vs the theoretical PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF / CDF and Monte Carlo comparison\n",
    "\n",
    "a, c, scale = 1.6, 1.2, 2.0\n",
    "rv = exponweib_dist(a, c, scale=scale)\n",
    "\n",
    "x_max = rv.ppf(0.995)\n",
    "xs = np.linspace(1e-6, x_max, 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=rv.pdf(xs), mode=\"lines\", name=\"pdf\"))\n",
    "fig.update_layout(title=\"Exponentiated Weibull PDF\", xaxis_title=\"x\", yaxis_title=\"pdf\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=rv.cdf(xs), mode=\"lines\", name=\"cdf\"))\n",
    "fig.update_layout(title=\"Exponentiated Weibull CDF\", xaxis_title=\"x\", yaxis_title=\"cdf\")\n",
    "fig.show()\n",
    "\n",
    "# Monte Carlo check\n",
    "samples = rv.rvs(size=40_000, random_state=rng)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=samples, nbinsx=60, histnorm=\"probability density\", name=\"samples\", opacity=0.7))\n",
    "fig.add_trace(go.Scatter(x=xs, y=rv.pdf(xs), mode=\"lines\", name=\"theoretical pdf\"))\n",
    "fig.update_layout(title=\"Samples vs theoretical PDF\", xaxis_title=\"x\", yaxis_title=\"density\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy exposes this distribution as `scipy.stats.exponweib` with methods:\n",
    "\n",
    "- `pdf`, `logpdf`, `cdf`, `sf`, `ppf`\n",
    "- `rvs` for random variates\n",
    "- `stats` / `moment` / `entropy`\n",
    "- `fit` for maximum-likelihood estimation\n",
    "\n",
    "A common workflow:\n",
    "\n",
    "1. Pick a parametric family (here, `exponweib`).\n",
    "2. Fit it to data.\n",
    "3. Check fit visually (histogram, QQ plot) and via likelihood-based criteria.\n",
    "4. Use the fitted model for inference or simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy usage: pdf/cdf/rvs/fit\n",
    "\n",
    "# 1) Create a frozen distribution\n",
    "rv = exponweib_dist(1.6, 1.2, loc=0.0, scale=2.0)\n",
    "\n",
    "xs = np.linspace(0, rv.ppf(0.99), 6)\n",
    "print(\"x grid:\", xs)\n",
    "print(\"pdf:\", rv.pdf(xs))\n",
    "print(\"cdf:\", rv.cdf(xs))\n",
    "\n",
    "# 2) Generate data\n",
    "x = rv.rvs(size=2_000, random_state=rng)\n",
    "\n",
    "# 3) Fit parameters (MLE)\n",
    "# For strictly-positive lifetime data it's often sensible to fix loc=0.\n",
    "a_hat, c_hat, loc_hat, scale_hat = exponweib_dist.fit(x, floc=0.0)\n",
    "print(\"\\nFitted params (floc=0):\")\n",
    "print({\"a\": a_hat, \"c\": c_hat, \"loc\": loc_hat, \"scale\": scale_hat})\n",
    "\n",
    "# 4) Compare to a Weibull fit (nested model a=1)\n",
    "a0, c0, loc0, scale0 = exponweib_dist.fit(x, fa=1.0, floc=0.0)\n",
    "print(\"\\nWeibull-as-exponweib (fa=1) fit:\")\n",
    "print({\"a\": a0, \"c\": c0, \"loc\": loc0, \"scale\": scale0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing (nested models)\n",
    "\n",
    "Because the Weibull distribution is the special case $a=1$, you can test\n",
    "\n",
    "- $H_0: a=1$ (Weibull) vs\n",
    "- $H_1: a \\ne 1$ (exponentiated Weibull)\n",
    "\n",
    "using a **likelihood ratio test (LRT)**:\n",
    "\n",
    "$$\\Lambda = 2\\bigl(\\ell(\\widehat{a},\\widehat{c},\\widehat{\\lambda}) - \\ell(a=1,\\widehat{c}_0,\\widehat{\\lambda}_0)\\bigr) \\;\\approx\\; \\chi^2_1.$$\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "\n",
    "Bayesian modeling treats $(a,c,\\lambda)$ as random and combines a prior with the likelihood:\n",
    "\n",
    "$$p(a,c,\\lambda\\mid x) \\propto p(x\\mid a,c,\\lambda)\\,p(a,c,\\lambda).$$\n",
    "\n",
    "There is no conjugacy here, but generic MCMC (e.g. Metropolis-Hastings) works well.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "\n",
    "Once fitted (frequentist or Bayesian), the distribution can be used to:\n",
    "\n",
    "- simulate lifetimes for stress testing\n",
    "- generate synthetic positive-valued data with realistic skew/tails\n",
    "- build parametric simulators inside larger pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Likelihood ratio test: Weibull (a=1) vs Exponentiated Weibull\n",
    "\n",
    "# Simulated data under the alternative\n",
    "true = dict(a=1.8, c=1.1, loc=0.0, scale=2.5)\n",
    "x = exponweib_dist.rvs(true[\"a\"], true[\"c\"], loc=true[\"loc\"], scale=true[\"scale\"], size=900, random_state=rng)\n",
    "\n",
    "# Fit alternative (a free) with loc fixed to 0\n",
    "(a1, c1, loc1, scale1) = exponweib_dist.fit(x, floc=0.0)\n",
    "ll1 = float(np.sum(exponweib_dist.logpdf(x, a1, c1, loc=0.0, scale=scale1)))\n",
    "\n",
    "# Fit null (a=1) with loc fixed to 0\n",
    "(a0, c0, loc0, scale0) = exponweib_dist.fit(x, fa=1.0, floc=0.0)\n",
    "ll0 = float(np.sum(exponweib_dist.logpdf(x, a0, c0, loc=0.0, scale=scale0)))\n",
    "\n",
    "lr_stat = 2.0 * (ll1 - ll0)\n",
    "p_value = chi2.sf(lr_stat, df=1)\n",
    "\n",
    "print(\"Alt fit:\", {\"a\": a1, \"c\": c1, \"scale\": scale1})\n",
    "print(\"Null fit:\", {\"a\": a0, \"c\": c0, \"scale\": scale0})\n",
    "print(f\"LR statistic={lr_stat:.3f}, p-value={p_value:.4g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian modeling (toy example): random-walk Metropolis on log-parameters\n",
    "\n",
    "x = exponweib_dist.rvs(1.8, 1.1, scale=2.5, size=500, random_state=rng)\n",
    "\n",
    "# Prior: independent normals on log-parameters (broad, weakly informative)\n",
    "prior_mu = np.array([0.0, 0.0, 0.0])\n",
    "prior_sigma = np.array([1.5, 1.5, 1.5])\n",
    "\n",
    "\n",
    "def log_prior(theta: np.ndarray) -> float:\n",
    "    d = theta.size\n",
    "    return float(\n",
    "        -0.5 * np.sum(((theta - prior_mu) / prior_sigma) ** 2)\n",
    "        - np.sum(np.log(prior_sigma))\n",
    "        - 0.5 * d * np.log(2 * np.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "def log_lik(theta: np.ndarray) -> float:\n",
    "    log_a, log_c, log_scale = theta\n",
    "    a, c, scale = np.exp([log_a, log_c, log_scale])\n",
    "    return float(np.sum(exponweib_dist.logpdf(x, a, c, loc=0.0, scale=scale)))\n",
    "\n",
    "\n",
    "def log_post(theta: np.ndarray) -> float:\n",
    "    return log_lik(theta) + log_prior(theta)\n",
    "\n",
    "\n",
    "n_steps = 6_000\n",
    "burn = 1_500\n",
    "step_scale = np.array([0.08, 0.08, 0.08])\n",
    "\n",
    "# Initialize at the MLE (good starting point)\n",
    "a_mle, c_mle, loc_mle, scale_mle = exponweib_dist.fit(x, floc=0.0)\n",
    "cur = np.log([a_mle, c_mle, scale_mle])\n",
    "cur_lp = log_post(cur)\n",
    "\n",
    "chain = np.empty((n_steps, 3), dtype=float)\n",
    "accepted = 0\n",
    "\n",
    "for t in range(n_steps):\n",
    "    prop = cur + step_scale * rng.normal(size=3)\n",
    "    prop_lp = log_post(prop)\n",
    "\n",
    "    if np.log(rng.uniform()) < (prop_lp - cur_lp):\n",
    "        cur, cur_lp = prop, prop_lp\n",
    "        accepted += 1\n",
    "\n",
    "    chain[t] = cur\n",
    "\n",
    "acc_rate = accepted / n_steps\n",
    "print(f\"Acceptance rate: {acc_rate:.3f}\")\n",
    "\n",
    "post = chain[burn:]\n",
    "a_s, c_s, scale_s = np.exp(post.T)\n",
    "\n",
    "summary = {\n",
    "    \"a_mean\": float(a_s.mean()),\n",
    "    \"c_mean\": float(c_s.mean()),\n",
    "    \"scale_mean\": float(scale_s.mean()),\n",
    "    \"a_95%\": tuple(np.quantile(a_s, [0.025, 0.975])),\n",
    "    \"c_95%\": tuple(np.quantile(c_s, [0.025, 0.975])),\n",
    "    \"scale_95%\": tuple(np.quantile(scale_s, [0.025, 0.975])),\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple posterior trace plots\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=a_s, mode=\"lines\", name=\"a\"))\n",
    "fig.update_layout(title=\"Posterior trace: a\", xaxis_title=\"iteration\", yaxis_title=\"a\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=c_s, mode=\"lines\", name=\"c\"))\n",
    "fig.update_layout(title=\"Posterior trace: c\", xaxis_title=\"iteration\", yaxis_title=\"c\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=scale_s, mode=\"lines\", name=\"scale\"))\n",
    "fig.update_layout(title=\"Posterior trace: scale\", xaxis_title=\"iteration\", yaxis_title=\"scale\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Generative modeling: posterior predictive vs fitted MLE\n",
    "\n",
    "# Use the same x from the Bayesian section\n",
    "\n",
    "# MLE predictive\n",
    "rv_mle = exponweib_dist(a_mle, c_mle, loc=0.0, scale=scale_mle)\n",
    "x_mle = rv_mle.rvs(size=15_000, random_state=rng)\n",
    "\n",
    "# Posterior predictive (draw parameters, then draw a sample)\n",
    "idx = rng.integers(0, len(a_s), size=400)\n",
    "pp_samples = []\n",
    "for i in idx:\n",
    "    rv_i = exponweib_dist(a_s[i], c_s[i], loc=0.0, scale=scale_s[i])\n",
    "    pp_samples.append(rv_i.rvs(size=40, random_state=rng))\n",
    "pp_samples = np.concatenate(pp_samples)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x, nbinsx=60, histnorm=\"probability density\", name=\"observed\", opacity=0.6))\n",
    "fig.add_trace(go.Histogram(x=x_mle, nbinsx=60, histnorm=\"probability density\", name=\"MLE predictive\", opacity=0.5))\n",
    "fig.add_trace(go.Histogram(x=pp_samples, nbinsx=60, histnorm=\"probability density\", name=\"Posterior predictive\", opacity=0.5))\n",
    "fig.update_layout(title=\"Observed vs predictive distributions\", barmode=\"overlay\", xaxis_title=\"x\", yaxis_title=\"density\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: require $a>0$, $c>0$, `scale>0`.\n",
    "- **Behavior at $x\\approx 0$**: depending on $c$ and $a$, the density may go to 0 or blow up; use `logpdf` for stability.\n",
    "- **CDF cancellation**: for very small $x$, $1-\\exp(-x^c)$ suffers cancellation; use `expm1` (`-expm1(-t)`).\n",
    "- **Inverse-CDF sampling**: avoid exact 0/1 uniforms; use `clip` and `log1p`.\n",
    "- **Fitting**: unconstrained `loc` can drift negative even for strictly-positive data; fix `floc=0` when appropriate.\n",
    "- **Moment calculations**: the binomial series can require many terms for extreme parameters; prefer SciPy numerical moments or Monte Carlo when in doubt.\n",
    "- **MGF existence**: for $c\\le 1$, the MGF may not exist for positive $t$; use the characteristic function or Laplace transform instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `exponweib` is a **continuous** distribution on $[0,\\infty)$ (standard form) with shape parameters $a>0$ and $c>0$.\n",
    "- It is defined by exponentiating a Weibull CDF: $F(x) = [1-\\exp(-x^c)]^a$ (with optional `loc`/`scale`).\n",
    "- It generalizes Weibull (`a=1`) and can represent diverse hazard shapes important in reliability/survival analysis.\n",
    "- Raw moments admit a useful **Gamma + binomial series** representation; SciPy provides numerical `stats` and `entropy`.\n",
    "- Sampling is easy via **inverse transform** and can be implemented with NumPy only.\n",
    "\n",
    "### References\n",
    "\n",
    "- SciPy documentation: `scipy.stats.exponweib`.\n",
    "- Wikipedia: “Exponentiated Weibull distribution”.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}