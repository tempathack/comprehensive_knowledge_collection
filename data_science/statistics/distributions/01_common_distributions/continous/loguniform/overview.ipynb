{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9e7c8b",
   "metadata": {},
   "source": [
    "# Loguniform Distribution — Uniform in Log Space\n",
    "\n",
    "The **loguniform distribution** (also called the **reciprocal distribution**) is a continuous distribution on a positive interval $[a,b]$ with density proportional to $1/x$.\n",
    "It models quantities that are *equally plausible across orders of magnitude*: the random variable is **uniform after taking a logarithm**.\n",
    "\n",
    "If $X \\sim \\mathrm{LogUniform}(a,b)$, then\n",
    "\n",
    "$$\\log X \\sim \\mathrm{Uniform}(\\log a,\\,\\log b).$$\n",
    "\n",
    "## What you’ll learn\n",
    "- what loguniform variables model and when they are a good (or bad) choice\n",
    "- PDF/CDF/quantile in closed form and the key identity: $\\log X$ is uniform\n",
    "- raw moments and derived mean/variance/skewness/kurtosis\n",
    "- MGF/CF via the exponential integral $\\mathrm{Ei}(\\cdot)$ and differential entropy\n",
    "- a **NumPy-only** sampler + Monte Carlo checks and plots\n",
    "- practical usage via `scipy.stats.loguniform` (`pdf`, `cdf`, `rvs`, `fit`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af463da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import stats, special\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24cf76e",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name:** loguniform (reciprocal) distribution\n",
    "- **Type:** continuous\n",
    "- **Support:** $x \\in [a,b]$\n",
    "- **Parameter space:** $0 < a < b < \\infty$\n",
    "\n",
    "Throughout, $\\log(\\cdot)$ denotes the natural logarithm (base $e$). Using a different base only rescales the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6af0f",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### 2.1 What it models\n",
    "A loguniform random variable puts **equal probability mass on equal multiplicative ranges**.\n",
    "For any factor $k>1$ and any $x$ such that $[x, kx] \\subseteq [a,b]$:\n",
    "\n",
    "$$\\mathbb{P}(x \\le X \\le kx) = \\frac{\\log k}{\\log(b/a)}.$$\n",
    "\n",
    "So the interval $[1,2]$ has the same probability as $[10,20]$ (same multiplicative width).\n",
    "\n",
    "### 2.2 Typical use cases\n",
    "- **Scale parameters across decades**: learning rates, regularization strengths, noise scales, physical constants known only up to order-of-magnitude.\n",
    "- **“Ignorance over scale” priors**: the (improper) Jeffreys prior for a positive scale is $p(\\theta) \\propto 1/\\theta$; truncating it to $[a,b]$ yields a proper loguniform prior.\n",
    "- **Sampling hyperparameters** in randomized search when the right scale is unknown.\n",
    "\n",
    "### 2.3 Relations to other distributions\n",
    "- If $Y = \\log X$ then $Y$ is **uniform**. This identity is often the easiest way to reason about loguniforms.\n",
    "- SciPy’s `loguniform(a,b)` is the same distribution as `reciprocal(a,b)`.\n",
    "- It is a **truncated** version of the scale-invariant density $1/x$ (which is not normalizable on $(0,\\infty)$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0084b",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $0 < a < b$. A random variable $X$ is loguniform on $[a,b]$ if its PDF is\n",
    "\n",
    "$$f(x; a,b) = \\frac{1}{x\\,\\log(b/a)}\\,\\mathbf{1}\\{a \\le x \\le b\\}.$$\n",
    "\n",
    "Its CDF is\n",
    "\n",
    "$$F(x; a,b) =\n",
    "\\begin{cases}\n",
    "0, & x < a, \\\\\n",
    "\\dfrac{\\log(x/a)}{\\log(b/a)}, & a \\le x \\le b, \\\\\n",
    "1, & x > b.\n",
    "\\end{cases}$$\n",
    "\n",
    "The quantile function (inverse CDF) is especially simple:\n",
    "\n",
    "$$F^{-1}(u) = a\\,(b/a)^u = \\exp\\big(\\log a + u(\\log b - \\log a)\\big), \\quad u\\in[0,1].$$\n",
    "\n",
    "Equivalently, if $U\\sim\\mathrm{Uniform}(0,1)$ then $X=F^{-1}(U)$ is loguniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_ab(a: float, b: float) -> None:\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    if not (np.isfinite(a) and np.isfinite(b)):\n",
    "        raise ValueError('a and b must be finite.')\n",
    "    if a <= 0:\n",
    "        raise ValueError('a must be > 0.')\n",
    "    if b <= a:\n",
    "        raise ValueError('b must be > a.')\n",
    "\n",
    "\n",
    "def loguniform_pdf(x: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    '''PDF of LogUniform(a,b): f(x)=1/(x*log(b/a)) on [a,b].'''\n",
    "    _check_ab(a, b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    log_b_over_a = np.log(b) - np.log(a)\n",
    "\n",
    "    pdf = np.zeros_like(x, dtype=float)\n",
    "    mask = (x >= a) & (x <= b)\n",
    "    pdf[mask] = 1.0 / (x[mask] * log_b_over_a)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def loguniform_logpdf(x: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    '''Log-PDF of LogUniform(a,b) on [a,b] (returns -inf outside support).'''\n",
    "    _check_ab(a, b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    log_b_over_a = np.log(b) - np.log(a)\n",
    "\n",
    "    logpdf = np.full_like(x, fill_value=-np.inf, dtype=float)\n",
    "    mask = (x >= a) & (x <= b)\n",
    "    logpdf[mask] = -np.log(x[mask]) - np.log(log_b_over_a)\n",
    "    return logpdf\n",
    "\n",
    "\n",
    "def loguniform_cdf(x: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    '''CDF of LogUniform(a,b).'''\n",
    "    _check_ab(a, b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    log_b_over_a = np.log(b) - np.log(a)\n",
    "\n",
    "    cdf = np.zeros_like(x, dtype=float)\n",
    "    cdf[x >= b] = 1.0\n",
    "\n",
    "    mask = (x >= a) & (x < b)\n",
    "    cdf[mask] = (np.log(x[mask]) - np.log(a)) / log_b_over_a\n",
    "    return cdf\n",
    "\n",
    "\n",
    "def loguniform_ppf(u: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    '''Quantile function (inverse CDF) of LogUniform(a,b).'''\n",
    "    _check_ab(a, b)\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    if np.any((u < 0) | (u > 1)):\n",
    "        raise ValueError('u must be in [0,1].')\n",
    "\n",
    "    log_a, log_b = np.log(a), np.log(b)\n",
    "    return np.exp(log_a + u * (log_b - log_a))\n",
    "\n",
    "\n",
    "# Quick sanity check: PDF integrates to ~1\n",
    "a0, b0 = 1e-2, 1e2\n",
    "xgrid = np.geomspace(a0, b0, 200_000)\n",
    "area = np.trapz(loguniform_pdf(xgrid, a0, b0), xgrid)\n",
    "area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c337a",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### 4.1 Mean, variance, skewness, kurtosis\n",
    "Let $L = \\log(b/a)$. For any $k\\ne 0$, the **raw moment** has a simple form:\n",
    "\n",
    "$$\\mathbb{E}[X^k] = \\int_a^b x^k\\,\\frac{1}{xL}\\,dx = \\frac{b^k - a^k}{kL}.$$\n",
    "\n",
    "In particular:\n",
    "\n",
    "- **Mean**\n",
    "  $$\\mathbb{E}[X] = \\frac{b-a}{\\log(b/a)}.$$\n",
    "\n",
    "- **Second raw moment**\n",
    "  $$\\mathbb{E}[X^2] = \\frac{b^2-a^2}{2\\log(b/a)}.$$\n",
    "\n",
    "- **Variance**\n",
    "  $$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2.$$\n",
    "\n",
    "Higher standardized moments can be computed from raw moments. Let $m_j = \\mathbb{E}[X^j]$.\n",
    "Define central moments\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_2 &= m_2 - m_1^2,\\\\\n",
    "\\mu_3 &= m_3 - 3m_1m_2 + 2m_1^3,\\\\\n",
    "\\mu_4 &= m_4 - 4m_1m_3 + 6m_1^2m_2 - 3m_1^4.\n",
    "\\end{align}\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\text{skewness }\\gamma_1 = \\frac{\\mu_3}{\\mu_2^{3/2}}, \\qquad \\text{excess kurtosis }\\gamma_2 = \\frac{\\mu_4}{\\mu_2^{2}} - 3.$$\n",
    "\n",
    "### 4.2 MGF and characteristic function\n",
    "Because the support is bounded, the MGF exists for all real $t$.\n",
    "Using the exponential integral $\\mathrm{Ei}(z)$ with $\\frac{d}{dz}\\mathrm{Ei}(z)=\\frac{e^z}{z}$,\n",
    "\n",
    "$$M_X(t) = \\mathbb{E}[e^{tX}] = \\frac{\\mathrm{Ei}(tb) - \\mathrm{Ei}(ta)}{\\log(b/a)}, \\quad t\\ne 0,$$\n",
    "\n",
    "and $M_X(0)=1$ by continuity.\n",
    "The characteristic function is\n",
    "\n",
    "$$\\varphi_X(t) = M_X(it) = \\frac{\\mathrm{Ei}(itb) - \\mathrm{Ei}(ita)}{\\log(b/a)}.$$\n",
    "\n",
    "### 4.3 Differential entropy\n",
    "The differential entropy (in **nats**) is\n",
    "\n",
    "\\begin{align}\n",
    "H(X) &= -\\mathbb{E}[\\log f(X)]\n",
    "= \\mathbb{E}[\\log X] + \\log\\big(\\log(b/a)\\big)\\\\\n",
    "&= \\frac{\\log a + \\log b}{2} + \\log\\big(\\log(b/a)\\big).\n",
    "\\end{align}\n",
    "\n",
    "(Use $\\log_2$ for bits: $H_{\\text{bits}} = H/\\log 2$.)\n",
    "\n",
    "### 4.4 Other useful properties\n",
    "- **Median / geometric mean:** since $\\log X$ is uniform, $\\mathrm{median}(X)=\\sqrt{ab}$.\n",
    "- **Scale equivariance:** if $c>0$ and $X\\sim\\mathrm{LogUniform}(a,b)$ then $cX\\sim\\mathrm{LogUniform}(ca,cb)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2916a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loguniform_raw_moment(k: float, a: float, b: float) -> float:\n",
    "    '''Raw moment E[X^k] for X~LogUniform(a,b).\n",
    "\n",
    "    Uses log-space to avoid forming (b/a) directly.\n",
    "    '''\n",
    "    _check_ab(a, b)\n",
    "    k = float(k)\n",
    "    if k == 0:\n",
    "        return 1.0\n",
    "\n",
    "    log_a, log_b = np.log(a), np.log(b)\n",
    "    L = log_b - log_a\n",
    "\n",
    "    return (np.exp(k * log_b) - np.exp(k * log_a)) / (k * L)\n",
    "\n",
    "\n",
    "def loguniform_entropy(a: float, b: float) -> float:\n",
    "    '''Differential entropy in nats.'''\n",
    "    _check_ab(a, b)\n",
    "    log_a, log_b = np.log(a), np.log(b)\n",
    "    L = log_b - log_a\n",
    "    return 0.5 * (log_a + log_b) + np.log(L)\n",
    "\n",
    "\n",
    "def loguniform_mgf(t: np.ndarray, a: float, b: float):\n",
    "    '''MGF M(t)=E[e^{tX}] via the exponential integral Ei.'''\n",
    "    _check_ab(a, b)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    L = np.log(b) - np.log(a)\n",
    "\n",
    "    out = (special.expi(t * b) - special.expi(t * a)) / L\n",
    "    out = np.where(t == 0, 1.0, out)\n",
    "    return out.item() if out.ndim == 0 else out\n",
    "\n",
    "\n",
    "def loguniform_cf(t: np.ndarray, a: float, b: float):\n",
    "    '''Characteristic function φ(t)=E[e^{itX}] via Ei.'''\n",
    "    _check_ab(a, b)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    L = np.log(b) - np.log(a)\n",
    "\n",
    "    z = 1j * t\n",
    "    out = (special.expi(z * b) - special.expi(z * a)) / L\n",
    "    out = np.where(t == 0, 1.0 + 0j, out)\n",
    "    return out.item() if out.ndim == 0 else out\n",
    "\n",
    "\n",
    "def loguniform_moments(a: float, b: float) -> dict:\n",
    "    '''Mean/variance/skewness/excess kurtosis + entropy + MGF/CF callables.'''\n",
    "    _check_ab(a, b)\n",
    "\n",
    "    m1 = loguniform_raw_moment(1, a, b)\n",
    "    m2 = loguniform_raw_moment(2, a, b)\n",
    "    m3 = loguniform_raw_moment(3, a, b)\n",
    "    m4 = loguniform_raw_moment(4, a, b)\n",
    "\n",
    "    var = m2 - m1**2\n",
    "    mu3 = m3 - 3 * m1 * m2 + 2 * m1**3\n",
    "    mu4 = m4 - 4 * m1 * m3 + 6 * (m1**2) * m2 - 3 * m1**4\n",
    "\n",
    "    skew = mu3 / (var ** 1.5)\n",
    "    excess_kurt = mu4 / (var**2) - 3.0\n",
    "\n",
    "    mgf = lambda t: loguniform_mgf(t, a, b)\n",
    "    cf = lambda t: loguniform_cf(t, a, b)\n",
    "\n",
    "    return {\n",
    "        'mean': m1,\n",
    "        'var': var,\n",
    "        'skew': skew,\n",
    "        'excess_kurtosis': excess_kurt,\n",
    "        'median': np.sqrt(a * b),\n",
    "        'entropy': loguniform_entropy(a, b),\n",
    "        'mgf': mgf,\n",
    "        'cf': cf,\n",
    "    }\n",
    "\n",
    "\n",
    "m = loguniform_moments(a0, b0)\n",
    "{k: v for k, v in m.items() if k not in {'mgf', 'cf'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124506e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check (using SciPy's sampler)\n",
    "n = 200_000\n",
    "samples_scipy = stats.loguniform(a0, b0).rvs(size=n, random_state=rng)\n",
    "\n",
    "mc_mean = samples_scipy.mean()\n",
    "mc_var = samples_scipy.var(ddof=0)\n",
    "mc_skew = stats.skew(samples_scipy, bias=True)\n",
    "mc_excess_kurt = stats.kurtosis(samples_scipy, fisher=True, bias=True)\n",
    "\n",
    "# MGF at a couple of small t (avoid overflow from e^{tX} when b is large)\n",
    "t1, t2 = 0.05, -0.05\n",
    "mc_mgf_t1 = np.mean(np.exp(t1 * samples_scipy))\n",
    "mc_mgf_t2 = np.mean(np.exp(t2 * samples_scipy))\n",
    "\n",
    "(\n",
    "    m['mean'],\n",
    "    mc_mean,\n",
    "    m['var'],\n",
    "    mc_var,\n",
    "    m['skew'],\n",
    "    mc_skew,\n",
    "    m['excess_kurtosis'],\n",
    "    mc_excess_kurt,\n",
    "    m['mgf'](t1),\n",
    "    mc_mgf_t1,\n",
    "    m['mgf'](t2),\n",
    "    mc_mgf_t2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5e1cc",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The parameters are direct:\n",
    "\n",
    "- $a$ is the **lower cutoff** (minimum possible value).\n",
    "- $b$ is the **upper cutoff** (maximum possible value).\n",
    "\n",
    "Two derived quantities are often more interpretable:\n",
    "\n",
    "- the **geometric mean** $\\sqrt{ab}$ (sets the *typical scale*), and\n",
    "- the **log-width** $L = \\log(b/a)$ (sets the spread in *orders of magnitude*).\n",
    "\n",
    "Key intuition:\n",
    "\n",
    "- On a **linear** axis, the PDF decreases as $1/x$, so the distribution looks heavily concentrated near $a$.\n",
    "- On a **log** axis, the distribution is **flat**.\n",
    "\n",
    "The probability of landing in a fixed multiplicative band depends only on the ratio $b/a$:\n",
    "\n",
    "$$\\mathbb{P}(x \\le X \\le kx) = \\frac{\\log k}{\\log(b/a)}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72899394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same lower bound a, different upper bounds b\n",
    "params = [(1e-2, 1e0), (1e-2, 1e2), (1e-2, 1e4)]\n",
    "a_min = min(a for a, _ in params)\n",
    "b_max = max(b for _, b in params)\n",
    "\n",
    "x = np.geomspace(a_min, b_max, 2000)\n",
    "\n",
    "fig = go.Figure()\n",
    "for a, b in params:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=loguniform_pdf(x, a, b),\n",
    "            mode='lines',\n",
    "            name=f'a={a:g}, b={b:g}',\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Loguniform PDF: effect of widening [a,b]',\n",
    "    yaxis_title='density',\n",
    "    legend_title='parameters',\n",
    ")\n",
    "fig.update_xaxes(title='x', type='log')\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In log-space the density is constant\n",
    "# If Y = log X then Y ~ Uniform(log a, log b).\n",
    "\n",
    "fig = go.Figure()\n",
    "for a, b in params:\n",
    "    log_a, log_b = np.log(a), np.log(b)\n",
    "    y = np.linspace(log_a, log_b, 200)\n",
    "    const = 1.0 / (log_b - log_a)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y,\n",
    "            y=np.full_like(y, const),\n",
    "            mode='lines',\n",
    "            name=f'log a={log_a:.2f}, log b={log_b:.2f}',\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Density of Y = log X is uniform',\n",
    "    xaxis_title='y = log x',\n",
    "    yaxis_title='density',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37374c9",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "Using the PDF $f(x)=\\frac{1}{x\\log(b/a)}$ on $[a,b]$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[X]\n",
    "&= \\int_a^b x\\,f(x)\\,dx\n",
    "= \\int_a^b x\\,\\frac{1}{x\\log(b/a)}\\,dx \\\\\n",
    "&= \\frac{1}{\\log(b/a)}\\int_a^b 1\\,dx\n",
    "= \\frac{b-a}{\\log(b/a)}.\n",
    "\\end{align}\n",
    "\n",
    "More generally, for $k\\ne 0$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[X^k]\n",
    "&= \\int_a^b x^k\\,\\frac{1}{x\\log(b/a)}\\,dx\n",
    "= \\frac{1}{\\log(b/a)}\\int_a^b x^{k-1}\\,dx \\\\\n",
    "&= \\frac{b^k-a^k}{k\\log(b/a)}.\n",
    "\\end{align}\n",
    "\n",
    "### 6.2 Variance\n",
    "Compute the second raw moment and subtract the squared mean:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[X^2] &= \\frac{b^2-a^2}{2\\log(b/a)}, \\\\\n",
    "\\mathrm{Var}(X) &= \\mathbb{E}[X^2] - \\big(\\mathbb{E}[X]\\big)^2.\n",
    "\\end{align}\n",
    "\n",
    "### 6.3 Likelihood (and the MLE sits on the boundary)\n",
    "Given i.i.d. data $x_1,\\dots,x_n$ and parameters $(a,b)$, the likelihood is\n",
    "\n",
    "$$\\mathcal{L}(a,b) = \\prod_{i=1}^n \\frac{1}{x_i\\,\\log(b/a)}\\;\\mathbf{1}\\{a\\le x_i\\le b\\}.$$\n",
    "\n",
    "Let $x_{(1)}=\\min_i x_i$ and $x_{(n)}=\\max_i x_i$.\n",
    "The indicator is nonzero exactly when $a \\le x_{(1)}$ and $b \\ge x_{(n)}$.\n",
    "Within that feasible region, the log-likelihood is\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(a,b)\n",
    "&= \\sum_{i=1}^n \\Big(-\\log x_i - \\log\\log(b/a)\\Big) \\\\\n",
    "&= -\\sum_{i=1}^n \\log x_i\\; -\\; n\\log\\log(b/a).\n",
    "\\end{align}\n",
    "\n",
    "The data term $-\\sum \\log x_i$ does not depend on $(a,b)$. So maximizing $\\ell$ means **minimizing** $\\log(b/a)$ subject to the constraints.\n",
    "That pushes $a$ up to $x_{(1)}$ and $b$ down to $x_{(n)}$:\n",
    "\n",
    "$$\\hat a_{\\text{MLE}} = x_{(1)}, \\qquad \\hat b_{\\text{MLE}} = x_{(n)}.$$\n",
    "\n",
    "This “support-parameter MLE” behavior is common in bounded-support distributions and is a practical reason to prefer Bayesian or penalized approaches when you need uncertainty on $a,b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01424aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loguniform_loglik(data: np.ndarray, a: float, b: float) -> float:\n",
    "    '''Log-likelihood for i.i.d. data under LogUniform(a,b).'''\n",
    "    _check_ab(a, b)\n",
    "    x = np.asarray(data, dtype=float)\n",
    "    if np.any((x < a) | (x > b)):\n",
    "        return -np.inf\n",
    "\n",
    "    n = x.size\n",
    "    L = np.log(b) - np.log(a)\n",
    "    return -np.sum(np.log(x)) - n * np.log(L)\n",
    "\n",
    "\n",
    "data = stats.loguniform(a0, b0).rvs(size=2_000, random_state=rng)\n",
    "\n",
    "a_hat = data.min()\n",
    "b_hat = data.max()\n",
    "\n",
    "ll_mle = loguniform_loglik(data, a_hat, b_hat)\n",
    "ll_expanded = loguniform_loglik(data, 0.9 * a_hat, 1.1 * b_hat)\n",
    "ll_shrunk_invalid = loguniform_loglik(data, 1.1 * a_hat, 0.9 * b_hat)  # infeasible\n",
    "\n",
    "ll_mle, ll_expanded, ll_shrunk_invalid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0797620",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### 7.1 Algorithm (inverse-CDF / log-space uniform)\n",
    "From the quantile function,\n",
    "\n",
    "$$X = F^{-1}(U) = a\\,(b/a)^U, \\quad U\\sim\\mathrm{Uniform}(0,1).$$\n",
    "\n",
    "A numerically stable way is to work in log-space:\n",
    "\n",
    "$$\\log X = \\log a + U(\\log b - \\log a), \\qquad X = \\exp(\\log X).$$\n",
    "\n",
    "This avoids computing $b/a$ directly (which can overflow when $b/a$ is huge).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26713cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loguniform_rvs_numpy(a: float, b: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''NumPy-only sampling from LogUniform(a,b) via inverse CDF in log-space.'''\n",
    "    _check_ab(a, b)\n",
    "    u = rng.uniform(0.0, 1.0, size=size)\n",
    "    log_a, log_b = np.log(a), np.log(b)\n",
    "    return np.exp(log_a + u * (log_b - log_a))\n",
    "\n",
    "\n",
    "samples_np = loguniform_rvs_numpy(a0, b0, size=200_000, rng=rng)\n",
    "\n",
    "# Monte Carlo check vs theory\n",
    "(\n",
    "    m['mean'],\n",
    "    samples_np.mean(),\n",
    "    m['var'],\n",
    "    samples_np.var(ddof=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500134df",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "\n",
    "- the **PDF** on a log-x axis (where the shape is easier to interpret),\n",
    "- the **CDF** and an empirical CDF from Monte Carlo samples,\n",
    "- the fact that $\\log X$ is uniform by plotting a histogram of $\\log X$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, b1 = 1e-3, 1e3\n",
    "samples = loguniform_rvs_numpy(a1, b1, size=60_000, rng=rng)\n",
    "\n",
    "x = np.geomspace(a1, b1, 1500)\n",
    "\n",
    "# PDF + histogram (log-x axis)\n",
    "fig_pdf = go.Figure()\n",
    "fig_pdf.add_trace(go.Scatter(x=x, y=loguniform_pdf(x, a1, b1), mode='lines', name='theory'))\n",
    "fig_pdf.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples,\n",
    "        nbinsx=70,\n",
    "        histnorm='probability density',\n",
    "        name='samples',\n",
    "        opacity=0.55,\n",
    "    )\n",
    ")\n",
    "fig_pdf.update_layout(title='Loguniform PDF (log-x axis)', yaxis_title='density')\n",
    "fig_pdf.update_xaxes(title='x', type='log')\n",
    "fig_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ef86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF + empirical CDF\n",
    "xs = np.sort(samples)\n",
    "ecdf = np.arange(1, xs.size + 1) / xs.size\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=x, y=loguniform_cdf(x, a1, b1), mode='lines', name='theory'))\n",
    "fig_cdf.add_trace(go.Scatter(x=xs, y=ecdf, mode='lines', name='empirical', line=dict(width=1)))\n",
    "fig_cdf.update_layout(title='CDF vs empirical CDF', yaxis_title='F(x)')\n",
    "fig_cdf.update_xaxes(title='x', type='log')\n",
    "fig_cdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check uniformity in log-space\n",
    "log_samples = np.log(samples)\n",
    "log_a, log_b = np.log(a1), np.log(b1)\n",
    "\n",
    "fig_log = px.histogram(\n",
    "    x=log_samples,\n",
    "    nbins=60,\n",
    "    histnorm='probability density',\n",
    "    title='Histogram of log(X) (should be uniform)',\n",
    "    labels={'x': 'log x'},\n",
    ")\n",
    "\n",
    "const = 1.0 / (log_b - log_a)\n",
    "fig_log.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[log_a, log_b],\n",
    "        y=[const, const],\n",
    "        mode='lines',\n",
    "        name='uniform density',\n",
    "    )\n",
    ")\n",
    "fig_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b99b0",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.loguniform`)\n",
    "\n",
    "SciPy provides `scipy.stats.loguniform` with shape parameters `(a, b)`.\n",
    "It is equivalent to `scipy.stats.reciprocal(a, b)`.\n",
    "\n",
    "Common methods:\n",
    "\n",
    "- `pdf(x)`, `logpdf(x)`\n",
    "- `cdf(x)`, `ppf(q)`\n",
    "- `rvs(size=..., random_state=...)`\n",
    "- `fit(data, ...)` (MLE; note `loc/scale` exist as generic wrappers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2391632",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.loguniform(a0, b0)  # loc=0, scale=1 by default\n",
    "x_test = np.array([a0, np.sqrt(a0 * b0), b0])\n",
    "\n",
    "pdf = dist.pdf(x_test)\n",
    "cdf = dist.cdf(x_test)\n",
    "ppf = dist.ppf(np.array([0.0, 0.5, 1.0]))\n",
    "samples_scipy_small = dist.rvs(size=5, random_state=rng)\n",
    "\n",
    "pdf, cdf, ppf, samples_scipy_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalence to reciprocal\n",
    "recip = stats.reciprocal(a0, b0)\n",
    "np.allclose(dist.pdf(xgrid), recip.pdf(xgrid)), np.allclose(dist.cdf(xgrid), recip.cdf(xgrid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting (MLE) with SciPy\n",
    "# If you KNOW loc=0 and scale=1 are appropriate, it's common to fix them.\n",
    "\n",
    "data_fit = dist.rvs(size=20_000, random_state=rng)\n",
    "a_hat, b_hat, loc_hat, scale_hat = stats.loguniform.fit(data_fit, floc=0, fscale=1)\n",
    "\n",
    "(a_hat, b_hat, loc_hat, scale_hat), (data_fit.min(), data_fit.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdcfa38",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing\n",
    "Because $\\log X$ is uniform, goodness-of-fit testing for loguniformity can be done by transforming the data:\n",
    "\n",
    "1. compute $y_i = \\log x_i$\n",
    "2. test whether $y_i$ are uniform on $[\\log a, \\log b]$ (e.g., KS test)\n",
    "\n",
    "If $(a,b)$ are estimated from the same data, the usual KS p-value is no longer exact (the null distribution changes).\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "Loguniform priors are popular for **positive scale parameters** when you want a prior that is roughly “uninformative over orders of magnitude”.\n",
    "A common pattern is a truncated Jeffreys prior:\n",
    "\n",
    "$$\\sigma \\sim \\mathrm{LogUniform}(a,b), \\qquad p(\\sigma) = \\frac{1}{\\sigma\\log(b/a)}\\,\\mathbf{1}\\{a\\le\\sigma\\le b\\}.$$\n",
    "\n",
    "### 10.3 Generative modeling / simulation\n",
    "When synthetic data require randomly varying scales (noise levels, frequencies, step sizes), drawing those scales loguniformly is often more realistic than drawing them uniformly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Hypothesis test demo: KS test in log-space (parameters known)\n",
    "a_test, b_test = 1e-3, 1e2\n",
    "x = stats.loguniform(a_test, b_test).rvs(size=2_000, random_state=rng)\n",
    "y = np.log(x)\n",
    "\n",
    "u = stats.uniform(loc=np.log(a_test), scale=np.log(b_test) - np.log(a_test))\n",
    "D, p_value = stats.kstest(y, u.cdf)\n",
    "D, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8223543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian modeling demo: loguniform prior on sigma in a Normal(0, sigma^2)\n",
    "\n",
    "sigma_true = 0.3\n",
    "n = 200\n",
    "obs = rng.normal(0.0, sigma_true, size=n)\n",
    "S = np.sum(obs**2)\n",
    "\n",
    "sigma_min, sigma_max = 1e-3, 3.0\n",
    "sigma_grid = np.geomspace(sigma_min, sigma_max, 2000)\n",
    "\n",
    "# Proper loguniform prior on [sigma_min, sigma_max]\n",
    "log_prior = -np.log(sigma_grid) - np.log(np.log(sigma_max / sigma_min))\n",
    "\n",
    "# Log-likelihood up to an additive constant (drop -(n/2)log(2π))\n",
    "log_lik = -n * np.log(sigma_grid) - 0.5 * S / (sigma_grid**2)\n",
    "\n",
    "log_post = log_prior + log_lik\n",
    "post_unnorm = np.exp(log_post - log_post.max())\n",
    "Z = np.trapz(post_unnorm, sigma_grid)\n",
    "post = post_unnorm / Z\n",
    "\n",
    "# Posterior mean and a 90% credible interval\n",
    "cdf = np.cumsum(0.5 * (post[1:] + post[:-1]) * np.diff(sigma_grid))\n",
    "cdf = np.concatenate([[0.0], cdf])\n",
    "\n",
    "q05 = np.interp(0.05, cdf, sigma_grid)\n",
    "q95 = np.interp(0.95, cdf, sigma_grid)\n",
    "post_mean = np.trapz(sigma_grid * post, sigma_grid)\n",
    "post_mode = sigma_grid[np.argmax(post)]\n",
    "\n",
    "post_mean, post_mode, (q05, q95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_post = go.Figure()\n",
    "fig_post.add_trace(go.Scatter(x=sigma_grid, y=post, mode='lines', name='posterior'))\n",
    "fig_post.add_vline(x=sigma_true, line_dash='dash', line_color='black', annotation_text='true σ')\n",
    "fig_post.add_vrect(x0=q05, x1=q95, opacity=0.15, fillcolor='blue', line_width=0, annotation_text='90% CI')\n",
    "\n",
    "fig_post.update_layout(title='Posterior over σ with LogUniform prior', yaxis_title='density')\n",
    "fig_post.update_xaxes(title='σ', type='log')\n",
    "fig_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Generative modeling / hyperparameter sampling: learning rates across decades\n",
    "lr = loguniform_rvs_numpy(1e-5, 1e-1, size=10_000, rng=rng)\n",
    "\n",
    "fig_lr = px.histogram(\n",
    "    x=np.log10(lr),\n",
    "    nbins=60,\n",
    "    title='log10(learning rate) sampled LogUniform(1e-5, 1e-1)',\n",
    "    labels={'x': 'log10(lr)'},\n",
    ")\n",
    "fig_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995694ac",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter validity:** must have $0<a<b$. In code, guard against $a\\le 0$ or $b\\le a$.\n",
    "- **“Looks concentrated near $a$” on linear plots:** that’s expected; the distribution is flat in log-space. Plot on a log-x axis.\n",
    "- **Huge ratios:** avoid computing $(b/a)^u$ directly when $b/a$ is extremely large; use\n",
    "  $$X = \\exp\\big(\\log a + u(\\log b - \\log a)\\big).$$\n",
    "- **Fitting caveat:** because $a,b$ set the support, the MLE sits at the sample min/max. That can be unstable if you expect measurement noise or outliers.\n",
    "- **`loc`/`scale` in SciPy:** `loguniform` supports generic `loc/scale`, but shifting a loguniform generally breaks the “uniform-in-log” interpretation; keep `loc=0` unless you truly want that transformed distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734530c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical pitfall demo: power form can overflow when b/a is huge\n",
    "\n",
    "a_big, b_big = 1e-200, 1e200\n",
    "u = rng.uniform(size=5)\n",
    "\n",
    "x_logspace = np.exp(np.log(a_big) + u * (np.log(b_big) - np.log(a_big)))\n",
    "\n",
    "# This overflows because (b/a) is inf in float64\n",
    "x_power = a_big * (b_big / a_big) ** u\n",
    "\n",
    "x_logspace, x_power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499bf1f",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- Loguniform is a **continuous** distribution on $[a,b]$ with $f(x) \\propto 1/x$.\n",
    "- The key identity is $\\log X \\sim \\mathrm{Uniform}(\\log a,\\log b)$, giving simple CDF/quantiles and an intuitive “uniform across orders of magnitude” interpretation.\n",
    "- Raw moments are closed form: $\\mathbb{E}[X^k]=\\frac{b^k-a^k}{k\\log(b/a)}$.\n",
    "- The MGF/CF involve the exponential integral $\\mathrm{Ei}$; entropy is $\\tfrac{1}{2}(\\log a+\\log b)+\\log\\log(b/a)$.\n",
    "- Sampling is easy with inverse CDF in log-space, and SciPy provides `scipy.stats.loguniform` (alias `reciprocal`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
