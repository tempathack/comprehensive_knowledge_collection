{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b858fa56",
   "metadata": {},
   "source": [
    "# Truncated Normal distribution (`truncnorm`)\n",
    "\n",
    "A **truncated normal** is what you get when a normal random variable is **restricted to an interval**. Formally, it’s a normal distribution *conditioned* on lying between a lower bound and an upper bound.\n",
    "\n",
    "It’s a natural model whenever the *untruncated* normal story is reasonable (additive noise, central-limit behavior), but the variable is **physically or logically constrained**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Title & classification\n",
    "\n",
    "| Item | Value |\n",
    "|---|---|\n",
    "| Name | Truncated normal (`truncnorm`) |\n",
    "| Type | **Continuous** |\n",
    "| Support | $x \\in [\\ell, u]$ with $\\ell < u$ (bounds may be $\\pm\\infty$) |\n",
    "| Parameters | location $\\mu\\in\\mathbb{R}$, scale $\\sigma>0$, bounds $\\ell<u$ |\n",
    "| SciPy shape params | $a = (\\ell-\\mu)/\\sigma$, $b=(u-\\mu)/\\sigma$ with $a<b$ |\n",
    "\n",
    "A compact way to write the model is:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\;\\big|\\; \\ell \\le X \\le u.\n",
    "$$\n",
    "\n",
    "### What you’ll be able to do after this notebook\n",
    "\n",
    "- write the PDF/CDF of a truncated normal and map parameters to SciPy\n",
    "- compute mean/variance/skewness/kurtosis (and understand *why* the mean shifts)\n",
    "- interpret how $(\\mu,\\sigma,\\ell,u)$ change the shape\n",
    "- derive the log-likelihood and fit $(\\mu,\\sigma)$ by MLE when truncation is known\n",
    "- sample from a truncated normal using **NumPy only** (rejection sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659da899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import optimize, special, stats\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f10d5",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "A truncation happens when values outside a range are **impossible to observe** (or are removed from the population).\n",
    "For example, imagine an underlying measurement model:\n",
    "\n",
    "$$Y = \\mu + \\sigma Z,\\quad Z\\sim\\mathcal{N}(0,1),$$\n",
    "\n",
    "but the process can only produce values in $[\\ell,u]$.\n",
    "Then the observed variable is the conditional distribution:\n",
    "\n",
    "$$X = Y\\mid (\\ell\\le Y \\le u).$$\n",
    "\n",
    "This is *not* the same as **clipping** (censoring) where out-of-range values are recorded at the boundary.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Physical constraints**: lengths, concentrations, or prices that cannot go negative (often $\\ell=0$).\n",
    "- **Quality control / selection bias**: only items within spec are kept; others are discarded.\n",
    "- **Psychometrics / surveys**: scores restricted to a finite scale (e.g. 0–100).\n",
    "- **Truncated regression**: outcomes are observed only when they fall within a range (distinct from Tobit censoring).\n",
    "- **Bayesian priors with constraints**: a normal prior truncated to enforce bounds.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- As $\\ell\\to-\\infty$ and $u\\to\\infty$, the truncated normal becomes an ordinary **normal**.\n",
    "- One-sided truncation with $(\\ell=0,\\;u=\\infty)$ yields a **half-normal** when $\\mu=0$ (up to reparameterization).\n",
    "- Truncation is a generic operation: many “bounded” distributions are best understood as **conditional** versions of simpler base models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f16eda",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "Let $\\varphi$ and $\\Phi$ denote the standard normal PDF and CDF:\n",
    "\n",
    "$$\n",
    "\\varphi(z)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right),\n",
    "\\qquad\n",
    "\\Phi(z)=\\int_{-\\infty}^{z}\\varphi(t)\\,dt.\n",
    "$$\n",
    "\n",
    "Define standardized truncation points\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{\\ell-\\mu}{\\sigma},\n",
    "\\qquad\n",
    "\\beta = \\frac{u-\\mu}{\\sigma},\n",
    "\\qquad\n",
    "Z = \\Phi(\\beta)-\\Phi(\\alpha).\n",
    "$$\n",
    "\n",
    "Here $Z$ is the **normalization constant** (the probability that an untruncated normal lands in $[\\ell,u]$).\n",
    "\n",
    "### PDF\n",
    "\n",
    "For $x\\in[\\ell,u]$:\n",
    "\n",
    "$$\n",
    " f(x\\mid\\mu,\\sigma,\\ell,u)\n",
    "= \\frac{1}{\\sigma\\,Z}\\;\\varphi\\!\\left(\\frac{x-\\mu}{\\sigma}\\right),\n",
    "\\qquad\n",
    "f(x)=0\\;\\text{outside }[\\ell,u].\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "$$\n",
    "F(x)=\n",
    "\\begin{cases}\n",
    "0,& x<\\ell,\\\\\n",
    "\\dfrac{\\Phi\\bigl((x-\\mu)/\\sigma\\bigr)-\\Phi(\\alpha)}{Z},& \\ell\\le x\\le u,\\\\\n",
    "1,& x>u.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### SciPy parameterization\n",
    "\n",
    "`scipy.stats.truncnorm` uses standardized bounds $(a,b)$ plus `loc` and `scale`:\n",
    "\n",
    "$$\n",
    "(a,b,\\texttt{loc},\\texttt{scale})\n",
    "=\\Bigl(\\alpha,\\beta,\\mu,\\sigma\\Bigr).\n",
    "$$\n",
    "\n",
    "So for known bounds $[\\ell,u]$ you typically compute:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "a = (lower - mu) / sigma\n",
    "b = (upper - mu) / sigma\n",
    "rv = stats.truncnorm(a, b, loc=mu, scale=sigma)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2de1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SQRT_2PI = 0.5 * np.log(2 * np.pi)\n",
    "\n",
    "\n",
    "def norm_logpdf(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return -0.5 * z**2 - LOG_SQRT_2PI\n",
    "\n",
    "\n",
    "def norm_pdf(z):\n",
    "    return np.exp(norm_logpdf(z))\n",
    "\n",
    "\n",
    "def norm_cdf(z):\n",
    "    # SciPy's ndtr is a fast and numerically stable normal CDF implementation\n",
    "    return special.ndtr(z)\n",
    "\n",
    "\n",
    "def norm_logcdf(z):\n",
    "    # log Phi(z), stable for large negative z\n",
    "    return special.log_ndtr(z)\n",
    "\n",
    "\n",
    "def _logdiffexp(log_a, log_b):\n",
    "    \"\"\"Compute log(exp(log_a) - exp(log_b)) for log_a >= log_b.\"\"\"\n",
    "    return log_a + np.log1p(-np.exp(log_b - log_a))\n",
    "\n",
    "\n",
    "def truncnorm_standardized_bounds(mu, sigma, lower, upper):\n",
    "    mu = float(mu)\n",
    "    sigma = float(sigma)\n",
    "    lower = float(lower)\n",
    "    upper = float(upper)\n",
    "\n",
    "    if sigma <= 0:\n",
    "        raise ValueError(\"sigma must be > 0\")\n",
    "    if not (lower < upper):\n",
    "        raise ValueError(\"require lower < upper\")\n",
    "\n",
    "    alpha = (lower - mu) / sigma\n",
    "    beta = (upper - mu) / sigma\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "def truncnorm_logZ(alpha, beta):\n",
    "    \"\"\"Compute log(Z) where Z = Phi(beta) - Phi(alpha).\"\"\"\n",
    "    alpha = np.asarray(alpha, dtype=float)\n",
    "    beta = np.asarray(beta, dtype=float)\n",
    "    if np.any(beta <= alpha):\n",
    "        raise ValueError(\"require beta > alpha\")\n",
    "\n",
    "    # Direct CDF difference works well in the lower tail / central region.\n",
    "    logPhi_a = norm_logcdf(alpha)\n",
    "    logPhi_b = norm_logcdf(beta)\n",
    "    logZ_cdf = _logdiffexp(logPhi_b, logPhi_a)\n",
    "\n",
    "    # In the upper tail, it's often more stable to use survival functions:\n",
    "    # Phi(beta) - Phi(alpha) = sf(alpha) - sf(beta) = Phi(-alpha) - Phi(-beta).\n",
    "    logSf_a = norm_logcdf(-alpha)\n",
    "    logSf_b = norm_logcdf(-beta)\n",
    "    logZ_sf = _logdiffexp(logSf_a, logSf_b)\n",
    "\n",
    "    use_sf = alpha > 0\n",
    "    return np.where(use_sf, logZ_sf, logZ_cdf)\n",
    "\n",
    "\n",
    "def truncnorm_logpdf(x, mu, sigma, lower, upper):\n",
    "    \"\"\"Log-PDF of TruncNorm(mu, sigma^2; [lower, upper]).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    alpha, beta = truncnorm_standardized_bounds(mu, sigma, lower, upper)\n",
    "\n",
    "    z = (x - mu) / sigma\n",
    "    logZ = truncnorm_logZ(alpha, beta)\n",
    "\n",
    "    logpdf = norm_logpdf(z) - np.log(sigma) - logZ\n",
    "    return np.where((x >= lower) & (x <= upper), logpdf, -np.inf)\n",
    "\n",
    "\n",
    "def truncnorm_pdf(x, mu, sigma, lower, upper):\n",
    "    return np.exp(truncnorm_logpdf(x, mu, sigma, lower, upper))\n",
    "\n",
    "\n",
    "def truncnorm_cdf(x, mu, sigma, lower, upper):\n",
    "    \"\"\"CDF of TruncNorm(mu, sigma^2; [lower, upper]).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    alpha, beta = truncnorm_standardized_bounds(mu, sigma, lower, upper)\n",
    "\n",
    "    z = (x - mu) / sigma\n",
    "    Z = norm_cdf(beta) - norm_cdf(alpha)\n",
    "\n",
    "    cdf = (norm_cdf(z) - norm_cdf(alpha)) / Z\n",
    "    cdf = np.where(x < lower, 0.0, cdf)\n",
    "    cdf = np.where(x > upper, 1.0, cdf)\n",
    "    return np.clip(cdf, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def truncnorm_moments(mu, sigma, lower, upper):\n",
    "    \"\"\"Mean/variance/skewness/excess kurtosis (using standardized raw moments).\"\"\"\n",
    "    alpha, beta = truncnorm_standardized_bounds(mu, sigma, lower, upper)\n",
    "\n",
    "    Z = norm_cdf(beta) - norm_cdf(alpha)\n",
    "    phi_a = norm_pdf(alpha)\n",
    "    phi_b = norm_pdf(beta)\n",
    "\n",
    "    # Standardized raw moments for T ~ N(0,1) | alpha <= T <= beta\n",
    "    m1 = (phi_a - phi_b) / Z\n",
    "    m2 = 1.0 + (alpha * phi_a - beta * phi_b) / Z\n",
    "    m3 = ((alpha**2 + 2.0) * phi_a - (beta**2 + 2.0) * phi_b) / Z\n",
    "    m4 = 3.0 + ((alpha**3 + 3.0 * alpha) * phi_a - (beta**3 + 3.0 * beta) * phi_b) / Z\n",
    "\n",
    "    var_t = m2 - m1**2\n",
    "\n",
    "    # Central moments (standardized)\n",
    "    mu3 = m3 - 3 * m1 * m2 + 2 * m1**3\n",
    "    mu4 = m4 - 4 * m1 * m3 + 6 * (m1**2) * m2 - 3 * m1**4\n",
    "\n",
    "    if var_t <= 0:\n",
    "        skew = np.nan\n",
    "        excess_kurt = np.nan\n",
    "    else:\n",
    "        skew = mu3 / (var_t ** 1.5)\n",
    "        excess_kurt = mu4 / (var_t**2) - 3.0\n",
    "\n",
    "    mean = mu + sigma * m1\n",
    "    var = (sigma**2) * var_t\n",
    "\n",
    "    return {\n",
    "        'alpha': alpha,\n",
    "        'beta': beta,\n",
    "        'Z': Z,\n",
    "        'mean': mean,\n",
    "        'variance': var,\n",
    "        'skewness': skew,\n",
    "        'excess_kurtosis': excess_kurt,\n",
    "        'm1': m1,\n",
    "        'm2': m2,\n",
    "        'm3': m3,\n",
    "        'm4': m4,\n",
    "    }\n",
    "\n",
    "\n",
    "def truncnorm_entropy(mu, sigma, lower, upper):\n",
    "    \"\"\"Differential entropy (natural log).\"\"\"\n",
    "    alpha, beta = truncnorm_standardized_bounds(mu, sigma, lower, upper)\n",
    "\n",
    "    Z = norm_cdf(beta) - norm_cdf(alpha)\n",
    "    phi_a = norm_pdf(alpha)\n",
    "    phi_b = norm_pdf(beta)\n",
    "\n",
    "    return (\n",
    "        0.5 * np.log(2 * np.pi * np.e * sigma**2)\n",
    "        + np.log(Z)\n",
    "        + (alpha * phi_a - beta * phi_b) / (2 * Z)\n",
    "    )\n",
    "\n",
    "\n",
    "def truncnorm_mgf(t, mu, sigma, lower, upper):\n",
    "    \"\"\"MGF M_X(t) for real t (exists for all real t since support is bounded).\"\"\"\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    alpha, beta = truncnorm_standardized_bounds(mu, sigma, lower, upper)\n",
    "    Z = norm_cdf(beta) - norm_cdf(alpha)\n",
    "\n",
    "    num = norm_cdf(beta - sigma * t) - norm_cdf(alpha - sigma * t)\n",
    "    return np.exp(mu * t + 0.5 * (sigma * t) ** 2) * (num / Z)\n",
    "\n",
    "\n",
    "def truncnorm_loglik(x, mu, sigma, lower, upper):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return np.sum(truncnorm_logpdf(x, mu, sigma, lower, upper))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff865d8",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "Let $\\alpha=(\\ell-\\mu)/\\sigma$, $\\beta=(u-\\mu)/\\sigma$, and $Z=\\Phi(\\beta)-\\Phi(\\alpha)$.\n",
    "Define the standardized truncated variable:\n",
    "\n",
    "$$\n",
    "T \\sim \\mathcal{N}(0,1)\\mid (\\alpha\\le T\\le \\beta),\n",
    "\\qquad X = \\mu + \\sigma T.\n",
    "$$\n",
    "\n",
    "A convenient summary is:\n",
    "\n",
    "| Quantity | Expression |\n",
    "|---|---|\n",
    "| Mean | $\\mathbb{E}[X]=\\mu+\\sigma\\,\\dfrac{\\varphi(\\alpha)-\\varphi(\\beta)}{Z}$ |\n",
    "| Variance | $\\mathrm{Var}(X)=\\sigma^2\\Bigl(1+\\dfrac{\\alpha\\varphi(\\alpha)-\\beta\\varphi(\\beta)}{Z}-\\bigl(\\dfrac{\\varphi(\\alpha)-\\varphi(\\beta)}{Z}\\bigr)^2\\Bigr)$ |\n",
    "| Skewness | computed from standardized central moments of $T$ (see below) |\n",
    "| Excess kurtosis | computed from standardized central moments of $T$ |\n",
    "| MGF | $M_X(t)=e^{\\mu t+\\tfrac12\\sigma^2 t^2}\\,\\dfrac{\\Phi(\\beta-\\sigma t)-\\Phi(\\alpha-\\sigma t)}{Z}$ |\n",
    "| CF | $\\varphi_X(\\omega)=M_X(i\\omega)$ (complex extension of $\\Phi$) |\n",
    "| Entropy | $h(X)=\\tfrac12\\log(2\\pi e\\sigma^2)+\\log Z+\\dfrac{\\alpha\\varphi(\\alpha)-\\beta\\varphi(\\beta)}{2Z}$ |\n",
    "\n",
    "### Raw-moment formulas (standardized)\n",
    "\n",
    "For $T$ as above:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[T] &= \\frac{\\varphi(\\alpha)-\\varphi(\\beta)}{Z}\\\\\n",
    "\\mathbb{E}[T^2] &= 1 + \\frac{\\alpha\\varphi(\\alpha)-\\beta\\varphi(\\beta)}{Z}\\\\\n",
    "\\mathbb{E}[T^3] &= \\frac{(\\alpha^2+2)\\varphi(\\alpha)-(\\beta^2+2)\\varphi(\\beta)}{Z}\\\\\n",
    "\\mathbb{E}[T^4] &= 3 + \\frac{(\\alpha^3+3\\alpha)\\varphi(\\alpha)-(\\beta^3+3\\beta)\\varphi(\\beta)}{Z}\n",
    "\\end{align}\n",
    "\n",
    "From these, compute central moments and then skewness/kurtosis in the usual way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89861ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, lower, upper = 0.5, 1.2, -1.0, 2.0\n",
    "\n",
    "mom = truncnorm_moments(mu, sigma, lower, upper)\n",
    "{\n",
    "    'mean': mom['mean'],\n",
    "    'variance': mom['variance'],\n",
    "    'skewness': mom['skewness'],\n",
    "    'excess_kurtosis': mom['excess_kurtosis'],\n",
    "    'entropy': truncnorm_entropy(mu, sigma, lower, upper),\n",
    "    'mgf(t=0.5)': truncnorm_mgf(0.5, mu, sigma, lower, upper),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4daaad2",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "Think in two layers:\n",
    "\n",
    "1. **Underlying normal**: $Y\\sim\\mathcal{N}(\\mu,\\sigma^2)$\n",
    "2. **Conditioning (truncation)**: keep only values in $[\\ell,u]$\n",
    "\n",
    "How parameters affect shape:\n",
    "\n",
    "- **$\\mu$ (location)** shifts the *underlying* normal; after truncation, the actual mean is pulled toward the interval.\n",
    "- **$\\sigma$ (scale)** controls spread; if $\\sigma$ is large relative to $(u-\\ell)$, most mass is chopped off and the shape can become highly skewed.\n",
    "- **Bounds $(\\ell,u)$** cut tails; narrow intervals force a near-uniform-looking bump (but still “normal-shaped” on the log scale).\n",
    "\n",
    "In standardized coordinates, $(a,b)=((\\ell-\\mu)/\\sigma,(u-\\mu)/\\sigma)$ are the bounds measured in **standard deviations** from $\\mu$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How truncation changes shape\n",
    "\n",
    "def plot_pdf_family(param_sets, title):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for ps in param_sets:\n",
    "        mu, sigma, lower, upper, name = ps\n",
    "        x = np.linspace(\n",
    "            lower if np.isfinite(lower) else mu - 4 * sigma,\n",
    "            upper if np.isfinite(upper) else mu + 4 * sigma,\n",
    "            600,\n",
    "        )\n",
    "        y = truncnorm_pdf(x, mu, sigma, lower, upper)\n",
    "        fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=name))\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title='x', yaxis_title='pdf')\n",
    "    return fig\n",
    "\n",
    "param_sets_1 = [\n",
    "    (0.0, 1.0, -2.0, 2.0, \"mu=0, sigma=1, [-2,2]\"),\n",
    "    (0.0, 1.0, 0.0, np.inf, \"mu=0, sigma=1, [0,∞)\"),\n",
    "    (0.0, 1.0, -0.5, 0.5, \"mu=0, sigma=1, [-0.5,0.5]\"),\n",
    "    (0.0, 1.0, 2.0, np.inf, \"mu=0, sigma=1, [2,∞)\"),\n",
    "]\n",
    "\n",
    "fig1 = plot_pdf_family(param_sets_1, \"Truncated normal PDFs (varying bounds)\")\n",
    "fig1.show()\n",
    "\n",
    "param_sets_2 = [\n",
    "    (-1.0, 1.0, -1.0, 1.0, \"mu=-1, sigma=1, [-1,1]\"),\n",
    "    (0.0, 1.0, -1.0, 1.0, \"mu=0, sigma=1, [-1,1]\"),\n",
    "    (1.0, 1.0, -1.0, 1.0, \"mu=1, sigma=1, [-1,1]\"),\n",
    "]\n",
    "\n",
    "fig2 = plot_pdf_family(param_sets_2, \"PDFs with fixed bounds but different mu\")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff882c",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "Below we derive the mean/variance for the standardized truncated normal.\n",
    "Let\n",
    "\n",
    "$$\n",
    "T \\sim \\mathcal{N}(0,1)\\mid (\\alpha\\le T\\le \\beta),\n",
    "\\qquad\n",
    "f_T(t)=\\frac{\\varphi(t)}{Z}\\;\\mathbf{1}_{[\\alpha,\\beta]}(t),\n",
    "\\qquad Z=\\Phi(\\beta)-\\Phi(\\alpha).\n",
    "$$\n",
    "\n",
    "### Expectation\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[T]\n",
    "&= \\frac{1}{Z}\\int_{\\alpha}^{\\beta} t\\,\\varphi(t)\\,dt.\n",
    "\\end{align}\n",
    "\n",
    "Use the key identity $\\varphi'(t)=-t\\varphi(t)$, so $t\\varphi(t)=-\\varphi'(t)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[T]\n",
    "&= \\frac{1}{Z}\\int_{\\alpha}^{\\beta} -\\varphi'(t)\\,dt\n",
    "= \\frac{1}{Z}\\bigl[ -\\varphi(t)\\bigr]_{\\alpha}^{\\beta}\n",
    "= \\frac{\\varphi(\\alpha)-\\varphi(\\beta)}{Z}.\n",
    "\\end{align}\n",
    "\n",
    "Then $X=\\mu+\\sigma T$ gives $\\mathbb{E}[X]=\\mu+\\sigma\\mathbb{E}[T]$.\n",
    "\n",
    "### Variance\n",
    "\n",
    "First compute $\\mathbb{E}[T^2]$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[T^2]\n",
    "&= \\frac{1}{Z}\\int_{\\alpha}^{\\beta} t^2\\varphi(t)\\,dt.\n",
    "\\end{align}\n",
    "\n",
    "Use integration by parts with $u=t$ and $dv=t\\varphi(t)dt=-\\varphi'(t)dt$:\n",
    "\n",
    "\\begin{align}\n",
    "\\int t^2\\varphi(t)\\,dt\n",
    "&= \\int t\\,(t\\varphi(t))\\,dt\n",
    "= \\int t\\,(-\\varphi'(t))\\,dt\n",
    "= -t\\varphi(t) + \\int \\varphi(t)\\,dt\n",
    "= -t\\varphi(t) + \\Phi(t).\n",
    "\\end{align}\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[T^2]\n",
    "&= \\frac{1}{Z}\\Bigl(\\bigl[-t\\varphi(t)+\\Phi(t)\\bigr]_{\\alpha}^{\\beta}\\Bigr)\n",
    "= \\frac{1}{Z}\\Bigl(Z + \\alpha\\varphi(\\alpha)-\\beta\\varphi(\\beta)\\Bigr)\n",
    "= 1 + \\frac{\\alpha\\varphi(\\alpha)-\\beta\\varphi(\\beta)}{Z}.\n",
    "\\end{align}\n",
    "\n",
    "Finally\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(T)=\\mathbb{E}[T^2]-\\mathbb{E}[T]^2,\n",
    "\\qquad\n",
    "\\mathrm{Var}(X)=\\sigma^2\\,\\mathrm{Var}(T).\n",
    "$$\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "For observations $x_1,\\dots,x_n\\in[\\ell,u]$:\n",
    "\n",
    "$$\n",
    "\\log f(x_i\\mid\\mu,\\sigma,\\ell,u)\n",
    "= -\\frac12\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2 - \\log\\sigma - \\log Z - \\frac12\\log(2\\pi).\n",
    "$$\n",
    "\n",
    "The log-likelihood is the sum over $i$. Because $Z=\\Phi(\\beta)-\\Phi(\\alpha)$ depends on $(\\mu,\\sigma)$, MLE typically requires **numerical optimization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_truncnorm_mle(x, lower, upper, mu0=None, sigma0=None):\n",
    "    \"\"\"MLE for (mu, sigma) given known truncation bounds [lower, upper].\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    lower = float(lower)\n",
    "    upper = float(upper)\n",
    "\n",
    "    if not np.all((x >= lower) & (x <= upper)):\n",
    "        raise ValueError(\"all observations must lie within [lower, upper]\")\n",
    "\n",
    "    if mu0 is None:\n",
    "        mu0 = float(np.mean(x))\n",
    "    if sigma0 is None:\n",
    "        sigma0 = float(np.std(x, ddof=1))\n",
    "    sigma0 = max(sigma0, 1e-6)\n",
    "\n",
    "    def nll(theta):\n",
    "        mu = float(theta[0])\n",
    "        sigma = float(np.exp(theta[1]))\n",
    "        ll = truncnorm_loglik(x, mu, sigma, lower, upper)\n",
    "        if not np.isfinite(ll):\n",
    "            return 1e300\n",
    "        return -ll\n",
    "\n",
    "    res = optimize.minimize(\n",
    "        nll,\n",
    "        x0=np.array([mu0, np.log(sigma0)]),\n",
    "        method='L-BFGS-B',\n",
    "    )\n",
    "    mu_hat = float(res.x[0])\n",
    "    sigma_hat = float(np.exp(res.x[1]))\n",
    "    return mu_hat, sigma_hat, res\n",
    "\n",
    "\n",
    "# Demo: recover parameters from synthetic data\n",
    "mu_true, sigma_true, lower, upper = 0.5, 1.2, -1.0, 2.0\n",
    "\n",
    "a = (lower - mu_true) / sigma_true\n",
    "b = (upper - mu_true) / sigma_true\n",
    "\n",
    "x = stats.truncnorm(a, b, loc=mu_true, scale=sigma_true).rvs(size=3000, random_state=rng)\n",
    "\n",
    "mu_hat, sigma_hat, res = fit_truncnorm_mle(x, lower, upper)\n",
    "(mu_true, sigma_true), (mu_hat, sigma_hat), res.success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c9299",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation (NumPy-only)\n",
    "\n",
    "A conceptually simple sampler is **rejection sampling** from the underlying normal.\n",
    "\n",
    "### Algorithm (accept–reject)\n",
    "\n",
    "1. Propose $Y\\sim\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "2. If $Y\\in[\\ell,u]$, accept and output $X=Y$. Otherwise reject and try again.\n",
    "\n",
    "This works because the truncated normal is exactly the conditional distribution $Y\\mid(\\ell\\le Y\\le u)$.\n",
    "\n",
    "### Efficiency\n",
    "\n",
    "The acceptance probability is\n",
    "\n",
    "$$\\mathbb{P}(\\ell\\le Y\\le u)=Z=\\Phi(\\beta)-\\Phi(\\alpha).$$\n",
    "\n",
    "So the expected number of proposals per accepted sample is $1/Z$.\n",
    "If $Z$ is tiny (very narrow interval or far out in the tails), naive rejection can be slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd21db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncnorm_rvs_rejection_numpy(mu, sigma, lower, upper, size=1, rng=None, batch_multiplier=4):\n",
    "    \"\"\"Sample from TruncNorm(mu, sigma^2; [lower, upper]) using NumPy-only rejection sampling.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    mu = float(mu)\n",
    "    sigma = float(sigma)\n",
    "    lower = float(lower)\n",
    "    upper = float(upper)\n",
    "\n",
    "    if sigma <= 0:\n",
    "        raise ValueError(\"sigma must be > 0\")\n",
    "    if not (lower < upper):\n",
    "        raise ValueError(\"require lower < upper\")\n",
    "\n",
    "    size_tuple = (size,) if isinstance(size, int) else tuple(size)\n",
    "    n = int(np.prod(size_tuple))\n",
    "\n",
    "    if np.isneginf(lower) and np.isposinf(upper):\n",
    "        return rng.normal(loc=mu, scale=sigma, size=size_tuple), n\n",
    "\n",
    "    out = np.empty(n, dtype=float)\n",
    "    filled = 0\n",
    "    proposed = 0\n",
    "\n",
    "    while filled < n:\n",
    "        m = n - filled\n",
    "        batch = max(batch_multiplier * m, 128)\n",
    "\n",
    "        y = rng.normal(loc=mu, scale=sigma, size=batch)\n",
    "        proposed += y.size\n",
    "\n",
    "        acc = y[(y >= lower) & (y <= upper)]\n",
    "        k = min(acc.size, m)\n",
    "        if k > 0:\n",
    "            out[filled : filled + k] = acc[:k]\n",
    "            filled += k\n",
    "\n",
    "    return out.reshape(size_tuple), proposed\n",
    "\n",
    "\n",
    "s, proposed = truncnorm_rvs_rejection_numpy(0.5, 1.2, -1.0, 2.0, size=50_000, rng=rng)\n",
    "\n",
    "s.min(), s.max(), s.mean(), proposed / s.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9c32b",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "\n",
    "- the PDF and CDF\n",
    "- Monte Carlo samples (histogram + empirical CDF)\n",
    "\n",
    "We’ll use the formulas from Sections 3–4 and the NumPy-only sampler from Section 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e15cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, lower, upper = 0.5, 1.2, -1.0, 2.0\n",
    "\n",
    "x_grid = np.linspace(lower, upper, 600)\n",
    "\n",
    "pdf = truncnorm_pdf(x_grid, mu, sigma, lower, upper)\n",
    "cdf = truncnorm_cdf(x_grid, mu, sigma, lower, upper)\n",
    "\n",
    "fig_pdf = px.line(x=x_grid, y=pdf, title=\"Truncated normal PDF\", labels={\"x\": \"x\", \"y\": \"pdf\"})\n",
    "fig_pdf.show()\n",
    "\n",
    "fig_cdf = px.line(x=x_grid, y=cdf, title=\"Truncated normal CDF\", labels={\"x\": \"x\", \"y\": \"cdf\"})\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo samples\n",
    "samples, _ = truncnorm_rvs_rejection_numpy(mu, sigma, lower, upper, size=20_000, rng=rng)\n",
    "\n",
    "fig_hist = px.histogram(\n",
    "    x=samples,\n",
    "    nbins=60,\n",
    "    histnorm='probability density',\n",
    "    title=\"Monte Carlo samples (histogram) with PDF overlay\",\n",
    "    labels={\"x\": \"x\"},\n",
    ")\n",
    "fig_hist.add_trace(go.Scatter(x=x_grid, y=pdf, mode='lines', name='theory pdf'))\n",
    "fig_hist.show()\n",
    "\n",
    "# Empirical CDF vs theoretical CDF\n",
    "xs = np.sort(samples)\n",
    "ecdf = np.arange(1, xs.size + 1) / xs.size\n",
    "\n",
    "fig_ecdf = go.Figure()\n",
    "fig_ecdf.add_trace(go.Scatter(x=xs, y=ecdf, mode='lines', name='empirical CDF'))\n",
    "fig_ecdf.add_trace(go.Scatter(x=x_grid, y=cdf, mode='lines', name='theory CDF'))\n",
    "fig_ecdf.update_layout(title=\"Empirical CDF vs theoretical CDF\", xaxis_title='x', yaxis_title='cdf')\n",
    "fig_ecdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b35d06",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.truncnorm`)\n",
    "\n",
    "SciPy’s truncated normal is `scipy.stats.truncnorm`.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- `a` and `b` are **standardized bounds** (in units of standard deviations).\n",
    "- `loc` and `scale` are the underlying normal’s $(\\mu,\\sigma)$.\n",
    "\n",
    "Mapping from *absolute* bounds $[\\ell,u]$:\n",
    "\n",
    "$$\n",
    "a = \\frac{\\ell-\\mu}{\\sigma},\\qquad b = \\frac{u-\\mu}{\\sigma}.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "rv = stats.truncnorm(a, b, loc=mu, scale=sigma)\n",
    "```\n",
    "\n",
    "If you call `fit`, SciPy estimates `(a, b, loc, scale)` directly.\n",
    "If your truncation bounds are **known and fixed** in absolute units, you usually want a custom likelihood fit (Section 6), because `(a,b)` depend on $(\\mu,\\sigma)$ through the mapping above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, lower, upper = 0.5, 1.2, -1.0, 2.0\n",
    "\n",
    "a = (lower - mu) / sigma\n",
    "b = (upper - mu) / sigma\n",
    "\n",
    "rv = stats.truncnorm(a, b, loc=mu, scale=sigma)\n",
    "\n",
    "# pdf/cdf/rvs\n",
    "x_grid = np.linspace(lower, upper, 5)\n",
    "rv.pdf(x_grid), rv.cdf(x_grid), rv.rvs(size=3, random_state=rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SciPy moments to our formulas\n",
    "\n",
    "m_scipy, v_scipy, skew_scipy, kurt_scipy = rv.stats(moments='mvsk')\n",
    "\n",
    "mom = truncnorm_moments(mu, sigma, lower, upper)\n",
    "\n",
    "{\n",
    "    'mean_scipy': float(m_scipy),\n",
    "    'mean_formula': mom['mean'],\n",
    "    'var_scipy': float(v_scipy),\n",
    "    'var_formula': mom['variance'],\n",
    "    'skew_scipy': float(skew_scipy),\n",
    "    'skew_formula': mom['skewness'],\n",
    "    'excess_kurt_scipy': float(kurt_scipy),\n",
    "    'excess_kurt_formula': mom['excess_kurtosis'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit example (SciPy parameterization)\n",
    "\n",
    "samples = rv.rvs(size=5000, random_state=rng)\n",
    "\n",
    "a_hat, b_hat, loc_hat, scale_hat = stats.truncnorm.fit(samples)\n",
    "\n",
    "{\n",
    "    'true': (a, b, mu, sigma),\n",
    "    'fit': (a_hat, b_hat, loc_hat, scale_hat),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858b9ad",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### A) Hypothesis testing\n",
    "\n",
    "When data are truncated, using “ordinary” normal-model tests can be badly biased.\n",
    "A principled approach is to write down the **truncated likelihood** and use standard likelihood tools:\n",
    "\n",
    "- **Likelihood ratio tests** (LRT)\n",
    "- **Wald tests** (via the observed Fisher information)\n",
    "- **Parametric bootstrap** (often more accurate in small samples)\n",
    "\n",
    "### B) Bayesian modeling\n",
    "\n",
    "A truncated normal is a common way to build a **normal prior with hard constraints**.\n",
    "With a normal likelihood, the *untruncated* posterior is normal; adding bounds yields a **truncated normal posterior**.\n",
    "\n",
    "### C) Generative modeling\n",
    "\n",
    "Truncated normals are used whenever you want “Gaussian-like” randomness but want to **avoid extreme outliers**.\n",
    "A famous example is **truncated normal weight initialization** in deep learning, where weights are drawn from a normal distribution but clipped by truncation (e.g. within $\\pm2\\sigma$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb606e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Likelihood ratio test for H0: mu = mu0 (bounds known)\n",
    "\n",
    "lower, upper = -1.0, 2.0\n",
    "mu_true, sigma_true = 0.6, 1.1\n",
    "mu0 = 0.0\n",
    "\n",
    "a = (lower - mu_true) / sigma_true\n",
    "b = (upper - mu_true) / sigma_true\n",
    "\n",
    "data = stats.truncnorm(a, b, loc=mu_true, scale=sigma_true).rvs(size=2500, random_state=rng)\n",
    "\n",
    "# Unrestricted MLE\n",
    "mu_hat, sigma_hat, _ = fit_truncnorm_mle(data, lower, upper)\n",
    "ll_alt = truncnorm_loglik(data, mu_hat, sigma_hat, lower, upper)\n",
    "\n",
    "# Null: mu fixed, optimize sigma only\n",
    "\n",
    "def fit_sigma_given_mu(x, mu_fixed, lower, upper, sigma0=None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if sigma0 is None:\n",
    "        sigma0 = float(np.std(x, ddof=1))\n",
    "    sigma0 = max(sigma0, 1e-6)\n",
    "\n",
    "    def nll(log_sigma):\n",
    "        sigma = float(np.exp(log_sigma))\n",
    "        ll = truncnorm_loglik(x, mu_fixed, sigma, lower, upper)\n",
    "        if not np.isfinite(ll):\n",
    "            return 1e300\n",
    "        return -ll\n",
    "\n",
    "    res = optimize.minimize_scalar(nll, bracket=(np.log(sigma0) - 1.0, np.log(sigma0) + 1.0))\n",
    "    return float(np.exp(res.x)), res\n",
    "\n",
    "sigma0_hat, _ = fit_sigma_given_mu(data, mu0, lower, upper)\n",
    "ll_null = truncnorm_loglik(data, mu0, sigma0_hat, lower, upper)\n",
    "\n",
    "lr_stat = 2 * (ll_alt - ll_null)\n",
    "p_value = stats.chi2.sf(lr_stat, df=1)\n",
    "\n",
    "{\n",
    "    'mu_hat': mu_hat,\n",
    "    'sigma_hat': sigma_hat,\n",
    "    'sigma_hat_under_H0': sigma0_hat,\n",
    "    'lr_stat': lr_stat,\n",
    "    'p_value_chi2_approx': p_value,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Bayesian modeling: truncated-normal prior + normal likelihood\n",
    "\n",
    "# Parameter theta is constrained to [0, 1]\n",
    "lower, upper = 0.0, 1.0\n",
    "\n",
    "# Prior: theta ~ TruncNorm(mu0, sigma0^2; [0,1])\n",
    "mu0, sigma0 = 0.5, 0.25\n",
    "\n",
    "# Data: y_i | theta ~ Normal(theta, sigma_y^2)\n",
    "sigma_y = 0.10\n",
    "\n",
    "theta_true = 0.7\n",
    "n = 30\n",
    "y = rng.normal(loc=theta_true, scale=sigma_y, size=n)\n",
    "\n",
    "# Untruncated Normal-Normal posterior parameters\n",
    "post_var = 1.0 / (1.0 / sigma0**2 + n / sigma_y**2)\n",
    "post_mu = post_var * (mu0 / sigma0**2 + n * y.mean() / sigma_y**2)\n",
    "post_sigma = float(np.sqrt(post_var))\n",
    "\n",
    "# With bounds, posterior is truncated normal with the same (post_mu, post_sigma) but restricted to [0,1]\n",
    "x_grid = np.linspace(lower, upper, 600)\n",
    "prior_pdf = truncnorm_pdf(x_grid, mu0, sigma0, lower, upper)\n",
    "post_pdf = truncnorm_pdf(x_grid, post_mu, post_sigma, lower, upper)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=prior_pdf, mode='lines', name='prior'))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=post_pdf, mode='lines', name='posterior'))\n",
    "fig.update_layout(title='Prior vs posterior (both truncated normal)', xaxis_title='theta', yaxis_title='density')\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    'y_mean': float(y.mean()),\n",
    "    'post_mu_untruncated': post_mu,\n",
    "    'post_sigma_untruncated': post_sigma,\n",
    "    'posterior_mean_truncated': truncnorm_moments(post_mu, post_sigma, lower, upper)['mean'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456dfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Generative modeling: truncated normal for \"Gaussian-like\" randomness without extreme outliers\n",
    "\n",
    "sigma = 1.0\n",
    "n = 200_000\n",
    "\n",
    "w_normal = rng.normal(loc=0.0, scale=sigma, size=n)\n",
    "w_trunc = stats.truncnorm(-2.0, 2.0, loc=0.0, scale=sigma).rvs(size=n, random_state=rng)\n",
    "\n",
    "{\n",
    "    'max_abs_normal': float(np.max(np.abs(w_normal))),\n",
    "    'max_abs_trunc': float(np.max(np.abs(w_trunc))),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tails visually\n",
    "\n",
    "x_grid = np.linspace(-4, 4, 800)\n",
    "\n",
    "pdf_normal = stats.norm(loc=0, scale=1).pdf(x_grid)\n",
    "pdf_trunc = stats.truncnorm(-2.0, 2.0, loc=0.0, scale=1.0).pdf(x_grid)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=pdf_normal, mode='lines', name='Normal(0,1)'))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=pdf_trunc, mode='lines', name='TruncNorm(0,1;[-2,2])'))\n",
    "fig.update_layout(title='Tail behavior: normal vs truncated normal', xaxis_title='x', yaxis_title='pdf')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e1106",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Truncation vs censoring**: truncation *removes* out-of-range values; censoring *records them at the boundary*.\n",
    "- **SciPy parameter confusion**: `a` and `b` are **standardized**; the actual support is `(loc + a*scale, loc + b*scale)`.\n",
    "- **Numerical issues**: when $Z=\\Phi(\\beta)-\\Phi(\\alpha)$ is tiny, naive subtraction can underflow to 0. Use log-CDF computations (or SciPy’s implementation).\n",
    "- **Slow rejection sampling**: when $Z$ is tiny, accept–reject becomes inefficient; consider inverse-CDF sampling or specialized tail samplers.\n",
    "- **Fitting with known bounds**: SciPy’s `fit` estimates `(a,b,loc,scale)`; if you have fixed absolute bounds, use a custom likelihood (Section 6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy parameterization pitfall: (a, b) are in *standard deviations*, not absolute units\n",
    "\n",
    "mu, sigma = 0.5, 0.2\n",
    "lower, upper = 0.0, 1.0\n",
    "\n",
    "rv_wrong = stats.truncnorm(0.0, 1.0, loc=mu, scale=sigma)\n",
    "rv_right = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "\n",
    "{\n",
    "    'wrong_support': rv_wrong.support(),\n",
    "    'right_support': rv_right.support(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68591698",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `truncnorm` is a normal distribution **conditioned** to lie in $[\\ell,u]$.\n",
    "- The PDF is the normal PDF divided by the probability mass $Z=\\Phi(\\beta)-\\Phi(\\alpha)$ in the interval.\n",
    "- Truncation shifts the mean toward the interval and can induce strong skewness.\n",
    "- Likelihood-based inference should use the **truncated** likelihood; naive normal-model procedures can be biased.\n",
    "- A simple NumPy-only sampler is accept–reject from $\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "- In SciPy, `a` and `b` are standardized bounds: `a=(lower-mu)/sigma`, `b=(upper-mu)/sigma`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}