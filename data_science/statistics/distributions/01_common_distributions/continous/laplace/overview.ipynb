{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c2a7f1",
   "metadata": {},
   "source": [
    "# Laplace distribution: `laplace` (double exponential)\n",
    "\n",
    "The **Laplace** distribution is a symmetric, continuous distribution with a **sharp peak** at its location and **exponentially decaying tails**. Compared to a Gaussian, it puts **more mass near the center** and **more mass in the tails**, which makes it a common choice for **robust** modeling.\n",
    "\n",
    "In SciPy it appears as `scipy.stats.laplace`.\n",
    "\n",
    "## Learning goals\n",
    "- understand what the Laplace distribution models and when it is useful\n",
    "- write down the PDF/CDF and connect them to sampling and likelihood\n",
    "- compute key moments (mean/variance/skewness/kurtosis) and entropy\n",
    "- derive the **closed-form MLE** (median + mean absolute deviation)\n",
    "- implement **NumPy-only** sampling and validate everything by Monte Carlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3dfc2",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & Classification\n",
    "2) Intuition & Motivation\n",
    "3) Formal Definition\n",
    "4) Moments & Properties\n",
    "5) Parameter Interpretation\n",
    "6) Derivations\n",
    "7) Sampling & Simulation (NumPy-only)\n",
    "8) Visualization (PDF/CDF + Monte Carlo)\n",
    "9) SciPy Integration\n",
    "10) Statistical Use Cases\n",
    "11) Pitfalls\n",
    "12) Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import chi2, laplace, norm\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "print(\"NumPy \", np.__version__)\n",
    "print(\"SciPy \", scipy.__version__)\n",
    "print(\"Plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1b36b",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `laplace`\n",
    "- **Type**: **continuous** distribution\n",
    "- **Support**:  x ∈ (-∞, ∞)\n",
    "- **Parameter space**: location μ ∈ ℝ and scale b > 0\n",
    "\n",
    "We write:\n",
    "\n",
    "$$X \\sim \\mathrm{Laplace}(\\mu, b).$$\n",
    "\n",
    "The **standard Laplace** is \\(\\mathrm{Laplace}(0, 1)\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0a0b6",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "The Laplace distribution is a natural model for **real-valued errors** that are:\n",
    "\n",
    "- **centered** around a typical value (μ)\n",
    "- **more sharply peaked** than a Gaussian\n",
    "- **heavy-tailed** relative to a Gaussian, but still with exponential tails\n",
    "\n",
    "A clean way to remember it:\n",
    "\n",
    "- Normal: log-density is quadratic in \\((x-\\mu)\\)\n",
    "- Laplace: log-density is linear in \\(|x-\\mu|\\)\n",
    "\n",
    "So Laplace errors correspond to an **L1 loss**.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Robust regression / signal processing**: modeling residuals with Laplace leads to least-absolute-deviation fitting.\n",
    "- **Sparsity-inducing priors**: the Laplace prior underpins the Bayesian view of **Lasso** (L1 regularization).\n",
    "- **Differential privacy (Laplace mechanism)**: adding Laplace noise to numeric queries for privacy guarantees.\n",
    "- **Noise with occasional big jumps**: exponential tails can be a better match than Gaussian in some domains.\n",
    "\n",
    "### Relations to other distributions\n",
    "- If \\(E_1, E_2\\) are i.i.d. exponential with mean \\(b\\), then \\(E_1 - E_2 \\sim \\mathrm{Laplace}(0, b)\\).\n",
    "- If \\(S\\in\\{-1,+1\\}\\) is a fair sign and \\(E\\sim\\mathrm{Exp}(\\text{mean}=b)\\), then \\(S\\,E\\sim\\mathrm{Laplace}(0, b)\\).\n",
    "- If \\(X\\sim\\mathrm{Laplace}(\\mu, b)\\), then \\(|X-\\mu|\\sim\\mathrm{Exp}(\\text{mean}=b)\\).\n",
    "- The Laplace has heavier tails than a Gaussian but all moments exist; its MGF exists only for \\(|t|<1/b\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a0fe3",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### PDF\n",
    "For \\(X\\sim\\mathrm{Laplace}(\\mu, b)\\):\n",
    "\n",
    "$$\n",
    "f(x; \\mu, b) = \\frac{1}{2b}\\,\\exp\\left(-\\frac{|x-\\mu|}{b}\\right), \\qquad b>0.\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "The CDF has a simple piecewise form:\n",
    "\n",
    "$$\n",
    "F(x; \\mu, b) =\n",
    "\\begin{cases}\n",
    "\\tfrac{1}{2}\\exp\\left(\\frac{x-\\mu}{b}\\right), & x<\\mu\\\\\n",
    "1 - \\tfrac{1}{2}\\exp\\left(-\\frac{x-\\mu}{b}\\right), & x\\ge \\mu.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Quantile function (inverse CDF)\n",
    "For \\(p\\in(0,1)\\):\n",
    "\n",
    "$$\n",
    "F^{-1}(p) =\n",
    "\\begin{cases}\n",
    "\\mu + b\\,\\log(2p), & 0<p<\\tfrac{1}{2}\\\\\n",
    "\\mu - b\\,\\log\\big(2(1-p)\\big), & \\tfrac{1}{2}\\le p<1.\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ab0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_scale(b: float) -> float:\n",
    "    b = float(b)\n",
    "    if not np.isfinite(b) or b <= 0:\n",
    "        raise ValueError(\"`b` (scale) must be a positive, finite number.\")\n",
    "    return b\n",
    "\n",
    "\n",
    "def laplace_pdf(x, mu: float = 0.0, b: float = 1.0) -> np.ndarray:\n",
    "    b = _check_scale(b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return np.exp(-np.abs(x - mu) / b) / (2.0 * b)\n",
    "\n",
    "\n",
    "def laplace_logpdf(x, mu: float = 0.0, b: float = 1.0) -> np.ndarray:\n",
    "    b = _check_scale(b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return -np.log(2.0 * b) - np.abs(x - mu) / b\n",
    "\n",
    "\n",
    "def laplace_cdf(x, mu: float = 0.0, b: float = 1.0) -> np.ndarray:\n",
    "    b = _check_scale(b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    z = (x - mu) / b\n",
    "    return np.where(x < mu, 0.5 * np.exp(z), 1.0 - 0.5 * np.exp(-z))\n",
    "\n",
    "\n",
    "def laplace_ppf(p, mu: float = 0.0, b: float = 1.0, eps: float = 1e-12) -> np.ndarray:\n",
    "    b = _check_scale(b)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if np.any((p <= 0) | (p >= 1)):\n",
    "        raise ValueError(\"p must be in (0, 1)\")\n",
    "    p = np.clip(p, eps, 1.0 - eps)\n",
    "    left = mu + b * np.log(2.0 * p)\n",
    "    right = mu - b * (np.log(2.0) + np.log1p(-p))  # log(2(1-p)) stably\n",
    "    return np.where(p < 0.5, left, right)\n",
    "\n",
    "\n",
    "# Quick cross-check vs SciPy\n",
    "mu, b = 0.4, 1.7\n",
    "x = np.linspace(-3, 3, 9)\n",
    "p = np.array([0.1, 0.5, 0.9])\n",
    "\n",
    "print(\"max |pdf - scipy|:\", float(np.max(np.abs(laplace_pdf(x, mu, b) - laplace.pdf(x, loc=mu, scale=b)))))\n",
    "print(\"max |cdf - scipy|:\", float(np.max(np.abs(laplace_cdf(x, mu, b) - laplace.cdf(x, loc=mu, scale=b)))))\n",
    "print(\"max |ppf - scipy|:\", float(np.max(np.abs(laplace_ppf(p, mu, b) - laplace.ppf(p, loc=mu, scale=b)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b9a5b",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "For \\(X\\sim\\mathrm{Laplace}(\\mu, b)\\):\n",
    "\n",
    "- **Mean**: \\(\\mathbb{E}[X]=\\mu\\)\n",
    "- **Variance**: \\(\\mathrm{Var}(X)=2b^2\\) (so \\(\\mathrm{sd}(X)=\\sqrt{2}\\,b\\))\n",
    "- **Skewness**: 0 (symmetry)\n",
    "- **Kurtosis**: 6 (Pearson), so **excess kurtosis** = 3\n",
    "\n",
    "### MGF and characteristic function\n",
    "- MGF exists only for \\(|t|<1/b\\):\n",
    "\n",
    "$$\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = \\frac{e^{\\mu t}}{1-b^2 t^2}, \\qquad |t|<1/b.\n",
    "$$\n",
    "\n",
    "- Characteristic function (exists for all real \\(t\\)):\n",
    "\n",
    "$$\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\frac{e^{i\\mu t}}{1+b^2 t^2}.\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "The differential entropy is:\n",
    "\n",
    "$$\n",
    "H(X) = 1 + \\log(2b).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2cc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_moments(mu: float = 0.0, b: float = 1.0):\n",
    "    b = _check_scale(b)\n",
    "    mean = float(mu)\n",
    "    var = float(2.0 * b * b)\n",
    "    skew = 0.0\n",
    "    kurt_excess = 3.0\n",
    "    entropy = float(1.0 + np.log(2.0 * b))\n",
    "    return mean, var, skew, kurt_excess, entropy\n",
    "\n",
    "\n",
    "def sample_moments(x: np.ndarray):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    m = float(x.mean())\n",
    "    c = x - m\n",
    "    v = float(np.mean(c**2))\n",
    "    skew = float(np.mean(c**3) / (v ** 1.5))\n",
    "    kurt_excess = float(np.mean(c**4) / (v**2) - 3.0)\n",
    "    return m, v, skew, kurt_excess\n",
    "\n",
    "\n",
    "mu, b = 1.5, 0.8\n",
    "\n",
    "mean_f, var_f, skew_f, kurt_f, ent_f = laplace_moments(mu, b)\n",
    "mean_s, var_s, skew_s, kurt_s = laplace.stats(loc=mu, scale=b, moments=\"mvsk\")\n",
    "ent_s = laplace.entropy(loc=mu, scale=b)\n",
    "\n",
    "print(\"theory  (mean, var, skew, kurt_excess, entropy):\", (mean_f, var_f, skew_f, kurt_f, ent_f))\n",
    "print(\"scipy   (mean, var, skew, kurt_excess, entropy):\", (float(mean_s), float(var_s), float(skew_s), float(kurt_s), float(ent_s)))\n",
    "\n",
    "x = laplace.rvs(loc=mu, scale=b, size=300_000, random_state=rng)\n",
    "mean_mc, var_mc, skew_mc, kurt_mc = sample_moments(x)\n",
    "print(\"monte   (mean, var, skew, kurt_excess):         \", (mean_mc, var_mc, skew_mc, kurt_mc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_cf(t, mu: float = 0.0, b: float = 1.0) -> np.ndarray:\n",
    "    b = _check_scale(b)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    return np.exp(1j * mu * t) / (1.0 + (b * t) ** 2)\n",
    "\n",
    "\n",
    "mu, b = 0.0, 1.0\n",
    "t = np.linspace(-15, 15, 2000)\n",
    "phi = laplace_cf(t, mu=mu, b=b)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Re φ(t)\", \"Im φ(t)\"))\n",
    "fig.add_trace(go.Scatter(x=t, y=np.real(phi), mode=\"lines\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=t, y=np.imag(phi), mode=\"lines\"), row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"t\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"t\", row=1, col=2)\n",
    "fig.update_layout(width=950, height=350, showlegend=False, title=\"Characteristic function of Laplace(0,1)\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2dc34",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "- μ (**location**) shifts the distribution left/right. For Laplace it is simultaneously the **mean**, **median**, and **mode**.\n",
    "- b (**scale**) controls both the peak height and tail thickness:\n",
    "  - \\(f(\\mu)=1/(2b)\\) so larger b means a lower peak.\n",
    "  - \\(\\mathrm{sd}(X)=\\sqrt{2}\\,b\\) so b is proportional to standard deviation.\n",
    "\n",
    "Below we visualize how \\((\\mu,b)\\) changes the PDF and CDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 3000)\n",
    "params = [\n",
    "    (0.0, 0.5),\n",
    "    (0.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (2.0, 1.0),\n",
    "]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"PDF\", \"CDF\"))\n",
    "for mu, b in params:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=laplace_pdf(x, mu=mu, b=b), mode=\"lines\", name=f\"μ={mu}, b={b}\"),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=laplace_cdf(x, mu=mu, b=b), mode=\"lines\", name=f\"μ={mu}, b={b}\"),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"f(x)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"F(x)\", row=1, col=2)\n",
    "fig.update_layout(width=1000, height=420)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c9de4",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation\n",
    "Let \\(Y=X-\\mu\\). Then \\(Y\\) has symmetric density\n",
    "\\(f_Y(y) = (1/(2b))\\exp(-|y|/b)\\).\n",
    "\n",
    "Because \\(y f_Y(y)\\) is an **odd** function,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Y] = \\int_{-\\infty}^{\\infty} y f_Y(y)\\,dy = 0 \\quad\\Rightarrow\\quad \\mathbb{E}[X] = \\mu.\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "Using symmetry again,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(X-\\mu)^2] = \\int_{-\\infty}^{\\infty} y^2\\frac{1}{2b}e^{-|y|/b}dy = \\frac{1}{b}\\int_{0}^{\\infty} y^2 e^{-y/b}dy.\n",
    "$$\n",
    "\n",
    "The integral is a Gamma-type integral:\n",
    "\\(\\int_0^{\\infty} y^2 e^{-y/b}dy = 2!\\,b^3 = 2b^3\\), hence\n",
    "\n",
    "$$\\mathrm{Var}(X)=\\mathbb{E}[(X-\\mu)^2]=2b^2.$$\n",
    "\n",
    "### Likelihood (and the MLE)\n",
    "For i.i.d. data \\(x_1,\\dots,x_n\\) from \\(\\mathrm{Laplace}(\\mu,b)\\), the likelihood is\n",
    "\n",
    "$$\n",
    "L(\\mu,b) = \\prod_{i=1}^n \\frac{1}{2b}\\exp\\left(-\\frac{|x_i-\\mu|}{b}\\right) = (2b)^{-n}\\exp\\left(-\\frac{1}{b}\\sum_{i=1}^n |x_i-\\mu|\\right).\n",
    "$$\n",
    "\n",
    "So the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(\\mu,b) = -n\\log(2b) - \\frac{1}{b}\\sum_{i=1}^n |x_i-\\mu|.\n",
    "$$\n",
    "\n",
    "**Key consequence:** for fixed \\(b\\), maximizing \\(\\ell\\) over \\(\\mu\\) is equivalent to minimizing \\(\\sum_i |x_i-\\mu|\\), which is minimized by any **median** of the sample.\n",
    "\n",
    "With \\(\\hat\\mu\\) chosen as a sample median, differentiating \\(\\ell(\\hat\\mu,b)\\) with respect to \\(b\\) gives the (closed-form) MLE:\n",
    "\n",
    "$$\\hat b = \\frac{1}{n}\\sum_{i=1}^n |x_i-\\hat\\mu|.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_loglik(data: np.ndarray, mu: float, b: float) -> float:\n",
    "    return float(np.sum(laplace_logpdf(data, mu=mu, b=b)))\n",
    "\n",
    "\n",
    "def laplace_mle_closed_form(data: np.ndarray, b_floor: float = 1e-12) -> tuple[float, float]:\n",
    "    \"\"\"Closed-form MLE for Laplace(μ,b): μ=median, b=mean absolute deviation from μ.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If all observations are identical, the likelihood increases as b -> 0+.\n",
    "      We return a small positive floor to stay inside the parameter space.\n",
    "    \"\"\"\n",
    "    x = np.asarray(data, dtype=float)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"data must be 1D\")\n",
    "    mu_hat = float(np.median(x))\n",
    "    b_hat = float(np.mean(np.abs(x - mu_hat)))\n",
    "    b_hat = float(max(b_hat, b_floor))\n",
    "    return mu_hat, b_hat\n",
    "\n",
    "\n",
    "mu_true, b_true = 1.0, 0.7\n",
    "data = laplace.rvs(loc=mu_true, scale=b_true, size=2000, random_state=rng)\n",
    "\n",
    "mu_hat, b_hat = laplace_mle_closed_form(data)\n",
    "mu_hat_sp, b_hat_sp = laplace.fit(data)\n",
    "\n",
    "print(\"true (μ, b)         =\", (mu_true, b_true))\n",
    "print(\"closed-form MLE     =\", (mu_hat, b_hat))\n",
    "print(\"scipy laplace.fit   =\", (float(mu_hat_sp), float(b_hat_sp)))\n",
    "print(\"loglik at MLE       =\", laplace_loglik(data, mu_hat, b_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a4f1d",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "Two convenient sampling views:\n",
    "\n",
    "1) **Inverse CDF**: sample \\(U\\sim\\mathrm{Uniform}(0,1)\\) and return \\(F^{-1}(U)\\).\n",
    "\n",
    "2) **Difference of exponentials**: if \\(E_1,E_2\\overset{iid}{\\sim}\\mathrm{Exp}(\\text{mean}=b)\\), then \\(\\mu + (E_1-E_2)\\sim\\mathrm{Laplace}(\\mu,b)\\).\n",
    "\n",
    "We'll implement both with NumPy and sanity-check them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d26b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_rvs_inverse(\n",
    "    rng: np.random.Generator,\n",
    "    size: int,\n",
    "    mu: float = 0.0,\n",
    "    b: float = 1.0,\n",
    "    eps: float = 1e-12,\n",
    ") -> np.ndarray:\n",
    "    b = _check_scale(b)\n",
    "    u = rng.random(size)\n",
    "    u = np.clip(u, eps, 1.0 - eps)\n",
    "    left = mu + b * np.log(2.0 * u)\n",
    "    right = mu - b * (np.log(2.0) + np.log1p(-u))\n",
    "    return np.where(u < 0.5, left, right)\n",
    "\n",
    "\n",
    "def laplace_rvs_exp_difference(\n",
    "    rng: np.random.Generator,\n",
    "    size: int,\n",
    "    mu: float = 0.0,\n",
    "    b: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    b = _check_scale(b)\n",
    "    e1 = rng.exponential(scale=b, size=size)\n",
    "    e2 = rng.exponential(scale=b, size=size)\n",
    "    return mu + (e1 - e2)\n",
    "\n",
    "\n",
    "mu, b = -0.5, 1.2\n",
    "n = 200_000\n",
    "\n",
    "x_inv = laplace_rvs_inverse(rng, size=n, mu=mu, b=b)\n",
    "x_diff = laplace_rvs_exp_difference(rng, size=n, mu=mu, b=b)\n",
    "\n",
    "q = [0.05, 0.25, 0.5, 0.75, 0.95]\n",
    "print(\"theory quantiles:\", np.round(laplace.ppf(q, loc=mu, scale=b), 4))\n",
    "print(\"inv   quantiles:\", np.round(np.quantile(x_inv, q), 4))\n",
    "print(\"diff  quantiles:\", np.round(np.quantile(x_diff, q), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e17b0d",
   "metadata": {},
   "source": [
    "## 8) Visualization (PDF/CDF + Monte Carlo)\n",
    "\n",
    "We'll plot the theoretical PDF/CDF and compare them to Monte Carlo samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c69cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, b = 0.0, 1.0\n",
    "x = np.linspace(-8, 8, 4000)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"PDF\", \"CDF\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=laplace_pdf(x, mu=mu, b=b), mode=\"lines\", name=\"pdf\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=laplace_cdf(x, mu=mu, b=b), mode=\"lines\", name=\"cdf\"), row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"f(x)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"F(x)\", row=1, col=2)\n",
    "fig.update_layout(width=950, height=380, showlegend=False, title=\"Laplace(0,1): PDF and CDF\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Monte Carlo histogram vs theory\n",
    "n = 120_000\n",
    "samples = laplace_rvs_inverse(rng, size=n, mu=mu, b=b)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples,\n",
    "        histnorm=\"probability density\",\n",
    "        nbinsx=120,\n",
    "        name=\"Monte Carlo\",\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=laplace_pdf(x, mu=mu, b=b), mode=\"lines\", name=\"Theory\"))\n",
    "fig.update_layout(\n",
    "    title=\"Laplace(0,1): histogram vs PDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Empirical CDF vs theory\n",
    "xs = np.sort(samples)\n",
    "ecdf = np.arange(1, xs.size + 1) / xs.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=ecdf, mode=\"lines\", name=\"Empirical\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=laplace_cdf(x, mu=mu, b=b), mode=\"lines\", name=\"Theory\"))\n",
    "fig.update_layout(title=\"Laplace(0,1): empirical CDF\", xaxis_title=\"x\", yaxis_title=\"F(x)\", width=900, height=420)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Q-Q plot (sample quantiles vs theoretical quantiles)\n",
    "p = np.linspace(0.01, 0.99, 200)\n",
    "q_theory = laplace_ppf(p, mu=mu, b=b)\n",
    "q_emp = np.quantile(samples, p)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=q_theory, y=q_emp, mode=\"markers\", name=\"quantiles\"))\n",
    "fig.add_trace(go.Scatter(x=q_theory, y=q_theory, mode=\"lines\", name=\"y=x\"))\n",
    "fig.update_layout(title=\"Laplace(0,1): Q-Q plot\", xaxis_title=\"theoretical quantile\", yaxis_title=\"sample quantile\", width=700, height=520)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ff8be",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy's `laplace` uses the same parameterization:\n",
    "\n",
    "- `loc` = μ\n",
    "- `scale` = b\n",
    "\n",
    "The most common methods:\n",
    "\n",
    "- `laplace.pdf(x, loc, scale)` / `laplace.logpdf(...)`\n",
    "- `laplace.cdf(x, loc, scale)` / `laplace.ppf(p, loc, scale)`\n",
    "- `laplace.rvs(loc, scale, size, random_state=...)`\n",
    "- `laplace.fit(data)` (MLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, b = 0.7, 1.2\n",
    "x = np.array([-2.0, 0.0, 1.0, 3.0])\n",
    "\n",
    "print(\"pdf:\", laplace.pdf(x, loc=mu, scale=b))\n",
    "print(\"cdf:\", laplace.cdf(x, loc=mu, scale=b))\n",
    "print(\"rvs:\", laplace.rvs(loc=mu, scale=b, size=5, random_state=rng))\n",
    "\n",
    "data = laplace.rvs(loc=mu, scale=b, size=3000, random_state=rng)\n",
    "mu_hat_sp, b_hat_sp = laplace.fit(data)\n",
    "mu_hat_cf, b_hat_cf = laplace_mle_closed_form(data)\n",
    "\n",
    "print(\"fit (scipy)      :\", (float(mu_hat_sp), float(b_hat_sp)))\n",
    "print(\"fit (closed-form):\", (mu_hat_cf, b_hat_cf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d9b6a",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing\n",
    "The Laplace likelihood leads to simple likelihood-ratio tests (LRTs). We'll demonstrate an LRT for the location parameter:\n",
    "\n",
    "- \\(H_0: \\mu = \\mu_0\\)\n",
    "- \\(H_1: \\mu\\) free\n",
    "\n",
    "We keep \\(b\\) unknown in both models (estimated by MLE). Asymptotically, the LRT statistic is \\(\\chi^2\\) with 1 degree of freedom.\n",
    "\n",
    "### Bayesian modeling\n",
    "A Laplace prior \\(p(\\theta) \\propto \\exp(-|\\theta|/b)\\) induces **sparsity/shrinkage**. Under a Gaussian likelihood, the MAP estimator becomes **soft-thresholding** (the 1D analog of Lasso).\n",
    "\n",
    "### Generative modeling\n",
    "Laplace noise is used as a primitive generator for **privacy-preserving releases** (Laplace mechanism), and also as a building block in mixture models and robust generative pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74706c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_lrt_mu(data: np.ndarray, mu0: float) -> tuple[float, float]:\n",
    "    \"\"\"Likelihood-ratio test for H0: μ=mu0 vs H1: μ free (b unknown in both).\"\"\"\n",
    "    x = np.asarray(data, dtype=float)\n",
    "    mu_hat, b_hat = laplace_mle_closed_form(x)\n",
    "\n",
    "    b0_hat = float(np.mean(np.abs(x - mu0)))\n",
    "    b0_hat = float(max(b0_hat, 1e-12))\n",
    "\n",
    "    ll_hat = laplace_loglik(x, mu=mu_hat, b=b_hat)\n",
    "    ll_0 = laplace_loglik(x, mu=mu0, b=b0_hat)\n",
    "\n",
    "    LR = 2.0 * (ll_hat - ll_0)\n",
    "    p_value = float(chi2.sf(LR, df=1))\n",
    "    return float(LR), p_value\n",
    "\n",
    "\n",
    "mu0 = 0.0\n",
    "b_true = 1.0\n",
    "n = 120\n",
    "\n",
    "# Under H0\n",
    "data0 = laplace_rvs_inverse(rng, size=n, mu=mu0, b=b_true)\n",
    "LR0, p0 = laplace_lrt_mu(data0, mu0=mu0)\n",
    "print(f\"H0 sample: LR={LR0:.3f}, p={p0:.3f}\")\n",
    "\n",
    "# Under H1\n",
    "mu_true = 0.7\n",
    "data1 = laplace_rvs_inverse(rng, size=n, mu=mu_true, b=b_true)\n",
    "LR1, p1 = laplace_lrt_mu(data1, mu0=mu0)\n",
    "print(f\"H1 sample: LR={LR1:.3f}, p={p1:.3e}\")\n",
    "\n",
    "# Quick type-I check (asymptotic calibration)\n",
    "m = 1500\n",
    "pvals = np.empty(m)\n",
    "for i in range(m):\n",
    "    d = laplace_rvs_inverse(rng, size=n, mu=mu0, b=b_true)\n",
    "    _, pvals[i] = laplace_lrt_mu(d, mu0=mu0)\n",
    "print(\"approx P(p<0.05) under H0:\", float(np.mean(pvals < 0.05)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(y: np.ndarray, tau: float) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    return np.sign(y) * np.maximum(np.abs(y) - tau, 0.0)\n",
    "\n",
    "\n",
    "# Bayesian demo: Gaussian likelihood with Laplace prior\n",
    "# y | theta ~ N(theta, sigma^2),   theta ~ Laplace(0, b)\n",
    "sigma = 1.0\n",
    "b_prior = 0.7\n",
    "\n",
    "# MAP has a closed-form soft-threshold in this 1D model\n",
    "tau = sigma**2 / b_prior\n",
    "ys = np.linspace(-4, 4, 401)\n",
    "theta_map = soft_threshold(ys, tau=tau)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ys, y=theta_map, mode=\"lines\", name=\"MAP( y )\"))\n",
    "fig.add_trace(go.Scatter(x=ys, y=ys, mode=\"lines\", name=\"MLE=y\", line=dict(dash=\"dash\")))\n",
    "fig.update_layout(\n",
    "    title=f\"Laplace prior shrinkage (b={b_prior}, sigma={sigma})\",\n",
    "    xaxis_title=\"observation y\",\n",
    "    yaxis_title=\"MAP estimate of theta\",\n",
    "    width=850,\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Visualize the posterior for a single observation\n",
    "y0 = 1.2\n",
    "theta_grid = np.linspace(-5, 5, 2001)\n",
    "\n",
    "log_like = norm.logpdf(y0, loc=theta_grid, scale=sigma)\n",
    "log_prior = laplace_logpdf(theta_grid, mu=0.0, b=b_prior)\n",
    "log_post = log_like + log_prior\n",
    "log_post -= log_post.max()\n",
    "post_unnorm = np.exp(log_post)\n",
    "post = post_unnorm / np.trapz(post_unnorm, theta_grid)\n",
    "\n",
    "theta_map_grid = float(theta_grid[np.argmax(post)])\n",
    "theta_map_closed = float(soft_threshold(y0, tau=tau))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=theta_grid, y=post, mode=\"lines\", name=\"posterior\"))\n",
    "fig.add_vline(x=theta_map_grid, line_dash=\"dash\", line_color=\"black\", annotation_text=\"MAP\")\n",
    "fig.update_layout(\n",
    "    title=f\"Posterior p(theta|y={y0}) with Laplace prior\",\n",
    "    xaxis_title=\"theta\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"MAP (grid)  :\", theta_map_grid)\n",
    "print(\"MAP (closed):\", theta_map_closed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ddbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative / differential privacy-style demo: Laplace mechanism for a count query\n",
    "# For a count, sensitivity Δf = 1. Laplace mechanism adds noise ~ Laplace(0, Δf/ε) = Laplace(0, 1/ε).\n",
    "\n",
    "true_count = 250\n",
    "eps_values = [0.25, 0.5, 1.0, 2.0]  # larger ε => less noise\n",
    "n = 60_000\n",
    "\n",
    "fig = go.Figure()\n",
    "for eps in eps_values:\n",
    "    b_noise = 1.0 / eps\n",
    "    noise = laplace_rvs_inverse(rng, size=n, mu=0.0, b=b_noise)\n",
    "    released = true_count + noise\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=released,\n",
    "            histnorm=\"probability density\",\n",
    "            nbinsx=140,\n",
    "            opacity=0.35,\n",
    "            name=f\"ε={eps} (b={b_noise:.2f})\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Laplace mechanism: noisy releases of a count\",\n",
    "    xaxis_title=\"released value\",\n",
    "    yaxis_title=\"density\",\n",
    "    barmode=\"overlay\",\n",
    "    width=950,\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Tail bound: P(|Noise| > t) = exp(-t/b) = exp(-ε t)\n",
    "alpha = 0.05\n",
    "for eps in eps_values:\n",
    "    b_noise = 1.0 / eps\n",
    "    t95 = b_noise * np.log(1.0 / alpha)\n",
    "    print(f\"ε={eps:>4}: P(|err|>t)={alpha} at t={t95:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1057d68",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: the scale must satisfy **b > 0**. Many routines will return NaNs or raise errors if b ≤ 0.\n",
    "- **Scale vs standard deviation**: for Laplace, \\(\\mathrm{sd}=\\sqrt{2}\\,b\\). If you want a target variance \\(\\sigma^2\\), set \\(b=\\sigma/\\sqrt{2}\\).\n",
    "- **MGF domain**: \\(M_X(t)\\) only exists for \\(|t|<1/b\\). (The characteristic function exists for all t.)\n",
    "- **Sampling numerics**: inverse-CDF sampling involves \\(\\log U\\). Clip `U` away from 0 and 1 to avoid `log(0)`.\n",
    "- **Degenerate samples**: if all observations are identical, the likelihood increases as b → 0+. Any practical MLE needs a small floor.\n",
    "- **Fitting and medians**: for even n, the minimizing set of medians is an interval. `np.median` returns the midpoint, which is still optimal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2f342",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `laplace` is a **continuous** distribution on \\((-∞,∞)\\) with location μ and scale b>0.\n",
    "- PDF: \\(f(x)=\\frac{1}{2b}\\exp(-|x-\\mu|/b)\\); CDF is piecewise exponential.\n",
    "- Mean = μ, variance = \\(2b^2\\), skewness = 0, excess kurtosis = 3, entropy = \\(1+\\log(2b)\\).\n",
    "- MLE: \\(\\hat\\mu\\) is a sample median and \\(\\hat b\\) is the mean absolute deviation from \\(\\hat\\mu\\).\n",
    "- Sampling (NumPy-only): inverse CDF or difference of exponentials.\n",
    "- Common uses: robust modeling (L1 loss), sparse priors (Bayesian Lasso), and privacy noise (Laplace mechanism).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}