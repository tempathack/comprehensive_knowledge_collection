{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2322b0fb",
   "metadata": {},
   "source": [
    "# Beta Distribution — Modeling Uncertain Probabilities\n",
    "\n",
    "The **beta distribution** is the workhorse distribution for *random variables constrained to* $[0,1]$.\n",
    "It is especially important for modeling **uncertain probabilities** (e.g., conversion rates, success probabilities, fractions).\n",
    "\n",
    "## What you’ll learn\n",
    "- what the beta distribution models and why it’s useful\n",
    "- the PDF/CDF and how it relates to the **beta function** and **gamma function**\n",
    "- closed-form moments (mean/variance/skewness/kurtosis), MGF/CF, and entropy\n",
    "- how $(\u0007lpha,\beta)$ control shape, mean, and concentration\n",
    "- a **NumPy-only** sampler (via Gamma sampling) + Monte Carlo validation\n",
    "- practical usage via `scipy.stats.beta` (`pdf`, `cdf`, `rvs`, `fit`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a741833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import stats, special\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48f6e5",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: Beta distribution\n",
    "- **Type**: **Continuous**\n",
    "- **Support**: $x \\in [0, 1]$ (density is defined for $0 < x < 1$)\n",
    "- **Parameters**: shape parameters $\u0007lpha > 0$, $\beta > 0$\n",
    "\n",
    "We write:\n",
    "\n",
    "$$X \\sim \\mathrm{Beta}(\u0007lpha, \beta).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56173736",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### 2.1 What it models\n",
    "The beta distribution models a **random proportion** or **random probability**.\n",
    "If you know a quantity must live in $[0,1]$ (a fraction, rate, probability, mixture weight), the beta distribution is often the first candidate.\n",
    "\n",
    "### 2.2 Real-world use cases\n",
    "- **Conversion rate / click-through rate**: unknown probability of success\n",
    "- **A/B testing**: uncertainty over two proportions\n",
    "- **Reliability / defect rates**: fraction of items that fail\n",
    "- **Normalized quantities**: e.g., fraction of budget, fraction of time spent\n",
    "- **Random mixing weights**: blend two components with a random weight $w \\in [0,1]$\n",
    "\n",
    "### 2.3 Relations to other distributions\n",
    "- **Conjugate prior** for Bernoulli/Binomial likelihoods (Beta–Bernoulli / Beta–Binomial).\n",
    "- **Special cases**:\n",
    "  - $\\mathrm{Beta}(1,1)$ is **Uniform** on $[0,1]$.\n",
    "  - $\\mathrm{Beta}(\tfrac{1}{2},\tfrac{1}{2})$ is the **arcsine** distribution.\n",
    "- **Order statistics**: if $U_{(k)}$ is the $k$-th order statistic of $n$ iid Uniform$(0,1)$ samples,\n",
    "  $$U_{(k)} \\sim \\mathrm{Beta}(k, n+1-k).$$\n",
    "- **Gamma connection**: if $G_1 \\sim \\mathrm{Gamma}(\u0007lpha,1)$ and $G_2 \\sim \\mathrm{Gamma}(\beta,1)$ independently,\n",
    "  $$\f",
    "rac{G_1}{G_1 + G_2} \\sim \\mathrm{Beta}(\u0007lpha,\beta).$$\n",
    "- **Dirichlet**: Beta is the 2D special case of a Dirichlet distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b5167",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### 3.1 Beta function\n",
    "The **beta function** is\n",
    "\n",
    "$$B(\u0007lpha,\beta) = \\int_0^1 t^{\u0007lpha-1}(1-t)^{\beta-1}\\,dt = \f",
    "rac{\\Gamma(\u0007lpha)\\Gamma(\beta)}{\\Gamma(\u0007lpha+\beta)}.$$\n",
    "\n",
    "### 3.2 PDF\n",
    "For $0 < x < 1$:\n",
    "\n",
    "$$f(x\\mid\u0007lpha,\beta) = \f",
    "rac{1}{B(\u0007lpha,\beta)}\\,x^{\u0007lpha-1}(1-x)^{\beta-1}.$$\n",
    "\n",
    "### 3.3 CDF\n",
    "The CDF can be written using the **regularized incomplete beta function** $I_x(\u0007lpha,\beta)$:\n",
    "\n",
    "$$F(x\\mid\u0007lpha,\beta) = I_x(\u0007lpha,\beta)\n",
    "= \f",
    "rac{\\int_0^x t^{\u0007lpha-1}(1-t)^{\beta-1}\\,dt}{B(\u0007lpha,\beta)}.$$\n",
    "\n",
    "In SciPy, the CDF is computed via this special function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245dbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_pdf(x: np.ndarray, alpha: float, beta: float) -> np.ndarray:\n",
    "    '''Numerically stable beta PDF via log-space (uses SciPy special functions).'''\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    log_pdf = (\n",
    "        (alpha - 1) * np.log(x)\n",
    "        + (beta - 1) * np.log1p(-x)\n",
    "        - special.betaln(alpha, beta)\n",
    "    )\n",
    "    return np.exp(log_pdf)\n",
    "\n",
    "\n",
    "def beta_cdf(x: np.ndarray, alpha: float, beta: float) -> np.ndarray:\n",
    "    '''Beta CDF via regularized incomplete beta I_x(alpha, beta).'''\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return special.betainc(alpha, beta, x)\n",
    "\n",
    "\n",
    "# Quick sanity check: PDF integrates to ~1\n",
    "alpha0, beta0 = 2.0, 5.0\n",
    "xgrid = np.linspace(1e-6, 1 - 1e-6, 200_000)\n",
    "area = np.trapz(beta_pdf(xgrid, alpha0, beta0), xgrid)\n",
    "area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e2a85",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### 4.1 Mean, variance, skewness, kurtosis\n",
    "For $X \\sim \\mathrm{Beta}(\u0007lpha,\beta)$:\n",
    "\n",
    "- **Mean**\n",
    "  $$\\mathbb{E}[X] = \f",
    "rac{\u0007lpha}{\u0007lpha+\beta}.$$\n",
    "\n",
    "- **Variance**\n",
    "  $$\\mathrm{Var}(X) = \f",
    "rac{\u0007lpha\beta}{(\u0007lpha+\beta)^2(\u0007lpha+\beta+1)}.$$\n",
    "\n",
    "- **Skewness**\n",
    "  $$\\gamma_1 = \f",
    "rac{2(\beta-\u0007lpha)\\sqrt{\u0007lpha+\beta+1}}{(\u0007lpha+\beta+2)\\sqrt{\u0007lpha\beta}}.$$\n",
    "\n",
    "- **Excess kurtosis** (kurtosis minus 3)\n",
    "  $$\\gamma_2 = \f",
    "rac{6\big[(\u0007lpha-\beta)^2(\u0007lpha+\beta+1) - \u0007lpha\beta(\u0007lpha+\beta+2)\big]}\n",
    "  {\u0007lpha\beta(\u0007lpha+\beta+2)(\u0007lpha+\beta+3)}.$$\n",
    "\n",
    "A useful compact identity for raw moments is:\n",
    "\n",
    "$$\\mathbb{E}[X^k] = \f",
    "rac{(\u0007lpha)_{k}}{(\u0007lpha+\beta)_{k}},$$\n",
    "\n",
    "where $(a)_k = a(a+1)\\cdots(a+k-1)$ is the rising factorial (Pochhammer symbol).\n",
    "\n",
    "### 4.2 Mode\n",
    "If $\u0007lpha > 1$ and $\beta > 1$ (unimodal interior case),\n",
    "\n",
    "$$\\mathrm{mode} = \f",
    "rac{\u0007lpha-1}{\u0007lpha+\beta-2}.$$\n",
    "\n",
    "### 4.3 MGF and characteristic function\n",
    "Because the support is bounded, the MGF exists for all real $t$:\n",
    "\n",
    "$$M_X(t) = \\mathbb{E}[e^{tX}] = {}_1F_1(\u0007lpha;\u0007lpha+\beta;t),$$\n",
    "\n",
    "where ${}_1F_1$ is the confluent hypergeometric function.\n",
    "The characteristic function is\n",
    "\n",
    "$$\u000b",
    "arphi_X(t) = M_X(it) = {}_1F_1(\u0007lpha;\u0007lpha+\beta;it).$$\n",
    "\n",
    "### 4.4 Differential entropy\n",
    "The differential entropy is\n",
    "\n",
    "$$h(X) = \\ln B(\u0007lpha,\beta)\n",
    "- (\u0007lpha-1)\\psi(\u0007lpha) - (\beta-1)\\psi(\beta) + (\u0007lpha+\beta-2)\\psi(\u0007lpha+\beta),$$\n",
    "\n",
    "where $\\psi$ is the digamma function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_moments(alpha: float, beta: float) -> dict:\n",
    "    a, b = float(alpha), float(beta)\n",
    "    mean = a / (a + b)\n",
    "    var = a * b / ((a + b) ** 2 * (a + b + 1))\n",
    "    skew = 2 * (b - a) * np.sqrt(a + b + 1) / ((a + b + 2) * np.sqrt(a * b))\n",
    "    excess_kurt = (\n",
    "        6\n",
    "        * (((a - b) ** 2) * (a + b + 1) - a * b * (a + b + 2))\n",
    "        / (a * b * (a + b + 2) * (a + b + 3))\n",
    "    )\n",
    "    mode = np.nan\n",
    "    if a > 1 and b > 1:\n",
    "        mode = (a - 1) / (a + b - 2)\n",
    "\n",
    "    mgf = lambda t: special.hyp1f1(a, a + b, t)\n",
    "\n",
    "    entropy = (\n",
    "        special.betaln(a, b)\n",
    "        - (a - 1) * special.digamma(a)\n",
    "        - (b - 1) * special.digamma(b)\n",
    "        + (a + b - 2) * special.digamma(a + b)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"var\": var,\n",
    "        \"skew\": skew,\n",
    "        \"excess_kurtosis\": excess_kurt,\n",
    "        \"mode\": mode,\n",
    "        \"entropy\": entropy,\n",
    "        \"mgf\": mgf,\n",
    "    }\n",
    "\n",
    "\n",
    "m = beta_moments(alpha0, beta0)\n",
    "{k: v for k, v in m.items() if k != \"mgf\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48350b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check of mean/variance + MGF at a few t\n",
    "n = 200_000\n",
    "samples_scipy = stats.beta(alpha0, beta0).rvs(size=n, random_state=rng)\n",
    "\n",
    "mc_mean = samples_scipy.mean()\n",
    "mc_var = samples_scipy.var(ddof=0)\n",
    "\n",
    "mc_mgf_1 = np.mean(np.exp(1.0 * samples_scipy))\n",
    "mc_mgf_m1 = np.mean(np.exp(-1.0 * samples_scipy))\n",
    "\n",
    "(\n",
    "    m[\"mean\"],\n",
    "    mc_mean,\n",
    "    m[\"var\"],\n",
    "    mc_var,\n",
    "    m[\"mgf\"](1.0),\n",
    "    mc_mgf_1,\n",
    "    m[\"mgf\"](-1.0),\n",
    "    mc_mgf_m1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b646030",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The parameters $(\u0007lpha,\beta)$ control both **where** the mass is (mean) and **how concentrated** it is.\n",
    "\n",
    "### 5.1 Mean–concentration reparameterization\n",
    "A common and very interpretable reparameterization uses:\n",
    "\n",
    "- mean $m \\in (0,1)$\n",
    "- concentration $\\kappa > 0$\n",
    "\n",
    "with\n",
    "\n",
    "$$\u0007lpha = \\kappa m, \\qquad \beta = \\kappa(1-m).$$\n",
    "\n",
    "- Increasing **$\\kappa$** (holding $m$ fixed) makes the distribution **tighter** around $m$.\n",
    "- Moving **$m$** (holding $\\kappa$ fixed) shifts the mass left/right.\n",
    "\n",
    "### 5.2 Shape regimes\n",
    "- $\u0007lpha = \beta = 1$: **uniform**\n",
    "- $\u0007lpha,\beta > 1$: unimodal, finite density at boundaries\n",
    "- $\u0007lpha < 1$ or $\beta < 1$: density diverges near 0 and/or 1 (still integrable)\n",
    "- $\u0007lpha = \beta$: symmetric around 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38004b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1e-4, 1 - 1e-4, 600)\n",
    "\n",
    "param_sets = [\n",
    "    (0.5, 0.5, \"U-shaped (0.5,0.5)\"),\n",
    "    (1.0, 1.0, \"Uniform (1,1)\"),\n",
    "    (2.0, 2.0, \"Symmetric peak (2,2)\"),\n",
    "    (2.0, 5.0, \"Skewed right (2,5)\"),\n",
    "    (5.0, 2.0, \"Skewed left (5,2)\"),\n",
    "    (8.0, 1.5, \"Mass near 1 (8,1.5)\"),\n",
    "]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a, b, label in param_sets:\n",
    "    fig.add_trace(go.Scatter(x=x, y=beta_pdf(x, a, b), mode=\"lines\", name=label))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Beta PDF for different (α, β)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191623d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same mean, different concentration κ\n",
    "m_fixed = 0.3\n",
    "kappas = [2, 5, 20, 100]\n",
    "\n",
    "fig = go.Figure()\n",
    "for kappa in kappas:\n",
    "    a = kappa * m_fixed\n",
    "    b = kappa * (1 - m_fixed)\n",
    "    fig.add_trace(go.Scatter(x=x, y=beta_pdf(x, a, b), mode=\"lines\", name=f\"κ={kappa}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Same mean (m=0.3), increasing concentration κ\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b0f32",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "Starting from the definition:\n",
    "\n",
    "\begin{align}\n",
    "\\mathbb{E}[X]\n",
    "&= \\int_0^1 x\\,f(x\\mid\u0007lpha,\beta)\\,dx \\\n",
    "&= \f",
    "rac{1}{B(\u0007lpha,\beta)}\\int_0^1 x^{\u0007lpha}(1-x)^{\beta-1}\\,dx \\\n",
    "&= \f",
    "rac{B(\u0007lpha+1,\beta)}{B(\u0007lpha,\beta)}.\n",
    "\\end{align}\n",
    "\n",
    "Using $B(\u0007lpha,\beta)=\f",
    "rac{\\Gamma(\u0007lpha)\\Gamma(\beta)}{\\Gamma(\u0007lpha+\beta)}$:\n",
    "\n",
    "\begin{align}\n",
    "\f",
    "rac{B(\u0007lpha+1,\beta)}{B(\u0007lpha,\beta)}\n",
    "&= \f",
    "rac{\\Gamma(\u0007lpha+1)\\Gamma(\beta)\\,\\Gamma(\u0007lpha+\beta)}{\\Gamma(\u0007lpha+\beta+1)\\,\\Gamma(\u0007lpha)\\Gamma(\beta)} \\\n",
    "&= \f",
    "rac{\u0007lpha\\,\\Gamma(\u0007lpha)\\,\\Gamma(\u0007lpha+\beta)}{(\u0007lpha+\beta)\\,\\Gamma(\u0007lpha+\beta)} \\\n",
    "&= \f",
    "rac{\u0007lpha}{\u0007lpha+\beta}.\n",
    "\\end{align}\n",
    "\n",
    "### 6.2 Variance\n",
    "Compute $\\mathbb{E}[X^2]$ similarly:\n",
    "\n",
    "\begin{align}\n",
    "\\mathbb{E}[X^2]\n",
    "&= \f",
    "rac{1}{B(\u0007lpha,\beta)}\\int_0^1 x^{\u0007lpha+1}(1-x)^{\beta-1}\\,dx \\\n",
    "&= \f",
    "rac{B(\u0007lpha+2,\beta)}{B(\u0007lpha,\beta)}\n",
    "= \f",
    "rac{\u0007lpha(\u0007lpha+1)}{(\u0007lpha+\beta)(\u0007lpha+\beta+1)}.\n",
    "\\end{align}\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\mathrm{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2\n",
    "= \f",
    "rac{\u0007lpha\beta}{(\u0007lpha+\beta)^2(\u0007lpha+\beta+1)}.$$\n",
    "\n",
    "### 6.3 Likelihood (iid sample)\n",
    "For data $x_1,\\ldots,x_n \\in (0,1)$ iid from $\\mathrm{Beta}(\u0007lpha,\beta)$:\n",
    "\n",
    "\begin{align}\n",
    "L(\u0007lpha,\beta) &= \\prod_{i=1}^n \f",
    "rac{1}{B(\u0007lpha,\beta)} x_i^{\u0007lpha-1}(1-x_i)^{\beta-1} \\\n",
    "\\ell(\u0007lpha,\beta) &= \\log L(\u0007lpha,\beta) \\\n",
    "&= -n\\log B(\u0007lpha,\beta) + (\u0007lpha-1)\\sum_{i=1}^n \\log x_i + (\beta-1)\\sum_{i=1}^n \\log(1-x_i).\n",
    "\\end{align}\n",
    "\n",
    "Maximizing $\\ell(\u0007lpha,\beta)$ (MLE) has no closed-form solution in general; it’s typically solved numerically.\n",
    "SciPy’s `beta.fit` does this for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_loglikelihood(x: np.ndarray, alpha: float, beta: float) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if np.any((x <= 0) | (x >= 1)):\n",
    "        return -np.inf\n",
    "\n",
    "    n = x.size\n",
    "    return (\n",
    "        -n * special.betaln(alpha, beta)\n",
    "        + (alpha - 1) * np.sum(np.log(x))\n",
    "        + (beta - 1) * np.sum(np.log1p(-x))\n",
    "    )\n",
    "\n",
    "\n",
    "# Example log-likelihood value\n",
    "beta_loglikelihood(samples_scipy[:1000], alpha0, beta0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b3fce",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "A numerically convenient way to sample from a beta distribution uses the **Gamma ratio identity**:\n",
    "\n",
    "1. Sample $G_1 \\sim \\mathrm{Gamma}(\u0007lpha, 1)$\n",
    "2. Sample $G_2 \\sim \\mathrm{Gamma}(\beta, 1)$\n",
    "3. Return\n",
    "   $$X = \f",
    "rac{G_1}{G_1 + G_2}.$$\n",
    "\n",
    "So the core task is: **sample Gamma(shape, scale=1)** using only NumPy.\n",
    "\n",
    "### 7.1 Gamma sampling: Marsaglia–Tsang\n",
    "For shape $k \\ge 1$, Marsaglia & Tsang (2000) propose an efficient acceptance–rejection method.\n",
    "For $k < 1$, we can use the reduction:\n",
    "\n",
    "$$\\mathrm{Gamma}(k) \\overset{d}{=} \\mathrm{Gamma}(k+1)\\,U^{1/k}, \\quad U\\sim\\mathrm{Uniform}(0,1).$$\n",
    "\n",
    "We implement this below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_rvs_numpy(shape: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''Sample Gamma(shape, scale=1) using NumPy only (Marsaglia-Tsang).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape:\n",
    "        k > 0\n",
    "    size:\n",
    "        number of samples\n",
    "    rng:\n",
    "        NumPy Generator\n",
    "    '''\n",
    "\n",
    "    k = float(shape)\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"shape must be > 0\")\n",
    "\n",
    "    # k < 1: boost to k+1 and apply power transform\n",
    "    if k < 1:\n",
    "        g = gamma_rvs_numpy(k + 1.0, size, rng)\n",
    "        u = rng.random(size)\n",
    "        return g * (u ** (1.0 / k))\n",
    "\n",
    "    # k >= 1: Marsaglia-Tsang\n",
    "    d = k - 1.0 / 3.0\n",
    "    c = 1.0 / np.sqrt(9.0 * d)\n",
    "\n",
    "    out = np.empty(size, dtype=float)\n",
    "    filled = 0\n",
    "\n",
    "    while filled < size:\n",
    "        n = size - filled\n",
    "        x = rng.standard_normal(n)\n",
    "        v = (1.0 + c * x)\n",
    "        v = v * v * v  # (1 + c x)^3\n",
    "        u = rng.random(n)\n",
    "\n",
    "        positive = v > 0\n",
    "\n",
    "        # First (cheap) acceptance\n",
    "        accept = positive & (u < 1.0 - 0.0331 * (x**4))\n",
    "\n",
    "        # Second acceptance (log test) - compute log(v) only where v > 0 to avoid warnings\n",
    "        log_v = np.zeros_like(v)\n",
    "        log_v[positive] = np.log(v[positive])\n",
    "\n",
    "        accept2 = positive & (~accept) & (\n",
    "            np.log(u) < 0.5 * x * x + d * (1.0 - v + log_v)\n",
    "        )\n",
    "\n",
    "        accept = accept | accept2\n",
    "        accepted = d * v[accept]\n",
    "\n",
    "        take = min(accepted.size, n)\n",
    "        out[filled : filled + take] = accepted[:take]\n",
    "        filled += take\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def beta_rvs_numpy(alpha: float, beta: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''Sample Beta(alpha, beta) using Gamma ratio with NumPy-only Gamma sampler.'''\n",
    "    g1 = gamma_rvs_numpy(alpha, size, rng)\n",
    "    g2 = gamma_rvs_numpy(beta, size, rng)\n",
    "    return g1 / (g1 + g2)\n",
    "\n",
    "\n",
    "# Monte Carlo validation against theory\n",
    "n = 200_000\n",
    "samples_numpy = beta_rvs_numpy(alpha0, beta0, n, rng)\n",
    "\n",
    "np.mean(samples_numpy), np.var(samples_numpy), m[\"mean\"], m[\"var\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d37381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NumPy-only sampler to SciPy sampler (quick KS test)\n",
    "ks = stats.ks_2samp(samples_numpy[:20_000], samples_scipy[:20_000])\n",
    "ks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29850bc4",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the theoretical **PDF** and **CDF**\n",
    "- **Monte Carlo** samples from our NumPy-only sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF + histogram (Monte Carlo)\n",
    "x = np.linspace(1e-4, 1 - 1e-4, 800)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples_numpy,\n",
    "        nbinsx=60,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"Monte Carlo (NumPy-only)\",\n",
    "        opacity=0.55,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=stats.beta(alpha0, beta0).pdf(x),\n",
    "        mode=\"lines\",\n",
    "        name=\"True PDF (SciPy)\",\n",
    "        line=dict(width=3),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Beta({alpha0}, {beta0}): histogram vs PDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF: theoretical vs empirical\n",
    "x = np.linspace(0, 1, 600)\n",
    "\n",
    "emp_x = np.sort(samples_numpy)\n",
    "emp_cdf = np.arange(1, emp_x.size + 1) / emp_x.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.beta(alpha0, beta0).cdf(x), mode=\"lines\", name=\"True CDF\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=emp_x[::200],\n",
    "        y=emp_cdf[::200],\n",
    "        mode=\"markers\",\n",
    "        name=\"Empirical CDF (subsampled)\",\n",
    "        marker=dict(size=4, opacity=0.6),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Beta({alpha0}, {beta0}): theoretical CDF vs empirical CDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"CDF\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d1b94",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.beta`)\n",
    "\n",
    "SciPy parameterization:\n",
    "\n",
    "```python\n",
    "stats.beta(a=alpha, b=beta, loc=0, scale=1)\n",
    "```\n",
    "\n",
    "- `a`, `b` are the shape parameters $\u0007lpha$, $\beta$.\n",
    "- `loc` and `scale` allow you to shift/scale the support from $[0,1]$ to $[\text{loc},\text{loc}+\text{scale}]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.beta(alpha0, beta0)  # loc=0, scale=1 by default\n",
    "\n",
    "x = np.linspace(0, 1, 6)\n",
    "\n",
    "pdf = dist.pdf(x)\n",
    "cdf = dist.cdf(x)\n",
    "samples = dist.rvs(size=5, random_state=rng)\n",
    "\n",
    "pdf, cdf, samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting (MLE) with SciPy\n",
    "# If you KNOW the data live on [0, 1], it's common to fix loc=0 and scale=1.\n",
    "\n",
    "a_hat, b_hat, loc_hat, scale_hat = stats.beta.fit(samples_numpy[:10_000], floc=0, fscale=1)\n",
    "a_hat, b_hat, loc_hat, scale_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb12599",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing / confidence intervals for proportions\n",
    "A classic exact confidence interval for a Binomial proportion (Clopper–Pearson) uses **Beta quantiles**.\n",
    "\n",
    "If $K \\sim \\mathrm{Binomial}(n, p)$ and you observe $k$ successes, a two-sided $(1-\u0007lpha)$ interval is:\n",
    "\n",
    "- lower endpoint: $\\mathrm{Beta}^{-1}(\u0007lpha/2;\\; k,\\; n-k+1)$\n",
    "- upper endpoint: $\\mathrm{Beta}^{-1}(1-\u0007lpha/2;\\; k+1,\\; n-k)$\n",
    "\n",
    "(where $\\mathrm{Beta}^{-1}$ is the inverse CDF / quantile function).\n",
    "\n",
    "### 10.2 Bayesian modeling (Beta–Bernoulli / Beta–Binomial)\n",
    "Let $p$ be an unknown success probability.\n",
    "\n",
    "- Prior: $p \\sim \\mathrm{Beta}(\u0007lpha_0,\beta_0)$\n",
    "- Data: $k$ successes and $n-k$ failures\n",
    "\n",
    "Posterior is conjugate:\n",
    "\n",
    "$$p\\mid\text{data} \\sim \\mathrm{Beta}(\u0007lpha_0+k,\beta_0+n-k).$$\n",
    "\n",
    "This is often interpreted as adding **pseudo-counts** to observed counts.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "- Sample random probabilities $p$ for Bernoulli events.\n",
    "- Sample mixing weights $w$ for two-component mixtures.\n",
    "- Generalization: Dirichlet for $K$-way mixture weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Clopper–Pearson interval\n",
    "n = 100\n",
    "k = 37\n",
    "alpha_level = 0.05\n",
    "\n",
    "if k == 0:\n",
    "    cp_low = 0.0\n",
    "else:\n",
    "    cp_low = stats.beta.ppf(alpha_level / 2, k, n - k + 1)\n",
    "\n",
    "if k == n:\n",
    "    cp_high = 1.0\n",
    "else:\n",
    "    cp_high = stats.beta.ppf(1 - alpha_level / 2, k + 1, n - k)\n",
    "\n",
    "(cp_low, cp_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Bayesian update for a Bernoulli probability\n",
    "alpha_prior, beta_prior = 2.0, 2.0\n",
    "\n",
    "alpha_post = alpha_prior + k\n",
    "beta_post = beta_prior + (n - k)\n",
    "\n",
    "prior = stats.beta(alpha_prior, beta_prior)\n",
    "post = stats.beta(alpha_post, beta_post)\n",
    "\n",
    "x = np.linspace(1e-4, 1 - 1e-4, 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=prior.pdf(x), mode=\"lines\", name=f\"Prior Beta({alpha_prior:.0f},{beta_prior:.0f})\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=post.pdf(x), mode=\"lines\", name=f\"Posterior Beta({alpha_post:.0f},{beta_post:.0f})\", line=dict(width=3)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Bayesian update (n={n}, k={k})\",\n",
    "    xaxis_title=\"p\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior probability of beating a threshold (Bayesian hypothesis-style query)\n",
    "threshold = 0.4\n",
    "post_prob = 1 - post.cdf(threshold)\n",
    "post_mean = post.mean()\n",
    "post_prob, post_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4e8de",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: $\u0007lpha \\le 0$ or $\beta \\le 0$ is not a valid beta distribution.\n",
    "- **Boundary data (0 or 1)**:\n",
    "  - The continuous Beta model assigns probability 0 to exact endpoints.\n",
    "  - Log-likelihood involves $\\log x$ and $\\log(1-x)$, which become $-\\infty$ at 0 or 1.\n",
    "  - If your data contain many exact 0/1 values, consider a **zero/one-inflated beta** model or add a measurement model.\n",
    "- **Numerical issues near 0 or 1**:\n",
    "  - For $\u0007lpha<1$ or $\beta<1$, the density can diverge near the boundaries; use **log-space** (`betaln`, `logpdf`).\n",
    "  - When plotting, avoid evaluating exactly at 0 or 1; use small epsilons.\n",
    "- **Fitting**:\n",
    "  - `stats.beta.fit` will estimate `loc` and `scale` unless fixed.\n",
    "  - If data are already in $[0,1]$, using `floc=0, fscale=1` avoids spurious shifts/scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366e425",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- The beta distribution is a flexible family on $[0,1]$, ideal for modeling **uncertain probabilities**.\n",
    "- $(\u0007lpha,\beta)$ control shape; $m=\u0007lpha/(\u0007lpha+\beta)$ is the mean and $\\kappa=\u0007lpha+\beta$ acts like a concentration.\n",
    "- Key formulas (mean/variance/skewness/kurtosis) are closed-form; the CDF uses the regularized incomplete beta function.\n",
    "- Sampling is easy via the **Gamma ratio** identity; SciPy provides a robust reference implementation (`scipy.stats.beta`).\n",
    "\n",
    "**References**\n",
    "- Marsaglia, G. & Tsang, W. W. (2000). *A Simple Method for Generating Gamma Variables*.\n",
    "- SciPy documentation: `scipy.stats.beta`, `scipy.special.betainc`, `scipy.special.hyp1f1`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
