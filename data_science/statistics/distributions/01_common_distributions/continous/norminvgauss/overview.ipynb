{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699f20ed",
   "metadata": {},
   "source": [
    "# Normal Inverse Gaussian Distribution (`norminvgauss`)\n",
    "\n",
    "The **Normal Inverse Gaussian (NIG)** distribution is a flexible **continuous** distribution on \\(\\mathbb{R}\\) that can capture **heavy tails** *and* **skewness**.\n",
    "\n",
    "A key intuition is that NIG is a **normal mean–variance mixture**:\n",
    "\n",
    "- first draw a positive random variance \\(V\\) from an **inverse Gaussian** distribution\n",
    "- then draw \\(X\\mid V\\) from a **normal distribution** with that random variance\n",
    "\n",
    "This mixture view explains why NIG is popular in applications (e.g. **financial returns**, **turbulence**, **stochastic volatility**) and also gives a clean **NumPy-only sampling algorithm**.\n",
    "\n",
    "**Important naming pitfall:** this is *not* the *Normal-Inverse-Gamma* distribution used in Bayesian conjugacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab37840",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & classification\n",
    "2) Intuition & motivation\n",
    "3) Formal definition (PDF/CDF)\n",
    "4) Moments & properties\n",
    "5) Parameter interpretation\n",
    "6) Derivations (\\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), likelihood)\n",
    "7) Sampling & simulation (NumPy-only)\n",
    "8) Visualization (PDF, CDF, Monte Carlo)\n",
    "9) SciPy integration (`scipy.stats.norminvgauss`)\n",
    "10) Statistical use cases\n",
    "11) Pitfalls\n",
    "12) Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy import special, stats\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "print(\"python\", __import__(\"sys\").version.split()[0])\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabcd5c",
   "metadata": {},
   "source": [
    "## Prerequisites & notation\n",
    "\n",
    "**Prerequisites**\n",
    "- comfort with PDFs/CDFs and expectation/variance\n",
    "- basic calculus (differentiation under the integral sign is helpful, but not required)\n",
    "- familiarity with moment generating functions (MGFs) / characteristic functions\n",
    "\n",
    "**Notation (common in the literature)**\n",
    "\n",
    "We will mainly use the \\((\\alpha,\\beta,\\delta,\\mu)\\) parameterization:\n",
    "\n",
    "- \\(\\alpha > 0\\): tail / steepness parameter\n",
    "- \\(\\beta\\in\\mathbb{R}\\): asymmetry (skew) parameter, with constraint \\(|\\beta| < \\alpha\\)\n",
    "- \\(\\delta > 0\\): scale parameter\n",
    "- \\(\\mu\\in\\mathbb{R}\\): location parameter\n",
    "\n",
    "Define\n",
    "\\[\n",
    "\\gamma = \\sqrt{\\alpha^2 - \\beta^2} \\;>\\; 0.\n",
    "\\]\n",
    "\n",
    "**SciPy parameterization**\n",
    "\n",
    "SciPy exposes NIG as `scipy.stats.norminvgauss(a, b, loc=0, scale=1)` with constraints\n",
    "\\(a>0\\) and \\(|b|\\le a\\). The mapping to \\((\\alpha,\\beta,\\delta,\\mu)\\) is:\n",
    "\n",
    "\\[\n",
    "a = \\alpha\\,\\delta,\\qquad b = \\beta\\,\\delta,\\qquad \\text{loc}=\\mu,\\qquad \\text{scale}=\\delta.\n",
    "\\]\n",
    "\n",
    "So if you think in \\((\\alpha,\\beta,\\delta,\\mu)\\), you pass `a=alpha*delta`, `b=beta*delta` to SciPy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd48fd",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `norminvgauss` (Normal Inverse Gaussian / NIG)\n",
    "- **Type**: **continuous**\n",
    "- **Support**: \\(x \\in \\mathbb{R}\\)\n",
    "- **Parameter space** (\\((\\alpha,\\beta,\\delta,\\mu)\\) form):\n",
    "  \\[\n",
    "  \\alpha>0,\\quad \\delta>0,\\quad \\mu\\in\\mathbb{R},\\quad \\beta\\in\\mathbb{R}\\ \\text{with}\\ |\\beta|<\\alpha.\n",
    "  \\]\n",
    "\n",
    "(SciPy form: \\(a>0\\), \\(|b|\\le a\\), `scale>0`, `loc` real; the boundary \\(|b|=a\\) corresponds to \\(\\gamma=0\\) and is numerically delicate.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79664f14",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "\n",
    "NIG is often used when you want a model that is:\n",
    "\n",
    "- **roughly bell-shaped**, but\n",
    "- has **heavier tails** than a normal distribution (more extreme events), and/or\n",
    "- has **skewness** (asymmetric left vs right tails).\n",
    "\n",
    "A powerful intuition is the **mixture representation**:\n",
    "\n",
    "\\[\n",
    "X \\mid V \\;\\sim\\; \\mathcal{N}(\\mu + \\beta V,\\; V),\n",
    "\\qquad\n",
    "V \\sim \\text{Inverse-Gaussian}\\left(\\nu=\\tfrac{\\delta}{\\gamma},\\;\\lambda=\\delta^2\\right),\n",
    "\\]\n",
    "\n",
    "where \\(V>0\\) is random.\n",
    "\n",
    "- Randomizing the **variance** creates heavy tails.\n",
    "- The term \\(\\beta V\\) randomizes the **mean** in a correlated way, creating skewness.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Finance**: log-returns (skew + heavy tails), NIG Lévy process increments\n",
    "- **Stochastic volatility / time change models**: Brownian motion evaluated at inverse-Gaussian random time\n",
    "- **Signal processing**: impulsive / heavy-tailed noise\n",
    "- **Environmental / turbulence data**: heavy-tailed fluctuations\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Generalized hyperbolic family**: NIG is a special case (with \\(\\lambda=-\\tfrac12\\)).\n",
    "- **Normal + inverse Gaussian**: NIG is a *normal mean–variance mixture* with IG mixing.\n",
    "- **Normal limit (heuristic)**: for large \\(\\alpha\\) (with other parameters scaled appropriately), tails become lighter and the distribution becomes closer to normal.\n",
    "\n",
    "A practical mental model:\n",
    "\n",
    "> *NIG behaves like a normal distribution whose variance (and drift) jitters randomly from sample to sample.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d34156",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### PDF (\\((\\alpha,\\beta,\\delta,\\mu)\\) parameterization)\n",
    "\n",
    "Let \\(X \\sim \\text{NIG}(\\alpha,\\beta,\\delta,\\mu)\\) with \\(\\alpha>0\\), \\(\\delta>0\\), and \\(|\\beta|<\\alpha\\).\n",
    "Define \\(\\gamma = \\sqrt{\\alpha^2-\\beta^2}\\).\n",
    "\n",
    "The PDF is\n",
    "\n",
    "\\[\n",
    "f(x\\mid\\alpha,\\beta,\\delta,\\mu)\n",
    "=\n",
    "\\frac{\\alpha\\,\\delta}{\\pi\\,\\sqrt{\\delta^2 + (x-\\mu)^2}}\n",
    "\\;K_1\\!\\left(\\alpha\\,\\sqrt{\\delta^2 + (x-\\mu)^2}\\right)\n",
    "\\;\\exp\\!\\left(\\delta\\,\\gamma + \\beta\\,(x-\\mu)\\right),\n",
    "\\]\n",
    "\n",
    "where \\(K_1\\) is the modified Bessel function of the second kind of order 1.\n",
    "\n",
    "### PDF (SciPy “standardized” form)\n",
    "\n",
    "SciPy defines a standardized density\n",
    "\n",
    "\\[\n",
    "f(y; a,b)\n",
    "= \\frac{a\\,K_1\\!\\left(a\\sqrt{1+y^2}\\right)}{\\pi\\,\\sqrt{1+y^2}}\n",
    "\\exp\\!\\left(\\sqrt{a^2-b^2} + b y\\right),\n",
    "\\qquad y\\in\\mathbb{R}.\n",
    "\\]\n",
    "\n",
    "and then applies a location-scale transform \\(y=(x-\\text{loc})/\\text{scale}\\).\n",
    "\n",
    "### CDF\n",
    "\n",
    "There is no simple elementary closed form for the CDF.\n",
    "It is defined by the integral\n",
    "\n",
    "\\[\n",
    "F(x) = \\mathbb{P}(X\\le x) = \\int_{-\\infty}^{x} f(t)\\,dt,\n",
    "\\]\n",
    "\n",
    "and in practice it is computed numerically (e.g. `scipy.stats.norminvgauss.cdf`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nig_validate(alpha: float, beta: float, delta: float) -> None:\n",
    "    if not (alpha > 0):\n",
    "        raise ValueError(\"alpha must be > 0\")\n",
    "    if not (delta > 0):\n",
    "        raise ValueError(\"delta must be > 0\")\n",
    "    if not (abs(beta) < alpha):\n",
    "        raise ValueError(\"need |beta| < alpha so gamma = sqrt(alpha^2 - beta^2) is real\")\n",
    "\n",
    "\n",
    "def nig_gamma(alpha: float, beta: float) -> float:\n",
    "    nig_validate(alpha, beta, delta=1.0)\n",
    "    return float(math.sqrt(alpha * alpha - beta * beta))\n",
    "\n",
    "\n",
    "def nig_logpdf(x: np.ndarray, alpha: float, beta: float, delta: float, mu: float) -> np.ndarray:\n",
    "    '''Log-PDF using a numerically stable Bessel-K computation.\n",
    "\n",
    "    Uses scipy.special.kve(1, z) = exp(z) * K_1(z) to avoid underflow for large z.\n",
    "    '''\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    xm = x - mu\n",
    "    s2 = delta * delta + xm * xm\n",
    "    s = np.sqrt(s2)\n",
    "\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "    z = alpha * s\n",
    "\n",
    "    # log K1(z) via scaled Bessel: K1(z) = exp(-z) * kve(1,z)\n",
    "    log_k1 = np.log(special.kve(1.0, z)) - z\n",
    "\n",
    "    return (\n",
    "        math.log(alpha * delta)\n",
    "        - math.log(math.pi)\n",
    "        - np.log(s)\n",
    "        + delta * gamma\n",
    "        + beta * xm\n",
    "        + log_k1\n",
    "    )\n",
    "\n",
    "\n",
    "def nig_pdf(x: np.ndarray, alpha: float, beta: float, delta: float, mu: float) -> np.ndarray:\n",
    "    return np.exp(nig_logpdf(x, alpha, beta, delta, mu))\n",
    "\n",
    "\n",
    "def to_scipy_params(alpha: float, beta: float, delta: float, mu: float) -> tuple[float, float, float, float]:\n",
    "    '''Map (alpha,beta,delta,mu) to SciPy's (a,b,loc,scale).'''\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    a = alpha * delta\n",
    "    b = beta * delta\n",
    "    return float(a), float(b), float(mu), float(delta)\n",
    "\n",
    "\n",
    "def from_scipy_params(a: float, b: float, loc: float, scale: float) -> tuple[float, float, float, float]:\n",
    "    '''Map SciPy's (a,b,loc,scale) to (alpha,beta,delta,mu).'''\n",
    "    if not (scale > 0):\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "    alpha = a / scale\n",
    "    beta = b / scale\n",
    "    delta = scale\n",
    "    mu = loc\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    return float(alpha), float(beta), float(delta), float(mu)\n",
    "\n",
    "\n",
    "# Quick sanity check: our PDF matches SciPy's parameter mapping\n",
    "from scipy.stats import norminvgauss\n",
    "\n",
    "alpha, beta, delta, mu = 2.5, 0.8, 1.2, -0.3\n",
    "a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "rv = norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "xg = np.linspace(rv.ppf(0.001), rv.ppf(0.999), 400)\n",
    "max_abs_diff = float(np.max(np.abs(rv.pdf(xg) - nig_pdf(xg, alpha, beta, delta, mu))))\n",
    "max_abs_diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806e287",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "A convenient starting point is the **cumulant generating function** (CGF)\n",
    "\\(K(t)=\\log M_X(t)\\), where \\(M_X(t)=\\mathbb{E}[e^{tX}]\\) is the MGF.\n",
    "\n",
    "### MGF / CGF\n",
    "\n",
    "For \\(t\\) in the interval where \\(|\\beta+t|<\\alpha\\):\n",
    "\n",
    "\\[\n",
    "M_X(t) = \\exp\\left(\\mu t + \\delta\\big(\\gamma - \\sqrt{\\alpha^2-(\\beta+t)^2}\\big)\\right),\n",
    "\\qquad \\gamma = \\sqrt{\\alpha^2-\\beta^2}.\n",
    "\\]\n",
    "\n",
    "Equivalently,\n",
    "\n",
    "\\[\n",
    "K(t) = \\mu t + \\delta\\big(\\gamma - \\sqrt{\\alpha^2-(\\beta+t)^2}\\big).\n",
    "\\]\n",
    "\n",
    "**Domain:** \\(t \\in (-\\alpha-\\beta,\\; \\alpha-\\beta)\\).\n",
    "\n",
    "### Characteristic function\n",
    "\n",
    "For real \\(u\\),\n",
    "\n",
    "\\[\n",
    "\\varphi_X(u) = \\mathbb{E}[e^{iuX}]\n",
    "= \\exp\\left(i\\mu u + \\delta\\big(\\gamma - \\sqrt{\\alpha^2-(\\beta+iu)^2}\\big)\\right).\n",
    "\\]\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "\n",
    "Using derivatives of \\(K(t)\\) at \\(t=0\\):\n",
    "\n",
    "- **Mean**:\n",
    "  \\(\\mathbb{E}[X] = \\mu + \\delta\\,\\beta/\\gamma\\)\n",
    "- **Variance**:\n",
    "  \\(\\mathrm{Var}(X) = \\delta\\,\\alpha^2/\\gamma^3\\)\n",
    "- **Skewness**:\n",
    "  \\(\\text{skew}(X) = \\dfrac{3\\,\\beta}{\\alpha\\,\\sqrt{\\delta\\,\\gamma}}\\)\n",
    "- **(Excess) kurtosis**:\n",
    "  \\(\\text{excess kurt}(X) = \\dfrac{3\\,(1+4\\beta^2/\\alpha^2)}{\\delta\\,\\gamma}\\)\n",
    "  (so kurtosis \\(=3+\\) excess)\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy is\n",
    "\n",
    "\\[\n",
    "H(X) = -\\mathbb{E}[\\log f(X)] = -\\int_{-\\infty}^{\\infty} f(x)\\log f(x)\\,dx.\n",
    "\\]\n",
    "\n",
    "There is no simple closed form in elementary functions; in practice you estimate it numerically (quadrature or Monte Carlo).\n",
    "\n",
    "### Other properties (high level)\n",
    "- **Infinitely divisible**: NIG defines a Lévy process (useful for increments / time series).\n",
    "- **Closure under convolution** (with common \\(\\alpha,\\beta\\)): if \\(X_1\\sim\\text{NIG}(\\alpha,\\beta,\\delta_1,\\mu_1)\\) and \\(X_2\\sim\\text{NIG}(\\alpha,\\beta,\\delta_2,\\mu_2)\\) are independent, then \\(X_1+X_2\\sim\\text{NIG}(\\alpha,\\beta,\\delta_1+\\delta_2,\\mu_1+\\mu_2)\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895527a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nig_mean(alpha: float, beta: float, delta: float, mu: float) -> float:\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "    return float(mu + delta * beta / gamma)\n",
    "\n",
    "\n",
    "def nig_var(alpha: float, beta: float, delta: float) -> float:\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "    return float(delta * alpha * alpha / (gamma**3))\n",
    "\n",
    "\n",
    "def nig_skew(alpha: float, beta: float, delta: float) -> float:\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "    return float(3.0 * beta / (alpha * math.sqrt(delta * gamma)))\n",
    "\n",
    "\n",
    "def nig_excess_kurt(alpha: float, beta: float, delta: float) -> float:\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "    return float(3.0 * (1.0 + 4.0 * (beta * beta) / (alpha * alpha)) / (delta * gamma))\n",
    "\n",
    "\n",
    "def nig_mgf(t: np.ndarray, alpha: float, beta: float, delta: float, mu: float) -> np.ndarray:\n",
    "    '''MGF evaluated on an array t (real); returns nan outside the domain.'''\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "\n",
    "    inside = alpha * alpha - (beta + t) ** 2\n",
    "    out = np.full_like(t, np.nan, dtype=float)\n",
    "    mask = inside > 0\n",
    "    out[mask] = np.exp(mu * t[mask] + delta * (gamma - np.sqrt(inside[mask])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def nig_cf(u: np.ndarray, alpha: float, beta: float, delta: float, mu: float) -> np.ndarray:\n",
    "    '''Characteristic function for real u.'''\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "    inner = np.sqrt(alpha * alpha - (beta + 1j * u) ** 2)\n",
    "    return np.exp(1j * mu * u + delta * (gamma - inner))\n",
    "\n",
    "\n",
    "alpha, beta, delta, mu = 2.5, 0.8, 1.2, -0.3\n",
    "{\n",
    "    \"mean\": nig_mean(alpha, beta, delta, mu),\n",
    "    \"var\": nig_var(alpha, beta, delta),\n",
    "    \"skew\": nig_skew(alpha, beta, delta),\n",
    "    \"excess_kurt\": nig_excess_kurt(alpha, beta, delta),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy (Monte Carlo estimate): H(X) = -E[log f(X)]\n",
    "# We'll reuse the NumPy-only sampler later, but SciPy's sampler works too.\n",
    "\n",
    "from scipy.stats import norminvgauss\n",
    "\n",
    "alpha, beta, delta, mu = 2.0, 0.5, 1.3, -0.2\n",
    "a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "rv = norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "x = rv.rvs(size=50_000, random_state=rng)\n",
    "entropy_mc = float(-np.mean(nig_logpdf(x, alpha, beta, delta, mu)))\n",
    "entropy_mc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192ca1b",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "Think in terms of \\((\\alpha,\\beta,\\delta,\\mu)\\):\n",
    "\n",
    "- **\\(\\mu\\) (location)** shifts the distribution left/right.\n",
    "- **\\(\\delta\\) (scale)** stretches/compresses the distribution.\n",
    "- **\\(\\beta\\) (asymmetry)** controls skewness:\n",
    "  - \\(\\beta>0\\): heavier/right tail (more mass on the right)\n",
    "  - \\(\\beta<0\\): heavier/left tail\n",
    "- **\\(\\alpha\\) (tail/steepness)** controls tail heaviness:\n",
    "  - larger \\(\\alpha\\) \\(\\Rightarrow\\) lighter tails (closer to normal)\n",
    "  - smaller \\(\\alpha\\) \\(\\Rightarrow\\) heavier tails (more extremes)\n",
    "\n",
    "The constraint \\(|\\beta|<\\alpha\\) ensures \\(\\gamma=\\sqrt{\\alpha^2-\\beta^2}\\) is real.\n",
    "\n",
    "**A useful derived quantity:**\n",
    "\n",
    "- \\(\\gamma\\) appears in the mean/variance and in the MGF domain.\n",
    "- As \\(|\\beta|\\to\\alpha\\), \\(\\gamma\\to 0\\) and the distribution becomes numerically unstable and extremely skew/heavy-tailed.\n",
    "\n",
    "**SciPy caution:** SciPy’s `a` and `b` are *scaled* by \\(\\delta\\) (since \\(a=\\alpha\\delta\\), \\(b=\\beta\\delta\\)).\n",
    "So changing `scale` while holding `a,b` fixed changes \\((\\alpha,\\beta)\\) as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f98f74",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "We sketch three core derivations.\n",
    "\n",
    "### A) Expectation\n",
    "\n",
    "Start from the CGF:\n",
    "\n",
    "\\[\n",
    "K(t) = \\mu t + \\delta\\Big(\\gamma - \\sqrt{\\alpha^2-(\\beta+t)^2}\\Big).\n",
    "\\]\n",
    "\n",
    "Differentiate:\n",
    "\n",
    "\\[\n",
    "K'(t) = \\mu + \\delta\\;\\frac{\\beta+t}{\\sqrt{\\alpha^2-(\\beta+t)^2}}.\n",
    "\\]\n",
    "\n",
    "Evaluating at \\(t=0\\) gives the mean:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = K'(0) = \\mu + \\delta\\,\\frac{\\beta}{\\gamma}.\n",
    "\\]\n",
    "\n",
    "### B) Variance\n",
    "\n",
    "Differentiate again:\n",
    "\n",
    "\\[\n",
    "K''(t) = \\delta\\;\\frac{\\alpha^2}{\\big(\\alpha^2-(\\beta+t)^2\\big)^{3/2}}.\n",
    "\\]\n",
    "\n",
    "so\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = K''(0) = \\delta\\;\\frac{\\alpha^2}{\\gamma^3}.\n",
    "\\]\n",
    "\n",
    "### C) Likelihood\n",
    "\n",
    "For i.i.d. data \\(x_1,\\dots,x_n\\), the likelihood is\n",
    "\n",
    "\\[\n",
    "L(\\alpha,\\beta,\\delta,\\mu) = \\prod_{i=1}^n f(x_i\\mid\\alpha,\\beta,\\delta,\\mu),\n",
    "\\]\n",
    "\n",
    "and the log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell(\\alpha,\\beta,\\delta,\\mu)\n",
    "= \\sum_{i=1}^n \\log f(x_i\\mid\\alpha,\\beta,\\delta,\\mu).\n",
    "\\]\n",
    "\n",
    "Because the density involves \\(K_1(\\cdot)\\), the MLE does not have a simple closed form; numerical optimization is typical.\n",
    "In SciPy you can use `norminvgauss.fit` (MLE under the loc/scale convention).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nig_loglik(alpha: float, beta: float, delta: float, mu: float, x: np.ndarray) -> float:\n",
    "    return float(np.sum(nig_logpdf(x, alpha, beta, delta, mu)))\n",
    "\n",
    "\n",
    "alpha, beta, delta, mu = 2.5, 0.8, 1.2, -0.3\n",
    "a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "rv = stats.norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "x = rv.rvs(size=2_000, random_state=rng)\n",
    "ll_true = nig_loglik(alpha, beta, delta, mu, x)\n",
    "\n",
    "# Compare to a slightly misspecified parameter (lower beta)\n",
    "ll_alt = nig_loglik(alpha, beta * 0.7, delta, mu, x)\n",
    "\n",
    "{\"loglik_true\": ll_true, \"loglik_alt\": ll_alt, \"diff\": ll_true - ll_alt}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845aa14",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "The mixture representation gives a simple sampler.\n",
    "\n",
    "### Step 1: sample the inverse Gaussian mixing variable\n",
    "\n",
    "We use the \\((\\nu,\\lambda)\\) parameterization with density\n",
    "\n",
    "\\[\n",
    "f(v;\\nu,\\lambda) = \\sqrt{\\frac{\\lambda}{2\\pi v^3}}\n",
    "\\exp\\left(-\\frac{\\lambda(v-\\nu)^2}{2\\nu^2 v}\\right),\n",
    "\\qquad v>0.\n",
    "\\]\n",
    "\n",
    "For NIG, we need\n",
    "\n",
    "\\[\n",
    "V \\sim \\text{IG}\\left(\\nu=\\frac{\\delta}{\\gamma},\\;\\lambda=\\delta^2\\right).\n",
    "\\]\n",
    "\n",
    "A classic exact sampler is the **Michael–Schucany–Haas** method.\n",
    "\n",
    "### Step 2: sample the conditional normal\n",
    "\n",
    "Given \\(V=v\\):\n",
    "\n",
    "\\[\n",
    "X\\mid V=v \\sim \\mathcal{N}(\\mu+\\beta v,\\; v).\n",
    "\\]\n",
    "\n",
    "So we can generate\n",
    "\n",
    "\\[\n",
    "X = \\mu + \\beta V + \\sqrt{V}\\,Z,\\qquad Z\\sim\\mathcal{N}(0,1).\n",
    "\\]\n",
    "\n",
    "Everything below uses **NumPy only**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_invgauss_msh(size: int, nu: float, lam: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''Sample IG(nu, lam) using the Michael–Schucany–Haas method.\n",
    "\n",
    "    Parameterization: mean = nu, shape = lam.\n",
    "    '''\n",
    "    if not (nu > 0):\n",
    "        raise ValueError(\"nu must be > 0\")\n",
    "    if not (lam > 0):\n",
    "        raise ValueError(\"lam must be > 0\")\n",
    "\n",
    "    v = rng.normal(size=size)\n",
    "    y = v * v\n",
    "\n",
    "    nu2 = nu * nu\n",
    "\n",
    "    x = (\n",
    "        nu\n",
    "        + (nu2 * y) / (2.0 * lam)\n",
    "        - (nu / (2.0 * lam)) * np.sqrt(4.0 * nu * lam * y + nu2 * y * y)\n",
    "    )\n",
    "\n",
    "    u = rng.uniform(size=size)\n",
    "    return np.where(u <= nu / (nu + x), x, nu2 / x)\n",
    "\n",
    "\n",
    "def sample_nig(size: int, alpha: float, beta: float, delta: float, mu: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    nig_validate(alpha, beta, delta)\n",
    "    gamma = math.sqrt(alpha * alpha - beta * beta)\n",
    "    nu = delta / gamma\n",
    "    lam = delta * delta\n",
    "\n",
    "    v = sample_invgauss_msh(size=size, nu=nu, lam=lam, rng=rng)\n",
    "    z = rng.normal(size=size)\n",
    "    return mu + beta * v + np.sqrt(v) * z\n",
    "\n",
    "\n",
    "# Quick simulation check: sample moments vs formulas\n",
    "alpha, beta, delta, mu = 2.0, 0.5, 1.3, -0.2\n",
    "x = sample_nig(size=200_000, alpha=alpha, beta=beta, delta=delta, mu=mu, rng=rng)\n",
    "\n",
    "{\n",
    "    \"sample_mean\": float(x.mean()),\n",
    "    \"theory_mean\": nig_mean(alpha, beta, delta, mu),\n",
    "    \"sample_var\": float(x.var(ddof=0)),\n",
    "    \"theory_var\": nig_var(alpha, beta, delta),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce445a5",
   "metadata": {},
   "source": [
    "## 8) Visualization (PDF, CDF, Monte Carlo)\n",
    "\n",
    "We’ll visualize:\n",
    "- how changing \\(\\alpha\\) and \\(\\beta\\) affects **tail heaviness** and **skewness**\n",
    "- the **PDF** and **CDF** for a chosen parameter set\n",
    "- a Monte Carlo histogram + empirical CDF compared to the theoretical curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5035a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nig_pdfs(param_sets: list[dict], q_low: float = 0.001, q_high: float = 0.999) -> go.Figure:\n",
    "    from scipy.stats import norminvgauss\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for ps in param_sets:\n",
    "        alpha, beta, delta, mu = ps[\"alpha\"], ps[\"beta\"], ps[\"delta\"], ps[\"mu\"]\n",
    "        a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "        rv = norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "        xs = np.linspace(rv.ppf(q_low), rv.ppf(q_high), 600)\n",
    "        ys = nig_pdf(xs, alpha, beta, delta, mu)\n",
    "\n",
    "        label = f\"α={alpha:g}, β={beta:g}, δ={delta:g}, μ={mu:g}\"\n",
    "        fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", name=label))\n",
    "\n",
    "    fig.update_layout(title=\"Normal Inverse Gaussian PDFs\", xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "param_sets = [\n",
    "    {\"alpha\": 3.0, \"beta\": 0.0, \"delta\": 1.0, \"mu\": 0.0},  # symmetric, lighter tails\n",
    "    {\"alpha\": 1.6, \"beta\": 0.0, \"delta\": 1.0, \"mu\": 0.0},  # symmetric, heavier tails\n",
    "    {\"alpha\": 2.0, \"beta\": 0.7, \"delta\": 1.0, \"mu\": 0.0},  # right-skew\n",
    "    {\"alpha\": 2.0, \"beta\": -0.7, \"delta\": 1.0, \"mu\": 0.0},  # left-skew\n",
    "]\n",
    "\n",
    "fig = plot_nig_pdfs(param_sets)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF + CDF for one parameter set (CDF via SciPy)\n",
    "from scipy.stats import norminvgauss\n",
    "\n",
    "alpha, beta, delta, mu = 2.0, 0.7, 1.2, -0.3\n",
    "a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "rv = norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "xs = np.linspace(rv.ppf(0.001), rv.ppf(0.999), 700)\n",
    "\n",
    "fig_pdf = go.Figure()\n",
    "fig_pdf.add_trace(go.Scatter(x=xs, y=rv.pdf(xs), mode=\"lines\", name=\"pdf\"))\n",
    "fig_pdf.update_layout(title=\"NIG PDF\", xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=xs, y=rv.cdf(xs), mode=\"lines\", name=\"cdf\"))\n",
    "fig_cdf.update_layout(title=\"NIG CDF\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "\n",
    "fig_pdf.show()\n",
    "fig_cdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo samples vs PDF\n",
    "alpha, beta, delta, mu = 2.0, 0.7, 1.2, -0.3\n",
    "a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "rv = stats.norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "n = 80_000\n",
    "x = sample_nig(size=n, alpha=alpha, beta=beta, delta=delta, mu=mu, rng=rng)\n",
    "\n",
    "xs = np.linspace(rv.ppf(0.001), rv.ppf(0.999), 600)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x,\n",
    "    nbins=90,\n",
    "    histnorm=\"probability density\",\n",
    "    title=f\"Monte Carlo samples vs PDF (n={n:,})\",\n",
    "    labels={\"value\": \"x\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=xs, y=rv.pdf(xs), mode=\"lines\", name=\"theoretical pdf\"))\n",
    "fig.update_layout(yaxis_title=\"density\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical CDF vs theoretical CDF\n",
    "alpha, beta, delta, mu = 2.0, 0.7, 1.2, -0.3\n",
    "a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "rv = stats.norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "n = 30_000\n",
    "x = sample_nig(size=n, alpha=alpha, beta=beta, delta=delta, mu=mu, rng=rng)\n",
    "\n",
    "xs = np.sort(x)\n",
    "ys = np.arange(1, n + 1) / n\n",
    "\n",
    "xg = np.linspace(rv.ppf(0.001), rv.ppf(0.999), 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", name=\"empirical CDF\"))\n",
    "fig.add_trace(go.Scatter(x=xg, y=rv.cdf(xg), mode=\"lines\", name=\"theoretical CDF\"))\n",
    "fig.update_layout(title=\"Empirical CDF vs theoretical CDF\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2fbfb",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.norminvgauss`)\n",
    "\n",
    "SciPy provides:\n",
    "\n",
    "- `pdf`, `logpdf`\n",
    "- `cdf`, `ppf` (numerical)\n",
    "- `rvs` (sampling)\n",
    "- `fit` (MLE)\n",
    "\n",
    "Remember the mapping:\n",
    "\n",
    "\\[\n",
    "a = \\alpha\\delta,\\; b=\\beta\\delta,\\; \\text{loc}=\\mu,\\; \\text{scale}=\\delta.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b00e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norminvgauss\n",
    "\n",
    "alpha, beta, delta, mu = 2.2, 0.6, 1.4, -0.1\n",
    "a, b, loc, scale = to_scipy_params(alpha, beta, delta, mu)\n",
    "\n",
    "rv = norminvgauss(a, b, loc=loc, scale=scale)\n",
    "\n",
    "# Basic API\n",
    "x0 = np.array([-2.0, 0.0, 1.0])\n",
    "out = {\n",
    "    \"pdf(x0)\": rv.pdf(x0),\n",
    "    \"cdf(x0)\": rv.cdf(x0),\n",
    "    \"mean\": rv.mean(),\n",
    "    \"var\": rv.var(),\n",
    "}\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2993e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit example: recover parameters from synthetic data\n",
    "alpha, beta, delta, mu = 2.0, 0.5, 1.3, -0.2\n",
    "a_true, b_true, loc_true, scale_true = to_scipy_params(alpha, beta, delta, mu)\n",
    "\n",
    "rv_true = norminvgauss(a_true, b_true, loc=loc_true, scale=scale_true)\n",
    "data = rv_true.rvs(size=3_000, random_state=rng)\n",
    "\n",
    "a_hat, b_hat, loc_hat, scale_hat = norminvgauss.fit(data)\n",
    "\n",
    "{\n",
    "    \"true\": (a_true, b_true, loc_true, scale_true),\n",
    "    \"hat\": (a_hat, b_hat, loc_hat, scale_hat),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d1a57",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing / goodness-of-fit\n",
    "\n",
    "In practice you often ask: *Do my residuals / increments look NIG?*\n",
    "\n",
    "A common approach is a **goodness-of-fit test** (e.g. Kolmogorov–Smirnov).\n",
    "\n",
    "**Important caution:** if you estimate parameters from the data and then run a standard KS test,\n",
    "the p-value is no longer valid (because the null distribution changed).\n",
    "A simple fix is a **parametric bootstrap**:\n",
    "\n",
    "1) fit parameters \\(\\hat\\theta\\)\n",
    "2) compute the test statistic on the observed data\n",
    "3) simulate many datasets from the fitted model\n",
    "4) refit + recompute the statistic for each simulated dataset\n",
    "5) estimate the p-value by the bootstrap tail probability\n",
    "\n",
    "### B) Bayesian modeling\n",
    "\n",
    "NIG is useful as a **likelihood** (or error model) when residuals are heavy-tailed and skewed.\n",
    "\n",
    "The mixture representation introduces latent variables \\(V_i\\) such that\n",
    "\n",
    "\\[\n",
    "X_i \\mid V_i \\sim \\mathcal{N}(\\mu+\\beta V_i,\\;V_i),\n",
    "\\quad V_i \\sim \\text{IG}(\\delta/\\gamma,\\;\\delta^2),\n",
    "\\]\n",
    "\n",
    "which can make Bayesian inference more tractable because the conditional model is Gaussian.\n",
    "\n",
    "### C) Generative modeling\n",
    "\n",
    "Because NIG is infinitely divisible, it is natural for **increments**.\n",
    "A simple generative model is a random walk with NIG innovations:\n",
    "\n",
    "\\[\n",
    "S_t = S_{t-1} + \\varepsilon_t,\\qquad \\varepsilon_t \\sim \\text{NIG}(\\alpha,\\beta,\\delta,\\mu).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Parametric bootstrap KS test demo (small B for speed)\n",
    "# We'll generate data from a known NIG, fit it, and test goodness-of-fit.\n",
    "\n",
    "from scipy.stats import kstest, norminvgauss\n",
    "\n",
    "rng_local = np.random.default_rng(123)\n",
    "\n",
    "alpha, beta, delta, mu = 2.0, 0.5, 1.3, -0.2\n",
    "a_true, b_true, loc_true, scale_true = to_scipy_params(alpha, beta, delta, mu)\n",
    "rv_true = norminvgauss(a_true, b_true, loc=loc_true, scale=scale_true)\n",
    "\n",
    "n = 800\n",
    "x = rv_true.rvs(size=n, random_state=rng_local)\n",
    "\n",
    "# Fit the model\n",
    "a_hat, b_hat, loc_hat, scale_hat = norminvgauss.fit(x)\n",
    "rv_hat = norminvgauss(a_hat, b_hat, loc=loc_hat, scale=scale_hat)\n",
    "\n",
    "# KS statistic on observed data against fitted CDF\n",
    "ks_obs = kstest(x, rv_hat.cdf).statistic\n",
    "\n",
    "# Parametric bootstrap: simulate from fitted, refit, recompute KS\n",
    "B = 80\n",
    "ks_boot = np.empty(B)\n",
    "for i in range(B):\n",
    "    xb = rv_hat.rvs(size=n, random_state=rng_local)\n",
    "    a_b, b_b, loc_b, scale_b = norminvgauss.fit(xb)\n",
    "    rv_b = norminvgauss(a_b, b_b, loc=loc_b, scale=scale_b)\n",
    "    ks_boot[i] = kstest(xb, rv_b.cdf).statistic\n",
    "\n",
    "p_boot = float(np.mean(ks_boot >= ks_obs))\n",
    "\n",
    "{\"ks_obs\": float(ks_obs), \"p_boot\": p_boot, \"B\": B}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Generative modeling: random walk with NIG vs normal innovations (matched mean/var)\n",
    "\n",
    "T = 300\n",
    "alpha, beta, delta, mu = 2.0, 0.5, 1.0, 0.0\n",
    "\n",
    "eps_nig = sample_nig(size=T, alpha=alpha, beta=beta, delta=delta, mu=mu, rng=rng)\n",
    "\n",
    "m = nig_mean(alpha, beta, delta, mu)\n",
    "v = nig_var(alpha, beta, delta)\n",
    "eps_norm = rng.normal(loc=m, scale=math.sqrt(v), size=T)\n",
    "\n",
    "s0 = 0.0\n",
    "s_nig = s0 + np.cumsum(eps_nig)\n",
    "s_norm = s0 + np.cumsum(eps_norm)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=s_nig, mode=\"lines\", name=\"NIG random walk\"))\n",
    "fig.add_trace(go.Scatter(y=s_norm, mode=\"lines\", name=\"Normal random walk (matched mean/var)\"))\n",
    "fig.update_layout(title=\"Random walk paths\", xaxis_title=\"t\", yaxis_title=\"S_t\")\n",
    "fig.show()\n",
    "\n",
    "# Compare tail behavior of innovations\n",
    "qs = [0.001, 0.01, 0.5, 0.99, 0.999]\n",
    "{\n",
    "    \"quantiles\": qs,\n",
    "    \"nig\": np.quantile(eps_nig, qs),\n",
    "    \"normal\": np.quantile(eps_norm, qs),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09451358",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter constraints**: you must have \\(\\alpha>0\\), \\(\\delta>0\\), and \\(|\\beta|<\\alpha\\).\n",
    "  Near the boundary \\(|\\beta|\\approx\\alpha\\), numerical issues are common.\n",
    "- **Confusing names**: `norminvgauss` (Normal Inverse Gaussian) is different from *normal-inverse-gamma*.\n",
    "- **Numerical stability**:\n",
    "  - the PDF involves \\(K_1(z)\\), which can underflow for large \\(z\\)\n",
    "  - prefer `logpdf` for inference and use scaled Bessel functions (e.g. `scipy.special.kve`) when implementing formulas\n",
    "- **Fitting can be unstable**:\n",
    "  - small samples may not pin down tail parameters well\n",
    "  - MLE may be sensitive to initialization / local optima\n",
    "  - always validate fit quality with diagnostic plots (QQ plots, tail behavior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3c476",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `norminvgauss` is a **continuous** distribution on \\(\\mathbb{R}\\) that models **skewed, heavy-tailed** data.\n",
    "- Its key intuition is a **normal mean–variance mixture** with an **inverse Gaussian** mixing variable.\n",
    "- The PDF involves a modified Bessel function \\(K_1\\); the CDF is typically computed numerically.\n",
    "- The CGF/MGF gives compact formulas for mean/variance/skewness/kurtosis.\n",
    "- Sampling is straightforward via the mixture representation and can be done with **NumPy only**.\n",
    "- SciPy’s `scipy.stats.norminvgauss` provides `pdf`, `cdf`, `rvs`, and `fit` with a clear mapping to \\((\\alpha,\\beta,\\delta,\\mu)\\).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}