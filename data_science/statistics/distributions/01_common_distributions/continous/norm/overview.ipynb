{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a12ca5",
   "metadata": {},
   "source": [
    "# Normal Distribution (`norm`)\n",
    "\n",
    "The normal (Gaussian) distribution is the canonical model for **additive noise** and **aggregated effects**.\n",
    "\n",
    "It appears throughout statistics and machine learning via the **Central Limit Theorem**, as the distribution of **measurement errors**, and as the maximum-entropy distribution under mean/variance constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d3347",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & classification\n",
    "2) Intuition & motivation\n",
    "3) Formal definition (PDF/CDF)\n",
    "4) Moments & properties\n",
    "5) Parameter interpretation\n",
    "6) Derivations ($\\mathbb{E}[X]$, $\\mathrm{Var}(X)$, likelihood)\n",
    "7) Sampling & simulation (NumPy-only)\n",
    "8) Visualization (PDF, CDF, Monte Carlo)\n",
    "9) SciPy integration (`scipy.stats.norm`)\n",
    "10) Statistical use cases\n",
    "11) Pitfalls\n",
    "12) Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special, stats\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328abff9",
   "metadata": {},
   "source": [
    "## Prerequisites & notation\n",
    "\n",
    "**Prerequisites**\n",
    "- comfort with basic calculus (integration by parts)\n",
    "- basic probability (PDF/CDF, expectation, likelihood)\n",
    "\n",
    "**Notation**\n",
    "- $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ means: mean $\\mu\\in\\mathbb{R}$, standard deviation $\\sigma>0$.\n",
    "- $Z \\sim \\mathcal{N}(0,1)$ denotes the **standard normal**.\n",
    "- $\\varphi$ and $\\Phi$ denote the standard normal **PDF** and **CDF**.\n",
    "\n",
    "SciPy uses a **location–scale** parameterization: `stats.norm(loc=μ, scale=σ)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d54f02",
   "metadata": {},
   "source": [
    "## 1) Title & classification\n",
    "\n",
    "- **Name**: `norm` (Normal / Gaussian distribution)\n",
    "- **Type**: **continuous**\n",
    "- **Support**: $x \\in (-\\infty, \\infty)$\n",
    "- **Parameter space**:\n",
    "  - location (mean): $\\mu \\in \\mathbb{R}$\n",
    "  - scale (std dev): $\\sigma \\in (0, \\infty)$\n",
    "\n",
    "Equivalent parameterizations you’ll also see:\n",
    "- variance $\\sigma^2 > 0$\n",
    "- precision $\\tau = 1/\\sigma^2 > 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb97da2",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "The normal distribution often models **the sum of many small, independent effects**.\n",
    "A classic mental model is **measurement error**:\n",
    "\n",
    "$\\text{observed} = \\text{true signal} + \\text{noise}$, where the noise is approximately Gaussian.\n",
    "\n",
    "Two key reasons it shows up so often:\n",
    "1) **Central Limit Theorem (CLT):** standardized sums of many weakly dependent variables tend toward a normal distribution.\n",
    "2) **Maximum entropy:** among all continuous distributions with a fixed mean and variance, the normal has the largest differential entropy (it is the “least informative” choice under those constraints).\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Sensors & experiments:** additive noise in physical measurements\n",
    "- **Averages/aggregates:** sampling distributions of means (often approximately normal)\n",
    "- **Error models:** regression residuals, Kalman filters, Gaussian processes\n",
    "- **Latent-variable models:** Gaussian priors and Gaussian likelihoods (conjugacy)\n",
    "\n",
    "### Relations to other distributions\n",
    "- Standardization: if $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$, then $(X-\\mu)/\\sigma \\sim \\mathcal{N}(0,1)$.\n",
    "- Chi-square: if $Z \\sim \\mathcal{N}(0,1)$, then $Z^2 \\sim \\chi^2_1$.\n",
    "- Additivity: sums of independent normals are normal (means/variances add).\n",
    "- Student-$t$: arises from a normal divided by a chi-square term.\n",
    "- Lognormal: if $Y \\sim \\mathcal{N}(\\mu,\\sigma^2)$, then $\\exp(Y)$ is lognormal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44ee5f",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "Let $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with $\\mu\\in\\mathbb{R}$ and $\\sigma>0$.\n",
    "\n",
    "### PDF\n",
    "\\[\n",
    "f(x\\mid\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\\qquad x\\in\\mathbb{R}.\n",
    "\\]\n",
    "\n",
    "For the standard normal $Z\\sim\\mathcal{N}(0,1)$, the PDF is\n",
    "\\[\n",
    "\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}}\\,e^{-z^2/2}.\n",
    "\\]\n",
    "\n",
    "### CDF\n",
    "The CDF is\n",
    "\\[\n",
    "F(x\\mid\\mu,\\sigma) = \\mathbb{P}(X\\le x) = \\Phi\\!\\left(\\frac{x-\\mu}{\\sigma}\\right),\n",
    "\\]\n",
    "where $\\Phi$ is the standard normal CDF.\n",
    "\n",
    "There is no elementary closed form, but it can be written using the error function:\n",
    "\\[\n",
    "\\Phi(z) = \\tfrac{1}{2}\\left(1 + \\operatorname{erf}\\!\\left(\\tfrac{z}{\\sqrt{2}}\\right)\\right).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7aaa10",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "For $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$:\n",
    "\n",
    "### Moments\n",
    "- **Mean**: $\\mathbb{E}[X] = \\mu$\n",
    "- **Variance**: $\\mathrm{Var}(X) = \\sigma^2$\n",
    "- **Skewness**: $0$ (symmetry)\n",
    "- **Kurtosis**: $3$ (excess kurtosis $0$)\n",
    "- **Median / mode**: $\\mu$\n",
    "\n",
    "### MGF and characteristic function\n",
    "- **MGF** (all real $t$):\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = \\exp\\!\\left(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2\\right).\n",
    "\\]\n",
    "\n",
    "- **Characteristic function**:\n",
    "\\[\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\exp\\!\\left(i\\mu t - \\tfrac{1}{2}\\sigma^2 t^2\\right).\n",
    "\\]\n",
    "\n",
    "### Entropy (differential, in nats)\n",
    "\\[\n",
    "H(X) = \\tfrac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^2\\right).\n",
    "\\]\n",
    "\n",
    "### Other notable properties\n",
    "- **Affine invariance**: if $Y=aX+b$, then $Y$ is normal with mean $a\\mu+b$ and variance $a^2\\sigma^2$.\n",
    "- **Additivity**: sums of independent normals are normal (and covariances add in the multivariate case).\n",
    "- **Maximum entropy** under fixed mean/variance constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aaa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQRT_2PI = math.sqrt(2.0 * math.pi)\n",
    "\n",
    "\n",
    "def norm_pdf(x: np.ndarray, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "    z = (x - loc) / scale\n",
    "    return np.exp(-0.5 * z**2) / (scale * SQRT_2PI)\n",
    "\n",
    "\n",
    "def norm_cdf(x: np.ndarray, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "    z = (x - loc) / scale\n",
    "    return special.ndtr(z)\n",
    "\n",
    "\n",
    "def norm_logpdf(x: np.ndarray, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "    z = (x - loc) / scale\n",
    "    return -0.5 * z**2 - math.log(scale) - 0.5 * math.log(2.0 * math.pi)\n",
    "\n",
    "\n",
    "def norm_loglik(loc: float, scale: float, x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0 or np.any(~np.isfinite(x)):\n",
    "        return -np.inf\n",
    "    return float(np.sum(norm_logpdf(x, loc=loc, scale=scale)))\n",
    "\n",
    "\n",
    "def norm_mle(x: np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"MLE for (μ, σ) under iid N(μ, σ²).\n",
    "\n",
    "    Note: the MLE for σ uses ddof=0 (biased as an estimator of σ).\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu_hat = float(np.mean(x))\n",
    "    sigma_hat = float(np.sqrt(np.mean((x - mu_hat) ** 2)))\n",
    "    return mu_hat, sigma_hat\n",
    "\n",
    "\n",
    "def sample_norm_box_muller(\n",
    "    n: int,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"NumPy-only sampling via the Box–Muller transform.\n",
    "\n",
    "    Returns n iid samples from N(loc, scale^2).\n",
    "    \"\"\"\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    if n < 0:\n",
    "        raise ValueError(\"n must be >= 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    m = (n + 1) // 2  # number of (Z0, Z1) pairs\n",
    "    u1 = rng.random(m)\n",
    "    u2 = rng.random(m)\n",
    "\n",
    "    # Avoid log(0) when u1 is exactly 0.\n",
    "    u1 = np.maximum(u1, np.nextafter(0.0, 1.0))\n",
    "\n",
    "    r = np.sqrt(-2.0 * np.log(u1))\n",
    "    theta = 2.0 * math.pi * u2\n",
    "\n",
    "    z0 = r * np.cos(theta)\n",
    "    z1 = r * np.sin(theta)\n",
    "\n",
    "    z = np.empty(2 * m, dtype=float)\n",
    "    z[0::2] = z0\n",
    "    z[1::2] = z1\n",
    "    z = z[:n]\n",
    "\n",
    "    return loc + scale * z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76c61e",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "### Location $\\mu$\n",
    "- Shifts the distribution left/right.\n",
    "- $\\mu$ is the center of symmetry, and it equals the mean/median/mode.\n",
    "\n",
    "### Scale $\\sigma$\n",
    "- Controls dispersion: larger $\\sigma$ spreads mass out and lowers the peak.\n",
    "- About 68% / 95% / 99.7% of mass lies within $\\mu \\pm 1\\sigma$, $\\mu \\pm 2\\sigma$, $\\mu \\pm 3\\sigma$ (the “68–95–99.7 rule”).\n",
    "\n",
    "### Shape changes\n",
    "All normal PDFs are bell-shaped and symmetric; changing $\\mu$ shifts the bell, changing $\\sigma$ changes its width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b75652",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8, 8, 800)\n",
    "\n",
    "params = [\n",
    "    (0.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (1.5, 1.0),\n",
    "    (-2.0, 0.6),\n",
    "]\n",
    "\n",
    "fig = go.Figure()\n",
    "for mu, sigma in params:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=norm_pdf(x, loc=mu, scale=sigma),\n",
    "            mode=\"lines\",\n",
    "            name=f\"μ={mu:g}, σ={sigma:g}\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_vline(x=mu, line_dash=\"dot\", opacity=0.25)\n",
    "\n",
    "fig.update_layout(title=\"Normal PDFs for different (μ, σ)\", xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1cd3f",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "We derive $\\mathbb{E}[X]$, $\\mathrm{Var}(X)$, and the likelihood/MLE.\n",
    "\n",
    "### Expectation\n",
    "For the standard normal $Z\\sim\\mathcal{N}(0,1)$ with PDF $\\varphi(z)$,\n",
    "\\[\n",
    "\\mathbb{E}[Z] = \\int_{-\\infty}^{\\infty} z\\,\\varphi(z)\\,dz.\n",
    "\\]\n",
    "The integrand $z\\,\\varphi(z)$ is an **odd function** (since $\\varphi$ is even), so the integral over a symmetric domain is $0$.\n",
    "\n",
    "For $X = \\mu + \\sigma Z$:\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\mu + \\sigma\\,\\mathbb{E}[Z] = \\mu.\n",
    "\\]\n",
    "\n",
    "### Variance\n",
    "First compute $\\mathbb{E}[Z^2]$:\n",
    "\\[\n",
    "\\mathbb{E}[Z^2] = \\int_{-\\infty}^{\\infty} z^2\\,\\varphi(z)\\,dz.\n",
    "\\]\n",
    "Use the fact that $\\varphi'(z) = -z\\,\\varphi(z)$, so $z\\,\\varphi(z) = -\\varphi'(z)$. Then\n",
    "\\[\n",
    "\\mathbb{E}[Z^2] = \\int z^2\\varphi(z)\\,dz = -\\int z\\,\\varphi'(z)\\,dz.\n",
    "\\]\n",
    "Integrate by parts with $u=z$ and $dv=\\varphi'(z)\\,dz$:\n",
    "\\[\n",
    "-\\int z\\,\\varphi'(z)\\,dz = -\\big[z\\,\\varphi(z)\\big]_{-\\infty}^{\\infty} + \\int \\varphi(z)\\,dz.\n",
    "\\]\n",
    "The boundary term is $0$ because $z\\,\\varphi(z)\\to 0$ as $|z|\\to\\infty$, and $\\int \\varphi(z)\\,dz = 1$. Hence $\\mathbb{E}[Z^2]=1$, so $\\mathrm{Var}(Z)=1$.\n",
    "\n",
    "For $X=\\mu+\\sigma Z$:\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\sigma^2\\,\\mathrm{Var}(Z) = \\sigma^2.\n",
    "\\]\n",
    "\n",
    "### Likelihood and MLE\n",
    "For iid data $x_1,\\dots,x_n$ from $\\mathcal{N}(\\mu,\\sigma^2)$, the likelihood is\n",
    "\\[\n",
    "L(\\mu,\\sigma) = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right).\n",
    "\\]\n",
    "The log-likelihood is\n",
    "\\[\n",
    "\\ell(\\mu,\\sigma) = -n\\ln\\sigma - \\tfrac{n}{2}\\ln(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2.\n",
    "\\]\n",
    "Setting derivatives to zero gives the MLEs:\n",
    "\\[\n",
    "\\hat\\mu = \\bar x,\\qquad \\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar x)^2.\n",
    "\\]\n",
    "(The familiar unbiased sample variance uses $n-1$ instead of $n$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75461b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE demo on simulated data\n",
    "true_mu = 1.5\n",
    "true_sigma = 0.8\n",
    "n = 600\n",
    "\n",
    "x = sample_norm_box_muller(n, loc=true_mu, scale=true_sigma, rng=rng)\n",
    "\n",
    "mu_hat, sigma_hat = norm_mle(x)\n",
    "\n",
    "loglik_true = norm_loglik(true_mu, true_sigma, x)\n",
    "loglik_hat = norm_loglik(mu_hat, sigma_hat, x)\n",
    "\n",
    "true_mu, true_sigma, mu_hat, sigma_hat, loglik_true, loglik_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf7486",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation (NumPy-only)\n",
    "\n",
    "### Box–Muller transform\n",
    "Let $U_1, U_2 \\sim \\mathrm{Uniform}(0,1)$ iid. Define\n",
    "\\[\n",
    "R = \\sqrt{-2\\ln U_1},\\qquad \\Theta = 2\\pi U_2.\n",
    "\\]\n",
    "Then\n",
    "\\[\n",
    "Z_0 = R\\cos\\Theta,\\qquad Z_1 = R\\sin\\Theta\n",
    "\\]\n",
    "are iid $\\mathcal{N}(0,1)$. Finally, to sample $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$, return $X = \\mu + \\sigma Z$.\n",
    "\n",
    "**Numerical note:** if $U_1=0$, then $\\ln U_1$ is undefined, so we clip $U_1$ away from 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling: compare histogram to the true PDF\n",
    "mu = 0.7\n",
    "sigma = 1.3\n",
    "n = 60_000\n",
    "\n",
    "samples = sample_norm_box_muller(n, loc=mu, scale=sigma, rng=rng)\n",
    "\n",
    "x_grid = np.linspace(mu - 4.5 * sigma, mu + 4.5 * sigma, 500)\n",
    "\n",
    "fig = px.histogram(\n",
    "    samples,\n",
    "    nbins=70,\n",
    "    histnorm=\"probability density\",\n",
    "    title=f\"Monte Carlo samples vs PDF (n={n}, μ={mu:g}, σ={sigma:g})\",\n",
    "    labels={\"value\": \"x\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=norm_pdf(x_grid, mu, sigma), mode=\"lines\", name=\"true pdf\"))\n",
    "fig.update_layout(yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "samples.mean(), samples.std(ddof=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae9bd9",
   "metadata": {},
   "source": [
    "## 8) Visualization (PDF, CDF, Monte Carlo)\n",
    "\n",
    "We’ll visualize:\n",
    "- the PDF for multiple $\\sigma$ values\n",
    "- the CDF and an empirical CDF from Monte Carlo samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF and CDF for multiple scales\n",
    "mu = 0.0\n",
    "sigmas = [0.5, 1.0, 2.0]\n",
    "x = np.linspace(-8, 8, 800)\n",
    "\n",
    "fig_pdf = go.Figure()\n",
    "fig_cdf = go.Figure()\n",
    "\n",
    "for s in sigmas:\n",
    "    fig_pdf.add_trace(go.Scatter(x=x, y=norm_pdf(x, mu, s), mode=\"lines\", name=f\"σ={s:g}\"))\n",
    "    fig_cdf.add_trace(go.Scatter(x=x, y=norm_cdf(x, mu, s), mode=\"lines\", name=f\"σ={s:g}\"))\n",
    "\n",
    "fig_pdf.update_layout(title=\"Normal PDF (μ=0)\", xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "fig_cdf.update_layout(title=\"Normal CDF (μ=0)\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "\n",
    "fig_pdf.show()\n",
    "fig_cdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffaf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical CDF vs true CDF\n",
    "mu = -0.5\n",
    "sigma = 1.2\n",
    "n = 25_000\n",
    "samples = sample_norm_box_muller(n, loc=mu, scale=sigma, rng=rng)\n",
    "\n",
    "xs = np.sort(samples)\n",
    "ys = np.arange(1, n + 1) / n\n",
    "\n",
    "x_grid = np.linspace(mu - 4.5 * sigma, mu + 4.5 * sigma, 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", name=\"empirical CDF\"))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=norm_cdf(x_grid, mu, sigma), mode=\"lines\", name=\"true CDF\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Empirical CDF vs true CDF (n={n}, μ={mu:g}, σ={sigma:g})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x)\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac777f",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.norm`)\n",
    "\n",
    "SciPy’s `norm` is parameterized as `stats.norm(loc=μ, scale=σ)`.\n",
    "\n",
    "Useful methods include:\n",
    "- `pdf`, `logpdf`\n",
    "- `cdf`, `sf` (survival function), and the numerically stable `logcdf`, `logsf`\n",
    "- `ppf` (quantiles)\n",
    "- `rvs` (sampling)\n",
    "- `fit` (MLE fitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06381d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 1.3\n",
    "dist = stats.norm(loc=mu, scale=sigma)\n",
    "\n",
    "x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 7)\n",
    "pdf_vals = dist.pdf(x)\n",
    "cdf_vals = dist.cdf(x)\n",
    "\n",
    "# Sampling\n",
    "samples = dist.rvs(size=5, random_state=rng)\n",
    "\n",
    "# Fit (MLE)\n",
    "big_sample = dist.rvs(size=5_000, random_state=rng)\n",
    "mu_fit, sigma_fit = stats.norm.fit(big_sample)\n",
    "\n",
    "x, pdf_vals, cdf_vals, samples, (mu_fit, sigma_fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589efc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail-stability: logcdf/logsf vs log(cdf/sf)\n",
    "z = -40.0\n",
    "cdf_direct = stats.norm.cdf(z)\n",
    "logcdf_stable = stats.norm.logcdf(z)\n",
    "\n",
    "z2 = 40.0\n",
    "sf_direct = stats.norm.sf(z2)\n",
    "logsf_stable = stats.norm.logsf(z2)\n",
    "\n",
    "(cdf_direct, logcdf_stable), (sf_direct, logsf_stable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d615",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### Hypothesis testing (z-test for a mean, $\\sigma$ known)\n",
    "If $X_1,\\dots,X_n \\sim \\mathcal{N}(\\mu,\\sigma^2)$ with known $\\sigma$, then under $H_0: \\mu=\\mu_0$,\n",
    "\\[\n",
    "Z = \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1).\n",
    "\\]\n",
    "A two-sided p-value is $p = 2\\,\\mathbb{P}(|Z|\\ge |z_{obs}|)$.\n",
    "\n",
    "### Bayesian modeling (Normal–Normal conjugacy for a mean, $\\sigma$ known)\n",
    "Prior: $\\mu \\sim \\mathcal{N}(\\mu_0,\\tau_0^2)$. Likelihood: $X_i\\mid\\mu \\sim \\mathcal{N}(\\mu,\\sigma^2)$ with known $\\sigma$.\n",
    "\n",
    "Posterior: $\\mu\\mid x \\sim \\mathcal{N}(\\mu_n,\\tau_n^2)$ where\n",
    "\\[\n",
    "\\tau_n^2 = \\left(\\tfrac{1}{\\tau_0^2} + \\tfrac{n}{\\sigma^2}\\right)^{-1},\\qquad\n",
    "\\mu_n = \\tau_n^2\\left(\\tfrac{\\mu_0}{\\tau_0^2} + \\tfrac{n\\bar x}{\\sigma^2}\\right).\n",
    "\\]\n",
    "\n",
    "### Generative modeling\n",
    "Normals are building blocks for generative models:\n",
    "- **Linear Gaussian models** (e.g., Kalman filters): Gaussian latent states + Gaussian noise\n",
    "- **Gaussian mixtures** (GMMs): weighted sums of normals for multi-modal densities\n",
    "- **Multivariate normal**: correlated features via linear transforms of independent normals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fcd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis test example: two-sided z-test for a mean (σ known)\n",
    "mu0 = 0.0\n",
    "sigma_known = 2.0\n",
    "n = 40\n",
    "\n",
    "# Simulated measurements with true mean != mu0\n",
    "true_mu = 0.9\n",
    "data = sample_norm_box_muller(n, loc=true_mu, scale=sigma_known, rng=rng)\n",
    "\n",
    "xbar = data.mean()\n",
    "z_obs = (xbar - mu0) / (sigma_known / math.sqrt(n))\n",
    "p_two_sided = 2.0 * stats.norm.sf(abs(z_obs))\n",
    "\n",
    "alpha = 0.05\n",
    "z_crit = stats.norm.ppf(1 - alpha / 2)\n",
    "ci = (\n",
    "    xbar - z_crit * sigma_known / math.sqrt(n),\n",
    "    xbar + z_crit * sigma_known / math.sqrt(n),\n",
    ")\n",
    "\n",
    "xbar, z_obs, p_two_sided, ci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b790277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian update for μ with known σ (Normal–Normal)\n",
    "mu0 = 0.0\n",
    "tau0 = 1.5  # prior std dev\n",
    "sigma = sigma_known\n",
    "\n",
    "xbar = data.mean()\n",
    "tau_n2 = 1.0 / (1.0 / tau0**2 + n / sigma**2)\n",
    "mu_n = tau_n2 * (mu0 / tau0**2 + n * xbar / sigma**2)\n",
    "tau_n = math.sqrt(tau_n2)\n",
    "\n",
    "mu_n, tau_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651aaca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prior vs posterior over μ\n",
    "mu_grid = np.linspace(mu_n - 5 * tau0, mu_n + 5 * tau0, 600)\n",
    "\n",
    "prior = stats.norm(loc=mu0, scale=tau0)\n",
    "post = stats.norm(loc=mu_n, scale=tau_n)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_grid, y=prior.pdf(mu_grid), mode=\"lines\", name=\"prior\"))\n",
    "fig.add_trace(go.Scatter(x=mu_grid, y=post.pdf(mu_grid), mode=\"lines\", name=\"posterior\"))\n",
    "fig.update_layout(title=\"Bayesian update for μ (σ known)\", xaxis_title=\"μ\", yaxis_title=\"density\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d32070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling example: 2D correlated Gaussian via a linear transform\n",
    "n = 3_000\n",
    "mu_vec = np.array([1.0, -1.0])\n",
    "Sigma = np.array([[1.0, 0.8], [0.8, 2.0]])\n",
    "L = np.linalg.cholesky(Sigma)\n",
    "\n",
    "z = sample_norm_box_muller(2 * n, loc=0.0, scale=1.0, rng=rng).reshape(n, 2)\n",
    "x = mu_vec + z @ L.T\n",
    "\n",
    "df = {\"x1\": x[:, 0], \"x2\": x[:, 1]}\n",
    "fig = px.scatter(df, x=\"x1\", y=\"x2\", opacity=0.35, title=\"Samples from a correlated 2D Gaussian\")\n",
    "fig.update_layout(xaxis_title=\"x1\", yaxis_title=\"x2\")\n",
    "fig.show()\n",
    "\n",
    "x.mean(axis=0), np.cov(x.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e7d99",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: $\\sigma\\le 0$ is not allowed. In code, guard against non-positive `scale`.\n",
    "- **Overconfidence in normality**: real data may be skewed, heavy-tailed, or multi-modal. Diagnose with histograms/QQ-plots; consider alternatives (e.g., Student-$t$, mixtures, robust losses).\n",
    "- **Outliers**: Gaussian likelihoods heavily penalize large residuals, so a few outliers can dominate fits.\n",
    "- **Numerical issues in the tails**: `cdf`/`sf` may underflow to 0; prefer `logcdf`/`logsf` or work in log-space.\n",
    "- **Sampling edge cases**: Box–Muller requires $U_1>0$; clip `u1` away from 0 to avoid `log(0)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd86605",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `norm` is a **continuous** distribution on $( -\\infty,\\infty )$ with parameters $\\mu\\in\\mathbb{R}$, $\\sigma>0$.\n",
    "- PDF: bell-shaped and symmetric; $\\mu$ shifts, $\\sigma$ spreads.\n",
    "- Key formulas: $\\mathbb{E}[X]=\\mu$, $\\mathrm{Var}(X)=\\sigma^2$, $M_X(t)=\\exp(\\mu t + \\tfrac12\\sigma^2 t^2)$, $H=\\tfrac12\\ln(2\\pi e\\sigma^2)$.\n",
    "- MLE: $\\hat\\mu=\\bar x$, $\\hat\\sigma^2 = \\tfrac1n\\sum(x_i-\\bar x)^2$.\n",
    "- For tails, prefer `stats.norm.logcdf/logsf` over taking `log` of `cdf/sf`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}