{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0b8d67",
   "metadata": {},
   "source": [
    "# Arcsine Distribution\n",
    "\n",
    "The **arcsine** distribution is a continuous distribution on a finite interval whose density **spikes near the endpoints**.\n",
    "\n",
    "It shows up in two classic ways:\n",
    "\n",
    "1. As a special Beta distribution:  \\(\\text{Beta}(\\tfrac12,\\tfrac12)\\) on \\([0,1]\\).\n",
    "2. As the limit law in the **arcsine laws** for symmetric random walks / Brownian motion (e.g., the fraction of time spent above 0).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end, you should be able to:\n",
    "\n",
    "- write down and interpret the pdf/cdf of the arcsine distribution\n",
    "- derive its mean and variance using a clean trig substitution\n",
    "- sample it efficiently with **NumPy only** (inverse CDF)\n",
    "- use `scipy.stats.arcsine` for `pdf`, `cdf`, `rvs`, and `fit`\n",
    "- recognize where it appears in statistics (Jeffreys prior) and stochastic processes (arcsine laws)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Calculus: change of variables, basic integrals\n",
    "- Probability: pdf/cdf, expectation and variance\n",
    "- Familiarity with the Beta distribution is helpful but not required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import special, stats\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da126e",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Distribution name:** arcsine\n",
    "\n",
    "**Type:** continuous\n",
    "\n",
    "**Support:**\n",
    "\n",
    "- Canonical form: \\(x \\in [0,1]\\)\n",
    "- General (endpoint) form: \\(x \\in [a,b]\\) with \\(a<b\\)\n",
    "\n",
    "**Parameter space:**\n",
    "\n",
    "- Endpoint form: \\((a,b) \\in \\mathbb{R}^2\\) with \\(a<b\\)\n",
    "- SciPy form (location/scale): `loc = a`, `scale = b-a` with `scale > 0`\n",
    "\n",
    "**Key identity:**\n",
    "\n",
    "\\[\n",
    "X \\sim \\text{arcsine on }[0,1] \\quad\\Longleftrightarrow\\quad X \\sim \\text{Beta}\\left(\\tfrac12,\\tfrac12\\right).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0b674",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "The arcsine distribution is a natural model for **fractions** and **time proportions** that tend to be **near 0 or near 1** more often than “in the middle”.\n",
    "\n",
    "Its pdf is **U-shaped** on \\([0,1]\\):\n",
    "\n",
    "- very high density near 0 and 1\n",
    "- lowest density near 1/2\n",
    "\n",
    "This makes it a good model when boundary behavior is common (e.g., “almost always” vs “almost never”).\n",
    "\n",
    "### Where it comes from (a memorable construction)\n",
    "\n",
    "Let \\(\\Theta \\sim \\mathrm{Unif}(0,\\pi)\\). Then\n",
    "\n",
    "\\[\n",
    "Y = \\cos\\Theta \\in [-1,1]\n",
    "\\]\n",
    "\n",
    "has density \\(f_Y(y)=\\tfrac{1}{\\pi\\sqrt{1-y^2}}\\), the classic arcsine law on \\([-1,1]\\).\n",
    "\n",
    "If we map \\([-1,1]\\) to \\([0,1]\\) via \\(X = \\tfrac{Y+1}{2}\\), we obtain\n",
    "\n",
    "\\[\n",
    "f_X(x) = \\frac{1}{\\pi\\sqrt{x(1-x)}}, \\qquad x\\in(0,1).\n",
    "\\]\n",
    "\n",
    "This trig construction is also the cleanest route to the mean/variance derivations later.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Stochastic processes (arcsine laws):** for a symmetric random walk / Brownian motion, the fraction of time the process stays above 0 has an arcsine limit law.\n",
    "- **Bayesian statistics:** \\(\\text{Beta}(\\tfrac12,\\tfrac12)\\) is the **Jeffreys prior** for a Bernoulli/Binomial probability parameter \\(p\\). It is “noninformative” in an information-geometric sense and is heavier near 0 and 1 than the uniform prior.\n",
    "- **Generative modeling of probabilities:** when you want random probabilities that are often extreme (near 0 or 1), arcsine is a simple choice.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- Special case of the Beta family: \\(\\text{arcsine} = \\text{Beta}(\\tfrac12,\\tfrac12)\\)\n",
    "- Location/scale transform to any \\([a,b]\\)\n",
    "- Closely related to the “arcsine on \\([-1,1]\\)” density \\(\\tfrac{1}{\\pi\\sqrt{1-y^2}}\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988493e",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "We’ll use the **endpoint parameterization** \\((a,b)\\) with \\(a<b\\).\n",
    "\n",
    "### PDF\n",
    "\n",
    "\\[\n",
    "f(x\\mid a,b) = \\frac{1}{\\pi\\sqrt{(x-a)(b-x)}},\n",
    "\\qquad x\\in(a,b)\n",
    "\\]\n",
    "\n",
    "and \\(f(x\\mid a,b)=0\\) for \\(x\\notin[a,b]\\).\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The pdf diverges as \\(x\\to a\\) or \\(x\\to b\\), but the divergence is integrable.\n",
    "- In SciPy, this corresponds to `stats.arcsine(loc=a, scale=b-a)`.\n",
    "\n",
    "### CDF\n",
    "\n",
    "\\[\n",
    "F(x\\mid a,b) =\n",
    "\\begin{cases}\n",
    "0, & x\\le a,\\\\\n",
    "\\frac{2}{\\pi}\\arcsin\\!\\Big(\\sqrt{\\frac{x-a}{b-a}}\\Big), & a<x<b,\\\\\n",
    "1, & x\\ge b.\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "In the canonical \\([0,1]\\) case:\n",
    "\n",
    "\\[\n",
    "f(x)=\\frac{1}{\\pi\\sqrt{x(1-x)}},\\quad\n",
    "F(x)=\\frac{2}{\\pi}\\arcsin(\\sqrt{x}).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_ab(a: float, b: float) -> None:\n",
    "    if not (np.isfinite(a) and np.isfinite(b) and a < b):\n",
    "        raise ValueError(\"Require finite endpoints with a < b.\")\n",
    "\n",
    "\n",
    "def arcsine_pdf(x, a: float = 0.0, b: float = 1.0):\n",
    "    '''Arcsine pdf on [a,b]. Vectorized over x.'''\n",
    "    _check_ab(a, b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    interior = (x > a) & (x < b)\n",
    "    boundary = (x == a) | (x == b)\n",
    "\n",
    "    out[boundary] = np.inf\n",
    "    out[interior] = 1.0 / (np.pi * np.sqrt((x[interior] - a) * (b - x[interior])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def arcsine_logpdf(x, a: float = 0.0, b: float = 1.0):\n",
    "    '''Numerically friendlier log-pdf (still +inf at boundaries).'''\n",
    "    _check_ab(a, b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    interior = (x > a) & (x < b)\n",
    "    boundary = (x == a) | (x == b)\n",
    "\n",
    "    out[boundary] = np.inf\n",
    "    out[interior] = (\n",
    "        -np.log(np.pi)\n",
    "        - 0.5 * np.log(x[interior] - a)\n",
    "        - 0.5 * np.log(b - x[interior])\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def arcsine_cdf(x, a: float = 0.0, b: float = 1.0):\n",
    "    '''Arcsine CDF on [a,b].'''\n",
    "    _check_ab(a, b)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    z = (x - a) / (b - a)\n",
    "    z = np.clip(z, 0.0, 1.0)  # protects sqrt/arcsin from tiny floating-point drift\n",
    "\n",
    "    out = (2.0 / np.pi) * np.arcsin(np.sqrt(z))\n",
    "    out = np.where(x <= a, 0.0, out)\n",
    "    out = np.where(x >= b, 1.0, out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def arcsine_ppf(u, a: float = 0.0, b: float = 1.0):\n",
    "    '''Inverse CDF (percent point function).'''\n",
    "    _check_ab(a, b)\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    if np.any((u < 0.0) | (u > 1.0)):\n",
    "        raise ValueError(\"u must be in [0,1].\")\n",
    "    return a + (b - a) * np.sin(0.5 * np.pi * u) ** 2\n",
    "\n",
    "\n",
    "def arcsine_rvs(size=None, a: float = 0.0, b: float = 1.0, rng: np.random.Generator | None = None):\n",
    "    '''NumPy-only sampling via inverse transform.'''\n",
    "    _check_ab(a, b)\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    u = rng.random(size=size)\n",
    "    return arcsine_ppf(u, a=a, b=b)\n",
    "\n",
    "\n",
    "# Quick sanity check against SciPy on [0,1]\n",
    "x_grid = np.linspace(1e-6, 1 - 1e-6, 5)\n",
    "np.c_[x_grid, arcsine_pdf(x_grid), stats.arcsine.pdf(x_grid)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588cadce",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let \\(X \\sim \\mathrm{Arcsine}(a,b)\\).\n",
    "\n",
    "### Mean and variance\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\frac{a+b}{2},\n",
    "\\qquad\n",
    "\\mathrm{Var}(X) = \\frac{(b-a)^2}{8}.\n",
    "\\]\n",
    "\n",
    "Because the distribution is symmetric around \\(\\tfrac{a+b}{2}\\), the **skewness is 0**.\n",
    "\n",
    "### Kurtosis\n",
    "\n",
    "For the canonical \\([0,1]\\) case (equivalently \\(\\text{Beta}(\\tfrac12,\\tfrac12)\\)):\n",
    "\n",
    "- skewness = 0\n",
    "- **excess kurtosis** = \\(-\\tfrac{3}{2}\\) (so kurtosis = \\(3 - \\tfrac{3}{2} = \\tfrac{3}{2}\\))\n",
    "\n",
    "These values stay the same under location/scale transforms.\n",
    "\n",
    "### MGF and characteristic function\n",
    "\n",
    "It’s convenient to express these using Bessel functions. Define:\n",
    "\n",
    "- \\(I_0\\): modified Bessel function of the first kind (order 0)\n",
    "- \\(J_0\\): Bessel function of the first kind (order 0)\n",
    "\n",
    "Then\n",
    "\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = \\exp\\Big( t\\,\\tfrac{a+b}{2}\\Big)\\, I_0\\Big( t\\,\\tfrac{b-a}{2}\\Big),\n",
    "\\qquad t\\in\\mathbb{R}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\exp\\Big( it\\,\\tfrac{a+b}{2}\\Big)\\, J_0\\Big( t\\,\\tfrac{b-a}{2}\\Big),\n",
    "\\qquad t\\in\\mathbb{R}.\n",
    "\\]\n",
    "\n",
    "### Entropy (differential)\n",
    "\n",
    "For the canonical \\([0,1]\\) distribution:\n",
    "\n",
    "\\[\n",
    "H(X) = \\log\\Big(\\frac{\\pi}{4}\\Big).\n",
    "\\]\n",
    "\n",
    "On \\([a,b]\\), scaling adds \\(\\log(b-a)\\):\n",
    "\n",
    "\\[\n",
    "H(X) = \\log(b-a) + \\log\\Big(\\frac{\\pi}{4}\\Big) = \\log\\Big( (b-a)\\,\\frac{\\pi}{4}\\Big).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcsine_mean(a: float = 0.0, b: float = 1.0) -> float:\n",
    "    _check_ab(a, b)\n",
    "    return 0.5 * (a + b)\n",
    "\n",
    "\n",
    "def arcsine_var(a: float = 0.0, b: float = 1.0) -> float:\n",
    "    _check_ab(a, b)\n",
    "    return (b - a) ** 2 / 8.0\n",
    "\n",
    "\n",
    "def arcsine_mgf(t, a: float = 0.0, b: float = 1.0):\n",
    "    '''MGF using modified Bessel I0.'''\n",
    "    _check_ab(a, b)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    return np.exp(t * 0.5 * (a + b)) * special.i0(t * 0.5 * (b - a))\n",
    "\n",
    "\n",
    "def arcsine_cf(t, a: float = 0.0, b: float = 1.0):\n",
    "    '''Characteristic function using Bessel J0.'''\n",
    "    _check_ab(a, b)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    return np.exp(1j * t * 0.5 * (a + b)) * special.j0(t * 0.5 * (b - a))\n",
    "\n",
    "\n",
    "def arcsine_entropy(a: float = 0.0, b: float = 1.0) -> float:\n",
    "    _check_ab(a, b)\n",
    "    return float(np.log((b - a) * np.pi / 4.0))\n",
    "\n",
    "\n",
    "# Monte Carlo check on [0,1]\n",
    "n_mc = 200_000\n",
    "x_mc = arcsine_rvs(n_mc, rng=rng)\n",
    "\n",
    "print(\"Monte Carlo mean:\", x_mc.mean(), \"| theory:\", arcsine_mean())\n",
    "print(\"Monte Carlo var :\", x_mc.var(), \"| theory:\", arcsine_var())\n",
    "print(\"Entropy theory  :\", arcsine_entropy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51d6e1",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The parameters \\(a\\) and \\(b\\) are **endpoints** of the support.\n",
    "\n",
    "- \\(a\\) shifts the distribution left/right\n",
    "- \\(b-a\\) scales (stretches) the interval\n",
    "\n",
    "Crucially, the *shape in normalized coordinates* does not change.\n",
    "\n",
    "If \\(X \\sim \\mathrm{Arcsine}(a,b)\\) and\n",
    "\n",
    "\\[\n",
    "Z = \\frac{X-a}{b-a},\n",
    "\\]\n",
    "\n",
    "then \\(Z \\sim \\mathrm{Arcsine}(0,1)\\) (equivalently \\(\\text{Beta}(\\tfrac12,\\tfrac12)\\)).\n",
    "\n",
    "So the family is “rigid”: parameters only **translate and rescale**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22272bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [(0.0, 1.0), (-1.0, 1.0), (2.0, 5.0)]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for a, b in intervals:\n",
    "    x = np.linspace(a + 1e-4 * (b - a), b - 1e-4 * (b - a), 800)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=arcsine_pdf(x, a=a, b=b),\n",
    "            name=f\"pdf on [{a:g}, {b:g}]\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Arcsine pdf for different endpoints (note the endpoint spikes)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63b7fd",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation and variance (trig substitution)\n",
    "\n",
    "A standard trick is to parameterize \\(x\\in[a,b]\\) by an angle.\n",
    "\n",
    "Let\n",
    "\n",
    "\\[\n",
    "x(\\theta) = \\frac{a+b}{2} + \\frac{b-a}{2}\\cos\\theta,\\qquad \\theta\\in(0,\\pi).\n",
    "\\]\n",
    "\n",
    "Then\n",
    "\n",
    "\\[\n",
    "(x-a)(b-x) = \\Big(\\tfrac{b-a}{2}\\Big)^2 \\sin^2\\theta,\n",
    "\\qquad\n",
    "\\mathrm{d}x = -\\tfrac{b-a}{2}\\sin\\theta\\,\\mathrm{d}\\theta.\n",
    "\\]\n",
    "\n",
    "Plugging into \\(f(x\\mid a,b)\\,\\mathrm{d}x\\):\n",
    "\n",
    "\\[\n",
    "\\frac{1}{\\pi\\sqrt{(x-a)(b-x)}}\\,\\mathrm{d}x\n",
    "= \\frac{1}{\\pi\\,(\\tfrac{b-a}{2})\\sin\\theta}\\,\\Big(-\\tfrac{b-a}{2}\\sin\\theta\\,\\mathrm{d}\\theta\\Big)\n",
    "= -\\frac{1}{\\pi}\\,\\mathrm{d}\\theta.\n",
    "\\]\n",
    "\n",
    "Flipping integration limits turns the minus sign into a plus, so effectively\n",
    "\n",
    "\\[\n",
    "f(x)\\,\\mathrm{d}x = \\frac{1}{\\pi}\\,\\mathrm{d}\\theta,\n",
    "\\quad \\theta\\sim\\mathrm{Unif}(0,\\pi).\n",
    "\\]\n",
    "\n",
    "**Mean.**\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\frac{1}{\\pi}\\int_0^\\pi x(\\theta)\\,\\mathrm{d}\\theta\n",
    "= \\frac{1}{\\pi}\\int_0^\\pi \\Big(\\tfrac{a+b}{2} + \\tfrac{b-a}{2}\\cos\\theta\\Big)\\,\\mathrm{d}\\theta\n",
    "= \\frac{a+b}{2}\n",
    "\\]\n",
    "\n",
    "because \\(\\int_0^\\pi \\cos\\theta\\,\\mathrm{d}\\theta = 0\\).\n",
    "\n",
    "**Variance.** Write \\(\\mu=\\tfrac{a+b}{2}\\). Then\n",
    "\n",
    "\\[\n",
    "X-\\mu = \\tfrac{b-a}{2}\\cos\\theta,\n",
    "\\qquad\n",
    "(X-\\mu)^2 = \\Big(\\tfrac{b-a}{2}\\Big)^2 \\cos^2\\theta.\n",
    "\\]\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\mathbb{E}[(X-\\mu)^2] = \\Big(\\tfrac{b-a}{2}\\Big)^2 \\cdot \\frac{1}{\\pi}\\int_0^\\pi \\cos^2\\theta\\,\\mathrm{d}\\theta\n",
    "= \\Big(\\tfrac{b-a}{2}\\Big)^2 \\cdot \\frac{1}{2}\n",
    "= \\frac{(b-a)^2}{8}.\n",
    "\\]\n",
    "\n",
    "### 6.2 Likelihood\n",
    "\n",
    "For i.i.d. data \\(x_1,\\dots,x_n\\) with all points in \\((a,b)\\), the likelihood is\n",
    "\n",
    "\\[\n",
    "L(a,b) = \\prod_{i=1}^n \\frac{1}{\\pi\\sqrt{(x_i-a)(b-x_i)}}.\n",
    "\\]\n",
    "\n",
    "The log-likelihood (up to an additive constant) is\n",
    "\n",
    "\\[\n",
    "\\ell(a,b) = -\\frac{1}{2}\\sum_{i=1}^n \\log(x_i-a) - \\frac{1}{2}\\sum_{i=1}^n \\log(b-x_i),\n",
    "\\qquad \\text{subject to } a<\\min_i x_i,\\; b>\\max_i x_i.\n",
    "\\]\n",
    "\n",
    "**Important:** because \\(\\log(x_i-a)\\to -\\infty\\) as \\(a\\uparrow \\min x_i\\), the log-likelihood can grow without bound.\n",
    "So the unconstrained MLE for \\((a,b)\\) does **not** exist (the likelihood is unbounded).\n",
    "Practical fitting methods therefore use constraints/regularization or alternative estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d04513",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### Inverse transform sampling (NumPy-only)\n",
    "\n",
    "Starting from the CDF for \\([a,b]\\):\n",
    "\n",
    "\\[\n",
    "u = F(x\\mid a,b) = \\frac{2}{\\pi}\\arcsin\\!\\Big(\\sqrt{\\frac{x-a}{b-a}}\\Big)\n",
    "\\]\n",
    "\n",
    "Solve for \\(x\\):\n",
    "\n",
    "\\[\n",
    "\\sqrt{\\frac{x-a}{b-a}} = \\sin\\Big(\\frac{\\pi u}{2}\\Big)\n",
    "\\quad\\Rightarrow\\quad\n",
    "x = a + (b-a)\\,\\sin^2\\Big(\\frac{\\pi u}{2}\\Big).\n",
    "\\]\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "1. Sample \\(u \\sim \\mathrm{Unif}(0,1)\\)\n",
    "2. Return \\(x = a + (b-a)\\sin^2(\\pi u/2)\\)\n",
    "\n",
    "This is exact, fast, and requires only NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14764b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling demo\n",
    "a, b = 0.0, 1.0\n",
    "x = arcsine_rvs(10, a=a, b=b, rng=rng)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f1cd9",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "\n",
    "- the pdf\n",
    "- the cdf\n",
    "- Monte Carlo samples vs the theoretical pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7163252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF and CDF on [0,1]\n",
    "eps = 1e-4\n",
    "x = np.linspace(eps, 1 - eps, 1000)\n",
    "\n",
    "fig_pdf = go.Figure()\n",
    "fig_pdf.add_trace(go.Scatter(x=x, y=arcsine_pdf(x), name=\"arcsine pdf\"))\n",
    "fig_pdf.add_trace(go.Scatter(x=x, y=np.ones_like(x), name=\"uniform(0,1) pdf\", line=dict(dash=\"dash\")))\n",
    "fig_pdf.update_layout(title=\"PDF on [0,1] (arcsine vs uniform)\", xaxis_title=\"x\", yaxis_title=\"density\")\n",
    "fig_pdf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=x, y=arcsine_cdf(x), name=\"arcsine cdf\"))\n",
    "fig_cdf.add_trace(go.Scatter(x=x, y=x, name=\"uniform(0,1) cdf\", line=dict(dash=\"dash\")))\n",
    "fig_cdf.update_layout(title=\"CDF on [0,1] (arcsine vs uniform)\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "fig_cdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7032e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo samples vs pdf\n",
    "n = 60_000\n",
    "samples = arcsine_rvs(n, rng=rng)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples,\n",
    "        nbinsx=80,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"samples (hist)\",\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=arcsine_pdf(x), name=\"theoretical pdf\", line=dict(color=\"black\")))\n",
    "fig.update_layout(\n",
    "    title=\"Monte Carlo histogram with theoretical pdf overlay\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"sample mean/var:\", samples.mean(), samples.var())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450f3e0",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides the distribution as `scipy.stats.arcsine`.\n",
    "\n",
    "- `stats.arcsine.pdf(x, loc=a, scale=b-a)`\n",
    "- `stats.arcsine.cdf(x, loc=a, scale=b-a)`\n",
    "- `stats.arcsine.rvs(size=..., loc=a, scale=b-a, random_state=...)`\n",
    "- `stats.arcsine.fit(data)` estimates `loc` and `scale`\n",
    "\n",
    "We’ll also verify the identity with \\(\\text{Beta}(\\tfrac12,\\tfrac12)\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feffabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy: pdf/cdf/rvs\n",
    "x = np.linspace(1e-4, 1 - 1e-4, 1000)\n",
    "\n",
    "pdf_scipy = stats.arcsine.pdf(x)\n",
    "cdf_scipy = stats.arcsine.cdf(x)\n",
    "\n",
    "# Identity: arcsine == Beta(1/2, 1/2)\n",
    "pdf_beta = stats.beta(a=0.5, b=0.5).pdf(x)\n",
    "\n",
    "print(\"max |pdf_scipy - pdf_beta|:\", np.max(np.abs(pdf_scipy - pdf_beta)))\n",
    "\n",
    "# Sampling\n",
    "s_scipy = stats.arcsine.rvs(size=5, random_state=rng)\n",
    "s_numpy = arcsine_rvs(5, rng=rng)\n",
    "print(\"SciPy rvs:\", s_scipy)\n",
    "print(\"NumPy rvs:\", s_numpy)\n",
    "\n",
    "# Fitting loc/scale (note: likelihood is tricky near the endpoints)\n",
    "data = arcsine_rvs(2_000, a=2.0, b=5.0, rng=rng)\n",
    "loc_hat, scale_hat = stats.arcsine.fit(data)\n",
    "print(\"fit loc, scale:\", loc_hat, scale_hat)\n",
    "print(\"true loc, scale:\", 2.0, 3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c4706",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing (arcsine law for random walks)\n",
    "\n",
    "For a symmetric random walk \\(S_t = \\sum_{i=1}^t \\epsilon_i\\) with \\(\\epsilon_i\\in\\{-1,+1\\}\\) i.i.d., one version of the arcsine law says:\n",
    "\n",
    "> the fraction of time the walk is positive converges in distribution to an arcsine law.\n",
    "\n",
    "We’ll simulate many random walks, compute the proportion of steps with \\(S_t>0\\), and compare to the arcsine distribution on \\([0,1]\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paths = 4000\n",
    "n_steps = 600\n",
    "\n",
    "steps = rng.choice([-1, 1], size=(n_paths, n_steps))\n",
    "paths = np.cumsum(steps, axis=1)\n",
    "\n",
    "frac_positive = (paths > 0).mean(axis=1)\n",
    "\n",
    "ks = stats.kstest(frac_positive, stats.arcsine.cdf)\n",
    "print(\"KS statistic:\", ks.statistic)\n",
    "print(\"KS p-value   :\", ks.pvalue)\n",
    "\n",
    "x = np.linspace(1e-4, 1 - 1e-4, 800)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=frac_positive,\n",
    "        nbinsx=60,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"random-walk fractions\",\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.arcsine.pdf(x), name=\"arcsine pdf\", line=dict(color=\"black\")))\n",
    "fig.update_layout(\n",
    "    title=\"Fraction of time positive in a symmetric random walk (simulation)\",\n",
    "    xaxis_title=\"fraction of steps with S_t > 0\",\n",
    "    yaxis_title=\"density\",\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea18a24",
   "metadata": {},
   "source": [
    "### 10.2 Bayesian modeling (Jeffreys prior for Bernoulli/Binomial)\n",
    "\n",
    "For a Bernoulli/Binomial success probability \\(p\\), the Jeffreys prior is\n",
    "\n",
    "\\[\n",
    "p \\sim \\mathrm{Beta}(\\tfrac12, \\tfrac12),\n",
    "\\]\n",
    "\n",
    "which is exactly the arcsine distribution on \\([0,1]\\).\n",
    "\n",
    "If we observe \\(k\\) successes in \\(n\\) trials, the posterior is\n",
    "\n",
    "\\[\n",
    "p \\mid k \\sim \\mathrm{Beta}\\Big(k+\\tfrac12,\\; n-k+\\tfrac12\\Big).\n",
    "\\]\n",
    "\n",
    "We’ll compare Jeffreys’ prior to a uniform prior \\(\\mathrm{Beta}(1,1)\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4272784",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, k = 20, 3\n",
    "\n",
    "prior_jeffreys = stats.beta(0.5, 0.5)\n",
    "prior_uniform = stats.beta(1.0, 1.0)\n",
    "\n",
    "post_jeffreys = stats.beta(k + 0.5, n - k + 0.5)\n",
    "post_uniform = stats.beta(k + 1.0, n - k + 1.0)\n",
    "\n",
    "x = np.linspace(1e-4, 1 - 1e-4, 1000)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=prior_jeffreys.pdf(x), name=\"Jeffreys prior Beta(1/2,1/2)\", line=dict(dash=\"dash\")))\n",
    "fig.add_trace(go.Scatter(x=x, y=prior_uniform.pdf(x), name=\"Uniform prior Beta(1,1)\", line=dict(dash=\"dash\")))\n",
    "fig.add_trace(go.Scatter(x=x, y=post_jeffreys.pdf(x), name=f\"Posterior (Jeffreys), k={k}, n={n}\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=post_uniform.pdf(x), name=f\"Posterior (uniform), k={k}, n={n}\"))\n",
    "\n",
    "ci_low, ci_high = post_jeffreys.ppf([0.025, 0.975])\n",
    "fig.add_vline(x=ci_low, line=dict(color=\"black\", dash=\"dot\"))\n",
    "fig.add_vline(x=ci_high, line=dict(color=\"black\", dash=\"dot\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Jeffreys (arcsine) prior and resulting posterior for a Binomial proportion\",\n",
    "    xaxis_title=\"p\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"Jeffreys posterior mean:\", post_jeffreys.mean())\n",
    "print(\"Jeffreys 95% credible interval:\", (ci_low, ci_high))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45969972",
   "metadata": {},
   "source": [
    "### 10.3 Generative modeling (sampling extreme probabilities)\n",
    "\n",
    "If you sample \\(p\\) from an arcsine distribution and then sample data conditional on \\(p\\), you generate datasets where the latent probability is often near 0 or 1.\n",
    "\n",
    "One simple example:\n",
    "\n",
    "- sample \\(p \\sim \\mathrm{Beta}(\\tfrac12,\\tfrac12)\\)\n",
    "- then sample a count \\(K \\mid p \\sim \\mathrm{Binomial}(n,p)\\)\n",
    "\n",
    "Compared to a uniform prior over \\(p\\), this produces more **extreme** counts (very small or very large \\(K\\)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3463e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "m = 30_000\n",
    "\n",
    "p_arcsine = stats.arcsine.rvs(size=m, random_state=rng)\n",
    "p_uniform = rng.random(size=m)\n",
    "\n",
    "k_arcsine = rng.binomial(n, p_arcsine)\n",
    "k_uniform = rng.binomial(n, p_uniform)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=k_arcsine, histnorm=\"probability\", name=\"p ~ arcsine\", opacity=0.6, nbinsx=n + 1))\n",
    "fig.add_trace(go.Histogram(x=k_uniform, histnorm=\"probability\", name=\"p ~ uniform\", opacity=0.6, nbinsx=n + 1))\n",
    "fig.update_layout(\n",
    "    title=f\"Counts from Binomial(n={n}, p) with different priors over p\",\n",
    "    xaxis_title=\"k (number of successes)\",\n",
    "    yaxis_title=\"probability\",\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"P(k=0)  arcsine vs uniform:\", np.mean(k_arcsine == 0), np.mean(k_uniform == 0))\n",
    "print(\"P(k=n)  arcsine vs uniform:\", np.mean(k_arcsine == n), np.mean(k_uniform == n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e131b",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters:** always ensure \\(a<b\\) (or `scale>0`).\n",
    "- **Endpoint singularities:** the pdf diverges at \\(a\\) and \\(b\\). This is mathematically fine, but numerically:\n",
    "  - avoid evaluating the pdf exactly at the endpoints when plotting\n",
    "  - prefer `logpdf` for likelihood computations\n",
    "- **Floating-point drift:** for the CDF, expressions like \\((x-a)/(b-a)\\) can be slightly below 0 or above 1 due to rounding; clipping prevents `sqrt`/`arcsin` from producing NaNs.\n",
    "- **Fitting endpoints:** the likelihood can become unbounded as endpoints approach the sample min/max, so naive MLE fitting is ill-posed without additional constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c783931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical illustration: pdf spikes and logpdf is usually safer\n",
    "x_test = np.array([0.0, 1e-12, 0.5, 1 - 1e-12, 1.0])\n",
    "print(\"x:\", x_test)\n",
    "print(\"pdf:\", stats.arcsine.pdf(x_test))\n",
    "print(\"logpdf:\", stats.arcsine.logpdf(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469851d3",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- The arcsine distribution on \\([0,1]\\) is \\(\\mathrm{Beta}(\\tfrac12,\\tfrac12)\\) with pdf \\(f(x)=\\tfrac{1}{\\pi\\sqrt{x(1-x)}}\\).\n",
    "- It has a **U-shaped** density: lots of mass near the boundaries.\n",
    "- Endpoint form on \\([a,b]\\): \\(f(x\\mid a,b)=\\tfrac{1}{\\pi\\sqrt{(x-a)(b-x)}}\\).\n",
    "- Mean \\(=\\tfrac{a+b}{2}\\), variance \\(=\\tfrac{(b-a)^2}{8}\\); skewness 0, excess kurtosis \\(-\\tfrac{3}{2}\\).\n",
    "- Fast exact sampling: \\(x = a + (b-a)\\sin^2(\\pi u/2)\\) with \\(u\\sim\\mathrm{Unif}(0,1)\\).\n",
    "- In practice, `scipy.stats.arcsine` gives `pdf`, `cdf`, `rvs`, and `fit` (with caution near endpoints).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}