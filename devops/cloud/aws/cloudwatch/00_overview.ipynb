{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a01f2d",
   "metadata": {},
   "source": [
    "# Amazon CloudWatch\n",
    "\n",
    "<img src=\"../_assets/aws_service_icons/cloudwatch.svg\" width=\"80\" alt=\"Amazon CloudWatch\">\n",
    "\n",
    "## Goals\n",
    "- Understand what CloudWatch is\n",
    "- Know what it\u2019s practically used for\n",
    "- See a simple **SDK pseudo-code** example (not executed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3a91b",
   "metadata": {},
   "source": [
    "## What is CloudWatch?\n",
    "\n",
    "**Amazon CloudWatch** is AWS\u2019s primary **monitoring + observability** service.\n",
    "\n",
    "At a high level, CloudWatch helps you:\n",
    "- Collect **metrics** (time-series measurements) from AWS services and your own applications\n",
    "- Collect and search **logs** (CloudWatch Logs)\n",
    "- Create **alarms** that notify or trigger actions when something crosses a threshold\n",
    "- Build **dashboards** for operational visibility\n",
    "\n",
    "Most AWS services publish built-in metrics automatically (e.g., CPU utilization, request counts, error rates). You can also publish **custom metrics** for application-level signals (e.g., inference latency, queue depth).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8c2a7",
   "metadata": {},
   "source": [
    "## What is CloudWatch used for (practically)?\n",
    "\n",
    "CloudWatch is commonly used to answer operational questions like:\n",
    "- \u201cIs the system healthy right now?\u201d (dashboards + key metrics)\n",
    "- \u201cDid something break?\u201d (alarms + notifications)\n",
    "- \u201cWhy did it break?\u201d (logs + correlation with metrics)\n",
    "- \u201cIs performance getting worse over time?\u201d (trend analysis, percentiles)\n",
    "\n",
    "Typical use cases:\n",
    "- **Alerting**: notify on high error rates, latency spikes, low disk space, elevated throttling\n",
    "- **Autoscaling signals**: drive scaling decisions based on CPU, queue depth, custom metrics\n",
    "- **Troubleshooting**: correlate deploys/incidents with metrics and log traces\n",
    "- **Compliance/operations**: retain logs with defined retention, build audit-style dashboards\n",
    "\n",
    "In ML systems (examples):\n",
    "- Monitor **training jobs** (duration, resource usage, failures)\n",
    "- Monitor **inference endpoints** (p50/p95 latency, error rate, saturation)\n",
    "- Track **data quality / drift** signals as custom metrics (where appropriate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce0a31",
   "metadata": {},
   "source": [
    "## Core concepts (minimum you should know)\n",
    "\n",
    "- **Metrics**: time-series data points (e.g., `Latency`, `5xxErrorRate`).\n",
    "- **Namespace**: logical grouping for metrics (AWS uses namespaces like `AWS/Lambda`; you can define your own).\n",
    "- **Dimensions**: key/value labels that slice a metric (e.g., `FunctionName=...`, `Model=...`).\n",
    "- **Statistics / percentiles**: summarize many samples per period (e.g., average, max, p95).\n",
    "- **Alarms**: evaluate a metric over time and trigger actions (e.g., notify via SNS).\n",
    "- **Dashboards**: visualize metrics for a service or system.\n",
    "- **CloudWatch Logs**: log groups/streams, retention policies, and search/query (Logs Insights).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fd9f6",
   "metadata": {},
   "source": [
    "## Using CloudWatch with an SDK (pseudo-code)\n",
    "This is **illustrative pseudo-code** showing a typical workflow:\n",
    "1) publish a custom metric,\n",
    "2) create an alarm,\n",
    "3) query recent datapoints.\n",
    "**Note**: requires AWS credentials/permissions and a region; add retries/logging/error handling in production.\n",
    "```python\n",
    "# PSEUDO-CODE (do not run)\n",
    "import boto3\n",
    "from datetime import datetime, timedelta, timezone\n",
    "region = \"us-east-1\"\n",
    "# CloudWatch (metrics + alarms)\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "# 1) Publish a custom application metric\n",
    "# Example: inference latency in milliseconds\n",
    "namespace = \"DemoApp\"\n",
    "metric_name = \"InferenceLatencyMs\"\n",
    "dims = [{\"Name\": \"Model\", \"Value\": \"recommender-v1\"}]\n",
    "cw.put_metric_data(\n",
    "    Namespace=namespace,\n",
    "    MetricData=[\n",
    "        {\n",
    "            \"MetricName\": metric_name,\n",
    "            \"Dimensions\": dims,\n",
    "            \"Timestamp\": datetime.now(timezone.utc),\n",
    "            \"Value\": 123.4,\n",
    "            \"Unit\": \"Milliseconds\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "# 2) Create an alarm (notify an SNS topic if p95 > 200ms for 3 minutes)\n",
    "alarm_name = \"demo-inference-latency-p95-high\"\n",
    "sns_topic_arn = \"arn:aws:sns:us-east-1:123456789012:ops-alerts\"\n",
    "cw.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    Namespace=namespace,\n",
    "    MetricName=metric_name,\n",
    "    Dimensions=dims,\n",
    "    Period=60,\n",
    "    EvaluationPeriods=3,\n",
    "    Threshold=200.0,\n",
    "    ComparisonOperator=\"GreaterThanThreshold\",\n",
    "    TreatMissingData=\"missing\",\n",
    "    ExtendedStatistic=\"p95\",\n",
    "    AlarmActions=[sns_topic_arn],\n",
    ")\n",
    "# 3) Query recent datapoints (for quick debugging / basic reporting)\n",
    "end = datetime.now(timezone.utc)\n",
    "start = end - timedelta(minutes=10)\n",
    "resp = cw.get_metric_statistics(\n",
    "    Namespace=namespace,\n",
    "    MetricName=metric_name,\n",
    "    Dimensions=dims,\n",
    "    StartTime=start,\n",
    "    EndTime=end,\n",
    "    Period=60,\n",
    "    Statistics=[\"Average\", \"Maximum\"],\n",
    ")\n",
    "datapoints = sorted(resp.get(\"Datapoints\", []), key=lambda d: d[\"Timestamp\"])\n",
    "print(datapoints)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0be95e",
   "metadata": {},
   "source": [
    "## Pitfalls & quick tips\n",
    "\n",
    "- Keep **dimension cardinality** under control (too many unique dimension values can get expensive/noisy).\n",
    "- Set **log retention** intentionally (never-ending retention can become costly).\n",
    "- Prefer percentiles (e.g., **p95**) for latency over averages when tail behavior matters.\n",
    "- Treat alarms as part of a system: route notifications (SNS) to the right on-call path and avoid alert fatigue.\n",
    "\n",
    "## References\n",
    "- AWS Docs: Amazon CloudWatch (metrics, alarms, dashboards)\n",
    "- AWS Docs: CloudWatch Logs + Logs Insights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}