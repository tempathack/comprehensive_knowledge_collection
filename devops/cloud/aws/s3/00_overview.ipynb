{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amazon S3 (Simple Storage Service)\n",
        "\n",
        "<img src=\"../_assets/aws_service_icons/s3.svg\" width=\"80\" alt=\"Amazon S3\">\n",
        "\n",
        "## Goals\n",
        "- Understand what **Amazon S3** is (and what it is not).\n",
        "- Know common practical use-cases.\n",
        "- See a minimal **AWS SDK** pseudo-code workflow (no execution).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "- Basic cloud concepts (regions, IAM) help.\n",
        "- Familiarity with files/paths and HTTP.\n",
        "\n",
        "> This notebook includes **pseudo-code only**. It does not run any AWS SDK calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What S3 is\n",
        "**Amazon S3** is AWS’s managed **object storage** service.\n",
        "\n",
        "Core concepts:\n",
        "- **Bucket**: top-level container for objects (bucket names are globally unique).\n",
        "- **Object**: the stored bytes + metadata.\n",
        "- **Key**: the object name inside a bucket (keys can include `/` to act like folder *prefixes*).\n",
        "\n",
        "Important properties and constraints:\n",
        "- S3 is accessed over an **HTTP API** (and SDKs wrap that API).\n",
        "- It’s not a traditional filesystem: you typically **upload/download whole objects** rather than doing in-place edits.\n",
        "- It’s designed for very high durability and availability, and supports features like versioning, lifecycle policies, encryption, and access controls.\n",
        "\n",
        "### What it is not\n",
        "- Not a block disk (like EBS) or a network filesystem (like EFS).\n",
        "- Not a database (no SQL queries without additional tools/services).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What S3 is practically used for\n",
        "S3 is commonly used as the default “place to put bytes” in AWS:\n",
        "- **Data lakes**: raw/bronze data, curated/silver data, and analytics-ready/gold data.\n",
        "- **Machine learning**: training datasets, feature files, model artifacts, experiment outputs.\n",
        "- **Static assets**: images, documents, client bundles, and (optionally) static website hosting.\n",
        "- **Backups and archives**: snapshots/exports and long-term retention with lifecycle rules.\n",
        "- **Logs and event data**: centralized storage before downstream processing.\n",
        "- **System-to-system exchange**: a simple integration point between services or teams.\n",
        "\n",
        "Common operational patterns:\n",
        "- Separate buckets (or prefixes) for **raw vs processed** data.\n",
        "- Use **IAM roles/policies** and **bucket policies** to control access.\n",
        "- Enable **encryption** (SSE-S3 or SSE-KMS) and consider **versioning** for recoverability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using S3 with the AWS SDK (pseudo-code)\n",
        "Below is a minimal, **non-executable** sketch of common S3 operations using an AWS SDK.\n",
        "\n",
        "Notes:\n",
        "- In real projects, avoid hardcoding credentials; prefer **IAM roles** (EC2/ECS/Lambda) or SSO-based local credentials.\n",
        "- S3 operations can read/write real data and incur cost; use a dedicated dev bucket and clean up.\n",
        "\n",
        "```python\n",
        "# PSEUDO-CODE (do not run)\n",
        "\n",
        "import boto3\n",
        "\n",
        "region = \"us-east-1\"\n",
        "s3 = boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "bucket = \"my-team-ml-artifacts\"\n",
        "prefix = \"experiments/run-001/\"\n",
        "\n",
        "# 1) (Optional) Create a bucket (naming must be globally unique).\n",
        "# s3.create_bucket(Bucket=bucket, CreateBucketConfiguration={\"LocationConstraint\": region})\n",
        "\n",
        "# 2) Upload a local file.\n",
        "local_path = \"./models/model.pkl\"\n",
        "key = f\"{prefix}model.pkl\"\n",
        "s3.upload_file(local_path, bucket, key)\n",
        "\n",
        "# 3) List objects under a prefix.\n",
        "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
        "for obj in resp.get(\"Contents\", []):\n",
        "    print(obj[\"Key\"], obj[\"Size\"])\n",
        "\n",
        "# 4) Download an object to disk.\n",
        "download_path = \"./downloads/model.pkl\"\n",
        "s3.download_file(bucket, key, download_path)\n",
        "\n",
        "# 5) Generate a temporary (pre-signed) URL for controlled access.\n",
        "url = s3.generate_presigned_url(\n",
        "    ClientMethod=\"get_object\",\n",
        "    Params={\"Bucket\": bucket, \"Key\": key},\n",
        "    ExpiresIn=3600,\n",
        ")\n",
        "print(url)\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
