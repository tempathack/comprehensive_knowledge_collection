{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Managed Workflows for Apache Airflow (MWAA)\n",
    "\n",
    "<img src=\"../_assets/aws_service_icons/mwaa.svg\" width=\"80\" alt=\"AWS MWAA\">\n",
    "\n",
    "## Goals\n",
    "- Understand what **MWAA** is (and what it is not).\n",
    "- Know what it\u2019s practically used for.\n",
    "- See a minimal **AWS SDK** pseudo-code workflow (no execution).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Basic Airflow concepts: DAGs, tasks, schedules, retries.\n",
    "- Basic AWS concepts: IAM roles, S3, VPC/subnets/security groups.\n",
    "- Familiarity with an AWS SDK (e.g., Python `boto3`).\n",
    "\n",
    "> This notebook includes **pseudo-code only**. It does not run any AWS SDK calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What MWAA is\n",
    "**Amazon Managed Workflows for Apache Airflow (MWAA)** is AWS\u2019s **managed** offering for running **Apache Airflow**.\n",
    "\n",
    "You bring:\n",
    "- **DAG code** (workflow definitions) and optional dependencies/plugins.\n",
    "- Connection details and IAM permissions for the systems your tasks interact with.\n",
    "\n",
    "AWS manages:\n",
    "- Provisioning and operating the **Airflow components** (scheduler, web server, workers).\n",
    "- Scaling, patching, and integrating with AWS primitives (CloudWatch logs/metrics, IAM, VPC networking).\n",
    "\n",
    "MWAA environments are typically configured with:\n",
    "- An **S3 bucket** for DAGs (and often plugins/requirements).\n",
    "- An **execution role** (IAM) used by the environment.\n",
    "- **VPC networking** (subnets + security groups).\n",
    "\n",
    "### What it is not\n",
    "- Not an ETL/compute engine by itself: Airflow **orchestrates** work; your tasks run on other compute/services.\n",
    "- Not a replacement for data processing tools (Spark/Glue), model training services (SageMaker), or storage (S3/RDS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What MWAA is practically used for\n",
    "MWAA is used when you want **Airflow-style orchestration** (dependencies, retries, schedules, backfills, observability) without running Airflow yourself.\n",
    "\n",
    "Common use-cases:\n",
    "- **Data pipelines**: ingest \u2192 validate \u2192 transform \u2192 load (ETL/ELT).\n",
    "- **ML pipelines**: feature generation, training, evaluation, batch inference, model promotion.\n",
    "- **Cross-service orchestration**: coordinate work across Lambda/ECS/Batch/Glue/EMR/SageMaker and databases.\n",
    "- **Operational workflows**: report generation, periodic maintenance jobs, data quality checks.\n",
    "\n",
    "Why teams choose MWAA:\n",
    "- Prefer the Airflow ecosystem (operators/sensors) but want AWS-managed ops.\n",
    "- Need a central scheduler with clear dependency graphs, retries, and auditability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MWAA with the AWS SDK (pseudo-code)\n",
    "Below is a minimal, **non-executable** sketch of a typical workflow:\n",
    "1) Upload a DAG to the S3 DAGs prefix.\n",
    "2) Create an MWAA environment pointing at that bucket/prefix.\n",
    "3) Wait for the environment to become available.\n",
    "4) Trigger a DAG run using an MWAA CLI token.\n",
    "\n",
    "```python\n",
    "# PSEUDO-CODE (do not run)\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "region = \"us-east-1\"\n",
    "\n",
    "# Service clients\n",
    "mwaa = boto3.client(\"mwaa\", region_name=region)\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# 1) Upload a DAG to the MWAA DAGs S3 prefix\n",
    "bucket = \"my-mwaa-artifacts\"\n",
    "dags_prefix = \"dags/\"  # MWAA reads DAGs from this prefix\n",
    "s3.upload_file(\n",
    "    Filename=\"./dags/example_pipeline.py\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{dags_prefix}example_pipeline.py\",\n",
    ")\n",
    "\n",
    "# 2) Create the MWAA environment (one-time setup)\n",
    "env_name = \"ml-pipelines\"\n",
    "mwaa.create_environment(\n",
    "    Name=env_name,\n",
    "    AirflowVersion=\"2.7.2\",  # example; choose a supported version\n",
    "    SourceBucketArn=f\"arn:aws:s3:::{bucket}\",\n",
    "    DagS3Path=dags_prefix,\n",
    "    ExecutionRoleArn=\"arn:aws:iam::<account-id>:role/<mwaa-execution-role>\",\n",
    "    NetworkConfiguration={\n",
    "        \"SecurityGroupIds\": [\"sg-xxxxxxxx\"],\n",
    "        \"SubnetIds\": [\"subnet-aaaaaaa\", \"subnet-bbbbbbb\"],\n",
    "    },\n",
    "    EnvironmentClass=\"mw1.small\",\n",
    "    LoggingConfiguration={\n",
    "        \"DagProcessingLogs\": {\"Enabled\": True, \"LogLevel\": \"INFO\"},\n",
    "        \"SchedulerLogs\": {\"Enabled\": True, \"LogLevel\": \"INFO\"},\n",
    "        \"TaskLogs\": {\"Enabled\": True, \"LogLevel\": \"INFO\"},\n",
    "        \"WebserverLogs\": {\"Enabled\": True, \"LogLevel\": \"INFO\"},\n",
    "        \"WorkerLogs\": {\"Enabled\": True, \"LogLevel\": \"INFO\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "# 3) Wait until the environment is ready (environment creation is async)\n",
    "def wait_until_available(name: str):\n",
    "    while True:\n",
    "        env = mwaa.get_environment(Name=name)[\"Environment\"]\n",
    "        if env[\"Status\"] == \"AVAILABLE\":\n",
    "            return env\n",
    "        sleep(60)\n",
    "\n",
    "env = wait_until_available(env_name)\n",
    "\n",
    "# 4) Trigger a DAG run via the MWAA Airflow CLI endpoint\n",
    "# MWAA provides a short-lived token to call the webserver's CLI endpoint.\n",
    "token = mwaa.create_cli_token(Name=env_name)\n",
    "webserver = token[\"WebServerHostname\"]\n",
    "cli_token = token[\"CliToken\"]\n",
    "\n",
    "resp = requests.post(\n",
    "    url=f\"https://{webserver}/aws_mwaa/cli\",\n",
    "    headers={\"Authorization\": f\"Bearer {cli_token}\", \"Content-Type\": \"text/plain\"},\n",
    "    data=\"dags trigger example_pipeline\",\n",
    "    timeout=30,\n",
    ")\n",
    "print(resp.text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls & quick tips\n",
    "- **Networking**: MWAA runs in your VPC; plan subnets/route tables/NAT so tasks can reach required endpoints.\n",
    "- **IAM**: the execution role must allow access to S3 DAGs, logs, and any services your tasks call.\n",
    "- **Dependencies**: Python deps are typically provided via a `requirements.txt` in S3 (and plugins via `plugins.zip`).\n",
    "- **Asynchronous ops**: environment creation/updates take time; build retry/polling into automation.\n",
    "- **Costs**: MWAA is billed while the environment exists; automate cleanup for demos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- AWS Docs: Amazon MWAA (concepts, setup, IAM, networking)\n",
    "- Apache Airflow Docs: DAGs, operators, scheduling, best practices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
