{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d4c33f",
   "metadata": {},
   "source": [
    "# Column-Family Stores: Cassandra & HBase\n",
    "\n",
    "Column-family stores (also called wide-column stores) are NoSQL databases designed for **massive scalability** and **high write throughput**. They organize data into rows and column families, providing flexible schemas while maintaining efficient storage and retrieval patterns.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Row Key** | Unique identifier for each row; primary index for data access |\n",
    "| **Column Family** | Logical grouping of columns stored together on disk |\n",
    "| **Column** | Name-value pair with optional timestamp |\n",
    "| **Super Column** | Column containing nested columns (deprecated in modern systems) |\n",
    "| **Keyspace** | Top-level namespace (similar to database in RDBMS) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117e4b6",
   "metadata": {},
   "source": [
    "## Wide-Column Data Model\n",
    "\n",
    "Unlike traditional relational tables, wide-column stores allow:\n",
    "- **Sparse data**: Each row can have different columns\n",
    "- **Dynamic columns**: Add columns without schema changes\n",
    "- **Denormalized design**: Data duplicated for read efficiency\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        KEYSPACE: sensors                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                   COLUMN FAMILY: readings                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Row Key    â”‚              Columns                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ sensor_001   â”‚ temp:72.5 | humidity:45 | pressure:1013          â”‚\n",
    "â”‚ sensor_002   â”‚ temp:68.2 | humidity:52                          â”‚\n",
    "â”‚ sensor_003   â”‚ temp:75.1 | co2:412 | light:850                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Notice how each row can have **different columns** - this flexibility is a core feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b4c39",
   "metadata": {},
   "source": [
    "## Python Simulation: Column-Family Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional, List\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "\n",
    "class Column:\n",
    "    \"\"\"Represents a single column with name, value, and timestamp.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, value: Any, timestamp: Optional[datetime] = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        self.timestamp = timestamp or datetime.now()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Column({self.name}={self.value}, ts={self.timestamp.isoformat()})\"\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"value\": self.value,\n",
    "            \"timestamp\": self.timestamp.isoformat()\n",
    "        }\n",
    "\n",
    "\n",
    "class ColumnFamily:\n",
    "    \"\"\"A logical grouping of columns stored together.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.columns: Dict[str, Column] = {}\n",
    "    \n",
    "    def put(self, column_name: str, value: Any, timestamp: Optional[datetime] = None):\n",
    "        \"\"\"Insert or update a column (last-write-wins based on timestamp).\"\"\"\n",
    "        new_col = Column(column_name, value, timestamp)\n",
    "        \n",
    "        if column_name in self.columns:\n",
    "            existing = self.columns[column_name]\n",
    "            # Last-write-wins conflict resolution\n",
    "            if new_col.timestamp >= existing.timestamp:\n",
    "                self.columns[column_name] = new_col\n",
    "        else:\n",
    "            self.columns[column_name] = new_col\n",
    "    \n",
    "    def get(self, column_name: str) -> Optional[Column]:\n",
    "        return self.columns.get(column_name)\n",
    "    \n",
    "    def delete(self, column_name: str):\n",
    "        \"\"\"Tombstone deletion (mark as deleted).\"\"\"\n",
    "        if column_name in self.columns:\n",
    "            del self.columns[column_name]\n",
    "    \n",
    "    def get_all_columns(self) -> List[Column]:\n",
    "        return list(self.columns.values())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        cols = \", \".join(f\"{c.name}:{c.value}\" for c in self.columns.values())\n",
    "        return f\"ColumnFamily({self.name}: [{cols}])\"\n",
    "\n",
    "\n",
    "print(\"Column-family classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82dfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Row:\n",
    "    \"\"\"A row containing multiple column families, identified by a row key.\"\"\"\n",
    "    \n",
    "    def __init__(self, row_key: str):\n",
    "        self.row_key = row_key\n",
    "        self.column_families: Dict[str, ColumnFamily] = {}\n",
    "    \n",
    "    def get_or_create_cf(self, cf_name: str) -> ColumnFamily:\n",
    "        if cf_name not in self.column_families:\n",
    "            self.column_families[cf_name] = ColumnFamily(cf_name)\n",
    "        return self.column_families[cf_name]\n",
    "    \n",
    "    def put(self, cf_name: str, column_name: str, value: Any):\n",
    "        cf = self.get_or_create_cf(cf_name)\n",
    "        cf.put(column_name, value)\n",
    "    \n",
    "    def get(self, cf_name: str, column_name: str) -> Optional[Any]:\n",
    "        cf = self.column_families.get(cf_name)\n",
    "        if cf:\n",
    "            col = cf.get(column_name)\n",
    "            return col.value if col else None\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Row({self.row_key}): {list(self.column_families.keys())}\"\n",
    "\n",
    "\n",
    "class ColumnFamilyStore:\n",
    "    \"\"\"\n",
    "    Simulates a column-family database with partitioning.\n",
    "    Inspired by Cassandra/HBase architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, num_partitions: int = 4):\n",
    "        self.name = name\n",
    "        self.num_partitions = num_partitions\n",
    "        # Each partition holds a subset of rows\n",
    "        self.partitions: List[Dict[str, Row]] = [{} for _ in range(num_partitions)]\n",
    "        self.write_count = 0\n",
    "        self.read_count = 0\n",
    "    \n",
    "    def _get_partition(self, row_key: str) -> int:\n",
    "        \"\"\"Consistent hash partitioning using MD5.\"\"\"\n",
    "        hash_val = int(hashlib.md5(row_key.encode()).hexdigest(), 16)\n",
    "        return hash_val % self.num_partitions\n",
    "    \n",
    "    def put(self, row_key: str, cf_name: str, column_name: str, value: Any):\n",
    "        \"\"\"Write a column value to the store.\"\"\"\n",
    "        partition_id = self._get_partition(row_key)\n",
    "        partition = self.partitions[partition_id]\n",
    "        \n",
    "        if row_key not in partition:\n",
    "            partition[row_key] = Row(row_key)\n",
    "        \n",
    "        partition[row_key].put(cf_name, column_name, value)\n",
    "        self.write_count += 1\n",
    "    \n",
    "    def get(self, row_key: str, cf_name: str, column_name: str) -> Optional[Any]:\n",
    "        \"\"\"Read a column value from the store.\"\"\"\n",
    "        partition_id = self._get_partition(row_key)\n",
    "        partition = self.partitions[partition_id]\n",
    "        self.read_count += 1\n",
    "        \n",
    "        row = partition.get(row_key)\n",
    "        if row:\n",
    "            return row.get(cf_name, column_name)\n",
    "        return None\n",
    "    \n",
    "    def get_row(self, row_key: str) -> Optional[Row]:\n",
    "        \"\"\"Retrieve entire row.\"\"\"\n",
    "        partition_id = self._get_partition(row_key)\n",
    "        return self.partitions[partition_id].get(row_key)\n",
    "    \n",
    "    def scan_partition(self, partition_id: int) -> List[Row]:\n",
    "        \"\"\"Scan all rows in a partition.\"\"\"\n",
    "        return list(self.partitions[partition_id].values())\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get storage statistics.\"\"\"\n",
    "        total_rows = sum(len(p) for p in self.partitions)\n",
    "        partition_sizes = [len(p) for p in self.partitions]\n",
    "        return {\n",
    "            \"total_rows\": total_rows,\n",
    "            \"partition_sizes\": partition_sizes,\n",
    "            \"writes\": self.write_count,\n",
    "            \"reads\": self.read_count\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"ColumnFamilyStore class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0781aae",
   "metadata": {},
   "source": [
    "## Example: IoT Sensor Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column-family store for IoT sensor data\n",
    "iot_store = ColumnFamilyStore(\"iot_sensors\", num_partitions=4)\n",
    "\n",
    "# Simulate sensor readings with composite row keys\n",
    "# Row key pattern: sensor_id#date (for time-series partitioning)\n",
    "sensor_data = [\n",
    "    (\"sensor_001#2026-02-02\", \"metrics\", \"temperature\", 72.5),\n",
    "    (\"sensor_001#2026-02-02\", \"metrics\", \"humidity\", 45.2),\n",
    "    (\"sensor_001#2026-02-02\", \"metrics\", \"pressure\", 1013.25),\n",
    "    (\"sensor_001#2026-02-02\", \"metadata\", \"location\", \"Building A\"),\n",
    "    (\"sensor_001#2026-02-02\", \"metadata\", \"type\", \"environmental\"),\n",
    "    \n",
    "    (\"sensor_002#2026-02-02\", \"metrics\", \"temperature\", 68.1),\n",
    "    (\"sensor_002#2026-02-02\", \"metrics\", \"co2_level\", 415),\n",
    "    \n",
    "    (\"sensor_003#2026-02-02\", \"metrics\", \"light_level\", 850),\n",
    "    (\"sensor_003#2026-02-02\", \"metrics\", \"motion_detected\", True),\n",
    "]\n",
    "\n",
    "# Insert data\n",
    "for row_key, cf_name, col_name, value in sensor_data:\n",
    "    iot_store.put(row_key, cf_name, col_name, value)\n",
    "\n",
    "print(\"Data inserted!\")\n",
    "print(f\"Store stats: {iot_store.get_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c32ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query specific values\n",
    "temp = iot_store.get(\"sensor_001#2026-02-02\", \"metrics\", \"temperature\")\n",
    "print(f\"Sensor 001 Temperature: {temp}Â°F\")\n",
    "\n",
    "location = iot_store.get(\"sensor_001#2026-02-02\", \"metadata\", \"location\")\n",
    "print(f\"Sensor 001 Location: {location}\")\n",
    "\n",
    "# Get entire row\n",
    "row = iot_store.get_row(\"sensor_001#2026-02-02\")\n",
    "print(f\"\\nFull row: {row}\")\n",
    "\n",
    "# Show column families and their contents\n",
    "if row:\n",
    "    for cf_name, cf in row.column_families.items():\n",
    "        print(f\"  {cf_name}: {cf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48142e35",
   "metadata": {},
   "source": [
    "## Write-Optimized Design (LSM Tree)\n",
    "\n",
    "Column-family stores like Cassandra use **Log-Structured Merge Trees (LSM)** for write optimization:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      WRITE PATH                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. Client Write                                                â”‚\n",
    "â”‚       â”‚                                                         â”‚\n",
    "â”‚       â–¼                                                         â”‚\n",
    "â”‚  2. Commit Log (WAL)  â”€â”€â–º Durability guarantee                  â”‚\n",
    "â”‚       â”‚                                                         â”‚\n",
    "â”‚       â–¼                                                         â”‚\n",
    "â”‚  3. MemTable (in-memory sorted structure)                       â”‚\n",
    "â”‚       â”‚                                                         â”‚\n",
    "â”‚       â–¼  (when full)                                            â”‚\n",
    "â”‚  4. Flush to SSTable (immutable, sorted on disk)                â”‚\n",
    "â”‚       â”‚                                                         â”‚\n",
    "â”‚       â–¼  (background)                                           â”‚\n",
    "â”‚  5. Compaction (merge SSTables, remove tombstones)              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Why This is Fast for Writes\n",
    "- **Sequential I/O**: All writes are sequential (append-only log)\n",
    "- **No random seeks**: Unlike B-trees that require random disk access\n",
    "- **Batch amortization**: Compaction happens in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemTable:\n",
    "    \"\"\"In-memory sorted structure (simplified Red-Black tree simulation).\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.data: Dict[str, Dict[str, Any]] = {}  # Sorted by key\n",
    "        self.max_size = max_size\n",
    "        self.size = 0\n",
    "    \n",
    "    def put(self, key: str, column: str, value: Any) -> bool:\n",
    "        \"\"\"Returns True if MemTable needs flushing.\"\"\"\n",
    "        if key not in self.data:\n",
    "            self.data[key] = {}\n",
    "        self.data[key][column] = value\n",
    "        self.size += 1\n",
    "        return self.size >= self.max_size\n",
    "    \n",
    "    def get(self, key: str, column: str) -> Optional[Any]:\n",
    "        return self.data.get(key, {}).get(column)\n",
    "    \n",
    "    def to_sstable(self) -> 'SSTable':\n",
    "        \"\"\"Flush to immutable SSTable.\"\"\"\n",
    "        sorted_data = dict(sorted(self.data.items()))\n",
    "        return SSTable(sorted_data)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.data = {}\n",
    "        self.size = 0\n",
    "\n",
    "\n",
    "class SSTable:\n",
    "    \"\"\"Sorted String Table - immutable, sorted on-disk structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: Dict[str, Dict[str, Any]]):\n",
    "        self.data = data\n",
    "        self.bloom_filter = self._build_bloom_filter()\n",
    "        self.index = self._build_sparse_index()\n",
    "        self.created_at = datetime.now()\n",
    "    \n",
    "    def _build_bloom_filter(self) -> set:\n",
    "        \"\"\"Simplified bloom filter (actual uses bit array with multiple hashes).\"\"\"\n",
    "        return set(self.data.keys())\n",
    "    \n",
    "    def _build_sparse_index(self) -> Dict[str, int]:\n",
    "        \"\"\"Sparse index for faster lookups.\"\"\"\n",
    "        keys = list(self.data.keys())\n",
    "        # Index every 10th key\n",
    "        return {keys[i]: i for i in range(0, len(keys), 10)} if keys else {}\n",
    "    \n",
    "    def might_contain(self, key: str) -> bool:\n",
    "        \"\"\"Bloom filter check - false positives possible, no false negatives.\"\"\"\n",
    "        return key in self.bloom_filter\n",
    "    \n",
    "    def get(self, key: str, column: str) -> Optional[Any]:\n",
    "        if not self.might_contain(key):\n",
    "            return None\n",
    "        return self.data.get(key, {}).get(column)\n",
    "\n",
    "\n",
    "print(\"LSM-Tree components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSMStore:\n",
    "    \"\"\"Complete LSM-based column-family store.\"\"\"\n",
    "    \n",
    "    def __init__(self, memtable_size: int = 10):\n",
    "        self.memtable = MemTable(memtable_size)\n",
    "        self.sstables: List[SSTable] = []  # Oldest to newest\n",
    "        self.commit_log: List[tuple] = []  # WAL simulation\n",
    "    \n",
    "    def put(self, key: str, column: str, value: Any):\n",
    "        # Step 1: Write to commit log (WAL)\n",
    "        self.commit_log.append((key, column, value, datetime.now()))\n",
    "        \n",
    "        # Step 2: Write to MemTable\n",
    "        needs_flush = self.memtable.put(key, column, value)\n",
    "        \n",
    "        # Step 3: Flush if MemTable is full\n",
    "        if needs_flush:\n",
    "            self._flush_memtable()\n",
    "    \n",
    "    def _flush_memtable(self):\n",
    "        \"\"\"Flush MemTable to SSTable.\"\"\"\n",
    "        if self.memtable.size > 0:\n",
    "            sstable = self.memtable.to_sstable()\n",
    "            self.sstables.append(sstable)\n",
    "            self.memtable.clear()\n",
    "            print(f\"Flushed MemTable to SSTable #{len(self.sstables)}\")\n",
    "    \n",
    "    def get(self, key: str, column: str) -> Optional[Any]:\n",
    "        # Check MemTable first (most recent)\n",
    "        value = self.memtable.get(key, column)\n",
    "        if value is not None:\n",
    "            return value\n",
    "        \n",
    "        # Check SSTables from newest to oldest\n",
    "        for sstable in reversed(self.sstables):\n",
    "            value = sstable.get(key, column)\n",
    "            if value is not None:\n",
    "                return value\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def compact(self):\n",
    "        \"\"\"Merge all SSTables into one (simplified compaction).\"\"\"\n",
    "        if len(self.sstables) < 2:\n",
    "            return\n",
    "        \n",
    "        merged_data = {}\n",
    "        for sstable in self.sstables:\n",
    "            for key, columns in sstable.data.items():\n",
    "                if key not in merged_data:\n",
    "                    merged_data[key] = {}\n",
    "                merged_data[key].update(columns)\n",
    "        \n",
    "        self.sstables = [SSTable(dict(sorted(merged_data.items())))]\n",
    "        print(f\"Compacted to 1 SSTable with {len(merged_data)} rows\")\n",
    "\n",
    "\n",
    "# Demonstrate LSM writes\n",
    "lsm = LSMStore(memtable_size=5)\n",
    "\n",
    "# Insert data that will trigger flushes\n",
    "for i in range(12):\n",
    "    lsm.put(f\"row_{i:03d}\", \"value\", f\"data_{i}\")\n",
    "\n",
    "print(f\"\\nTotal SSTables: {len(lsm.sstables)}\")\n",
    "print(f\"MemTable size: {lsm.memtable.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164757ac",
   "metadata": {},
   "source": [
    "## Partitioning Strategies\n",
    "\n",
    "Effective partitioning is crucial for scalability and performance.\n",
    "\n",
    "### 1. Hash Partitioning (Cassandra Default)\n",
    "- Partition key hashed to determine node\n",
    "- Even data distribution\n",
    "- No range queries across partitions\n",
    "\n",
    "### 2. Range Partitioning (HBase)\n",
    "- Data sorted by row key\n",
    "- Enables efficient range scans\n",
    "- Risk of hotspots\n",
    "\n",
    "### 3. Composite Partitioning\n",
    "- Combine partition key + clustering key\n",
    "- Group related data on same partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ad2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitioningDemo:\n",
    "    \"\"\"Demonstrate different partitioning strategies.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def hash_partition(key: str, num_nodes: int) -> int:\n",
    "        \"\"\"Murmur3-like hash partitioning (Cassandra style).\"\"\"\n",
    "        hash_val = int(hashlib.md5(key.encode()).hexdigest(), 16)\n",
    "        return hash_val % num_nodes\n",
    "    \n",
    "    @staticmethod\n",
    "    def range_partition(key: str, ranges: List[str]) -> int:\n",
    "        \"\"\"Range-based partitioning (HBase style).\"\"\"\n",
    "        for i, boundary in enumerate(ranges):\n",
    "            if key < boundary:\n",
    "                return i\n",
    "        return len(ranges)\n",
    "    \n",
    "    @staticmethod\n",
    "    def composite_key(partition_key: str, clustering_key: str) -> str:\n",
    "        \"\"\"Create composite key for time-series data.\"\"\"\n",
    "        return f\"{partition_key}#{clustering_key}\"\n",
    "\n",
    "\n",
    "# Demonstrate partitioning\n",
    "demo = PartitioningDemo()\n",
    "\n",
    "# Hash partitioning example\n",
    "print(\"Hash Partitioning (4 nodes):\")\n",
    "sensors = [\"sensor_001\", \"sensor_002\", \"sensor_003\", \"sensor_004\", \"sensor_005\"]\n",
    "for sensor in sensors:\n",
    "    node = demo.hash_partition(sensor, 4)\n",
    "    print(f\"  {sensor} â†’ Node {node}\")\n",
    "\n",
    "# Range partitioning example\n",
    "print(\"\\nRange Partitioning (alphabetical boundaries):\")\n",
    "ranges = [\"h\", \"p\", \"x\"]  # Boundaries: <h, h-p, p-x, >x\n",
    "keys = [\"apple\", \"mango\", \"zebra\", \"grape\"]\n",
    "for key in keys:\n",
    "    partition = demo.range_partition(key, ranges)\n",
    "    print(f\"  {key} â†’ Partition {partition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series partitioning strategy\n",
    "print(\"Time-Series Composite Key Strategy:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Pattern: sensor_id#date as partition key, timestamp as clustering key\n",
    "# This groups one day's data for a sensor on the same partition\n",
    "\n",
    "time_series_keys = [\n",
    "    (\"sensor_001\", \"2026-02-02\", \"10:00:00\"),\n",
    "    (\"sensor_001\", \"2026-02-02\", \"10:05:00\"),\n",
    "    (\"sensor_001\", \"2026-02-02\", \"10:10:00\"),\n",
    "    (\"sensor_001\", \"2026-02-03\", \"10:00:00\"),  # New partition\n",
    "    (\"sensor_002\", \"2026-02-02\", \"10:00:00\"),  # Different sensor\n",
    "]\n",
    "\n",
    "partition_map = defaultdict(list)\n",
    "\n",
    "for sensor, date, time in time_series_keys:\n",
    "    partition_key = f\"{sensor}#{date}\"\n",
    "    clustering_key = time\n",
    "    partition_id = demo.hash_partition(partition_key, 4)\n",
    "    partition_map[partition_key].append(clustering_key)\n",
    "    print(f\"  {partition_key}:{clustering_key} â†’ Node {partition_id}\")\n",
    "\n",
    "print(\"\\nData grouped by partition key:\")\n",
    "for pk, times in partition_map.items():\n",
    "    print(f\"  {pk}: {times}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6af22",
   "metadata": {},
   "source": [
    "## When to Use Wide-Column Stores\n",
    "\n",
    "### âœ… Ideal Use Cases\n",
    "\n",
    "| Use Case | Why Column-Family Works |\n",
    "|----------|-------------------------|\n",
    "| **Time-Series Data** | Sequential writes, range queries by time |\n",
    "| **IoT Sensor Data** | High write throughput, sparse columns |\n",
    "| **Event Logging** | Append-only, chronological access |\n",
    "| **User Activity Feeds** | Wide rows, sorted by timestamp |\n",
    "| **Metrics/Analytics** | Aggregation-friendly, columnar access |\n",
    "| **Messaging Systems** | Message queues with time-based partitioning |\n",
    "\n",
    "### âŒ Poor Fit Scenarios\n",
    "\n",
    "| Scenario | Why Not Column-Family |\n",
    "|----------|------------------------|\n",
    "| Complex JOINs | No native join support |\n",
    "| ACID transactions | Limited transaction scope |\n",
    "| Ad-hoc queries | Requires predefined access patterns |\n",
    "| Small datasets | Overhead not justified |\n",
    "| Frequent updates | Not optimized for in-place updates |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5598478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example: Time-series IoT data model\n",
    "\n",
    "class IoTTimeSeriesStore:\n",
    "    \"\"\"Production-like IoT data store with Cassandra-style modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.store = ColumnFamilyStore(\"iot_timeseries\", num_partitions=8)\n",
    "    \n",
    "    def _make_partition_key(self, device_id: str, date: str) -> str:\n",
    "        \"\"\"Partition by device + date for bounded partition size.\"\"\"\n",
    "        return f\"{device_id}#{date}\"\n",
    "    \n",
    "    def record_reading(self, device_id: str, timestamp: datetime, \n",
    "                       readings: Dict[str, Any]):\n",
    "        \"\"\"Record sensor readings.\"\"\"\n",
    "        date_str = timestamp.strftime(\"%Y-%m-%d\")\n",
    "        time_str = timestamp.strftime(\"%H:%M:%S.%f\")\n",
    "        row_key = self._make_partition_key(device_id, date_str)\n",
    "        \n",
    "        for metric, value in readings.items():\n",
    "            # Column name includes timestamp for clustering\n",
    "            col_name = f\"{time_str}:{metric}\"\n",
    "            self.store.put(row_key, \"readings\", col_name, value)\n",
    "    \n",
    "    def get_daily_readings(self, device_id: str, date: str) -> List[Dict]:\n",
    "        \"\"\"Get all readings for a device on a specific date.\"\"\"\n",
    "        row_key = self._make_partition_key(device_id, date)\n",
    "        row = self.store.get_row(row_key)\n",
    "        \n",
    "        if not row or \"readings\" not in row.column_families:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        cf = row.column_families[\"readings\"]\n",
    "        for col in sorted(cf.columns.values(), key=lambda c: c.name):\n",
    "            time_str, metric = col.name.rsplit(\":\", 1)\n",
    "            results.append({\n",
    "                \"time\": time_str,\n",
    "                \"metric\": metric,\n",
    "                \"value\": col.value\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "# Demo\n",
    "ts_store = IoTTimeSeriesStore()\n",
    "\n",
    "# Record sensor data\n",
    "from datetime import timedelta\n",
    "\n",
    "base_time = datetime(2026, 2, 2, 10, 0, 0)\n",
    "for i in range(5):\n",
    "    ts = base_time + timedelta(minutes=i*5)\n",
    "    ts_store.record_reading(\n",
    "        device_id=\"temp_sensor_001\",\n",
    "        timestamp=ts,\n",
    "        readings={\n",
    "            \"temperature\": 70 + i * 0.5,\n",
    "            \"humidity\": 45 + i * 0.2\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Query daily readings\n",
    "readings = ts_store.get_daily_readings(\"temp_sensor_001\", \"2026-02-02\")\n",
    "print(\"Daily readings for temp_sensor_001:\")\n",
    "for r in readings:\n",
    "    print(f\"  {r['time']} - {r['metric']}: {r['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbf21b",
   "metadata": {},
   "source": [
    "## Cassandra vs HBase Comparison\n",
    "\n",
    "| Feature | Cassandra | HBase |\n",
    "|---------|-----------|-------|\n",
    "| **Architecture** | Peer-to-peer, no master | Master-slave (HMaster) |\n",
    "| **Consistency** | Tunable (ONE, QUORUM, ALL) | Strong consistency |\n",
    "| **Query Language** | CQL (SQL-like) | Java API, Thrift |\n",
    "| **Partitioning** | Consistent hashing | Range-based |\n",
    "| **Ecosystem** | Standalone | Hadoop ecosystem |\n",
    "| **Write Speed** | Excellent | Very good |\n",
    "| **Range Scans** | Limited | Excellent |\n",
    "| **Operations** | Simpler | More complex |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd8887",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Key Takeaways\n",
    "\n",
    "### Data Model\n",
    "- **Wide-column** = rows with dynamic, sparse columns grouped into column families\n",
    "- **Row key** is the primary index; design keys for your access patterns\n",
    "- **Denormalization** is expected; duplicate data for read efficiency\n",
    "\n",
    "### Write Optimization\n",
    "- **LSM Trees** enable O(1) writes via sequential I/O\n",
    "- **MemTable â†’ SSTable â†’ Compaction** pipeline\n",
    "- Trade read complexity for write speed\n",
    "\n",
    "### Partitioning\n",
    "- **Hash partitioning** for even distribution (Cassandra)\n",
    "- **Range partitioning** for sequential scans (HBase)\n",
    "- **Composite keys** for time-series: `partition_key#clustering_key`\n",
    "\n",
    "### Best Practices\n",
    "1. **Model for queries**, not relationships\n",
    "2. **Bound partition sizes** (< 100MB recommended)\n",
    "3. **Use time-bucketing** for time-series data\n",
    "4. **Avoid hotspots** with proper key design\n",
    "5. **Accept eventual consistency** for scalability\n",
    "\n",
    "### When to Choose\n",
    "- âœ… High write throughput, time-series, IoT, logging\n",
    "- âŒ Complex transactions, ad-hoc queries, small datasets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
