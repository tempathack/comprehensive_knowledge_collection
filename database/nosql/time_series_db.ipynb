{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1a7409",
   "metadata": {},
   "source": [
    "# Time Series Databases: InfluxDB & TimescaleDB\n",
    "\n",
    "Time series databases (TSDBs) are specialized database systems optimized for storing, querying, and analyzing time-stamped data. Unlike traditional relational databases, TSDBs are designed to handle the unique characteristics and challenges of time series workloads.\n",
    "\n",
    "## What is Time Series Data?\n",
    "\n",
    "Time series data consists of sequences of data points indexed by time. Each data point typically includes:\n",
    "- **Timestamp**: When the measurement was taken\n",
    "- **Metrics/Values**: Numerical measurements (e.g., temperature, CPU usage, stock price)\n",
    "- **Tags/Labels**: Metadata for categorization (e.g., sensor_id, region, host)\n",
    "\n",
    "## Key Characteristics of Time Series Data\n",
    "\n",
    "| Characteristic | Description |\n",
    "|----------------|-------------|\n",
    "| **Time-ordered** | Data arrives in chronological sequence |\n",
    "| **Append-only** | New data is typically inserted, rarely updated |\n",
    "| **High volume** | Millions of data points per second |\n",
    "| **Recent data focus** | Most queries access recent data |\n",
    "| **Aggregation-heavy** | Common operations: averages, sums, percentiles over time windows |\n",
    "| **Downsampling** | Older data often stored at lower resolution |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8982fb8",
   "metadata": {},
   "source": [
    "## InfluxDB vs TimescaleDB\n",
    "\n",
    "### InfluxDB\n",
    "- Purpose-built time series database\n",
    "- Custom query language (Flux) and InfluxQL\n",
    "- Schema-less design\n",
    "- Built-in retention policies and continuous queries\n",
    "- Excellent for metrics and IoT data\n",
    "\n",
    "### TimescaleDB\n",
    "- PostgreSQL extension for time series\n",
    "- Full SQL support with time series optimizations\n",
    "- Hypertables for automatic partitioning\n",
    "- Compression and continuous aggregates\n",
    "- Best when you need relational + time series capabilities\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Time Series Database                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚  InfluxDB   â”‚    â”‚ TimescaleDB â”‚    â”‚   Others    â”‚      â”‚\n",
    "â”‚  â”‚             â”‚    â”‚             â”‚    â”‚ (QuestDB,   â”‚      â”‚\n",
    "â”‚  â”‚  - Flux     â”‚    â”‚  - SQL      â”‚    â”‚  Prometheus)â”‚      â”‚\n",
    "â”‚  â”‚  - Schema-  â”‚    â”‚  - Postgres â”‚    â”‚             â”‚      â”‚\n",
    "â”‚  â”‚    less     â”‚    â”‚    based    â”‚    â”‚             â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f326442",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff3f2e",
   "metadata": {},
   "source": [
    "## Generating Time Series Data\n",
    "\n",
    "Let's simulate realistic time series data representing different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bee9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_data(start_date, periods, freq='1min', num_sensors=3):\n",
    "    \"\"\"\n",
    "    Generate simulated IoT sensor data with realistic patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date for the time series\n",
    "    periods : int\n",
    "        Number of data points to generate\n",
    "    freq : str\n",
    "        Frequency of data points (e.g., '1min', '5min', '1H')\n",
    "    num_sensors : int\n",
    "        Number of sensors to simulate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with timestamp, sensor_id, temperature, humidity, pressure\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    \n",
    "    data = []\n",
    "    for sensor_id in range(1, num_sensors + 1):\n",
    "        # Base values with sensor-specific offsets\n",
    "        base_temp = 20 + sensor_id * 2\n",
    "        base_humidity = 50 + sensor_id * 5\n",
    "        base_pressure = 1013 + sensor_id * 0.5\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            # Add daily seasonality (temperature higher during day)\n",
    "            hour = date.hour\n",
    "            daily_factor = np.sin(2 * np.pi * (hour - 6) / 24) * 5\n",
    "            \n",
    "            # Add noise and occasional spikes\n",
    "            noise = np.random.normal(0, 0.5)\n",
    "            spike = 10 if np.random.random() < 0.01 else 0\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': date,\n",
    "                'sensor_id': f'sensor_{sensor_id}',\n",
    "                'temperature': base_temp + daily_factor + noise + spike,\n",
    "                'humidity': base_humidity + noise * 2 + np.random.normal(0, 3),\n",
    "                'pressure': base_pressure + np.random.normal(0, 0.2)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate 24 hours of sensor data at 1-minute intervals\n",
    "sensor_df = generate_sensor_data('2024-01-15', periods=1440, freq='1min', num_sensors=3)\n",
    "print(f\"Generated {len(sensor_df):,} data points\")\n",
    "print(f\"Date range: {sensor_df['timestamp'].min()} to {sensor_df['timestamp'].max()}\")\n",
    "sensor_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fcf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_financial_data(start_date, periods, freq='1min'):\n",
    "    \"\"\"\n",
    "    Generate simulated stock price data with OHLCV format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date for the time series\n",
    "    periods : int\n",
    "        Number of data points to generate\n",
    "    freq : str\n",
    "        Frequency of data points\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with OHLCV data\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    \n",
    "    # Generate random walk for price\n",
    "    returns = np.random.normal(0.0001, 0.002, periods)\n",
    "    price = 100 * np.cumprod(1 + returns)\n",
    "    \n",
    "    # Generate OHLC from the price\n",
    "    data = {\n",
    "        'timestamp': dates,\n",
    "        'symbol': 'TECH',\n",
    "        'open': price,\n",
    "        'high': price * (1 + np.abs(np.random.normal(0, 0.001, periods))),\n",
    "        'low': price * (1 - np.abs(np.random.normal(0, 0.001, periods))),\n",
    "        'close': price * (1 + np.random.normal(0, 0.0005, periods)),\n",
    "        'volume': np.random.randint(1000, 100000, periods)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate financial tick data\n",
    "financial_df = generate_financial_data('2024-01-15 09:30:00', periods=390, freq='1min')\n",
    "print(f\"Generated {len(financial_df):,} OHLCV records\")\n",
    "financial_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9357ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_server_metrics(start_date, periods, freq='10s', num_hosts=5):\n",
    "    \"\"\"\n",
    "    Generate simulated server monitoring metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date for the time series\n",
    "    periods : int\n",
    "        Number of data points to generate\n",
    "    freq : str\n",
    "        Frequency of data points\n",
    "    num_hosts : int\n",
    "        Number of hosts to simulate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with server metrics\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    \n",
    "    data = []\n",
    "    for host_id in range(1, num_hosts + 1):\n",
    "        # Simulate different baseline loads for different hosts\n",
    "        base_cpu = 20 + host_id * 10\n",
    "        base_memory = 40 + host_id * 8\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            # Simulate load spikes during business hours\n",
    "            hour = date.hour\n",
    "            business_hours_factor = 1.5 if 9 <= hour <= 17 else 1.0\n",
    "            \n",
    "            # Random spikes to simulate incidents\n",
    "            incident = np.random.random() < 0.005\n",
    "            incident_factor = 2.0 if incident else 1.0\n",
    "            \n",
    "            cpu = min(100, base_cpu * business_hours_factor * incident_factor + np.random.normal(0, 5))\n",
    "            memory = min(100, base_memory + np.random.normal(0, 3))\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': date,\n",
    "                'host': f'web-server-{host_id:02d}',\n",
    "                'region': ['us-east', 'us-west', 'eu-west', 'ap-south', 'ap-east'][host_id - 1],\n",
    "                'cpu_percent': max(0, cpu),\n",
    "                'memory_percent': max(0, memory),\n",
    "                'disk_io_mbps': max(0, np.random.exponential(50)),\n",
    "                'network_mbps': max(0, np.random.exponential(100)),\n",
    "                'request_count': max(0, int(np.random.poisson(100 * business_hours_factor)))\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate 1 hour of server metrics at 10-second intervals\n",
    "server_df = generate_server_metrics('2024-01-15 10:00:00', periods=360, freq='10s', num_hosts=5)\n",
    "print(f\"Generated {len(server_df):,} server metric records\")\n",
    "server_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932f4bf",
   "metadata": {},
   "source": [
    "## Visualizing Time Series Data with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize IoT sensor data\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    subplot_titles=('Temperature (Â°C)', 'Humidity (%)', 'Pressure (hPa)'),\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "for i, sensor in enumerate(sensor_df['sensor_id'].unique()):\n",
    "    sensor_data = sensor_df[sensor_df['sensor_id'] == sensor]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sensor_data['timestamp'], y=sensor_data['temperature'],\n",
    "                   name=sensor, line=dict(color=colors[i]), legendgroup=sensor),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sensor_data['timestamp'], y=sensor_data['humidity'],\n",
    "                   name=sensor, line=dict(color=colors[i]), legendgroup=sensor,\n",
    "                   showlegend=False),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sensor_data['timestamp'], y=sensor_data['pressure'],\n",
    "                   name=sensor, line=dict(color=colors[i]), legendgroup=sensor,\n",
    "                   showlegend=False),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='IoT Sensor Data - 24 Hour Overview',\n",
    "    height=700,\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2582d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize financial data with candlestick chart\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    row_heights=[0.7, 0.3],\n",
    "    subplot_titles=('Price (Candlestick)', 'Volume'),\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# Candlestick chart\n",
    "fig.add_trace(\n",
    "    go.Candlestick(\n",
    "        x=financial_df['timestamp'],\n",
    "        open=financial_df['open'],\n",
    "        high=financial_df['high'],\n",
    "        low=financial_df['low'],\n",
    "        close=financial_df['close'],\n",
    "        name='TECH'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Volume bar chart\n",
    "colors_volume = ['red' if close < open else 'green' \n",
    "                 for close, open in zip(financial_df['close'], financial_df['open'])]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=financial_df['timestamp'],\n",
    "        y=financial_df['volume'],\n",
    "        marker_color=colors_volume,\n",
    "        name='Volume'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Financial Time Series - OHLCV Data',\n",
    "    height=600,\n",
    "    template='plotly_white',\n",
    "    xaxis_rangeslider_visible=False,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdcafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize server metrics with heatmap for CPU across hosts\n",
    "server_pivot = server_df.pivot_table(\n",
    "    index='host', \n",
    "    columns='timestamp', \n",
    "    values='cpu_percent'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=server_pivot.values,\n",
    "    x=server_pivot.columns,\n",
    "    y=server_pivot.index,\n",
    "    colorscale='RdYlGn_r',\n",
    "    colorbar=dict(title='CPU %')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Server CPU Utilization Heatmap',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Host',\n",
    "    height=400,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2232e68",
   "metadata": {},
   "source": [
    "## Retention Policies & Data Lifecycle Management\n",
    "\n",
    "Retention policies define how long data is stored before automatic deletion. This is crucial for:\n",
    "- **Storage cost management**: Old high-resolution data consumes space\n",
    "- **Compliance**: Some data must be retained for specific periods\n",
    "- **Performance**: Smaller datasets query faster\n",
    "\n",
    "### InfluxDB Retention Policies\n",
    "\n",
    "```sql\n",
    "-- Create a retention policy in InfluxDB\n",
    "CREATE RETENTION POLICY \"30_days\" ON \"metrics_db\" \n",
    "  DURATION 30d \n",
    "  REPLICATION 1 \n",
    "  DEFAULT;\n",
    "\n",
    "-- Create policy for long-term aggregated data\n",
    "CREATE RETENTION POLICY \"1_year\" ON \"metrics_db\" \n",
    "  DURATION 365d \n",
    "  REPLICATION 1;\n",
    "```\n",
    "\n",
    "### TimescaleDB Data Retention\n",
    "\n",
    "```sql\n",
    "-- Add automatic data retention policy\n",
    "SELECT add_retention_policy('sensor_data', INTERVAL '30 days');\n",
    "\n",
    "-- Drop chunks older than 7 days manually\n",
    "SELECT drop_chunks('sensor_data', older_than => INTERVAL '7 days');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_retention_policy(df, timestamp_col, retention_days):\n",
    "    \"\"\"\n",
    "    Simulate applying a retention policy to time series data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with time series data\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column\n",
    "    retention_days : int\n",
    "        Number of days to retain data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with old data removed\n",
    "    \"\"\"\n",
    "    cutoff_date = df[timestamp_col].max() - timedelta(days=retention_days)\n",
    "    retained_df = df[df[timestamp_col] > cutoff_date].copy()\n",
    "    \n",
    "    removed_count = len(df) - len(retained_df)\n",
    "    print(f\"Retention policy: {retention_days} days\")\n",
    "    print(f\"Cutoff date: {cutoff_date}\")\n",
    "    print(f\"Records removed: {removed_count:,}\")\n",
    "    print(f\"Records retained: {len(retained_df):,}\")\n",
    "    \n",
    "    return retained_df\n",
    "\n",
    "# Example: Apply 12-hour retention (simulated)\n",
    "retained_sensor_df = apply_retention_policy(\n",
    "    sensor_df, 'timestamp', retention_days=0.5  # 12 hours\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745586e",
   "metadata": {},
   "source": [
    "## Downsampling & Aggregation\n",
    "\n",
    "Downsampling reduces data resolution by aggregating fine-grained data into coarser time buckets. This is essential for:\n",
    "- **Long-term storage**: Store 1-minute data for 7 days, hourly data for 1 year\n",
    "- **Query performance**: Aggregated data is faster to query\n",
    "- **Visualization**: Minute-level data isn't useful for yearly trends\n",
    "\n",
    "### Common Aggregation Functions\n",
    "- `mean()`: Average value in time window\n",
    "- `min()`/`max()`: Extremes in time window\n",
    "- `sum()`: Total in time window\n",
    "- `count()`: Number of data points\n",
    "- `first()`/`last()`: First/last value in window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_time_series(df, timestamp_col, group_cols, value_cols, freq, agg_funcs=None):\n",
    "    \"\"\"\n",
    "    Downsample time series data to a lower resolution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with time series data\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column\n",
    "    group_cols : list\n",
    "        Columns to group by (e.g., sensor_id, host)\n",
    "    value_cols : list\n",
    "        Columns to aggregate\n",
    "    freq : str\n",
    "        Target frequency (e.g., '5min', '1H', '1D')\n",
    "    agg_funcs : dict\n",
    "        Aggregation functions per column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Downsampled DataFrame\n",
    "    \"\"\"\n",
    "    if agg_funcs is None:\n",
    "        agg_funcs = {col: ['mean', 'min', 'max'] for col in value_cols}\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy['time_bucket'] = df_copy[timestamp_col].dt.floor(freq)\n",
    "    \n",
    "    grouped = df_copy.groupby(['time_bucket'] + group_cols)\n",
    "    \n",
    "    result = grouped[value_cols].agg(agg_funcs)\n",
    "    result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Downsample sensor data from 1-minute to 15-minute intervals\n",
    "downsampled_sensor = downsample_time_series(\n",
    "    sensor_df,\n",
    "    timestamp_col='timestamp',\n",
    "    group_cols=['sensor_id'],\n",
    "    value_cols=['temperature', 'humidity', 'pressure'],\n",
    "    freq='15min'\n",
    ")\n",
    "\n",
    "print(f\"Original records: {len(sensor_df):,}\")\n",
    "print(f\"Downsampled records: {len(downsampled_sensor):,}\")\n",
    "print(f\"Compression ratio: {len(sensor_df) / len(downsampled_sensor):.1f}x\")\n",
    "downsampled_sensor.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da51500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original vs downsampled data\n",
    "sensor1_original = sensor_df[sensor_df['sensor_id'] == 'sensor_1']\n",
    "sensor1_downsampled = downsampled_sensor[downsampled_sensor['sensor_id'] == 'sensor_1']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sensor1_original['timestamp'],\n",
    "    y=sensor1_original['temperature'],\n",
    "    name='Original (1-min)',\n",
    "    line=dict(color='lightblue', width=1),\n",
    "    opacity=0.7\n",
    "))\n",
    "\n",
    "# Downsampled mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sensor1_downsampled['time_bucket'],\n",
    "    y=sensor1_downsampled['temperature_mean'],\n",
    "    name='Downsampled Mean (15-min)',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "# Min/Max range\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=pd.concat([sensor1_downsampled['time_bucket'], sensor1_downsampled['time_bucket'][::-1]]),\n",
    "    y=pd.concat([sensor1_downsampled['temperature_max'], sensor1_downsampled['temperature_min'][::-1]]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(0,100,80,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    name='Min-Max Range'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Original vs Downsampled Temperature Data (Sensor 1)',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Temperature (Â°C)',\n",
    "    template='plotly_white',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114135a",
   "metadata": {},
   "source": [
    "## Compression Strategies\n",
    "\n",
    "Time series databases use specialized compression algorithms optimized for time-ordered data:\n",
    "\n",
    "### Timestamp Compression\n",
    "- **Delta encoding**: Store differences between consecutive timestamps\n",
    "- **Delta-of-delta**: Store differences of differences (excellent for regular intervals)\n",
    "\n",
    "### Value Compression\n",
    "- **Gorilla compression**: XOR-based compression for floating-point values\n",
    "- **Run-length encoding (RLE)**: Compress repeated values\n",
    "- **Dictionary encoding**: Replace repeated strings with integer codes\n",
    "\n",
    "### TimescaleDB Compression\n",
    "\n",
    "```sql\n",
    "-- Enable compression on a hypertable\n",
    "ALTER TABLE sensor_data SET (\n",
    "    timescaledb.compress,\n",
    "    timescaledb.compress_segmentby = 'sensor_id',\n",
    "    timescaledb.compress_orderby = 'timestamp DESC'\n",
    ");\n",
    "\n",
    "-- Add automatic compression policy\n",
    "SELECT add_compression_policy('sensor_data', INTERVAL '7 days');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compression_potential(df, timestamp_col, value_cols):\n",
    "    \"\"\"\n",
    "    Analyze the compression potential of time series data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column\n",
    "    value_cols : list\n",
    "        Numeric columns to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Compression analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Timestamp analysis - delta encoding potential\n",
    "    timestamps = pd.to_datetime(df[timestamp_col]).astype(int) // 10**9  # Convert to seconds\n",
    "    deltas = np.diff(timestamps)\n",
    "    unique_deltas = len(np.unique(deltas))\n",
    "    \n",
    "    results['timestamp_analysis'] = {\n",
    "        'total_timestamps': len(timestamps),\n",
    "        'unique_deltas': unique_deltas,\n",
    "        'delta_encoding_ratio': len(timestamps) / max(unique_deltas, 1),\n",
    "        'most_common_delta_seconds': int(np.median(deltas)) if len(deltas) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Value analysis\n",
    "    results['value_analysis'] = {}\n",
    "    for col in value_cols:\n",
    "        values = df[col].values\n",
    "        \n",
    "        # Calculate value deltas\n",
    "        value_deltas = np.diff(values)\n",
    "        \n",
    "        # Estimate Gorilla compression potential (smaller deltas = better compression)\n",
    "        mean_delta = np.mean(np.abs(value_deltas))\n",
    "        std_values = np.std(values)\n",
    "        \n",
    "        results['value_analysis'][col] = {\n",
    "            'mean_value': float(np.mean(values)),\n",
    "            'std_value': float(std_values),\n",
    "            'mean_delta': float(mean_delta),\n",
    "            'delta_std_ratio': float(mean_delta / std_values) if std_values > 0 else 0,\n",
    "            'unique_values': len(np.unique(values.round(2))),\n",
    "            'total_values': len(values)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze compression potential for sensor data\n",
    "compression_analysis = analyze_compression_potential(\n",
    "    sensor_df[sensor_df['sensor_id'] == 'sensor_1'],\n",
    "    'timestamp',\n",
    "    ['temperature', 'humidity', 'pressure']\n",
    ")\n",
    "\n",
    "print(\"=== Compression Analysis ===\")\n",
    "print(\"\\nTimestamp Analysis:\")\n",
    "for key, value in compression_analysis['timestamp_analysis'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nValue Column Analysis:\")\n",
    "for col, metrics in compression_analysis['value_analysis'].items():\n",
    "    print(f\"\\n  {col}:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"    {key}: {value:.4f}\" if isinstance(value, float) else f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e401801",
   "metadata": {},
   "source": [
    "## Continuous Aggregates (Materialized Views)\n",
    "\n",
    "Continuous aggregates automatically maintain pre-computed aggregations that update incrementally as new data arrives.\n",
    "\n",
    "### TimescaleDB Continuous Aggregates\n",
    "\n",
    "```sql\n",
    "-- Create a continuous aggregate for hourly averages\n",
    "CREATE MATERIALIZED VIEW sensor_hourly\n",
    "WITH (timescaledb.continuous) AS\n",
    "SELECT \n",
    "    time_bucket('1 hour', timestamp) AS bucket,\n",
    "    sensor_id,\n",
    "    AVG(temperature) AS avg_temp,\n",
    "    MIN(temperature) AS min_temp,\n",
    "    MAX(temperature) AS max_temp,\n",
    "    COUNT(*) AS sample_count\n",
    "FROM sensor_data\n",
    "GROUP BY bucket, sensor_id;\n",
    "\n",
    "-- Add refresh policy\n",
    "SELECT add_continuous_aggregate_policy('sensor_hourly',\n",
    "    start_offset => INTERVAL '3 hours',\n",
    "    end_offset => INTERVAL '1 hour',\n",
    "    schedule_interval => INTERVAL '1 hour');\n",
    "```\n",
    "\n",
    "### InfluxDB Continuous Queries\n",
    "\n",
    "```sql\n",
    "-- Create a continuous query for downsampling\n",
    "CREATE CONTINUOUS QUERY \"cq_hourly_temp\" ON \"metrics_db\"\n",
    "BEGIN\n",
    "    SELECT mean(\"temperature\") AS \"avg_temp\",\n",
    "           min(\"temperature\") AS \"min_temp\",\n",
    "           max(\"temperature\") AS \"max_temp\"\n",
    "    INTO \"metrics_db\".\"1_year\".\"hourly_temperature\"\n",
    "    FROM \"sensor_data\"\n",
    "    GROUP BY time(1h), \"sensor_id\"\n",
    "END\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_aggregate(df, timestamp_col, group_cols, value_cols, freq):\n",
    "    \"\"\"\n",
    "    Simulate creating a continuous aggregate (materialized view).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    timestamp_col : str\n",
    "        Timestamp column name\n",
    "    group_cols : list\n",
    "        Columns to group by\n",
    "    value_cols : list\n",
    "        Columns to aggregate\n",
    "    freq : str\n",
    "        Time bucket frequency\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Aggregated DataFrame\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy['time_bucket'] = df_copy[timestamp_col].dt.floor(freq)\n",
    "    \n",
    "    agg_dict = {}\n",
    "    for col in value_cols:\n",
    "        agg_dict[col] = ['mean', 'min', 'max', 'std', 'count']\n",
    "    \n",
    "    result = df_copy.groupby(['time_bucket'] + group_cols).agg(agg_dict)\n",
    "    result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create hourly continuous aggregate\n",
    "hourly_aggregate = create_continuous_aggregate(\n",
    "    sensor_df,\n",
    "    'timestamp',\n",
    "    ['sensor_id'],\n",
    "    ['temperature', 'humidity'],\n",
    "    '1H'\n",
    ")\n",
    "\n",
    "print(\"Hourly Continuous Aggregate:\")\n",
    "hourly_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620f68b",
   "metadata": {},
   "source": [
    "## Use Cases for Time Series Databases\n",
    "\n",
    "### 1. Infrastructure Monitoring & Observability\n",
    "- Server metrics (CPU, memory, disk, network)\n",
    "- Application performance metrics (latency, throughput, error rates)\n",
    "- Log aggregation and analysis\n",
    "- Alerting on anomalies\n",
    "\n",
    "### 2. Internet of Things (IoT)\n",
    "- Sensor data collection (temperature, humidity, motion)\n",
    "- Smart home/building automation\n",
    "- Industrial equipment monitoring\n",
    "- Predictive maintenance\n",
    "\n",
    "### 3. Financial Data\n",
    "- Stock prices and trading volumes\n",
    "- Cryptocurrency markets\n",
    "- Algorithmic trading signals\n",
    "- Risk metrics and VaR calculations\n",
    "\n",
    "### 4. Business Analytics\n",
    "- Website traffic and user behavior\n",
    "- Sales and revenue metrics\n",
    "- Customer engagement tracking\n",
    "- Real-time dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dashboard showing all use cases\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'IoT: Sensor Temperature Over Time',\n",
    "        'Monitoring: Server CPU by Host',\n",
    "        'Financial: Stock Price Movement',\n",
    "        'Analytics: Request Distribution'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"histogram\"}]\n",
    "    ],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# IoT: Temperature trend\n",
    "for i, sensor in enumerate(sensor_df['sensor_id'].unique()):\n",
    "    sensor_data = sensor_df[sensor_df['sensor_id'] == sensor]\n",
    "    # Sample to reduce points\n",
    "    sampled = sensor_data.iloc[::10]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sampled['timestamp'], y=sampled['temperature'],\n",
    "                   name=sensor, mode='lines'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Monitoring: Average CPU by host\n",
    "cpu_by_host = server_df.groupby('host')['cpu_percent'].mean().sort_values(ascending=True)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=cpu_by_host.values, y=cpu_by_host.index, orientation='h',\n",
    "           marker_color=px.colors.sequential.Viridis,\n",
    "           showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Financial: Close price\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=financial_df['timestamp'], y=financial_df['close'],\n",
    "               mode='lines', name='TECH', line=dict(color='green'),\n",
    "               showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Analytics: Request count distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=server_df['request_count'], nbinsx=30,\n",
    "                 marker_color='steelblue', showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Time Series Database Use Cases Dashboard',\n",
    "    height=700,\n",
    "    template='plotly_white',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Time', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Avg CPU %', row=1, col=2)\n",
    "fig.update_xaxes(title_text='Time', row=2, col=1)\n",
    "fig.update_xaxes(title_text='Requests per Interval', row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text='Temperature (Â°C)', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Host', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Price ($)', row=2, col=1)\n",
    "fig.update_yaxes(title_text='Frequency', row=2, col=2)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5cb9f",
   "metadata": {},
   "source": [
    "## Query Patterns for Time Series Databases\n",
    "\n",
    "### Common Query Types\n",
    "\n",
    "| Query Type | Description | Example |\n",
    "|------------|-------------|----------|\n",
    "| **Point query** | Get value at specific time | `WHERE time = '2024-01-15T10:00:00Z'` |\n",
    "| **Range query** | Get values in time range | `WHERE time >= '2024-01-15' AND time < '2024-01-16'` |\n",
    "| **Aggregation** | Compute statistics over time | `SELECT AVG(temp) GROUP BY time(1h)` |\n",
    "| **Last value** | Get most recent value | `SELECT LAST(temp) FROM sensors` |\n",
    "| **Top-N** | Get top values by metric | `ORDER BY cpu_percent DESC LIMIT 10` |\n",
    "| **Gap filling** | Fill missing time periods | `time_bucket_gapfill('1 hour', time)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate common time series query patterns with pandas\n",
    "\n",
    "print(\"=== Common Time Series Query Patterns ===\")\n",
    "\n",
    "# 1. Point Query - Get value at specific time\n",
    "specific_time = pd.Timestamp('2024-01-15 10:00:00')\n",
    "point_query = sensor_df[\n",
    "    (sensor_df['timestamp'] == specific_time) & \n",
    "    (sensor_df['sensor_id'] == 'sensor_1')\n",
    "]\n",
    "print(f\"\\n1. Point Query (at {specific_time}):\")\n",
    "print(point_query)\n",
    "\n",
    "# 2. Range Query - Get values in time range\n",
    "start_time = pd.Timestamp('2024-01-15 09:00:00')\n",
    "end_time = pd.Timestamp('2024-01-15 10:00:00')\n",
    "range_query = sensor_df[\n",
    "    (sensor_df['timestamp'] >= start_time) & \n",
    "    (sensor_df['timestamp'] < end_time) &\n",
    "    (sensor_df['sensor_id'] == 'sensor_1')\n",
    "]\n",
    "print(f\"\\n2. Range Query ({start_time} to {end_time}):\")\n",
    "print(f\"   Records found: {len(range_query)}\")\n",
    "print(f\"   Avg temperature: {range_query['temperature'].mean():.2f}Â°C\")\n",
    "\n",
    "# 3. Aggregation Query - Hourly averages\n",
    "hourly_avg = sensor_df.set_index('timestamp').groupby(\n",
    "    ['sensor_id', pd.Grouper(freq='1H')]\n",
    ")['temperature'].mean().reset_index()\n",
    "print(f\"\\n3. Aggregation Query (Hourly Averages):\")\n",
    "print(hourly_avg.head())\n",
    "\n",
    "# 4. Last Value Query\n",
    "last_values = sensor_df.groupby('sensor_id').apply(\n",
    "    lambda x: x.nlargest(1, 'timestamp')\n",
    ").reset_index(drop=True)[['sensor_id', 'timestamp', 'temperature']]\n",
    "print(f\"\\n4. Last Value Query (Most Recent per Sensor):\")\n",
    "print(last_values)\n",
    "\n",
    "# 5. Top-N Query - Highest CPU hosts\n",
    "top_cpu = server_df.nlargest(5, 'cpu_percent')[['timestamp', 'host', 'cpu_percent']]\n",
    "print(f\"\\n5. Top-N Query (Top 5 CPU Readings):\")\n",
    "print(top_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap Filling - Important for time series analysis\n",
    "def gap_fill_time_series(df, timestamp_col, freq, value_col, method='ffill'):\n",
    "    \"\"\"\n",
    "    Fill gaps in time series data with missing timestamps.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    timestamp_col : str\n",
    "        Timestamp column name\n",
    "    freq : str\n",
    "        Expected frequency\n",
    "    value_col : str\n",
    "        Value column to interpolate\n",
    "    method : str\n",
    "        Fill method: 'ffill', 'bfill', 'interpolate', 'zero'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with gaps filled\n",
    "    \"\"\"\n",
    "    # Create complete time range\n",
    "    full_range = pd.date_range(\n",
    "        start=df[timestamp_col].min(),\n",
    "        end=df[timestamp_col].max(),\n",
    "        freq=freq\n",
    "    )\n",
    "    \n",
    "    # Reindex to full range\n",
    "    df_indexed = df.set_index(timestamp_col)\n",
    "    df_reindexed = df_indexed.reindex(full_range)\n",
    "    \n",
    "    # Fill missing values\n",
    "    if method == 'ffill':\n",
    "        df_filled = df_reindexed.ffill()\n",
    "    elif method == 'bfill':\n",
    "        df_filled = df_reindexed.bfill()\n",
    "    elif method == 'interpolate':\n",
    "        df_filled = df_reindexed.interpolate(method='time')\n",
    "    elif method == 'zero':\n",
    "        df_filled = df_reindexed.fillna(0)\n",
    "    \n",
    "    return df_filled.reset_index().rename(columns={'index': timestamp_col})\n",
    "\n",
    "# Simulate data with gaps by removing some records\n",
    "sensor1 = sensor_df[sensor_df['sensor_id'] == 'sensor_1'].copy()\n",
    "# Remove 10% of records randomly\n",
    "mask = np.random.random(len(sensor1)) > 0.1\n",
    "sensor1_with_gaps = sensor1[mask]\n",
    "\n",
    "print(f\"Original records: {len(sensor1)}\")\n",
    "print(f\"Records with gaps: {len(sensor1_with_gaps)}\")\n",
    "print(f\"Missing records: {len(sensor1) - len(sensor1_with_gaps)}\")\n",
    "\n",
    "# Fill gaps using interpolation\n",
    "sensor1_filled = gap_fill_time_series(\n",
    "    sensor1_with_gaps[['timestamp', 'temperature']],\n",
    "    'timestamp', '1min', 'temperature', 'interpolate'\n",
    ")\n",
    "\n",
    "print(f\"Records after gap fill: {len(sensor1_filled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced0c3d",
   "metadata": {},
   "source": [
    "## Schema Design Best Practices\n",
    "\n",
    "### InfluxDB Line Protocol\n",
    "```\n",
    "measurement,tag_key=tag_value field_key=field_value timestamp\n",
    "\n",
    "# Example:\n",
    "temperature,sensor_id=sensor_1,location=room_a value=22.5 1705312800000000000\n",
    "```\n",
    "\n",
    "### TimescaleDB Hypertables\n",
    "```sql\n",
    "-- Create regular table\n",
    "CREATE TABLE sensor_data (\n",
    "    time        TIMESTAMPTZ NOT NULL,\n",
    "    sensor_id   TEXT NOT NULL,\n",
    "    temperature DOUBLE PRECISION,\n",
    "    humidity    DOUBLE PRECISION,\n",
    "    pressure    DOUBLE PRECISION\n",
    ");\n",
    "\n",
    "-- Convert to hypertable (auto-partitioned by time)\n",
    "SELECT create_hypertable('sensor_data', 'time');\n",
    "\n",
    "-- Add space partitioning for high-cardinality data\n",
    "SELECT create_hypertable('sensor_data', 'time',\n",
    "    partitioning_column => 'sensor_id',\n",
    "    number_partitions => 4);\n",
    "```\n",
    "\n",
    "### Design Guidelines\n",
    "\n",
    "1. **Use appropriate tags/dimensions**: Low cardinality fields (sensor_id, region)\n",
    "2. **Choose fields wisely**: High cardinality/numeric values as fields/measures\n",
    "3. **Timestamp precision**: Match to your actual data resolution\n",
    "4. **Chunk intervals**: Align with query patterns (1 hour, 1 day, 1 week)\n",
    "5. **Index strategy**: Time + high-selectivity dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c0a6f",
   "metadata": {},
   "source": [
    "## Performance Optimization Tips\n",
    "\n",
    "### Write Optimization\n",
    "- **Batch writes**: Group multiple data points into single write operations\n",
    "- **Async writes**: Use asynchronous I/O for high-throughput ingestion\n",
    "- **Buffer management**: Configure appropriate write buffers\n",
    "\n",
    "### Query Optimization\n",
    "- **Time-bound queries**: Always include time range predicates\n",
    "- **Use continuous aggregates**: Pre-compute common aggregations\n",
    "- **Limit result sets**: Use LIMIT and pagination\n",
    "- **Index high-selectivity columns**: Reduce scan scope\n",
    "\n",
    "### Storage Optimization\n",
    "- **Enable compression**: Typically 10-20x storage reduction\n",
    "- **Set retention policies**: Automatically remove old data\n",
    "- **Use tiered storage**: Hot/warm/cold data tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate batch write simulation\n",
    "import time\n",
    "\n",
    "def simulate_batch_writes(data, batch_size):\n",
    "    \"\"\"\n",
    "    Simulate batched writes to a time series database.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Data to write\n",
    "    batch_size : int\n",
    "        Number of records per batch\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Write statistics\n",
    "    \"\"\"\n",
    "    total_records = len(data)\n",
    "    num_batches = (total_records + batch_size - 1) // batch_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data.iloc[i:i+batch_size]\n",
    "        # Simulate write latency\n",
    "        time.sleep(0.001)  # 1ms per batch\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'total_records': total_records,\n",
    "        'batch_size': batch_size,\n",
    "        'num_batches': num_batches,\n",
    "        'elapsed_seconds': elapsed,\n",
    "        'records_per_second': total_records / elapsed\n",
    "    }\n",
    "\n",
    "# Compare different batch sizes\n",
    "batch_sizes = [100, 500, 1000, 5000]\n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    stats = simulate_batch_writes(sensor_df, batch_size)\n",
    "    results.append(stats)\n",
    "    print(f\"Batch size {batch_size:,}: {stats['num_batches']} batches, \"\n",
    "          f\"{stats['records_per_second']:,.0f} records/sec\")\n",
    "\n",
    "# Visualize batch performance\n",
    "results_df = pd.DataFrame(results)\n",
    "fig = px.bar(\n",
    "    results_df,\n",
    "    x='batch_size',\n",
    "    y='records_per_second',\n",
    "    title='Write Throughput by Batch Size',\n",
    "    labels={'batch_size': 'Batch Size', 'records_per_second': 'Records/Second'},\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.update_layout(height=400)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86ef44",
   "metadata": {},
   "source": [
    "## ğŸ“Œ Key Takeaways\n",
    "\n",
    "### Time Series Databases Overview\n",
    "\n",
    "| Aspect | Key Points |\n",
    "|--------|------------|\n",
    "| **Purpose** | Optimized storage and querying of time-stamped data |\n",
    "| **Data Model** | Time-ordered, append-mostly, with tags and fields |\n",
    "| **Key Features** | Compression, retention policies, downsampling, continuous aggregates |\n",
    "\n",
    "### InfluxDB vs TimescaleDB\n",
    "\n",
    "| Feature | InfluxDB | TimescaleDB |\n",
    "|---------|----------|-------------|\n",
    "| **Query Language** | Flux, InfluxQL | Full SQL |\n",
    "| **Schema** | Schema-less | Schema-based (PostgreSQL) |\n",
    "| **Best For** | Pure metrics/IoT | SQL + time series hybrid |\n",
    "| **Ecosystem** | TICK stack | PostgreSQL ecosystem |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Retention Policies**: Define data lifecycle based on business requirements\n",
    "2. **Downsampling**: Store high-resolution recent data, aggregate older data\n",
    "3. **Compression**: Enable compression for 10-20x storage savings\n",
    "4. **Continuous Aggregates**: Pre-compute common queries for faster dashboards\n",
    "5. **Batch Writes**: Group writes for better throughput\n",
    "6. **Time-Bounded Queries**: Always filter by time range\n",
    "\n",
    "### Use Case Selection Guide\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Choose InfluxDB when:              Choose TimescaleDB when:â”‚\n",
    "â”‚  â€¢ Pure metrics/monitoring          â€¢ Need full SQL         â”‚\n",
    "â”‚  â€¢ DevOps/infrastructure            â€¢ Relational + TS data  â”‚\n",
    "â”‚  â€¢ Rapid prototyping                â€¢ Complex analytics     â”‚\n",
    "â”‚  â€¢ TICK stack integration           â€¢ PostgreSQL ecosystem  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "- **Write throughput**: 100K-1M+ points/second achievable with proper batching\n",
    "- **Query latency**: Sub-second for well-indexed recent data\n",
    "- **Compression ratios**: 10-20x typical for sensor/metrics data\n",
    "- **Retention trade-offs**: Balance storage costs vs. historical analysis needs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}