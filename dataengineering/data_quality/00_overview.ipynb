{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac22267",
   "metadata": {},
   "source": [
    "# Data Quality â€” Overview\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Data quality is the foundation of trustworthy analytics and reliable data-driven decision-making. This notebook provides a comprehensive overview of data quality concepts, dimensions, validation strategies, and tools that help ensure your data pipelines produce accurate, consistent, and timely data.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "1. What are the core dimensions of data quality?\n",
    "2. How do we measure and validate data quality?\n",
    "3. What tools and frameworks are available for data quality management?\n",
    "4. How do data contracts and SLAs help maintain data quality?\n",
    "5. What are best practices for implementing data quality in production pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb86b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Quality Dimensions\n",
    "\n",
    "Data quality is typically evaluated across multiple dimensions. Understanding these dimensions helps identify quality issues and establish meaningful metrics.\n",
    "\n",
    "### 1.1 Accuracy\n",
    "\n",
    "**Definition**: The degree to which data correctly represents the real-world entity or event it describes.\n",
    "\n",
    "| Aspect | Description | Example |\n",
    "|--------|-------------|--------|\n",
    "| **Semantic Accuracy** | Data values match real-world truth | Customer age = 35 (actual age is 35) |\n",
    "| **Syntactic Accuracy** | Data conforms to expected format | Email follows `user@domain.com` pattern |\n",
    "| **Precision** | Level of detail in measurements | Temperature recorded to 2 decimal places |\n",
    "\n",
    "```python\n",
    "# Example: Accuracy validation\n",
    "def validate_accuracy(df, column, valid_range):\n",
    "    \"\"\"Check if values fall within expected range.\"\"\"\n",
    "    min_val, max_val = valid_range\n",
    "    invalid = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "    accuracy_rate = 1 - (len(invalid) / len(df))\n",
    "    return accuracy_rate\n",
    "```\n",
    "\n",
    "### 1.2 Completeness\n",
    "\n",
    "**Definition**: The extent to which all required data is present and no values are missing.\n",
    "\n",
    "| Type | Description | Measurement |\n",
    "|------|-------------|------------|\n",
    "| **Column Completeness** | Percentage of non-null values in a column | `1 - (null_count / total_rows)` |\n",
    "| **Row Completeness** | Percentage of complete rows (no nulls) | `complete_rows / total_rows` |\n",
    "| **Schema Completeness** | All expected columns are present | `present_columns / expected_columns` |\n",
    "\n",
    "```python\n",
    "# Example: Completeness check\n",
    "def check_completeness(df, required_columns):\n",
    "    \"\"\"Calculate completeness metrics for a DataFrame.\"\"\"\n",
    "    results = {}\n",
    "    for col in required_columns:\n",
    "        if col in df.columns:\n",
    "            completeness = df[col].notna().mean()\n",
    "            results[col] = round(completeness * 100, 2)\n",
    "        else:\n",
    "            results[col] = 0.0  # Column missing\n",
    "    return results\n",
    "```\n",
    "\n",
    "### 1.3 Consistency\n",
    "\n",
    "**Definition**: Data values are uniform and non-contradictory across datasets, systems, and time.\n",
    "\n",
    "| Consistency Type | Description | Example |\n",
    "|------------------|-------------|--------|\n",
    "| **Intra-record** | Values within a single record are consistent | `end_date >= start_date` |\n",
    "| **Inter-record** | Values across related records match | Order total = sum of line items |\n",
    "| **Cross-system** | Same entity has same values across systems | Customer name in CRM = name in billing |\n",
    "| **Temporal** | Historical data remains consistent over time | Yesterday's final figures unchanged |\n",
    "\n",
    "```python\n",
    "# Example: Consistency validation\n",
    "def check_referential_integrity(orders_df, customers_df):\n",
    "    \"\"\"Verify all orders reference valid customers.\"\"\"\n",
    "    valid_customer_ids = set(customers_df['customer_id'])\n",
    "    orphan_orders = orders_df[~orders_df['customer_id'].isin(valid_customer_ids)]\n",
    "    integrity_rate = 1 - (len(orphan_orders) / len(orders_df))\n",
    "    return integrity_rate, orphan_orders\n",
    "```\n",
    "\n",
    "### 1.4 Timeliness\n",
    "\n",
    "**Definition**: Data is available when needed and reflects the current state of the real world.\n",
    "\n",
    "| Metric | Description | Calculation |\n",
    "|--------|-------------|------------|\n",
    "| **Freshness** | How recent is the data? | `current_time - last_update_time` |\n",
    "| **Latency** | Time from event to availability | `available_time - event_time` |\n",
    "| **SLA Compliance** | Data arrives within agreed timeframe | `on_time_deliveries / total_deliveries` |\n",
    "\n",
    "```python\n",
    "# Example: Timeliness check\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_freshness(df, timestamp_col, max_age_hours=24):\n",
    "    \"\"\"Check if data is within acceptable freshness window.\"\"\"\n",
    "    latest_record = df[timestamp_col].max()\n",
    "    age = datetime.now() - latest_record\n",
    "    is_fresh = age <= timedelta(hours=max_age_hours)\n",
    "    return is_fresh, age\n",
    "```\n",
    "\n",
    "### Additional Dimensions\n",
    "\n",
    "| Dimension | Description |\n",
    "|-----------|------------|\n",
    "| **Uniqueness** | No duplicate records exist for the same entity |\n",
    "| **Validity** | Data conforms to defined business rules and constraints |\n",
    "| **Integrity** | Relationships between data elements are maintained |\n",
    "| **Conformity** | Data follows standard formats and conventions |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59a133",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Validation Strategies\n",
    "\n",
    "### 2.1 Validation Approaches\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Data Validation Strategies                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Schema-Based  â”‚  Rule-Based     â”‚  Statistical    â”‚  ML-Based     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ Data types    â”‚ â€¢ Business      â”‚ â€¢ Distribution  â”‚ â€¢ Anomaly     â”‚\n",
    "â”‚ â€¢ Constraints   â”‚   logic checks  â”‚   analysis      â”‚   detection   â”‚\n",
    "â”‚ â€¢ Nullability   â”‚ â€¢ Regex         â”‚ â€¢ Outlier       â”‚ â€¢ Pattern     â”‚\n",
    "â”‚ â€¢ Referential   â”‚   patterns      â”‚   detection     â”‚   learning    â”‚\n",
    "â”‚   integrity     â”‚ â€¢ Range checks  â”‚ â€¢ Drift         â”‚ â€¢ Profiling   â”‚\n",
    "â”‚                 â”‚                 â”‚   detection     â”‚               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 2.2 Schema Validation\n",
    "\n",
    "```python\n",
    "# Using Pydantic for schema validation\n",
    "from pydantic import BaseModel, validator, EmailStr\n",
    "from typing import Optional\n",
    "from datetime import date\n",
    "\n",
    "class CustomerRecord(BaseModel):\n",
    "    customer_id: int\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    registration_date: date\n",
    "    age: Optional[int] = None\n",
    "    \n",
    "    @validator('age')\n",
    "    def validate_age(cls, v):\n",
    "        if v is not None and (v < 0 or v > 150):\n",
    "            raise ValueError('Age must be between 0 and 150')\n",
    "        return v\n",
    "```\n",
    "\n",
    "### 2.3 Rule-Based Validation\n",
    "\n",
    "```python\n",
    "# Custom validation rules\n",
    "class DataQualityRules:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.results = []\n",
    "    \n",
    "    def check_not_null(self, column):\n",
    "        \"\"\"Column should not contain null values.\"\"\"\n",
    "        null_count = self.df[column].isna().sum()\n",
    "        passed = null_count == 0\n",
    "        self.results.append({\n",
    "            'rule': f'{column}_not_null',\n",
    "            'passed': passed,\n",
    "            'details': f'{null_count} null values found'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def check_unique(self, column):\n",
    "        \"\"\"Column should contain unique values.\"\"\"\n",
    "        duplicate_count = self.df[column].duplicated().sum()\n",
    "        passed = duplicate_count == 0\n",
    "        self.results.append({\n",
    "            'rule': f'{column}_unique',\n",
    "            'passed': passed,\n",
    "            'details': f'{duplicate_count} duplicates found'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def check_range(self, column, min_val, max_val):\n",
    "        \"\"\"Values should be within specified range.\"\"\"\n",
    "        out_of_range = self.df[\n",
    "            (self.df[column] < min_val) | (self.df[column] > max_val)\n",
    "        ]\n",
    "        passed = len(out_of_range) == 0\n",
    "        self.results.append({\n",
    "            'rule': f'{column}_in_range',\n",
    "            'passed': passed,\n",
    "            'details': f'{len(out_of_range)} values out of range'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "```\n",
    "\n",
    "### 2.4 Statistical Validation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class StatisticalValidator:\n",
    "    def __init__(self, baseline_df, current_df):\n",
    "        self.baseline = baseline_df\n",
    "        self.current = current_df\n",
    "    \n",
    "    def detect_distribution_drift(self, column, threshold=0.05):\n",
    "        \"\"\"Detect if distribution has shifted using KS test.\"\"\"\n",
    "        statistic, p_value = stats.ks_2samp(\n",
    "            self.baseline[column].dropna(),\n",
    "            self.current[column].dropna()\n",
    "        )\n",
    "        drift_detected = p_value < threshold\n",
    "        return {\n",
    "            'column': column,\n",
    "            'drift_detected': drift_detected,\n",
    "            'p_value': p_value,\n",
    "            'ks_statistic': statistic\n",
    "        }\n",
    "    \n",
    "    def detect_outliers(self, column, method='zscore', threshold=3):\n",
    "        \"\"\"Identify outliers using z-score method.\"\"\"\n",
    "        if method == 'zscore':\n",
    "            z_scores = np.abs(stats.zscore(self.current[column].dropna()))\n",
    "            outliers = z_scores > threshold\n",
    "            return outliers.sum(), outliers.mean()\n",
    "        elif method == 'iqr':\n",
    "            Q1 = self.current[column].quantile(0.25)\n",
    "            Q3 = self.current[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = (self.current[column] < Q1 - 1.5*IQR) | \\\n",
    "                       (self.current[column] > Q3 + 1.5*IQR)\n",
    "            return outliers.sum(), outliers.mean()\n",
    "```\n",
    "\n",
    "### 2.5 Validation Placement in Pipelines\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Source  â”‚â”€â”€â”€â–¶â”‚ Ingest   â”‚â”€â”€â”€â–¶â”‚ Transform â”‚â”€â”€â”€â–¶â”‚  Load    â”‚â”€â”€â”€â–¶â”‚  Serve   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚               â”‚               â”‚               â”‚\n",
    "                     â–¼               â–¼               â–¼               â–¼\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚ Schema   â”‚    â”‚ Business  â”‚   â”‚ Integrityâ”‚    â”‚ Freshnessâ”‚\n",
    "               â”‚ Checks   â”‚    â”‚ Rules     â”‚   â”‚ Checks   â”‚    â”‚ Checks   â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6b814",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Great Expectations and Other Tools\n",
    "\n",
    "### 3.1 Great Expectations\n",
    "\n",
    "Great Expectations (GX) is the leading open-source data quality framework.\n",
    "\n",
    "**Core Concepts:**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|------------|\n",
    "| **Expectation** | A declarative assertion about data (e.g., column is not null) |\n",
    "| **Expectation Suite** | A collection of expectations for a dataset |\n",
    "| **Validator** | Validates data against expectation suites |\n",
    "| **Checkpoint** | Bundles validation runs with actions |\n",
    "| **Data Docs** | Auto-generated documentation and reports |\n",
    "\n",
    "```python\n",
    "# Great Expectations Example\n",
    "import great_expectations as gx\n",
    "\n",
    "# Initialize context\n",
    "context = gx.get_context()\n",
    "\n",
    "# Connect to data\n",
    "datasource = context.sources.add_pandas(\"my_datasource\")\n",
    "data_asset = datasource.add_dataframe_asset(name=\"orders\")\n",
    "\n",
    "# Build batch request\n",
    "batch_request = data_asset.build_batch_request(dataframe=orders_df)\n",
    "\n",
    "# Create expectations\n",
    "expectation_suite = context.add_expectation_suite(\"orders_suite\")\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"orders_suite\"\n",
    ")\n",
    "\n",
    "# Add expectations\n",
    "validator.expect_column_to_exist(\"order_id\")\n",
    "validator.expect_column_values_to_not_be_null(\"order_id\")\n",
    "validator.expect_column_values_to_be_unique(\"order_id\")\n",
    "validator.expect_column_values_to_be_between(\n",
    "    column=\"amount\",\n",
    "    min_value=0,\n",
    "    max_value=100000\n",
    ")\n",
    "validator.expect_column_values_to_match_regex(\n",
    "    column=\"email\",\n",
    "    regex=r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n",
    ")\n",
    "\n",
    "# Validate and get results\n",
    "validation_result = validator.validate()\n",
    "print(f\"Success: {validation_result.success}\")\n",
    "```\n",
    "\n",
    "### 3.2 Tool Comparison\n",
    "\n",
    "| Tool | Type | Best For | Key Features |\n",
    "|------|------|----------|-------------|\n",
    "| **Great Expectations** | Framework | Comprehensive DQ | Expectations, Data Docs, Profiling |\n",
    "| **dbt tests** | SQL-based | dbt users | Built-in + custom tests, CI/CD |\n",
    "| **Soda Core** | CLI/Cloud | SQL warehouses | SodaCL language, monitoring |\n",
    "| **Pandera** | Python | Pandas/Pyspark | Schema + statistical validation |\n",
    "| **Deequ** | Spark | Big data | Unit tests for data, anomaly detection |\n",
    "| **Monte Carlo** | SaaS | Enterprise | ML-powered, observability |\n",
    "| **Datafold** | SaaS | Data diffing | Column-level lineage, CI testing |\n",
    "\n",
    "### 3.3 dbt Tests\n",
    "\n",
    "```yaml\n",
    "# schema.yml - dbt tests\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: orders\n",
    "    columns:\n",
    "      - name: order_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: customer_id\n",
    "        tests:\n",
    "          - not_null\n",
    "          - relationships:\n",
    "              to: ref('customers')\n",
    "              field: customer_id\n",
    "      - name: status\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              values: ['pending', 'shipped', 'delivered', 'cancelled']\n",
    "      - name: amount\n",
    "        tests:\n",
    "          - dbt_utils.expression_is_true:\n",
    "              expression: \">= 0\"\n",
    "```\n",
    "\n",
    "### 3.4 Soda Core\n",
    "\n",
    "```yaml\n",
    "# checks.yml - Soda checks\n",
    "checks for orders:\n",
    "  - row_count > 0\n",
    "  - missing_count(order_id) = 0\n",
    "  - duplicate_count(order_id) = 0\n",
    "  - invalid_percent(email) < 5%:\n",
    "      valid regex: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n",
    "  - avg(amount) between 50 and 500\n",
    "  - freshness(created_at) < 1d\n",
    "  - schema:\n",
    "      fail:\n",
    "        when missing column: [order_id, customer_id, amount]\n",
    "        when wrong type:\n",
    "          order_id: integer\n",
    "          amount: decimal\n",
    "```\n",
    "\n",
    "### 3.5 Pandera\n",
    "\n",
    "```python\n",
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "# Define schema with validation rules\n",
    "order_schema = DataFrameSchema({\n",
    "    \"order_id\": Column(int, Check.greater_than(0), unique=True),\n",
    "    \"customer_id\": Column(int, Check.greater_than(0)),\n",
    "    \"amount\": Column(float, Check.in_range(0, 100000)),\n",
    "    \"status\": Column(str, Check.isin(['pending', 'shipped', 'delivered'])),\n",
    "    \"email\": Column(str, Check.str_matches(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')),\n",
    "    \"created_at\": Column(pa.DateTime),\n",
    "})\n",
    "\n",
    "# Validate DataFrame\n",
    "validated_df = order_schema.validate(orders_df)\n",
    "\n",
    "# Use as decorator\n",
    "@pa.check_input(order_schema)\n",
    "def process_orders(df):\n",
    "    # Processing logic here\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5843a6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Contracts and SLAs\n",
    "\n",
    "### 4.1 What is a Data Contract?\n",
    "\n",
    "A **data contract** is a formal agreement between data producers and consumers that defines:\n",
    "\n",
    "- Schema and data types\n",
    "- Semantic meaning of fields\n",
    "- Quality expectations\n",
    "- Freshness requirements\n",
    "- Access patterns\n",
    "- Ownership and responsibilities\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         Data Contract                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Producer: Team A                    Consumer: Team B               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚   Schema    â”‚    â”‚   Quality    â”‚    â”‚    SLA      â”‚            â”‚\n",
    "â”‚  â”‚  Definition â”‚    â”‚   Rules      â”‚    â”‚  Agreement  â”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Versioning  â”‚  Breaking Change Policy  â”‚  Deprecation Process     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 4.2 Data Contract Example (YAML)\n",
    "\n",
    "```yaml\n",
    "# data_contract.yaml\n",
    "dataContractSpecification: 0.9.0\n",
    "id: urn:datacontract:orders:v1\n",
    "info:\n",
    "  title: Orders Data Contract\n",
    "  version: 1.0.0\n",
    "  owner: data-platform-team\n",
    "  contact:\n",
    "    name: Data Platform Team\n",
    "    email: data-platform@company.com\n",
    "\n",
    "servers:\n",
    "  production:\n",
    "    type: bigquery\n",
    "    project: analytics-prod\n",
    "    dataset: orders\n",
    "\n",
    "models:\n",
    "  orders:\n",
    "    description: Customer order records\n",
    "    type: table\n",
    "    fields:\n",
    "      - name: order_id\n",
    "        type: string\n",
    "        required: true\n",
    "        unique: true\n",
    "        description: Unique order identifier\n",
    "      - name: customer_id\n",
    "        type: string\n",
    "        required: true\n",
    "        description: Reference to customer\n",
    "      - name: amount\n",
    "        type: decimal\n",
    "        required: true\n",
    "        description: Order total in USD\n",
    "        constraints:\n",
    "          minimum: 0\n",
    "      - name: created_at\n",
    "        type: timestamp\n",
    "        required: true\n",
    "        description: Order creation timestamp\n",
    "\n",
    "quality:\n",
    "  type: great-expectations\n",
    "  specification:\n",
    "    - expectation: expect_table_row_count_to_be_between\n",
    "      kwargs:\n",
    "        min_value: 1000\n",
    "    - expectation: expect_column_values_to_not_be_null\n",
    "      kwargs:\n",
    "        column: order_id\n",
    "\n",
    "sla:\n",
    "  freshness:\n",
    "    max_age: PT1H  # ISO 8601 duration: 1 hour\n",
    "  availability:\n",
    "    uptime: 99.9%\n",
    "  latency:\n",
    "    p95: 500ms\n",
    "```\n",
    "\n",
    "### 4.3 Service Level Agreements (SLAs)\n",
    "\n",
    "| SLA Component | Description | Example |\n",
    "|---------------|-------------|--------|\n",
    "| **Freshness** | Maximum age of data | Data updated every hour |\n",
    "| **Availability** | Uptime percentage | 99.9% availability |\n",
    "| **Completeness** | Minimum completeness | >99% non-null for required fields |\n",
    "| **Accuracy** | Error rate threshold | <0.1% data errors |\n",
    "| **Latency** | Query response time | P95 < 500ms |\n",
    "\n",
    "### 4.4 Implementing Data Contracts\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class SLA:\n",
    "    freshness: timedelta\n",
    "    completeness_threshold: float\n",
    "    accuracy_threshold: float\n",
    "    availability_target: float\n",
    "\n",
    "@dataclass \n",
    "class DataContract:\n",
    "    name: str\n",
    "    version: str\n",
    "    owner: str\n",
    "    schema: Dict\n",
    "    quality_rules: List[Dict]\n",
    "    sla: SLA\n",
    "    \n",
    "    def validate(self, df) -> Dict:\n",
    "        \"\"\"Validate data against contract.\"\"\"\n",
    "        results = {\n",
    "            'schema_valid': self._validate_schema(df),\n",
    "            'quality_passed': self._run_quality_checks(df),\n",
    "            'sla_met': self._check_sla(df)\n",
    "        }\n",
    "        results['contract_fulfilled'] = all(results.values())\n",
    "        return results\n",
    "    \n",
    "    def _validate_schema(self, df) -> bool:\n",
    "        # Schema validation logic\n",
    "        return True\n",
    "    \n",
    "    def _run_quality_checks(self, df) -> bool:\n",
    "        # Quality rule execution\n",
    "        return True\n",
    "    \n",
    "    def _check_sla(self, df) -> bool:\n",
    "        # SLA verification\n",
    "        return True\n",
    "```\n",
    "\n",
    "### 4.5 Contract Testing in CI/CD\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/data-contract-ci.yml\n",
    "name: Data Contract Validation\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - 'contracts/**'\n",
    "      - 'schemas/**'\n",
    "\n",
    "jobs:\n",
    "  validate-contracts:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Validate contract syntax\n",
    "        run: |\n",
    "          pip install datacontract-cli\n",
    "          datacontract lint contracts/*.yaml\n",
    "          \n",
    "      - name: Check breaking changes\n",
    "        run: |\n",
    "          datacontract breaking contracts/orders.yaml --with main\n",
    "          \n",
    "      - name: Test with sample data\n",
    "        run: |\n",
    "          datacontract test contracts/orders.yaml \\\n",
    "            --examples samples/orders.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e7d72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Quality Architecture\n",
    "\n",
    "### 5.1 Centralized vs Decentralized Approach\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Centralized DQ                                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\n",
    "â”‚  â”‚ Team A   â”‚    â”‚ Team B   â”‚    â”‚ Team C   â”‚                       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                       â”‚\n",
    "â”‚       â”‚               â”‚               â”‚                              â”‚\n",
    "â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚\n",
    "â”‚                       â–¼                                              â”‚\n",
    "â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚\n",
    "â”‚              â”‚  Central DQ Team â”‚                                    â”‚\n",
    "â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Decentralized (Mesh) DQ                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚   Domain A   â”‚  â”‚   Domain B   â”‚  â”‚   Domain C   â”‚              â”‚\n",
    "â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚\n",
    "â”‚  â”‚ â”‚ DQ Rules â”‚ â”‚  â”‚ â”‚ DQ Rules â”‚ â”‚  â”‚ â”‚ DQ Rules â”‚ â”‚              â”‚\n",
    "â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                         â”‚                                           â”‚\n",
    "â”‚                         â–¼                                           â”‚\n",
    "â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚              â”‚  Shared DQ Platform  â”‚                               â”‚\n",
    "â”‚              â”‚  (Standards + Tools) â”‚                               â”‚\n",
    "â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 5.2 Data Quality Monitoring Dashboard\n",
    "\n",
    "Key metrics to track:\n",
    "\n",
    "| Metric Category | Metrics |\n",
    "|-----------------|--------|\n",
    "| **Coverage** | % tables with DQ checks, % columns validated |\n",
    "| **Pass Rate** | % checks passing, trend over time |\n",
    "| **Incidents** | DQ failures by severity, MTTR |\n",
    "| **SLA Compliance** | Freshness SLA met, availability % |\n",
    "| **Data Volume** | Row counts, anomaly detection |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db23b16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Best Practices\n",
    "\n",
    "### 6.1 Implementation Guidelines\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|------------|\n",
    "| **Shift Left** | Validate early in the pipeline, close to the source |\n",
    "| **Automate** | Integrate DQ checks into CI/CD and orchestration |\n",
    "| **Alert Smartly** | Set thresholds to avoid alert fatigue |\n",
    "| **Document** | Maintain living documentation with data contracts |\n",
    "| **Measure ROI** | Track cost of bad data vs investment in DQ |\n",
    "| **Iterate** | Start simple, add complexity based on needs |\n",
    "\n",
    "### 6.2 Common Anti-Patterns\n",
    "\n",
    "| Anti-Pattern | Why It's Bad | Better Approach |\n",
    "|--------------|--------------|----------------|\n",
    "| Validating only at the end | Late detection, expensive fixes | Validate at each stage |\n",
    "| Too many alerts | Alert fatigue, ignored warnings | Prioritize critical checks |\n",
    "| No baseline metrics | Can't detect drift | Profile data regularly |\n",
    "| Manual quality checks | Not scalable, error-prone | Automate everything |\n",
    "| Siloed DQ ownership | Gaps between teams | Shared responsibility model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ff6f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Takeaway\n",
    "\n",
    "| Concept | Key Points |\n",
    "|---------|------------|\n",
    "| **Dimensions** | Focus on accuracy, completeness, consistency, and timeliness as core quality pillars |\n",
    "| **Validation** | Layer schema, rule-based, and statistical validation throughout your pipelines |\n",
    "| **Tools** | Great Expectations, dbt tests, Soda, and Pandera provide robust validation frameworks |\n",
    "| **Contracts** | Data contracts formalize agreements between producers and consumers |\n",
    "| **SLAs** | Define measurable quality targets for freshness, availability, and accuracy |\n",
    "| **Architecture** | Balance centralized standards with decentralized domain ownership |\n",
    "\n",
    "### Quick Reference Commands\n",
    "\n",
    "```bash\n",
    "# Great Expectations\n",
    "pip install great_expectations\n",
    "great_expectations init\n",
    "great_expectations checkpoint run my_checkpoint\n",
    "\n",
    "# Soda Core\n",
    "pip install soda-core\n",
    "soda scan -d my_datasource -c configuration.yml checks.yml\n",
    "\n",
    "# dbt tests\n",
    "dbt test --select model_name\n",
    "dbt test --select tag:data_quality\n",
    "```\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Great Expectations Documentation](https://docs.greatexpectations.io/)\n",
    "- [Soda Core Documentation](https://docs.soda.io/)\n",
    "- [Data Contracts Specification](https://datacontract.com/)\n",
    "- [dbt Testing Documentation](https://docs.getdbt.com/docs/build/tests)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
