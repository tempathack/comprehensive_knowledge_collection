{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd62593",
   "metadata": {},
   "source": [
    "# Data Validation Techniques\n",
    "\n",
    "Data validation is a critical component of any data pipeline, ensuring that data meets quality standards before processing. This notebook covers comprehensive validation approaches including schema validation, business rules, statistical tests, and anomaly detection.\n",
    "\n",
    "## Why Data Validation Matters\n",
    "\n",
    "- **Garbage In, Garbage Out**: Poor quality data leads to unreliable analytics and ML models\n",
    "- **Early Detection**: Catch issues at ingestion rather than downstream\n",
    "- **Compliance**: Meet regulatory requirements (GDPR, HIPAA, SOX)\n",
    "- **Trust**: Build confidence in data-driven decisions\n",
    "\n",
    "## Validation Approaches Overview\n",
    "\n",
    "| Approach | Purpose | When to Use |\n",
    "|----------|---------|-------------|\n",
    "| **Schema Validation** | Verify structure, types, constraints | Every data ingestion |\n",
    "| **Business Rules** | Enforce domain-specific logic | After schema validation |\n",
    "| **Statistical Tests** | Detect distribution shifts, outliers | Batch processing, monitoring |\n",
    "| **Anomaly Detection** | Identify unexpected patterns | Continuous monitoring |\n",
    "| **Referential Integrity** | Verify relationships between datasets | Multi-source pipelines |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Callable, Optional, Union\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfe214",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Schema Validation\n",
    "\n",
    "Schema validation ensures data conforms to expected structure, data types, and constraints. It's the first line of defense in data quality.\n",
    "\n",
    "### Key Components\n",
    "- **Data Types**: Ensure columns have correct types (int, float, string, datetime)\n",
    "- **Nullability**: Define which fields can contain null values\n",
    "- **Constraints**: Value ranges, string lengths, patterns\n",
    "- **Required Fields**: Mandatory columns that must be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataType(Enum):\n",
    "    \"\"\"Supported data types for validation.\"\"\"\n",
    "    STRING = \"string\"\n",
    "    INTEGER = \"integer\"\n",
    "    FLOAT = \"float\"\n",
    "    BOOLEAN = \"boolean\"\n",
    "    DATETIME = \"datetime\"\n",
    "    DATE = \"date\"\n",
    "    EMAIL = \"email\"\n",
    "    UUID = \"uuid\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColumnSchema:\n",
    "    \"\"\"Schema definition for a single column.\"\"\"\n",
    "    name: str\n",
    "    dtype: DataType\n",
    "    nullable: bool = True\n",
    "    min_value: Optional[Union[int, float]] = None\n",
    "    max_value: Optional[Union[int, float]] = None\n",
    "    min_length: Optional[int] = None\n",
    "    max_length: Optional[int] = None\n",
    "    pattern: Optional[str] = None  # Regex pattern\n",
    "    allowed_values: Optional[List[Any]] = None\n",
    "    unique: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TableSchema:\n",
    "    \"\"\"Schema definition for a complete table/dataframe.\"\"\"\n",
    "    name: str\n",
    "    columns: List[ColumnSchema]\n",
    "    primary_key: Optional[List[str]] = None\n",
    "    min_rows: Optional[int] = None\n",
    "    max_rows: Optional[int] = None\n",
    "    \n",
    "    def get_column(self, name: str) -> Optional[ColumnSchema]:\n",
    "        \"\"\"Get column schema by name.\"\"\"\n",
    "        for col in self.columns:\n",
    "            if col.name == name:\n",
    "                return col\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def required_columns(self) -> List[str]:\n",
    "        \"\"\"Get list of non-nullable columns.\"\"\"\n",
    "        return [col.name for col in self.columns if not col.nullable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of a validation check.\"\"\"\n",
    "    is_valid: bool\n",
    "    rule_name: str\n",
    "    column: Optional[str] = None\n",
    "    message: str = \"\"\n",
    "    failed_count: int = 0\n",
    "    total_count: int = 0\n",
    "    failed_indices: List[int] = field(default_factory=list)\n",
    "    severity: str = \"ERROR\"  # ERROR, WARNING, INFO\n",
    "    \n",
    "    @property\n",
    "    def pass_rate(self) -> float:\n",
    "        \"\"\"Calculate pass rate as percentage.\"\"\"\n",
    "        if self.total_count == 0:\n",
    "            return 100.0\n",
    "        return ((self.total_count - self.failed_count) / self.total_count) * 100\n",
    "    \n",
    "    def __repr__(self):\n",
    "        status = \"✓ PASS\" if self.is_valid else \"✗ FAIL\"\n",
    "        col_info = f\" [{self.column}]\" if self.column else \"\"\n",
    "        return f\"{status}{col_info}: {self.rule_name} - {self.message}\"\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class ValidationReport:\n",
    "    \"\"\"Aggregated validation report.\"\"\"\n",
    "    results: List[ValidationResult]\n",
    "    validated_at: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    @property\n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"Check if all validations passed.\"\"\"\n",
    "        return all(r.is_valid for r in self.results if r.severity == \"ERROR\")\n",
    "    \n",
    "    @property\n",
    "    def error_count(self) -> int:\n",
    "        return sum(1 for r in self.results if not r.is_valid and r.severity == \"ERROR\")\n",
    "    \n",
    "    @property\n",
    "    def warning_count(self) -> int:\n",
    "        return sum(1 for r in self.results if not r.is_valid and r.severity == \"WARNING\")\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Generate summary report.\"\"\"\n",
    "        lines = [\n",
    "            f\"\\n{'='*60}\",\n",
    "            f\"VALIDATION REPORT - {self.validated_at.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"{'='*60}\",\n",
    "            f\"Overall Status: {'✓ PASSED' if self.is_valid else '✗ FAILED'}\",\n",
    "            f\"Total Checks: {len(self.results)}\",\n",
    "            f\"Errors: {self.error_count} | Warnings: {self.warning_count}\",\n",
    "            f\"{'-'*60}\"\n",
    "        ]\n",
    "        \n",
    "        for result in self.results:\n",
    "            lines.append(str(result))\n",
    "        \n",
    "        lines.append(f\"{'='*60}\\n\")\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SchemaValidator:\n",
    "    \"\"\"Validates dataframes against defined schemas.\"\"\"\n",
    "    \n",
    "    # Type mapping for pandas dtype checking\n",
    "    TYPE_MAPPING = {\n",
    "        DataType.STRING: ['object', 'string', 'str'],\n",
    "        DataType.INTEGER: ['int64', 'int32', 'int16', 'int8', 'Int64'],\n",
    "        DataType.FLOAT: ['float64', 'float32', 'float16'],\n",
    "        DataType.BOOLEAN: ['bool', 'boolean'],\n",
    "        DataType.DATETIME: ['datetime64[ns]', 'datetime64'],\n",
    "        DataType.DATE: ['datetime64[ns]', 'object'],\n",
    "    }\n",
    "    \n",
    "    # Regex patterns for special types\n",
    "    PATTERNS = {\n",
    "        DataType.EMAIL: r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$',\n",
    "        DataType.UUID: r'^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, schema: TableSchema):\n",
    "        self.schema = schema\n",
    "        self.results: List[ValidationResult] = []\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> ValidationReport:\n",
    "        \"\"\"Run all schema validations.\"\"\"\n",
    "        self.results = []\n",
    "        \n",
    "        # Table-level validations\n",
    "        self._validate_required_columns(df)\n",
    "        self._validate_row_count(df)\n",
    "        self._validate_primary_key(df)\n",
    "        \n",
    "        # Column-level validations\n",
    "        for col_schema in self.schema.columns:\n",
    "            if col_schema.name in df.columns:\n",
    "                self._validate_column(df, col_schema)\n",
    "        \n",
    "        return ValidationReport(results=self.results)\n",
    "    \n",
    "    def _validate_required_columns(self, df: pd.DataFrame):\n",
    "        \"\"\"Check all required columns are present.\"\"\"\n",
    "        expected = {col.name for col in self.schema.columns}\n",
    "        actual = set(df.columns)\n",
    "        missing = expected - actual\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=len(missing) == 0,\n",
    "            rule_name=\"required_columns\",\n",
    "            message=f\"Missing columns: {missing}\" if missing else \"All required columns present\",\n",
    "            failed_count=len(missing),\n",
    "            total_count=len(expected)\n",
    "        ))\n",
    "    \n",
    "    def _validate_row_count(self, df: pd.DataFrame):\n",
    "        \"\"\"Validate row count constraints.\"\"\"\n",
    "        row_count = len(df)\n",
    "        is_valid = True\n",
    "        messages = []\n",
    "        \n",
    "        if self.schema.min_rows and row_count < self.schema.min_rows:\n",
    "            is_valid = False\n",
    "            messages.append(f\"Row count {row_count} < minimum {self.schema.min_rows}\")\n",
    "        \n",
    "        if self.schema.max_rows and row_count > self.schema.max_rows:\n",
    "            is_valid = False\n",
    "            messages.append(f\"Row count {row_count} > maximum {self.schema.max_rows}\")\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=is_valid,\n",
    "            rule_name=\"row_count\",\n",
    "            message=\"; \".join(messages) if messages else f\"Row count {row_count} within bounds\",\n",
    "            total_count=row_count\n",
    "        ))\n",
    "    \n",
    "    def _validate_primary_key(self, df: pd.DataFrame):\n",
    "        \"\"\"Validate primary key uniqueness.\"\"\"\n",
    "        if not self.schema.primary_key:\n",
    "            return\n",
    "        \n",
    "        pk_cols = [c for c in self.schema.primary_key if c in df.columns]\n",
    "        if len(pk_cols) != len(self.schema.primary_key):\n",
    "            return  # Missing PK columns handled elsewhere\n",
    "        \n",
    "        duplicates = df.duplicated(subset=pk_cols, keep=False)\n",
    "        dup_count = duplicates.sum()\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=dup_count == 0,\n",
    "            rule_name=\"primary_key_unique\",\n",
    "            message=f\"{dup_count} duplicate primary key values\" if dup_count > 0 else \"Primary key is unique\",\n",
    "            failed_count=dup_count,\n",
    "            total_count=len(df),\n",
    "            failed_indices=df[duplicates].index.tolist()[:100]  # Limit to first 100\n",
    "        ))\n",
    "    \n",
    "    def _validate_column(self, df: pd.DataFrame, col_schema: ColumnSchema):\n",
    "        \"\"\"Validate a single column against its schema.\"\"\"\n",
    "        col_data = df[col_schema.name]\n",
    "        \n",
    "        # Null check\n",
    "        self._validate_nullability(col_data, col_schema)\n",
    "        \n",
    "        # Type check\n",
    "        self._validate_dtype(col_data, col_schema)\n",
    "        \n",
    "        # Range check\n",
    "        if col_schema.min_value is not None or col_schema.max_value is not None:\n",
    "            self._validate_range(col_data, col_schema)\n",
    "        \n",
    "        # Length check (for strings)\n",
    "        if col_schema.dtype == DataType.STRING:\n",
    "            if col_schema.min_length or col_schema.max_length:\n",
    "                self._validate_length(col_data, col_schema)\n",
    "        \n",
    "        # Pattern check\n",
    "        if col_schema.pattern or col_schema.dtype in self.PATTERNS:\n",
    "            self._validate_pattern(col_data, col_schema)\n",
    "        \n",
    "        # Allowed values check\n",
    "        if col_schema.allowed_values:\n",
    "            self._validate_allowed_values(col_data, col_schema)\n",
    "        \n",
    "        # Uniqueness check\n",
    "        if col_schema.unique:\n",
    "            self._validate_uniqueness(col_data, col_schema)\n",
    "    \n",
    "    def _validate_nullability(self, col_data: pd.Series, col_schema: ColumnSchema):\n",
    "        \"\"\"Check for null values in non-nullable columns.\"\"\"\n",
    "        null_count = col_data.isna().sum()\n",
    "        \n",
    "        if not col_schema.nullable:\n",
    "            self.results.append(ValidationResult(\n",
    "                is_valid=null_count == 0,\n",
    "                rule_name=\"not_null\",\n",
    "                column=col_schema.name,\n",
    "                message=f\"{null_count} null values found\" if null_count > 0 else \"No null values\",\n",
    "                failed_count=null_count,\n",
    "                total_count=len(col_data)\n",
    "            ))\n",
    "    \n",
    "    def _validate_dtype(self, col_data: pd.Series, col_schema: ColumnSchema):\n",
    "        \"\"\"Validate column data type.\"\"\"\n",
    "        actual_dtype = str(col_data.dtype)\n",
    "        expected_types = self.TYPE_MAPPING.get(col_schema.dtype, [])\n",
    "        \n",
    "        is_valid = any(actual_dtype.startswith(t) for t in expected_types)\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=is_valid,\n",
    "            rule_name=\"data_type\",\n",
    "            column=col_schema.name,\n",
    "            message=f\"Expected {col_schema.dtype.value}, got {actual_dtype}\" if not is_valid else f\"Type {actual_dtype} valid\",\n",
    "            severity=\"WARNING\" if is_valid else \"ERROR\"\n",
    "        ))\n",
    "    \n",
    "    def _validate_range(self, col_data: pd.Series, col_schema: ColumnSchema):\n",
    "        \"\"\"Validate numeric range constraints.\"\"\"\n",
    "        non_null = col_data.dropna()\n",
    "        failed_mask = pd.Series([False] * len(non_null), index=non_null.index)\n",
    "        \n",
    "        if col_schema.min_value is not None:\n",
    "            failed_mask |= non_null < col_schema.min_value\n",
    "        if col_schema.max_value is not None:\n",
    "            failed_mask |= non_null > col_schema.max_value\n",
    "        \n",
    "        failed_count = failed_mask.sum()\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=failed_count == 0,\n",
    "            rule_name=\"value_range\",\n",
    "            column=col_schema.name,\n",
    "            message=f\"{failed_count} values outside range [{col_schema.min_value}, {col_schema.max_value}]\",\n",
    "            failed_count=failed_count,\n",
    "            total_count=len(non_null)\n",
    "        ))\n",
    "    \n",
    "    def _validate_length(self, col_data: pd.Series, col_schema: ColumnSchema):\n",
    "        \"\"\"Validate string length constraints.\"\"\"\n",
    "        non_null = col_data.dropna().astype(str)\n",
    "        lengths = non_null.str.len()\n",
    "        failed_mask = pd.Series([False] * len(lengths), index=lengths.index)\n",
    "        \n",
    "        if col_schema.min_length:\n",
    "            failed_mask |= lengths < col_schema.min_length\n",
    "        if col_schema.max_length:\n",
    "            failed_mask |= lengths > col_schema.max_length\n",
    "        \n",
    "        failed_count = failed_mask.sum()\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=failed_count == 0,\n",
    "            rule_name=\"string_length\",\n",
    "            column=col_schema.name,\n",
    "            message=f\"{failed_count} values with invalid length\",\n",
    "            failed_count=failed_count,\n",
    "            total_count=len(non_null)\n",
    "        ))\n",
    "    \n",
    "    def _validate_pattern(self, col_data: pd.Series, col_schema: ColumnSchema):\n",
    "        \"\"\"Validate string pattern (regex).\"\"\"\n",
    "        pattern = col_schema.pattern or self.PATTERNS.get(col_schema.dtype)\n",
    "        if not pattern:\n",
    "            return\n",
    "        \n",
    "        non_null = col_data.dropna().astype(str)\n",
    "        matches = non_null.str.match(pattern)\n",
    "        failed_count = (~matches).sum()\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=failed_count == 0,\n",
    "            rule_name=\"pattern_match\",\n",
    "            column=col_schema.name,\n",
    "            message=f\"{failed_count} values don't match pattern\",\n",
    "            failed_count=failed_count,\n",
    "            total_count=len(non_null)\n",
    "        ))\n",
    "    \n",
    "    def _validate_allowed_values(self, col_data: pd.Series, col_schema: ColumnSchema):\n",
    "        \"\"\"Validate against allowed values list.\"\"\"\n",
    "        non_null = col_data.dropna()\n",
    "        invalid = ~non_null.isin(col_schema.allowed_values)\n",
    "        failed_count = invalid.sum()\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=failed_count == 0,\n",
    "            rule_name=\"allowed_values\",\n",
    "            column=col_schema.name,\n",
    "            message=f\"{failed_count} values not in allowed list\",\n",
    "            failed_count=failed_count,\n",
    "            total_count=len(non_null)\n",
    "        ))\n",
    "    \n",
    "    def _validate_uniqueness(self, col_data: pd.Series, col_schema: ColumnSchema):\n",
    "        \"\"\"Validate column uniqueness.\"\"\"\n",
    "        duplicates = col_data.duplicated(keep=False)\n",
    "        dup_count = duplicates.sum()\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=dup_count == 0,\n",
    "            rule_name=\"unique\",\n",
    "            column=col_schema.name,\n",
    "            message=f\"{dup_count} duplicate values\" if dup_count > 0 else \"All values unique\",\n",
    "            failed_count=dup_count,\n",
    "            total_count=len(col_data)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207dd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Schema Validation in Action\n",
    "\n",
    "# Define schema for customer data\n",
    "customer_schema = TableSchema(\n",
    "    name=\"customers\",\n",
    "    columns=[\n",
    "        ColumnSchema(\"customer_id\", DataType.STRING, nullable=False, unique=True),\n",
    "        ColumnSchema(\"email\", DataType.EMAIL, nullable=False),\n",
    "        ColumnSchema(\"age\", DataType.INTEGER, nullable=True, min_value=0, max_value=150),\n",
    "        ColumnSchema(\"status\", DataType.STRING, allowed_values=[\"active\", \"inactive\", \"pending\"]),\n",
    "        ColumnSchema(\"credit_score\", DataType.INTEGER, min_value=300, max_value=850),\n",
    "    ],\n",
    "    primary_key=[\"customer_id\"],\n",
    "    min_rows=1\n",
    ")\n",
    "\n",
    "# Sample data with some issues\n",
    "sample_data = pd.DataFrame({\n",
    "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C005\"],  # Duplicate C002\n",
    "    \"email\": [\"john@example.com\", \"invalid-email\", \"jane@test.org\", \"bob@company.com\", \"alice@corp.net\"],\n",
    "    \"age\": [25, 35, -5, 45, 200],  # Invalid: -5 and 200\n",
    "    \"status\": [\"active\", \"inactive\", \"pending\", \"unknown\", \"active\"],  # Invalid: unknown\n",
    "    \"credit_score\": [720, 650, 800, 250, 780]  # Invalid: 250 below min\n",
    "})\n",
    "\n",
    "# Run validation\n",
    "validator = SchemaValidator(customer_schema)\n",
    "report = validator.validate(sample_data)\n",
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9dd8d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Business Rules Validation\n",
    "\n",
    "Business rules go beyond schema validation to enforce domain-specific logic and complex conditions that span multiple columns or require external context.\n",
    "\n",
    "### Common Business Rules\n",
    "- **Cross-field validation**: Start date must be before end date\n",
    "- **Conditional requirements**: If status is 'completed', completion_date is required\n",
    "- **Computed validations**: Total must equal sum of components\n",
    "- **External lookups**: Product ID must exist in master catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caac1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessRule(ABC):\n",
    "    \"\"\"Abstract base class for business rules.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, severity: str = \"ERROR\"):\n",
    "        self.name = name\n",
    "        self.severity = severity\n",
    "    \n",
    "    @abstractmethod\n",
    "    def validate(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        \"\"\"Execute the validation rule.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class CrossFieldRule(BusinessRule):\n",
    "    \"\"\"Validates relationships between multiple columns.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, condition: Callable[[pd.DataFrame], pd.Series], \n",
    "                 message: str, severity: str = \"ERROR\"):\n",
    "        super().__init__(name, severity)\n",
    "        self.condition = condition\n",
    "        self.message = message\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        try:\n",
    "            # Condition should return True for valid rows\n",
    "            valid_mask = self.condition(df)\n",
    "            failed_count = (~valid_mask).sum()\n",
    "            \n",
    "            return ValidationResult(\n",
    "                is_valid=failed_count == 0,\n",
    "                rule_name=self.name,\n",
    "                message=f\"{failed_count} rows failed: {self.message}\" if failed_count > 0 else \"Rule passed\",\n",
    "                failed_count=failed_count,\n",
    "                total_count=len(df),\n",
    "                failed_indices=df[~valid_mask].index.tolist()[:100],\n",
    "                severity=self.severity\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                is_valid=False,\n",
    "                rule_name=self.name,\n",
    "                message=f\"Rule execution error: {str(e)}\",\n",
    "                severity=\"ERROR\"\n",
    "            )\n",
    "\n",
    "\n",
    "class ConditionalRequiredRule(BusinessRule):\n",
    "    \"\"\"Field is required when condition is met.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, condition_field: str, condition_value: Any,\n",
    "                 required_field: str, severity: str = \"ERROR\"):\n",
    "        super().__init__(name, severity)\n",
    "        self.condition_field = condition_field\n",
    "        self.condition_value = condition_value\n",
    "        self.required_field = required_field\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        # Find rows where condition is met but required field is null\n",
    "        condition_met = df[self.condition_field] == self.condition_value\n",
    "        required_null = df[self.required_field].isna()\n",
    "        failed = condition_met & required_null\n",
    "        failed_count = failed.sum()\n",
    "        \n",
    "        return ValidationResult(\n",
    "            is_valid=failed_count == 0,\n",
    "            rule_name=self.name,\n",
    "            message=f\"{self.required_field} required when {self.condition_field}={self.condition_value}\",\n",
    "            failed_count=failed_count,\n",
    "            total_count=condition_met.sum(),\n",
    "            failed_indices=df[failed].index.tolist(),\n",
    "            severity=self.severity\n",
    "        )\n",
    "\n",
    "\n",
    "class SumValidationRule(BusinessRule):\n",
    "    \"\"\"Validates that a total column equals sum of component columns.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, total_column: str, component_columns: List[str],\n",
    "                 tolerance: float = 0.01, severity: str = \"ERROR\"):\n",
    "        super().__init__(name, severity)\n",
    "        self.total_column = total_column\n",
    "        self.component_columns = component_columns\n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        calculated_sum = df[self.component_columns].sum(axis=1)\n",
    "        actual_total = df[self.total_column]\n",
    "        \n",
    "        # Check if difference is within tolerance\n",
    "        diff = (calculated_sum - actual_total).abs()\n",
    "        failed = diff > self.tolerance\n",
    "        failed_count = failed.sum()\n",
    "        \n",
    "        return ValidationResult(\n",
    "            is_valid=failed_count == 0,\n",
    "            rule_name=self.name,\n",
    "            column=self.total_column,\n",
    "            message=f\"{failed_count} rows where total doesn't match sum of components\",\n",
    "            failed_count=failed_count,\n",
    "            total_count=len(df),\n",
    "            failed_indices=df[failed].index.tolist(),\n",
    "            severity=self.severity\n",
    "        )\n",
    "\n",
    "\n",
    "class ReferentialIntegrityRule(BusinessRule):\n",
    "    \"\"\"Validates foreign key references exist in parent table.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, fk_column: str, reference_values: set,\n",
    "                 severity: str = \"ERROR\"):\n",
    "        super().__init__(name, severity)\n",
    "        self.fk_column = fk_column\n",
    "        self.reference_values = reference_values\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        non_null = df[self.fk_column].dropna()\n",
    "        invalid = ~non_null.isin(self.reference_values)\n",
    "        failed_count = invalid.sum()\n",
    "        \n",
    "        return ValidationResult(\n",
    "            is_valid=failed_count == 0,\n",
    "            rule_name=self.name,\n",
    "            column=self.fk_column,\n",
    "            message=f\"{failed_count} values don't exist in reference table\",\n",
    "            failed_count=failed_count,\n",
    "            total_count=len(non_null),\n",
    "            severity=self.severity\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessRuleValidator:\n",
    "    \"\"\"Orchestrates multiple business rule validations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules: List[BusinessRule] = []\n",
    "    \n",
    "    def add_rule(self, rule: BusinessRule) -> 'BusinessRuleValidator':\n",
    "        \"\"\"Add a rule (fluent interface).\"\"\"\n",
    "        self.rules.append(rule)\n",
    "        return self\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> ValidationReport:\n",
    "        \"\"\"Execute all business rules.\"\"\"\n",
    "        results = [rule.validate(df) for rule in self.rules]\n",
    "        return ValidationReport(results=results)\n",
    "\n",
    "\n",
    "# Example: Business Rules Validation\n",
    "order_data = pd.DataFrame({\n",
    "    \"order_id\": [\"O001\", \"O002\", \"O003\", \"O004\", \"O005\"],\n",
    "    \"order_date\": pd.to_datetime([\"2024-01-01\", \"2024-02-01\", \"2024-03-01\", \"2024-04-01\", \"2024-05-01\"]),\n",
    "    \"ship_date\": pd.to_datetime([\"2024-01-05\", \"2024-01-15\", \"2024-02-28\", None, \"2024-05-10\"]),\n",
    "    \"status\": [\"shipped\", \"shipped\", \"pending\", \"shipped\", \"delivered\"],\n",
    "    \"subtotal\": [100.0, 200.0, 150.0, 300.0, 250.0],\n",
    "    \"tax\": [8.0, 16.0, 12.0, 24.0, 20.0],\n",
    "    \"shipping\": [10.0, 15.0, 12.0, 20.0, 18.0],\n",
    "    \"total\": [118.0, 230.0, 174.0, 344.0, 288.0],  # O002 is wrong (should be 231)\n",
    "    \"product_id\": [\"P001\", \"P002\", \"P003\", \"P999\", \"P001\"]  # P999 doesn't exist\n",
    "})\n",
    "\n",
    "# Valid product IDs (simulating lookup table)\n",
    "valid_products = {\"P001\", \"P002\", \"P003\", \"P004\", \"P005\"}\n",
    "\n",
    "# Define business rules\n",
    "br_validator = BusinessRuleValidator()\n",
    "br_validator.add_rule(\n",
    "    CrossFieldRule(\n",
    "        name=\"ship_after_order\",\n",
    "        condition=lambda df: df[\"ship_date\"].isna() | (df[\"ship_date\"] >= df[\"order_date\"]),\n",
    "        message=\"Ship date must be on or after order date\"\n",
    "    )\n",
    ").add_rule(\n",
    "    ConditionalRequiredRule(\n",
    "        name=\"shipped_requires_ship_date\",\n",
    "        condition_field=\"status\",\n",
    "        condition_value=\"shipped\",\n",
    "        required_field=\"ship_date\"\n",
    "    )\n",
    ").add_rule(\n",
    "    SumValidationRule(\n",
    "        name=\"total_calculation\",\n",
    "        total_column=\"total\",\n",
    "        component_columns=[\"subtotal\", \"tax\", \"shipping\"],\n",
    "        tolerance=0.01\n",
    "    )\n",
    ").add_rule(\n",
    "    ReferentialIntegrityRule(\n",
    "        name=\"valid_product\",\n",
    "        fk_column=\"product_id\",\n",
    "        reference_values=valid_products\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run validation\n",
    "br_report = br_validator.validate(order_data)\n",
    "print(br_report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716ee5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Statistical Validation Tests\n",
    "\n",
    "Statistical tests detect distribution shifts, outliers, and data drift that schema/business rules can't catch. These are essential for maintaining data quality over time.\n",
    "\n",
    "### Key Statistical Tests\n",
    "- **Distribution Tests**: Kolmogorov-Smirnov, Chi-Square for categorical\n",
    "- **Outlier Detection**: Z-score, IQR, Modified Z-score\n",
    "- **Data Drift**: Population Stability Index (PSI)\n",
    "- **Completeness**: Null rates, coverage metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "class StatisticalValidator:\n",
    "    \"\"\"Statistical validation tests for data quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_df: Optional[pd.DataFrame] = None):\n",
    "        \"\"\"\n",
    "        Initialize with optional reference data for comparison.\n",
    "        \n",
    "        Args:\n",
    "            reference_df: Baseline data for distribution comparison\n",
    "        \"\"\"\n",
    "        self.reference_df = reference_df\n",
    "        self.results: List[ValidationResult] = []\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame, columns: Optional[List[str]] = None) -> ValidationReport:\n",
    "        \"\"\"Run all statistical validations.\"\"\"\n",
    "        self.results = []\n",
    "        \n",
    "        cols = columns or df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                self._validate_column_stats(df, col)\n",
    "        \n",
    "        return ValidationReport(results=self.results)\n",
    "    \n",
    "    def _validate_column_stats(self, df: pd.DataFrame, column: str):\n",
    "        \"\"\"Run statistical tests on a single column.\"\"\"\n",
    "        col_data = df[column].dropna()\n",
    "        \n",
    "        if len(col_data) < 2:\n",
    "            return\n",
    "        \n",
    "        # Outlier detection\n",
    "        self._detect_outliers_zscore(col_data, column)\n",
    "        self._detect_outliers_iqr(col_data, column)\n",
    "        \n",
    "        # Distribution comparison (if reference exists)\n",
    "        if self.reference_df is not None and column in self.reference_df.columns:\n",
    "            self._compare_distributions(col_data, column)\n",
    "    \n",
    "    def _detect_outliers_zscore(self, data: pd.Series, column: str, \n",
    "                                 threshold: float = 3.0):\n",
    "        \"\"\"Detect outliers using Z-score method.\"\"\"\n",
    "        mean = data.mean()\n",
    "        std = data.std()\n",
    "        \n",
    "        if std == 0:\n",
    "            return\n",
    "        \n",
    "        z_scores = np.abs((data - mean) / std)\n",
    "        outliers = z_scores > threshold\n",
    "        outlier_count = outliers.sum()\n",
    "        outlier_pct = (outlier_count / len(data)) * 100\n",
    "        \n",
    "        # Warning if outliers exceed 5% of data\n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=outlier_pct <= 5.0,\n",
    "            rule_name=\"zscore_outliers\",\n",
    "            column=column,\n",
    "            message=f\"{outlier_count} outliers ({outlier_pct:.1f}%) detected (|Z| > {threshold})\",\n",
    "            failed_count=outlier_count,\n",
    "            total_count=len(data),\n",
    "            severity=\"WARNING\"\n",
    "        ))\n",
    "    \n",
    "    def _detect_outliers_iqr(self, data: pd.Series, column: str,\n",
    "                             multiplier: float = 1.5):\n",
    "        \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "        q1 = data.quantile(0.25)\n",
    "        q3 = data.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        lower_bound = q1 - multiplier * iqr\n",
    "        upper_bound = q3 + multiplier * iqr\n",
    "        \n",
    "        outliers = (data < lower_bound) | (data > upper_bound)\n",
    "        outlier_count = outliers.sum()\n",
    "        outlier_pct = (outlier_count / len(data)) * 100\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=outlier_pct <= 5.0,\n",
    "            rule_name=\"iqr_outliers\",\n",
    "            column=column,\n",
    "            message=f\"{outlier_count} outliers ({outlier_pct:.1f}%) outside [{lower_bound:.2f}, {upper_bound:.2f}]\",\n",
    "            failed_count=outlier_count,\n",
    "            total_count=len(data),\n",
    "            severity=\"WARNING\"\n",
    "        ))\n",
    "    \n",
    "    def _compare_distributions(self, data: pd.Series, column: str,\n",
    "                               alpha: float = 0.05):\n",
    "        \"\"\"Compare distribution with reference using KS test.\"\"\"\n",
    "        ref_data = self.reference_df[column].dropna()\n",
    "        \n",
    "        if len(ref_data) < 2:\n",
    "            return\n",
    "        \n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_stat, p_value = stats.ks_2samp(data, ref_data)\n",
    "        \n",
    "        self.results.append(ValidationResult(\n",
    "            is_valid=p_value >= alpha,\n",
    "            rule_name=\"distribution_drift\",\n",
    "            column=column,\n",
    "            message=f\"KS statistic: {ks_stat:.4f}, p-value: {p_value:.4f}\" + \n",
    "                    (\" - Distribution drift detected!\" if p_value < alpha else \" - No significant drift\"),\n",
    "            severity=\"WARNING\" if p_value >= alpha else \"ERROR\"\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_psi(expected: pd.Series, actual: pd.Series, \n",
    "                      buckets: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Population Stability Index (PSI).\n",
    "        \n",
    "        PSI < 0.1: No significant change\n",
    "        0.1 <= PSI < 0.2: Moderate change\n",
    "        PSI >= 0.2: Significant change\n",
    "        \"\"\"\n",
    "        # Create buckets based on expected distribution\n",
    "        breakpoints = np.percentile(expected.dropna(), \n",
    "                                    np.linspace(0, 100, buckets + 1))\n",
    "        breakpoints = np.unique(breakpoints)\n",
    "        \n",
    "        # Calculate proportions in each bucket\n",
    "        expected_counts = np.histogram(expected.dropna(), bins=breakpoints)[0]\n",
    "        actual_counts = np.histogram(actual.dropna(), bins=breakpoints)[0]\n",
    "        \n",
    "        # Normalize to proportions\n",
    "        expected_pct = expected_counts / expected_counts.sum()\n",
    "        actual_pct = actual_counts / actual_counts.sum()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        expected_pct = np.where(expected_pct == 0, 0.0001, expected_pct)\n",
    "        actual_pct = np.where(actual_pct == 0, 0.0001, actual_pct)\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "        \n",
    "        return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Statistical Validation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Reference/baseline data (training period)\n",
    "reference_data = pd.DataFrame({\n",
    "    \"transaction_amount\": np.random.lognormal(4, 1, 1000),\n",
    "    \"customer_age\": np.random.normal(35, 10, 1000),\n",
    "    \"purchase_frequency\": np.random.poisson(5, 1000)\n",
    "})\n",
    "\n",
    "# Current data with some drift\n",
    "current_data = pd.DataFrame({\n",
    "    \"transaction_amount\": np.concatenate([\n",
    "        np.random.lognormal(4, 1, 900),\n",
    "        np.random.lognormal(6, 1, 100)  # 10% shifted distribution\n",
    "    ]),\n",
    "    \"customer_age\": np.random.normal(40, 12, 1000),  # Mean shift\n",
    "    \"purchase_frequency\": np.random.poisson(5, 1000)  # No change\n",
    "})\n",
    "\n",
    "# Add some outliers\n",
    "current_data.loc[0, \"transaction_amount\"] = 50000  # Extreme outlier\n",
    "current_data.loc[1, \"transaction_amount\"] = 45000\n",
    "\n",
    "# Run statistical validation\n",
    "stat_validator = StatisticalValidator(reference_df=reference_data)\n",
    "stat_report = stat_validator.validate(current_data)\n",
    "print(stat_report.summary())\n",
    "\n",
    "# Calculate PSI for transaction amounts\n",
    "psi = StatisticalValidator.calculate_psi(\n",
    "    reference_data[\"transaction_amount\"],\n",
    "    current_data[\"transaction_amount\"]\n",
    ")\n",
    "print(f\"\\nPSI for transaction_amount: {psi:.4f}\")\n",
    "print(f\"Interpretation: {'No significant change' if psi < 0.1 else 'Moderate change' if psi < 0.2 else 'Significant change!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d9029",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Anomaly Detection for Data Quality\n",
    "\n",
    "Anomaly detection identifies unusual patterns that may indicate data quality issues, fraud, or system problems. Unlike rule-based validation, anomaly detection learns patterns from data.\n",
    "\n",
    "### Anomaly Detection Approaches\n",
    "- **Univariate**: Extreme values in single columns\n",
    "- **Multivariate**: Unusual combinations of values\n",
    "- **Time-series**: Temporal anomalies, seasonality violations\n",
    "- **Categorical**: Rare category occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Multi-method anomaly detection for data quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, contamination: float = 0.05):\n",
    "        \"\"\"\n",
    "        Initialize anomaly detector.\n",
    "        \n",
    "        Args:\n",
    "            contamination: Expected proportion of anomalies (0-0.5)\n",
    "        \"\"\"\n",
    "        self.contamination = contamination\n",
    "        self.scaler = StandardScaler()\n",
    "        self.isolation_forest = None\n",
    "        self.lof = None\n",
    "    \n",
    "    def detect_isolation_forest(self, df: pd.DataFrame, \n",
    "                                 columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Detect anomalies using Isolation Forest.\n",
    "        \n",
    "        Isolation Forest works by randomly selecting features and split values,\n",
    "        isolating observations. Anomalies are isolated quickly (short path length).\n",
    "        \"\"\"\n",
    "        data = df[columns].dropna()\n",
    "        scaled_data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        self.isolation_forest = IsolationForest(\n",
    "            contamination=self.contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        # Predict: -1 for anomalies, 1 for normal\n",
    "        predictions = self.isolation_forest.fit_predict(scaled_data)\n",
    "        scores = self.isolation_forest.decision_function(scaled_data)\n",
    "        \n",
    "        result = df.loc[data.index].copy()\n",
    "        result['is_anomaly_if'] = predictions == -1\n",
    "        result['anomaly_score_if'] = -scores  # Higher = more anomalous\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def detect_lof(self, df: pd.DataFrame, columns: List[str],\n",
    "                   n_neighbors: int = 20) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Detect anomalies using Local Outlier Factor.\n",
    "        \n",
    "        LOF measures local density deviation. Points with substantially lower\n",
    "        density than neighbors are considered outliers.\n",
    "        \"\"\"\n",
    "        data = df[columns].dropna()\n",
    "        scaled_data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        self.lof = LocalOutlierFactor(\n",
    "            n_neighbors=n_neighbors,\n",
    "            contamination=self.contamination\n",
    "        )\n",
    "        \n",
    "        predictions = self.lof.fit_predict(scaled_data)\n",
    "        scores = -self.lof.negative_outlier_factor_\n",
    "        \n",
    "        result = df.loc[data.index].copy()\n",
    "        result['is_anomaly_lof'] = predictions == -1\n",
    "        result['anomaly_score_lof'] = scores\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def detect_mahalanobis(self, df: pd.DataFrame, \n",
    "                           columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Detect anomalies using Mahalanobis distance.\n",
    "        \n",
    "        Mahalanobis distance accounts for correlations between variables\n",
    "        and variable scales.\n",
    "        \"\"\"\n",
    "        data = df[columns].dropna()\n",
    "        \n",
    "        # Calculate mean and covariance\n",
    "        mean = data.mean()\n",
    "        cov = data.cov()\n",
    "        \n",
    "        try:\n",
    "            cov_inv = np.linalg.inv(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Use pseudo-inverse if singular\n",
    "            cov_inv = np.linalg.pinv(cov)\n",
    "        \n",
    "        # Calculate Mahalanobis distance for each row\n",
    "        diff = data - mean\n",
    "        mahal_dist = np.sqrt(\n",
    "            np.sum(np.dot(diff, cov_inv) * diff, axis=1)\n",
    "        )\n",
    "        \n",
    "        # Chi-square threshold (p < 0.001)\n",
    "        threshold = np.sqrt(stats.chi2.ppf(0.999, df=len(columns)))\n",
    "        \n",
    "        result = df.loc[data.index].copy()\n",
    "        result['mahalanobis_dist'] = mahal_dist\n",
    "        result['is_anomaly_mahal'] = mahal_dist > threshold\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def ensemble_detection(self, df: pd.DataFrame, \n",
    "                           columns: List[str],\n",
    "                           min_votes: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ensemble anomaly detection combining multiple methods.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            columns: Columns to use for detection\n",
    "            min_votes: Minimum methods that must flag as anomaly\n",
    "        \"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Run all methods\n",
    "        if_result = self.detect_isolation_forest(df, columns)\n",
    "        lof_result = self.detect_lof(df, columns)\n",
    "        mahal_result = self.detect_mahalanobis(df, columns)\n",
    "        \n",
    "        # Combine results\n",
    "        valid_idx = if_result.index\n",
    "        result = result.loc[valid_idx].copy()\n",
    "        \n",
    "        result['votes_anomaly'] = (\n",
    "            if_result['is_anomaly_if'].astype(int) +\n",
    "            lof_result['is_anomaly_lof'].astype(int) +\n",
    "            mahal_result['is_anomaly_mahal'].astype(int)\n",
    "        )\n",
    "        \n",
    "        result['is_anomaly_ensemble'] = result['votes_anomaly'] >= min_votes\n",
    "        result['anomaly_score_if'] = if_result['anomaly_score_if']\n",
    "        result['anomaly_score_lof'] = lof_result['anomaly_score_lof']\n",
    "        result['mahalanobis_dist'] = mahal_result['mahalanobis_dist']\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa846ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Anomaly Detection\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate normal transaction data\n",
    "n_normal = 950\n",
    "normal_data = pd.DataFrame({\n",
    "    \"amount\": np.random.lognormal(4, 0.5, n_normal),\n",
    "    \"frequency\": np.random.poisson(3, n_normal),\n",
    "    \"avg_basket_size\": np.random.normal(5, 1.5, n_normal)\n",
    "})\n",
    "\n",
    "# Add anomalies\n",
    "n_anomalies = 50\n",
    "anomaly_data = pd.DataFrame({\n",
    "    \"amount\": np.random.lognormal(6, 0.8, n_anomalies),  # Higher amounts\n",
    "    \"frequency\": np.random.poisson(15, n_anomalies),     # Higher frequency\n",
    "    \"avg_basket_size\": np.random.normal(2, 0.5, n_anomalies)  # Lower basket\n",
    "})\n",
    "\n",
    "# Combine and shuffle\n",
    "full_data = pd.concat([normal_data, anomaly_data], ignore_index=True)\n",
    "full_data = full_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# True labels (for evaluation)\n",
    "true_labels = np.concatenate([np.zeros(n_normal), np.ones(n_anomalies)])\n",
    "\n",
    "# Run ensemble detection\n",
    "detector = AnomalyDetector(contamination=0.05)\n",
    "results = detector.ensemble_detection(\n",
    "    full_data, \n",
    "    columns=[\"amount\", \"frequency\", \"avg_basket_size\"]\n",
    ")\n",
    "\n",
    "# Display top anomalies\n",
    "anomalies = results[results['is_anomaly_ensemble']].sort_values(\n",
    "    'votes_anomaly', ascending=False\n",
    ")\n",
    "print(f\"Detected {len(anomalies)} anomalies using ensemble method\\n\")\n",
    "print(\"Top 10 anomalies:\")\n",
    "display(anomalies.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaab232",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pipeline Integration\n",
    "\n",
    "Integrating validation into data pipelines ensures quality checks happen automatically at critical points. Here we build a complete validation pipeline framework.\n",
    "\n",
    "### Integration Patterns\n",
    "- **Gate Validation**: Block pipeline if validation fails\n",
    "- **Alert Validation**: Continue but notify on issues\n",
    "- **Quarantine Pattern**: Route bad records to separate storage\n",
    "- **Enrichment Pattern**: Add validation metadata to records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea352483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ValidationAction(Enum):\n",
    "    \"\"\"Actions to take on validation failure.\"\"\"\n",
    "    GATE = \"gate\"          # Stop pipeline\n",
    "    ALERT = \"alert\"        # Continue with alerts\n",
    "    QUARANTINE = \"quarantine\"  # Separate bad records\n",
    "    ENRICH = \"enrich\"      # Add metadata and continue\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationStage:\n",
    "    \"\"\"A stage in the validation pipeline.\"\"\"\n",
    "    name: str\n",
    "    validator: Any  # SchemaValidator, BusinessRuleValidator, etc.\n",
    "    action: ValidationAction = ValidationAction.GATE\n",
    "    threshold: float = 1.0  # Pass rate threshold (0-1)\n",
    "    \n",
    "\n",
    "class DataValidationPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive data validation pipeline.\n",
    "    \n",
    "    Combines schema, business rule, and statistical validation\n",
    "    with configurable actions on failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.stages: List[ValidationStage] = []\n",
    "        self.alerts: List[str] = []\n",
    "        self.quarantine_data: Optional[pd.DataFrame] = None\n",
    "    \n",
    "    def add_stage(self, stage: ValidationStage) -> 'DataValidationPipeline':\n",
    "        \"\"\"Add a validation stage.\"\"\"\n",
    "        self.stages.append(stage)\n",
    "        return self\n",
    "    \n",
    "    def run(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, ValidationReport]]:\n",
    "        \"\"\"\n",
    "        Execute the validation pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (processed dataframe, dict of reports by stage)\n",
    "        \"\"\"\n",
    "        self.alerts = []\n",
    "        self.quarantine_data = pd.DataFrame()\n",
    "        reports = {}\n",
    "        current_df = df.copy()\n",
    "        \n",
    "        for stage in self.stages:\n",
    "            print(f\"\\n▶ Running stage: {stage.name}\")\n",
    "            \n",
    "            # Run validation\n",
    "            report = stage.validator.validate(current_df)\n",
    "            reports[stage.name] = report\n",
    "            \n",
    "            # Calculate pass rate\n",
    "            pass_rate = self._calculate_pass_rate(report)\n",
    "            \n",
    "            # Handle based on action type\n",
    "            if pass_rate < stage.threshold:\n",
    "                current_df = self._handle_failure(\n",
    "                    stage, report, current_df, pass_rate\n",
    "                )\n",
    "                \n",
    "                if current_df is None:\n",
    "                    # Pipeline halted\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"  ✓ Passed ({pass_rate:.1%} >= {stage.threshold:.1%})\")\n",
    "        \n",
    "        return current_df, reports\n",
    "    \n",
    "    def _calculate_pass_rate(self, report: ValidationReport) -> float:\n",
    "        \"\"\"Calculate overall pass rate from report.\"\"\"\n",
    "        if not report.results:\n",
    "            return 1.0\n",
    "        \n",
    "        passed = sum(1 for r in report.results if r.is_valid)\n",
    "        return passed / len(report.results)\n",
    "    \n",
    "    def _handle_failure(self, stage: ValidationStage, report: ValidationReport,\n",
    "                        df: pd.DataFrame, pass_rate: float) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Handle validation failure based on action type.\"\"\"\n",
    "        \n",
    "        if stage.action == ValidationAction.GATE:\n",
    "            print(f\"  ✗ GATE - Pipeline stopped ({pass_rate:.1%} < {stage.threshold:.1%})\")\n",
    "            self.alerts.append(f\"Pipeline halted at {stage.name}\")\n",
    "            return None\n",
    "        \n",
    "        elif stage.action == ValidationAction.ALERT:\n",
    "            alert = f\"Validation issues in {stage.name}: {report.error_count} errors, {report.warning_count} warnings\"\n",
    "            self.alerts.append(alert)\n",
    "            print(f\"  ⚠ ALERT - {alert}\")\n",
    "            return df\n",
    "        \n",
    "        elif stage.action == ValidationAction.QUARANTINE:\n",
    "            # Collect failed row indices\n",
    "            failed_indices = set()\n",
    "            for result in report.results:\n",
    "                if not result.is_valid:\n",
    "                    failed_indices.update(result.failed_indices)\n",
    "            \n",
    "            if failed_indices:\n",
    "                quarantine = df.loc[list(failed_indices)].copy()\n",
    "                quarantine['_quarantine_reason'] = stage.name\n",
    "                quarantine['_quarantine_time'] = datetime.now()\n",
    "                self.quarantine_data = pd.concat([self.quarantine_data, quarantine])\n",
    "                \n",
    "                # Remove quarantined rows\n",
    "                df = df.drop(index=list(failed_indices), errors='ignore')\n",
    "                print(f\"  ⚡ QUARANTINE - {len(failed_indices)} rows moved to quarantine\")\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        elif stage.action == ValidationAction.ENRICH:\n",
    "            # Add validation metadata\n",
    "            df['_validation_stage'] = stage.name\n",
    "            df['_validation_passed'] = pass_rate >= stage.threshold\n",
    "            df['_validation_pass_rate'] = pass_rate\n",
    "            df['_validation_time'] = datetime.now()\n",
    "            print(f\"  📝 ENRICH - Added validation metadata\")\n",
    "            return df\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_quarantined(self) -> pd.DataFrame:\n",
    "        \"\"\"Get all quarantined records.\"\"\"\n",
    "        return self.quarantine_data\n",
    "    \n",
    "    def get_alerts(self) -> List[str]:\n",
    "        \"\"\"Get all alerts generated.\"\"\"\n",
    "        return self.alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7638e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete Validation Pipeline\n",
    "\n",
    "# Define schemas\n",
    "transaction_schema = TableSchema(\n",
    "    name=\"transactions\",\n",
    "    columns=[\n",
    "        ColumnSchema(\"transaction_id\", DataType.STRING, nullable=False, unique=True),\n",
    "        ColumnSchema(\"amount\", DataType.FLOAT, nullable=False, min_value=0),\n",
    "        ColumnSchema(\"customer_id\", DataType.STRING, nullable=False),\n",
    "        ColumnSchema(\"category\", DataType.STRING, allowed_values=[\"retail\", \"food\", \"travel\", \"other\"]),\n",
    "    ],\n",
    "    primary_key=[\"transaction_id\"]\n",
    ")\n",
    "\n",
    "# Create sample data with issues\n",
    "transactions = pd.DataFrame({\n",
    "    \"transaction_id\": [\"T001\", \"T002\", \"T003\", \"T004\", \"T005\", \"T006\", \"T002\"],  # Duplicate\n",
    "    \"amount\": [100.0, 250.0, -50.0, 1000.0, 75.0, 50000.0, 150.0],  # Negative and outlier\n",
    "    \"customer_id\": [\"C001\", \"C002\", None, \"C004\", \"C005\", \"C006\", \"C002\"],  # Null\n",
    "    \"category\": [\"retail\", \"food\", \"travel\", \"invalid\", \"other\", \"retail\", \"food\"]  # Invalid\n",
    "})\n",
    "\n",
    "# Business rules\n",
    "br_validator = BusinessRuleValidator()\n",
    "br_validator.add_rule(\n",
    "    CrossFieldRule(\n",
    "        name=\"high_value_check\",\n",
    "        condition=lambda df: df[\"amount\"] < 10000,\n",
    "        message=\"Amounts over $10,000 require review\",\n",
    "        severity=\"WARNING\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = DataValidationPipeline(\"transaction_validation\")\n",
    "pipeline.add_stage(ValidationStage(\n",
    "    name=\"schema_validation\",\n",
    "    validator=SchemaValidator(transaction_schema),\n",
    "    action=ValidationAction.QUARANTINE,\n",
    "    threshold=0.8\n",
    ")).add_stage(ValidationStage(\n",
    "    name=\"business_rules\",\n",
    "    validator=br_validator,\n",
    "    action=ValidationAction.ALERT,\n",
    "    threshold=0.9\n",
    "))\n",
    "\n",
    "# Run pipeline\n",
    "result_df, reports = pipeline.run(transactions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nInput rows: {len(transactions)}\")\n",
    "print(f\"Output rows: {len(result_df) if result_df is not None else 0}\")\n",
    "print(f\"Quarantined rows: {len(pipeline.get_quarantined())}\")\n",
    "print(f\"Alerts: {len(pipeline.get_alerts())}\")\n",
    "\n",
    "if len(pipeline.get_quarantined()) > 0:\n",
    "    print(\"\\nQuarantined records:\")\n",
    "    display(pipeline.get_quarantined())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5abe4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Great Expectations Integration\n",
    "\n",
    "For production systems, **Great Expectations** is the industry-standard library for data validation. Here's how to integrate it with custom validation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db34f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great Expectations-style expectation implementation\n",
    "# This mimics the GE API for those familiar with the library\n",
    "\n",
    "class ExpectationSuite:\n",
    "    \"\"\"\n",
    "    Lightweight Great Expectations-style validation suite.\n",
    "    \n",
    "    In production, use the actual great_expectations library:\n",
    "    pip install great_expectations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.expectations: List[Dict] = []\n",
    "    \n",
    "    def expect_column_to_exist(self, column: str):\n",
    "        self.expectations.append({\n",
    "            \"expectation_type\": \"expect_column_to_exist\",\n",
    "            \"kwargs\": {\"column\": column}\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_to_not_be_null(self, column: str):\n",
    "        self.expectations.append({\n",
    "            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "            \"kwargs\": {\"column\": column}\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_to_be_between(self, column: str, \n",
    "                                           min_value: float = None, \n",
    "                                           max_value: float = None):\n",
    "        self.expectations.append({\n",
    "            \"expectation_type\": \"expect_column_values_to_be_between\",\n",
    "            \"kwargs\": {\"column\": column, \"min_value\": min_value, \"max_value\": max_value}\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_to_be_in_set(self, column: str, value_set: List):\n",
    "        self.expectations.append({\n",
    "            \"expectation_type\": \"expect_column_values_to_be_in_set\",\n",
    "            \"kwargs\": {\"column\": column, \"value_set\": value_set}\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_to_be_unique(self, column: str):\n",
    "        self.expectations.append({\n",
    "            \"expectation_type\": \"expect_column_values_to_be_unique\",\n",
    "            \"kwargs\": {\"column\": column}\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Run all expectations against dataframe.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for exp in self.expectations:\n",
    "            result = self._evaluate_expectation(df, exp)\n",
    "            results.append(result)\n",
    "        \n",
    "        return {\n",
    "            \"success\": all(r[\"success\"] for r in results),\n",
    "            \"results\": results,\n",
    "            \"statistics\": {\n",
    "                \"evaluated_expectations\": len(results),\n",
    "                \"successful_expectations\": sum(1 for r in results if r[\"success\"]),\n",
    "                \"unsuccessful_expectations\": sum(1 for r in results if not r[\"success\"])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _evaluate_expectation(self, df: pd.DataFrame, exp: Dict) -> Dict:\n",
    "        \"\"\"Evaluate a single expectation.\"\"\"\n",
    "        exp_type = exp[\"expectation_type\"]\n",
    "        kwargs = exp[\"kwargs\"]\n",
    "        \n",
    "        evaluators = {\n",
    "            \"expect_column_to_exist\": self._eval_column_exists,\n",
    "            \"expect_column_values_to_not_be_null\": self._eval_not_null,\n",
    "            \"expect_column_values_to_be_between\": self._eval_between,\n",
    "            \"expect_column_values_to_be_in_set\": self._eval_in_set,\n",
    "            \"expect_column_values_to_be_unique\": self._eval_unique,\n",
    "        }\n",
    "        \n",
    "        evaluator = evaluators.get(exp_type)\n",
    "        if evaluator:\n",
    "            return evaluator(df, kwargs)\n",
    "        \n",
    "        return {\"success\": False, \"message\": f\"Unknown expectation: {exp_type}\"}\n",
    "    \n",
    "    def _eval_column_exists(self, df: pd.DataFrame, kwargs: Dict) -> Dict:\n",
    "        col = kwargs[\"column\"]\n",
    "        exists = col in df.columns\n",
    "        return {\n",
    "            \"expectation_type\": \"expect_column_to_exist\",\n",
    "            \"success\": exists,\n",
    "            \"result\": {\"observed_value\": list(df.columns)}\n",
    "        }\n",
    "    \n",
    "    def _eval_not_null(self, df: pd.DataFrame, kwargs: Dict) -> Dict:\n",
    "        col = kwargs[\"column\"]\n",
    "        null_count = df[col].isna().sum()\n",
    "        return {\n",
    "            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "            \"success\": null_count == 0,\n",
    "            \"result\": {\"unexpected_count\": int(null_count)}\n",
    "        }\n",
    "    \n",
    "    def _eval_between(self, df: pd.DataFrame, kwargs: Dict) -> Dict:\n",
    "        col = kwargs[\"column\"]\n",
    "        min_val = kwargs.get(\"min_value\")\n",
    "        max_val = kwargs.get(\"max_value\")\n",
    "        \n",
    "        data = df[col].dropna()\n",
    "        failed = pd.Series([False] * len(data), index=data.index)\n",
    "        \n",
    "        if min_val is not None:\n",
    "            failed |= data < min_val\n",
    "        if max_val is not None:\n",
    "            failed |= data > max_val\n",
    "        \n",
    "        return {\n",
    "            \"expectation_type\": \"expect_column_values_to_be_between\",\n",
    "            \"success\": failed.sum() == 0,\n",
    "            \"result\": {\"unexpected_count\": int(failed.sum())}\n",
    "        }\n",
    "    \n",
    "    def _eval_in_set(self, df: pd.DataFrame, kwargs: Dict) -> Dict:\n",
    "        col = kwargs[\"column\"]\n",
    "        value_set = set(kwargs[\"value_set\"])\n",
    "        \n",
    "        data = df[col].dropna()\n",
    "        unexpected = ~data.isin(value_set)\n",
    "        \n",
    "        return {\n",
    "            \"expectation_type\": \"expect_column_values_to_be_in_set\",\n",
    "            \"success\": unexpected.sum() == 0,\n",
    "            \"result\": {\"unexpected_count\": int(unexpected.sum())}\n",
    "        }\n",
    "    \n",
    "    def _eval_unique(self, df: pd.DataFrame, kwargs: Dict) -> Dict:\n",
    "        col = kwargs[\"column\"]\n",
    "        duplicates = df[col].duplicated().sum()\n",
    "        \n",
    "        return {\n",
    "            \"expectation_type\": \"expect_column_values_to_be_unique\",\n",
    "            \"success\": duplicates == 0,\n",
    "            \"result\": {\"unexpected_count\": int(duplicates)}\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "suite = ExpectationSuite(\"customer_data_suite\")\n",
    "suite.expect_column_to_exist(\"customer_id\") \\\n",
    "     .expect_column_values_to_not_be_null(\"customer_id\") \\\n",
    "     .expect_column_values_to_be_unique(\"customer_id\") \\\n",
    "     .expect_column_values_to_be_between(\"age\", min_value=0, max_value=120) \\\n",
    "     .expect_column_values_to_be_in_set(\"status\", [\"active\", \"inactive\"])\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    \"customer_id\": [\"C001\", \"C002\", \"C001\", \"C004\"],  # Duplicate\n",
    "    \"age\": [25, 150, 35, -5],  # Out of range\n",
    "    \"status\": [\"active\", \"inactive\", \"pending\", \"active\"]  # Invalid value\n",
    "})\n",
    "\n",
    "validation_result = suite.validate(test_data)\n",
    "print(json.dumps(validation_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766be95d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Monitoring and Alerting\n",
    "\n",
    "Continuous monitoring tracks validation metrics over time, detecting degradation patterns and triggering alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf397f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationMetric:\n",
    "    \"\"\"A validation metric data point.\"\"\"\n",
    "    timestamp: datetime\n",
    "    metric_name: str\n",
    "    value: float\n",
    "    threshold: float\n",
    "    is_breach: bool\n",
    "    context: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class ValidationMonitor:\n",
    "    \"\"\"\n",
    "    Monitors validation metrics over time.\n",
    "    \n",
    "    Tracks:\n",
    "    - Pass rates by validation type\n",
    "    - Null rates per column\n",
    "    - Anomaly detection rates\n",
    "    - Schema conformance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_history: List[ValidationMetric] = []\n",
    "        self.thresholds: Dict[str, float] = {\n",
    "            \"schema_pass_rate\": 0.95,\n",
    "            \"business_rule_pass_rate\": 0.90,\n",
    "            \"null_rate\": 0.05,\n",
    "            \"anomaly_rate\": 0.10,\n",
    "            \"duplicate_rate\": 0.01\n",
    "        }\n",
    "    \n",
    "    def record_metric(self, metric_name: str, value: float, \n",
    "                      context: Dict = None) -> ValidationMetric:\n",
    "        \"\"\"Record a validation metric.\"\"\"\n",
    "        threshold = self.thresholds.get(metric_name, 1.0)\n",
    "        \n",
    "        # For pass rates, breach is when below threshold\n",
    "        # For rates (null, anomaly, duplicate), breach is when above threshold\n",
    "        if \"pass_rate\" in metric_name:\n",
    "            is_breach = value < threshold\n",
    "        else:\n",
    "            is_breach = value > threshold\n",
    "        \n",
    "        metric = ValidationMetric(\n",
    "            timestamp=datetime.now(),\n",
    "            metric_name=metric_name,\n",
    "            value=value,\n",
    "            threshold=threshold,\n",
    "            is_breach=is_breach,\n",
    "            context=context or {}\n",
    "        )\n",
    "        \n",
    "        self.metrics_history.append(metric)\n",
    "        return metric\n",
    "    \n",
    "    def record_from_report(self, report: ValidationReport, \n",
    "                           source: str = \"unknown\") -> List[ValidationMetric]:\n",
    "        \"\"\"Extract and record metrics from a validation report.\"\"\"\n",
    "        metrics = []\n",
    "        \n",
    "        # Overall pass rate\n",
    "        total = len(report.results)\n",
    "        passed = sum(1 for r in report.results if r.is_valid)\n",
    "        pass_rate = passed / total if total > 0 else 1.0\n",
    "        \n",
    "        metrics.append(self.record_metric(\n",
    "            \"schema_pass_rate\", pass_rate, {\"source\": source}\n",
    "        ))\n",
    "        \n",
    "        # Per-column null rates\n",
    "        for result in report.results:\n",
    "            if result.rule_name == \"not_null\" and result.column:\n",
    "                null_rate = result.failed_count / result.total_count if result.total_count > 0 else 0\n",
    "                metrics.append(self.record_metric(\n",
    "                    \"null_rate\", null_rate, \n",
    "                    {\"source\": source, \"column\": result.column}\n",
    "                ))\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_breaches(self, since: datetime = None) -> List[ValidationMetric]:\n",
    "        \"\"\"Get all threshold breaches.\"\"\"\n",
    "        breaches = [m for m in self.metrics_history if m.is_breach]\n",
    "        \n",
    "        if since:\n",
    "            breaches = [m for m in breaches if m.timestamp >= since]\n",
    "        \n",
    "        return breaches\n",
    "    \n",
    "    def get_trend(self, metric_name: str, periods: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Get trend data for a metric.\"\"\"\n",
    "        relevant = [m for m in self.metrics_history if m.metric_name == metric_name]\n",
    "        relevant = sorted(relevant, key=lambda x: x.timestamp)[-periods:]\n",
    "        \n",
    "        return pd.DataFrame([\n",
    "            {\n",
    "                \"timestamp\": m.timestamp,\n",
    "                \"value\": m.value,\n",
    "                \"threshold\": m.threshold,\n",
    "                \"is_breach\": m.is_breach\n",
    "            }\n",
    "            for m in relevant\n",
    "        ])\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Generate monitoring summary.\"\"\"\n",
    "        lines = [\n",
    "            \"\\n\" + \"=\"*60,\n",
    "            \"VALIDATION MONITORING SUMMARY\",\n",
    "            \"=\"*60\n",
    "        ]\n",
    "        \n",
    "        # Group by metric name\n",
    "        by_metric = {}\n",
    "        for m in self.metrics_history:\n",
    "            if m.metric_name not in by_metric:\n",
    "                by_metric[m.metric_name] = []\n",
    "            by_metric[m.metric_name].append(m)\n",
    "        \n",
    "        for metric_name, metrics in by_metric.items():\n",
    "            values = [m.value for m in metrics]\n",
    "            breaches = sum(1 for m in metrics if m.is_breach)\n",
    "            \n",
    "            lines.append(f\"\\n{metric_name}:\")\n",
    "            lines.append(f\"  Latest: {values[-1]:.3f}\")\n",
    "            lines.append(f\"  Average: {np.mean(values):.3f}\")\n",
    "            lines.append(f\"  Breaches: {breaches}/{len(metrics)}\")\n",
    "        \n",
    "        lines.append(\"\\n\" + \"=\"*60)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Example: Monitoring\n",
    "monitor = ValidationMonitor()\n",
    "\n",
    "# Simulate historical metrics\n",
    "for i in range(10):\n",
    "    # Simulate declining quality over time\n",
    "    pass_rate = 0.98 - (i * 0.01) + np.random.uniform(-0.02, 0.02)\n",
    "    null_rate = 0.02 + (i * 0.005) + np.random.uniform(-0.01, 0.01)\n",
    "    \n",
    "    monitor.record_metric(\"schema_pass_rate\", max(0, min(1, pass_rate)))\n",
    "    monitor.record_metric(\"null_rate\", max(0, null_rate))\n",
    "\n",
    "print(monitor.summary())\n",
    "\n",
    "# Check for breaches\n",
    "breaches = monitor.get_breaches()\n",
    "if breaches:\n",
    "    print(f\"\\n⚠ {len(breaches)} threshold breaches detected!\")\n",
    "    for b in breaches[-5:]:\n",
    "        print(f\"  - {b.metric_name}: {b.value:.3f} (threshold: {b.threshold})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15e867",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 Key Takeaways\n",
    "\n",
    "### Validation Strategy\n",
    "\n",
    "| Layer | Purpose | Tools |\n",
    "|-------|---------|-------|\n",
    "| **Schema** | Structure, types, constraints | Pydantic, JSON Schema, Custom |\n",
    "| **Business Rules** | Domain logic, cross-field | Custom rules, dbt tests |\n",
    "| **Statistical** | Distribution, outliers, drift | SciPy, Custom metrics |\n",
    "| **Anomaly** | Pattern detection | Isolation Forest, LOF, Mahalanobis |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Layer Your Validation**\n",
    "   - Start with fast schema checks\n",
    "   - Add business rules for domain logic\n",
    "   - Use statistical tests for drift detection\n",
    "\n",
    "2. **Choose Appropriate Actions**\n",
    "   - **GATE**: For critical quality issues that must block processing\n",
    "   - **ALERT**: For warnings that need attention but shouldn't halt pipelines\n",
    "   - **QUARANTINE**: For isolating bad records while processing good ones\n",
    "   - **ENRICH**: For adding metadata for downstream filtering\n",
    "\n",
    "3. **Monitor Continuously**\n",
    "   - Track validation metrics over time\n",
    "   - Set up alerts for threshold breaches\n",
    "   - Review trends to catch gradual degradation\n",
    "\n",
    "4. **Document Expectations**\n",
    "   - Use Great Expectations for self-documenting validation\n",
    "   - Generate data docs for stakeholder communication\n",
    "   - Version control your validation rules\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "```python\n",
    "# Recommended libraries for production\n",
    "# pip install great-expectations  # Comprehensive validation\n",
    "# pip install pandera             # DataFrame validation\n",
    "# pip install pydantic            # Schema validation\n",
    "# pip install evidently           # Data drift detection\n",
    "```\n",
    "\n",
    "### Validation Checklist\n",
    "\n",
    "- [ ] Schema validation for all ingested data\n",
    "- [ ] Business rules documented and tested\n",
    "- [ ] Statistical baselines established for drift detection\n",
    "- [ ] Anomaly detection for high-value fields\n",
    "- [ ] Quarantine process for bad records\n",
    "- [ ] Monitoring dashboards with alerts\n",
    "- [ ] Regular review of validation rules"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
