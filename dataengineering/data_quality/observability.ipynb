{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41171f1a",
   "metadata": {},
   "source": [
    "# Data Observability\n",
    "\n",
    "Data observability is the ability to understand the health and state of data in your system. It extends traditional software observability concepts to the data layer, enabling teams to identify, troubleshoot, and resolve data issues before they impact downstream consumers.\n",
    "\n",
    "## Why Data Observability Matters\n",
    "\n",
    "- **Trust**: Stakeholders need confidence that data is accurate and reliable\n",
    "- **Speed**: Quickly detect and resolve data issues before they propagate\n",
    "- **Scale**: Monitor thousands of tables and pipelines automatically\n",
    "- **Collaboration**: Bridge gaps between data engineers, analysts, and consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb752a5e",
   "metadata": {},
   "source": [
    "## The Five Pillars of Data Observability\n",
    "\n",
    "Data observability is built on five core pillars that together provide comprehensive visibility into data health:\n",
    "\n",
    "| Pillar | Description | Key Questions |\n",
    "|--------|-------------|---------------|\n",
    "| **Freshness** | Is the data up-to-date? | When was the table last updated? Is it within SLA? |\n",
    "| **Volume** | Is the data complete? | Are row counts within expected ranges? Any missing partitions? |\n",
    "| **Schema** | Has the structure changed? | Are there new/removed columns? Data type changes? |\n",
    "| **Distribution** | Are values within expected ranges? | Are there null spikes? Statistical anomalies? |\n",
    "| **Lineage** | Where does data come from and go? | What upstream/downstream dependencies exist? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504b3ea",
   "metadata": {},
   "source": [
    "## 1. Freshness Monitoring\n",
    "\n",
    "Freshness tracks when data was last updated. Stale data can lead to incorrect business decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70139a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class FreshnessCheck:\n",
    "    \"\"\"Monitor data freshness against SLA thresholds.\"\"\"\n",
    "    table_name: str\n",
    "    expected_update_frequency: timedelta\n",
    "    last_updated: datetime\n",
    "    \n",
    "    def is_fresh(self) -> bool:\n",
    "        \"\"\"Check if data is within freshness SLA.\"\"\"\n",
    "        age = datetime.now() - self.last_updated\n",
    "        return age <= self.expected_update_frequency\n",
    "    \n",
    "    def staleness_hours(self) -> float:\n",
    "        \"\"\"Return how many hours past SLA the data is.\"\"\"\n",
    "        age = datetime.now() - self.last_updated\n",
    "        overdue = age - self.expected_update_frequency\n",
    "        return max(0, overdue.total_seconds() / 3600)\n",
    "    \n",
    "    def status_report(self) -> dict:\n",
    "        \"\"\"Generate freshness status report.\"\"\"\n",
    "        return {\n",
    "            \"table\": self.table_name,\n",
    "            \"last_updated\": self.last_updated.isoformat(),\n",
    "            \"expected_frequency_hours\": self.expected_update_frequency.total_seconds() / 3600,\n",
    "            \"is_fresh\": self.is_fresh(),\n",
    "            \"staleness_hours\": self.staleness_hours()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "check = FreshnessCheck(\n",
    "    table_name=\"orders\",\n",
    "    expected_update_frequency=timedelta(hours=1),\n",
    "    last_updated=datetime.now() - timedelta(hours=2, minutes=30)\n",
    ")\n",
    "\n",
    "print(f\"Is fresh: {check.is_fresh()}\")\n",
    "print(f\"Staleness: {check.staleness_hours():.2f} hours past SLA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26a103",
   "metadata": {},
   "source": [
    "## 2. Volume Monitoring\n",
    "\n",
    "Volume monitoring ensures data completeness by tracking row counts and detecting anomalies like missing data or unexpected spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2171f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class VolumeMonitor:\n",
    "    \"\"\"Monitor data volume for anomalies using statistical methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, historical_counts: List[int], z_threshold: float = 3.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            historical_counts: List of historical row counts\n",
    "            z_threshold: Number of standard deviations for anomaly detection\n",
    "        \"\"\"\n",
    "        self.historical_counts = np.array(historical_counts)\n",
    "        self.z_threshold = z_threshold\n",
    "        self.mean = np.mean(self.historical_counts)\n",
    "        self.std = np.std(self.historical_counts)\n",
    "    \n",
    "    def check_volume(self, current_count: int) -> Tuple[bool, str]:\n",
    "        \"\"\"Check if current volume is within expected range.\"\"\"\n",
    "        if self.std == 0:\n",
    "            # Handle case with no variance\n",
    "            is_anomaly = current_count != self.mean\n",
    "            z_score = 0\n",
    "        else:\n",
    "            z_score = (current_count - self.mean) / self.std\n",
    "            is_anomaly = abs(z_score) > self.z_threshold\n",
    "        \n",
    "        if is_anomaly:\n",
    "            direction = \"high\" if z_score > 0 else \"low\"\n",
    "            message = f\"ANOMALY: Volume is unusually {direction} (z-score: {z_score:.2f})\"\n",
    "        else:\n",
    "            message = f\"Volume is within normal range (z-score: {z_score:.2f})\"\n",
    "        \n",
    "        return not is_anomaly, message\n",
    "    \n",
    "    def get_expected_range(self) -> Tuple[float, float]:\n",
    "        \"\"\"Return expected volume range based on threshold.\"\"\"\n",
    "        lower = self.mean - (self.z_threshold * self.std)\n",
    "        upper = self.mean + (self.z_threshold * self.std)\n",
    "        return max(0, lower), upper\n",
    "\n",
    "# Example: Historical daily row counts\n",
    "historical = [10000, 10200, 9800, 10100, 10050, 9900, 10300, 10150]\n",
    "monitor = VolumeMonitor(historical, z_threshold=2.5)\n",
    "\n",
    "# Test different scenarios\n",
    "test_counts = [10100, 5000, 15000, 0]\n",
    "for count in test_counts:\n",
    "    is_ok, message = monitor.check_volume(count)\n",
    "    print(f\"Count {count:,}: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf5c47",
   "metadata": {},
   "source": [
    "## 3. Schema Monitoring\n",
    "\n",
    "Schema changes can break downstream pipelines. Monitoring detects additions, removals, and type changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91653ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Set, List\n",
    "from enum import Enum\n",
    "\n",
    "class SchemaChangeType(Enum):\n",
    "    COLUMN_ADDED = \"column_added\"\n",
    "    COLUMN_REMOVED = \"column_removed\"\n",
    "    TYPE_CHANGED = \"type_changed\"\n",
    "    NULLABLE_CHANGED = \"nullable_changed\"\n",
    "\n",
    "@dataclass\n",
    "class ColumnInfo:\n",
    "    name: str\n",
    "    data_type: str\n",
    "    nullable: bool = True\n",
    "\n",
    "@dataclass\n",
    "class SchemaChange:\n",
    "    change_type: SchemaChangeType\n",
    "    column_name: str\n",
    "    old_value: str = None\n",
    "    new_value: str = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.change_type == SchemaChangeType.COLUMN_ADDED:\n",
    "            return f\"+ Column '{self.column_name}' added ({self.new_value})\"\n",
    "        elif self.change_type == SchemaChangeType.COLUMN_REMOVED:\n",
    "            return f\"- Column '{self.column_name}' removed\"\n",
    "        elif self.change_type == SchemaChangeType.TYPE_CHANGED:\n",
    "            return f\"~ Column '{self.column_name}' type: {self.old_value} -> {self.new_value}\"\n",
    "        elif self.change_type == SchemaChangeType.NULLABLE_CHANGED:\n",
    "            return f\"~ Column '{self.column_name}' nullable: {self.old_value} -> {self.new_value}\"\n",
    "\n",
    "class SchemaMonitor:\n",
    "    \"\"\"Detect and track schema changes between versions.\"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_schema: Dict[str, ColumnInfo]):\n",
    "        self.baseline = baseline_schema\n",
    "    \n",
    "    def compare(self, current_schema: Dict[str, ColumnInfo]) -> List[SchemaChange]:\n",
    "        \"\"\"Compare current schema against baseline.\"\"\"\n",
    "        changes = []\n",
    "        baseline_cols = set(self.baseline.keys())\n",
    "        current_cols = set(current_schema.keys())\n",
    "        \n",
    "        # Detect removed columns\n",
    "        for col in baseline_cols - current_cols:\n",
    "            changes.append(SchemaChange(\n",
    "                change_type=SchemaChangeType.COLUMN_REMOVED,\n",
    "                column_name=col\n",
    "            ))\n",
    "        \n",
    "        # Detect added columns\n",
    "        for col in current_cols - baseline_cols:\n",
    "            changes.append(SchemaChange(\n",
    "                change_type=SchemaChangeType.COLUMN_ADDED,\n",
    "                column_name=col,\n",
    "                new_value=current_schema[col].data_type\n",
    "            ))\n",
    "        \n",
    "        # Detect type/nullable changes\n",
    "        for col in baseline_cols & current_cols:\n",
    "            old = self.baseline[col]\n",
    "            new = current_schema[col]\n",
    "            \n",
    "            if old.data_type != new.data_type:\n",
    "                changes.append(SchemaChange(\n",
    "                    change_type=SchemaChangeType.TYPE_CHANGED,\n",
    "                    column_name=col,\n",
    "                    old_value=old.data_type,\n",
    "                    new_value=new.data_type\n",
    "                ))\n",
    "            \n",
    "            if old.nullable != new.nullable:\n",
    "                changes.append(SchemaChange(\n",
    "                    change_type=SchemaChangeType.NULLABLE_CHANGED,\n",
    "                    column_name=col,\n",
    "                    old_value=str(old.nullable),\n",
    "                    new_value=str(new.nullable)\n",
    "                ))\n",
    "        \n",
    "        return changes\n",
    "\n",
    "# Example usage\n",
    "baseline = {\n",
    "    \"id\": ColumnInfo(\"id\", \"INTEGER\", nullable=False),\n",
    "    \"name\": ColumnInfo(\"name\", \"VARCHAR(100)\"),\n",
    "    \"email\": ColumnInfo(\"email\", \"VARCHAR(255)\"),\n",
    "    \"created_at\": ColumnInfo(\"created_at\", \"TIMESTAMP\")\n",
    "}\n",
    "\n",
    "current = {\n",
    "    \"id\": ColumnInfo(\"id\", \"BIGINT\", nullable=False),  # Type changed\n",
    "    \"name\": ColumnInfo(\"name\", \"VARCHAR(100)\"),\n",
    "    \"phone\": ColumnInfo(\"phone\", \"VARCHAR(20)\"),  # Added\n",
    "    \"created_at\": ColumnInfo(\"created_at\", \"TIMESTAMP\")\n",
    "    # email removed\n",
    "}\n",
    "\n",
    "monitor = SchemaMonitor(baseline)\n",
    "changes = monitor.compare(current)\n",
    "\n",
    "print(\"Schema Changes Detected:\")\n",
    "print(\"-\" * 40)\n",
    "for change in changes:\n",
    "    print(change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b6970",
   "metadata": {},
   "source": [
    "## 4. Distribution Monitoring\n",
    "\n",
    "Distribution monitoring tracks statistical properties of data values to detect drift, null rate changes, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc48be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DistributionMonitor:\n",
    "    \"\"\"Monitor data distribution for drift and anomalies.\"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_data: pd.Series):\n",
    "        self.baseline = baseline_data.dropna()\n",
    "        self.baseline_stats = self._compute_stats(self.baseline)\n",
    "    \n",
    "    def _compute_stats(self, data: pd.Series) -> dict:\n",
    "        \"\"\"Compute statistical summary.\"\"\"\n",
    "        return {\n",
    "            \"mean\": data.mean(),\n",
    "            \"std\": data.std(),\n",
    "            \"min\": data.min(),\n",
    "            \"max\": data.max(),\n",
    "            \"median\": data.median(),\n",
    "            \"null_rate\": 1 - (len(data) / len(self.baseline)),\n",
    "            \"unique_count\": data.nunique()\n",
    "        }\n",
    "    \n",
    "    def check_distribution(self, current_data: pd.Series, \n",
    "                          ks_threshold: float = 0.05) -> dict:\n",
    "        \"\"\"\n",
    "        Check current data against baseline distribution.\n",
    "        Uses Kolmogorov-Smirnov test for distribution comparison.\n",
    "        \"\"\"\n",
    "        current = current_data.dropna()\n",
    "        current_stats = self._compute_stats(current)\n",
    "        \n",
    "        # Perform KS test\n",
    "        ks_statistic, p_value = stats.ks_2samp(self.baseline, current)\n",
    "        \n",
    "        # Check for significant drift\n",
    "        has_drift = p_value < ks_threshold\n",
    "        \n",
    "        # Check null rate change\n",
    "        original_null_rate = 1 - (len(current) / len(current_data))\n",
    "        baseline_null_rate = 1 - (len(self.baseline) / len(self.baseline))\n",
    "        null_rate_change = abs(original_null_rate - baseline_null_rate)\n",
    "        \n",
    "        return {\n",
    "            \"has_drift\": has_drift,\n",
    "            \"ks_statistic\": ks_statistic,\n",
    "            \"p_value\": p_value,\n",
    "            \"baseline_stats\": self.baseline_stats,\n",
    "            \"current_stats\": current_stats,\n",
    "            \"null_rate_change\": null_rate_change,\n",
    "            \"mean_shift\": current_stats[\"mean\"] - self.baseline_stats[\"mean\"]\n",
    "        }\n",
    "\n",
    "# Example: Monitoring price distribution\n",
    "np.random.seed(42)\n",
    "\n",
    "# Baseline: Normal distribution around $100\n",
    "baseline_prices = pd.Series(np.random.normal(100, 15, 1000))\n",
    "\n",
    "# Current: Shifted distribution (prices increased)\n",
    "current_prices = pd.Series(np.random.normal(115, 15, 1000))\n",
    "\n",
    "monitor = DistributionMonitor(baseline_prices)\n",
    "result = monitor.check_distribution(current_prices)\n",
    "\n",
    "print(\"Distribution Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Drift Detected: {result['has_drift']}\")\n",
    "print(f\"KS Statistic: {result['ks_statistic']:.4f}\")\n",
    "print(f\"P-Value: {result['p_value']:.4e}\")\n",
    "print(f\"Mean Shift: ${result['mean_shift']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d4f00",
   "metadata": {},
   "source": [
    "## 5. Data Lineage\n",
    "\n",
    "Lineage tracks data flow from source to destination, enabling impact analysis and root cause identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Set, List, Tuple\n",
    "\n",
    "class DataLineage:\n",
    "    \"\"\"Track data lineage relationships between datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.upstream: Dict[str, Set[str]] = defaultdict(set)  # table -> sources\n",
    "        self.downstream: Dict[str, Set[str]] = defaultdict(set)  # table -> consumers\n",
    "        self.metadata: Dict[str, dict] = {}  # table -> metadata\n",
    "    \n",
    "    def add_table(self, table: str, metadata: dict = None):\n",
    "        \"\"\"Register a table with optional metadata.\"\"\"\n",
    "        if table not in self.upstream:\n",
    "            self.upstream[table] = set()\n",
    "        if metadata:\n",
    "            self.metadata[table] = metadata\n",
    "    \n",
    "    def add_lineage(self, source: str, target: str):\n",
    "        \"\"\"Add lineage relationship: source -> target.\"\"\"\n",
    "        self.upstream[target].add(source)\n",
    "        self.downstream[source].add(target)\n",
    "    \n",
    "    def get_upstream(self, table: str, recursive: bool = True) -> Set[str]:\n",
    "        \"\"\"Get all upstream dependencies.\"\"\"\n",
    "        if not recursive:\n",
    "            return self.upstream.get(table, set())\n",
    "        \n",
    "        visited = set()\n",
    "        to_visit = list(self.upstream.get(table, set()))\n",
    "        \n",
    "        while to_visit:\n",
    "            current = to_visit.pop()\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                to_visit.extend(self.upstream.get(current, set()))\n",
    "        \n",
    "        return visited\n",
    "    \n",
    "    def get_downstream(self, table: str, recursive: bool = True) -> Set[str]:\n",
    "        \"\"\"Get all downstream dependents.\"\"\"\n",
    "        if not recursive:\n",
    "            return self.downstream.get(table, set())\n",
    "        \n",
    "        visited = set()\n",
    "        to_visit = list(self.downstream.get(table, set()))\n",
    "        \n",
    "        while to_visit:\n",
    "            current = to_visit.pop()\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                to_visit.extend(self.downstream.get(current, set()))\n",
    "        \n",
    "        return visited\n",
    "    \n",
    "    def impact_analysis(self, table: str) -> dict:\n",
    "        \"\"\"Analyze impact of changes to a table.\"\"\"\n",
    "        downstream = self.get_downstream(table)\n",
    "        return {\n",
    "            \"table\": table,\n",
    "            \"direct_dependents\": list(self.downstream.get(table, set())),\n",
    "            \"total_impacted\": len(downstream),\n",
    "            \"all_impacted_tables\": list(downstream)\n",
    "        }\n",
    "\n",
    "# Build example lineage graph\n",
    "lineage = DataLineage()\n",
    "\n",
    "# Source systems\n",
    "lineage.add_table(\"raw_orders\", {\"layer\": \"bronze\", \"owner\": \"ingestion\"})\n",
    "lineage.add_table(\"raw_customers\", {\"layer\": \"bronze\", \"owner\": \"ingestion\"})\n",
    "lineage.add_table(\"raw_products\", {\"layer\": \"bronze\", \"owner\": \"ingestion\"})\n",
    "\n",
    "# Cleaned tables\n",
    "lineage.add_lineage(\"raw_orders\", \"clean_orders\")\n",
    "lineage.add_lineage(\"raw_customers\", \"clean_customers\")\n",
    "lineage.add_lineage(\"raw_products\", \"clean_products\")\n",
    "\n",
    "# Aggregated tables\n",
    "lineage.add_lineage(\"clean_orders\", \"order_facts\")\n",
    "lineage.add_lineage(\"clean_customers\", \"order_facts\")\n",
    "lineage.add_lineage(\"clean_products\", \"order_facts\")\n",
    "\n",
    "# Marts\n",
    "lineage.add_lineage(\"order_facts\", \"sales_dashboard\")\n",
    "lineage.add_lineage(\"order_facts\", \"customer_360\")\n",
    "lineage.add_lineage(\"clean_customers\", \"customer_360\")\n",
    "\n",
    "# Impact analysis\n",
    "print(\"Impact Analysis for 'raw_customers':\")\n",
    "print(\"-\" * 40)\n",
    "impact = lineage.impact_analysis(\"raw_customers\")\n",
    "print(f\"Direct dependents: {impact['direct_dependents']}\")\n",
    "print(f\"Total impacted tables: {impact['total_impacted']}\")\n",
    "print(f\"All impacted: {impact['all_impacted_tables']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c045194",
   "metadata": {},
   "source": [
    "## Monitoring and Alerting Strategies\n",
    "\n",
    "Effective data observability requires a well-designed alerting system:\n",
    "\n",
    "### Alert Severity Levels\n",
    "\n",
    "| Level | Description | Response Time | Examples |\n",
    "|-------|-------------|---------------|----------|\n",
    "| **Critical** | Business-impacting issues | Immediate | Production dashboard down, data loss |\n",
    "| **High** | SLA violations | < 1 hour | Pipeline delays > 2 hours, data quality drops |\n",
    "| **Medium** | Potential issues | Same day | Schema changes, volume anomalies |\n",
    "| **Low** | Informational | Review weekly | Minor drift, performance degradation |\n",
    "\n",
    "### Alert Best Practices\n",
    "\n",
    "1. **Avoid alert fatigue**: Tune thresholds to minimize false positives\n",
    "2. **Include context**: Provide links to dashboards, runbooks, and relevant data\n",
    "3. **Route intelligently**: Send alerts to the right team based on table ownership\n",
    "4. **Enable aggregation**: Group related alerts to reduce noise\n",
    "5. **Track SLOs**: Define and monitor Service Level Objectives for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import List, Callable, Optional\n",
    "import json\n",
    "\n",
    "class AlertSeverity(Enum):\n",
    "    CRITICAL = \"critical\"\n",
    "    HIGH = \"high\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LOW = \"low\"\n",
    "\n",
    "class AlertStatus(Enum):\n",
    "    ACTIVE = \"active\"\n",
    "    ACKNOWLEDGED = \"acknowledged\"\n",
    "    RESOLVED = \"resolved\"\n",
    "\n",
    "@dataclass\n",
    "class Alert:\n",
    "    \"\"\"Data quality alert.\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    severity: AlertSeverity\n",
    "    table: str\n",
    "    check_type: str  # freshness, volume, schema, distribution\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    status: AlertStatus = AlertStatus.ACTIVE\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "    \n",
    "    def to_slack_message(self) -> dict:\n",
    "        \"\"\"Format alert for Slack notification.\"\"\"\n",
    "        emoji = {\n",
    "            AlertSeverity.CRITICAL: \"ðŸ”´\",\n",
    "            AlertSeverity.HIGH: \"ðŸŸ \",\n",
    "            AlertSeverity.MEDIUM: \"ðŸŸ¡\",\n",
    "            AlertSeverity.LOW: \"ðŸ”µ\"\n",
    "        }\n",
    "        return {\n",
    "            \"text\": f\"{emoji[self.severity]} [{self.severity.value.upper()}] {self.title}\",\n",
    "            \"blocks\": [\n",
    "                {\n",
    "                    \"type\": \"section\",\n",
    "                    \"text\": {\n",
    "                        \"type\": \"mrkdwn\",\n",
    "                        \"text\": f\"*{emoji[self.severity]} {self.title}*\\n{self.description}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"context\",\n",
    "                    \"elements\": [\n",
    "                        {\"type\": \"mrkdwn\", \"text\": f\"*Table:* {self.table}\"},\n",
    "                        {\"type\": \"mrkdwn\", \"text\": f\"*Check:* {self.check_type}\"},\n",
    "                        {\"type\": \"mrkdwn\", \"text\": f\"*Time:* {self.created_at.isoformat()}\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "class AlertManager:\n",
    "    \"\"\"Manage data quality alerts with routing and aggregation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alerts: List[Alert] = []\n",
    "        self.handlers: List[Callable[[Alert], None]] = []\n",
    "        self.owner_routing: Dict[str, str] = {}  # table -> owner\n",
    "    \n",
    "    def add_handler(self, handler: Callable[[Alert], None]):\n",
    "        \"\"\"Add alert handler (e.g., Slack, PagerDuty, email).\"\"\"\n",
    "        self.handlers.append(handler)\n",
    "    \n",
    "    def set_table_owner(self, table: str, owner: str):\n",
    "        \"\"\"Set table ownership for routing.\"\"\"\n",
    "        self.owner_routing[table] = owner\n",
    "    \n",
    "    def fire_alert(self, alert: Alert):\n",
    "        \"\"\"Fire an alert and route to handlers.\"\"\"\n",
    "        self.alerts.append(alert)\n",
    "        \n",
    "        # Add owner to metadata\n",
    "        if alert.table in self.owner_routing:\n",
    "            alert.metadata[\"owner\"] = self.owner_routing[alert.table]\n",
    "        \n",
    "        # Route to all handlers\n",
    "        for handler in self.handlers:\n",
    "            handler(alert)\n",
    "    \n",
    "    def get_active_alerts(self, severity: Optional[AlertSeverity] = None) -> List[Alert]:\n",
    "        \"\"\"Get active alerts, optionally filtered by severity.\"\"\"\n",
    "        active = [a for a in self.alerts if a.status == AlertStatus.ACTIVE]\n",
    "        if severity:\n",
    "            active = [a for a in active if a.severity == severity]\n",
    "        return active\n",
    "\n",
    "# Example usage\n",
    "def log_handler(alert: Alert):\n",
    "    print(f\"ALERT: [{alert.severity.value}] {alert.title} - {alert.table}\")\n",
    "\n",
    "manager = AlertManager()\n",
    "manager.add_handler(log_handler)\n",
    "manager.set_table_owner(\"orders\", \"data-platform-team\")\n",
    "\n",
    "# Fire test alert\n",
    "alert = Alert(\n",
    "    id=\"alert-001\",\n",
    "    title=\"Orders table stale\",\n",
    "    description=\"Table has not been updated in 3 hours (SLA: 1 hour)\",\n",
    "    severity=AlertSeverity.HIGH,\n",
    "    table=\"orders\",\n",
    "    check_type=\"freshness\"\n",
    ")\n",
    "\n",
    "manager.fire_alert(alert)\n",
    "print(f\"\\nActive alerts: {len(manager.get_active_alerts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79b856",
   "metadata": {},
   "source": [
    "## Comprehensive Data Quality Monitor\n",
    "\n",
    "A complete monitoring system that combines all pillars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "@dataclass\n",
    "class CheckResult:\n",
    "    \"\"\"Result of a data quality check.\"\"\"\n",
    "    check_name: str\n",
    "    passed: bool\n",
    "    message: str\n",
    "    timestamp: datetime\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class DataQualityCheck(ABC):\n",
    "    \"\"\"Base class for data quality checks.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run(self) -> CheckResult:\n",
    "        pass\n",
    "\n",
    "class DataQualityOrchestrator:\n",
    "    \"\"\"Orchestrate multiple data quality checks.\"\"\"\n",
    "    \n",
    "    def __init__(self, table_name: str, alert_manager: AlertManager = None):\n",
    "        self.table_name = table_name\n",
    "        self.checks: List[DataQualityCheck] = []\n",
    "        self.results: List[CheckResult] = []\n",
    "        self.alert_manager = alert_manager\n",
    "    \n",
    "    def add_check(self, check: DataQualityCheck):\n",
    "        \"\"\"Add a quality check to the orchestrator.\"\"\"\n",
    "        self.checks.append(check)\n",
    "    \n",
    "    def run_all(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run all registered checks.\"\"\"\n",
    "        self.results = []\n",
    "        \n",
    "        for check in self.checks:\n",
    "            result = check.run()\n",
    "            self.results.append(result)\n",
    "            \n",
    "            # Fire alert if check failed\n",
    "            if not result.passed and self.alert_manager:\n",
    "                alert = Alert(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    title=f\"{result.check_name} failed\",\n",
    "                    description=result.message,\n",
    "                    severity=AlertSeverity.HIGH,\n",
    "                    table=self.table_name,\n",
    "                    check_type=result.check_name\n",
    "                )\n",
    "                self.alert_manager.fire_alert(alert)\n",
    "        \n",
    "        passed = sum(1 for r in self.results if r.passed)\n",
    "        failed = len(self.results) - passed\n",
    "        \n",
    "        return {\n",
    "            \"table\": self.table_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_checks\": len(self.results),\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"health_score\": passed / len(self.results) if self.results else 1.0,\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"check\": r.check_name,\n",
    "                    \"passed\": r.passed,\n",
    "                    \"message\": r.message\n",
    "                } for r in self.results\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Example: Create concrete checks\n",
    "class MockFreshnessCheck(DataQualityCheck):\n",
    "    def run(self) -> CheckResult:\n",
    "        return CheckResult(\n",
    "            check_name=\"freshness\",\n",
    "            passed=True,\n",
    "            message=\"Table updated 15 minutes ago (SLA: 1 hour)\",\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "\n",
    "class MockVolumeCheck(DataQualityCheck):\n",
    "    def run(self) -> CheckResult:\n",
    "        return CheckResult(\n",
    "            check_name=\"volume\",\n",
    "            passed=False,\n",
    "            message=\"Row count 5,000 is 50% below expected (10,000)\",\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "\n",
    "class MockSchemaCheck(DataQualityCheck):\n",
    "    def run(self) -> CheckResult:\n",
    "        return CheckResult(\n",
    "            check_name=\"schema\",\n",
    "            passed=True,\n",
    "            message=\"No schema changes detected\",\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "\n",
    "# Run orchestrator\n",
    "orchestrator = DataQualityOrchestrator(\"orders\", alert_manager=manager)\n",
    "orchestrator.add_check(MockFreshnessCheck())\n",
    "orchestrator.add_check(MockVolumeCheck())\n",
    "orchestrator.add_check(MockSchemaCheck())\n",
    "\n",
    "report = orchestrator.run_all()\n",
    "\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Table: {report['table']}\")\n",
    "print(f\"Health Score: {report['health_score']:.0%}\")\n",
    "print(f\"Checks: {report['passed']}/{report['total_checks']} passed\")\n",
    "print(\"\\nDetails:\")\n",
    "for r in report['results']:\n",
    "    status = \"âœ…\" if r['passed'] else \"âŒ\"\n",
    "    print(f\"  {status} {r['check']}: {r['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f27021",
   "metadata": {},
   "source": [
    "## Data Observability Tools Overview\n",
    "\n",
    "### Enterprise Tools\n",
    "\n",
    "| Tool | Type | Key Features | Best For |\n",
    "|------|------|--------------|----------|\n",
    "| **Monte Carlo** | Platform | ML-powered anomaly detection, end-to-end lineage, incident management | Enterprise data teams |\n",
    "| **Datadog Data Jobs** | Observability | Pipeline monitoring, cost tracking, integrates with APM | DevOps-oriented teams |\n",
    "| **Atlan** | Catalog + Observability | Active metadata, lineage, data discovery | Data governance focus |\n",
    "| **Soda** | Testing + Monitoring | SQL-based checks, SodaCL language, Soda Cloud | Developer-first teams |\n",
    "| **Great Expectations** | Open Source | Python-native, extensive expectations library | Data engineering teams |\n",
    "| **dbt Tests** | Built-in | Schema tests, freshness, relationships | dbt users |\n",
    "\n",
    "### Comparison Matrix\n",
    "\n",
    "```\n",
    "                    Monte Carlo  Datadog   Atlan   Soda    Great Expectations  dbt\n",
    "Freshness               âœ…          âœ…       âœ…      âœ…            âœ…           âœ…\n",
    "Volume                  âœ…          âœ…       âœ…      âœ…            âœ…           âš ï¸\n",
    "Schema                  âœ…          âš ï¸       âœ…      âœ…            âœ…           âœ…\n",
    "Distribution            âœ…          âš ï¸       âš ï¸      âœ…            âœ…           âš ï¸\n",
    "Lineage                 âœ…          âš ï¸       âœ…      âš ï¸            âŒ           âœ…\n",
    "ML Anomaly Detection    âœ…          âœ…       âš ï¸      âœ…            âš ï¸           âŒ\n",
    "Self-hosted             âŒ          âŒ       âŒ      âœ…            âœ…           âœ…\n",
    "```\n",
    "\n",
    "âœ… = Full support | âš ï¸ = Partial/Limited | âŒ = Not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee8456",
   "metadata": {},
   "source": [
    "### Great Expectations Example\n",
    "\n",
    "Great Expectations is a popular open-source tool for data validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great Expectations-style validation (conceptual example)\n",
    "# In practice, you would use the actual great_expectations library\n",
    "\n",
    "class ExpectationSuite:\n",
    "    \"\"\"Simplified Great Expectations-style validator.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.expectations = []\n",
    "        self.results = []\n",
    "    \n",
    "    def expect_column_values_to_not_be_null(self, column: str, mostly: float = 1.0):\n",
    "        self.expectations.append({\n",
    "            \"type\": \"not_null\",\n",
    "            \"column\": column,\n",
    "            \"mostly\": mostly\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_to_be_between(self, column: str, min_val: float, max_val: float):\n",
    "        self.expectations.append({\n",
    "            \"type\": \"between\",\n",
    "            \"column\": column,\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_to_be_unique(self, column: str):\n",
    "        self.expectations.append({\n",
    "            \"type\": \"unique\",\n",
    "            \"column\": column\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def expect_table_row_count_to_be_between(self, min_count: int, max_count: int):\n",
    "        self.expectations.append({\n",
    "            \"type\": \"row_count_between\",\n",
    "            \"min\": min_count,\n",
    "            \"max\": max_count\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Validate dataframe against expectations.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for exp in self.expectations:\n",
    "            if exp[\"type\"] == \"not_null\":\n",
    "                null_rate = df[exp[\"column\"]].isnull().mean()\n",
    "                passed = (1 - null_rate) >= exp[\"mostly\"]\n",
    "                results.append({\n",
    "                    \"expectation\": f\"{exp['column']} not null\",\n",
    "                    \"passed\": passed,\n",
    "                    \"observed\": f\"{(1-null_rate):.1%} non-null\"\n",
    "                })\n",
    "            \n",
    "            elif exp[\"type\"] == \"between\":\n",
    "                col = df[exp[\"column\"]]\n",
    "                in_range = ((col >= exp[\"min\"]) & (col <= exp[\"max\"])).all()\n",
    "                results.append({\n",
    "                    \"expectation\": f\"{exp['column']} between {exp['min']} and {exp['max']}\",\n",
    "                    \"passed\": in_range,\n",
    "                    \"observed\": f\"min={col.min():.2f}, max={col.max():.2f}\"\n",
    "                })\n",
    "            \n",
    "            elif exp[\"type\"] == \"unique\":\n",
    "                is_unique = df[exp[\"column\"]].is_unique\n",
    "                results.append({\n",
    "                    \"expectation\": f\"{exp['column']} is unique\",\n",
    "                    \"passed\": is_unique,\n",
    "                    \"observed\": f\"{df[exp['column']].nunique()} unique of {len(df)} rows\"\n",
    "                })\n",
    "            \n",
    "            elif exp[\"type\"] == \"row_count_between\":\n",
    "                count = len(df)\n",
    "                passed = exp[\"min\"] <= count <= exp[\"max\"]\n",
    "                results.append({\n",
    "                    \"expectation\": f\"row count between {exp['min']} and {exp['max']}\",\n",
    "                    \"passed\": passed,\n",
    "                    \"observed\": f\"{count} rows\"\n",
    "                })\n",
    "        \n",
    "        passed_count = sum(1 for r in results if r[\"passed\"])\n",
    "        return {\n",
    "            \"suite\": self.name,\n",
    "            \"success\": passed_count == len(results),\n",
    "            \"passed\": passed_count,\n",
    "            \"failed\": len(results) - passed_count,\n",
    "            \"results\": results\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "df = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\"],\n",
    "    \"amount\": [100.5, 250.0, 75.25, 500.0, 125.75],\n",
    "    \"age\": [25, 30, 35, 200, 28]  # 200 is invalid\n",
    "})\n",
    "\n",
    "suite = ExpectationSuite(\"orders_validation\")\n",
    "suite.expect_column_values_to_not_be_null(\"id\")\n",
    "suite.expect_column_values_to_not_be_null(\"name\", mostly=0.8)\n",
    "suite.expect_column_values_to_be_unique(\"id\")\n",
    "suite.expect_column_values_to_be_between(\"amount\", 0, 1000)\n",
    "suite.expect_column_values_to_be_between(\"age\", 0, 120)\n",
    "suite.expect_table_row_count_to_be_between(1, 100)\n",
    "\n",
    "validation_result = suite.validate(df)\n",
    "\n",
    "print(f\"Suite: {validation_result['suite']}\")\n",
    "print(f\"Overall Success: {validation_result['success']}\")\n",
    "print(f\"Results: {validation_result['passed']}/{validation_result['passed'] + validation_result['failed']} passed\")\n",
    "print(\"\\nDetails:\")\n",
    "for r in validation_result['results']:\n",
    "    status = \"âœ…\" if r['passed'] else \"âŒ\"\n",
    "    print(f\"  {status} {r['expectation']}: {r['observed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e7d84",
   "metadata": {},
   "source": [
    "### Soda-style SQL Checks\n",
    "\n",
    "Soda uses a YAML-based configuration with SQL under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33577be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soda-style check configuration (conceptual)\n",
    "soda_checks_yaml = \"\"\"\n",
    "# SodaCL Check Definition\n",
    "checks for orders:\n",
    "  # Freshness check\n",
    "  - freshness(updated_at) < 1h\n",
    "  \n",
    "  # Volume checks\n",
    "  - row_count > 0\n",
    "  - row_count between 9000 and 11000\n",
    "  \n",
    "  # Validity checks\n",
    "  - missing_count(order_id) = 0\n",
    "  - duplicate_count(order_id) = 0\n",
    "  \n",
    "  # Distribution checks\n",
    "  - avg(order_amount) between 80 and 120\n",
    "  - max(order_amount) < 10000\n",
    "  \n",
    "  # Schema check\n",
    "  - schema:\n",
    "      name: Check schema\n",
    "      fail:\n",
    "        when missing column:\n",
    "          - order_id\n",
    "          - customer_id\n",
    "          - order_amount\n",
    "        when wrong type:\n",
    "          order_amount: decimal\n",
    "\"\"\"\n",
    "\n",
    "print(\"Soda Check Configuration Example:\")\n",
    "print(soda_checks_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c703df8",
   "metadata": {},
   "source": [
    "## Implementing Data Observability: Best Practices\n",
    "\n",
    "### 1. Start with Critical Pipelines\n",
    "- Identify business-critical data assets\n",
    "- Implement freshness and volume checks first\n",
    "- Gradually expand coverage\n",
    "\n",
    "### 2. Define SLAs and SLOs\n",
    "- **SLA (Service Level Agreement)**: External commitment to stakeholders\n",
    "- **SLO (Service Level Objective)**: Internal target (typically stricter)\n",
    "- **SLI (Service Level Indicator)**: Actual metric being measured\n",
    "\n",
    "### 3. Build Data Contracts\n",
    "- Define expected schema, freshness, and quality rules\n",
    "- Version contracts alongside code\n",
    "- Enforce contracts in CI/CD pipelines\n",
    "\n",
    "### 4. Implement Circuit Breakers\n",
    "- Stop pipeline execution when quality degrades\n",
    "- Prevent bad data from propagating downstream\n",
    "\n",
    "### 5. Create Runbooks\n",
    "- Document common failure scenarios\n",
    "- Include resolution steps\n",
    "- Link runbooks from alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBreaker:\n",
    "    \"\"\"Pipeline circuit breaker for data quality failures.\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 3, reset_timeout_minutes: int = 30):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.reset_timeout = timedelta(minutes=reset_timeout_minutes)\n",
    "        self.failures = 0\n",
    "        self.last_failure_time: Optional[datetime] = None\n",
    "        self.state = \"closed\"  # closed, open, half-open\n",
    "    \n",
    "    def record_success(self):\n",
    "        \"\"\"Record successful check.\"\"\"\n",
    "        self.failures = 0\n",
    "        self.state = \"closed\"\n",
    "    \n",
    "    def record_failure(self):\n",
    "        \"\"\"Record failed check.\"\"\"\n",
    "        self.failures += 1\n",
    "        self.last_failure_time = datetime.now()\n",
    "        \n",
    "        if self.failures >= self.failure_threshold:\n",
    "            self.state = \"open\"\n",
    "    \n",
    "    def can_proceed(self) -> bool:\n",
    "        \"\"\"Check if pipeline can proceed.\"\"\"\n",
    "        if self.state == \"closed\":\n",
    "            return True\n",
    "        \n",
    "        if self.state == \"open\":\n",
    "            # Check if reset timeout has passed\n",
    "            if datetime.now() - self.last_failure_time > self.reset_timeout:\n",
    "                self.state = \"half-open\"\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # half-open: allow one attempt\n",
    "        return True\n",
    "    \n",
    "    def status(self) -> dict:\n",
    "        return {\n",
    "            \"state\": self.state,\n",
    "            \"failures\": self.failures,\n",
    "            \"can_proceed\": self.can_proceed()\n",
    "        }\n",
    "\n",
    "# Example: Circuit breaker in action\n",
    "breaker = CircuitBreaker(failure_threshold=3)\n",
    "\n",
    "# Simulate failures\n",
    "print(\"Simulating quality check failures:\")\n",
    "for i in range(5):\n",
    "    if breaker.can_proceed():\n",
    "        print(f\"  Check {i+1}: Running... (state: {breaker.state})\")\n",
    "        breaker.record_failure()  # Simulate failure\n",
    "    else:\n",
    "        print(f\"  Check {i+1}: BLOCKED - Circuit breaker open\")\n",
    "\n",
    "print(f\"\\nFinal status: {breaker.status()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7731e",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **Five Pillars**: Freshness, Volume, Schema, Distribution, and Lineage provide comprehensive data health visibility\n",
    "\n",
    "2. **Proactive Monitoring**: Detect issues before they impact downstream consumers and business decisions\n",
    "\n",
    "3. **Automated Detection**: Use statistical methods (z-scores, KS tests) for anomaly detection at scale\n",
    "\n",
    "4. **Impact Analysis**: Lineage enables understanding of upstream dependencies and downstream impact\n",
    "\n",
    "5. **Alert Management**: Proper severity levels, routing, and aggregation prevent alert fatigue\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "| Phase | Focus | Activities |\n",
    "|-------|-------|------------|\n",
    "| **1. Foundation** | Critical assets | Deploy freshness/volume checks on key tables |\n",
    "| **2. Expansion** | Broader coverage | Add schema/distribution monitoring, build lineage |\n",
    "| **3. Automation** | Self-service | ML-based anomaly detection, automated remediation |\n",
    "| **4. Governance** | Data contracts | SLAs, ownership, quality certification |\n",
    "\n",
    "### Tool Selection Criteria\n",
    "\n",
    "- **Scale**: How many tables/pipelines need monitoring?\n",
    "- **Complexity**: Custom checks vs. out-of-box detection?\n",
    "- **Integration**: Does it work with your data stack?\n",
    "- **Cost**: Open-source vs. managed service tradeoffs?\n",
    "- **Team Skills**: SQL-based vs. Python-based configuration?\n",
    "\n",
    "### Remember\n",
    "\n",
    "> \"You can't fix what you can't see. Data observability is not optionalâ€”it's essential for building trust in your data platform.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Related Topics**: Data Quality, Data Contracts, DataOps, MLOps"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
