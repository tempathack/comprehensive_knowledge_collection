{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5745e90b",
   "metadata": {},
   "source": [
    "# Metadata and Data Catalogs\n",
    "\n",
    "**Metadata** is \"data about data\" – it provides context, meaning, and governance information that makes data assets discoverable, understandable, and trustworthy. A **Data Catalog** is a centralized inventory that organizes and manages metadata, enabling users to find, understand, and govern data across an organization.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Metadata Matters\n",
    "\n",
    "| Challenge | How Metadata Helps |\n",
    "|-----------|-------------------|\n",
    "| Data silos | Provides unified visibility across systems |\n",
    "| Lack of trust | Documents lineage, quality, and ownership |\n",
    "| Compliance risk | Tracks sensitive data and access policies |\n",
    "| Slow discovery | Enables search and self-service analytics |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feda374",
   "metadata": {},
   "source": [
    "## Types of Metadata\n",
    "\n",
    "Metadata can be categorized into three primary types:\n",
    "\n",
    "### 1. Technical Metadata\n",
    "Describes the **structure and format** of data assets.\n",
    "\n",
    "| Attribute | Description | Example |\n",
    "|-----------|-------------|--------|\n",
    "| Schema | Table/column definitions | `customer_id INT PRIMARY KEY` |\n",
    "| Data Types | Column data types | `VARCHAR(255)`, `TIMESTAMP` |\n",
    "| Constraints | Keys, indexes, partitions | `FOREIGN KEY`, `UNIQUE INDEX` |\n",
    "| Storage Location | Physical location | `s3://bucket/table/` |\n",
    "| File Format | Serialization format | Parquet, Avro, JSON |\n",
    "| Row Count | Volume statistics | `1,250,000 rows` |\n",
    "\n",
    "### 2. Business Metadata\n",
    "Provides **context and meaning** for business users.\n",
    "\n",
    "| Attribute | Description | Example |\n",
    "|-----------|-------------|--------|\n",
    "| Business Name | Human-readable name | \"Customer Lifetime Value\" |\n",
    "| Description | Plain-language explanation | \"Total revenue per customer\" |\n",
    "| Domain/Category | Business classification | Finance, Marketing, Sales |\n",
    "| Owner | Responsible team/person | \"Data Engineering Team\" |\n",
    "| Tags | Searchable labels | `#PII`, `#Revenue`, `#Critical` |\n",
    "| Glossary Terms | Standardized definitions | \"Churn Rate\", \"ARR\" |\n",
    "\n",
    "### 3. Operational Metadata\n",
    "Captures **runtime and process** information.\n",
    "\n",
    "| Attribute | Description | Example |\n",
    "|-----------|-------------|--------|\n",
    "| Lineage | Data flow and transformations | Source → ETL → Target |\n",
    "| Last Updated | Freshness timestamp | `2026-02-01 14:30:00 UTC` |\n",
    "| Job Statistics | ETL run metrics | Duration, records processed |\n",
    "| Access Logs | Usage patterns | Query frequency, top users |\n",
    "| Quality Metrics | Data quality scores | Completeness: 98.5% |\n",
    "| SLA Status | Pipeline health | On-time delivery rate |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5ec0f",
   "metadata": {},
   "source": [
    "## Data Catalog: Features and Importance\n",
    "\n",
    "A **Data Catalog** serves as the single source of truth for all data assets in an organization.\n",
    "\n",
    "### Core Features\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      DATA CATALOG                               │\n",
    "├─────────────────┬─────────────────┬─────────────────────────────┤\n",
    "│   Discovery     │   Governance    │      Collaboration          │\n",
    "├─────────────────┼─────────────────┼─────────────────────────────┤\n",
    "│ • Search        │ • Lineage       │ • Comments & Reviews        │\n",
    "│ • Browse        │ • Access Control│ • Ratings & Endorsements    │\n",
    "│ • Filtering     │ • Classification│ • Shared Collections        │\n",
    "│ • Recommendations│ • Audit Trails │ • Knowledge Sharing         │\n",
    "└─────────────────┴─────────────────┴─────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Capabilities\n",
    "\n",
    "| Capability | Description |\n",
    "|------------|-------------|\n",
    "| **Automated Ingestion** | Crawlers that extract metadata from sources |\n",
    "| **Search & Discovery** | Full-text and faceted search across assets |\n",
    "| **Data Lineage** | Visual representation of data flow |\n",
    "| **Business Glossary** | Standardized terminology definitions |\n",
    "| **Data Classification** | PII/sensitive data tagging |\n",
    "| **Access Management** | Role-based permissions |\n",
    "| **APIs & Integrations** | Programmatic access and tool connectivity |\n",
    "\n",
    "### Business Value\n",
    "\n",
    "- **Reduce time-to-insight**: Analysts find data 5-10x faster\n",
    "- **Improve data quality**: Visibility enables proactive fixes\n",
    "- **Ensure compliance**: Track sensitive data for GDPR, CCPA, HIPAA\n",
    "- **Enable self-service**: Reduce dependency on IT for data access\n",
    "- **Foster trust**: Clear ownership and lineage build confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f26d67",
   "metadata": {},
   "source": [
    "## Data Catalog Tools Comparison\n",
    "\n",
    "### Popular Data Catalog Solutions\n",
    "\n",
    "| Tool | Type | Best For | Key Strengths |\n",
    "|------|------|----------|---------------|\n",
    "| **AWS Glue Data Catalog** | Cloud-native | AWS ecosystem | Tight Athena/Redshift integration |\n",
    "| **Apache Atlas** | Open-source | Hadoop ecosystem | Deep Hadoop lineage support |\n",
    "| **DataHub** | Open-source | Modern data stack | Extensible, active community |\n",
    "| **Atlan** | Commercial | Enterprise collaboration | User experience, AI features |\n",
    "\n",
    "---\n",
    "\n",
    "### AWS Glue Data Catalog\n",
    "\n",
    "The central metadata repository for AWS analytics services.\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                  AWS GLUE DATA CATALOG                       │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│  ┌─────────┐   ┌─────────┐   ┌─────────┐   ┌─────────┐      │\n",
    "│  │ Athena  │   │ Redshift│   │  EMR    │   │Lake Form│      │\n",
    "│  │ Spectrum│   │         │   │         │   │ ation   │      │\n",
    "│  └────┬────┘   └────┬────┘   └────┬────┘   └────┬────┘      │\n",
    "│       │             │             │             │           │\n",
    "│       └─────────────┴─────────────┴─────────────┘           │\n",
    "│                         │                                    │\n",
    "│              ┌──────────▼──────────┐                        │\n",
    "│              │   Glue Data Catalog │                        │\n",
    "│              │  • Databases        │                        │\n",
    "│              │  • Tables           │                        │\n",
    "│              │  • Partitions       │                        │\n",
    "│              │  • Connections      │                        │\n",
    "│              └──────────┬──────────┘                        │\n",
    "│                         │                                    │\n",
    "│              ┌──────────▼──────────┐                        │\n",
    "│              │    Glue Crawlers    │                        │\n",
    "│              └──────────┬──────────┘                        │\n",
    "│                         │                                    │\n",
    "│    ┌────────────┬───────┴────────┬────────────┐             │\n",
    "│    ▼            ▼                ▼            ▼             │\n",
    "│  ┌────┐      ┌─────┐         ┌──────┐     ┌──────┐         │\n",
    "│  │ S3 │      │ RDS │         │Redshift│   │DynamoDB│        │\n",
    "│  └────┘      └─────┘         └──────┘     └──────┘         │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic schema discovery via Crawlers\n",
    "- Hive-compatible metastore\n",
    "- Integration with Lake Formation for fine-grained access\n",
    "- Pay-per-use pricing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae3bc6",
   "metadata": {},
   "source": [
    "### Apache Atlas\n",
    "\n",
    "Open-source metadata management and governance framework for Hadoop.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│                     APACHE ATLAS                           │\n",
    "├────────────────────────────────────────────────────────────┤\n",
    "│                                                            │\n",
    "│  ┌─────────────────┐     ┌──────────────────────────────┐ │\n",
    "│  │  Type System    │     │      Core Services           │ │\n",
    "│  │  • Entities     │     │  • Metadata Store            │ │\n",
    "│  │  • Classifications│   │  • Search & Indexing         │ │\n",
    "│  │  • Relationships │    │  • Lineage Engine            │ │\n",
    "│  └─────────────────┘     │  • Notification System       │ │\n",
    "│                          └──────────────────────────────┘ │\n",
    "│                                                            │\n",
    "│  ┌─────────────────────────────────────────────────────┐  │\n",
    "│  │              Integration Hooks                       │  │\n",
    "│  │   Hive │ Sqoop │ Storm │ Falcon │ Kafka │ NiFi      │  │\n",
    "│  └─────────────────────────────────────────────────────┘  │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Extensible type system for custom metadata\n",
    "- Native lineage tracking for Hadoop ecosystem\n",
    "- Classification propagation (e.g., PII tags flow downstream)\n",
    "- REST API for programmatic access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1103b4",
   "metadata": {},
   "source": [
    "### DataHub (LinkedIn Open Source)\n",
    "\n",
    "Modern, extensible data catalog for the modern data stack.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                        DATAHUB                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│   ┌─────────────────────────────────────────────────────┐  │\n",
    "│   │                   Frontend (React)                   │  │\n",
    "│   │   Search │ Browse │ Lineage │ Governance │ Profiles │  │\n",
    "│   └────────────────────────┬────────────────────────────┘  │\n",
    "│                            │                                │\n",
    "│   ┌────────────────────────▼────────────────────────────┐  │\n",
    "│   │               GraphQL / REST API                     │  │\n",
    "│   └────────────────────────┬────────────────────────────┘  │\n",
    "│                            │                                │\n",
    "│   ┌────────────────────────▼────────────────────────────┐  │\n",
    "│   │          Metadata Service (GMS)                      │  │\n",
    "│   │   • Entity Registry  • Aspect Store  • Search Index │  │\n",
    "│   └────────────────────────┬────────────────────────────┘  │\n",
    "│                            │                                │\n",
    "│   ┌─────────┬──────────────┼──────────────┬─────────────┐  │\n",
    "│   │         │              │              │             │  │\n",
    "│   ▼         ▼              ▼              ▼             ▼  │\n",
    "│ MySQL  Elasticsearch    Kafka       Neo4j (opt)   MCE/MAE │\n",
    "│                                                             │\n",
    "│   ┌─────────────────────────────────────────────────────┐  │\n",
    "│   │              Ingestion Framework                     │  │\n",
    "│   │ Snowflake│BigQuery│dbt│Airflow│Spark│Looker│Tableau │  │\n",
    "│   └─────────────────────────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- 50+ native integrations\n",
    "- Real-time metadata updates via Kafka\n",
    "- GraphQL API for flexible queries\n",
    "- dbt integration for transformation lineage\n",
    "- Active open-source community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac4dcd",
   "metadata": {},
   "source": [
    "### Atlan\n",
    "\n",
    "Enterprise data catalog with emphasis on collaboration and user experience.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                         ATLAN                               │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │\n",
    "│  │   Search    │  │  Lineage    │  │   Collaboration     │ │\n",
    "│  │  • AI-powered│ │  • Column   │  │   • Slack-like      │ │\n",
    "│  │  • Natural   │  │  • Impact   │  │   • @mentions       │ │\n",
    "│  │    language  │  │  • Bi-direct│  │   • Announcements   │ │\n",
    "│  └─────────────┘  └─────────────┘  └─────────────────────┘ │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐   │\n",
    "│  │              Active Metadata Platform                │   │\n",
    "│  │  • Playbooks (automation)  • Personas (custom views) │   │\n",
    "│  │  • Policies (governance)   • Insights (analytics)    │   │\n",
    "│  └─────────────────────────────────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐   │\n",
    "│  │                  Integrations                        │   │\n",
    "│  │  Snowflake│Databricks│BigQuery│Redshift│Tableau│dbt │   │\n",
    "│  └─────────────────────────────────────────────────────┘   │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- AI-powered search and recommendations\n",
    "- Playbooks for automated governance workflows\n",
    "- Column-level lineage\n",
    "- Slack/Teams integration for notifications\n",
    "- SOC 2 Type II certified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d08d67",
   "metadata": {},
   "source": [
    "## Python Code for Metadata Extraction\n",
    "\n",
    "Let's explore practical examples of extracting metadata using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Extract metadata from a Pandas DataFrame\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_dataframe_metadata(df: pd.DataFrame, name: str = \"dataset\") -> dict:\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata from a pandas DataFrame.\n",
    "    \n",
    "    Returns technical, statistical, and quality metadata.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"name\": name,\n",
    "        \"extraction_timestamp\": datetime.now().isoformat(),\n",
    "        \n",
    "        # Technical metadata\n",
    "        \"technical\": {\n",
    "            \"row_count\": len(df),\n",
    "            \"column_count\": len(df.columns),\n",
    "            \"memory_usage_bytes\": df.memory_usage(deep=True).sum(),\n",
    "            \"columns\": []\n",
    "        },\n",
    "        \n",
    "        # Quality metadata\n",
    "        \"quality\": {\n",
    "            \"completeness\": {},\n",
    "            \"uniqueness\": {},\n",
    "            \"total_null_count\": int(df.isnull().sum().sum())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Extract column-level metadata\n",
    "    for col in df.columns:\n",
    "        col_meta = {\n",
    "            \"name\": col,\n",
    "            \"dtype\": str(df[col].dtype),\n",
    "            \"nullable\": bool(df[col].isnull().any()),\n",
    "            \"null_count\": int(df[col].isnull().sum()),\n",
    "            \"unique_count\": int(df[col].nunique()),\n",
    "            \"sample_values\": df[col].dropna().head(3).tolist()\n",
    "        }\n",
    "        \n",
    "        # Add statistics for numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_meta[\"statistics\"] = {\n",
    "                \"min\": float(df[col].min()) if not pd.isna(df[col].min()) else None,\n",
    "                \"max\": float(df[col].max()) if not pd.isna(df[col].max()) else None,\n",
    "                \"mean\": float(df[col].mean()) if not pd.isna(df[col].mean()) else None,\n",
    "                \"std\": float(df[col].std()) if not pd.isna(df[col].std()) else None\n",
    "            }\n",
    "        \n",
    "        metadata[\"technical\"][\"columns\"].append(col_meta)\n",
    "        \n",
    "        # Quality metrics\n",
    "        total_rows = len(df)\n",
    "        metadata[\"quality\"][\"completeness\"][col] = round(\n",
    "            (1 - df[col].isnull().sum() / total_rows) * 100, 2\n",
    "        )\n",
    "        metadata[\"quality\"][\"uniqueness\"][col] = round(\n",
    "            (df[col].nunique() / total_rows) * 100, 2\n",
    "        )\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Demo with sample data\n",
    "sample_df = pd.DataFrame({\n",
    "    \"customer_id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", None, \"Eve\"],\n",
    "    \"revenue\": [1500.50, 2300.00, 890.25, 3400.75, 1200.00],\n",
    "    \"signup_date\": pd.to_datetime([\"2024-01-15\", \"2024-02-20\", \"2024-03-10\", \"2024-04-05\", \"2024-05-01\"])\n",
    "})\n",
    "\n",
    "metadata = extract_dataframe_metadata(sample_df, \"customer_metrics\")\n",
    "print(\"=== DataFrame Metadata ===\")\n",
    "import json\n",
    "print(json.dumps(metadata, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Extract metadata from SQL database using SQLAlchemy\n",
    "from sqlalchemy import create_engine, inspect, MetaData\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def extract_database_metadata(connection_string: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract metadata from a SQL database.\n",
    "    \n",
    "    Returns schema information including tables, columns, \n",
    "    primary keys, and foreign keys.\n",
    "    \"\"\"\n",
    "    engine = create_engine(connection_string)\n",
    "    inspector = inspect(engine)\n",
    "    \n",
    "    db_metadata = {\n",
    "        \"database_type\": engine.dialect.name,\n",
    "        \"schemas\": []\n",
    "    }\n",
    "    \n",
    "    for schema_name in inspector.get_schema_names():\n",
    "        schema_info = {\n",
    "            \"name\": schema_name,\n",
    "            \"tables\": []\n",
    "        }\n",
    "        \n",
    "        for table_name in inspector.get_table_names(schema=schema_name):\n",
    "            # Get columns\n",
    "            columns = []\n",
    "            for col in inspector.get_columns(table_name, schema=schema_name):\n",
    "                columns.append({\n",
    "                    \"name\": col[\"name\"],\n",
    "                    \"type\": str(col[\"type\"]),\n",
    "                    \"nullable\": col.get(\"nullable\", True),\n",
    "                    \"default\": str(col.get(\"default\")) if col.get(\"default\") else None\n",
    "                })\n",
    "            \n",
    "            # Get primary key\n",
    "            pk = inspector.get_pk_constraint(table_name, schema=schema_name)\n",
    "            \n",
    "            # Get foreign keys\n",
    "            fks = inspector.get_foreign_keys(table_name, schema=schema_name)\n",
    "            \n",
    "            # Get indexes\n",
    "            indexes = inspector.get_indexes(table_name, schema=schema_name)\n",
    "            \n",
    "            table_info = {\n",
    "                \"name\": table_name,\n",
    "                \"columns\": columns,\n",
    "                \"primary_key\": pk.get(\"constrained_columns\", []),\n",
    "                \"foreign_keys\": [\n",
    "                    {\n",
    "                        \"columns\": fk[\"constrained_columns\"],\n",
    "                        \"references\": f\"{fk['referred_table']}.{fk['referred_columns']}\"\n",
    "                    }\n",
    "                    for fk in fks\n",
    "                ],\n",
    "                \"indexes\": [\n",
    "                    {\"name\": idx[\"name\"], \"columns\": idx[\"column_names\"], \"unique\": idx[\"unique\"]}\n",
    "                    for idx in indexes\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            schema_info[\"tables\"].append(table_info)\n",
    "        \n",
    "        db_metadata[\"schemas\"].append(schema_info)\n",
    "    \n",
    "    return db_metadata\n",
    "\n",
    "# Example usage (commented out - requires actual database)\n",
    "# metadata = extract_database_metadata(\"postgresql://user:pass@localhost:5432/mydb\")\n",
    "# print(json.dumps(metadata, indent=2))\n",
    "\n",
    "print(\"Database metadata extraction function defined.\")\n",
    "print(\"Usage: extract_database_metadata('postgresql://user:pass@host:port/db')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: AWS Glue Data Catalog interaction (boto3)\n",
    "import json\n",
    "\n",
    "# Note: Requires AWS credentials configured\n",
    "# pip install boto3\n",
    "\n",
    "class GlueCatalogClient:\n",
    "    \"\"\"\n",
    "    Client for interacting with AWS Glue Data Catalog.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, region_name: str = \"us-east-1\"):\n",
    "        try:\n",
    "            import boto3\n",
    "            self.client = boto3.client('glue', region_name=region_name)\n",
    "        except ImportError:\n",
    "            print(\"boto3 not installed. Run: pip install boto3\")\n",
    "            self.client = None\n",
    "    \n",
    "    def list_databases(self) -> list:\n",
    "        \"\"\"List all databases in the Glue catalog.\"\"\"\n",
    "        if not self.client:\n",
    "            return []\n",
    "        \n",
    "        databases = []\n",
    "        paginator = self.client.get_paginator('get_databases')\n",
    "        \n",
    "        for page in paginator.paginate():\n",
    "            for db in page['DatabaseList']:\n",
    "                databases.append({\n",
    "                    \"name\": db['Name'],\n",
    "                    \"description\": db.get('Description', ''),\n",
    "                    \"location\": db.get('LocationUri', ''),\n",
    "                    \"create_time\": str(db.get('CreateTime', ''))\n",
    "                })\n",
    "        \n",
    "        return databases\n",
    "    \n",
    "    def get_table_metadata(self, database: str, table: str) -> dict:\n",
    "        \"\"\"Get detailed metadata for a specific table.\"\"\"\n",
    "        if not self.client:\n",
    "            return {}\n",
    "        \n",
    "        response = self.client.get_table(DatabaseName=database, Name=table)\n",
    "        table_data = response['Table']\n",
    "        \n",
    "        return {\n",
    "            \"name\": table_data['Name'],\n",
    "            \"database\": database,\n",
    "            \"description\": table_data.get('Description', ''),\n",
    "            \"location\": table_data.get('StorageDescriptor', {}).get('Location', ''),\n",
    "            \"input_format\": table_data.get('StorageDescriptor', {}).get('InputFormat', ''),\n",
    "            \"output_format\": table_data.get('StorageDescriptor', {}).get('OutputFormat', ''),\n",
    "            \"columns\": [\n",
    "                {\n",
    "                    \"name\": col['Name'],\n",
    "                    \"type\": col['Type'],\n",
    "                    \"comment\": col.get('Comment', '')\n",
    "                }\n",
    "                for col in table_data.get('StorageDescriptor', {}).get('Columns', [])\n",
    "            ],\n",
    "            \"partition_keys\": [\n",
    "                {\"name\": pk['Name'], \"type\": pk['Type']}\n",
    "                for pk in table_data.get('PartitionKeys', [])\n",
    "            ],\n",
    "            \"table_type\": table_data.get('TableType', ''),\n",
    "            \"create_time\": str(table_data.get('CreateTime', '')),\n",
    "            \"update_time\": str(table_data.get('UpdateTime', ''))\n",
    "        }\n",
    "    \n",
    "    def search_tables(self, search_text: str, max_results: int = 10) -> list:\n",
    "        \"\"\"Search tables across all databases.\"\"\"\n",
    "        if not self.client:\n",
    "            return []\n",
    "        \n",
    "        response = self.client.search_tables(\n",
    "            SearchText=search_text,\n",
    "            MaxResults=max_results\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"database\": t['DatabaseName'],\n",
    "                \"table\": t['Name'],\n",
    "                \"description\": t.get('Description', '')\n",
    "            }\n",
    "            for t in response.get('TableList', [])\n",
    "        ]\n",
    "\n",
    "# Example usage (requires AWS credentials)\n",
    "print(\"AWS Glue Catalog client class defined.\")\n",
    "print(\"\")\n",
    "print(\"Usage example:\")\n",
    "print(\"  glue = GlueCatalogClient(region_name='us-east-1')\")\n",
    "print(\"  databases = glue.list_databases()\")\n",
    "print(\"  table_meta = glue.get_table_metadata('my_database', 'my_table')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: DataHub metadata ingestion using Python SDK\n",
    "# pip install acryl-datahub\n",
    "\n",
    "def datahub_emit_dataset_example():\n",
    "    \"\"\"\n",
    "    Example of emitting dataset metadata to DataHub.\n",
    "    \n",
    "    Requires: pip install acryl-datahub\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from datahub.emitter.mce_builder import make_dataset_urn\n",
    "        from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "        from datahub.metadata.schema_classes import (\n",
    "            DatasetPropertiesClass,\n",
    "            MetadataChangeEventClass,\n",
    "            SchemaMetadataClass,\n",
    "            SchemaFieldClass,\n",
    "            StringTypeClass,\n",
    "            NumberTypeClass\n",
    "        )\n",
    "        \n",
    "        # Initialize emitter (connects to DataHub GMS)\n",
    "        emitter = DatahubRestEmitter(\"http://localhost:8080\")\n",
    "        \n",
    "        # Create dataset URN\n",
    "        dataset_urn = make_dataset_urn(\n",
    "            platform=\"snowflake\",\n",
    "            name=\"analytics.public.customer_metrics\"\n",
    "        )\n",
    "        \n",
    "        # Dataset properties (business metadata)\n",
    "        properties = DatasetPropertiesClass(\n",
    "            name=\"Customer Metrics\",\n",
    "            description=\"Aggregated customer metrics including LTV and churn risk\",\n",
    "            customProperties={\n",
    "                \"owner\": \"data-engineering\",\n",
    "                \"domain\": \"Finance\",\n",
    "                \"pii\": \"true\",\n",
    "                \"refresh_frequency\": \"daily\"\n",
    "            },\n",
    "            tags=[\"production\", \"critical\", \"pii\"]\n",
    "        )\n",
    "        \n",
    "        # Emit to DataHub\n",
    "        # emitter.emit_mcp(...)  # Actual emission\n",
    "        \n",
    "        print(\"DataHub emission example prepared.\")\n",
    "        return dataset_urn, properties\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"acryl-datahub not installed.\")\n",
    "        print(\"Install with: pip install acryl-datahub\")\n",
    "        return None, None\n",
    "\n",
    "# Show the code structure\n",
    "print(\"=== DataHub Python SDK Example ===\")\n",
    "print(\"\"\"\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.emitter.mce_builder import make_dataset_urn\n",
    "\n",
    "# Connect to DataHub\n",
    "emitter = DatahubRestEmitter(\"http://localhost:8080\")\n",
    "\n",
    "# Create dataset URN\n",
    "dataset_urn = make_dataset_urn(\n",
    "    platform=\"snowflake\",\n",
    "    name=\"analytics.public.customer_metrics\"\n",
    ")\n",
    "\n",
    "# Emit metadata\n",
    "emitter.emit_mcp(\n",
    "    entityUrn=dataset_urn,\n",
    "    aspectName=\"datasetProperties\",\n",
    "    aspect=DatasetPropertiesClass(\n",
    "        name=\"Customer Metrics\",\n",
    "        description=\"Customer LTV and churn metrics\"\n",
    "    )\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ada26f",
   "metadata": {},
   "source": [
    "## Data Discovery and Search\n",
    "\n",
    "Effective data discovery enables users to find relevant data assets quickly. Modern catalogs provide multiple discovery mechanisms:\n",
    "\n",
    "### Discovery Approaches\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    DATA DISCOVERY METHODS                       │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  ┌─────────────────┐  ┌─────────────────┐  ┌────────────────┐  │\n",
    "│  │   SEARCH        │  │    BROWSE       │  │   RECOMMEND    │  │\n",
    "│  ├─────────────────┤  ├─────────────────┤  ├────────────────┤  │\n",
    "│  │ • Keyword       │  │ • By Domain     │  │ • Popular      │  │\n",
    "│  │ • Semantic      │  │ • By Owner      │  │ • Similar      │  │\n",
    "│  │ • Filters       │  │ • By Tag        │  │ • Trending     │  │\n",
    "│  │ • Faceted       │  │ • By Source     │  │ • Personalized │  │\n",
    "│  └─────────────────┘  └─────────────────┘  └────────────────┘  │\n",
    "│                                                                 │\n",
    "│  ┌─────────────────┐  ┌─────────────────┐  ┌────────────────┐  │\n",
    "│  │   LINEAGE       │  │   GLOSSARY      │  │   GOVERNANCE   │  │\n",
    "│  ├─────────────────┤  ├─────────────────┤  ├────────────────┤  │\n",
    "│  │ • Upstream      │  │ • Term search   │  │ • By policy    │  │\n",
    "│  │ • Downstream    │  │ • Definitions   │  │ • Compliance   │  │\n",
    "│  │ • Impact        │  │ • Related terms │  │ • Certified    │  │\n",
    "│  └─────────────────┘  └─────────────────┘  └────────────────┘  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Search Best Practices\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Rich descriptions** | Write clear, searchable descriptions |\n",
    "| **Consistent tagging** | Use standardized tag taxonomy |\n",
    "| **Business terms** | Link to glossary terms |\n",
    "| **Ownership** | Assign clear data owners |\n",
    "| **Certification** | Mark trusted, verified datasets |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67880068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Simple in-memory data catalog with search\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class DataAsset:\n",
    "    \"\"\"Represents a data asset in the catalog.\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    description: str\n",
    "    platform: str  # e.g., \"snowflake\", \"s3\", \"postgres\"\n",
    "    schema: str\n",
    "    owner: str\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    domain: str = \"\"\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    updated_at: datetime = field(default_factory=datetime.now)\n",
    "    columns: List[Dict] = field(default_factory=list)\n",
    "    certified: bool = False\n",
    "    pii: bool = False\n",
    "\n",
    "\n",
    "class SimpleDataCatalog:\n",
    "    \"\"\"A simple in-memory data catalog with search capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.assets: Dict[str, DataAsset] = {}\n",
    "        self.search_index: Dict[str, set] = {}  # term -> asset_ids\n",
    "    \n",
    "    def register_asset(self, asset: DataAsset) -> None:\n",
    "        \"\"\"Register a new data asset in the catalog.\"\"\"\n",
    "        self.assets[asset.id] = asset\n",
    "        self._index_asset(asset)\n",
    "        print(f\"Registered: {asset.name}\")\n",
    "    \n",
    "    def _index_asset(self, asset: DataAsset) -> None:\n",
    "        \"\"\"Build search index for the asset.\"\"\"\n",
    "        # Index name, description, tags, owner, domain\n",
    "        terms = set()\n",
    "        \n",
    "        # Tokenize and add to terms\n",
    "        for text in [asset.name, asset.description, asset.owner, asset.domain, asset.platform]:\n",
    "            terms.update(re.findall(r'\\w+', text.lower()))\n",
    "        \n",
    "        terms.update(t.lower() for t in asset.tags)\n",
    "        \n",
    "        # Add to inverted index\n",
    "        for term in terms:\n",
    "            if term not in self.search_index:\n",
    "                self.search_index[term] = set()\n",
    "            self.search_index[term].add(asset.id)\n",
    "    \n",
    "    def search(self, query: str, filters: Optional[Dict] = None) -> List[DataAsset]:\n",
    "        \"\"\"\n",
    "        Search for assets matching the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Search terms\n",
    "            filters: Optional filters (platform, domain, certified, pii)\n",
    "        \"\"\"\n",
    "        query_terms = re.findall(r'\\w+', query.lower())\n",
    "        \n",
    "        if not query_terms:\n",
    "            matching_ids = set(self.assets.keys())\n",
    "        else:\n",
    "            # Find assets matching all query terms (AND logic)\n",
    "            matching_ids = None\n",
    "            for term in query_terms:\n",
    "                term_matches = self.search_index.get(term, set())\n",
    "                if matching_ids is None:\n",
    "                    matching_ids = term_matches.copy()\n",
    "                else:\n",
    "                    matching_ids &= term_matches\n",
    "            \n",
    "            matching_ids = matching_ids or set()\n",
    "        \n",
    "        # Apply filters\n",
    "        results = [self.assets[aid] for aid in matching_ids]\n",
    "        \n",
    "        if filters:\n",
    "            if \"platform\" in filters:\n",
    "                results = [a for a in results if a.platform == filters[\"platform\"]]\n",
    "            if \"domain\" in filters:\n",
    "                results = [a for a in results if a.domain == filters[\"domain\"]]\n",
    "            if \"certified\" in filters:\n",
    "                results = [a for a in results if a.certified == filters[\"certified\"]]\n",
    "            if \"pii\" in filters:\n",
    "                results = [a for a in results if a.pii == filters[\"pii\"]]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def browse_by_domain(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Browse assets grouped by domain.\"\"\"\n",
    "        domains = {}\n",
    "        for asset in self.assets.values():\n",
    "            domain = asset.domain or \"Uncategorized\"\n",
    "            if domain not in domains:\n",
    "                domains[domain] = []\n",
    "            domains[domain].append(asset.name)\n",
    "        return domains\n",
    "    \n",
    "    def get_asset(self, asset_id: str) -> Optional[DataAsset]:\n",
    "        \"\"\"Get a specific asset by ID.\"\"\"\n",
    "        return self.assets.get(asset_id)\n",
    "\n",
    "\n",
    "# Demo the catalog\n",
    "catalog = SimpleDataCatalog()\n",
    "\n",
    "# Register sample assets\n",
    "catalog.register_asset(DataAsset(\n",
    "    id=\"ds-001\",\n",
    "    name=\"customer_transactions\",\n",
    "    description=\"Daily customer transaction records with payment details\",\n",
    "    platform=\"snowflake\",\n",
    "    schema=\"analytics.finance\",\n",
    "    owner=\"finance-team\",\n",
    "    tags=[\"transactions\", \"finance\", \"daily\"],\n",
    "    domain=\"Finance\",\n",
    "    certified=True,\n",
    "    pii=True\n",
    "))\n",
    "\n",
    "catalog.register_asset(DataAsset(\n",
    "    id=\"ds-002\",\n",
    "    name=\"product_catalog\",\n",
    "    description=\"Master product catalog with SKU and pricing information\",\n",
    "    platform=\"postgres\",\n",
    "    schema=\"ecommerce.products\",\n",
    "    owner=\"product-team\",\n",
    "    tags=[\"products\", \"pricing\", \"master-data\"],\n",
    "    domain=\"E-Commerce\",\n",
    "    certified=True\n",
    "))\n",
    "\n",
    "catalog.register_asset(DataAsset(\n",
    "    id=\"ds-003\",\n",
    "    name=\"user_events\",\n",
    "    description=\"Clickstream and user behavior events from web and mobile\",\n",
    "    platform=\"s3\",\n",
    "    schema=\"raw.events\",\n",
    "    owner=\"analytics-team\",\n",
    "    tags=[\"events\", \"clickstream\", \"real-time\"],\n",
    "    domain=\"Analytics\"\n",
    "))\n",
    "\n",
    "print(\"\\n=== Search Examples ===\")\n",
    "print(\"\\nSearch 'customer':\")\n",
    "for asset in catalog.search(\"customer\"):\n",
    "    print(f\"  - {asset.name} ({asset.platform})\")\n",
    "\n",
    "print(\"\\nSearch 'finance' with certified=True filter:\")\n",
    "for asset in catalog.search(\"finance\", filters={\"certified\": True}):\n",
    "    print(f\"  - {asset.name} (certified: {asset.certified})\")\n",
    "\n",
    "print(\"\\nBrowse by domain:\")\n",
    "for domain, assets in catalog.browse_by_domain().items():\n",
    "    print(f\"  {domain}: {assets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207f886",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Concept | Key Points |\n",
    "|---------|------------|\n",
    "| **Metadata Types** | Technical (schema), Business (context), Operational (runtime) |\n",
    "| **Data Catalog** | Centralized inventory for discovery, governance, and collaboration |\n",
    "| **Tool Selection** | Choose based on ecosystem (AWS→Glue, Hadoop→Atlas, Modern→DataHub) |\n",
    "| **Discovery** | Enable search, browse, and recommendations for self-service |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Automate metadata collection** – Use crawlers and integrations to keep metadata fresh\n",
    "2. **Define ownership** – Every dataset needs a clear owner and steward\n",
    "3. **Establish a business glossary** – Standardize terminology across the organization\n",
    "4. **Track lineage** – Understand data flow for impact analysis and debugging\n",
    "5. **Classify sensitive data** – Tag PII/sensitive data for compliance\n",
    "6. **Enable collaboration** – Allow comments, ratings, and knowledge sharing\n",
    "\n",
    "### Tool Selection Guide\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────────────┐\n",
    "                    │     Which catalog to use?       │\n",
    "                    └────────────────┬────────────────┘\n",
    "                                     │\n",
    "                    ┌────────────────▼────────────────┐\n",
    "                    │   Using AWS ecosystem heavily?  │\n",
    "                    └────────────────┬────────────────┘\n",
    "                           Yes │           │ No\n",
    "                    ┌──────────▼──┐  ┌─────▼──────────┐\n",
    "                    │  AWS Glue   │  │ Hadoop-based?  │\n",
    "                    │  Data       │  └────────┬───────┘\n",
    "                    │  Catalog    │     Yes │     │ No\n",
    "                    └─────────────┘  ┌──────▼──┐ ┌─▼──────────┐\n",
    "                                     │ Apache  │ │ Enterprise │\n",
    "                                     │ Atlas   │ │ features?  │\n",
    "                                     └─────────┘ └────┬───────┘\n",
    "                                                Yes │     │ No\n",
    "                                               ┌────▼──┐ ┌─▼──────┐\n",
    "                                               │ Atlan │ │DataHub │\n",
    "                                               └───────┘ └────────┘\n",
    "```\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [DataHub Documentation](https://datahubproject.io/docs/)\n",
    "- [AWS Glue Data Catalog Guide](https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html)\n",
    "- [Apache Atlas Architecture](https://atlas.apache.org/Architecture.html)\n",
    "- [The Data Catalog Vendor Landscape (Atlan Blog)](https://atlan.com/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
