{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f457eee",
   "metadata": {},
   "source": [
    "# Data Transformation Patterns\n",
    "\n",
    "Data transformation is the **T** in ETL/ELT pipelinesâ€”the process of converting raw data into a format suitable for analysis, reporting, or downstream applications.\n",
    "\n",
    "## Why Transformation Matters\n",
    "\n",
    "| Challenge | Transformation Solution |\n",
    "|-----------|------------------------|\n",
    "| Inconsistent formats | Standardization & normalization |\n",
    "| Missing/invalid data | Cleansing & imputation |\n",
    "| Disparate sources | Integration & joining |\n",
    "| Raw granularity | Aggregation & summarization |\n",
    "| Lacking context | Enrichment & derivation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4ac77",
   "metadata": {},
   "source": [
    "## Transformation Categories\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  DATA TRANSFORMATIONS                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   STRUCTURAL    â”‚   SEMANTIC      â”‚   ANALYTICAL            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ Type casting  â”‚ â€¢ Cleansing     â”‚ â€¢ Aggregation           â”‚\n",
    "â”‚ â€¢ Renaming      â”‚ â€¢ Normalization â”‚ â€¢ Joining/Merging       â”‚\n",
    "â”‚ â€¢ Reshaping     â”‚ â€¢ Enrichment    â”‚ â€¢ Window functions      â”‚\n",
    "â”‚ â€¢ Filtering     â”‚ â€¢ Standardizing â”‚ â€¢ Derived metrics       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba2b9c",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Cleansing\n",
    "\n",
    "**Data cleansing** fixes or removes incorrect, corrupted, duplicate, or incomplete data.\n",
    "\n",
    "### Common Cleansing Operations\n",
    "- **Null/Missing handling**: Fill, drop, or interpolate\n",
    "- **Duplicate removal**: Identify and deduplicate records\n",
    "- **Outlier detection**: Cap, remove, or flag anomalies\n",
    "- **Format standardization**: Consistent date/string formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dirty data\n",
    "raw_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Bob', None, 'Charlie', 'Diana'],\n",
    "    'email': ['alice@test.com', 'BOB@TEST.COM', 'bob@test.com', 'invalid', 'charlie@test.com', None],\n",
    "    'amount': [100.0, 200.0, 200.0, -50.0, 1000000.0, 300.0],\n",
    "    'date': ['2024-01-15', '01/20/2024', '2024-01-20', '2024-02-01', '2024-02-15', 'invalid']\n",
    "})\n",
    "\n",
    "print(\"=== Raw Data ===\")\n",
    "print(raw_data)\n",
    "print(f\"\\nDuplicates: {raw_data.duplicated(subset=['customer_id']).sum()}\")\n",
    "print(f\"Null values:\\n{raw_data.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e44ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply comprehensive data cleansing.\"\"\"\n",
    "    cleaned = df.copy()\n",
    "    \n",
    "    # 1. Remove duplicates (keep first occurrence)\n",
    "    cleaned = cleaned.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    cleaned['name'] = cleaned['name'].fillna('Unknown')\n",
    "    cleaned['email'] = cleaned['email'].fillna('no-email@placeholder.com')\n",
    "    \n",
    "    # 3. Standardize email format (lowercase)\n",
    "    cleaned['email'] = cleaned['email'].str.lower().str.strip()\n",
    "    \n",
    "    # 4. Handle outliers in amount (cap at 99th percentile)\n",
    "    cap_value = cleaned['amount'].quantile(0.99)\n",
    "    cleaned['amount'] = cleaned['amount'].clip(lower=0, upper=cap_value)\n",
    "    \n",
    "    # 5. Standardize date format\n",
    "    cleaned['date'] = pd.to_datetime(cleaned['date'], errors='coerce')\n",
    "    cleaned['date'] = cleaned['date'].fillna(pd.Timestamp.now())\n",
    "    \n",
    "    return cleaned.reset_index(drop=True)\n",
    "\n",
    "cleaned_data = cleanse_data(raw_data)\n",
    "print(\"=== Cleaned Data ===\")\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b5f76",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Normalization\n",
    "\n",
    "**Normalization** transforms data to a common scale or format without distorting differences.\n",
    "\n",
    "### Types of Normalization\n",
    "\n",
    "| Type | Formula | Use Case |\n",
    "|------|---------|----------|\n",
    "| Min-Max | $(x - min) / (max - min)$ | Scale to [0, 1] range |\n",
    "| Z-Score | $(x - \\mu) / \\sigma$ | Standardize distribution |\n",
    "| Decimal Scaling | $x / 10^j$ | Move decimal point |\n",
    "| Log Transform | $\\log(x)$ | Reduce skewness |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization examples\n",
    "sales_data = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'revenue': [1000, 5000, 15000, 50000, 200000],\n",
    "    'units_sold': [10, 50, 100, 300, 800]\n",
    "})\n",
    "\n",
    "def normalize_column(series: pd.Series, method: str = 'minmax') -> pd.Series:\n",
    "    \"\"\"Normalize a numeric series using various methods.\"\"\"\n",
    "    if method == 'minmax':\n",
    "        return (series - series.min()) / (series.max() - series.min())\n",
    "    elif method == 'zscore':\n",
    "        return (series - series.mean()) / series.std()\n",
    "    elif method == 'log':\n",
    "        return np.log1p(series)  # log(1 + x) to handle zeros\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "# Apply different normalizations\n",
    "normalized = sales_data.copy()\n",
    "normalized['revenue_minmax'] = normalize_column(sales_data['revenue'], 'minmax')\n",
    "normalized['revenue_zscore'] = normalize_column(sales_data['revenue'], 'zscore')\n",
    "normalized['revenue_log'] = normalize_column(sales_data['revenue'], 'log')\n",
    "\n",
    "print(\"=== Normalized Data ===\")\n",
    "print(normalized.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d1c79",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Enrichment\n",
    "\n",
    "**Enrichment** enhances data by adding context, derived fields, or external information.\n",
    "\n",
    "### Common Enrichment Patterns\n",
    "- **Derived fields**: Calculate new columns from existing ones\n",
    "- **Lookups**: Join with reference/dimension tables\n",
    "- **Geocoding**: Add location data from addresses\n",
    "- **Time-based**: Extract date parts, add business days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6be83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction data\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': range(1, 6),\n",
    "    'customer_id': [101, 102, 101, 103, 102],\n",
    "    'product_id': ['P001', 'P002', 'P003', 'P001', 'P002'],\n",
    "    'quantity': [2, 1, 5, 3, 2],\n",
    "    'unit_price': [29.99, 149.99, 9.99, 29.99, 149.99],\n",
    "    'transaction_date': pd.to_datetime(['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'])\n",
    "})\n",
    "\n",
    "# Reference tables for enrichment\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103],\n",
    "    'customer_name': ['Alice Corp', 'Bob LLC', 'Charlie Inc'],\n",
    "    'tier': ['Gold', 'Silver', 'Bronze']\n",
    "})\n",
    "\n",
    "products = pd.DataFrame({\n",
    "    'product_id': ['P001', 'P002', 'P003'],\n",
    "    'product_name': ['Widget', 'Gadget', 'Accessory'],\n",
    "    'category': ['Hardware', 'Electronics', 'Accessories']\n",
    "})\n",
    "\n",
    "print(\"=== Original Transactions ===\")\n",
    "print(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89765eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_transactions(txn: pd.DataFrame, cust: pd.DataFrame, prod: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Enrich transactions with customer and product details.\"\"\"\n",
    "    enriched = txn.copy()\n",
    "    \n",
    "    # 1. Derived fields\n",
    "    enriched['total_amount'] = enriched['quantity'] * enriched['unit_price']\n",
    "    enriched['day_of_week'] = enriched['transaction_date'].dt.day_name()\n",
    "    enriched['is_weekend'] = enriched['transaction_date'].dt.dayofweek >= 5\n",
    "    enriched['fiscal_quarter'] = enriched['transaction_date'].dt.quarter\n",
    "    \n",
    "    # 2. Lookup enrichment - join with dimension tables\n",
    "    enriched = enriched.merge(cust[['customer_id', 'customer_name', 'tier']], \n",
    "                               on='customer_id', how='left')\n",
    "    enriched = enriched.merge(prod[['product_id', 'product_name', 'category']], \n",
    "                               on='product_id', how='left')\n",
    "    \n",
    "    # 3. Tier-based discount calculation\n",
    "    tier_discounts = {'Gold': 0.15, 'Silver': 0.10, 'Bronze': 0.05}\n",
    "    enriched['discount_rate'] = enriched['tier'].map(tier_discounts)\n",
    "    enriched['discounted_amount'] = enriched['total_amount'] * (1 - enriched['discount_rate'])\n",
    "    \n",
    "    return enriched\n",
    "\n",
    "enriched_txn = enrich_transactions(transactions, customers, products)\n",
    "print(\"=== Enriched Transactions ===\")\n",
    "print(enriched_txn[['transaction_id', 'customer_name', 'product_name', 'total_amount', 'discounted_amount', 'day_of_week']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9abba",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Type Conversion\n",
    "\n",
    "**Type conversion** ensures data types are appropriate for storage, computation, and analysis.\n",
    "\n",
    "### Common Conversions\n",
    "\n",
    "| From | To | Method |\n",
    "|------|-----|--------|\n",
    "| String â†’ Date | `pd.to_datetime()` | Parse date strings |\n",
    "| Float â†’ Int | `.astype(int)` | Truncate decimals |\n",
    "| Object â†’ Category | `.astype('category')` | Memory optimization |\n",
    "| String â†’ Numeric | `pd.to_numeric()` | Parse numbers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with mixed types\n",
    "mixed_data = pd.DataFrame({\n",
    "    'id': ['1', '2', '3', '4', '5'],\n",
    "    'price': ['100.50', '200', '350.75', 'N/A', '500'],\n",
    "    'quantity': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    'date_str': ['2024-01-01', '2024/02/15', 'Mar 20, 2024', '20-04-2024', '2024.05.01'],\n",
    "    'status': ['active', 'inactive', 'active', 'active', 'inactive']\n",
    "})\n",
    "\n",
    "print(\"=== Original Data Types ===\")\n",
    "print(mixed_data.dtypes)\n",
    "print(f\"\\nMemory usage: {mixed_data.memory_usage(deep=True).sum()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert columns to appropriate data types.\"\"\"\n",
    "    converted = df.copy()\n",
    "    \n",
    "    # String to integer\n",
    "    converted['id'] = pd.to_numeric(converted['id'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    # String to float (handle N/A)\n",
    "    converted['price'] = pd.to_numeric(converted['price'], errors='coerce')\n",
    "    \n",
    "    # Float to integer\n",
    "    converted['quantity'] = converted['quantity'].astype('Int64')\n",
    "    \n",
    "    # String to datetime (handle multiple formats)\n",
    "    converted['date'] = pd.to_datetime(converted['date_str'], errors='coerce', dayfirst=False)\n",
    "    converted = converted.drop('date_str', axis=1)\n",
    "    \n",
    "    # String to category (memory optimization)\n",
    "    converted['status'] = converted['status'].astype('category')\n",
    "    \n",
    "    return converted\n",
    "\n",
    "converted_data = convert_types(mixed_data)\n",
    "print(\"=== Converted Data Types ===\")\n",
    "print(converted_data.dtypes)\n",
    "print(f\"\\nMemory usage: {converted_data.memory_usage(deep=True).sum()} bytes\")\n",
    "print(\"\\n\", converted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1d70c",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Aggregation\n",
    "\n",
    "**Aggregation** summarizes detailed data into higher-level metrics.\n",
    "\n",
    "### Common Aggregation Functions\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Numeric: sum, mean, median, min, max, std, var, count â”‚\n",
    "â”‚ String:  first, last, count, nunique                  â”‚\n",
    "â”‚ Date:    min, max, count                              â”‚\n",
    "â”‚ Custom:  lambda x: your_function(x)                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa73d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed sales data\n",
    "sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=20, freq='D').repeat(3),\n",
    "    'region': ['North', 'South', 'West'] * 20,\n",
    "    'product': np.random.choice(['A', 'B', 'C'], 60),\n",
    "    'quantity': np.random.randint(1, 100, 60),\n",
    "    'revenue': np.random.uniform(100, 1000, 60).round(2)\n",
    "})\n",
    "\n",
    "print(\"=== Sample Sales Data (first 10 rows) ===\")\n",
    "print(sales.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregation examples\n",
    "\n",
    "# 1. Simple group-by aggregation\n",
    "region_summary = sales.groupby('region').agg({\n",
    "    'quantity': 'sum',\n",
    "    'revenue': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "region_summary.columns = ['total_qty', 'total_revenue', 'avg_revenue', 'num_transactions']\n",
    "print(\"=== By Region ===\")\n",
    "print(region_summary)\n",
    "\n",
    "# 2. Multi-level aggregation\n",
    "pivot_summary = sales.pivot_table(\n",
    "    values='revenue',\n",
    "    index='region',\n",
    "    columns='product',\n",
    "    aggfunc=['sum', 'count'],\n",
    "    fill_value=0\n",
    ").round(2)\n",
    "print(\"\\n=== Pivot Table (Region x Product) ===\")\n",
    "print(pivot_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Rolling/Window aggregations\n",
    "daily_sales = sales.groupby('date')['revenue'].sum().reset_index()\n",
    "daily_sales['rolling_7d_avg'] = daily_sales['revenue'].rolling(window=7).mean()\n",
    "daily_sales['cumulative_revenue'] = daily_sales['revenue'].cumsum()\n",
    "daily_sales['pct_change'] = daily_sales['revenue'].pct_change() * 100\n",
    "\n",
    "print(\"=== Time-based Aggregations ===\")\n",
    "print(daily_sales.round(2).tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7853f64",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Joining/Merging\n",
    "\n",
    "**Joining** combines data from multiple sources based on common keys.\n",
    "\n",
    "### Join Types\n",
    "\n",
    "```\n",
    "      LEFT JOIN                 INNER JOIN              FULL OUTER JOIN\n",
    "    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
    "    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚             â”‚     â”‚     â”‚             â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚\n",
    "    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆ â”‚             â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆ â”‚             â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚\n",
    "    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚             â”‚     â”‚     â”‚             â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚\n",
    "    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n",
    "    All from left +           Only matching             All from both\n",
    "    matching right            records                   sides\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample tables\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3, 4, 5],\n",
    "    'customer_id': [101, 102, 103, 104, 105],\n",
    "    'amount': [500, 750, 200, 1200, 350]\n",
    "})\n",
    "\n",
    "customers_dim = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103, 106],  # Note: 104, 105 missing; 106 extra\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Frank'],\n",
    "    'segment': ['Enterprise', 'SMB', 'Enterprise', 'SMB']\n",
    "})\n",
    "\n",
    "print(\"=== Orders ===\")\n",
    "print(orders)\n",
    "print(\"\\n=== Customers ===\")\n",
    "print(customers_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8612ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different join types\n",
    "print(\"=== INNER JOIN ===\")\n",
    "inner = orders.merge(customers_dim, on='customer_id', how='inner')\n",
    "print(inner)\n",
    "\n",
    "print(\"\\n=== LEFT JOIN ===\")\n",
    "left = orders.merge(customers_dim, on='customer_id', how='left')\n",
    "print(left)\n",
    "\n",
    "print(\"\\n=== OUTER JOIN ===\")\n",
    "outer = orders.merge(customers_dim, on='customer_id', how='outer')\n",
    "print(outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71909621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Multiple key join and self-join\n",
    "order_items = pd.DataFrame({\n",
    "    'order_id': [1, 1, 2, 2, 3],\n",
    "    'product_id': ['P1', 'P2', 'P1', 'P3', 'P2'],\n",
    "    'quantity': [2, 1, 3, 2, 1]\n",
    "})\n",
    "\n",
    "prices = pd.DataFrame({\n",
    "    'product_id': ['P1', 'P2', 'P3'],\n",
    "    'price': [100, 250, 75]\n",
    "})\n",
    "\n",
    "# Multi-step join\n",
    "order_details = (\n",
    "    order_items\n",
    "    .merge(prices, on='product_id')\n",
    "    .assign(line_total=lambda x: x['quantity'] * x['price'])\n",
    "    .merge(orders[['order_id', 'customer_id']], on='order_id')\n",
    ")\n",
    "\n",
    "print(\"=== Order Details with Prices ===\")\n",
    "print(order_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf73f7e",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. SQL vs Python Transformations\n",
    "\n",
    "Most transformations can be done in either SQL or Python. Here's a comparison:\n",
    "\n",
    "| Operation | SQL | Python (pandas) |\n",
    "|-----------|-----|------------------|\n",
    "| Filter | `WHERE condition` | `df[df['col'] > value]` |\n",
    "| Select | `SELECT col1, col2` | `df[['col1', 'col2']]` |\n",
    "| Aggregate | `GROUP BY ... SUM()` | `df.groupby().agg()` |\n",
    "| Join | `JOIN ... ON` | `df.merge()` |\n",
    "| Sort | `ORDER BY` | `df.sort_values()` |\n",
    "| Window | `OVER (PARTITION BY)` | `df.groupby().transform()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL-equivalent transformations in pandas\n",
    "\n",
    "# Sample data\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': range(1, 11),\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry', 'Ivy', 'Jack'],\n",
    "    'department': ['Sales', 'Sales', 'Engineering', 'Engineering', 'Sales', 'HR', 'Engineering', 'HR', 'Sales', 'Engineering'],\n",
    "    'salary': [70000, 65000, 95000, 105000, 72000, 55000, 88000, 58000, 68000, 92000],\n",
    "    'hire_date': pd.to_datetime(['2020-01-15', '2019-06-01', '2021-03-20', '2018-11-10', '2022-02-28',\n",
    "                                  '2020-08-05', '2021-07-12', '2019-04-22', '2023-01-10', '2022-09-01'])\n",
    "})\n",
    "\n",
    "print(\"=== Employee Data ===\")\n",
    "print(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1805600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL: SELECT * FROM employees WHERE salary > 70000 ORDER BY salary DESC\n",
    "high_earners = (\n",
    "    employees[employees['salary'] > 70000]\n",
    "    .sort_values('salary', ascending=False)\n",
    ")\n",
    "print(\"=== High Earners (salary > 70k) ===\")\n",
    "print(high_earners)\n",
    "\n",
    "# SQL: SELECT department, COUNT(*), AVG(salary) FROM employees GROUP BY department\n",
    "dept_stats = (\n",
    "    employees\n",
    "    .groupby('department')\n",
    "    .agg(\n",
    "        employee_count=('emp_id', 'count'),\n",
    "        avg_salary=('salary', 'mean'),\n",
    "        total_salary=('salary', 'sum')\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "print(\"\\n=== Department Statistics ===\")\n",
    "print(dept_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650af1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL: Window function equivalent\n",
    "# SELECT *, \n",
    "#   AVG(salary) OVER (PARTITION BY department) as dept_avg,\n",
    "#   RANK() OVER (PARTITION BY department ORDER BY salary DESC) as salary_rank\n",
    "# FROM employees\n",
    "\n",
    "employees_with_windows = employees.copy()\n",
    "\n",
    "# Department average (window function)\n",
    "employees_with_windows['dept_avg_salary'] = (\n",
    "    employees_with_windows\n",
    "    .groupby('department')['salary']\n",
    "    .transform('mean')\n",
    ")\n",
    "\n",
    "# Salary rank within department\n",
    "employees_with_windows['salary_rank'] = (\n",
    "    employees_with_windows\n",
    "    .groupby('department')['salary']\n",
    "    .rank(ascending=False, method='dense')\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Salary vs department average\n",
    "employees_with_windows['vs_dept_avg'] = (\n",
    "    (employees_with_windows['salary'] / employees_with_windows['dept_avg_salary'] - 1) * 100\n",
    ").round(1)\n",
    "\n",
    "print(\"=== Employees with Window Calculations ===\")\n",
    "print(employees_with_windows[['name', 'department', 'salary', 'dept_avg_salary', 'salary_rank', 'vs_dept_avg']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e3436",
   "metadata": {},
   "source": [
    "### SQL Transformation Example\n",
    "\n",
    "```sql\n",
    "-- Equivalent SQL for the above transformations\n",
    "\n",
    "-- Cleansing: Remove duplicates\n",
    "WITH deduplicated AS (\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY created_at) as rn\n",
    "    FROM raw_customers\n",
    ")\n",
    "SELECT * FROM deduplicated WHERE rn = 1;\n",
    "\n",
    "-- Aggregation with window functions\n",
    "SELECT \n",
    "    emp_id,\n",
    "    name,\n",
    "    department,\n",
    "    salary,\n",
    "    AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,\n",
    "    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as salary_rank,\n",
    "    ROUND((salary / AVG(salary) OVER (PARTITION BY department) - 1) * 100, 1) as vs_dept_avg\n",
    "FROM employees;\n",
    "\n",
    "-- Joining multiple tables\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    c.customer_name,\n",
    "    p.product_name,\n",
    "    oi.quantity * p.price as line_total\n",
    "FROM orders o\n",
    "INNER JOIN customers c ON o.customer_id = c.customer_id\n",
    "INNER JOIN order_items oi ON o.order_id = oi.order_id\n",
    "INNER JOIN products p ON oi.product_id = p.product_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc329f3",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Transformation Pipeline Pattern\n",
    "\n",
    "A reusable pattern for building maintainable transformation pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "from functools import reduce\n",
    "\n",
    "class TransformationPipeline:\n",
    "    \"\"\"A composable transformation pipeline for DataFrames.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps: List[Callable[[pd.DataFrame], pd.DataFrame]] = []\n",
    "    \n",
    "    def add_step(self, func: Callable[[pd.DataFrame], pd.DataFrame], name: str = None):\n",
    "        \"\"\"Add a transformation step to the pipeline.\"\"\"\n",
    "        func.__name__ = name or func.__name__\n",
    "        self.steps.append(func)\n",
    "        return self\n",
    "    \n",
    "    def run(self, df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Execute all transformation steps in order.\"\"\"\n",
    "        result = df.copy()\n",
    "        for step in self.steps:\n",
    "            if verbose:\n",
    "                print(f\"Running: {step.__name__}\")\n",
    "            result = step(result)\n",
    "        return result\n",
    "\n",
    "# Define individual transformation functions\n",
    "def clean_nulls(df):\n",
    "    return df.fillna({'name': 'Unknown', 'amount': 0})\n",
    "\n",
    "def standardize_text(df):\n",
    "    df = df.copy()\n",
    "    df['name'] = df['name'].str.strip().str.title()\n",
    "    return df\n",
    "\n",
    "def add_derived_fields(df):\n",
    "    df = df.copy()\n",
    "    df['amount_category'] = pd.cut(df['amount'], bins=[0, 100, 500, float('inf')], labels=['Low', 'Medium', 'High'])\n",
    "    return df\n",
    "\n",
    "# Build and run pipeline\n",
    "pipeline = (\n",
    "    TransformationPipeline()\n",
    "    .add_step(clean_nulls, 'Clean Nulls')\n",
    "    .add_step(standardize_text, 'Standardize Text')\n",
    "    .add_step(add_derived_fields, 'Add Categories')\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['  alice  ', None, 'BOB'],\n",
    "    'amount': [50, 250, None]\n",
    "})\n",
    "\n",
    "result = pipeline.run(test_data, verbose=True)\n",
    "print(\"\\n=== Pipeline Result ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b27d4",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Œ Key Takeaways\n",
    "\n",
    "### Transformation Categories\n",
    "\n",
    "| Category | Purpose | Key Operations |\n",
    "|----------|---------|----------------|\n",
    "| **Cleansing** | Fix data quality issues | Null handling, deduplication, outlier treatment |\n",
    "| **Normalization** | Standardize scales/formats | Min-max, z-score, log transform |\n",
    "| **Enrichment** | Add context/derived data | Lookups, calculated fields, geocoding |\n",
    "| **Type Conversion** | Ensure correct data types | Parse dates, numeric conversion, categorization |\n",
    "| **Aggregation** | Summarize data | Group by, pivot, window functions |\n",
    "| **Joining** | Combine data sources | Inner, left, outer, cross joins |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Idempotency**: Transformations should produce the same result when run multiple times\n",
    "2. **Immutability**: Create new DataFrames instead of modifying in place\n",
    "3. **Composability**: Build small, focused functions that can be chained\n",
    "4. **Testing**: Validate transformations with unit tests and data quality checks\n",
    "5. **Documentation**: Document transformation logic and business rules\n",
    "\n",
    "### SQL vs Python Decision\n",
    "\n",
    "| Use SQL When | Use Python When |\n",
    "|--------------|------------------|\n",
    "| Data is already in database | Complex logic/loops required |\n",
    "| Simple aggregations/joins | ML preprocessing needed |\n",
    "| Performance-critical queries | Prototyping/exploration |\n",
    "| Team SQL proficiency is high | External API calls needed |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
