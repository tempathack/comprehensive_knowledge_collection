{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bc4aac",
   "metadata": {},
   "source": [
    "# Data Loading Strategies\n",
    "\n",
    "Data loading is the final and critical phase of the ETL/ELT pipeline where transformed data is written to the target destination. The choice of loading strategy significantly impacts:\n",
    "\n",
    "- **Performance**: How fast data can be loaded\n",
    "- **Data Integrity**: Ensuring consistency and accuracy\n",
    "- **Resource Utilization**: CPU, memory, and I/O consumption\n",
    "- **Downtime**: Impact on target system availability\n",
    "\n",
    "This notebook covers the essential loading patterns, strategies, and best practices used in modern data engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84562823",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Loading Patterns Overview\n",
    "\n",
    "### Common Loading Destinations\n",
    "\n",
    "| Destination | Use Case | Common Tools |\n",
    "|-------------|----------|-------------|\n",
    "| **Data Warehouse** | Analytics, BI reporting | Snowflake, BigQuery, Redshift |\n",
    "| **Data Lake** | Raw storage, ML pipelines | S3, ADLS, GCS |\n",
    "| **OLTP Database** | Transactional systems | PostgreSQL, MySQL, SQL Server |\n",
    "| **NoSQL Database** | Flexible schema, high throughput | MongoDB, DynamoDB, Cassandra |\n",
    "| **Search Engine** | Full-text search, log analytics | Elasticsearch, OpenSearch |\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "1. **Volume**: How much data needs to be loaded?\n",
    "2. **Velocity**: How frequently does data arrive?\n",
    "3. **Variety**: What format is the data in?\n",
    "4. **Consistency Requirements**: ACID vs eventual consistency\n",
    "5. **Target System Constraints**: Lock contention, index maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082d16d",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Full Load vs Incremental Load vs Upsert\n",
    "\n",
    "### 2.1 Full Load (Complete Refresh)\n",
    "\n",
    "**Description**: Replace all existing data with the complete source dataset.\n",
    "\n",
    "```\n",
    "┌─────────────────┐         ┌─────────────────┐\n",
    "│   Source Data   │  ───►   │   Target Table  │\n",
    "│  (Complete Set) │  LOAD   │  (Truncated &   │\n",
    "│                 │         │   Reloaded)     │\n",
    "└─────────────────┘         └─────────────────┘\n",
    "```\n",
    "\n",
    "**Pros**:\n",
    "- Simple to implement and understand\n",
    "- Ensures complete data consistency\n",
    "- No need to track changes\n",
    "\n",
    "**Cons**:\n",
    "- Slow for large datasets\n",
    "- High resource consumption\n",
    "- May cause downtime\n",
    "\n",
    "**Best For**: Small reference tables, dimension tables with infrequent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "\n",
    "def full_load(df: pd.DataFrame, table_name: str, engine, schema: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Perform a full load (truncate and reload) to target table.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to load\n",
    "        table_name: Target table name\n",
    "        engine: SQLAlchemy engine\n",
    "        schema: Optional schema name\n",
    "    \n",
    "    Returns:\n",
    "        dict: Load statistics\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Use replace to truncate and reload\n",
    "    df.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        schema=schema,\n",
    "        if_exists='replace',  # Drops and recreates table\n",
    "        index=False,\n",
    "        method='multi'  # Batch inserts\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    return {\n",
    "        'load_type': 'full',\n",
    "        'rows_loaded': len(df),\n",
    "        'duration_seconds': (end_time - start_time).total_seconds(),\n",
    "        'timestamp': end_time.isoformat()\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# engine = create_engine('postgresql://user:pass@localhost:5432/mydb')\n",
    "# df = pd.read_csv('products.csv')\n",
    "# stats = full_load(df, 'dim_products', engine)\n",
    "# print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814c5e9",
   "metadata": {},
   "source": [
    "### 2.2 Incremental Load (Delta Load)\n",
    "\n",
    "**Description**: Load only new or changed records since the last load.\n",
    "\n",
    "```\n",
    "┌─────────────────┐                    ┌─────────────────┐\n",
    "│   Source Data   │                    │   Target Table  │\n",
    "│  ┌───────────┐  │                    │                 │\n",
    "│  │  Changed  │  │  ───► APPEND ───►  │  + New Records  │\n",
    "│  │  Records  │  │                    │                 │\n",
    "│  └───────────┘  │                    │                 │\n",
    "└─────────────────┘                    └─────────────────┘\n",
    "```\n",
    "\n",
    "**Change Detection Methods**:\n",
    "1. **Timestamp-based**: Use `modified_at` or `created_at` columns\n",
    "2. **Sequence-based**: Use auto-increment IDs\n",
    "3. **Hash-based**: Compare row hashes\n",
    "4. **CDC (Change Data Capture)**: Database log-based tracking\n",
    "\n",
    "**Pros**:\n",
    "- Faster than full load\n",
    "- Lower resource consumption\n",
    "- Minimal impact on target system\n",
    "\n",
    "**Cons**:\n",
    "- Requires change tracking mechanism\n",
    "- Cannot detect hard deletes (without CDC)\n",
    "- More complex implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import hashlib\n",
    "\n",
    "def get_last_watermark(engine, table_name: str, watermark_column: str) -> Optional[datetime]:\n",
    "    \"\"\"\n",
    "    Get the last watermark (max timestamp/ID) from target table.\n",
    "    \"\"\"\n",
    "    query = text(f\"SELECT MAX({watermark_column}) FROM {table_name}\")\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query).scalar()\n",
    "    return result\n",
    "\n",
    "\n",
    "def incremental_load_timestamp(\n",
    "    source_df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    engine,\n",
    "    timestamp_column: str = 'modified_at'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Perform incremental load using timestamp-based change detection.\n",
    "    \n",
    "    Args:\n",
    "        source_df: Complete source DataFrame with timestamp column\n",
    "        table_name: Target table name\n",
    "        engine: SQLAlchemy engine\n",
    "        timestamp_column: Column used for change detection\n",
    "    \n",
    "    Returns:\n",
    "        dict: Load statistics\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Get last loaded timestamp (watermark)\n",
    "    last_watermark = get_last_watermark(engine, table_name, timestamp_column)\n",
    "    \n",
    "    if last_watermark:\n",
    "        # Filter for only new/changed records\n",
    "        delta_df = source_df[source_df[timestamp_column] > last_watermark]\n",
    "    else:\n",
    "        # First load - take all records\n",
    "        delta_df = source_df\n",
    "    \n",
    "    if len(delta_df) > 0:\n",
    "        # Append new records\n",
    "        delta_df.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    return {\n",
    "        'load_type': 'incremental',\n",
    "        'detection_method': 'timestamp',\n",
    "        'last_watermark': str(last_watermark),\n",
    "        'new_watermark': str(source_df[timestamp_column].max()),\n",
    "        'rows_loaded': len(delta_df),\n",
    "        'rows_skipped': len(source_df) - len(delta_df),\n",
    "        'duration_seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# stats = incremental_load_timestamp(df, 'fact_orders', engine, 'updated_at')\n",
    "# print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84260d",
   "metadata": {},
   "source": [
    "### 2.3 Upsert (Merge) Load\n",
    "\n",
    "**Description**: Insert new records and update existing ones based on a key.\n",
    "\n",
    "```\n",
    "┌─────────────────┐                    ┌─────────────────┐\n",
    "│   Source Data   │                    │   Target Table  │\n",
    "│  ┌───────────┐  │   Key Match?       │  ┌───────────┐  │\n",
    "│  │ Record A  │──┼───► YES ──────────►│  │ Update A  │  │\n",
    "│  ├───────────┤  │                    │  ├───────────┤  │\n",
    "│  │ Record B  │──┼───► NO  ──────────►│  │ Insert B  │  │\n",
    "│  └───────────┘  │                    │  └───────────┘  │\n",
    "└─────────────────┘                    └─────────────────┘\n",
    "```\n",
    "\n",
    "**Database Support**:\n",
    "- PostgreSQL: `INSERT ... ON CONFLICT DO UPDATE`\n",
    "- MySQL: `INSERT ... ON DUPLICATE KEY UPDATE`\n",
    "- SQL Server: `MERGE`\n",
    "- Snowflake: `MERGE`\n",
    "\n",
    "**Pros**:\n",
    "- Handles both inserts and updates\n",
    "- Maintains data currency\n",
    "- Idempotent operations\n",
    "\n",
    "**Cons**:\n",
    "- Slower than append-only\n",
    "- Requires proper key management\n",
    "- May cause lock contention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683dbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.dialects.postgresql import insert as pg_insert\n",
    "from sqlalchemy import Table, MetaData\n",
    "\n",
    "def upsert_postgresql(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    engine,\n",
    "    primary_keys: list,\n",
    "    update_columns: list = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Perform upsert (INSERT ... ON CONFLICT DO UPDATE) for PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to upsert\n",
    "        table_name: Target table name\n",
    "        engine: SQLAlchemy engine\n",
    "        primary_keys: List of primary key columns\n",
    "        update_columns: Columns to update on conflict (default: all non-key columns)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Upsert statistics\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Reflect table structure\n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    \n",
    "    # Determine columns to update\n",
    "    if update_columns is None:\n",
    "        update_columns = [col for col in df.columns if col not in primary_keys]\n",
    "    \n",
    "    # Convert DataFrame to records\n",
    "    records = df.to_dict(orient='records')\n",
    "    \n",
    "    # Build upsert statement\n",
    "    stmt = pg_insert(table).values(records)\n",
    "    \n",
    "    # Define update on conflict\n",
    "    update_dict = {col: stmt.excluded[col] for col in update_columns}\n",
    "    update_dict['updated_at'] = datetime.now()  # Track update time\n",
    "    \n",
    "    upsert_stmt = stmt.on_conflict_do_update(\n",
    "        index_elements=primary_keys,\n",
    "        set_=update_dict\n",
    "    )\n",
    "    \n",
    "    # Execute upsert\n",
    "    with engine.begin() as conn:\n",
    "        result = conn.execute(upsert_stmt)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    return {\n",
    "        'load_type': 'upsert',\n",
    "        'rows_processed': len(df),\n",
    "        'rows_affected': result.rowcount,\n",
    "        'duration_seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# stats = upsert_postgresql(\n",
    "#     df=customer_df,\n",
    "#     table_name='dim_customers',\n",
    "#     engine=engine,\n",
    "#     primary_keys=['customer_id'],\n",
    "#     update_columns=['name', 'email', 'phone']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f27ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_generic(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    engine,\n",
    "    primary_keys: list,\n",
    "    batch_size: int = 1000\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Database-agnostic upsert using staging table approach.\n",
    "    Works with any SQL database.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Load data to staging table\n",
    "    2. Delete matching records from target\n",
    "    3. Insert all from staging to target\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    staging_table = f\"stg_{table_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load to staging table\n",
    "        df.to_sql(\n",
    "            name=staging_table,\n",
    "            con=engine,\n",
    "            if_exists='replace',\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Build key matching condition\n",
    "        key_conditions = ' AND '.join(\n",
    "            [f\"{table_name}.{k} = {staging_table}.{k}\" for k in primary_keys]\n",
    "        )\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            # Step 2: Delete existing matching records\n",
    "            delete_sql = text(f\"\"\"\n",
    "                DELETE FROM {table_name}\n",
    "                WHERE EXISTS (\n",
    "                    SELECT 1 FROM {staging_table}\n",
    "                    WHERE {key_conditions}\n",
    "                )\n",
    "            \"\"\")\n",
    "            delete_result = conn.execute(delete_sql)\n",
    "            \n",
    "            # Step 3: Insert all records from staging\n",
    "            columns = ', '.join(df.columns)\n",
    "            insert_sql = text(f\"\"\"\n",
    "                INSERT INTO {table_name} ({columns})\n",
    "                SELECT {columns} FROM {staging_table}\n",
    "            \"\"\")\n",
    "            insert_result = conn.execute(insert_sql)\n",
    "            \n",
    "            # Cleanup staging table\n",
    "            conn.execute(text(f\"DROP TABLE IF EXISTS {staging_table}\"))\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Cleanup on error\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(f\"DROP TABLE IF EXISTS {staging_table}\"))\n",
    "        raise e\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    return {\n",
    "        'load_type': 'upsert_generic',\n",
    "        'rows_processed': len(df),\n",
    "        'rows_deleted': delete_result.rowcount,\n",
    "        'rows_inserted': insert_result.rowcount,\n",
    "        'duration_seconds': (end_time - start_time).total_seconds()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d6e26",
   "metadata": {},
   "source": [
    "### 2.4 Comparison Summary\n",
    "\n",
    "| Aspect | Full Load | Incremental | Upsert |\n",
    "|--------|-----------|-------------|--------|\n",
    "| **Complexity** | Low | Medium | Medium-High |\n",
    "| **Speed** | Slow (large data) | Fast | Medium |\n",
    "| **Resource Usage** | High | Low | Medium |\n",
    "| **Handles Deletes** | Yes | No* | No* |\n",
    "| **Handles Updates** | Yes | Append only | Yes |\n",
    "| **Data Freshness** | Complete | Delta only | Current |\n",
    "| **Idempotent** | Yes | No | Yes |\n",
    "\n",
    "*Requires soft deletes or CDC for delete handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2baf83",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Slowly Changing Dimensions (SCD)\n",
    "\n",
    "SCDs are dimension tables that change slowly over time. The approach to handling these changes affects historical analysis capabilities.\n",
    "\n",
    "### Common Example: Customer Dimension\n",
    "```\n",
    "Customer moves from New York to Los Angeles\n",
    "- How do we handle historical orders placed when they lived in NY?\n",
    "- Should reports show NY or LA for those orders?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150eb29",
   "metadata": {},
   "source": [
    "### 3.1 SCD Type 0 - Retain Original\n",
    "\n",
    "**Description**: Never update the dimension. Original values are preserved.\n",
    "\n",
    "```\n",
    "Original Record: {customer_id: 1, city: 'New York'}\n",
    "After Move:      {customer_id: 1, city: 'New York'}  ← Unchanged\n",
    "```\n",
    "\n",
    "**Use Case**: Birth date, original signup date, SSN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4485db0",
   "metadata": {},
   "source": [
    "### 3.2 SCD Type 1 - Overwrite\n",
    "\n",
    "**Description**: Update the dimension with new values. No history preserved.\n",
    "\n",
    "```\n",
    "Before: {customer_id: 1, name: 'John', city: 'New York'}\n",
    "After:  {customer_id: 1, name: 'John', city: 'Los Angeles'}  ← Overwritten\n",
    "```\n",
    "\n",
    "**Pros**: Simple, space-efficient\n",
    "**Cons**: No historical tracking\n",
    "\n",
    "**Use Case**: Correcting data errors, non-critical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scd_type1_update(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    engine,\n",
    "    natural_key: str,\n",
    "    update_columns: list\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    SCD Type 1: Simple overwrite of changed values.\n",
    "    \n",
    "    Args:\n",
    "        df: Source DataFrame with current values\n",
    "        table_name: Target dimension table\n",
    "        engine: SQLAlchemy engine\n",
    "        natural_key: Business key column\n",
    "        update_columns: Columns to update when changed\n",
    "    \"\"\"\n",
    "    updates = 0\n",
    "    inserts = 0\n",
    "    \n",
    "    # Get existing records\n",
    "    existing_df = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "    existing_keys = set(existing_df[natural_key].tolist())\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            if row[natural_key] in existing_keys:\n",
    "                # Update existing record\n",
    "                set_clause = ', '.join([f\"{col} = :{col}\" for col in update_columns])\n",
    "                update_sql = text(f\"\"\"\n",
    "                    UPDATE {table_name}\n",
    "                    SET {set_clause}, updated_at = :updated_at\n",
    "                    WHERE {natural_key} = :{natural_key}\n",
    "                \"\"\")\n",
    "                params = {col: row[col] for col in update_columns}\n",
    "                params[natural_key] = row[natural_key]\n",
    "                params['updated_at'] = datetime.now()\n",
    "                conn.execute(update_sql, params)\n",
    "                updates += 1\n",
    "            else:\n",
    "                # Insert new record\n",
    "                inserts += 1\n",
    "    \n",
    "    # Bulk insert new records\n",
    "    new_records = df[~df[natural_key].isin(existing_keys)]\n",
    "    if len(new_records) > 0:\n",
    "        new_records.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "    \n",
    "    return {'scd_type': 1, 'updates': updates, 'inserts': len(new_records)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b423dab",
   "metadata": {},
   "source": [
    "### 3.3 SCD Type 2 - Add New Row (Historical Tracking)\n",
    "\n",
    "**Description**: Create a new row for each change, preserving full history.\n",
    "\n",
    "```\n",
    "┌─────────────┬────────┬─────────────┬────────────┬────────────┬─────────┐\n",
    "│ surrogate_key│ cust_id│ city        │ valid_from │ valid_to   │ current │\n",
    "├─────────────┼────────┼─────────────┼────────────┼────────────┼─────────┤\n",
    "│ 1           │ 101    │ New York    │ 2020-01-01 │ 2024-06-30 │ N       │\n",
    "│ 2           │ 101    │ Los Angeles │ 2024-07-01 │ 9999-12-31 │ Y       │\n",
    "└─────────────┴────────┴─────────────┴────────────┴────────────┴─────────┘\n",
    "```\n",
    "\n",
    "**Key Components**:\n",
    "- **Surrogate Key**: System-generated unique identifier\n",
    "- **Natural/Business Key**: Original identifier (customer_id)\n",
    "- **Valid From/To**: Date range when record was current\n",
    "- **Current Flag**: Indicates the active record\n",
    "\n",
    "**Pros**: Full historical tracking, supports time-travel queries\n",
    "**Cons**: Table growth, complex queries for current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scd_type2_process(\n",
    "    source_df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    engine,\n",
    "    natural_key: str,\n",
    "    tracked_columns: list\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    SCD Type 2: Historical tracking with row versioning.\n",
    "    \n",
    "    Table Requirements:\n",
    "    - surrogate_key (auto-increment)\n",
    "    - natural_key columns\n",
    "    - tracked_columns\n",
    "    - valid_from (DATE)\n",
    "    - valid_to (DATE)\n",
    "    - is_current (BOOLEAN)\n",
    "    \"\"\"\n",
    "    today = datetime.now().date()\n",
    "    far_future = datetime(9999, 12, 31).date()\n",
    "    \n",
    "    stats = {'new_records': 0, 'expired_records': 0, 'unchanged': 0}\n",
    "    \n",
    "    # Get current active records from target\n",
    "    current_query = f\"\"\"\n",
    "        SELECT * FROM {table_name}\n",
    "        WHERE is_current = TRUE\n",
    "    \"\"\"\n",
    "    target_df = pd.read_sql(current_query, engine)\n",
    "    \n",
    "    records_to_expire = []\n",
    "    records_to_insert = []\n",
    "    \n",
    "    for _, source_row in source_df.iterrows():\n",
    "        # Find matching current record in target\n",
    "        match = target_df[target_df[natural_key] == source_row[natural_key]]\n",
    "        \n",
    "        if len(match) == 0:\n",
    "            # New record - insert with current flag\n",
    "            new_record = source_row.to_dict()\n",
    "            new_record['valid_from'] = today\n",
    "            new_record['valid_to'] = far_future\n",
    "            new_record['is_current'] = True\n",
    "            records_to_insert.append(new_record)\n",
    "            stats['new_records'] += 1\n",
    "        else:\n",
    "            # Check if tracked columns changed\n",
    "            target_row = match.iloc[0]\n",
    "            has_change = any(\n",
    "                source_row[col] != target_row[col] \n",
    "                for col in tracked_columns\n",
    "            )\n",
    "            \n",
    "            if has_change:\n",
    "                # Expire old record\n",
    "                records_to_expire.append({\n",
    "                    'surrogate_key': target_row['surrogate_key'],\n",
    "                    'valid_to': today,\n",
    "                    'is_current': False\n",
    "                })\n",
    "                \n",
    "                # Insert new version\n",
    "                new_record = source_row.to_dict()\n",
    "                new_record['valid_from'] = today\n",
    "                new_record['valid_to'] = far_future\n",
    "                new_record['is_current'] = True\n",
    "                records_to_insert.append(new_record)\n",
    "                stats['expired_records'] += 1\n",
    "            else:\n",
    "                stats['unchanged'] += 1\n",
    "    \n",
    "    # Execute updates\n",
    "    with engine.begin() as conn:\n",
    "        # Expire old records\n",
    "        for rec in records_to_expire:\n",
    "            expire_sql = text(f\"\"\"\n",
    "                UPDATE {table_name}\n",
    "                SET valid_to = :valid_to, is_current = :is_current\n",
    "                WHERE surrogate_key = :surrogate_key\n",
    "            \"\"\")\n",
    "            conn.execute(expire_sql, rec)\n",
    "    \n",
    "    # Insert new records\n",
    "    if records_to_insert:\n",
    "        insert_df = pd.DataFrame(records_to_insert)\n",
    "        insert_df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "    \n",
    "    return {'scd_type': 2, **stats}\n",
    "\n",
    "# Example query for SCD Type 2\n",
    "scd2_query_examples = \"\"\"\n",
    "-- Get current customer data\n",
    "SELECT * FROM dim_customer WHERE is_current = TRUE;\n",
    "\n",
    "-- Get customer data as of a specific date\n",
    "SELECT * FROM dim_customer \n",
    "WHERE customer_id = 101 \n",
    "  AND '2023-06-15' BETWEEN valid_from AND valid_to;\n",
    "\n",
    "-- Get complete history for a customer\n",
    "SELECT * FROM dim_customer \n",
    "WHERE customer_id = 101 \n",
    "ORDER BY valid_from;\n",
    "\"\"\"\n",
    "print(scd2_query_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a47f0",
   "metadata": {},
   "source": [
    "### 3.4 SCD Type 3 - Add New Column\n",
    "\n",
    "**Description**: Store current and previous value in separate columns.\n",
    "\n",
    "```\n",
    "┌──────────┬──────────────┬─────────────────┬────────────┐\n",
    "│ cust_id  │ current_city │ previous_city   │ city_change│\n",
    "├──────────┼──────────────┼─────────────────┼────────────┤\n",
    "│ 101      │ Los Angeles  │ New York        │ 2024-07-01 │\n",
    "└──────────┴──────────────┴─────────────────┴────────────┘\n",
    "```\n",
    "\n",
    "**Pros**: Simple queries, limited history\n",
    "**Cons**: Only stores one previous value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd9c9b",
   "metadata": {},
   "source": [
    "### 3.5 SCD Type Comparison\n",
    "\n",
    "| Type | History | Storage | Complexity | Use Case |\n",
    "|------|---------|---------|------------|----------|\n",
    "| **Type 0** | None | Minimal | Low | Immutable attributes |\n",
    "| **Type 1** | None | Minimal | Low | Error corrections |\n",
    "| **Type 2** | Full | High | High | Regulatory compliance |\n",
    "| **Type 3** | Limited | Medium | Medium | Before/after comparison |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd8ca3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Python Code for Database Loading\n",
    "\n",
    "### 4.1 Loading to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77176b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import io\n",
    "\n",
    "class PostgresLoader:\n",
    "    \"\"\"\n",
    "    High-performance PostgreSQL data loader with multiple strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string: str):\n",
    "        self.conn_string = connection_string\n",
    "    \n",
    "    def load_with_copy(self, df: pd.DataFrame, table_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Fastest method: Use PostgreSQL COPY command.\n",
    "        Up to 10x faster than INSERT for large datasets.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Create in-memory CSV buffer\n",
    "        buffer = io.StringIO()\n",
    "        df.to_csv(buffer, index=False, header=False, sep='\\t')\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        conn = psycopg2.connect(self.conn_string)\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.copy_from(\n",
    "                    file=buffer,\n",
    "                    table=table_name,\n",
    "                    sep='\\t',\n",
    "                    columns=list(df.columns),\n",
    "                    null=''\n",
    "                )\n",
    "            conn.commit()\n",
    "        finally:\n",
    "            conn.close()\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        return {\n",
    "            'method': 'COPY',\n",
    "            'rows': len(df),\n",
    "            'duration_seconds': (end_time - start_time).total_seconds()\n",
    "        }\n",
    "    \n",
    "    def load_with_execute_values(self, df: pd.DataFrame, table_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Fast batch insert using psycopg2's execute_values.\n",
    "        Good balance of speed and flexibility.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        columns = ', '.join(df.columns)\n",
    "        values = [tuple(row) for row in df.values]\n",
    "        \n",
    "        conn = psycopg2.connect(self.conn_string)\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                insert_sql = f\"INSERT INTO {table_name} ({columns}) VALUES %s\"\n",
    "                execute_values(cursor, insert_sql, values, page_size=1000)\n",
    "            conn.commit()\n",
    "        finally:\n",
    "            conn.close()\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        return {\n",
    "            'method': 'execute_values',\n",
    "            'rows': len(df),\n",
    "            'duration_seconds': (end_time - start_time).total_seconds()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "# loader = PostgresLoader('postgresql://user:pass@localhost:5432/mydb')\n",
    "# stats = loader.load_with_copy(df, 'fact_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85643521",
   "metadata": {},
   "source": [
    "### 4.2 Loading to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b675ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "class SnowflakeLoader:\n",
    "    \"\"\"\n",
    "    Snowflake data loader using optimal loading patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, account: str, user: str, password: str, \n",
    "                 warehouse: str, database: str, schema: str):\n",
    "        self.config = {\n",
    "            'account': account,\n",
    "            'user': user,\n",
    "            'password': password,\n",
    "            'warehouse': warehouse,\n",
    "            'database': database,\n",
    "            'schema': schema\n",
    "        }\n",
    "    \n",
    "    def load_dataframe(self, df: pd.DataFrame, table_name: str, \n",
    "                       auto_create_table: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Load DataFrame to Snowflake using write_pandas.\n",
    "        Uses internal stage and COPY command for optimal performance.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        conn = snowflake.connector.connect(**self.config)\n",
    "        try:\n",
    "            success, num_chunks, num_rows, output = write_pandas(\n",
    "                conn=conn,\n",
    "                df=df,\n",
    "                table_name=table_name.upper(),\n",
    "                auto_create_table=auto_create_table,\n",
    "                quote_identifiers=False\n",
    "            )\n",
    "        finally:\n",
    "            conn.close()\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        return {\n",
    "            'method': 'write_pandas',\n",
    "            'success': success,\n",
    "            'chunks': num_chunks,\n",
    "            'rows': num_rows,\n",
    "            'duration_seconds': (end_time - start_time).total_seconds()\n",
    "        }\n",
    "    \n",
    "    def load_from_stage(self, stage_path: str, table_name: str,\n",
    "                        file_format: str = 'CSV') -> dict:\n",
    "        \"\"\"\n",
    "        Load data from external stage (S3, Azure, GCS).\n",
    "        Best for very large datasets.\n",
    "        \"\"\"\n",
    "        conn = snowflake.connector.connect(**self.config)\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Create file format if not exists\n",
    "            cursor.execute(f\"\"\"\n",
    "                CREATE FILE FORMAT IF NOT EXISTS my_csv_format\n",
    "                TYPE = '{file_format}'\n",
    "                FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "                SKIP_HEADER = 1\n",
    "            \"\"\")\n",
    "            \n",
    "            # Execute COPY command\n",
    "            copy_sql = f\"\"\"\n",
    "                COPY INTO {table_name}\n",
    "                FROM @{stage_path}\n",
    "                FILE_FORMAT = my_csv_format\n",
    "                ON_ERROR = 'CONTINUE'\n",
    "            \"\"\"\n",
    "            cursor.execute(copy_sql)\n",
    "            \n",
    "            result = cursor.fetchall()\n",
    "            \n",
    "        finally:\n",
    "            conn.close()\n",
    "        \n",
    "        return {'method': 'COPY_FROM_STAGE', 'result': result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465d112",
   "metadata": {},
   "source": [
    "### 4.3 Loading to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5057b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, UpdateOne\n",
    "from typing import List, Dict\n",
    "\n",
    "class MongoLoader:\n",
    "    \"\"\"\n",
    "    MongoDB data loader with bulk operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string: str, database: str):\n",
    "        self.client = MongoClient(connection_string)\n",
    "        self.db = self.client[database]\n",
    "    \n",
    "    def bulk_insert(self, collection_name: str, \n",
    "                    documents: List[Dict], ordered: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Bulk insert documents to MongoDB.\n",
    "        \n",
    "        Args:\n",
    "            ordered: If False, continues on error (faster)\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        collection = self.db[collection_name]\n",
    "        result = collection.insert_many(documents, ordered=ordered)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        return {\n",
    "            'method': 'insert_many',\n",
    "            'inserted_count': len(result.inserted_ids),\n",
    "            'duration_seconds': (end_time - start_time).total_seconds()\n",
    "        }\n",
    "    \n",
    "    def bulk_upsert(self, collection_name: str, documents: List[Dict],\n",
    "                    key_field: str, batch_size: int = 1000) -> dict:\n",
    "        \"\"\"\n",
    "        Bulk upsert using bulk_write with UpdateOne operations.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        collection = self.db[collection_name]\n",
    "        \n",
    "        total_modified = 0\n",
    "        total_upserted = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            \n",
    "            operations = [\n",
    "                UpdateOne(\n",
    "                    filter={key_field: doc[key_field]},\n",
    "                    update={'$set': doc},\n",
    "                    upsert=True\n",
    "                )\n",
    "                for doc in batch\n",
    "            ]\n",
    "            \n",
    "            result = collection.bulk_write(operations, ordered=False)\n",
    "            total_modified += result.modified_count\n",
    "            total_upserted += result.upserted_count\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        return {\n",
    "            'method': 'bulk_upsert',\n",
    "            'modified_count': total_modified,\n",
    "            'upserted_count': total_upserted,\n",
    "            'duration_seconds': (end_time - start_time).total_seconds()\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "\n",
    "# Example usage\n",
    "# loader = MongoLoader('mongodb://localhost:27017', 'analytics')\n",
    "# docs = df.to_dict(orient='records')\n",
    "# stats = loader.bulk_upsert('customers', docs, key_field='customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9dd925",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Bulk Loading Best Practices\n",
    "\n",
    "### 5.1 General Optimization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd361ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from typing import Generator\n",
    "\n",
    "class BulkLoadOptimizer:\n",
    "    \"\"\"\n",
    "    Collection of bulk loading optimization techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> Generator:\n",
    "        \"\"\"\n",
    "        Split DataFrame into chunks for batch processing.\n",
    "        Prevents memory issues with large datasets.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            yield df.iloc[i:i + chunk_size]\n",
    "    \n",
    "    @staticmethod\n",
    "    @contextmanager\n",
    "    def disable_indexes(engine, table_name: str):\n",
    "        \"\"\"\n",
    "        Context manager to disable/rebuild indexes during bulk load.\n",
    "        Significantly speeds up large inserts.\n",
    "        \"\"\"\n",
    "        # Get existing indexes\n",
    "        with engine.connect() as conn:\n",
    "            # Disable indexes (PostgreSQL example)\n",
    "            conn.execute(text(f\"\"\"\n",
    "                UPDATE pg_index \n",
    "                SET indisready = false \n",
    "                WHERE indrelid = '{table_name}'::regclass\n",
    "            \"\"\"))\n",
    "        \n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            # Rebuild indexes\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(f\"REINDEX TABLE {table_name}\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def parallel_load(df: pd.DataFrame, table_name: str, engine,\n",
    "                      num_workers: int = 4, chunk_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Load data in parallel using multiple connections.\n",
    "        \"\"\"\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        \n",
    "        def load_chunk(chunk_df, worker_id):\n",
    "            chunk_df.to_sql(\n",
    "                name=table_name,\n",
    "                con=engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            return len(chunk_df)\n",
    "        \n",
    "        chunks = list(BulkLoadOptimizer.chunk_dataframe(df, chunk_size))\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(load_chunk, chunk, i): i \n",
    "                for i, chunk in enumerate(chunks)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                worker_id = futures[future]\n",
    "                try:\n",
    "                    rows = future.result()\n",
    "                    results.append({'worker': worker_id, 'rows': rows})\n",
    "                except Exception as e:\n",
    "                    results.append({'worker': worker_id, 'error': str(e)})\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                      BULK LOADING BEST PRACTICES                             ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                              ║\n",
    "║  1. BATCH SIZE TUNING                                                        ║\n",
    "║     • Start with 1,000-10,000 rows per batch                                 ║\n",
    "║     • Increase until diminishing returns                                     ║\n",
    "║     • Monitor memory usage                                                   ║\n",
    "║                                                                              ║\n",
    "║  2. INDEX MANAGEMENT                                                         ║\n",
    "║     • Drop indexes before bulk load                                          ║\n",
    "║     • Rebuild after load completes                                           ║\n",
    "║     • Consider partial indexes                                               ║\n",
    "║                                                                              ║\n",
    "║  3. TRANSACTION HANDLING                                                     ║\n",
    "║     • Use larger transactions for throughput                                 ║\n",
    "║     • Commit every N rows (e.g., 100,000)                                    ║\n",
    "║     • Disable auto-commit during bulk operations                             ║\n",
    "║                                                                              ║\n",
    "║  4. CONSTRAINT MANAGEMENT                                                    ║\n",
    "║     • Defer constraint checking until commit                                 ║\n",
    "║     • Disable triggers during load                                           ║\n",
    "║     • Re-enable and validate after                                           ║\n",
    "║                                                                              ║\n",
    "║  5. PARALLEL LOADING                                                         ║\n",
    "║     • Partition data by key ranges                                           ║\n",
    "║     • Use multiple connections/workers                                       ║\n",
    "║     • Avoid overlapping key ranges                                           ║\n",
    "║                                                                              ║\n",
    "║  6. LOGGING & WAL                                                            ║\n",
    "║     • Use unlogged tables for staging                                        ║\n",
    "║     • Increase checkpoint interval                                           ║\n",
    "║     • Consider minimal logging modes                                         ║\n",
    "║                                                                              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb92f6f",
   "metadata": {},
   "source": [
    "### 5.2 Database-Specific Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Optimizations\n",
    "postgresql_optimizations = \"\"\"\n",
    "-- Before bulk load\n",
    "ALTER TABLE target_table SET UNLOGGED;  -- Disable WAL\n",
    "ALTER TABLE target_table DISABLE TRIGGER ALL;\n",
    "DROP INDEX idx_target_column;  -- Drop indexes\n",
    "\n",
    "-- Increase checkpoint distance\n",
    "SET checkpoint_timeout = '1h';\n",
    "SET max_wal_size = '10GB';\n",
    "\n",
    "-- Use COPY for fastest loading\n",
    "COPY target_table FROM '/path/to/data.csv' WITH (FORMAT csv, HEADER true);\n",
    "\n",
    "-- After bulk load\n",
    "CREATE INDEX idx_target_column ON target_table(column);\n",
    "ALTER TABLE target_table ENABLE TRIGGER ALL;\n",
    "ALTER TABLE target_table SET LOGGED;  -- Re-enable WAL\n",
    "ANALYZE target_table;  -- Update statistics\n",
    "\"\"\"\n",
    "\n",
    "# MySQL Optimizations\n",
    "mysql_optimizations = \"\"\"\n",
    "-- Before bulk load\n",
    "SET FOREIGN_KEY_CHECKS = 0;\n",
    "SET UNIQUE_CHECKS = 0;\n",
    "SET autocommit = 0;\n",
    "ALTER TABLE target_table DISABLE KEYS;\n",
    "\n",
    "-- Use LOAD DATA for fastest loading\n",
    "LOAD DATA INFILE '/path/to/data.csv'\n",
    "INTO TABLE target_table\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\\\n'\n",
    "IGNORE 1 ROWS;\n",
    "\n",
    "-- After bulk load\n",
    "ALTER TABLE target_table ENABLE KEYS;\n",
    "SET FOREIGN_KEY_CHECKS = 1;\n",
    "SET UNIQUE_CHECKS = 1;\n",
    "COMMIT;\n",
    "ANALYZE TABLE target_table;\n",
    "\"\"\"\n",
    "\n",
    "print(\"PostgreSQL Optimizations:\")\n",
    "print(postgresql_optimizations)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"MySQL Optimizations:\")\n",
    "print(mysql_optimizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626ae33",
   "metadata": {},
   "source": [
    "### 5.3 Error Handling and Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "\n",
    "@dataclass\n",
    "class LoadResult:\n",
    "    \"\"\"Container for load operation results.\"\"\"\n",
    "    success: bool\n",
    "    rows_loaded: int = 0\n",
    "    rows_failed: int = 0\n",
    "    errors: list = field(default_factory=list)\n",
    "    duration_seconds: float = 0.0\n",
    "\n",
    "class ResilientLoader:\n",
    "    \"\"\"\n",
    "    Data loader with retry logic and error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, engine, max_retries: int = 3, \n",
    "                 error_threshold: float = 0.05):\n",
    "        self.engine = engine\n",
    "        self.max_retries = max_retries\n",
    "        self.error_threshold = error_threshold  # 5% error tolerance\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def load_with_retry(self, df: pd.DataFrame, table_name: str,\n",
    "                        chunk_size: int = 1000) -> LoadResult:\n",
    "        \"\"\"\n",
    "        Load data with automatic retry on failure.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        total_rows = len(df)\n",
    "        loaded_rows = 0\n",
    "        failed_rows = 0\n",
    "        errors = []\n",
    "        \n",
    "        chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            retry_count = 0\n",
    "            success = False\n",
    "            \n",
    "            while retry_count < self.max_retries and not success:\n",
    "                try:\n",
    "                    chunk.to_sql(\n",
    "                        name=table_name,\n",
    "                        con=self.engine,\n",
    "                        if_exists='append',\n",
    "                        index=False\n",
    "                    )\n",
    "                    loaded_rows += len(chunk)\n",
    "                    success = True\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    retry_count += 1\n",
    "                    self.logger.warning(\n",
    "                        f\"Chunk {chunk_idx} failed (attempt {retry_count}): {e}\"\n",
    "                    )\n",
    "                    \n",
    "                    if retry_count >= self.max_retries:\n",
    "                        failed_rows += len(chunk)\n",
    "                        errors.append({\n",
    "                            'chunk_idx': chunk_idx,\n",
    "                            'error': str(e),\n",
    "                            'rows_affected': len(chunk)\n",
    "                        })\n",
    "            \n",
    "            # Check error threshold\n",
    "            error_rate = failed_rows / total_rows\n",
    "            if error_rate > self.error_threshold:\n",
    "                self.logger.error(\n",
    "                    f\"Error threshold exceeded: {error_rate:.2%} > {self.error_threshold:.2%}\"\n",
    "                )\n",
    "                break\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        return LoadResult(\n",
    "            success=(failed_rows == 0),\n",
    "            rows_loaded=loaded_rows,\n",
    "            rows_failed=failed_rows,\n",
    "            errors=errors,\n",
    "            duration_seconds=(end_time - start_time).total_seconds()\n",
    "        )\n",
    "    \n",
    "    def load_with_dead_letter_queue(\n",
    "        self, df: pd.DataFrame, table_name: str, dlq_table: str\n",
    "    ) -> LoadResult:\n",
    "        \"\"\"\n",
    "        Load data, sending failed records to a dead letter queue.\n",
    "        \"\"\"\n",
    "        loaded_rows = 0\n",
    "        failed_records = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                row_df = pd.DataFrame([row])\n",
    "                row_df.to_sql(\n",
    "                    name=table_name,\n",
    "                    con=self.engine,\n",
    "                    if_exists='append',\n",
    "                    index=False\n",
    "                )\n",
    "                loaded_rows += 1\n",
    "            except Exception as e:\n",
    "                # Add to dead letter queue\n",
    "                dlq_record = row.to_dict()\n",
    "                dlq_record['_error'] = str(e)\n",
    "                dlq_record['_timestamp'] = datetime.now().isoformat()\n",
    "                failed_records.append(dlq_record)\n",
    "        \n",
    "        # Write failed records to DLQ\n",
    "        if failed_records:\n",
    "            dlq_df = pd.DataFrame(failed_records)\n",
    "            dlq_df.to_sql(\n",
    "                name=dlq_table,\n",
    "                con=self.engine,\n",
    "                if_exists='append',\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        return LoadResult(\n",
    "            success=(len(failed_records) == 0),\n",
    "            rows_loaded=loaded_rows,\n",
    "            rows_failed=len(failed_records)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5a27f",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Key Takeaways\n",
    "\n",
    "### Loading Strategy Selection Guide\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    LOADING STRATEGY DECISION TREE                       │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "                         Is the dataset small?\n",
    "                                 │\n",
    "                    ┌────────────┴────────────┐\n",
    "                   YES                        NO\n",
    "                    │                          │\n",
    "               FULL LOAD               Need history tracking?\n",
    "                                              │\n",
    "                                 ┌────────────┴────────────┐\n",
    "                                YES                        NO\n",
    "                                 │                          │\n",
    "                            SCD TYPE 2              Need to update existing?\n",
    "                                                           │\n",
    "                                              ┌────────────┴────────────┐\n",
    "                                             YES                        NO\n",
    "                                              │                          │\n",
    "                                           UPSERT               INCREMENTAL\n",
    "```\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Strategy | When to Use | Performance | Complexity |\n",
    "|----------|------------|-------------|------------|\n",
    "| **Full Load** | Small datasets, dimension tables | Slow | Low |\n",
    "| **Incremental** | Large fact tables, append-only | Fast | Medium |\n",
    "| **Upsert** | Frequently updated entities | Medium | Medium |\n",
    "| **SCD Type 1** | Error corrections, no history needed | Fast | Low |\n",
    "| **SCD Type 2** | Audit requirements, historical analysis | Slow | High |\n",
    "| **Bulk COPY** | Initial loads, large migrations | Fastest | Low |\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "- [ ] **Choose the right loading pattern** based on data volume and requirements\n",
    "- [ ] **Batch operations** - avoid row-by-row processing\n",
    "- [ ] **Use native bulk loading** (COPY, LOAD DATA) when possible\n",
    "- [ ] **Disable indexes and constraints** during large loads\n",
    "- [ ] **Implement proper error handling** with retry logic\n",
    "- [ ] **Monitor and log** load statistics for optimization\n",
    "- [ ] **Use staging tables** for complex transformations\n",
    "- [ ] **Validate data quality** after loading\n",
    "- [ ] **Update statistics** after bulk operations\n",
    "- [ ] **Test with production-like volumes** before deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
