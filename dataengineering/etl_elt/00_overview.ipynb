{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249c62ac",
   "metadata": {},
   "source": [
    "# ETL/ELT — Overview\n",
    "\n",
    "## Purpose\n",
    "ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) are two fundamental data integration patterns used to move data from source systems to target destinations (data warehouses, data lakes, etc.). Understanding when and how to apply each pattern is critical for building efficient, scalable data pipelines.\n",
    "\n",
    "## Key Questions\n",
    "1. What are the core differences between ETL and ELT?\n",
    "2. When should you choose ETL over ELT, and vice versa?\n",
    "3. How do transformation strategies differ between the two approaches?\n",
    "4. What are the performance and scalability implications of each pattern?\n",
    "5. How do modern cloud data platforms influence the ETL vs ELT decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c677024",
   "metadata": {},
   "source": [
    "---\n",
    "## ETL vs ELT: Core Concepts\n",
    "\n",
    "### ETL (Extract, Transform, Load)\n",
    "```\n",
    "┌──────────┐    ┌───────────────┐    ┌──────────────┐\n",
    "│  Source  │───▶│  Transform    │───▶│   Target     │\n",
    "│  Systems │    │  (Staging)    │    │  (Warehouse) │\n",
    "└──────────┘    └───────────────┘    └──────────────┘\n",
    "   Extract      Transform first       Load cleaned data\n",
    "```\n",
    "\n",
    "- **Extract**: Pull data from source systems (databases, APIs, files)\n",
    "- **Transform**: Clean, validate, aggregate, and reshape data in a staging area\n",
    "- **Load**: Insert transformed data into the target system\n",
    "\n",
    "### ELT (Extract, Load, Transform)\n",
    "```\n",
    "┌──────────┐    ┌──────────────┐    ┌───────────────┐\n",
    "│  Source  │───▶│   Target     │───▶│   Transform   │\n",
    "│  Systems │    │  (Raw Zone)  │    │  (In-place)   │\n",
    "└──────────┘    └──────────────┘    └───────────────┘\n",
    "   Extract      Load raw data       Transform in target\n",
    "```\n",
    "\n",
    "- **Extract**: Pull data from source systems\n",
    "- **Load**: Load raw data directly into target system (data lake/warehouse)\n",
    "- **Transform**: Use target system's compute power to transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc8859",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Use Each Approach\n",
    "\n",
    "| Criteria | ETL | ELT |\n",
    "|----------|-----|-----|\n",
    "| **Data Volume** | Small to medium datasets | Large-scale, big data workloads |\n",
    "| **Target System** | Traditional data warehouses | Cloud data warehouses (Snowflake, BigQuery, Redshift) |\n",
    "| **Compute Resources** | External processing server | Leverage target's compute power |\n",
    "| **Data Quality** | Clean before loading | Store raw, clean as needed |\n",
    "| **Flexibility** | Fixed transformations | Schema-on-read flexibility |\n",
    "| **Latency Requirements** | Batch-oriented | Near real-time possible |\n",
    "| **Compliance/Security** | Transform sensitive data before loading | May require additional security layers |\n",
    "\n",
    "### Choose ETL When:\n",
    "- Working with legacy on-premises data warehouses\n",
    "- Data needs to be cleaned/masked before entering target system\n",
    "- Complex transformations require specialized tools (Informatica, Talend)\n",
    "- Strict data governance requires validated data only\n",
    "\n",
    "### Choose ELT When:\n",
    "- Using modern cloud data platforms with MPP (Massively Parallel Processing)\n",
    "- Data volumes are large and growing rapidly\n",
    "- Need flexibility to re-transform historical data\n",
    "- Want to leverage SQL-based transformations (dbt, Dataform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0601402",
   "metadata": {},
   "source": [
    "---\n",
    "## Python ETL Example: Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b47663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Simulated source data (Extract phase would typically read from DB/API/Files)\n",
    "raw_sales_data = [\n",
    "    {\"order_id\": \"ORD001\", \"product\": \"Widget A\", \"quantity\": 10, \"price\": \"$25.50\", \"date\": \"2025-01-15\"},\n",
    "    {\"order_id\": \"ORD002\", \"product\": \"Widget B\", \"quantity\": 5, \"price\": \"$45.00\", \"date\": \"2025-01-16\"},\n",
    "    {\"order_id\": \"ORD003\", \"product\": \"Widget A\", \"quantity\": 8, \"price\": \"$25.50\", \"date\": \"2025-01-16\"},\n",
    "    {\"order_id\": \"ORD004\", \"product\": \"Widget C\", \"quantity\": 3, \"price\": \"invalid\", \"date\": \"2025-01-17\"},\n",
    "    {\"order_id\": \"ORD005\", \"product\": \"Widget B\", \"quantity\": -2, \"price\": \"$45.00\", \"date\": \"2025-01-17\"},\n",
    "]\n",
    "\n",
    "print(\"Raw Sales Data:\")\n",
    "pd.DataFrame(raw_sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    \"\"\"Simple ETL pipeline demonstrating Extract, Transform, Load phases.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extracted_data = []\n",
    "        self.transformed_data = []\n",
    "        self.errors = []\n",
    "    \n",
    "    # ===============================\n",
    "    # EXTRACT PHASE\n",
    "    # ===============================\n",
    "    def extract(self, source_data: List[Dict]) -> 'ETLPipeline':\n",
    "        \"\"\"Extract data from source (simulated here with in-memory data).\"\"\"\n",
    "        print(f\"[EXTRACT] Reading {len(source_data)} records from source...\")\n",
    "        self.extracted_data = source_data.copy()\n",
    "        return self\n",
    "    \n",
    "    # ===============================\n",
    "    # TRANSFORM PHASE\n",
    "    # ===============================\n",
    "    def transform(self) -> 'ETLPipeline':\n",
    "        \"\"\"Apply transformations: clean, validate, enrich data.\"\"\"\n",
    "        print(\"[TRANSFORM] Applying transformations...\")\n",
    "        \n",
    "        for record in self.extracted_data:\n",
    "            try:\n",
    "                transformed = self._transform_record(record)\n",
    "                if transformed:\n",
    "                    self.transformed_data.append(transformed)\n",
    "            except Exception as e:\n",
    "                self.errors.append({\"record\": record, \"error\": str(e)})\n",
    "        \n",
    "        print(f\"[TRANSFORM] Successfully transformed {len(self.transformed_data)} records\")\n",
    "        print(f\"[TRANSFORM] Rejected {len(self.errors)} records with errors\")\n",
    "        return self\n",
    "    \n",
    "    def _transform_record(self, record: Dict) -> Dict:\n",
    "        \"\"\"Transform individual record.\"\"\"\n",
    "        # Clean price: remove $ and convert to float\n",
    "        price_str = record[\"price\"].replace(\"$\", \"\").strip()\n",
    "        price = float(price_str)  # Will raise ValueError for invalid prices\n",
    "        \n",
    "        # Validate quantity\n",
    "        quantity = int(record[\"quantity\"])\n",
    "        if quantity <= 0:\n",
    "            raise ValueError(f\"Invalid quantity: {quantity}\")\n",
    "        \n",
    "        # Calculate derived fields\n",
    "        total = price * quantity\n",
    "        \n",
    "        # Parse and standardize date\n",
    "        parsed_date = datetime.strptime(record[\"date\"], \"%Y-%m-%d\")\n",
    "        \n",
    "        return {\n",
    "            \"order_id\": record[\"order_id\"],\n",
    "            \"product\": record[\"product\"].upper(),  # Standardize to uppercase\n",
    "            \"quantity\": quantity,\n",
    "            \"unit_price\": price,\n",
    "            \"total_amount\": round(total, 2),\n",
    "            \"order_date\": parsed_date.date(),\n",
    "            \"order_month\": parsed_date.strftime(\"%Y-%m\"),\n",
    "            \"processed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    # ===============================\n",
    "    # LOAD PHASE\n",
    "    # ===============================\n",
    "    def load(self) -> pd.DataFrame:\n",
    "        \"\"\"Load transformed data to target (returning DataFrame as simulation).\"\"\"\n",
    "        print(f\"[LOAD] Loading {len(self.transformed_data)} records to target...\")\n",
    "        df = pd.DataFrame(self.transformed_data)\n",
    "        print(\"[LOAD] Complete!\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# Run the ETL pipeline\n",
    "pipeline = ETLPipeline()\n",
    "result_df = (\n",
    "    pipeline\n",
    "    .extract(raw_sales_data)\n",
    "    .transform()\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\n=== Transformed Data ===\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a139dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rejected records\n",
    "print(\"=== Rejected Records ===\")\n",
    "for error in pipeline.errors:\n",
    "    print(f\"  Order {error['record']['order_id']}: {error['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a42a64",
   "metadata": {},
   "source": [
    "---\n",
    "## Python ELT Example: Load First, Transform Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELTPipeline:\n",
    "    \"\"\"Simple ELT pipeline: Load raw data first, transform in-place.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw_table = None  # Simulates raw data zone\n",
    "        self.transformed_view = None  # Simulates transformed view/table\n",
    "    \n",
    "    # ===============================\n",
    "    # EXTRACT PHASE\n",
    "    # ===============================\n",
    "    def extract(self, source_data: List[Dict]) -> 'ELTPipeline':\n",
    "        \"\"\"Extract from source (minimal processing).\"\"\"\n",
    "        print(f\"[EXTRACT] Reading {len(source_data)} records...\")\n",
    "        self._source_data = source_data\n",
    "        return self\n",
    "    \n",
    "    # ===============================\n",
    "    # LOAD PHASE (happens BEFORE transform in ELT)\n",
    "    # ===============================\n",
    "    def load_raw(self) -> 'ELTPipeline':\n",
    "        \"\"\"Load raw data directly to target (no transformation).\"\"\"\n",
    "        print(\"[LOAD] Loading raw data to staging zone...\")\n",
    "        \n",
    "        # Add metadata columns (common in ELT)\n",
    "        raw_with_metadata = []\n",
    "        for record in self._source_data:\n",
    "            enriched = record.copy()\n",
    "            enriched[\"_loaded_at\"] = datetime.now().isoformat()\n",
    "            enriched[\"_source\"] = \"sales_api\"\n",
    "            enriched[\"_raw_record\"] = str(record)  # Keep original for debugging\n",
    "            raw_with_metadata.append(enriched)\n",
    "        \n",
    "        self.raw_table = pd.DataFrame(raw_with_metadata)\n",
    "        print(f\"[LOAD] Loaded {len(self.raw_table)} records to raw zone\")\n",
    "        return self\n",
    "    \n",
    "    # ===============================\n",
    "    # TRANSFORM PHASE (happens in target system)\n",
    "    # ===============================\n",
    "    def transform_in_place(self) -> pd.DataFrame:\n",
    "        \"\"\"Transform data using target system's compute (SQL-like operations).\"\"\"\n",
    "        print(\"[TRANSFORM] Transforming data in target system...\")\n",
    "        \n",
    "        # Simulate SQL-based transformations that would run in the data warehouse\n",
    "        df = self.raw_table.copy()\n",
    "        \n",
    "        # Clean price column\n",
    "        df[\"unit_price\"] = df[\"price\"].str.replace(\"$\", \"\", regex=False)\n",
    "        df[\"unit_price\"] = pd.to_numeric(df[\"unit_price\"], errors=\"coerce\")\n",
    "        \n",
    "        # Convert quantity and filter invalid\n",
    "        df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\")\n",
    "        \n",
    "        # Calculate total (NULL if any component is invalid)\n",
    "        df[\"total_amount\"] = df[\"unit_price\"] * df[\"quantity\"]\n",
    "        \n",
    "        # Add validation flag\n",
    "        df[\"is_valid\"] = (df[\"unit_price\"].notna()) & (df[\"quantity\"] > 0)\n",
    "        \n",
    "        # Parse date\n",
    "        df[\"order_date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        \n",
    "        self.transformed_view = df\n",
    "        print(\"[TRANSFORM] Transformation complete (all records preserved with validity flag)\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# Run the ELT pipeline\n",
    "elt_pipeline = ELTPipeline()\n",
    "elt_result = (\n",
    "    elt_pipeline\n",
    "    .extract(raw_sales_data)\n",
    "    .load_raw()  # Load BEFORE transform\n",
    "    .transform_in_place()\n",
    ")\n",
    "\n",
    "print(\"\\n=== ELT Result (all records with validity flag) ===\")\n",
    "elt_result[[\"order_id\", \"product\", \"quantity\", \"unit_price\", \"total_amount\", \"is_valid\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c83f6f",
   "metadata": {},
   "source": [
    "---\n",
    "## ETL vs ELT: Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99667821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Comparison data\n",
    "categories = [\n",
    "    \"Data Volume Handling\",\n",
    "    \"Transformation Flexibility\",\n",
    "    \"Real-time Capability\",\n",
    "    \"Data Lineage Tracking\",\n",
    "    \"Cost Efficiency (Cloud)\",\n",
    "    \"Legacy System Support\"\n",
    "]\n",
    "\n",
    "etl_scores = [3, 3, 2, 4, 2, 5]\n",
    "elt_scores = [5, 5, 4, 3, 4, 2]\n",
    "\n",
    "# Create radar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=etl_scores + [etl_scores[0]],  # Close the polygon\n",
    "    theta=categories + [categories[0]],\n",
    "    fill='toself',\n",
    "    name='ETL',\n",
    "    line_color='#636EFA',\n",
    "    fillcolor='rgba(99, 110, 250, 0.3)'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=elt_scores + [elt_scores[0]],\n",
    "    theta=categories + [categories[0]],\n",
    "    fill='toself',\n",
    "    name='ELT',\n",
    "    line_color='#EF553B',\n",
    "    fillcolor='rgba(239, 85, 59, 0.3)'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 5],\n",
    "            tickvals=[1, 2, 3, 4, 5],\n",
    "            ticktext=['1-Poor', '2', '3-Average', '4', '5-Excellent']\n",
    "        )\n",
    "    ),\n",
    "    title=dict(\n",
    "        text=\"ETL vs ELT: Capability Comparison\",\n",
    "        x=0.5,\n",
    "        font=dict(size=18)\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend=dict(x=0.85, y=0.95),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use case suitability comparison\n",
    "use_cases = [\n",
    "    \"Traditional DW Migration\",\n",
    "    \"Cloud Data Lake\",\n",
    "    \"Real-time Analytics\",\n",
    "    \"Data Quality First\",\n",
    "    \"Schema Evolution\",\n",
    "    \"Big Data Processing\"\n",
    "]\n",
    "\n",
    "etl_fit = [90, 40, 30, 85, 35, 30]\n",
    "elt_fit = [30, 95, 75, 50, 85, 90]\n",
    "\n",
    "fig2 = go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    "    name='ETL',\n",
    "    x=use_cases,\n",
    "    y=etl_fit,\n",
    "    marker_color='#636EFA',\n",
    "    text=etl_fit,\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    "    name='ELT',\n",
    "    x=use_cases,\n",
    "    y=elt_fit,\n",
    "    marker_color='#EF553B',\n",
    "    text=elt_fit,\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=dict(\n",
    "        text=\"ETL vs ELT: Use Case Suitability (%)\",\n",
    "        x=0.5,\n",
    "        font=dict(size=18)\n",
    "    ),\n",
    "    barmode='group',\n",
    "    yaxis_title=\"Suitability Score (%)\",\n",
    "    yaxis=dict(range=[0, 110]),\n",
    "    xaxis_tickangle=-30,\n",
    "    height=450,\n",
    "    legend=dict(x=0.85, y=0.95)\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949c4b9",
   "metadata": {},
   "source": [
    "---\n",
    "## Modern Hybrid Approach: ETLT\n",
    "\n",
    "Many modern data platforms use a **hybrid ETLT** pattern:\n",
    "\n",
    "```\n",
    "┌──────────┐    ┌─────────────┐    ┌──────────────┐    ┌─────────────┐\n",
    "│  Source  │───▶│  Light ETL  │───▶│   Raw Zone   │───▶│  Transform  │\n",
    "│  Systems │    │  (Minimal)  │    │  (Data Lake) │    │  (in DW)    │\n",
    "└──────────┘    └─────────────┘    └──────────────┘    └─────────────┘\n",
    "   Extract      Basic cleaning      Load raw +         Heavy transforms\n",
    "                (PII masking)       metadata           (dbt, SQL)\n",
    "```\n",
    "\n",
    "### Benefits of ETLT:\n",
    "- **Pre-load**: Apply essential transformations (PII masking, schema validation)\n",
    "- **Post-load**: Leverage cloud DW compute for heavy transformations\n",
    "- **Flexibility**: Keep raw data for re-processing while maintaining data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b3df0",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "| Aspect | ETL | ELT |\n",
    "|--------|-----|-----|\n",
    "| **Order** | Extract → Transform → Load | Extract → Load → Transform |\n",
    "| **Best For** | On-prem, regulated data, smaller volumes | Cloud DW, big data, flexible schemas |\n",
    "| **Transform Engine** | External (ETL server) | Target system (DW compute) |\n",
    "| **Data Quality** | Validated before load | Raw preserved, validated later |\n",
    "| **Modern Tools** | Informatica, Talend, SSIS | dbt, Dataform, Spark SQL |\n",
    "| **Scalability** | Limited by ETL server | Scales with cloud resources |\n",
    "\n",
    "### Decision Framework:\n",
    "\n",
    "1. **Start with ELT** if using modern cloud data platforms (Snowflake, BigQuery, Databricks)\n",
    "2. **Use ETL** when data must be cleaned/masked before entering target system\n",
    "3. **Consider hybrid ETLT** for enterprise environments with mixed requirements\n",
    "4. **Prioritize data quality** regardless of pattern—bad data in = bad insights out\n",
    "\n",
    "### Next Steps:\n",
    "- Explore **dbt** for SQL-based transformations in ELT pipelines\n",
    "- Learn **Apache Airflow** for orchestrating complex data pipelines\n",
    "- Study **streaming ETL** with Kafka, Spark Streaming for real-time needs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}