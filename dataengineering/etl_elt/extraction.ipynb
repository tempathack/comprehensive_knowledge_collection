{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4616a2f",
   "metadata": {},
   "source": [
    "# Data Extraction Techniques\n",
    "\n",
    "Data extraction is the first step in any ETL/ELT pipeline. It involves retrieving data from various source systems and preparing it for transformation and loading. This notebook covers essential extraction patterns, strategies, and practical implementations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Extraction Patterns Overview](#extraction-patterns-overview)\n",
    "2. [Full Extraction vs Incremental Extraction](#full-vs-incremental)\n",
    "3. [Change Data Capture (CDC)](#change-data-capture)\n",
    "4. [Extracting from APIs](#extracting-from-apis)\n",
    "5. [Extracting from Databases](#extracting-from-databases)\n",
    "6. [Extracting from Files](#extracting-from-files)\n",
    "7. [Handling Extraction Challenges](#handling-challenges)\n",
    "8. [Key Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2a00c",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Extraction Patterns Overview <a id='extraction-patterns-overview'></a>\n",
    "\n",
    "### Common Data Sources\n",
    "\n",
    "| Source Type | Examples | Common Protocols |\n",
    "|------------|----------|------------------|\n",
    "| **Databases** | PostgreSQL, MySQL, MongoDB, Oracle | JDBC, ODBC, Native drivers |\n",
    "| **APIs** | REST, GraphQL, SOAP | HTTP/HTTPS |\n",
    "| **Files** | CSV, JSON, Parquet, XML | FTP, SFTP, S3, Local FS |\n",
    "| **Streams** | Kafka, Kinesis, Pub/Sub | TCP, WebSocket |\n",
    "| **SaaS** | Salesforce, HubSpot, Stripe | OAuth, API Keys |\n",
    "\n",
    "### Extraction Strategies\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    EXTRACTION STRATEGIES                        │\n",
    "├─────────────────────┬─────────────────────┬─────────────────────┤\n",
    "│   Full Extraction   │ Incremental Extract │  Change Data Capture│\n",
    "├─────────────────────┼─────────────────────┼─────────────────────┤\n",
    "│ • Extract all data  │ • Extract only new/ │ • Capture changes   │\n",
    "│ • Simple to impl    │   modified records  │   at database level │\n",
    "│ • High resource use │ • Requires tracking │ • Real-time capable │\n",
    "│ • Good for small    │ • Lower bandwidth   │ • Log-based or      │\n",
    "│   datasets          │ • Needs watermarks  │   trigger-based     │\n",
    "└─────────────────────┴─────────────────────┴─────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899df508",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Full Extraction vs Incremental Extraction <a id='full-vs-incremental'></a>\n",
    "\n",
    "### Full Extraction\n",
    "\n",
    "Full extraction involves reading the entire dataset from the source system every time the extraction runs.\n",
    "\n",
    "**When to use:**\n",
    "- Small datasets (< 1 million rows)\n",
    "- Source systems without reliable change tracking\n",
    "- Initial data loads\n",
    "- When data consistency is critical\n",
    "\n",
    "**Pros:**\n",
    "- Simple implementation\n",
    "- Guaranteed data consistency\n",
    "- No need to track state\n",
    "\n",
    "**Cons:**\n",
    "- High resource consumption\n",
    "- Long extraction times for large datasets\n",
    "- Network bandwidth intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e872a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class FullExtractor:\n",
    "    \"\"\"Full extraction pattern - extracts all data from source.\"\"\"\n",
    "    \n",
    "    def __init__(self, source_connection: str):\n",
    "        self.source_connection = source_connection\n",
    "        self.extraction_timestamp = None\n",
    "    \n",
    "    def extract(self, table_name: str, batch_size: int = 10000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract all records from a table.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the source table\n",
    "            batch_size: Number of records per batch for memory efficiency\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing all extracted records\n",
    "        \"\"\"\n",
    "        self.extraction_timestamp = datetime.now()\n",
    "        logger.info(f\"Starting full extraction from {table_name} at {self.extraction_timestamp}\")\n",
    "        \n",
    "        # Simulated extraction - in practice, use actual DB connection\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        \n",
    "        # Example with chunked reading for memory efficiency\n",
    "        chunks = []\n",
    "        # In practice: pd.read_sql(query, connection, chunksize=batch_size)\n",
    "        # for chunk in pd.read_sql(query, self.source_connection, chunksize=batch_size):\n",
    "        #     chunks.append(chunk)\n",
    "        \n",
    "        logger.info(f\"Full extraction completed. Total batches: {len(chunks)}\")\n",
    "        return pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Full Extraction Pattern:\")\n",
    "print(\"- Extracts ALL data every run\")\n",
    "print(\"- Best for: small datasets, initial loads, data consistency requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8fba04",
   "metadata": {},
   "source": [
    "### Incremental Extraction\n",
    "\n",
    "Incremental extraction only retrieves records that have been added or modified since the last extraction.\n",
    "\n",
    "**Common Incremental Strategies:**\n",
    "\n",
    "| Strategy | Description | Requirements |\n",
    "|----------|-------------|-------------|\n",
    "| **Timestamp-based** | Use `updated_at` column | Reliable timestamp column |\n",
    "| **ID-based** | Track max ID extracted | Sequential IDs, inserts only |\n",
    "| **Checksum** | Compare row checksums | Compute & store checksums |\n",
    "| **Version columns** | Track row versions | Version/sequence column |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExtractionState:\n",
    "    \"\"\"Tracks extraction state for incremental loads.\"\"\"\n",
    "    table_name: str\n",
    "    last_extracted_at: datetime\n",
    "    last_extracted_id: Optional[int] = None\n",
    "    last_watermark: Optional[str] = None\n",
    "    records_extracted: int = 0\n",
    "\n",
    "\n",
    "class IncrementalExtractor:\n",
    "    \"\"\"Incremental extraction with watermark tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_file: str = \"extraction_state.json\"):\n",
    "        self.state_file = Path(state_file)\n",
    "        self.states: dict[str, ExtractionState] = self._load_state()\n",
    "    \n",
    "    def _load_state(self) -> dict:\n",
    "        \"\"\"Load extraction state from file.\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            with open(self.state_file) as f:\n",
    "                data = json.load(f)\n",
    "                return {\n",
    "                    k: ExtractionState(\n",
    "                        table_name=v['table_name'],\n",
    "                        last_extracted_at=datetime.fromisoformat(v['last_extracted_at']),\n",
    "                        last_extracted_id=v.get('last_extracted_id'),\n",
    "                        records_extracted=v.get('records_extracted', 0)\n",
    "                    ) for k, v in data.items()\n",
    "                }\n",
    "        return {}\n",
    "    \n",
    "    def _save_state(self):\n",
    "        \"\"\"Persist extraction state to file.\"\"\"\n",
    "        data = {\n",
    "            k: {\n",
    "                'table_name': v.table_name,\n",
    "                'last_extracted_at': v.last_extracted_at.isoformat(),\n",
    "                'last_extracted_id': v.last_extracted_id,\n",
    "                'records_extracted': v.records_extracted\n",
    "            } for k, v in self.states.items()\n",
    "        }\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def extract_by_timestamp(\n",
    "        self,\n",
    "        table_name: str,\n",
    "        timestamp_column: str = \"updated_at\",\n",
    "        overlap_minutes: int = 5\n",
    "    ) -> tuple[str, datetime]:\n",
    "        \"\"\"\n",
    "        Generate incremental extraction query using timestamps.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Source table name\n",
    "            timestamp_column: Column containing update timestamps\n",
    "            overlap_minutes: Safety overlap to handle clock skew\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (SQL query, new watermark timestamp)\n",
    "        \"\"\"\n",
    "        # Get last extraction timestamp or use epoch\n",
    "        if table_name in self.states:\n",
    "            last_ts = self.states[table_name].last_extracted_at\n",
    "            # Apply overlap for safety\n",
    "            watermark = last_ts - timedelta(minutes=overlap_minutes)\n",
    "        else:\n",
    "            watermark = datetime(1970, 1, 1)\n",
    "        \n",
    "        current_ts = datetime.now()\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM {table_name}\n",
    "        WHERE {timestamp_column} > '{watermark.isoformat()}'\n",
    "          AND {timestamp_column} <= '{current_ts.isoformat()}'\n",
    "        ORDER BY {timestamp_column} ASC\n",
    "        \"\"\"\n",
    "        \n",
    "        return query.strip(), current_ts\n",
    "    \n",
    "    def extract_by_id(\n",
    "        self,\n",
    "        table_name: str,\n",
    "        id_column: str = \"id\"\n",
    "    ) -> tuple[str, None]:\n",
    "        \"\"\"\n",
    "        Generate incremental extraction query using sequential IDs.\n",
    "        Best for append-only tables with auto-incrementing IDs.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Source table name\n",
    "            id_column: Column containing sequential IDs\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (SQL query, None)\n",
    "        \"\"\"\n",
    "        last_id = 0\n",
    "        if table_name in self.states and self.states[table_name].last_extracted_id:\n",
    "            last_id = self.states[table_name].last_extracted_id\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM {table_name}\n",
    "        WHERE {id_column} > {last_id}\n",
    "        ORDER BY {id_column} ASC\n",
    "        \"\"\"\n",
    "        \n",
    "        return query.strip(), None\n",
    "    \n",
    "    def update_state(\n",
    "        self,\n",
    "        table_name: str,\n",
    "        extracted_at: datetime,\n",
    "        last_id: Optional[int] = None,\n",
    "        records_count: int = 0\n",
    "    ):\n",
    "        \"\"\"Update extraction state after successful extraction.\"\"\"\n",
    "        self.states[table_name] = ExtractionState(\n",
    "            table_name=table_name,\n",
    "            last_extracted_at=extracted_at,\n",
    "            last_extracted_id=last_id,\n",
    "            records_extracted=records_count\n",
    "        )\n",
    "        self._save_state()\n",
    "        logger.info(f\"Updated state for {table_name}: {records_count} records\")\n",
    "\n",
    "\n",
    "# Demo\n",
    "extractor = IncrementalExtractor()\n",
    "query, watermark = extractor.extract_by_timestamp(\"orders\", \"updated_at\")\n",
    "print(\"Timestamp-based incremental query:\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b54b74",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Change Data Capture (CDC) <a id='change-data-capture'></a>\n",
    "\n",
    "Change Data Capture identifies and captures changes made to data in a database, enabling real-time or near-real-time data replication.\n",
    "\n",
    "### CDC Approaches\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│                         CDC APPROACHES                                  │\n",
    "├────────────────────┬───────────────────────┬───────────────────────────┤\n",
    "│   Log-Based CDC    │    Trigger-Based CDC  │     Query-Based CDC       │\n",
    "├────────────────────┼───────────────────────┼───────────────────────────┤\n",
    "│ Read database logs │ DB triggers capture   │ Poll source with queries  │\n",
    "│ (WAL, binlog)      │ INSERT/UPDATE/DELETE  │ using timestamps/versions │\n",
    "│                    │                       │                           │\n",
    "│ ✓ No source impact │ ✓ Real-time           │ ✓ Simple implementation   │\n",
    "│ ✓ Captures deletes │ ✓ Captures all ops    │ ✓ Works with any DB       │\n",
    "│ ✗ DB-specific      │ ✗ Adds DB overhead    │ ✗ May miss deletes        │\n",
    "│ ✗ Complex setup    │ ✗ Schema changes      │ ✗ Not truly real-time     │\n",
    "└────────────────────┴───────────────────────┴───────────────────────────┘\n",
    "```\n",
    "\n",
    "### Popular CDC Tools\n",
    "\n",
    "| Tool | Type | Supported Sources |\n",
    "|------|------|-------------------|\n",
    "| **Debezium** | Log-based | PostgreSQL, MySQL, MongoDB, SQL Server |\n",
    "| **AWS DMS** | Log-based | Most major databases |\n",
    "| **Fivetran** | Managed | 150+ connectors |\n",
    "| **Airbyte** | Various | Open-source, 300+ connectors |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ed1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "\n",
    "class OperationType(Enum):\n",
    "    \"\"\"CDC operation types.\"\"\"\n",
    "    INSERT = \"INSERT\"\n",
    "    UPDATE = \"UPDATE\"\n",
    "    DELETE = \"DELETE\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CDCEvent:\n",
    "    \"\"\"Represents a change data capture event.\"\"\"\n",
    "    table: str\n",
    "    operation: OperationType\n",
    "    timestamp: datetime\n",
    "    primary_key: dict\n",
    "    before: Optional[dict] = None  # Previous state (for UPDATE/DELETE)\n",
    "    after: Optional[dict] = None   # New state (for INSERT/UPDATE)\n",
    "    transaction_id: Optional[str] = None\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'table': self.table,\n",
    "            'operation': self.operation.value,\n",
    "            'timestamp': self.timestamp.isoformat(),\n",
    "            'primary_key': self.primary_key,\n",
    "            'before': self.before,\n",
    "            'after': self.after\n",
    "        }\n",
    "\n",
    "\n",
    "class QueryBasedCDC:\n",
    "    \"\"\"\n",
    "    Query-based CDC implementation using checksums.\n",
    "    Compares current state with previous snapshot to detect changes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, snapshot_store: dict = None):\n",
    "        self.snapshots = snapshot_store or {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_row_hash(row: dict) -> str:\n",
    "        \"\"\"Compute hash of a row for change detection.\"\"\"\n",
    "        row_str = json.dumps(row, sort_keys=True, default=str)\n",
    "        return hashlib.md5(row_str.encode()).hexdigest()\n",
    "    \n",
    "    def detect_changes(\n",
    "        self,\n",
    "        table_name: str,\n",
    "        current_data: list[dict],\n",
    "        primary_key: str\n",
    "    ) -> list[CDCEvent]:\n",
    "        \"\"\"\n",
    "        Detect INSERT, UPDATE, DELETE by comparing with previous snapshot.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table being tracked\n",
    "            current_data: Current state of the data\n",
    "            primary_key: Name of the primary key column\n",
    "            \n",
    "        Returns:\n",
    "            List of CDC events representing detected changes\n",
    "        \"\"\"\n",
    "        events = []\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Build current state lookup\n",
    "        current_lookup = {row[primary_key]: row for row in current_data}\n",
    "        current_hashes = {\n",
    "            pk: self.compute_row_hash(row) \n",
    "            for pk, row in current_lookup.items()\n",
    "        }\n",
    "        \n",
    "        # Get previous snapshot\n",
    "        prev_snapshot = self.snapshots.get(table_name, {})\n",
    "        prev_lookup = prev_snapshot.get('data', {})\n",
    "        prev_hashes = prev_snapshot.get('hashes', {})\n",
    "        \n",
    "        # Detect INSERTs and UPDATEs\n",
    "        for pk, row in current_lookup.items():\n",
    "            if pk not in prev_lookup:\n",
    "                # New record - INSERT\n",
    "                events.append(CDCEvent(\n",
    "                    table=table_name,\n",
    "                    operation=OperationType.INSERT,\n",
    "                    timestamp=current_time,\n",
    "                    primary_key={primary_key: pk},\n",
    "                    after=row\n",
    "                ))\n",
    "            elif current_hashes[pk] != prev_hashes.get(pk):\n",
    "                # Hash changed - UPDATE\n",
    "                events.append(CDCEvent(\n",
    "                    table=table_name,\n",
    "                    operation=OperationType.UPDATE,\n",
    "                    timestamp=current_time,\n",
    "                    primary_key={primary_key: pk},\n",
    "                    before=prev_lookup[pk],\n",
    "                    after=row\n",
    "                ))\n",
    "        \n",
    "        # Detect DELETEs\n",
    "        for pk in prev_lookup:\n",
    "            if pk not in current_lookup:\n",
    "                events.append(CDCEvent(\n",
    "                    table=table_name,\n",
    "                    operation=OperationType.DELETE,\n",
    "                    timestamp=current_time,\n",
    "                    primary_key={primary_key: pk},\n",
    "                    before=prev_lookup[pk]\n",
    "                ))\n",
    "        \n",
    "        # Update snapshot\n",
    "        self.snapshots[table_name] = {\n",
    "            'data': current_lookup,\n",
    "            'hashes': current_hashes\n",
    "        }\n",
    "        \n",
    "        return events\n",
    "\n",
    "\n",
    "# Demo query-based CDC\n",
    "cdc = QueryBasedCDC()\n",
    "\n",
    "# Initial snapshot\n",
    "initial_data = [\n",
    "    {'id': 1, 'name': 'Alice', 'email': 'alice@example.com'},\n",
    "    {'id': 2, 'name': 'Bob', 'email': 'bob@example.com'},\n",
    "]\n",
    "\n",
    "events = cdc.detect_changes('users', initial_data, 'id')\n",
    "print(f\"Initial load: {len(events)} INSERT events\")\n",
    "\n",
    "# Simulate changes\n",
    "updated_data = [\n",
    "    {'id': 1, 'name': 'Alice Smith', 'email': 'alice@example.com'},  # Updated\n",
    "    # id=2 deleted\n",
    "    {'id': 3, 'name': 'Charlie', 'email': 'charlie@example.com'},    # Inserted\n",
    "]\n",
    "\n",
    "events = cdc.detect_changes('users', updated_data, 'id')\n",
    "print(f\"\\nDetected changes:\")\n",
    "for event in events:\n",
    "    print(f\"  {event.operation.value}: PK={event.primary_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77875083",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Extracting from APIs <a id='extracting-from-apis'></a>\n",
    "\n",
    "API extraction is common when integrating with SaaS platforms, web services, or microservices.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- **Authentication**: API keys, OAuth 2.0, JWT tokens\n",
    "- **Rate Limiting**: Respect API quotas to avoid being blocked\n",
    "- **Pagination**: Handle large result sets efficiently\n",
    "- **Error Handling**: Retries, exponential backoff\n",
    "- **Data Formats**: JSON, XML, Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d30fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Generator, Any\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RateLimitConfig:\n",
    "    \"\"\"Rate limiting configuration.\"\"\"\n",
    "    requests_per_second: float = 10.0\n",
    "    burst_limit: int = 100\n",
    "    retry_after_header: str = \"Retry-After\"\n",
    "\n",
    "\n",
    "class APIExtractor:\n",
    "    \"\"\"\n",
    "    Robust API data extractor with rate limiting, \n",
    "    pagination, and retry logic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str,\n",
    "        auth_token: str,\n",
    "        rate_limit: RateLimitConfig = None\n",
    "    ):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.auth_token = auth_token\n",
    "        self.rate_limit = rate_limit or RateLimitConfig()\n",
    "        self.last_request_time = 0.0\n",
    "        \n",
    "    def _get_headers(self) -> dict:\n",
    "        \"\"\"Build request headers with authentication.\"\"\"\n",
    "        return {\n",
    "            'Authorization': f'Bearer {self.auth_token}',\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "    \n",
    "    def _wait_for_rate_limit(self):\n",
    "        \"\"\"Enforce rate limiting between requests.\"\"\"\n",
    "        min_interval = 1.0 / self.rate_limit.requests_per_second\n",
    "        elapsed = time.time() - self.last_request_time\n",
    "        \n",
    "        if elapsed < min_interval:\n",
    "            sleep_time = min_interval - elapsed\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    def _make_request_with_retry(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        params: dict = None,\n",
    "        max_retries: int = 3,\n",
    "        base_delay: float = 1.0\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Make HTTP request with exponential backoff retry.\n",
    "        \n",
    "        Args:\n",
    "            endpoint: API endpoint path\n",
    "            params: Query parameters\n",
    "            max_retries: Maximum retry attempts\n",
    "            base_delay: Initial delay between retries (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            JSON response data\n",
    "        \"\"\"\n",
    "        # In production, use 'requests' library\n",
    "        # import requests\n",
    "        \n",
    "        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                self._wait_for_rate_limit()\n",
    "                \n",
    "                # Simulated request - replace with actual HTTP call\n",
    "                # response = requests.get(\n",
    "                #     url, \n",
    "                #     headers=self._get_headers(),\n",
    "                #     params=params,\n",
    "                #     timeout=30\n",
    "                # )\n",
    "                \n",
    "                # Simulated response\n",
    "                logger.info(f\"GET {url} (attempt {attempt + 1})\")\n",
    "                return {'data': [], 'next_cursor': None}  # Simulated\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == max_retries:\n",
    "                    logger.error(f\"Max retries exceeded for {url}\")\n",
    "                    raise\n",
    "                \n",
    "                # Exponential backoff with jitter\n",
    "                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                logger.warning(f\"Request failed, retrying in {delay:.2f}s: {e}\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    def extract_paginated(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        page_size: int = 100,\n",
    "        max_pages: Optional[int] = None\n",
    "    ) -> Generator[list[dict], None, None]:\n",
    "        \"\"\"\n",
    "        Extract data with cursor-based pagination.\n",
    "        \n",
    "        Args:\n",
    "            endpoint: API endpoint\n",
    "            page_size: Records per page\n",
    "            max_pages: Maximum pages to fetch (None for all)\n",
    "            \n",
    "        Yields:\n",
    "            Lists of records from each page\n",
    "        \"\"\"\n",
    "        cursor = None\n",
    "        page_count = 0\n",
    "        \n",
    "        while True:\n",
    "            params = {'limit': page_size}\n",
    "            if cursor:\n",
    "                params['cursor'] = cursor\n",
    "            \n",
    "            response = self._make_request_with_retry(endpoint, params)\n",
    "            data = response.get('data', [])\n",
    "            \n",
    "            if data:\n",
    "                yield data\n",
    "                page_count += 1\n",
    "                logger.info(f\"Extracted page {page_count}: {len(data)} records\")\n",
    "            \n",
    "            # Check for next page\n",
    "            cursor = response.get('next_cursor')\n",
    "            if not cursor or not data:\n",
    "                break\n",
    "            \n",
    "            if max_pages and page_count >= max_pages:\n",
    "                logger.info(f\"Reached max pages limit: {max_pages}\")\n",
    "                break\n",
    "        \n",
    "        logger.info(f\"Pagination complete. Total pages: {page_count}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"API Extractor Features:\")\n",
    "print(\"- Rate limiting with configurable RPS\")\n",
    "print(\"- Exponential backoff retry\")\n",
    "print(\"- Cursor-based pagination\")\n",
    "print(\"- Authentication handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world API extraction example with requests library\n",
    "\n",
    "def extract_from_rest_api(\n",
    "    api_url: str,\n",
    "    headers: dict,\n",
    "    params: dict = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Production-ready REST API extraction.\n",
    "    \n",
    "    Example:\n",
    "        df = extract_from_rest_api(\n",
    "            'https://api.example.com/v1/orders',\n",
    "            headers={'Authorization': 'Bearer token123'},\n",
    "            params={'status': 'completed', 'limit': 1000}\n",
    "        )\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    \n",
    "    # Configure retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    \n",
    "    # Create session with retry\n",
    "    session = requests.Session()\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    all_records = []\n",
    "    next_url = api_url\n",
    "    \n",
    "    while next_url:\n",
    "        response = session.get(\n",
    "            next_url,\n",
    "            headers=headers,\n",
    "            params=params if next_url == api_url else None,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        records = data.get('results', data.get('data', []))\n",
    "        all_records.extend(records)\n",
    "        \n",
    "        # Handle pagination (common patterns)\n",
    "        next_url = data.get('next') or data.get('next_page_url')\n",
    "        params = None  # Clear params for subsequent pages\n",
    "    \n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "\n",
    "print(\"REST API extraction function ready\")\n",
    "print(\"Supports: retry, pagination, rate limit handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abfb1b",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Extracting from Databases <a id='extracting-from-databases'></a>\n",
    "\n",
    "Database extraction requires understanding connection management, query optimization, and efficient data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from typing import Iterator\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class DatabaseExtractor(ABC):\n",
    "    \"\"\"Abstract base class for database extractors.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute query and return results.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PostgreSQLExtractor(DatabaseExtractor):\n",
    "    \"\"\"\n",
    "    PostgreSQL extractor with connection pooling and chunked reads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        host: str,\n",
    "        port: int,\n",
    "        database: str,\n",
    "        user: str,\n",
    "        password: str,\n",
    "        pool_size: int = 5\n",
    "    ):\n",
    "        self.connection_string = (\n",
    "            f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "        )\n",
    "        self.pool_size = pool_size\n",
    "        self._engine = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Create SQLAlchemy engine with connection pooling.\"\"\"\n",
    "        from sqlalchemy import create_engine\n",
    "        \n",
    "        self._engine = create_engine(\n",
    "            self.connection_string,\n",
    "            pool_size=self.pool_size,\n",
    "            pool_pre_ping=True,  # Validate connections\n",
    "            pool_recycle=3600    # Recycle connections after 1 hour\n",
    "        )\n",
    "        logger.info(\"PostgreSQL connection pool created\")\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get connection from pool.\"\"\"\n",
    "        if not self._engine:\n",
    "            self.connect()\n",
    "        \n",
    "        conn = self._engine.connect()\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def extract(self, query: str, chunk_size: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract data using a SQL query.\n",
    "        \n",
    "        Args:\n",
    "            query: SQL query to execute\n",
    "            chunk_size: If set, read in chunks (for large datasets)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with query results\n",
    "        \"\"\"\n",
    "        with self.get_connection() as conn:\n",
    "            if chunk_size:\n",
    "                # Chunked reading for large datasets\n",
    "                chunks = pd.read_sql(query, conn, chunksize=chunk_size)\n",
    "                return pd.concat(chunks, ignore_index=True)\n",
    "            else:\n",
    "                return pd.read_sql(query, conn)\n",
    "    \n",
    "    def extract_incremental(\n",
    "        self,\n",
    "        table: str,\n",
    "        timestamp_col: str,\n",
    "        since: datetime,\n",
    "        columns: list[str] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Incremental extraction using timestamp watermark.\n",
    "        \"\"\"\n",
    "        cols = ', '.join(columns) if columns else '*'\n",
    "        query = f\"\"\"\n",
    "        SELECT {cols}\n",
    "        FROM {table}\n",
    "        WHERE {timestamp_col} > %(since)s\n",
    "        ORDER BY {timestamp_col}\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.get_connection() as conn:\n",
    "            return pd.read_sql(query, conn, params={'since': since})\n",
    "    \n",
    "    def extract_with_cursor(\n",
    "        self,\n",
    "        query: str,\n",
    "        batch_size: int = 10000\n",
    "    ) -> Iterator[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Server-side cursor extraction for very large datasets.\n",
    "        Minimizes memory usage by fetching in batches.\n",
    "        \"\"\"\n",
    "        import psycopg2\n",
    "        import psycopg2.extras\n",
    "        \n",
    "        # Use server-side cursor\n",
    "        with self.get_connection() as conn:\n",
    "            cursor_name = f\"extract_cursor_{int(time.time())}\"\n",
    "            \n",
    "            with conn.connection.cursor(\n",
    "                name=cursor_name,\n",
    "                cursor_factory=psycopg2.extras.RealDictCursor\n",
    "            ) as cursor:\n",
    "                cursor.execute(query)\n",
    "                \n",
    "                while True:\n",
    "                    rows = cursor.fetchmany(batch_size)\n",
    "                    if not rows:\n",
    "                        break\n",
    "                    yield pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "print(\"PostgreSQL Extractor Features:\")\n",
    "print(\"- Connection pooling\")\n",
    "print(\"- Chunked reads for large datasets\")\n",
    "print(\"- Incremental extraction\")\n",
    "print(\"- Server-side cursors for memory efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Extractor Example\n",
    "\n",
    "class MongoDBExtractor:\n",
    "    \"\"\"\n",
    "    MongoDB extractor for document databases.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_uri: str, database: str):\n",
    "        self.connection_uri = connection_uri\n",
    "        self.database_name = database\n",
    "        self._client = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Connect to MongoDB.\"\"\"\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        self._client = MongoClient(\n",
    "            self.connection_uri,\n",
    "            maxPoolSize=10,\n",
    "            serverSelectionTimeoutMS=5000\n",
    "        )\n",
    "        # Verify connection\n",
    "        self._client.admin.command('ping')\n",
    "        logger.info(f\"Connected to MongoDB: {self.database_name}\")\n",
    "    \n",
    "    @property\n",
    "    def db(self):\n",
    "        if not self._client:\n",
    "            self.connect()\n",
    "        return self._client[self.database_name]\n",
    "    \n",
    "    def extract_collection(\n",
    "        self,\n",
    "        collection: str,\n",
    "        query: dict = None,\n",
    "        projection: dict = None,\n",
    "        batch_size: int = 1000\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract documents from a MongoDB collection.\n",
    "        \n",
    "        Args:\n",
    "            collection: Collection name\n",
    "            query: MongoDB query filter\n",
    "            projection: Fields to include/exclude\n",
    "            batch_size: Cursor batch size\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted documents\n",
    "        \"\"\"\n",
    "        cursor = self.db[collection].find(\n",
    "            filter=query or {},\n",
    "            projection=projection,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        documents = list(cursor)\n",
    "        logger.info(f\"Extracted {len(documents)} documents from {collection}\")\n",
    "        \n",
    "        return pd.DataFrame(documents)\n",
    "    \n",
    "    def extract_incremental(\n",
    "        self,\n",
    "        collection: str,\n",
    "        timestamp_field: str,\n",
    "        since: datetime\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Incremental extraction based on timestamp field.\n",
    "        \"\"\"\n",
    "        query = {timestamp_field: {'$gt': since}}\n",
    "        return self.extract_collection(collection, query=query)\n",
    "    \n",
    "    def extract_with_aggregation(\n",
    "        self,\n",
    "        collection: str,\n",
    "        pipeline: list[dict]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract using MongoDB aggregation pipeline.\n",
    "        Useful for complex transformations at source.\n",
    "        \"\"\"\n",
    "        cursor = self.db[collection].aggregate(\n",
    "            pipeline,\n",
    "            allowDiskUse=True  # For large aggregations\n",
    "        )\n",
    "        return pd.DataFrame(list(cursor))\n",
    "\n",
    "\n",
    "# Example aggregation pipeline\n",
    "example_pipeline = [\n",
    "    {'$match': {'status': 'active'}},\n",
    "    {'$group': {\n",
    "        '_id': '$category',\n",
    "        'total': {'$sum': '$amount'},\n",
    "        'count': {'$sum': 1}\n",
    "    }},\n",
    "    {'$sort': {'total': -1}}\n",
    "]\n",
    "\n",
    "print(\"MongoDB Extractor ready\")\n",
    "print(f\"Example pipeline: {example_pipeline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5a442",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Extracting from Files <a id='extracting-from-files'></a>\n",
    "\n",
    "File-based extraction handles CSV, JSON, Parquet, Excel, and other file formats from local storage, cloud storage, or remote servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "class FileExtractor:\n",
    "    \"\"\"\n",
    "    Multi-format file extractor with parallel processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_FORMATS = {\n",
    "        '.csv': 'csv',\n",
    "        '.json': 'json',\n",
    "        '.jsonl': 'jsonl',\n",
    "        '.parquet': 'parquet',\n",
    "        '.xlsx': 'excel',\n",
    "        '.xls': 'excel',\n",
    "        '.xml': 'xml'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, max_workers: int = 4):\n",
    "        self.max_workers = max_workers\n",
    "    \n",
    "    def _read_file(self, file_path: Path, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Read a single file based on its extension.\"\"\"\n",
    "        suffix = file_path.suffix.lower()\n",
    "        \n",
    "        if suffix not in self.SUPPORTED_FORMATS:\n",
    "            raise ValueError(f\"Unsupported format: {suffix}\")\n",
    "        \n",
    "        format_type = self.SUPPORTED_FORMATS[suffix]\n",
    "        \n",
    "        readers = {\n",
    "            'csv': lambda p: pd.read_csv(p, **kwargs),\n",
    "            'json': lambda p: pd.read_json(p, **kwargs),\n",
    "            'jsonl': lambda p: pd.read_json(p, lines=True, **kwargs),\n",
    "            'parquet': lambda p: pd.read_parquet(p, **kwargs),\n",
    "            'excel': lambda p: pd.read_excel(p, **kwargs),\n",
    "            'xml': lambda p: pd.read_xml(p, **kwargs)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Reading {format_type} file: {file_path}\")\n",
    "        return readers[format_type](file_path)\n",
    "    \n",
    "    def extract_file(self, file_path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract data from a single file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file\n",
    "            **kwargs: Additional arguments for pandas reader\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with file contents\n",
    "        \"\"\"\n",
    "        path = Path(file_path)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        return self._read_file(path, **kwargs)\n",
    "    \n",
    "    def extract_directory(\n",
    "        self,\n",
    "        directory: str,\n",
    "        pattern: str = \"*.*\",\n",
    "        recursive: bool = True,\n",
    "        parallel: bool = True,\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract and combine all matching files from a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory: Directory path\n",
    "            pattern: Glob pattern for file matching\n",
    "            recursive: Search subdirectories\n",
    "            parallel: Use parallel processing\n",
    "            \n",
    "        Returns:\n",
    "            Combined DataFrame from all files\n",
    "        \"\"\"\n",
    "        dir_path = Path(directory)\n",
    "        \n",
    "        if recursive:\n",
    "            files = list(dir_path.rglob(pattern))\n",
    "        else:\n",
    "            files = list(dir_path.glob(pattern))\n",
    "        \n",
    "        # Filter to supported formats\n",
    "        files = [f for f in files if f.suffix.lower() in self.SUPPORTED_FORMATS]\n",
    "        \n",
    "        if not files:\n",
    "            logger.warning(f\"No matching files found in {directory}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        logger.info(f\"Found {len(files)} files to extract\")\n",
    "        \n",
    "        if parallel and len(files) > 1:\n",
    "            return self._extract_parallel(files, **kwargs)\n",
    "        else:\n",
    "            return self._extract_sequential(files, **kwargs)\n",
    "    \n",
    "    def _extract_sequential(self, files: list, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Extract files sequentially.\"\"\"\n",
    "        dfs = []\n",
    "        for file_path in files:\n",
    "            try:\n",
    "                df = self._read_file(file_path, **kwargs)\n",
    "                df['_source_file'] = str(file_path)\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "    \n",
    "    def _extract_parallel(self, files: list, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Extract files in parallel using thread pool.\"\"\"\n",
    "        dfs = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_file = {\n",
    "                executor.submit(self._read_file, f, **kwargs): f \n",
    "                for f in files\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_file):\n",
    "                file_path = future_to_file[future]\n",
    "                try:\n",
    "                    df = future.result()\n",
    "                    df['_source_file'] = str(file_path)\n",
    "                    dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Demo\n",
    "extractor = FileExtractor(max_workers=4)\n",
    "print(\"Supported formats:\", list(FileExtractor.SUPPORTED_FORMATS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Storage Extraction (S3 Example)\n",
    "\n",
    "class S3Extractor:\n",
    "    \"\"\"\n",
    "    Extract files from Amazon S3 with support for large datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket: str,\n",
    "        aws_access_key: str = None,\n",
    "        aws_secret_key: str = None,\n",
    "        region: str = 'us-east-1'\n",
    "    ):\n",
    "        self.bucket = bucket\n",
    "        self.region = region\n",
    "        \n",
    "        # Initialize boto3 client\n",
    "        # import boto3\n",
    "        # self.s3_client = boto3.client(\n",
    "        #     's3',\n",
    "        #     aws_access_key_id=aws_access_key,\n",
    "        #     aws_secret_access_key=aws_secret_key,\n",
    "        #     region_name=region\n",
    "        # )\n",
    "    \n",
    "    def list_objects(self, prefix: str = '') -> list[str]:\n",
    "        \"\"\"List objects in bucket with optional prefix filter.\"\"\"\n",
    "        objects = []\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        \n",
    "        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                objects.append(obj['Key'])\n",
    "        \n",
    "        return objects\n",
    "    \n",
    "    def extract_parquet(self, key: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract Parquet file directly from S3.\n",
    "        Uses pyarrow for efficient columnar reads.\n",
    "        \"\"\"\n",
    "        s3_path = f\"s3://{self.bucket}/{key}\"\n",
    "        return pd.read_parquet(s3_path)\n",
    "    \n",
    "    def extract_csv_chunked(\n",
    "        self,\n",
    "        key: str,\n",
    "        chunk_size: int = 100000\n",
    "    ) -> Iterator[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Stream large CSV from S3 in chunks.\n",
    "        \n",
    "        Yields:\n",
    "            DataFrame chunks\n",
    "        \"\"\"\n",
    "        import io\n",
    "        \n",
    "        response = self.s3_client.get_object(Bucket=self.bucket, Key=key)\n",
    "        \n",
    "        for chunk in pd.read_csv(\n",
    "            io.BytesIO(response['Body'].read()),\n",
    "            chunksize=chunk_size\n",
    "        ):\n",
    "            yield chunk\n",
    "    \n",
    "    def extract_partitioned_data(\n",
    "        self,\n",
    "        prefix: str,\n",
    "        partition_filter: dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract partitioned Parquet data (Hive-style partitioning).\n",
    "        \n",
    "        Example:\n",
    "            s3://bucket/data/year=2024/month=01/data.parquet\n",
    "        \"\"\"\n",
    "        import pyarrow.parquet as pq\n",
    "        import pyarrow.dataset as ds\n",
    "        \n",
    "        s3_path = f\"s3://{self.bucket}/{prefix}\"\n",
    "        \n",
    "        dataset = ds.dataset(\n",
    "            s3_path,\n",
    "            partitioning='hive'\n",
    "        )\n",
    "        \n",
    "        if partition_filter:\n",
    "            # Build filter expression\n",
    "            filter_expr = None\n",
    "            for col, value in partition_filter.items():\n",
    "                condition = ds.field(col) == value\n",
    "                filter_expr = condition if filter_expr is None else filter_expr & condition\n",
    "            \n",
    "            return dataset.to_table(filter=filter_expr).to_pandas()\n",
    "        \n",
    "        return dataset.to_table().to_pandas()\n",
    "\n",
    "\n",
    "print(\"S3 Extractor Features:\")\n",
    "print(\"- Direct Parquet reads\")\n",
    "print(\"- Chunked CSV streaming\")\n",
    "print(\"- Partition pruning for efficient queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc866d5",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Handling Extraction Challenges <a id='handling-challenges'></a>\n",
    "\n",
    "### Common Challenges\n",
    "\n",
    "| Challenge | Impact | Solution |\n",
    "|-----------|--------|----------|\n",
    "| **Rate Limiting** | Blocked requests, failed extractions | Token bucket, exponential backoff |\n",
    "| **Pagination** | Missing data, incomplete extracts | Cursor tracking, offset management |\n",
    "| **Large Datasets** | Memory exhaustion, timeouts | Chunking, streaming, partitioning |\n",
    "| **Network Issues** | Failed transfers | Retries, checkpointing |\n",
    "| **Schema Changes** | Data type mismatches | Schema evolution handling |\n",
    "| **Duplicates** | Data quality issues | Deduplication, idempotent operations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0266fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class TokenBucketRateLimiter:\n",
    "    \"\"\"\n",
    "    Token bucket rate limiter for API requests.\n",
    "    Allows bursting while maintaining average rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        rate: float,        # Tokens per second\n",
    "        capacity: int       # Maximum burst size\n",
    "    ):\n",
    "        self.rate = rate\n",
    "        self.capacity = capacity\n",
    "        self.tokens = capacity\n",
    "        self.last_update = time.time()\n",
    "        self._lock = None  # Use threading.Lock() in production\n",
    "    \n",
    "    def _refill(self):\n",
    "        \"\"\"Refill tokens based on elapsed time.\"\"\"\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_update\n",
    "        self.tokens = min(\n",
    "            self.capacity,\n",
    "            self.tokens + elapsed * self.rate\n",
    "        )\n",
    "        self.last_update = now\n",
    "    \n",
    "    def acquire(self, tokens: int = 1) -> float:\n",
    "        \"\"\"\n",
    "        Acquire tokens, blocking if necessary.\n",
    "        \n",
    "        Returns:\n",
    "            Wait time in seconds (0 if no wait needed)\n",
    "        \"\"\"\n",
    "        self._refill()\n",
    "        \n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate wait time\n",
    "        tokens_needed = tokens - self.tokens\n",
    "        wait_time = tokens_needed / self.rate\n",
    "        \n",
    "        time.sleep(wait_time)\n",
    "        self._refill()\n",
    "        self.tokens -= tokens\n",
    "        \n",
    "        return wait_time\n",
    "    \n",
    "    def can_proceed(self, tokens: int = 1) -> bool:\n",
    "        \"\"\"Check if request can proceed without blocking.\"\"\"\n",
    "        self._refill()\n",
    "        return self.tokens >= tokens\n",
    "\n",
    "\n",
    "# Demo\n",
    "limiter = TokenBucketRateLimiter(rate=10, capacity=20)\n",
    "print(f\"Rate limiter: {limiter.rate} req/sec, burst: {limiter.capacity}\")\n",
    "print(f\"Can proceed: {limiter.can_proceed()}\")\n",
    "print(f\"Available tokens: {limiter.tokens:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2436c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class PaginationType(Enum):\n",
    "    \"\"\"Types of pagination strategies.\"\"\"\n",
    "    OFFSET = \"offset\"         # ?offset=100&limit=50\n",
    "    PAGE = \"page\"             # ?page=2&per_page=50\n",
    "    CURSOR = \"cursor\"         # ?cursor=abc123\n",
    "    KEYSET = \"keyset\"         # ?after_id=1000\n",
    "    LINK_HEADER = \"link\"      # Link: <url>; rel=\"next\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PaginationConfig:\n",
    "    \"\"\"Configuration for pagination handling.\"\"\"\n",
    "    type: PaginationType\n",
    "    page_size: int = 100\n",
    "    max_pages: Optional[int] = None\n",
    "    \n",
    "    # Field names in API response/request\n",
    "    cursor_param: str = \"cursor\"\n",
    "    cursor_response_field: str = \"next_cursor\"\n",
    "    offset_param: str = \"offset\"\n",
    "    page_param: str = \"page\"\n",
    "    limit_param: str = \"limit\"\n",
    "    data_field: str = \"data\"\n",
    "    total_field: str = \"total\"\n",
    "\n",
    "\n",
    "class PaginationHandler:\n",
    "    \"\"\"\n",
    "    Unified pagination handler supporting multiple strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PaginationConfig):\n",
    "        self.config = config\n",
    "        self._current_offset = 0\n",
    "        self._current_page = 1\n",
    "        self._current_cursor = None\n",
    "        self._pages_fetched = 0\n",
    "        self._exhausted = False\n",
    "    \n",
    "    def get_params(self) -> dict:\n",
    "        \"\"\"Get query parameters for next request.\"\"\"\n",
    "        params = {self.config.limit_param: self.config.page_size}\n",
    "        \n",
    "        if self.config.type == PaginationType.OFFSET:\n",
    "            params[self.config.offset_param] = self._current_offset\n",
    "        \n",
    "        elif self.config.type == PaginationType.PAGE:\n",
    "            params[self.config.page_param] = self._current_page\n",
    "        \n",
    "        elif self.config.type == PaginationType.CURSOR:\n",
    "            if self._current_cursor:\n",
    "                params[self.config.cursor_param] = self._current_cursor\n",
    "        \n",
    "        elif self.config.type == PaginationType.KEYSET:\n",
    "            if self._current_cursor:\n",
    "                params['after_id'] = self._current_cursor\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def process_response(self, response: dict) -> list:\n",
    "        \"\"\"\n",
    "        Process API response and update pagination state.\n",
    "        \n",
    "        Returns:\n",
    "            List of records from response\n",
    "        \"\"\"\n",
    "        data = response.get(self.config.data_field, [])\n",
    "        self._pages_fetched += 1\n",
    "        \n",
    "        # Update state based on pagination type\n",
    "        if self.config.type == PaginationType.OFFSET:\n",
    "            self._current_offset += len(data)\n",
    "            total = response.get(self.config.total_field)\n",
    "            if total and self._current_offset >= total:\n",
    "                self._exhausted = True\n",
    "        \n",
    "        elif self.config.type == PaginationType.PAGE:\n",
    "            self._current_page += 1\n",
    "        \n",
    "        elif self.config.type in (PaginationType.CURSOR, PaginationType.KEYSET):\n",
    "            self._current_cursor = response.get(self.config.cursor_response_field)\n",
    "            if not self._current_cursor:\n",
    "                self._exhausted = True\n",
    "        \n",
    "        # Check if we've hit page limit or no more data\n",
    "        if not data:\n",
    "            self._exhausted = True\n",
    "        \n",
    "        if self.config.max_pages and self._pages_fetched >= self.config.max_pages:\n",
    "            self._exhausted = True\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def has_more(self) -> bool:\n",
    "        \"\"\"Check if more pages are available.\"\"\"\n",
    "        return not self._exhausted\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset pagination state for fresh extraction.\"\"\"\n",
    "        self._current_offset = 0\n",
    "        self._current_page = 1\n",
    "        self._current_cursor = None\n",
    "        self._pages_fetched = 0\n",
    "        self._exhausted = False\n",
    "\n",
    "\n",
    "# Example usage\n",
    "config = PaginationConfig(\n",
    "    type=PaginationType.CURSOR,\n",
    "    page_size=100,\n",
    "    max_pages=50\n",
    ")\n",
    "\n",
    "handler = PaginationHandler(config)\n",
    "print(f\"Pagination type: {config.type.value}\")\n",
    "print(f\"Initial params: {handler.get_params()}\")\n",
    "\n",
    "# Simulate response\n",
    "mock_response = {\n",
    "    'data': [{'id': i} for i in range(100)],\n",
    "    'next_cursor': 'abc123'\n",
    "}\n",
    "handler.process_response(mock_response)\n",
    "print(f\"After first page - has_more: {handler.has_more()}\")\n",
    "print(f\"Next params: {handler.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9020d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing for resumable extractions\n",
    "\n",
    "@dataclass\n",
    "class ExtractionCheckpoint:\n",
    "    \"\"\"Checkpoint for resumable extraction jobs.\"\"\"\n",
    "    job_id: str\n",
    "    source: str\n",
    "    started_at: datetime\n",
    "    last_updated: datetime\n",
    "    records_extracted: int = 0\n",
    "    last_offset: int = 0\n",
    "    last_cursor: Optional[str] = None\n",
    "    last_timestamp: Optional[datetime] = None\n",
    "    status: str = \"running\"\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    Manages extraction checkpoints for fault tolerance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str = \"./checkpoints\"):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def _checkpoint_path(self, job_id: str) -> Path:\n",
    "        return self.checkpoint_dir / f\"{job_id}.json\"\n",
    "    \n",
    "    def create(self, job_id: str, source: str) -> ExtractionCheckpoint:\n",
    "        \"\"\"Create new extraction checkpoint.\"\"\"\n",
    "        now = datetime.now()\n",
    "        checkpoint = ExtractionCheckpoint(\n",
    "            job_id=job_id,\n",
    "            source=source,\n",
    "            started_at=now,\n",
    "            last_updated=now\n",
    "        )\n",
    "        self.save(checkpoint)\n",
    "        return checkpoint\n",
    "    \n",
    "    def save(self, checkpoint: ExtractionCheckpoint):\n",
    "        \"\"\"Persist checkpoint to disk.\"\"\"\n",
    "        checkpoint.last_updated = datetime.now()\n",
    "        \n",
    "        data = {\n",
    "            'job_id': checkpoint.job_id,\n",
    "            'source': checkpoint.source,\n",
    "            'started_at': checkpoint.started_at.isoformat(),\n",
    "            'last_updated': checkpoint.last_updated.isoformat(),\n",
    "            'records_extracted': checkpoint.records_extracted,\n",
    "            'last_offset': checkpoint.last_offset,\n",
    "            'last_cursor': checkpoint.last_cursor,\n",
    "            'last_timestamp': (\n",
    "                checkpoint.last_timestamp.isoformat() \n",
    "                if checkpoint.last_timestamp else None\n",
    "            ),\n",
    "            'status': checkpoint.status,\n",
    "            'error_message': checkpoint.error_message\n",
    "        }\n",
    "        \n",
    "        with open(self._checkpoint_path(checkpoint.job_id), 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def load(self, job_id: str) -> Optional[ExtractionCheckpoint]:\n",
    "        \"\"\"Load checkpoint from disk.\"\"\"\n",
    "        path = self._checkpoint_path(job_id)\n",
    "        \n",
    "        if not path.exists():\n",
    "            return None\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        return ExtractionCheckpoint(\n",
    "            job_id=data['job_id'],\n",
    "            source=data['source'],\n",
    "            started_at=datetime.fromisoformat(data['started_at']),\n",
    "            last_updated=datetime.fromisoformat(data['last_updated']),\n",
    "            records_extracted=data['records_extracted'],\n",
    "            last_offset=data['last_offset'],\n",
    "            last_cursor=data.get('last_cursor'),\n",
    "            last_timestamp=(\n",
    "                datetime.fromisoformat(data['last_timestamp'])\n",
    "                if data.get('last_timestamp') else None\n",
    "            ),\n",
    "            status=data['status'],\n",
    "            error_message=data.get('error_message')\n",
    "        )\n",
    "    \n",
    "    def mark_complete(self, checkpoint: ExtractionCheckpoint):\n",
    "        \"\"\"Mark extraction as successfully completed.\"\"\"\n",
    "        checkpoint.status = \"completed\"\n",
    "        self.save(checkpoint)\n",
    "    \n",
    "    def mark_failed(self, checkpoint: ExtractionCheckpoint, error: str):\n",
    "        \"\"\"Mark extraction as failed with error message.\"\"\"\n",
    "        checkpoint.status = \"failed\"\n",
    "        checkpoint.error_message = error\n",
    "        self.save(checkpoint)\n",
    "\n",
    "\n",
    "# Demo\n",
    "mgr = CheckpointManager()\n",
    "print(\"CheckpointManager: Enables resumable extractions\")\n",
    "print(\"- Create checkpoints before starting\")\n",
    "print(\"- Update periodically during extraction\")\n",
    "print(\"- Resume from last checkpoint on failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74c672",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Key Takeaways <a id='takeaways'></a>\n",
    "\n",
    "### Summary\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│                    DATA EXTRACTION BEST PRACTICES                       │\n",
    "├────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  1. CHOOSE THE RIGHT STRATEGY                                          │\n",
    "│     • Full extraction: Small datasets, initial loads                   │\n",
    "│     • Incremental: Large datasets, frequent updates                    │\n",
    "│     • CDC: Real-time requirements, audit trails                        │\n",
    "│                                                                         │\n",
    "│  2. IMPLEMENT ROBUST ERROR HANDLING                                    │\n",
    "│     • Exponential backoff for retries                                  │\n",
    "│     • Circuit breakers for failing sources                             │\n",
    "│     • Checkpointing for resumability                                   │\n",
    "│                                                                         │\n",
    "│  3. RESPECT SOURCE SYSTEMS                                             │\n",
    "│     • Honor rate limits                                                │\n",
    "│     • Use connection pooling                                           │\n",
    "│     • Schedule extractions during off-peak hours                       │\n",
    "│                                                                         │\n",
    "│  4. OPTIMIZE FOR SCALE                                                 │\n",
    "│     • Chunked reads for large datasets                                 │\n",
    "│     • Parallel extraction where possible                               │\n",
    "│     • Partition pruning for cloud storage                              │\n",
    "│                                                                         │\n",
    "│  5. ENSURE DATA QUALITY                                                │\n",
    "│     • Validate schemas on extraction                                   │\n",
    "│     • Implement deduplication                                          │\n",
    "│     • Track extraction lineage                                         │\n",
    "│                                                                         │\n",
    "└────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Extraction Pattern Decision Tree\n",
    "\n",
    "```\n",
    "                    Is real-time required?\n",
    "                           │\n",
    "              ┌────────────┴────────────┐\n",
    "              │ Yes                     │ No\n",
    "              ▼                         ▼\n",
    "        Use CDC/Streaming        Dataset size?\n",
    "        (Debezium, Kafka)              │\n",
    "                           ┌──────────┴──────────┐\n",
    "                           │ < 1M rows           │ > 1M rows\n",
    "                           ▼                     ▼\n",
    "                    Full Extraction        Has timestamp\n",
    "                                           or version?\n",
    "                                                │\n",
    "                                   ┌───────────┴───────────┐\n",
    "                                   │ Yes                   │ No\n",
    "                                   ▼                       ▼\n",
    "                            Incremental            Consider CDC\n",
    "                            Extraction             or Full Extract\n",
    "```\n",
    "\n",
    "### Essential Tools & Libraries\n",
    "\n",
    "| Category | Tools |\n",
    "|----------|-------|\n",
    "| **Database Connectors** | SQLAlchemy, psycopg2, pymongo, pyodbc |\n",
    "| **API Clients** | requests, httpx, aiohttp |\n",
    "| **File Processing** | pandas, pyarrow, polars |\n",
    "| **Cloud Storage** | boto3, google-cloud-storage, azure-storage-blob |\n",
    "| **CDC** | Debezium, AWS DMS, Airbyte |\n",
    "| **Orchestration** | Apache Airflow, Prefect, Dagster |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Reference: Extraction Patterns Comparison\n",
    "\n",
    "comparison_data = {\n",
    "    'Pattern': ['Full Extraction', 'Incremental', 'CDC (Log-based)', 'CDC (Query-based)'],\n",
    "    'Latency': ['High', 'Medium', 'Low (near real-time)', 'Medium'],\n",
    "    'Resource Usage': ['High', 'Low', 'Low', 'Medium'],\n",
    "    'Complexity': ['Low', 'Medium', 'High', 'Medium'],\n",
    "    'Captures Deletes': ['Yes', 'No*', 'Yes', 'Yes'],\n",
    "    'Best For': [\n",
    "        'Small tables, initial loads',\n",
    "        'Large tables with timestamps',\n",
    "        'Real-time sync, audit logs',\n",
    "        'When log CDC not available'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Extraction Patterns Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n* Incremental can capture deletes with soft-delete patterns\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
