{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ebc988",
   "metadata": {},
   "source": [
    "# Stream Processing — Overview\n",
    "\n",
    "## Purpose\n",
    "Stream processing enables the continuous ingestion, processing, and analysis of data in real-time as it arrives. Unlike batch processing, which operates on bounded datasets, stream processing handles unbounded, continuously flowing data with low latency.\n",
    "\n",
    "## Key Questions\n",
    "- What is stream processing and how does it differ from batch processing?\n",
    "- What are the core concepts of event-driven architecture?\n",
    "- How do popular stream processing frameworks (Kafka, Flink, Spark Streaming) compare?\n",
    "- What is the difference between event time and processing time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c876d39",
   "metadata": {},
   "source": [
    "---\n",
    "## What is Stream Processing?\n",
    "\n",
    "**Stream processing** is a data processing paradigm that processes data records continuously and sequentially as they arrive, rather than waiting for all data to be collected first.\n",
    "\n",
    "### Characteristics of Stream Processing\n",
    "\n",
    "| Characteristic | Description |\n",
    "|----------------|-------------|\n",
    "| **Real-time** | Data is processed immediately upon arrival (milliseconds to seconds latency) |\n",
    "| **Unbounded data** | Works with infinite, continuously arriving data streams |\n",
    "| **Stateful/Stateless** | Can maintain state across events or process each event independently |\n",
    "| **Fault-tolerant** | Handles failures gracefully with exactly-once or at-least-once guarantees |\n",
    "\n",
    "### Stream Processing vs Batch Processing\n",
    "\n",
    "| Aspect | Stream Processing | Batch Processing |\n",
    "|--------|-------------------|------------------|\n",
    "| **Data** | Unbounded, continuous | Bounded, finite |\n",
    "| **Latency** | Milliseconds to seconds | Minutes to hours |\n",
    "| **Processing** | Record-by-record or micro-batch | Large chunks at once |\n",
    "| **Use Cases** | Fraud detection, real-time analytics | ETL, reporting, ML training |\n",
    "| **Complexity** | Higher (state management, ordering) | Lower (simpler semantics) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f0e1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Event-Driven Architecture Concepts\n",
    "\n",
    "Event-driven architecture (EDA) is a design pattern where the flow of the program is determined by events—significant changes in state.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────────┐     ┌──────────────────┐\n",
    "│   Producer   │────▶│   Event Broker   │────▶│    Consumer      │\n",
    "│ (Event Source)│     │  (Message Queue) │     │ (Event Handler)  │\n",
    "└──────────────┘     └──────────────────┘     └──────────────────┘\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Event** | An immutable record of something that happened (e.g., \"OrderPlaced\", \"UserClicked\") |\n",
    "| **Producer** | Generates and publishes events to a channel/topic |\n",
    "| **Consumer** | Subscribes to and processes events from channels/topics |\n",
    "| **Event Broker** | Middleware that routes events between producers and consumers |\n",
    "| **Topic/Channel** | Named destination where events are published and consumed |\n",
    "\n",
    "### Event Processing Patterns\n",
    "\n",
    "1. **Simple Event Processing**: React to individual events immediately\n",
    "2. **Event Stream Processing**: Process continuous streams of events\n",
    "3. **Complex Event Processing (CEP)**: Detect patterns across multiple events over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89194d3d",
   "metadata": {},
   "source": [
    "---\n",
    "## Stream Processing Frameworks Overview\n",
    "\n",
    "### Apache Kafka\n",
    "\n",
    "**Kafka** is a distributed event streaming platform designed for high-throughput, fault-tolerant messaging.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    Kafka Cluster                        │\n",
    "│  ┌─────────┐  ┌─────────┐  ┌─────────┐                 │\n",
    "│  │ Broker 1│  │ Broker 2│  │ Broker 3│                 │\n",
    "│  │         │  │         │  │         │                 │\n",
    "│  │Topic A  │  │Topic A  │  │Topic B  │                 │\n",
    "│  │Partition│  │Partition│  │Partition│                 │\n",
    "│  │  0,1    │  │  2,3    │  │  0,1,2  │                 │\n",
    "│  └─────────┘  └─────────┘  └─────────┘                 │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Distributed log-based storage\n",
    "- Horizontal scalability via partitions\n",
    "- Consumer groups for parallel processing\n",
    "- Kafka Streams for stream processing within the Kafka ecosystem\n",
    "\n",
    "---\n",
    "\n",
    "### Apache Flink\n",
    "\n",
    "**Flink** is a distributed stream processing framework with true event-at-a-time processing.\n",
    "\n",
    "**Key Features:**\n",
    "- Native streaming (not micro-batch)\n",
    "- Advanced windowing and event time support\n",
    "- Exactly-once state consistency\n",
    "- Unified batch and streaming API\n",
    "\n",
    "```python\n",
    "# Flink conceptual example (PyFlink)\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "stream = env.from_collection([1, 2, 3, 4, 5])\n",
    "result = stream.map(lambda x: x * 2).filter(lambda x: x > 4)\n",
    "result.print()\n",
    "env.execute(\"Flink Example\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Apache Spark Streaming\n",
    "\n",
    "**Spark Streaming** (and Structured Streaming) processes data in micro-batches using the Spark engine.\n",
    "\n",
    "**Key Features:**\n",
    "- Micro-batch processing model\n",
    "- Integration with Spark ecosystem (MLlib, SQL)\n",
    "- DataFrame/Dataset API for streaming\n",
    "- Exactly-once guarantees with checkpointing\n",
    "\n",
    "```python\n",
    "# Spark Structured Streaming conceptual example\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamExample\").getOrCreate()\n",
    "\n",
    "# Read from Kafka\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()\n",
    "\n",
    "# Process and write\n",
    "query = stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison\n",
    "\n",
    "| Feature | Kafka Streams | Apache Flink | Spark Streaming |\n",
    "|---------|---------------|--------------|------------------|\n",
    "| **Processing Model** | Record-at-a-time | Record-at-a-time | Micro-batch |\n",
    "| **Latency** | Very low | Very low | Low (seconds) |\n",
    "| **State Management** | RocksDB | RocksDB, heap | In-memory, external |\n",
    "| **Exactly-once** | ✅ | ✅ | ✅ |\n",
    "| **Deployment** | Lightweight (library) | Cluster | Cluster |\n",
    "| **Best For** | Kafka-centric apps | Complex event processing | Spark ecosystem users |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806637a",
   "metadata": {},
   "source": [
    "---\n",
    "## Ordering, Keys, and Backpressure\n",
    "\n",
    "**Ordering** is only guaranteed **within a partition** (Kafka) or **keyed stream** (Flink).\n",
    "- Choose partition keys carefully to preserve order where needed.\n",
    "- Cross-partition ordering requires additional coordination.\n",
    "\n",
    "**Keys and parallelism:**\n",
    "- Partitioning by key enables **stateful per-entity** processing.\n",
    "- Too many unique keys can create hot partitions or state blow-up.\n",
    "\n",
    "**Backpressure** protects the system when downstream is slow.\n",
    "- Systems propagate backpressure to producers or buffer with limits.\n",
    "- Watch for excessive lag, large queues, or growing watermark delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225455b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Reliability, State, and Fault Tolerance\n",
    "\n",
    "Stream processors often keep **state** (e.g., counts, windows). Correctness depends on how state is stored and recovered.\n",
    "\n",
    "**Key mechanisms:**\n",
    "- **Checkpointing**: periodic snapshots of state to durable storage.\n",
    "- **State backends**: RocksDB (disk), in-memory + spill, or external stores.\n",
    "- **Exactly-once processing**: coordinated checkpoints + transactional sinks.\n",
    "- **Idempotent sinks**: avoid duplicates if at-least-once delivery.\n",
    "\n",
    "**Failure recovery flow:**\n",
    "1. Detect failure and restart tasks\n",
    "2. Restore from latest checkpoint\n",
    "3. Reprocess events after the checkpoint offset\n",
    "4. Resume normal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e4a52",
   "metadata": {},
   "source": [
    "---\n",
    "## Event Time vs Processing Time\n",
    "\n",
    "Understanding time semantics is crucial for correct stream processing results.\n",
    "\n",
    "### Time Concepts\n",
    "\n",
    "| Time Type | Definition | Example |\n",
    "|-----------|------------|----------|\n",
    "| **Event Time** | When the event actually occurred (embedded in the event) | User clicked at 10:00:00 |\n",
    "| **Processing Time** | When the event is processed by the system | System processes click at 10:00:05 |\n",
    "| **Ingestion Time** | When the event enters the streaming system | Event arrives at broker at 10:00:02 |\n",
    "\n",
    "### Why Event Time Matters\n",
    "\n",
    "```\n",
    "Event Time:      10:00:00   10:00:01   10:00:02   10:00:03\n",
    "                    │          │          │          │\n",
    "Events:            [A]        [B]        [C]        [D]\n",
    "                    │          │          │          │\n",
    "Arrival Order:     [A]        [C]        [B]        [D]  ← Out of order!\n",
    "                    │          │          │          │\n",
    "Processing Time: 10:00:05  10:00:06   10:00:07   10:00:08\n",
    "```\n",
    "\n",
    "**Challenges with Event Time:**\n",
    "- Events can arrive out of order\n",
    "- Events can be delayed (network issues, mobile devices offline)\n",
    "- Need to handle \"late\" data\n",
    "\n",
    "### Watermarks\n",
    "\n",
    "**Watermarks** are a mechanism to track progress in event time and handle late data.\n",
    "\n",
    "```\n",
    "Watermark = \"All events with event time ≤ W have arrived\"\n",
    "\n",
    "Event Stream: ──[E1:10:00]──[E3:10:02]──[E2:10:01]──[W:10:02]──[E4:10:03]──▶\n",
    "                                                      │\n",
    "                                           Watermark triggers window close\n",
    "```\n",
    "\n",
    "### Windowing with Event Time\n",
    "\n",
    "| Window Type | Description |\n",
    "|-------------|-------------|\n",
    "| **Tumbling** | Fixed-size, non-overlapping windows |\n",
    "| **Sliding** | Fixed-size, overlapping windows |\n",
    "| **Session** | Dynamic windows based on activity gaps |\n",
    "\n",
    "```\n",
    "Tumbling Windows (5 min):\n",
    "├────────────┼────────────┼────────────┤\n",
    "│  Window 1  │  Window 2  │  Window 3  │\n",
    "│ 10:00-10:05│ 10:05-10:10│ 10:10-10:15│\n",
    "\n",
    "Sliding Windows (5 min window, 2 min slide):\n",
    "├────────────┤\n",
    "│  Window 1  │\n",
    "│ 10:00-10:05│\n",
    "    ├────────────┤\n",
    "    │  Window 2  │\n",
    "    │ 10:02-10:07│\n",
    "        ├────────────┤\n",
    "        │  Window 3  │\n",
    "        │ 10:04-10:09│\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af787f3f",
   "metadata": {},
   "source": [
    "---\n",
    "## Takeaway\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **Stream processing** handles unbounded, continuously flowing data with low latency—ideal for real-time analytics, monitoring, and event-driven systems\n",
    "\n",
    "2. **Event-driven architecture** decouples producers and consumers through an event broker, enabling scalable and loosely coupled systems\n",
    "\n",
    "3. **Choose your framework wisely:**\n",
    "   - **Kafka Streams**: Lightweight, Kafka-native applications\n",
    "   - **Apache Flink**: Complex event processing, low-latency requirements\n",
    "   - **Spark Streaming**: When you need the broader Spark ecosystem\n",
    "\n",
    "4. **Event time vs processing time** is a critical distinction—use event time when correctness matters and handle late data with watermarks\n",
    "\n",
    "5. **Windowing** enables aggregations over infinite streams by grouping events into finite chunks\n",
    "\n",
    "### When to Use Stream Processing\n",
    "\n",
    "| Use Case | Why Stream Processing |\n",
    "|----------|----------------------|\n",
    "| Fraud detection | Immediate response needed |\n",
    "| Real-time dashboards | Live metrics and monitoring |\n",
    "| IoT sensor data | Continuous data from devices |\n",
    "| Log aggregation | Centralized, real-time log analysis |\n",
    "| Recommendation engines | React to user behavior instantly |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
