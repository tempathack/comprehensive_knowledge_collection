{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981a1f01",
   "metadata": {},
   "source": [
    "# Apache Kafka Fundamentals\n",
    "\n",
    "Apache Kafka is a distributed event streaming platform designed for high-throughput, fault-tolerant, and scalable real-time data pipelines. Originally developed at LinkedIn, Kafka has become the de facto standard for building streaming data architectures.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Kafka Architecture](#kafka-architecture)\n",
    "2. [Producers](#producers)\n",
    "3. [Consumers](#consumers)\n",
    "4. [Consumer Groups & Offset Management](#consumer-groups--offset-management)\n",
    "5. [Python Code Examples](#python-code-examples)\n",
    "6. [Exactly-Once Semantics](#exactly-once-semantics)\n",
    "7. [Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90303a11",
   "metadata": {},
   "source": [
    "## Kafka Architecture\n",
    "\n",
    "Kafka's architecture is built around three core components: **Brokers**, **Topics**, and **Partitions**.\n",
    "\n",
    "### Brokers\n",
    "\n",
    "A **broker** is a Kafka server that stores data and serves client requests. Key characteristics:\n",
    "\n",
    "- Each broker is identified by a unique integer ID\n",
    "- Brokers form a **cluster** for fault tolerance and scalability\n",
    "- One broker acts as the **controller** (manages partition leadership)\n",
    "- Brokers are stateless; they rely on ZooKeeper (or KRaft in newer versions) for metadata\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      Kafka Cluster                         │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │\n",
    "│  │  Broker 1   │  │  Broker 2   │  │  Broker 3   │         │\n",
    "│  │ (Controller)│  │             │  │             │         │\n",
    "│  │             │  │             │  │             │         │\n",
    "│  │ Partition 0 │  │ Partition 1 │  │ Partition 2 │         │\n",
    "│  │  (Leader)   │  │  (Leader)   │  │  (Leader)   │         │\n",
    "│  │ Partition 1 │  │ Partition 2 │  │ Partition 0 │         │\n",
    "│  │  (Replica)  │  │  (Replica)  │  │  (Replica)  │         │\n",
    "│  └─────────────┘  └─────────────┘  └─────────────┘         │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Topics\n",
    "\n",
    "A **topic** is a logical category or feed name to which records are published:\n",
    "\n",
    "- Topics are identified by unique names (e.g., `orders`, `user-events`)\n",
    "- Topics can have multiple **producers** and **consumers**\n",
    "- Data in topics is retained for a configurable period (default: 7 days)\n",
    "- Topics are **append-only** logs\n",
    "\n",
    "### Partitions\n",
    "\n",
    "Each topic is divided into **partitions** for parallelism and scalability:\n",
    "\n",
    "- Partitions are ordered, immutable sequences of records\n",
    "- Each record within a partition has a unique sequential **offset**\n",
    "- Partitions enable horizontal scaling (more partitions = more parallelism)\n",
    "- **Partition key** determines which partition a record goes to\n",
    "- Each partition has one **leader** and zero or more **replicas**\n",
    "\n",
    "```\n",
    "Topic: orders (3 partitions)\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ Partition 0: [0|1|2|3|4|5|6|7|8|9|...]  → Offset increases  │\n",
    "│ Partition 1: [0|1|2|3|4|5|...]          → Offset increases  │\n",
    "│ Partition 2: [0|1|2|3|4|5|6|7|...]      → Offset increases  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Replication Factor\n",
    "\n",
    "Kafka replicates partitions across multiple brokers for fault tolerance:\n",
    "\n",
    "- **Replication factor** = number of copies of each partition\n",
    "- One replica is the **leader** (handles all reads/writes)\n",
    "- Other replicas are **followers** (sync from leader)\n",
    "- **ISR (In-Sync Replicas)**: replicas that are fully caught up with the leader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9f24c",
   "metadata": {},
   "source": [
    "## Producers\n",
    "\n",
    "**Producers** are client applications that publish (write) records to Kafka topics.\n",
    "\n",
    "### Key Producer Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Partitioner** | Determines which partition receives each record |\n",
    "| **Batching** | Groups multiple records for efficient network usage |\n",
    "| **Compression** | Reduces message size (gzip, snappy, lz4, zstd) |\n",
    "| **Acknowledgments (acks)** | Controls durability guarantees |\n",
    "| **Retries** | Automatic retry on transient failures |\n",
    "\n",
    "### Acknowledgment Levels\n",
    "\n",
    "```\n",
    "acks=0: Fire and forget (fastest, least durable)\n",
    "        Producer ──────► Broker (no response waited)\n",
    "\n",
    "acks=1: Leader acknowledgment (balanced)\n",
    "        Producer ──────► Leader ──────► Producer (after leader writes)\n",
    "\n",
    "acks=all (-1): Full ISR acknowledgment (slowest, most durable)\n",
    "        Producer ──────► Leader ──────► Replicas ──────► Producer\n",
    "```\n",
    "\n",
    "### Partitioning Strategies\n",
    "\n",
    "1. **Key-based partitioning**: `hash(key) % num_partitions`\n",
    "   - Records with same key always go to same partition\n",
    "   - Guarantees ordering for a given key\n",
    "\n",
    "2. **Round-robin**: When no key is specified\n",
    "   - Distributes load evenly across partitions\n",
    "   - No ordering guarantees\n",
    "\n",
    "3. **Custom partitioner**: Implement your own logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe56ed1",
   "metadata": {},
   "source": [
    "## Consumers\n",
    "\n",
    "**Consumers** are client applications that subscribe to topics and process the records.\n",
    "\n",
    "### Consumer Pull Model\n",
    "\n",
    "Kafka uses a **pull-based** model where consumers request data from brokers:\n",
    "\n",
    "- Consumers control the rate of consumption\n",
    "- Enables backpressure handling\n",
    "- Consumers can rewind and re-read data\n",
    "\n",
    "### Consumer Configuration\n",
    "\n",
    "| Configuration | Description | Typical Value |\n",
    "|---------------|-------------|---------------|\n",
    "| `fetch.min.bytes` | Minimum data to fetch | 1 byte |\n",
    "| `fetch.max.wait.ms` | Max wait time if min bytes not available | 500ms |\n",
    "| `max.partition.fetch.bytes` | Max data per partition per fetch | 1MB |\n",
    "| `auto.offset.reset` | Where to start if no offset exists | `earliest` or `latest` |\n",
    "| `enable.auto.commit` | Automatically commit offsets | `true` |\n",
    "| `auto.commit.interval.ms` | Frequency of auto-commit | 5000ms |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685c42d",
   "metadata": {},
   "source": [
    "## Consumer Groups & Offset Management\n",
    "\n",
    "### Consumer Groups\n",
    "\n",
    "A **consumer group** is a set of consumers that cooperatively consume from topics:\n",
    "\n",
    "- Each partition is consumed by exactly **one consumer** in the group\n",
    "- Enables parallel processing and horizontal scaling\n",
    "- Adding consumers triggers **rebalancing**\n",
    "\n",
    "```\n",
    "Topic: orders (4 partitions)\n",
    "\n",
    "Consumer Group A (3 consumers):\n",
    "┌─────────────┐  ┌─────────────┐  ┌─────────────┐\n",
    "│ Consumer 1  │  │ Consumer 2  │  │ Consumer 3  │\n",
    "│ Partition 0 │  │ Partition 1 │  │ Partition 2 │\n",
    "│             │  │             │  │ Partition 3 │\n",
    "└─────────────┘  └─────────────┘  └─────────────┘\n",
    "\n",
    "Consumer Group B (2 consumers) - Independent:\n",
    "┌─────────────┐  ┌─────────────┐\n",
    "│ Consumer 1  │  │ Consumer 2  │\n",
    "│ Partition 0 │  │ Partition 2 │\n",
    "│ Partition 1 │  │ Partition 3 │\n",
    "└─────────────┘  └─────────────┘\n",
    "```\n",
    "\n",
    "### Offset Management\n",
    "\n",
    "**Offsets** track the position of each consumer in each partition:\n",
    "\n",
    "- Stored in a special topic: `__consumer_offsets`\n",
    "- **Committed offset**: Last processed position acknowledged to Kafka\n",
    "- **Current position**: Where the consumer is reading\n",
    "\n",
    "```\n",
    "Partition: [0|1|2|3|4|5|6|7|8|9|10|11|12|...]\n",
    "                    ▲         ▲\n",
    "                    │         │\n",
    "             Committed    Current\n",
    "              Offset      Position\n",
    "```\n",
    "\n",
    "### Offset Commit Strategies\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **Auto-commit** | Simple, no code needed | May lose/duplicate messages on failure |\n",
    "| **Sync commit** | Precise control | Blocks processing |\n",
    "| **Async commit** | Non-blocking | May have gaps on failure |\n",
    "| **Commit specific offset** | Fine-grained control | Most complex |\n",
    "\n",
    "### Rebalancing\n",
    "\n",
    "Rebalancing occurs when:\n",
    "- Consumer joins or leaves the group\n",
    "- Consumer crashes or times out\n",
    "- Partitions are added to subscribed topics\n",
    "\n",
    "**Partition Assignment Strategies:**\n",
    "- `RangeAssignor`: Assigns ranges of partitions to consumers\n",
    "- `RoundRobinAssignor`: Round-robin distribution\n",
    "- `StickyAssignor`: Minimizes partition movement during rebalance\n",
    "- `CooperativeStickyAssignor`: Incremental rebalancing (less disruption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf0eca",
   "metadata": {},
   "source": [
    "## Python Code Examples\n",
    "\n",
    "Using the `kafka-python` library for interacting with Apache Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "# pip install kafka-python\n",
    "\n",
    "from kafka import KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import KafkaError\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d26047",
   "metadata": {},
   "source": [
    "### Topic Administration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ad473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic(topic_name: str, num_partitions: int = 3, replication_factor: int = 1):\n",
    "    \"\"\"\n",
    "    Create a Kafka topic with specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        topic_name: Name of the topic to create\n",
    "        num_partitions: Number of partitions for parallel processing\n",
    "        replication_factor: Number of replicas for fault tolerance\n",
    "    \"\"\"\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        client_id='topic-admin'\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=topic_name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor,\n",
    "        topic_configs={\n",
    "            'retention.ms': '604800000',  # 7 days\n",
    "            'cleanup.policy': 'delete'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=[topic], validate_only=False)\n",
    "        print(f\"Topic '{topic_name}' created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating topic: {e}\")\n",
    "    finally:\n",
    "        admin_client.close()\n",
    "\n",
    "# Example usage\n",
    "# create_topic('orders', num_partitions=6, replication_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb062e9",
   "metadata": {},
   "source": [
    "### Basic Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_producer():\n",
    "    \"\"\"\n",
    "    Create a Kafka producer with JSON serialization.\n",
    "    \n",
    "    Returns:\n",
    "        KafkaProducer: Configured producer instance\n",
    "    \"\"\"\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        \n",
    "        # Serialization\n",
    "        key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        \n",
    "        # Durability\n",
    "        acks='all',  # Wait for all ISR replicas\n",
    "        \n",
    "        # Reliability\n",
    "        retries=3,\n",
    "        retry_backoff_ms=100,\n",
    "        \n",
    "        # Performance\n",
    "        batch_size=16384,  # 16KB batch\n",
    "        linger_ms=10,  # Wait up to 10ms to batch\n",
    "        compression_type='gzip',\n",
    "        \n",
    "        # Idempotence for exactly-once\n",
    "        enable_idempotence=True\n",
    "    )\n",
    "    return producer\n",
    "\n",
    "\n",
    "def send_message(producer: KafkaProducer, topic: str, key: str, value: dict):\n",
    "    \"\"\"\n",
    "    Send a message to Kafka with callbacks.\n",
    "    \n",
    "    Args:\n",
    "        producer: KafkaProducer instance\n",
    "        topic: Target topic name\n",
    "        key: Message key (determines partition)\n",
    "        value: Message payload as dictionary\n",
    "    \"\"\"\n",
    "    def on_success(metadata):\n",
    "        print(f\"Sent to {metadata.topic}:{metadata.partition} @ offset {metadata.offset}\")\n",
    "    \n",
    "    def on_error(exception):\n",
    "        print(f\"Error sending message: {exception}\")\n",
    "    \n",
    "    future = producer.send(\n",
    "        topic,\n",
    "        key=key,\n",
    "        value=value,\n",
    "        headers=[('source', b'python-client')]  # Optional headers\n",
    "    )\n",
    "    \n",
    "    # Add callbacks\n",
    "    future.add_callback(on_success)\n",
    "    future.add_errback(on_error)\n",
    "    \n",
    "    return future\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# producer = create_producer()\n",
    "# \n",
    "# orders = [\n",
    "#     {'order_id': 'ORD001', 'customer': 'Alice', 'amount': 150.00},\n",
    "#     {'order_id': 'ORD002', 'customer': 'Bob', 'amount': 275.50},\n",
    "#     {'order_id': 'ORD003', 'customer': 'Charlie', 'amount': 89.99},\n",
    "# ]\n",
    "# \n",
    "# for order in orders:\n",
    "#     send_message(producer, 'orders', order['customer'], order)\n",
    "# \n",
    "# producer.flush()  # Ensure all messages are sent\n",
    "# producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218be6f",
   "metadata": {},
   "source": [
    "### Basic Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumer(group_id: str, topics: list):\n",
    "    \"\"\"\n",
    "    Create a Kafka consumer with JSON deserialization.\n",
    "    \n",
    "    Args:\n",
    "        group_id: Consumer group identifier\n",
    "        topics: List of topics to subscribe to\n",
    "        \n",
    "    Returns:\n",
    "        KafkaConsumer: Configured consumer instance\n",
    "    \"\"\"\n",
    "    consumer = KafkaConsumer(\n",
    "        *topics,\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        group_id=group_id,\n",
    "        \n",
    "        # Deserialization\n",
    "        key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "        value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "        \n",
    "        # Offset behavior\n",
    "        auto_offset_reset='earliest',  # Start from beginning if no offset\n",
    "        enable_auto_commit=False,  # Manual commit for control\n",
    "        \n",
    "        # Performance\n",
    "        fetch_min_bytes=1,\n",
    "        fetch_max_wait_ms=500,\n",
    "        max_poll_records=500,\n",
    "        \n",
    "        # Session management\n",
    "        session_timeout_ms=30000,\n",
    "        heartbeat_interval_ms=10000\n",
    "    )\n",
    "    return consumer\n",
    "\n",
    "\n",
    "def consume_messages(consumer: KafkaConsumer, process_fn: callable):\n",
    "    \"\"\"\n",
    "    Consume and process messages with manual offset commit.\n",
    "    \n",
    "    Args:\n",
    "        consumer: KafkaConsumer instance\n",
    "        process_fn: Function to process each message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            # Poll for messages (returns dict of TopicPartition -> list of records)\n",
    "            records = consumer.poll(timeout_ms=1000)\n",
    "            \n",
    "            for topic_partition, messages in records.items():\n",
    "                for message in messages:\n",
    "                    print(f\"Received: {message.topic}:{message.partition} @ {message.offset}\")\n",
    "                    print(f\"  Key: {message.key}\")\n",
    "                    print(f\"  Value: {message.value}\")\n",
    "                    print(f\"  Timestamp: {message.timestamp}\")\n",
    "                    \n",
    "                    # Process the message\n",
    "                    process_fn(message)\n",
    "            \n",
    "            # Commit offsets after processing batch\n",
    "            if records:\n",
    "                consumer.commit()\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Shutting down consumer...\")\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# def process_order(message):\n",
    "#     order = message.value\n",
    "#     print(f\"Processing order {order['order_id']} for ${order['amount']}\")\n",
    "#\n",
    "# consumer = create_consumer('order-processors', ['orders'])\n",
    "# consume_messages(consumer, process_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7f094",
   "metadata": {},
   "source": [
    "### Manual Offset Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76487114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_with_manual_offset_control(consumer: KafkaConsumer):\n",
    "    \"\"\"\n",
    "    Demonstrates fine-grained offset management.\n",
    "    \n",
    "    Args:\n",
    "        consumer: KafkaConsumer instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            records = consumer.poll(timeout_ms=1000)\n",
    "            \n",
    "            for topic_partition, messages in records.items():\n",
    "                for message in messages:\n",
    "                    try:\n",
    "                        # Process message\n",
    "                        process_message(message)\n",
    "                        \n",
    "                        # Commit specific offset (next offset to read)\n",
    "                        consumer.commit({\n",
    "                            topic_partition: message.offset + 1\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing message: {e}\")\n",
    "                        # Don't commit - message will be reprocessed\n",
    "                        \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "\n",
    "def seek_to_specific_offset(consumer: KafkaConsumer, topic: str, partition: int, offset: int):\n",
    "    \"\"\"\n",
    "    Seek to a specific offset in a partition.\n",
    "    Useful for replaying messages or skipping ahead.\n",
    "    \n",
    "    Args:\n",
    "        consumer: KafkaConsumer instance\n",
    "        topic: Topic name\n",
    "        partition: Partition number\n",
    "        offset: Target offset\n",
    "    \"\"\"\n",
    "    tp = TopicPartition(topic, partition)\n",
    "    \n",
    "    # Assign specific partition (bypasses consumer group)\n",
    "    consumer.assign([tp])\n",
    "    \n",
    "    # Seek to specific offset\n",
    "    consumer.seek(tp, offset)\n",
    "    \n",
    "    print(f\"Seeked to {topic}:{partition} @ offset {offset}\")\n",
    "\n",
    "\n",
    "def seek_to_timestamp(consumer: KafkaConsumer, topic: str, partition: int, timestamp_ms: int):\n",
    "    \"\"\"\n",
    "    Seek to a specific timestamp in a partition.\n",
    "    \n",
    "    Args:\n",
    "        consumer: KafkaConsumer instance\n",
    "        topic: Topic name\n",
    "        partition: Partition number\n",
    "        timestamp_ms: Target timestamp in milliseconds\n",
    "    \"\"\"\n",
    "    tp = TopicPartition(topic, partition)\n",
    "    consumer.assign([tp])\n",
    "    \n",
    "    # Get offset for timestamp\n",
    "    offsets = consumer.offsets_for_times({tp: timestamp_ms})\n",
    "    \n",
    "    if offsets[tp]:\n",
    "        consumer.seek(tp, offsets[tp].offset)\n",
    "        print(f\"Seeked to offset {offsets[tp].offset} for timestamp {timestamp_ms}\")\n",
    "    else:\n",
    "        print(f\"No offset found for timestamp {timestamp_ms}\")\n",
    "\n",
    "\n",
    "def process_message(message):\n",
    "    \"\"\"Placeholder for message processing logic.\"\"\"\n",
    "    print(f\"Processing: {message.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e1ebc",
   "metadata": {},
   "source": [
    "### Consumer Group Pattern with Graceful Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11430288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import threading\n",
    "from typing import Callable, Optional\n",
    "\n",
    "\n",
    "class KafkaConsumerWorker:\n",
    "    \"\"\"\n",
    "    A robust Kafka consumer with graceful shutdown support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        topics: list,\n",
    "        group_id: str,\n",
    "        bootstrap_servers: list = ['localhost:9092'],\n",
    "        process_fn: Optional[Callable] = None\n",
    "    ):\n",
    "        self.topics = topics\n",
    "        self.group_id = group_id\n",
    "        self.bootstrap_servers = bootstrap_servers\n",
    "        self.process_fn = process_fn or self._default_processor\n",
    "        self._shutdown = threading.Event()\n",
    "        self._consumer: Optional[KafkaConsumer] = None\n",
    "        \n",
    "    def _default_processor(self, message):\n",
    "        \"\"\"Default message processor.\"\"\"\n",
    "        print(f\"Message: {message.value}\")\n",
    "    \n",
    "    def _create_consumer(self) -> KafkaConsumer:\n",
    "        \"\"\"Create and configure the consumer.\"\"\"\n",
    "        return KafkaConsumer(\n",
    "            *self.topics,\n",
    "            bootstrap_servers=self.bootstrap_servers,\n",
    "            group_id=self.group_id,\n",
    "            key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "            value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=False,\n",
    "            max_poll_records=100,\n",
    "            session_timeout_ms=30000,\n",
    "            heartbeat_interval_ms=10000\n",
    "        )\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start consuming messages.\"\"\"\n",
    "        self._consumer = self._create_consumer()\n",
    "        print(f\"Consumer started for topics: {self.topics}\")\n",
    "        \n",
    "        try:\n",
    "            while not self._shutdown.is_set():\n",
    "                records = self._consumer.poll(timeout_ms=1000)\n",
    "                \n",
    "                for topic_partition, messages in records.items():\n",
    "                    for message in messages:\n",
    "                        if self._shutdown.is_set():\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            self.process_fn(message)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing message: {e}\")\n",
    "                            # Could implement dead letter queue here\n",
    "                \n",
    "                if records:\n",
    "                    self._consumer.commit()\n",
    "                    \n",
    "        finally:\n",
    "            self._cleanup()\n",
    "    \n",
    "    def shutdown(self):\n",
    "        \"\"\"Signal graceful shutdown.\"\"\"\n",
    "        print(\"Shutdown signal received...\")\n",
    "        self._shutdown.set()\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self._consumer:\n",
    "            try:\n",
    "                self._consumer.commit()  # Final commit\n",
    "            except Exception:\n",
    "                pass\n",
    "            self._consumer.close()\n",
    "            print(\"Consumer closed gracefully\")\n",
    "\n",
    "\n",
    "# Example usage with signal handling\n",
    "# def order_processor(message):\n",
    "#     order = message.value\n",
    "#     print(f\"Processing order: {order}\")\n",
    "#     time.sleep(0.1)  # Simulate processing\n",
    "#\n",
    "# worker = KafkaConsumerWorker(\n",
    "#     topics=['orders'],\n",
    "#     group_id='order-service',\n",
    "#     process_fn=order_processor\n",
    "# )\n",
    "#\n",
    "# # Handle SIGINT/SIGTERM\n",
    "# signal.signal(signal.SIGINT, lambda sig, frame: worker.shutdown())\n",
    "# signal.signal(signal.SIGTERM, lambda sig, frame: worker.shutdown())\n",
    "#\n",
    "# worker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2211bfd",
   "metadata": {},
   "source": [
    "## Exactly-Once Semantics\n",
    "\n",
    "Kafka provides **exactly-once semantics (EOS)** to ensure each record is processed exactly once, even in the presence of failures.\n",
    "\n",
    "### Delivery Guarantees\n",
    "\n",
    "| Guarantee | Description | Potential Issues |\n",
    "|-----------|-------------|------------------|\n",
    "| **At-most-once** | Messages may be lost, never duplicated | Data loss |\n",
    "| **At-least-once** | Messages never lost, may be duplicated | Duplicate processing |\n",
    "| **Exactly-once** | Messages processed exactly once | Complex implementation |\n",
    "\n",
    "### Components of Exactly-Once\n",
    "\n",
    "1. **Idempotent Producer** (`enable.idempotence=true`)\n",
    "   - Producer assigns sequence numbers to messages\n",
    "   - Broker detects and deduplicates retries\n",
    "   - Guarantees exactly-once delivery to a single partition\n",
    "\n",
    "2. **Transactional Producer**\n",
    "   - Atomic writes across multiple partitions\n",
    "   - All-or-nothing semantics\n",
    "   - Requires `transactional.id`\n",
    "\n",
    "3. **Transactional Consumer** (`isolation.level=read_committed`)\n",
    "   - Only reads committed messages\n",
    "   - Skips aborted transactions\n",
    "\n",
    "### Transaction Flow\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Transaction Lifecycle                    │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  1. initTransactions()    → Register transactional.id      │\n",
    "│  2. beginTransaction()    → Start atomic unit              │\n",
    "│  3. send()                → Buffer messages                │\n",
    "│  4. sendOffsetsToTxn()    → Include consumer offsets       │\n",
    "│  5. commitTransaction()   → Atomically commit all          │\n",
    "│     OR abortTransaction() → Rollback everything            │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transactional_producer(transactional_id: str):\n",
    "    \"\"\"\n",
    "    Create a transactional producer for exactly-once semantics.\n",
    "    \n",
    "    Args:\n",
    "        transactional_id: Unique identifier for this producer's transactions\n",
    "        \n",
    "    Returns:\n",
    "        KafkaProducer: Transactional producer instance\n",
    "    \"\"\"\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        \n",
    "        # Exactly-once configuration\n",
    "        enable_idempotence=True,\n",
    "        transactional_id=transactional_id,\n",
    "        \n",
    "        # Required for transactions\n",
    "        acks='all',\n",
    "        retries=2147483647,  # Max retries\n",
    "        max_in_flight_requests_per_connection=5\n",
    "    )\n",
    "    \n",
    "    # Initialize transactions (must be called once)\n",
    "    producer.init_transactions()\n",
    "    \n",
    "    return producer\n",
    "\n",
    "\n",
    "def send_with_transaction(producer: KafkaProducer, messages: list):\n",
    "    \"\"\"\n",
    "    Send multiple messages atomically in a transaction.\n",
    "    \n",
    "    Args:\n",
    "        producer: Transactional KafkaProducer\n",
    "        messages: List of (topic, key, value) tuples\n",
    "    \"\"\"\n",
    "    try:\n",
    "        producer.begin_transaction()\n",
    "        \n",
    "        for topic, key, value in messages:\n",
    "            producer.send(topic, key=key, value=value)\n",
    "        \n",
    "        producer.commit_transaction()\n",
    "        print(f\"Transaction committed: {len(messages)} messages\")\n",
    "        \n",
    "    except KafkaError as e:\n",
    "        print(f\"Transaction failed: {e}\")\n",
    "        producer.abort_transaction()\n",
    "\n",
    "\n",
    "# Example: Atomic multi-topic write\n",
    "# producer = create_transactional_producer('order-processor-1')\n",
    "#\n",
    "# messages = [\n",
    "#     ('orders', 'order-123', {'order_id': '123', 'status': 'completed'}),\n",
    "#     ('inventory', 'sku-456', {'sku': '456', 'quantity': -1}),\n",
    "#     ('notifications', 'user-789', {'user_id': '789', 'message': 'Order shipped'})\n",
    "# ]\n",
    "#\n",
    "# send_with_transaction(producer, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac46fed",
   "metadata": {},
   "source": [
    "### Read-Process-Write Pattern (Consume-Transform-Produce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a30d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import OffsetAndMetadata\n",
    "\n",
    "\n",
    "def exactly_once_consume_transform_produce(\n",
    "    consumer: KafkaConsumer,\n",
    "    producer: KafkaProducer,\n",
    "    transform_fn: Callable,\n",
    "    output_topic: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements exactly-once consume-transform-produce pattern.\n",
    "    \n",
    "    This pattern ensures:\n",
    "    - Input is consumed exactly once\n",
    "    - Output is produced exactly once\n",
    "    - Consumer offsets and output are committed atomically\n",
    "    \n",
    "    Args:\n",
    "        consumer: KafkaConsumer (with enable_auto_commit=False)\n",
    "        producer: Transactional KafkaProducer\n",
    "        transform_fn: Function to transform input message to output\n",
    "        output_topic: Topic to write transformed messages\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            records = consumer.poll(timeout_ms=1000)\n",
    "            \n",
    "            if not records:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Start transaction\n",
    "                producer.begin_transaction()\n",
    "                \n",
    "                offsets_to_commit = {}\n",
    "                \n",
    "                for topic_partition, messages in records.items():\n",
    "                    for message in messages:\n",
    "                        # Transform and produce\n",
    "                        output_value = transform_fn(message.value)\n",
    "                        producer.send(\n",
    "                            output_topic,\n",
    "                            key=message.key,\n",
    "                            value=output_value\n",
    "                        )\n",
    "                        \n",
    "                        # Track offsets\n",
    "                        offsets_to_commit[topic_partition] = OffsetAndMetadata(\n",
    "                            message.offset + 1, None\n",
    "                        )\n",
    "                \n",
    "                # Commit offsets as part of transaction\n",
    "                producer.send_offsets_to_transaction(\n",
    "                    offsets_to_commit,\n",
    "                    consumer.group_id\n",
    "                )\n",
    "                \n",
    "                # Commit transaction atomically\n",
    "                producer.commit_transaction()\n",
    "                print(f\"Transaction committed: {len(offsets_to_commit)} partitions\")\n",
    "                \n",
    "            except KafkaError as e:\n",
    "                print(f\"Transaction failed, aborting: {e}\")\n",
    "                producer.abort_transaction()\n",
    "                # Consumer will re-read uncommitted messages\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Shutting down...\")\n",
    "    finally:\n",
    "        consumer.close()\n",
    "        producer.close()\n",
    "\n",
    "\n",
    "# Example: Order enrichment pipeline\n",
    "# def enrich_order(order: dict) -> dict:\n",
    "#     \"\"\"Add processing timestamp and status.\"\"\"\n",
    "#     return {\n",
    "#         **order,\n",
    "#         'processed_at': time.time(),\n",
    "#         'status': 'enriched'\n",
    "#     }\n",
    "#\n",
    "# consumer = KafkaConsumer(\n",
    "#     'raw-orders',\n",
    "#     bootstrap_servers=['localhost:9092'],\n",
    "#     group_id='order-enricher',\n",
    "#     enable_auto_commit=False,\n",
    "#     isolation_level='read_committed',  # Only read committed messages\n",
    "#     value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    "# )\n",
    "#\n",
    "# producer = create_transactional_producer('order-enricher-1')\n",
    "#\n",
    "# exactly_once_consume_transform_produce(\n",
    "#     consumer, producer, enrich_order, 'enriched-orders'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7329319",
   "metadata": {},
   "source": [
    "### Idempotent Consumer Pattern\n",
    "\n",
    "When exactly-once semantics aren't available end-to-end, make consumers idempotent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "\n",
    "class IdempotentOrderProcessor:\n",
    "    \"\"\"\n",
    "    Idempotent consumer that tracks processed message IDs.\n",
    "    \n",
    "    In production, use a persistent store (Redis, database) instead of in-memory set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In production: use Redis SET or database table\n",
    "        self._processed_ids: Set[str] = set()\n",
    "    \n",
    "    def _get_message_id(self, message) -> str:\n",
    "        \"\"\"Extract unique message identifier.\"\"\"\n",
    "        # Option 1: Use message offset (partition-specific)\n",
    "        # return f\"{message.topic}-{message.partition}-{message.offset}\"\n",
    "        \n",
    "        # Option 2: Use business key from message value\n",
    "        return message.value.get('order_id', str(message.offset))\n",
    "    \n",
    "    def _is_processed(self, message_id: str) -> bool:\n",
    "        \"\"\"Check if message was already processed.\"\"\"\n",
    "        return message_id in self._processed_ids\n",
    "    \n",
    "    def _mark_processed(self, message_id: str):\n",
    "        \"\"\"Mark message as processed.\"\"\"\n",
    "        self._processed_ids.add(message_id)\n",
    "        # In production: SET with TTL or database insert\n",
    "    \n",
    "    def process(self, message) -> bool:\n",
    "        \"\"\"\n",
    "        Process message idempotently.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if processed, False if duplicate\n",
    "        \"\"\"\n",
    "        message_id = self._get_message_id(message)\n",
    "        \n",
    "        if self._is_processed(message_id):\n",
    "            print(f\"Skipping duplicate: {message_id}\")\n",
    "            return False\n",
    "        \n",
    "        # Process the message\n",
    "        order = message.value\n",
    "        print(f\"Processing order {order['order_id']}: ${order.get('amount', 0)}\")\n",
    "        \n",
    "        # Simulate processing (database write, API call, etc.)\n",
    "        # If this fails, message will be reprocessed but idempotency prevents duplicates\n",
    "        \n",
    "        self._mark_processed(message_id)\n",
    "        return True\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# processor = IdempotentOrderProcessor()\n",
    "# consumer = create_consumer('order-handlers', ['orders'])\n",
    "#\n",
    "# for message in consumer:\n",
    "#     processor.process(message)\n",
    "#     consumer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0e136",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Key Points |\n",
    "|---------|------------|\n",
    "| **Architecture** | Brokers form clusters; topics are split into partitions; partitions replicated across brokers |\n",
    "| **Partitioning** | Key-based partitioning guarantees ordering per key; enables horizontal scaling |\n",
    "| **Consumer Groups** | Partition-to-consumer mapping; enables parallel processing; triggers rebalancing on changes |\n",
    "| **Offset Management** | Stored in `__consumer_offsets`; commit strategies affect delivery guarantees |\n",
    "| **Exactly-Once** | Idempotent producers + transactions + read_committed consumers |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Producer Configuration**\n",
    "   - Use `acks=all` for durability\n",
    "   - Enable idempotence for exactly-once delivery\n",
    "   - Configure appropriate batch size and linger time\n",
    "\n",
    "2. **Consumer Configuration**\n",
    "   - Disable auto-commit for precise control\n",
    "   - Use `read_committed` isolation for transactional reads\n",
    "   - Implement graceful shutdown to avoid rebalancing delays\n",
    "\n",
    "3. **Partition Design**\n",
    "   - Choose partition key based on access patterns\n",
    "   - Plan partition count for peak throughput (hard to reduce later)\n",
    "   - Consider consumer group size when planning partitions\n",
    "\n",
    "4. **Error Handling**\n",
    "   - Implement idempotent consumers for at-least-once delivery\n",
    "   - Use dead letter queues for poison messages\n",
    "   - Monitor consumer lag to detect processing issues\n",
    "\n",
    "5. **Operational Excellence**\n",
    "   - Set appropriate retention policies\n",
    "   - Monitor ISR shrinkage as indicator of broker health\n",
    "   - Use Cooperative Sticky assignor to minimize rebalance impact\n",
    "\n",
    "### When to Use Kafka\n",
    "\n",
    "✅ **Good Fit:**\n",
    "- High-throughput event streaming\n",
    "- Real-time data pipelines\n",
    "- Event sourcing and CQRS\n",
    "- Log aggregation\n",
    "- Microservices communication\n",
    "\n",
    "❌ **Consider Alternatives:**\n",
    "- Simple request-response (use HTTP/gRPC)\n",
    "- Small-scale messaging (use Redis/RabbitMQ)\n",
    "- Complex routing logic (use RabbitMQ)\n",
    "- Guaranteed ordering across topics (use database)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
