{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0190fc0d",
   "metadata": {},
   "source": [
    "# Orchestration — Overview\n",
    "\n",
    "## Purpose\n",
    "Data pipeline orchestration is the automated coordination, scheduling, and management of complex data workflows. It ensures that data tasks execute in the correct order, handle failures gracefully, and provide visibility into pipeline health.\n",
    "\n",
    "## Key Questions\n",
    "- What is orchestration and why is it essential for data engineering?\n",
    "- How do popular orchestration tools (Airflow, Dagster, Prefect) compare?\n",
    "- What are DAGs and how do task dependencies work?\n",
    "- How do we schedule, monitor, and maintain data pipelines?\n",
    "- What are best practices for production-grade orchestration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26525527",
   "metadata": {},
   "source": [
    "---\n",
    "## What is Orchestration and Why It Matters\n",
    "\n",
    "### Definition\n",
    "**Orchestration** is the automated arrangement, coordination, and management of complex data workflows. It acts as the \"conductor\" that ensures all components of a data pipeline work together harmoniously.\n",
    "\n",
    "### Why Orchestration Matters\n",
    "\n",
    "| Challenge | Without Orchestration | With Orchestration |\n",
    "|-----------|----------------------|--------------------|\n",
    "| **Dependency Management** | Manual tracking of task order | Automatic dependency resolution |\n",
    "| **Failure Handling** | Silent failures, data corruption | Retries, alerts, and rollback |\n",
    "| **Scheduling** | Cron jobs scattered across systems | Centralized scheduling with UI |\n",
    "| **Visibility** | No insight into pipeline status | Real-time monitoring dashboards |\n",
    "| **Scalability** | Hard to manage growing pipelines | Designed for complex workflows |\n",
    "\n",
    "### Core Responsibilities of an Orchestrator\n",
    "\n",
    "1. **Task Scheduling** — Execute tasks at specified times or intervals\n",
    "2. **Dependency Management** — Ensure tasks run in the correct order\n",
    "3. **Resource Allocation** — Manage compute resources efficiently\n",
    "4. **Error Handling** — Retry failed tasks, send alerts, handle exceptions\n",
    "5. **Monitoring & Logging** — Provide visibility into pipeline execution\n",
    "6. **Backfilling** — Re-run historical data processing when logic changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800068b",
   "metadata": {},
   "source": [
    "---\n",
    "## DAGs and Task Dependencies\n",
    "\n",
    "### What is a DAG?\n",
    "\n",
    "A **Directed Acyclic Graph (DAG)** is the fundamental structure for defining workflows:\n",
    "\n",
    "- **Directed** — Tasks have a clear direction (upstream → downstream)\n",
    "- **Acyclic** — No circular dependencies (prevents infinite loops)\n",
    "- **Graph** — Tasks (nodes) connected by dependencies (edges)\n",
    "\n",
    "```\n",
    "    [Extract A]    [Extract B]\n",
    "         \\            /\n",
    "          \\          /\n",
    "           [Transform]\n",
    "               |\n",
    "           [Validate]\n",
    "               |\n",
    "            [Load]\n",
    "```\n",
    "\n",
    "### Dependency Types\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|----------|\n",
    "| **Sequential** | Task B waits for Task A | Extract → Transform → Load |\n",
    "| **Parallel** | Tasks run concurrently | Extract from multiple sources |\n",
    "| **Fan-out** | One task triggers many | Split data for parallel processing |\n",
    "| **Fan-in** | Many tasks feed into one | Aggregate results from parallel tasks |\n",
    "| **Conditional** | Task runs based on condition | Run cleanup only if transform fails |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple DAG structure in Python (conceptual)\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Callable\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"Represents a single task in a DAG.\"\"\"\n",
    "    name: str\n",
    "    execute: Callable\n",
    "    dependencies: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.dependencies = self.dependencies or []\n",
    "\n",
    "# Define tasks\n",
    "def extract_data():\n",
    "    print(\"Extracting data from source...\")\n",
    "    return {\"records\": 1000}\n",
    "\n",
    "def transform_data():\n",
    "    print(\"Transforming data...\")\n",
    "    return {\"transformed\": True}\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading data to warehouse...\")\n",
    "    return {\"loaded\": True}\n",
    "\n",
    "# Create DAG structure\n",
    "dag = [\n",
    "    Task(\"extract\", extract_data, dependencies=[]),\n",
    "    Task(\"transform\", transform_data, dependencies=[\"extract\"]),\n",
    "    Task(\"load\", load_data, dependencies=[\"transform\"]),\n",
    "]\n",
    "\n",
    "print(\"DAG Tasks:\")\n",
    "for task in dag:\n",
    "    deps = task.dependencies if task.dependencies else \"None\"\n",
    "    print(f\"  {task.name} -> depends on: {deps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27a92e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Apache Airflow, Dagster, Prefect Comparison\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Feature | Apache Airflow | Dagster | Prefect |\n",
    "|---------|---------------|---------|----------|\n",
    "| **Released** | 2015 (Airbnb) | 2019 | 2018 |\n",
    "| **Paradigm** | Task-centric | Asset-centric | Task-centric |\n",
    "| **Configuration** | Python DSL | Python DSL | Python DSL |\n",
    "| **UI** | Web-based | Web-based | Web-based (Cloud) |\n",
    "| **Scaling** | Celery, Kubernetes | Kubernetes, Dagster+ | Kubernetes, Prefect Cloud |\n",
    "| **Learning Curve** | Moderate | Steeper | Gentle |\n",
    "\n",
    "### Apache Airflow\n",
    "\n",
    "**Strengths:**\n",
    "- Industry standard with massive community\n",
    "- Extensive operator ecosystem (AWS, GCP, databases)\n",
    "- Battle-tested at scale (Airbnb, Lyft, Spotify)\n",
    "- Strong scheduling capabilities\n",
    "\n",
    "**Weaknesses:**\n",
    "- Complex local development setup\n",
    "- DAGs parsed on scheduler (can be slow)\n",
    "- Testing workflows is challenging\n",
    "\n",
    "### Dagster\n",
    "\n",
    "**Strengths:**\n",
    "- Asset-centric (data lineage built-in)\n",
    "- Excellent local development experience\n",
    "- Strong typing and data contracts\n",
    "- First-class testing support\n",
    "\n",
    "**Weaknesses:**\n",
    "- Smaller community than Airflow\n",
    "- Different mental model (assets vs tasks)\n",
    "- Fewer third-party integrations\n",
    "\n",
    "### Prefect\n",
    "\n",
    "**Strengths:**\n",
    "- Pythonic API (feels natural)\n",
    "- Dynamic workflows at runtime\n",
    "- Easy migration from scripts\n",
    "- Excellent hybrid execution model\n",
    "\n",
    "**Weaknesses:**\n",
    "- Prefect Cloud for full features (cost)\n",
    "- Less mature than Airflow\n",
    "- Changing API between versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow Example (conceptual - requires Airflow installation)\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def extract():\n",
    "    return \"data extracted\"\n",
    "\n",
    "def transform(ti):\n",
    "    data = ti.xcom_pull(task_ids='extract_task')\n",
    "    return f\"transformed: {data}\"\n",
    "\n",
    "def load(ti):\n",
    "    data = ti.xcom_pull(task_ids='transform_task')\n",
    "    print(f\"Loading: {data}\")\n",
    "\n",
    "with DAG(\n",
    "    'etl_pipeline',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    \n",
    "    extract_task = PythonOperator(\n",
    "        task_id='extract_task',\n",
    "        python_callable=extract\n",
    "    )\n",
    "    \n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_task',\n",
    "        python_callable=transform\n",
    "    )\n",
    "    \n",
    "    load_task = PythonOperator(\n",
    "        task_id='load_task',\n",
    "        python_callable=load\n",
    "    )\n",
    "    \n",
    "    extract_task >> transform_task >> load_task\n",
    "\"\"\"\n",
    "print(\"Airflow DAG: Uses operators and >> syntax for dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dagster Example (conceptual - requires Dagster installation)\n",
    "\"\"\"\n",
    "from dagster import asset, Definitions\n",
    "\n",
    "@asset\n",
    "def raw_data():\n",
    "    '''Extract raw data from source.'''\n",
    "    return [{\"id\": 1, \"value\": 100}, {\"id\": 2, \"value\": 200}]\n",
    "\n",
    "@asset\n",
    "def cleaned_data(raw_data):\n",
    "    '''Transform and clean the raw data.'''\n",
    "    return [{\"id\": r[\"id\"], \"value\": r[\"value\"] * 2} for r in raw_data]\n",
    "\n",
    "@asset\n",
    "def aggregated_data(cleaned_data):\n",
    "    '''Aggregate cleaned data.'''\n",
    "    total = sum(r[\"value\"] for r in cleaned_data)\n",
    "    return {\"total_value\": total, \"count\": len(cleaned_data)}\n",
    "\n",
    "defs = Definitions(assets=[raw_data, cleaned_data, aggregated_data])\n",
    "\"\"\"\n",
    "print(\"Dagster: Asset-centric approach with automatic dependency inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefect Example (conceptual - requires Prefect installation)\n",
    "\"\"\"\n",
    "from prefect import flow, task\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=60)\n",
    "def extract():\n",
    "    return {\"records\": [1, 2, 3, 4, 5]}\n",
    "\n",
    "@task\n",
    "def transform(data):\n",
    "    return {\"records\": [r * 2 for r in data[\"records\"]]}\n",
    "\n",
    "@task\n",
    "def load(data):\n",
    "    print(f\"Loading {len(data['records'])} records\")\n",
    "    return True\n",
    "\n",
    "@flow(name=\"ETL Pipeline\")\n",
    "def etl_pipeline():\n",
    "    raw = extract()\n",
    "    transformed = transform(raw)\n",
    "    load(transformed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_pipeline()\n",
    "\"\"\"\n",
    "print(\"Prefect: Pythonic decorators with built-in retry logic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a988e0f",
   "metadata": {},
   "source": [
    "---\n",
    "## Scheduling and Monitoring\n",
    "\n",
    "### Scheduling Strategies\n",
    "\n",
    "| Strategy | Use Case | Example |\n",
    "|----------|----------|----------|\n",
    "| **Cron-based** | Regular intervals | `0 2 * * *` (daily at 2 AM) |\n",
    "| **Event-driven** | React to data arrival | S3 file upload triggers pipeline |\n",
    "| **Sensor-based** | Wait for condition | Check if upstream table is updated |\n",
    "| **Manual** | Ad-hoc runs | Backfill historical data |\n",
    "| **Data-aware** | Based on data freshness | Run when source data changes |\n",
    "\n",
    "### Common Cron Expressions\n",
    "\n",
    "```\n",
    "┌───────────── minute (0-59)\n",
    "│ ┌───────────── hour (0-23)\n",
    "│ │ ┌───────────── day of month (1-31)\n",
    "│ │ │ ┌───────────── month (1-12)\n",
    "│ │ │ │ ┌───────────── day of week (0-6)\n",
    "│ │ │ │ │\n",
    "* * * * *\n",
    "\n",
    "@hourly   = 0 * * * *      (every hour)\n",
    "@daily    = 0 0 * * *      (midnight daily)\n",
    "@weekly   = 0 0 * * 0      (midnight Sunday)\n",
    "@monthly  = 0 0 1 * *      (midnight first of month)\n",
    "```\n",
    "\n",
    "### Monitoring Best Practices\n",
    "\n",
    "1. **SLAs (Service Level Agreements)**\n",
    "   - Define expected completion times\n",
    "   - Alert when SLAs are breached\n",
    "\n",
    "2. **Key Metrics to Track**\n",
    "   - Task success/failure rates\n",
    "   - Pipeline duration trends\n",
    "   - Queue depth and resource utilization\n",
    "   - Data freshness (time since last update)\n",
    "\n",
    "3. **Alerting Strategy**\n",
    "   - Critical failures → PagerDuty/immediate alert\n",
    "   - Warnings → Slack notification\n",
    "   - Info → Dashboard/logs only\n",
    "\n",
    "4. **Observability Stack**\n",
    "   - Logs: Structured logging with context\n",
    "   - Metrics: Prometheus/Grafana dashboards\n",
    "   - Traces: OpenTelemetry for distributed tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple monitoring pattern\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "class TaskMonitor:\n",
    "    \"\"\"Simple task monitoring utility.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics: Dict[str, Any] = {}\n",
    "    \n",
    "    def record_start(self, task_name: str):\n",
    "        self.metrics[task_name] = {\n",
    "            \"start_time\": datetime.now(),\n",
    "            \"status\": \"running\"\n",
    "        }\n",
    "        print(f\"[{datetime.now()}] Task '{task_name}' started\")\n",
    "    \n",
    "    def record_success(self, task_name: str):\n",
    "        if task_name in self.metrics:\n",
    "            duration = (datetime.now() - self.metrics[task_name][\"start_time\"]).total_seconds()\n",
    "            self.metrics[task_name][\"status\"] = \"success\"\n",
    "            self.metrics[task_name][\"duration_seconds\"] = duration\n",
    "            print(f\"[{datetime.now()}] Task '{task_name}' completed in {duration:.2f}s\")\n",
    "    \n",
    "    def record_failure(self, task_name: str, error: str):\n",
    "        if task_name in self.metrics:\n",
    "            self.metrics[task_name][\"status\"] = \"failed\"\n",
    "            self.metrics[task_name][\"error\"] = error\n",
    "            print(f\"[{datetime.now()}] Task '{task_name}' FAILED: {error}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        return {\n",
    "            \"total_tasks\": len(self.metrics),\n",
    "            \"successful\": sum(1 for m in self.metrics.values() if m[\"status\"] == \"success\"),\n",
    "            \"failed\": sum(1 for m in self.metrics.values() if m[\"status\"] == \"failed\")\n",
    "        }\n",
    "\n",
    "# Demo\n",
    "monitor = TaskMonitor()\n",
    "\n",
    "monitor.record_start(\"extract\")\n",
    "time.sleep(0.1)  # Simulate work\n",
    "monitor.record_success(\"extract\")\n",
    "\n",
    "monitor.record_start(\"transform\")\n",
    "time.sleep(0.05)\n",
    "monitor.record_success(\"transform\")\n",
    "\n",
    "print(f\"\\nPipeline Summary: {monitor.get_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dd5d0",
   "metadata": {},
   "source": [
    "---\n",
    "## Decision Framework: Choosing an Orchestrator\n",
    "\n",
    "```\n",
    "                    START\n",
    "                      │\n",
    "         ┌────────────┴────────────┐\n",
    "         │ Need enterprise support │\n",
    "         │    and large team?      │\n",
    "         └────────────┬────────────┘\n",
    "                      │\n",
    "              ┌───────┴───────┐\n",
    "             YES              NO\n",
    "              │               │\n",
    "              ▼               ▼\n",
    "        ┌─────────┐   ┌──────────────┐\n",
    "        │ Airflow │   │ Data lineage │\n",
    "        │ (MWAA)  │   │  important?  │\n",
    "        └─────────┘   └──────┬───────┘\n",
    "                             │\n",
    "                     ┌───────┴───────┐\n",
    "                    YES              NO\n",
    "                     │               │\n",
    "                     ▼               ▼\n",
    "               ┌─────────┐    ┌──────────────┐\n",
    "               │ Dagster │    │ Simple Python│\n",
    "               └─────────┘    │   scripts?   │\n",
    "                              └──────┬───────┘\n",
    "                                     │\n",
    "                             ┌───────┴───────┐\n",
    "                            YES              NO\n",
    "                             │               │\n",
    "                             ▼               ▼\n",
    "                       ┌─────────┐    ┌─────────┐\n",
    "                       │ Prefect │    │ Airflow │\n",
    "                       └─────────┘    └─────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9de92",
   "metadata": {},
   "source": [
    "---\n",
    "## Takeaway\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Summary |\n",
    "|---------|----------|\n",
    "| **Orchestration** | Automated coordination of data workflow tasks |\n",
    "| **DAG** | Directed Acyclic Graph defining task dependencies |\n",
    "| **Scheduling** | Cron, event-driven, or sensor-based triggers |\n",
    "| **Monitoring** | SLAs, metrics, alerting, and observability |\n",
    "\n",
    "### Tool Selection Guide\n",
    "\n",
    "- **Apache Airflow** — Industry standard, extensive ecosystem, best for large teams\n",
    "- **Dagster** — Asset-centric, great for data lineage and testing\n",
    "- **Prefect** — Pythonic, dynamic workflows, easy migration from scripts\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Idempotency** — Tasks should be safely re-runnable\n",
    "2. **Atomicity** — Tasks succeed or fail completely\n",
    "3. **Small Tasks** — Prefer many small tasks over few large ones\n",
    "4. **Retry Logic** — Implement exponential backoff for failures\n",
    "5. **Documentation** — Document DAGs, dependencies, and data contracts\n",
    "6. **Testing** — Test DAGs in isolation before production\n",
    "7. **Version Control** — Store DAG definitions in Git\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore specific orchestrator deep-dives (Airflow, Dagster, Prefect)\n",
    "- Learn about data quality checks in pipelines\n",
    "- Study event-driven architectures for real-time orchestration"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
