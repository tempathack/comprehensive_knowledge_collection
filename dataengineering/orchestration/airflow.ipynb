{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91f21b0",
   "metadata": {},
   "source": [
    "# Apache Airflow Deep Dive\n",
    "\n",
    "Apache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. Originally created at Airbnb, it has become the de facto standard for orchestrating complex data pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Airflow Architecture](#architecture)\n",
    "2. [DAG Structure](#dag-structure)\n",
    "3. [Operators](#operators)\n",
    "4. [XCom for Task Communication](#xcom)\n",
    "5. [Best Practices for DAG Design](#best-practices)\n",
    "6. [DAG Examples](#examples)\n",
    "7. [Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06562ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Airflow Architecture <a id='architecture'></a>\n",
    "\n",
    "Airflow follows a **distributed architecture** with several key components working together:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          AIRFLOW ARCHITECTURE                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚    â”‚   Web UI     â”‚     â”‚  Scheduler   â”‚     â”‚   Executor   â”‚          â”‚\n",
    "â”‚    â”‚  (Flask)     â”‚     â”‚              â”‚     â”‚              â”‚          â”‚\n",
    "â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚           â”‚                    â”‚                    â”‚                  â”‚\n",
    "â”‚           â”‚                    â–¼                    â”‚                  â”‚\n",
    "â”‚           â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚                  â”‚\n",
    "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Metadata   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                       â”‚   Database   â”‚                                 â”‚\n",
    "â”‚                       â”‚  (Postgres)  â”‚                                 â”‚\n",
    "â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚\n",
    "â”‚                              â–²                                         â”‚\n",
    "â”‚                              â”‚                                         â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚    â”‚                                                   â”‚               â”‚\n",
    "â”‚    â–¼                    â–¼                    â–¼         â”‚               â”‚\n",
    "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚               â”‚\n",
    "â”‚ â”‚Worker 1â”‚         â”‚Worker 2â”‚         â”‚Worker Nâ”‚       â”‚               â”‚\n",
    "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚               â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Core Components\n",
    "\n",
    "| Component | Description | Responsibilities |\n",
    "|-----------|-------------|------------------|\n",
    "| **Scheduler** | The brain of Airflow | Parses DAGs, monitors task states, triggers task instances, manages dependencies |\n",
    "| **Webserver** | Flask-based UI | Provides DAG visualization, task logs, trigger/pause DAGs, user authentication |\n",
    "| **Workers** | Task executors | Execute the actual task logic, report status back to scheduler |\n",
    "| **Metadata DB** | State storage (PostgreSQL/MySQL) | Stores DAG runs, task instances, variables, connections, XComs |\n",
    "| **Executor** | Task distribution strategy | Determines how tasks are run (Local, Celery, Kubernetes, etc.) |\n",
    "\n",
    "### Executor Types\n",
    "\n",
    "| Executor | Use Case | Scalability |\n",
    "|----------|----------|-------------|\n",
    "| `SequentialExecutor` | Development/testing | Single task at a time |\n",
    "| `LocalExecutor` | Small to medium workloads | Parallel on single machine |\n",
    "| `CeleryExecutor` | Production, distributed | Horizontal scaling with message broker |\n",
    "| `KubernetesExecutor` | Cloud-native, dynamic | Each task runs in its own pod |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51104564",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DAG Structure <a id='dag-structure'></a>\n",
    "\n",
    "A **DAG (Directed Acyclic Graph)** is a collection of tasks with defined dependencies and execution order.\n",
    "\n",
    "### Key DAG Concepts\n",
    "\n",
    "```\n",
    "DAG Lifecycle:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Parsed    â”‚â”€â”€â”€â–ºâ”‚  Scheduled  â”‚â”€â”€â”€â–ºâ”‚   Queued    â”‚â”€â”€â”€â–ºâ”‚   Running   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                â”‚\n",
    "                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "                   â”‚   Failed    â”‚â—„â”€â”€â”€â”‚   Success   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### DAG Parameters\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|--------|\n",
    "| `dag_id` | Unique identifier | `\"my_etl_pipeline\"` |\n",
    "| `schedule_interval` | How often DAG runs | `\"@daily\"`, `\"0 0 * * *\"` |\n",
    "| `start_date` | When DAG becomes active | `datetime(2024, 1, 1)` |\n",
    "| `catchup` | Backfill past runs | `True` / `False` |\n",
    "| `default_args` | Default task arguments | `{\"retries\": 3}` |\n",
    "| `tags` | DAG categorization | `[\"etl\", \"production\"]` |\n",
    "| `max_active_runs` | Concurrent DAG runs | `1`, `3`, etc. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DAG Structure Example\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "# Default arguments applied to all tasks\n",
    "default_args = {\n",
    "    'owner': 'data_team',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['alerts@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(hours=2),\n",
    "}\n",
    "\n",
    "# DAG Definition\n",
    "with DAG(\n",
    "    dag_id='example_basic_dag',\n",
    "    default_args=default_args,\n",
    "    description='A simple example DAG',\n",
    "    schedule_interval='@daily',  # or cron: '0 0 * * *'\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['example', 'tutorial'],\n",
    "    max_active_runs=1,\n",
    ") as dag:\n",
    "    \n",
    "    start = EmptyOperator(task_id='start')\n",
    "    end = EmptyOperator(task_id='end')\n",
    "    \n",
    "    # Define task dependencies\n",
    "    start >> end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b01211",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Operators <a id='operators'></a>\n",
    "\n",
    "Operators define **what** a task does. They are the building blocks of workflows.\n",
    "\n",
    "### Operator Categories\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      OPERATOR TYPES                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Action Operators â”‚  Transfer Operatorsâ”‚   Sensor Operators  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ PythonOperator   â”‚ â€¢ S3ToRedshiftOp   â”‚ â€¢ FileSensor        â”‚\n",
    "â”‚ â€¢ BashOperator     â”‚ â€¢ GCSToGCSOperator â”‚ â€¢ S3KeySensor       â”‚\n",
    "â”‚ â€¢ EmailOperator    â”‚ â€¢ MySqlToHiveOp    â”‚ â€¢ HttpSensor        â”‚\n",
    "â”‚ â€¢ DockerOperator   â”‚ â€¢ PostgresToS3Op   â”‚ â€¢ ExternalTaskSensorâ”‚\n",
    "â”‚ â€¢ KubernetesPodOp  â”‚                    â”‚ â€¢ SqlSensor         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PythonOperator - Execute Python Callables\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_data(**context):\n",
    "    \"\"\"Extract data from source.\"\"\"\n",
    "    print(f\"Extracting data for {context['ds']}\")\n",
    "    data = {'records': 1000, 'source': 'api'}\n",
    "    return data  # This will be pushed to XCom\n",
    "\n",
    "def transform_data(ti, **context):\n",
    "    \"\"\"Transform extracted data.\"\"\"\n",
    "    # Pull data from previous task\n",
    "    data = ti.xcom_pull(task_ids='extract')\n",
    "    print(f\"Transforming {data['records']} records\")\n",
    "    data['transformed'] = True\n",
    "    return data\n",
    "\n",
    "def load_data(ti, **context):\n",
    "    \"\"\"Load data to destination.\"\"\"\n",
    "    data = ti.xcom_pull(task_ids='transform')\n",
    "    print(f\"Loading {data['records']} transformed records\")\n",
    "\n",
    "with DAG(\n",
    "    'python_operator_example',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    extract = PythonOperator(\n",
    "        task_id='extract',\n",
    "        python_callable=extract_data,\n",
    "        provide_context=True,  # Deprecated in 2.0+, context is auto-provided\n",
    "    )\n",
    "    \n",
    "    transform = PythonOperator(\n",
    "        task_id='transform',\n",
    "        python_callable=transform_data,\n",
    "    )\n",
    "    \n",
    "    load = PythonOperator(\n",
    "        task_id='load',\n",
    "        python_callable=load_data,\n",
    "    )\n",
    "    \n",
    "    extract >> transform >> load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BashOperator - Execute Shell Commands\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    'bash_operator_example',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    # Simple command\n",
    "    check_date = BashOperator(\n",
    "        task_id='check_date',\n",
    "        bash_command='date',\n",
    "    )\n",
    "    \n",
    "    # Command with templating\n",
    "    process_file = BashOperator(\n",
    "        task_id='process_file',\n",
    "        bash_command='echo \"Processing data for {{ ds }}\"',\n",
    "    )\n",
    "    \n",
    "    # Run external script\n",
    "    run_script = BashOperator(\n",
    "        task_id='run_script',\n",
    "        bash_command='/opt/scripts/etl.sh {{ ds }} ',  # Note trailing space!\n",
    "        env={'DATA_PATH': '/data/raw'},\n",
    "    )\n",
    "    \n",
    "    # Chain with environment variables\n",
    "    cleanup = BashOperator(\n",
    "        task_id='cleanup',\n",
    "        bash_command='rm -rf /tmp/airflow_temp_{{ ds_nodash }}',\n",
    "        trigger_rule='all_done',  # Run even if upstream fails\n",
    "    )\n",
    "    \n",
    "    check_date >> process_file >> run_script >> cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9751d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor Operators - Wait for External Conditions\n",
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with DAG(\n",
    "    'sensor_operators_example',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    # Wait for file to appear\n",
    "    wait_for_file = FileSensor(\n",
    "        task_id='wait_for_file',\n",
    "        filepath='/data/incoming/{{ ds }}/data.csv',\n",
    "        poke_interval=60,  # Check every 60 seconds\n",
    "        timeout=3600,      # Timeout after 1 hour\n",
    "        mode='poke',       # 'poke' or 'reschedule'\n",
    "        soft_fail=False,   # If True, marks as skipped on timeout\n",
    "    )\n",
    "    \n",
    "    # Wait for S3 file\n",
    "    wait_for_s3 = S3KeySensor(\n",
    "        task_id='wait_for_s3_file',\n",
    "        bucket_name='my-data-bucket',\n",
    "        bucket_key='raw/{{ ds }}/events.parquet',\n",
    "        aws_conn_id='aws_default',\n",
    "        poke_interval=300,\n",
    "        timeout=7200,\n",
    "        mode='reschedule',  # Frees worker slot between checks\n",
    "    )\n",
    "    \n",
    "    # Wait for API to be ready\n",
    "    wait_for_api = HttpSensor(\n",
    "        task_id='wait_for_api',\n",
    "        http_conn_id='api_default',\n",
    "        endpoint='/health',\n",
    "        response_check=lambda response: response.status_code == 200,\n",
    "        poke_interval=30,\n",
    "        timeout=600,\n",
    "    )\n",
    "    \n",
    "    # Wait for another DAG's task to complete\n",
    "    wait_for_upstream_dag = ExternalTaskSensor(\n",
    "        task_id='wait_for_upstream',\n",
    "        external_dag_id='upstream_dag',\n",
    "        external_task_id='final_task',\n",
    "        execution_delta=timedelta(hours=0),  # Same execution date\n",
    "        timeout=3600,\n",
    "        mode='reschedule',\n",
    "    )\n",
    "    \n",
    "    process = PythonOperator(\n",
    "        task_id='process_data',\n",
    "        python_callable=lambda: print(\"Processing...\"),\n",
    "    )\n",
    "    \n",
    "    # All sensors must complete before processing\n",
    "    [wait_for_file, wait_for_s3, wait_for_api, wait_for_upstream_dag] >> process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672730f6",
   "metadata": {},
   "source": [
    "### Sensor Modes\n",
    "\n",
    "| Mode | Behavior | Worker Usage | Best For |\n",
    "|------|----------|--------------|----------|\n",
    "| `poke` | Holds worker slot | Continuous | Short waits, high-frequency checks |\n",
    "| `reschedule` | Releases worker | Intermittent | Long waits, resource-constrained environments |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c246a91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. XCom for Task Communication <a id='xcom'></a>\n",
    "\n",
    "**XCom (Cross-Communication)** allows tasks to exchange small amounts of data.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           XCOM FLOW                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚   â”‚  Task A  â”‚â”€â”€pushâ”€â”€â–ºâ”‚   Metadata   â”‚â—„â”€â”€pullâ”€â”€â”‚  Task B  â”‚       â”‚\n",
    "â”‚   â”‚          â”‚         â”‚   Database   â”‚         â”‚          â”‚       â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   (XComs)    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚   Key: (dag_id, task_id, run_id, key)                              â”‚\n",
    "â”‚   Value: Serialized data (JSON by default)                         â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### XCom Best Practices\n",
    "\n",
    "| âœ… Do | âŒ Don't |\n",
    "|-------|----------|\n",
    "| Pass small metadata (IDs, paths, counts) | Pass large datasets |\n",
    "| Use for file paths | Store entire DataFrames |\n",
    "| Pass configuration values | Transfer binary files |\n",
    "| Use custom XCom backends for large data | Exceed 48KB (default limit) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbd56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XCom Examples - Multiple Methods\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.decorators import task\n",
    "from datetime import datetime\n",
    "\n",
    "# Method 1: Traditional XCom with ti\n",
    "def push_via_ti(ti):\n",
    "    \"\"\"Push to XCom using TaskInstance.\"\"\"\n",
    "    data = {'file_path': '/data/processed/output.parquet', 'row_count': 50000}\n",
    "    ti.xcom_push(key='etl_metadata', value=data)\n",
    "    ti.xcom_push(key='status', value='success')\n",
    "\n",
    "def pull_via_ti(ti):\n",
    "    \"\"\"Pull from XCom using TaskInstance.\"\"\"\n",
    "    metadata = ti.xcom_pull(task_ids='push_data', key='etl_metadata')\n",
    "    status = ti.xcom_pull(task_ids='push_data', key='status')\n",
    "    print(f\"File: {metadata['file_path']}, Rows: {metadata['row_count']}, Status: {status}\")\n",
    "\n",
    "# Method 2: Return value (auto-pushed with key='return_value')\n",
    "def push_via_return():\n",
    "    \"\"\"Return value is automatically pushed to XCom.\"\"\"\n",
    "    return {'processed_at': datetime.now().isoformat(), 'records': 1000}\n",
    "\n",
    "def pull_return_value(ti):\n",
    "    \"\"\"Pull the return value from another task.\"\"\"\n",
    "    result = ti.xcom_pull(task_ids='auto_push')  # No key needed for return_value\n",
    "    print(f\"Received: {result}\")\n",
    "\n",
    "# Method 3: Using Jinja templating\n",
    "def process_with_template(**context):\n",
    "    \"\"\"Access XCom via templates in operator parameters.\"\"\"\n",
    "    print(f\"Processing...\")\n",
    "\n",
    "with DAG(\n",
    "    'xcom_examples',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    push_data = PythonOperator(\n",
    "        task_id='push_data',\n",
    "        python_callable=push_via_ti,\n",
    "    )\n",
    "    \n",
    "    pull_data = PythonOperator(\n",
    "        task_id='pull_data',\n",
    "        python_callable=pull_via_ti,\n",
    "    )\n",
    "    \n",
    "    auto_push = PythonOperator(\n",
    "        task_id='auto_push',\n",
    "        python_callable=push_via_return,\n",
    "    )\n",
    "    \n",
    "    auto_pull = PythonOperator(\n",
    "        task_id='auto_pull',\n",
    "        python_callable=pull_return_value,\n",
    "    )\n",
    "    \n",
    "    push_data >> pull_data\n",
    "    auto_push >> auto_pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00692fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern TaskFlow API with XCom (Airflow 2.0+)\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='taskflow_xcom_example',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    "    tags=['taskflow', 'modern'],\n",
    ")\n",
    "def taskflow_etl():\n",
    "    \"\"\"ETL pipeline using TaskFlow API with implicit XCom.\"\"\"\n",
    "    \n",
    "    @task\n",
    "    def extract() -> dict:\n",
    "        \"\"\"Extract data from source.\"\"\"\n",
    "        return {\n",
    "            'data': [1, 2, 3, 4, 5],\n",
    "            'source': 'api',\n",
    "            'extracted_at': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    @task(multiple_outputs=True)  # Allows dict unpacking\n",
    "    def transform(raw_data: dict) -> dict:\n",
    "        \"\"\"Transform extracted data.\"\"\"\n",
    "        transformed = [x * 2 for x in raw_data['data']]\n",
    "        return {\n",
    "            'transformed_data': transformed,\n",
    "            'count': len(transformed),\n",
    "            'sum': sum(transformed),\n",
    "        }\n",
    "    \n",
    "    @task\n",
    "    def load(transformed_data: list, count: int, sum_val: int) -> None:\n",
    "        \"\"\"Load data to destination.\"\"\"\n",
    "        print(f\"Loading {count} records with sum {sum_val}\")\n",
    "        print(f\"Data: {transformed_data}\")\n",
    "    \n",
    "    @task\n",
    "    def notify(message: str) -> None:\n",
    "        \"\"\"Send notification.\"\"\"\n",
    "        print(f\"Notification: {message}\")\n",
    "    \n",
    "    # XCom is handled implicitly!\n",
    "    raw = extract()\n",
    "    result = transform(raw)\n",
    "    load(result['transformed_data'], result['count'], result['sum'])\n",
    "    notify(\"ETL completed successfully!\")\n",
    "\n",
    "# Instantiate the DAG\n",
    "taskflow_dag = taskflow_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22015224",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Best Practices for DAG Design <a id='best-practices'></a>\n",
    "\n",
    "### âœ… DAG Design Principles\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DAG DESIGN BEST PRACTICES                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  1. IDEMPOTENCY           â”‚  Re-running produces same result            â”‚\n",
    "â”‚  2. ATOMICITY             â”‚  Tasks succeed or fail completely           â”‚\n",
    "â”‚  3. INCREMENTAL LOADS     â”‚  Process only new/changed data              â”‚\n",
    "â”‚  4. SMALL TASKS           â”‚  Single responsibility per task             â”‚\n",
    "â”‚  5. AVOID TOP-LEVEL CODE  â”‚  No heavy imports at module level           â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Configuration Management\n",
    "\n",
    "| Approach | Use Case | Example |\n",
    "|----------|----------|--------|\n",
    "| **Variables** | Runtime config | API keys, thresholds |\n",
    "| **Connections** | External systems | DB credentials, S3 |\n",
    "| **Params** | DAG-level config | Passed at trigger time |\n",
    "| **Environment Variables** | Sensitive data | AWS credentials |\n",
    "\n",
    "### Common Anti-Patterns to Avoid\n",
    "\n",
    "| âŒ Anti-Pattern | âœ… Better Approach |\n",
    "|-----------------|--------------------|\n",
    "| Heavy top-level imports | Import inside task functions |\n",
    "| Hardcoded values | Use Variables/Connections |\n",
    "| Large XCom payloads | Store in S3, pass path via XCom |\n",
    "| Complex DAG logic | Use DAG factories or TaskGroups |\n",
    "| Many sequential tasks | Parallelize where possible |\n",
    "| No retries | Set appropriate retry policies |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices Example - Production-Ready DAG\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "from airflow.models import Variable\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Best Practice: Define default args separately\n",
    "default_args = {\n",
    "    'owner': 'data_engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'retry_exponential_backoff': True,\n",
    "    'max_retry_delay': timedelta(minutes=30),\n",
    "    'execution_timeout': timedelta(hours=1),\n",
    "}\n",
    "\n",
    "# Best Practice: Use factory function for dynamic DAGs\n",
    "def create_etl_dag(source_name: str, schedule: str):\n",
    "    \"\"\"Factory function to create ETL DAGs.\"\"\"\n",
    "    \n",
    "    dag_id = f'etl_{source_name}'\n",
    "    \n",
    "    with DAG(\n",
    "        dag_id=dag_id,\n",
    "        default_args=default_args,\n",
    "        description=f'ETL pipeline for {source_name}',\n",
    "        schedule_interval=schedule,\n",
    "        start_date=datetime(2024, 1, 1),\n",
    "        catchup=False,\n",
    "        tags=['etl', source_name, 'production'],\n",
    "        doc_md=f\"\"\"## ETL Pipeline: {source_name}\n",
    "        \n",
    "        This DAG extracts, transforms, and loads data from {source_name}.\n",
    "        \n",
    "        **Owner:** Data Engineering\n",
    "        **Schedule:** {schedule}\n",
    "        \"\"\",\n",
    "    ) as dag:\n",
    "        \n",
    "        start = EmptyOperator(task_id='start')\n",
    "        \n",
    "        # Best Practice: Use TaskGroups for organization\n",
    "        with TaskGroup('extract_tasks', tooltip='Data extraction') as extract_group:\n",
    "            \n",
    "            def extract_data(source, **context):\n",
    "                # Best Practice: Heavy imports inside function\n",
    "                import pandas as pd\n",
    "                from sqlalchemy import create_engine\n",
    "                \n",
    "                # Best Practice: Use Variables for config\n",
    "                config = Variable.get(f'{source}_config', deserialize_json=True)\n",
    "                logger.info(f\"Extracting from {source} with config: {config}\")\n",
    "                \n",
    "                # Best Practice: Return path, not data\n",
    "                output_path = f\"/data/raw/{source}/{context['ds']}/data.parquet\"\n",
    "                return output_path\n",
    "            \n",
    "            extract = PythonOperator(\n",
    "                task_id='extract',\n",
    "                python_callable=extract_data,\n",
    "                op_kwargs={'source': source_name},\n",
    "            )\n",
    "        \n",
    "        with TaskGroup('transform_tasks', tooltip='Data transformation') as transform_group:\n",
    "            \n",
    "            def transform_data(ti, **context):\n",
    "                input_path = ti.xcom_pull(task_ids='extract_tasks.extract')\n",
    "                logger.info(f\"Transforming data from {input_path}\")\n",
    "                \n",
    "                output_path = f\"/data/processed/{source_name}/{context['ds']}/data.parquet\"\n",
    "                return output_path\n",
    "            \n",
    "            transform = PythonOperator(\n",
    "                task_id='transform',\n",
    "                python_callable=transform_data,\n",
    "            )\n",
    "        \n",
    "        with TaskGroup('load_tasks', tooltip='Data loading') as load_group:\n",
    "            \n",
    "            def load_data(ti, **context):\n",
    "                input_path = ti.xcom_pull(task_ids='transform_tasks.transform')\n",
    "                logger.info(f\"Loading data from {input_path}\")\n",
    "                return {'rows_loaded': 10000, 'status': 'success'}\n",
    "            \n",
    "            load = PythonOperator(\n",
    "                task_id='load',\n",
    "                python_callable=load_data,\n",
    "            )\n",
    "        \n",
    "        end = EmptyOperator(\n",
    "            task_id='end',\n",
    "            trigger_rule='none_failed',  # Runs if no upstream failed\n",
    "        )\n",
    "        \n",
    "        # Define dependencies\n",
    "        start >> extract_group >> transform_group >> load_group >> end\n",
    "    \n",
    "    return dag\n",
    "\n",
    "# Create DAGs dynamically\n",
    "sources = [\n",
    "    ('salesforce', '@daily'),\n",
    "    ('hubspot', '@hourly'),\n",
    "    ('stripe', '0 */4 * * *'),\n",
    "]\n",
    "\n",
    "for source_name, schedule in sources:\n",
    "    globals()[f'dag_etl_{source_name}'] = create_etl_dag(source_name, schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a123d08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. DAG Examples <a id='examples'></a>\n",
    "\n",
    "### Complete Production Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e683b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Data Lake Ingestion Pipeline\n",
    "from airflow import DAG\n",
    "from airflow.decorators import task, dag\n",
    "from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator\n",
    "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dag(\n",
    "    dag_id='data_lake_ingestion',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@hourly',\n",
    "    catchup=False,\n",
    "    default_args={\n",
    "        'retries': 3,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "    },\n",
    "    tags=['data-lake', 'ingestion', 'production'],\n",
    ")\n",
    "def data_lake_ingestion():\n",
    "    \"\"\"Ingest data from multiple sources into data lake.\"\"\"\n",
    "    \n",
    "    @task\n",
    "    def check_source_availability() -> dict:\n",
    "        \"\"\"Check which data sources have new data.\"\"\"\n",
    "        # Simulate checking sources\n",
    "        return {\n",
    "            'api_source': True,\n",
    "            'db_source': True,\n",
    "            'file_source': False,\n",
    "        }\n",
    "    \n",
    "    @task\n",
    "    def ingest_api_data() -> str:\n",
    "        \"\"\"Ingest data from REST API.\"\"\"\n",
    "        import json\n",
    "        import requests\n",
    "        \n",
    "        # Simulated API call\n",
    "        data = {'records': 500, 'source': 'api'}\n",
    "        output_path = 's3://data-lake/raw/api/{{ ds }}/data.json'\n",
    "        return output_path\n",
    "    \n",
    "    @task\n",
    "    def ingest_db_data() -> str:\n",
    "        \"\"\"Ingest data from database.\"\"\"\n",
    "        output_path = 's3://data-lake/raw/db/{{ ds }}/data.parquet'\n",
    "        return output_path\n",
    "    \n",
    "    @task\n",
    "    def merge_and_dedupe(paths: list) -> str:\n",
    "        \"\"\"Merge data from all sources and deduplicate.\"\"\"\n",
    "        output_path = 's3://data-lake/processed/{{ ds }}/merged.parquet'\n",
    "        return output_path\n",
    "    \n",
    "    @task\n",
    "    def update_catalog(path: str) -> None:\n",
    "        \"\"\"Update data catalog with new partition.\"\"\"\n",
    "        print(f\"Registering {path} in catalog\")\n",
    "    \n",
    "    # Pipeline logic\n",
    "    sources = check_source_availability()\n",
    "    api_path = ingest_api_data()\n",
    "    db_path = ingest_db_data()\n",
    "    merged_path = merge_and_dedupe([api_path, db_path])\n",
    "    update_catalog(merged_path)\n",
    "\n",
    "data_lake_dag = data_lake_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3330d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: ML Pipeline with Branching Logic\n",
    "from airflow import DAG\n",
    "from airflow.decorators import task\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with DAG(\n",
    "    dag_id='ml_training_pipeline',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@weekly',\n",
    "    catchup=False,\n",
    "    tags=['ml', 'training'],\n",
    ") as dag:\n",
    "    \n",
    "    @task\n",
    "    def prepare_training_data():\n",
    "        \"\"\"Prepare and validate training dataset.\"\"\"\n",
    "        # Simulate data preparation\n",
    "        return {\n",
    "            'path': 's3://ml-data/training/dataset.parquet',\n",
    "            'rows': 100000,\n",
    "            'features': 50,\n",
    "        }\n",
    "    \n",
    "    @task\n",
    "    def train_model(data_info: dict):\n",
    "        \"\"\"Train the ML model.\"\"\"\n",
    "        return {\n",
    "            'model_path': 's3://ml-models/model_v2.pkl',\n",
    "            'accuracy': 0.95,\n",
    "            'f1_score': 0.93,\n",
    "        }\n",
    "    \n",
    "    @task\n",
    "    def evaluate_model(model_info: dict) -> dict:\n",
    "        \"\"\"Evaluate model performance.\"\"\"\n",
    "        return {\n",
    "            'accuracy': model_info['accuracy'],\n",
    "            'meets_threshold': model_info['accuracy'] > 0.90,\n",
    "        }\n",
    "    \n",
    "    def decide_deployment(ti):\n",
    "        \"\"\"Decide whether to deploy based on metrics.\"\"\"\n",
    "        metrics = ti.xcom_pull(task_ids='evaluate_model')\n",
    "        if metrics['meets_threshold']:\n",
    "            return 'deploy_model'\n",
    "        return 'notify_failure'\n",
    "    \n",
    "    @task\n",
    "    def deploy_model():\n",
    "        \"\"\"Deploy model to production.\"\"\"\n",
    "        print(\"Model deployed successfully!\")\n",
    "    \n",
    "    @task\n",
    "    def notify_failure():\n",
    "        \"\"\"Notify team of failed quality check.\"\"\"\n",
    "        print(\"Model did not meet quality threshold\")\n",
    "    \n",
    "    # Pipeline definition\n",
    "    data = prepare_training_data()\n",
    "    model = train_model(data)\n",
    "    metrics = evaluate_model(model)\n",
    "    \n",
    "    branch = BranchPythonOperator(\n",
    "        task_id='decide_deployment',\n",
    "        python_callable=decide_deployment,\n",
    "    )\n",
    "    \n",
    "    deploy = deploy_model()\n",
    "    notify = notify_failure()\n",
    "    \n",
    "    end = EmptyOperator(\n",
    "        task_id='end',\n",
    "        trigger_rule='none_failed_min_one_success',\n",
    "    )\n",
    "    \n",
    "    metrics >> branch >> [deploy, notify] >> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Dynamic Task Mapping (Airflow 2.3+)\n",
    "from airflow import DAG\n",
    "from airflow.decorators import task\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id='dynamic_task_mapping',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    "    tags=['dynamic', 'parallel'],\n",
    ") as dag:\n",
    "    \n",
    "    @task\n",
    "    def get_partitions() -> list:\n",
    "        \"\"\"Get list of partitions to process.\"\"\"\n",
    "        # This could query a database or list S3 prefixes\n",
    "        return [\n",
    "            {'partition': 'us-east', 'size': 1000},\n",
    "            {'partition': 'us-west', 'size': 800},\n",
    "            {'partition': 'eu-west', 'size': 1200},\n",
    "            {'partition': 'ap-south', 'size': 600},\n",
    "        ]\n",
    "    \n",
    "    @task\n",
    "    def process_partition(partition_info: dict) -> dict:\n",
    "        \"\"\"Process a single partition.\"\"\"\n",
    "        partition = partition_info['partition']\n",
    "        size = partition_info['size']\n",
    "        print(f\"Processing partition {partition} with {size} records\")\n",
    "        return {\n",
    "            'partition': partition,\n",
    "            'processed': size,\n",
    "            'status': 'success',\n",
    "        }\n",
    "    \n",
    "    @task\n",
    "    def aggregate_results(results: list) -> dict:\n",
    "        \"\"\"Aggregate results from all partitions.\"\"\"\n",
    "        total = sum(r['processed'] for r in results)\n",
    "        return {\n",
    "            'total_processed': total,\n",
    "            'partitions': len(results),\n",
    "            'all_successful': all(r['status'] == 'success' for r in results),\n",
    "        }\n",
    "    \n",
    "    # Dynamic task mapping - creates N parallel tasks\n",
    "    partitions = get_partitions()\n",
    "    \n",
    "    # .expand() creates one task instance per item in the list\n",
    "    processed = process_partition.expand(partition_info=partitions)\n",
    "    \n",
    "    # Aggregate automatically waits for all mapped tasks\n",
    "    final = aggregate_results(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c52a07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Takeaways <a id='takeaways'></a>\n",
    "\n",
    "### ğŸ¯ Key Concepts Summary\n",
    "\n",
    "| Concept | Key Points |\n",
    "|---------|------------|\n",
    "| **Architecture** | Scheduler orchestrates, Workers execute, Metadata DB stores state |\n",
    "| **DAGs** | Define workflows as code, set schedules, manage dependencies |\n",
    "| **Operators** | Action (Python, Bash), Transfer (S3, DB), Sensor (wait for events) |\n",
    "| **XCom** | Pass small data between tasks; use paths for large data |\n",
    "| **TaskFlow API** | Modern Python syntax with implicit XCom handling |\n",
    "| **Dynamic Mapping** | Create parallel tasks at runtime based on data |\n",
    "\n",
    "### ğŸ“‹ Production Checklist\n",
    "\n",
    "```\n",
    "âœ… DAG Design\n",
    "   â–¡ Idempotent tasks (re-runnable without side effects)\n",
    "   â–¡ Proper retry configuration\n",
    "   â–¡ Appropriate timeouts set\n",
    "   â–¡ Meaningful task IDs and DAG documentation\n",
    "   â–¡ Use TaskGroups for organization\n",
    "\n",
    "âœ… Performance\n",
    "   â–¡ Avoid heavy top-level imports\n",
    "   â–¡ Use 'reschedule' mode for long-running sensors\n",
    "   â–¡ Parallelize where possible\n",
    "   â–¡ Set appropriate max_active_runs\n",
    "\n",
    "âœ… Data Handling\n",
    "   â–¡ Small metadata in XCom, large data in object storage\n",
    "   â–¡ Use Connections for credentials\n",
    "   â–¡ Use Variables for runtime configuration\n",
    "\n",
    "âœ… Monitoring\n",
    "   â–¡ Email alerts configured\n",
    "   â–¡ SLA monitoring enabled\n",
    "   â–¡ Meaningful logging in tasks\n",
    "   â–¡ Custom metrics where needed\n",
    "```\n",
    "\n",
    "### ğŸ“š Additional Resources\n",
    "\n",
    "- [Apache Airflow Documentation](https://airflow.apache.org/docs/)\n",
    "- [Astronomer Guides](https://docs.astronomer.io/learn)\n",
    "- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n",
    "- [Provider Packages](https://airflow.apache.org/docs/apache-airflow-providers/index.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
