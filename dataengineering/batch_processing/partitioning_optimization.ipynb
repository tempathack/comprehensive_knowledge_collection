{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814a799b",
   "metadata": {},
   "source": [
    "# Spark Partitioning and Optimization\n",
    "\n",
    "This notebook covers essential concepts for optimizing Apache Spark applications through effective partitioning strategies, minimizing shuffles, and leveraging caching mechanisms.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Partitioning in Spark](#data-partitioning)\n",
    "2. [Shuffle Operations](#shuffle-operations)\n",
    "3. [Broadcast Joins](#broadcast-joins)\n",
    "4. [Partition Pruning](#partition-pruning)\n",
    "5. [Caching and Persistence](#caching-persistence)\n",
    "6. [Key Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3952bf4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Partitioning in Spark <a id='data-partitioning'></a>\n",
    "\n",
    "**Partitioning** is the process of dividing data into smaller, manageable chunks (partitions) that can be processed in parallel across a cluster.\n",
    "\n",
    "### Why Partitioning Matters\n",
    "\n",
    "| Aspect | Impact |\n",
    "|--------|--------|\n",
    "| **Parallelism** | More partitions = more parallel tasks |\n",
    "| **Memory** | Each partition must fit in executor memory |\n",
    "| **Shuffles** | Poor partitioning leads to expensive data movement |\n",
    "| **Skew** | Uneven partitions cause stragglers |\n",
    "\n",
    "### Types of Partitioning\n",
    "\n",
    "1. **Hash Partitioning**: Distributes data based on hash of partition key\n",
    "2. **Range Partitioning**: Distributes data based on ordered ranges\n",
    "3. **Round-Robin Partitioning**: Distributes data evenly regardless of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, broadcast, spark_partition_id\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PartitioningOptimization\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Default parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23429cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = [(i, f\"user_{i % 100}\", i * 10) for i in range(10000)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"user\", \"amount\"])\n",
    "\n",
    "# Check current number of partitions\n",
    "print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# View partition distribution\n",
    "df.withColumn(\"partition_id\", spark_partition_id()) \\\n",
    "    .groupBy(\"partition_id\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"partition_id\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a77b0",
   "metadata": {},
   "source": [
    "### Controlling Partitions\n",
    "\n",
    "- **`repartition(n)`**: Increases or decreases partitions (causes full shuffle)\n",
    "- **`coalesce(n)`**: Decreases partitions (avoids full shuffle, more efficient)\n",
    "- **`repartition(col)`**: Partitions by specific column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition by number\n",
    "df_8_parts = df.repartition(8)\n",
    "print(f\"After repartition(8): {df_8_parts.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Coalesce to reduce partitions (no shuffle)\n",
    "df_4_parts = df_8_parts.coalesce(4)\n",
    "print(f\"After coalesce(4): {df_4_parts.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Repartition by column (useful for joins)\n",
    "df_by_user = df.repartition(10, \"user\")\n",
    "print(f\"After repartition by 'user': {df_by_user.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Check partition distribution after repartitioning by column\n",
    "df_by_user.withColumn(\"partition_id\", spark_partition_id()) \\\n",
    "    .groupBy(\"partition_id\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"partition_id\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca4b35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Shuffle Operations <a id='shuffle-operations'></a>\n",
    "\n",
    "A **shuffle** is Spark's mechanism for redistributing data across partitions. It's one of the most expensive operations.\n",
    "\n",
    "### Operations That Cause Shuffles\n",
    "\n",
    "| Operation | Description |\n",
    "|-----------|-------------|\n",
    "| `groupBy()` | Aggregates require data with same key on same partition |\n",
    "| `join()` | Default sort-merge join shuffles both datasets |\n",
    "| `distinct()` | Requires comparing all records |\n",
    "| `repartition()` | Explicitly redistributes data |\n",
    "| `orderBy()` / `sort()` | Global ordering requires data movement |\n",
    "\n",
    "### Minimizing Shuffles\n",
    "\n",
    "1. **Pre-partition data** on join keys\n",
    "2. **Use broadcast joins** for small tables\n",
    "3. **Filter early** to reduce data volume\n",
    "4. **Use `reduceByKey`** instead of `groupByKey` (for RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Shuffle caused by groupBy\n",
    "aggregated_df = df.groupBy(\"user\").agg(\n",
    "    F.sum(\"amount\").alias(\"total_amount\"),\n",
    "    F.count(\"*\").alias(\"transaction_count\")\n",
    ")\n",
    "\n",
    "# Explain the execution plan to see shuffle\n",
    "print(\"=== GroupBy Execution Plan ===\")\n",
    "aggregated_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01015752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two DataFrames for join example\n",
    "orders = spark.createDataFrame(\n",
    "    [(1, \"user_1\", 100), (2, \"user_2\", 200), (3, \"user_1\", 150)],\n",
    "    [\"order_id\", \"user_id\", \"amount\"]\n",
    ")\n",
    "\n",
    "users = spark.createDataFrame(\n",
    "    [(\"user_1\", \"Alice\"), (\"user_2\", \"Bob\"), (\"user_3\", \"Charlie\")],\n",
    "    [\"user_id\", \"name\"]\n",
    ")\n",
    "\n",
    "# Default join causes shuffle on both sides\n",
    "joined_df = orders.join(users, \"user_id\")\n",
    "print(\"=== Default Join (Sort-Merge) Execution Plan ===\")\n",
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize shuffles by pre-partitioning on join key\n",
    "orders_partitioned = orders.repartition(4, \"user_id\")\n",
    "users_partitioned = users.repartition(4, \"user_id\")\n",
    "\n",
    "# Now join - data already co-located by key\n",
    "joined_optimized = orders_partitioned.join(users_partitioned, \"user_id\")\n",
    "print(\"=== Pre-partitioned Join Execution Plan ===\")\n",
    "joined_optimized.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9794382",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Broadcast Joins <a id='broadcast-joins'></a>\n",
    "\n",
    "**Broadcast joins** send the smaller dataset to all executors, avoiding shuffle of the larger dataset.\n",
    "\n",
    "### When to Use Broadcast Joins\n",
    "\n",
    "- One table is **small enough to fit in memory** (< 10MB default, configurable)\n",
    "- Joining a **fact table with dimension tables**\n",
    "- **Lookup operations** with reference data\n",
    "\n",
    "### Configuration\n",
    "\n",
    "```python\n",
    "# Auto-broadcast threshold (default 10MB)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # bytes\n",
    "\n",
    "# Disable auto-broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large orders table and small lookup table\n",
    "large_orders = spark.createDataFrame(\n",
    "    [(i, f\"product_{i % 10}\", i * 5) for i in range(10000)],\n",
    "    [\"order_id\", \"product_id\", \"quantity\"]\n",
    ")\n",
    "\n",
    "# Small dimension/lookup table\n",
    "products = spark.createDataFrame(\n",
    "    [(f\"product_{i}\", f\"Product Name {i}\", i * 10.0) for i in range(10)],\n",
    "    [\"product_id\", \"product_name\", \"price\"]\n",
    ")\n",
    "\n",
    "print(f\"Orders count: {large_orders.count()}\")\n",
    "print(f\"Products count: {products.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "broadcast_joined = large_orders.join(\n",
    "    broadcast(products),  # Broadcast the small table\n",
    "    \"product_id\"\n",
    ")\n",
    "\n",
    "print(\"=== Broadcast Join Execution Plan ===\")\n",
    "broadcast_joined.explain()\n",
    "\n",
    "# Notice: No Exchange (shuffle) on the large table side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23421007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance: Regular join vs Broadcast join\n",
    "import time\n",
    "\n",
    "# Disable auto-broadcast for comparison\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Regular Sort-Merge Join\n",
    "start = time.time()\n",
    "regular_join = large_orders.join(products, \"product_id\")\n",
    "regular_join.count()  # Force execution\n",
    "regular_time = time.time() - start\n",
    "\n",
    "# Broadcast Join\n",
    "start = time.time()\n",
    "broadcast_join = large_orders.join(broadcast(products), \"product_id\")\n",
    "broadcast_join.count()  # Force execution\n",
    "broadcast_time = time.time() - start\n",
    "\n",
    "print(f\"Regular Join Time: {regular_time:.3f}s\")\n",
    "print(f\"Broadcast Join Time: {broadcast_time:.3f}s\")\n",
    "print(f\"Speedup: {regular_time / broadcast_time:.2f}x\")\n",
    "\n",
    "# Re-enable auto-broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08fd12b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Partition Pruning <a id='partition-pruning'></a>\n",
    "\n",
    "**Partition pruning** is an optimization that skips reading data partitions that don't match filter conditions.\n",
    "\n",
    "### Types of Partition Pruning\n",
    "\n",
    "1. **Static Partition Pruning**: Filter on partition column is known at compile time\n",
    "2. **Dynamic Partition Pruning (DPP)**: Filter values determined at runtime from another table\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Partition data by commonly filtered columns (date, region, etc.)\n",
    "- Use **Parquet** or **Delta Lake** for efficient partition pruning\n",
    "- Filter on partition columns **before** other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partitioned data and write to disk\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = spark.createDataFrame(\n",
    "    [(i, f\"2024-0{(i % 3) + 1}-01\", f\"region_{i % 5}\", i * 100) \n",
    "     for i in range(1000)],\n",
    "    [\"sale_id\", \"sale_date\", \"region\", \"amount\"]\n",
    ")\n",
    "\n",
    "# Create temp directory for partitioned data\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "partitioned_path = os.path.join(temp_dir, \"sales_partitioned\")\n",
    "\n",
    "# Write partitioned by date and region\n",
    "sales_data.write \\\n",
    "    .partitionBy(\"sale_date\", \"region\") \\\n",
    "    .parquet(partitioned_path)\n",
    "\n",
    "print(f\"Data written to: {partitioned_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partitioned data\n",
    "partitioned_sales = spark.read.parquet(partitioned_path)\n",
    "\n",
    "# Static Partition Pruning - filter on partition column\n",
    "filtered_sales = partitioned_sales.filter(\n",
    "    (col(\"sale_date\") == \"2024-01-01\") & \n",
    "    (col(\"region\") == \"region_0\")\n",
    ")\n",
    "\n",
    "print(\"=== Static Partition Pruning ===\")\n",
    "filtered_sales.explain()\n",
    "# Look for \"PartitionFilters\" in the plan - shows pruned partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Partition Pruning Example\n",
    "# Enable DPP (enabled by default in Spark 3.0+)\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "\n",
    "# Dimension table with filter\n",
    "regions_to_include = spark.createDataFrame(\n",
    "    [(\"region_0\",), (\"region_1\",)],\n",
    "    [\"region\"]\n",
    ")\n",
    "\n",
    "# Join with partitioned fact table - DPP kicks in\n",
    "dpp_result = partitioned_sales.join(\n",
    "    regions_to_include,\n",
    "    \"region\"\n",
    ")\n",
    "\n",
    "print(\"=== Dynamic Partition Pruning ===\")\n",
    "dpp_result.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e388b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Caching and Persistence <a id='caching-persistence'></a>\n",
    "\n",
    "**Caching** stores intermediate DataFrames in memory (or disk) to avoid recomputation.\n",
    "\n",
    "### Storage Levels\n",
    "\n",
    "| Level | Description |\n",
    "|-------|-------------|\n",
    "| `MEMORY_ONLY` | Store as deserialized Java objects in JVM (default for `cache()`) |\n",
    "| `MEMORY_AND_DISK` | Spill to disk if memory is insufficient |\n",
    "| `MEMORY_ONLY_SER` | Store as serialized objects (more space-efficient) |\n",
    "| `DISK_ONLY` | Store only on disk |\n",
    "| `OFF_HEAP` | Store in off-heap memory |\n",
    "\n",
    "### When to Cache\n",
    "\n",
    "- DataFrame is **used multiple times**\n",
    "- Result of **expensive computation** (joins, aggregations)\n",
    "- **Iterative algorithms** (ML training)\n",
    "\n",
    "### When NOT to Cache\n",
    "\n",
    "- DataFrame is used only once\n",
    "- Data is too large to fit in memory\n",
    "- Computation is simple (reads from Parquet with pushdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Create an expensive DataFrame (simulating complex computation)\n",
    "expensive_df = large_orders.join(broadcast(products), \"product_id\") \\\n",
    "    .withColumn(\"total_value\", col(\"quantity\") * col(\"price\")) \\\n",
    "    .groupBy(\"product_name\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_value\").alias(\"revenue\"),\n",
    "        F.count(\"*\").alias(\"order_count\")\n",
    "    )\n",
    "\n",
    "# Cache in memory and disk\n",
    "expensive_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Force caching by triggering an action\n",
    "expensive_df.count()\n",
    "\n",
    "print(\"DataFrame cached!\")\n",
    "print(f\"Storage Level: {expensive_df.storageLevel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple operations on cached DataFrame (no recomputation)\n",
    "import time\n",
    "\n",
    "# First action - may still compute if not fully cached\n",
    "start = time.time()\n",
    "print(\"Top products by revenue:\")\n",
    "expensive_df.orderBy(col(\"revenue\").desc()).show(5)\n",
    "print(f\"Time: {time.time() - start:.3f}s\")\n",
    "\n",
    "# Second action - reads from cache\n",
    "start = time.time()\n",
    "print(\"\\nTotal revenue:\")\n",
    "print(expensive_df.agg(F.sum(\"revenue\")).collect()[0][0])\n",
    "print(f\"Time: {time.time() - start:.3f}s\")\n",
    "\n",
    "# Third action - reads from cache\n",
    "start = time.time()\n",
    "print(f\"\\nAverage order count: {expensive_df.agg(F.avg('order_count')).collect()[0][0]:.2f}\")\n",
    "print(f\"Time: {time.time() - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76db674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's cached\n",
    "print(\"=== Cached DataFrames ===\")\n",
    "for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "    print(f\"RDD ID: {id}, Name: {rdd.name()}, Storage Level: {rdd.getStorageLevel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b741f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist when done to free memory\n",
    "expensive_df.unpersist()\n",
    "print(\"DataFrame unpersisted!\")\n",
    "\n",
    "# Alternative: cache() is shorthand for persist(MEMORY_AND_DISK)\n",
    "# df.cache()  # Equivalent to df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb95ef",
   "metadata": {},
   "source": [
    "### Checkpointing vs Caching\n",
    "\n",
    "| Feature | Caching | Checkpointing |\n",
    "|---------|---------|---------------|\n",
    "| **Storage** | Memory/Disk (temporary) | HDFS/S3 (durable) |\n",
    "| **Lineage** | Preserved | Truncated |\n",
    "| **Use Case** | Iterative algorithms | Breaking long lineages, fault tolerance |\n",
    "| **Speed** | Faster | Slower (writes to distributed storage) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1baa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing example (requires checkpoint directory)\n",
    "checkpoint_dir = os.path.join(temp_dir, \"checkpoints\")\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
    "\n",
    "# Checkpoint breaks lineage - useful for iterative algorithms\n",
    "# Uncomment to use:\n",
    "# long_lineage_df = df\n",
    "# for i in range(10):\n",
    "#     long_lineage_df = long_lineage_df.withColumn(f\"col_{i}\", col(\"amount\") + i)\n",
    "# \n",
    "# # Checkpoint to truncate lineage\n",
    "# checkpointed_df = long_lineage_df.checkpoint()\n",
    "\n",
    "print(f\"Checkpoint directory set to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0d41d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Additional Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Query Execution (AQE) - Spark 3.0+\n",
    "# Dynamically optimizes query plans at runtime\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "print(\"AQE Configuration:\")\n",
    "print(f\"  AQE Enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"  Coalesce Partitions: {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\n",
    "print(f\"  Skew Join: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb4559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Pruning - Only select needed columns\n",
    "# Bad: Select all then filter\n",
    "# all_cols = partitioned_sales.select(\"*\").filter(col(\"amount\") > 500)\n",
    "\n",
    "# Good: Select only needed columns\n",
    "pruned_cols = partitioned_sales.select(\"sale_id\", \"amount\") \\\n",
    "    .filter(col(\"amount\") > 500)\n",
    "\n",
    "print(\"=== Column Pruning Plan ===\")\n",
    "pruned_cols.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec301af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicate Pushdown - Filter early\n",
    "# Filters are pushed down to data source level\n",
    "\n",
    "filtered_early = spark.read.parquet(partitioned_path) \\\n",
    "    .filter(col(\"amount\") > 50000)  # Filter pushed to Parquet reader\n",
    "\n",
    "print(\"=== Predicate Pushdown Plan ===\")\n",
    "filtered_early.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e0bc82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways <a id='takeaways'></a>\n",
    "\n",
    "### Partitioning Best Practices\n",
    "\n",
    "| Strategy | When to Use |\n",
    "|----------|-------------|\n",
    "| **Use `coalesce()` over `repartition()`** | When reducing partitions (avoids shuffle) |\n",
    "| **Partition by join keys** | When joining same datasets repeatedly |\n",
    "| **Match partitions to cluster cores** | 2-4 partitions per CPU core is optimal |\n",
    "\n",
    "### Shuffle Optimization\n",
    "\n",
    "| Technique | Benefit |\n",
    "|-----------|--------|\n",
    "| **Broadcast joins** | Eliminates shuffle for small tables |\n",
    "| **Pre-partitioning** | Reduces shuffle during joins |\n",
    "| **Filter early** | Less data to shuffle |\n",
    "\n",
    "### Caching Strategy\n",
    "\n",
    "| Guideline | Recommendation |\n",
    "|-----------|----------------|\n",
    "| **Cache reused DataFrames** | Especially after expensive operations |\n",
    "| **Use appropriate storage level** | `MEMORY_AND_DISK` for large datasets |\n",
    "| **Unpersist when done** | Free memory for other operations |\n",
    "| **Monitor cache usage** | Check Spark UI Storage tab |\n",
    "\n",
    "### Quick Reference Configurations\n",
    "\n",
    "```python\n",
    "# Key optimization configs\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # Adjust based on data size\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # Enable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")  # Handle skew\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebe3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# Clean up temp directory\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"Cleaned up: {temp_dir}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
