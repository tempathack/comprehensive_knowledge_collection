{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83384ef2",
   "metadata": {},
   "source": [
    "# Batch Processing — Overview\n",
    "\n",
    "## Purpose\n",
    "Understand the fundamentals of batch processing, when to apply it, and how Apache Spark enables scalable data transformations on large datasets.\n",
    "\n",
    "## Key Questions\n",
    "1. What is batch processing and how does it differ from real-time processing?\n",
    "2. When should you choose batch over streaming?\n",
    "3. How does Apache Spark architecture enable distributed batch processing?\n",
    "4. What are the core PySpark APIs for batch workloads?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c9ab2",
   "metadata": {},
   "source": [
    "---\n",
    "## What Is Batch Processing?\n",
    "\n",
    "**Batch processing** is a method of running high-volume, repetitive data jobs on a scheduled or on-demand basis. Data is collected over a period, then processed as a single unit (batch).\n",
    "\n",
    "### Characteristics\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Latency** | Minutes to hours (not real-time) |\n",
    "| **Volume** | Large datasets (GBs to PBs) |\n",
    "| **Scheduling** | Cron, Airflow, or orchestrators |\n",
    "| **Use Cases** | ETL, reporting, ML training, data warehousing |\n",
    "\n",
    "### When to Use Batch Processing\n",
    "- **Historical analysis**: Aggregate logs, sales, or events over days/weeks\n",
    "- **Cost efficiency**: Process during off-peak hours\n",
    "- **Complex transformations**: Joins, aggregations, and ML pipelines\n",
    "- **Data doesn't require instant results**: Reports, dashboards refreshed periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1acd54",
   "metadata": {},
   "source": [
    "---\n",
    "## Apache Spark — Overview & Architecture\n",
    "\n",
    "**Apache Spark** is a unified analytics engine for large-scale data processing with built-in modules for SQL, streaming, ML, and graph processing.\n",
    "\n",
    "### Core Components\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    Driver Program                       │\n",
    "│  ┌─────────────┐                                        │\n",
    "│  │ SparkContext│ ──► Cluster Manager (YARN/K8s/Mesos)   │\n",
    "│  └─────────────┘                                        │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "          ┌───────────────┼───────────────┐\n",
    "          ▼               ▼               ▼\n",
    "    ┌──────────┐    ┌──────────┐    ┌──────────┐\n",
    "    │ Executor │    │ Executor │    │ Executor │\n",
    "    │  (Node)  │    │  (Node)  │    │  (Node)  │\n",
    "    │ ┌──────┐ │    │ ┌──────┐ │    │ ┌──────┐ │\n",
    "    │ │Tasks │ │    │ │Tasks │ │    │ │Tasks │ │\n",
    "    │ └──────┘ │    │ └──────┘ │    │ └──────┘ │\n",
    "    └──────────┘    └──────────┘    └──────────┘\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **RDD** | Resilient Distributed Dataset — immutable, partitioned collection |\n",
    "| **DataFrame** | Distributed collection with named columns (like a table) |\n",
    "| **Transformation** | Lazy operations (map, filter, join) that define a computation |\n",
    "| **Action** | Operations that trigger execution (collect, count, write) |\n",
    "| **DAG** | Directed Acyclic Graph of stages built from transformations |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f8ba1",
   "metadata": {},
   "source": [
    "---\n",
    "## PySpark Basics — Code Examples\n",
    "\n",
    "Below are foundational PySpark patterns for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchProcessingDemo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ff22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from sample data\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"ORD001\", \"C100\", \"Laptop\", 1, 999.99),\n",
    "    (\"ORD002\", \"C101\", \"Mouse\", 2, 29.99),\n",
    "    (\"ORD003\", \"C100\", \"Keyboard\", 1, 79.99),\n",
    "    (\"ORD004\", \"C102\", \"Monitor\", 2, 299.99),\n",
    "    (\"ORD005\", \"C101\", \"Laptop\", 1, 999.99)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98caf502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations: filter, select, compute\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count\n",
    "\n",
    "# Filter orders with quantity > 1\n",
    "high_qty_orders = df.filter(col(\"quantity\") > 1)\n",
    "print(\"Orders with quantity > 1:\")\n",
    "high_qty_orders.show()\n",
    "\n",
    "# Add a total column\n",
    "df_with_total = df.withColumn(\"total\", col(\"quantity\") * col(\"price\"))\n",
    "print(\"Orders with total:\")\n",
    "df_with_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab424b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations: Group by customer\n",
    "customer_summary = df_with_total.groupBy(\"customer_id\").agg(\n",
    "    count(\"order_id\").alias(\"num_orders\"),\n",
    "    spark_sum(\"total\").alias(\"total_spent\")\n",
    ")\n",
    "\n",
    "print(\"Customer spending summary:\")\n",
    "customer_summary.orderBy(col(\"total_spent\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and Writing Data (common batch patterns)\n",
    "\n",
    "# Read from CSV (example path)\n",
    "# df = spark.read.csv(\"data/orders.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Read from Parquet (columnar, efficient)\n",
    "# df = spark.read.parquet(\"data/orders.parquet\")\n",
    "\n",
    "# Write to Parquet (partitioned by date for efficiency)\n",
    "# df_with_total.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .partitionBy(\"customer_id\") \\\n",
    "#     .parquet(\"output/orders_processed\")\n",
    "\n",
    "print(\"Common I/O patterns shown above (commented for demo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1da49",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch vs Stream Processing Comparison\n",
    "\n",
    "| Dimension | Batch Processing | Stream Processing |\n",
    "|-----------|------------------|-------------------|\n",
    "| **Latency** | Minutes to hours | Milliseconds to seconds |\n",
    "| **Data Scope** | Bounded (finite dataset) | Unbounded (continuous flow) |\n",
    "| **Processing Model** | Process all at once | Process as events arrive |\n",
    "| **Complexity** | Simpler (no state management) | Complex (windowing, watermarks) |\n",
    "| **Fault Tolerance** | Rerun entire job | Checkpointing, exactly-once |\n",
    "| **Use Cases** | ETL, ML training, reports | Real-time alerts, dashboards |\n",
    "| **Tools** | Spark, Hive, Flink (batch) | Kafka Streams, Flink, Spark Streaming |\n",
    "| **Cost** | Often cheaper (scheduled) | Higher (always-on infrastructure) |\n",
    "\n",
    "### When to Choose Each\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  Choose BATCH when:              Choose STREAM when:           │\n",
    "│  • Latency of hours is OK        • Need sub-second response    │\n",
    "│  • Data arrives in bulk          • Data is continuous          │\n",
    "│  • Complex joins/aggregations    • Simple event processing     │\n",
    "│  • Cost optimization matters     • Real-time decisions needed  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4e27d",
   "metadata": {},
   "source": [
    "---\n",
    "## Takeaways\n",
    "\n",
    "1. **Batch processing** handles large volumes of data efficiently when real-time results aren't required\n",
    "\n",
    "2. **Apache Spark** provides a distributed computing framework with:\n",
    "   - Lazy evaluation via transformations\n",
    "   - Optimized execution through DAG scheduling\n",
    "   - Unified APIs for SQL, ML, and graph processing\n",
    "\n",
    "3. **PySpark essentials**:\n",
    "   - `SparkSession` is the entry point\n",
    "   - `DataFrame` API for structured data\n",
    "   - Transformations are lazy; actions trigger execution\n",
    "\n",
    "4. **Batch vs Stream** is a latency and complexity trade-off — use batch for cost-effective, high-volume historical processing\n",
    "\n",
    "5. **Best practices**:\n",
    "   - Use Parquet for efficient columnar storage\n",
    "   - Partition data by common query dimensions\n",
    "   - Cache intermediate results for iterative algorithms"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
