{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9487c66b",
   "metadata": {},
   "source": [
    "# Apache Spark Fundamentals\n",
    "\n",
    "Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general execution graphs.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Spark Architecture](#spark-architecture)\n",
    "2. [Core Abstractions: RDDs, DataFrames, Datasets](#core-abstractions)\n",
    "3. [Transformations and Actions](#transformations-and-actions)\n",
    "4. [Spark SQL](#spark-sql)\n",
    "5. [Performance Tuning](#performance-tuning)\n",
    "6. [Key Takeaways](#key-takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feffcd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Spark Architecture <a id='spark-architecture'></a>\n",
    "\n",
    "Spark follows a **master-worker** architecture with three main components:\n",
    "\n",
    "### 1. Driver Program\n",
    "- The **central coordinator** of a Spark application\n",
    "- Runs the `main()` function and creates the `SparkContext`\n",
    "- Converts user code into tasks and schedules them on executors\n",
    "- Maintains information about the Spark application\n",
    "\n",
    "### 2. Cluster Manager\n",
    "- Responsible for **resource allocation** across the cluster\n",
    "- Types: Standalone, YARN, Mesos, Kubernetes\n",
    "- Negotiates resources between Driver and Worker nodes\n",
    "\n",
    "### 3. Executors\n",
    "- **Worker processes** that run on cluster nodes\n",
    "- Execute tasks assigned by the Driver\n",
    "- Store data in memory/disk for caching\n",
    "- Report task status back to the Driver\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                        SPARK APPLICATION                        │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│    ┌─────────────────┐                                          │\n",
    "│    │     DRIVER      │                                          │\n",
    "│    │  ┌───────────┐  │                                          │\n",
    "│    │  │SparkContext│ │                                          │\n",
    "│    │  └───────────┘  │                                          │\n",
    "│    │  ┌───────────┐  │         ┌──────────────────────┐         │\n",
    "│    │  │ DAG       │  │         │   CLUSTER MANAGER    │         │\n",
    "│    │  │ Scheduler │  │◄───────►│ (YARN/Mesos/K8s)     │         │\n",
    "│    │  └───────────┘  │         └──────────────────────┘         │\n",
    "│    │  ┌───────────┐  │                   │                      │\n",
    "│    │  │Task       │  │                   │                      │\n",
    "│    │  │Scheduler  │  │                   ▼                      │\n",
    "│    │  └───────────┘  │    ┌─────────────────────────────┐       │\n",
    "│    └────────┬────────┘    │         WORKER NODE         │       │\n",
    "│             │             │  ┌───────────────────────┐  │       │\n",
    "│             │             │  │      EXECUTOR         │  │       │\n",
    "│             └────────────►│  │ ┌─────┐ ┌─────┐       │  │       │\n",
    "│                           │  │ │Task1│ │Task2│ ...   │  │       │\n",
    "│                           │  │ └─────┘ └─────┘       │  │       │\n",
    "│                           │  │ ┌─────────────────┐   │  │       │\n",
    "│                           │  │ │  Cache/Storage  │   │  │       │\n",
    "│                           │  │ └─────────────────┘   │  │       │\n",
    "│                           │  └───────────────────────┘  │       │\n",
    "│                           └─────────────────────────────┘       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### DAG (Directed Acyclic Graph)\n",
    "\n",
    "Spark's execution engine uses DAGs to optimize query plans:\n",
    "\n",
    "1. **DAG Construction**: When you call transformations, Spark builds a DAG of stages\n",
    "2. **Stage Division**: DAG is divided into stages at **shuffle boundaries**\n",
    "3. **Task Creation**: Each stage is divided into tasks (one per partition)\n",
    "4. **Optimization**: Catalyst optimizer and Tungsten engine optimize the execution plan\n",
    "\n",
    "```\n",
    "User Code → Logical Plan → Optimized Logical Plan → Physical Plan → DAG → Tasks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b9cf8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Core Abstractions: RDDs, DataFrames, Datasets <a id='core-abstractions'></a>\n",
    "\n",
    "### RDD (Resilient Distributed Dataset)\n",
    "\n",
    "The **fundamental data structure** of Spark - an immutable, distributed collection of objects.\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| **Resilient** | Fault-tolerant via lineage graph |\n",
    "| **Distributed** | Data partitioned across cluster nodes |\n",
    "| **Dataset** | Collection of partitioned data |\n",
    "| **Immutable** | Cannot be changed once created |\n",
    "| **Lazy** | Transformations are not executed until an action is called |\n",
    "\n",
    "### DataFrame\n",
    "\n",
    "A **distributed collection of data organized into named columns** - like a table in a relational database.\n",
    "\n",
    "- Built on top of RDDs with schema information\n",
    "- Optimized by Catalyst query optimizer\n",
    "- API available in Python, Scala, Java, and R\n",
    "- **Preferred for most use cases** due to optimizations\n",
    "\n",
    "### Dataset (Scala/Java only)\n",
    "\n",
    "A **strongly-typed** collection of domain-specific objects.\n",
    "\n",
    "- Combines benefits of RDDs (type-safety) and DataFrames (optimization)\n",
    "- Compile-time type checking\n",
    "- Not available in Python (PySpark uses DataFrames)\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Feature | RDD | DataFrame | Dataset |\n",
    "|---------|-----|-----------|----------|\n",
    "| **Type Safety** | Yes | No | Yes |\n",
    "| **Optimization** | No | Yes (Catalyst) | Yes (Catalyst) |\n",
    "| **Schema** | No | Yes | Yes |\n",
    "| **Python Support** | Yes | Yes | No |\n",
    "| **Serialization** | Java/Kryo | Tungsten | Tungsten |\n",
    "| **Use Case** | Low-level control | SQL-like operations | Type-safe OOP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6218150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession (entry point for Spark 2.0+)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Fundamentals\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"App Name: {sc.appName}\")\n",
    "print(f\"Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDDs\n",
    "\n",
    "# Method 1: From a Python collection (parallelize)\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd_from_list = sc.parallelize(data, numSlices=4)  # 4 partitions\n",
    "\n",
    "print(f\"Number of partitions: {rdd_from_list.getNumPartitions()}\")\n",
    "print(f\"First 5 elements: {rdd_from_list.take(5)}\")\n",
    "\n",
    "# Method 2: From external data source\n",
    "# rdd_from_file = sc.textFile(\"hdfs://path/to/file.txt\")\n",
    "\n",
    "# Method 3: From existing RDD (transformation)\n",
    "rdd_squared = rdd_from_list.map(lambda x: x ** 2)\n",
    "print(f\"Squared values: {rdd_squared.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Method 1: From Python list with schema inference\n",
    "data = [\n",
    "    (\"Alice\", \"Engineering\", 75000),\n",
    "    (\"Bob\", \"Marketing\", 65000),\n",
    "    (\"Charlie\", \"Engineering\", 80000),\n",
    "    (\"Diana\", \"Sales\", 70000),\n",
    "    (\"Eve\", \"Marketing\", 72000)\n",
    "]\n",
    "\n",
    "df_inferred = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])\n",
    "df_inferred.show()\n",
    "df_inferred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: With explicit schema definition\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"department\", StringType(), nullable=True),\n",
    "    StructField(\"salary\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df_explicit = spark.createDataFrame(data, schema)\n",
    "df_explicit.printSchema()\n",
    "\n",
    "# Method 3: From files (CSV, JSON, Parquet, etc.)\n",
    "# df_csv = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "# df_json = spark.read.json(\"path/to/file.json\")\n",
    "# df_parquet = spark.read.parquet(\"path/to/file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf43c1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Transformations and Actions <a id='transformations-and-actions'></a>\n",
    "\n",
    "Spark operations are divided into two categories:\n",
    "\n",
    "### Transformations (Lazy)\n",
    "- Create a **new RDD/DataFrame** from an existing one\n",
    "- **Not executed immediately** - just build the DAG\n",
    "- Two types:\n",
    "  - **Narrow**: Each input partition contributes to one output partition (e.g., `map`, `filter`)\n",
    "  - **Wide**: Input partitions contribute to multiple output partitions (e.g., `groupBy`, `join`) - **requires shuffle**\n",
    "\n",
    "### Actions (Eager)\n",
    "- **Trigger execution** of the DAG\n",
    "- Return results to the driver or write to storage\n",
    "- Examples: `collect()`, `count()`, `save()`\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                    NARROW TRANSFORMATIONS                    │\n",
    "│  ┌─────────┐     ┌─────────┐     ┌─────────┐                │\n",
    "│  │Partition│────►│Partition│────►│Partition│                │\n",
    "│  │    1    │     │    1'   │     │    1''  │                │\n",
    "│  └─────────┘     └─────────┘     └─────────┘                │\n",
    "│                                                              │\n",
    "│  ┌─────────┐     ┌─────────┐     ┌─────────┐                │\n",
    "│  │Partition│────►│Partition│────►│Partition│                │\n",
    "│  │    2    │     │    2'   │     │    2''  │                │\n",
    "│  └─────────┘     └─────────┘     └─────────┘                │\n",
    "│       map()          filter()        map()                   │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                    WIDE TRANSFORMATIONS                      │\n",
    "│  ┌─────────┐                     ┌─────────┐                │\n",
    "│  │Partition│─────────┬──────────►│Partition│                │\n",
    "│  │    1    │         │           │    A    │                │\n",
    "│  └─────────┘         │           └─────────┘                │\n",
    "│                      │  SHUFFLE                              │\n",
    "│  ┌─────────┐         │           ┌─────────┐                │\n",
    "│  │Partition│─────────┴──────────►│Partition│                │\n",
    "│  │    2    │                     │    B    │                │\n",
    "│  └─────────┘                     └─────────┘                │\n",
    "│                  groupByKey()                                │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RDD Transformations Examples\n",
    "# ============================================\n",
    "\n",
    "numbers = sc.parallelize(range(1, 11))\n",
    "\n",
    "# map() - Apply function to each element\n",
    "squared = numbers.map(lambda x: x ** 2)\n",
    "print(f\"map (squared): {squared.collect()}\")\n",
    "\n",
    "# filter() - Keep elements that satisfy condition\n",
    "evens = numbers.filter(lambda x: x % 2 == 0)\n",
    "print(f\"filter (evens): {evens.collect()}\")\n",
    "\n",
    "# flatMap() - Map + flatten results\n",
    "words = sc.parallelize([\"hello world\", \"spark is great\"])\n",
    "split_words = words.flatMap(lambda line: line.split(\" \"))\n",
    "print(f\"flatMap: {split_words.collect()}\")\n",
    "\n",
    "# distinct() - Remove duplicates\n",
    "duplicates = sc.parallelize([1, 1, 2, 2, 3, 3, 3])\n",
    "unique = duplicates.distinct()\n",
    "print(f\"distinct: {unique.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RDD Pair Operations (Key-Value RDDs)\n",
    "# ============================================\n",
    "\n",
    "# Create pair RDD\n",
    "sales = sc.parallelize([\n",
    "    (\"Electronics\", 1000),\n",
    "    (\"Clothing\", 500),\n",
    "    (\"Electronics\", 1500),\n",
    "    (\"Food\", 300),\n",
    "    (\"Clothing\", 700),\n",
    "    (\"Food\", 400)\n",
    "])\n",
    "\n",
    "# reduceByKey() - Aggregate values by key (preferred over groupByKey)\n",
    "total_by_category = sales.reduceByKey(lambda a, b: a + b)\n",
    "print(f\"reduceByKey: {total_by_category.collect()}\")\n",
    "\n",
    "# groupByKey() - Group values by key (creates iterator)\n",
    "grouped = sales.groupByKey().mapValues(list)\n",
    "print(f\"groupByKey: {grouped.collect()}\")\n",
    "\n",
    "# sortByKey() - Sort by key\n",
    "sorted_sales = total_by_category.sortByKey()\n",
    "print(f\"sortByKey: {sorted_sales.collect()}\")\n",
    "\n",
    "# mapValues() - Apply function to values only\n",
    "doubled = sales.mapValues(lambda x: x * 2)\n",
    "print(f\"mapValues: {doubled.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb176a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RDD Actions Examples\n",
    "# ============================================\n",
    "\n",
    "numbers = sc.parallelize(range(1, 11))\n",
    "\n",
    "# collect() - Return all elements to driver (use with caution!)\n",
    "all_data = numbers.collect()\n",
    "print(f\"collect: {all_data}\")\n",
    "\n",
    "# count() - Count number of elements\n",
    "print(f\"count: {numbers.count()}\")\n",
    "\n",
    "# first() - Return first element\n",
    "print(f\"first: {numbers.first()}\")\n",
    "\n",
    "# take(n) - Return first n elements\n",
    "print(f\"take(3): {numbers.take(3)}\")\n",
    "\n",
    "# reduce() - Aggregate all elements\n",
    "total = numbers.reduce(lambda a, b: a + b)\n",
    "print(f\"reduce (sum): {total}\")\n",
    "\n",
    "# aggregate() - More flexible aggregation\n",
    "# (initial_value, seq_op, comb_op)\n",
    "sum_count = numbers.aggregate(\n",
    "    (0, 0),\n",
    "    lambda acc, val: (acc[0] + val, acc[1] + 1),\n",
    "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    ")\n",
    "print(f\"aggregate (sum, count): {sum_count}\")\n",
    "print(f\"average: {sum_count[0] / sum_count[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91782fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DataFrame Transformations\n",
    "# ============================================\n",
    "from pyspark.sql.functions import col, avg, sum as spark_sum, count, when, lit, upper\n",
    "\n",
    "# Sample employee data\n",
    "employees = spark.createDataFrame([\n",
    "    (1, \"Alice\", \"Engineering\", 75000, 5),\n",
    "    (2, \"Bob\", \"Marketing\", 65000, 3),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000, 7),\n",
    "    (4, \"Diana\", \"Sales\", 70000, 4),\n",
    "    (5, \"Eve\", \"Marketing\", 72000, 6),\n",
    "    (6, \"Frank\", \"Engineering\", 90000, 10),\n",
    "    (7, \"Grace\", \"Sales\", 68000, 2)\n",
    "], [\"id\", \"name\", \"department\", \"salary\", \"years_exp\"])\n",
    "\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b333ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select() - Choose columns\n",
    "employees.select(\"name\", \"salary\").show()\n",
    "\n",
    "# selectExpr() - Select with SQL expressions\n",
    "employees.selectExpr(\"name\", \"salary * 1.1 as new_salary\").show()\n",
    "\n",
    "# filter() / where() - Filter rows\n",
    "high_earners = employees.filter(col(\"salary\") > 70000)\n",
    "high_earners.show()\n",
    "\n",
    "# Multiple conditions\n",
    "experienced_engineers = employees.filter(\n",
    "    (col(\"department\") == \"Engineering\") & (col(\"years_exp\") >= 5)\n",
    ")\n",
    "experienced_engineers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff52ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn() - Add or modify columns\n",
    "employees_with_bonus = employees.withColumn(\n",
    "    \"bonus\",\n",
    "    when(col(\"years_exp\") >= 5, col(\"salary\") * 0.15)\n",
    "    .otherwise(col(\"salary\") * 0.10)\n",
    ")\n",
    "employees_with_bonus.show()\n",
    "\n",
    "# drop() - Remove columns\n",
    "employees.drop(\"years_exp\").show()\n",
    "\n",
    "# withColumnRenamed() - Rename column\n",
    "employees.withColumnRenamed(\"salary\", \"annual_salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Aggregations\n",
    "# ============================================\n",
    "\n",
    "# groupBy() with aggregation functions\n",
    "dept_stats = employees.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    spark_sum(\"salary\").alias(\"total_salary\")\n",
    ")\n",
    "dept_stats.show()\n",
    "\n",
    "# orderBy() / sort() - Sort results\n",
    "employees.orderBy(col(\"salary\").desc()).show()\n",
    "\n",
    "# Multiple sort columns\n",
    "employees.orderBy(\"department\", col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80241c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Joins\n",
    "# ============================================\n",
    "\n",
    "# Department details\n",
    "departments = spark.createDataFrame([\n",
    "    (\"Engineering\", \"Building A\", 100),\n",
    "    (\"Marketing\", \"Building B\", 50),\n",
    "    (\"Sales\", \"Building C\", 75),\n",
    "    (\"HR\", \"Building D\", 25)\n",
    "], [\"dept_name\", \"location\", \"budget_k\"])\n",
    "\n",
    "# Inner Join (default)\n",
    "joined = employees.join(\n",
    "    departments,\n",
    "    employees.department == departments.dept_name,\n",
    "    \"inner\"\n",
    ")\n",
    "joined.select(\"name\", \"department\", \"location\").show()\n",
    "\n",
    "# Left Outer Join\n",
    "left_joined = employees.join(\n",
    "    departments,\n",
    "    employees.department == departments.dept_name,\n",
    "    \"left_outer\"\n",
    ")\n",
    "left_joined.select(\"name\", \"department\", \"location\").show()\n",
    "\n",
    "# Join types: inner, left_outer, right_outer, full_outer, cross, semi, anti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a474e17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Spark SQL <a id='spark-sql'></a>\n",
    "\n",
    "Spark SQL allows you to query structured data using SQL syntax. It provides:\n",
    "\n",
    "- **Seamless integration** between SQL and DataFrame API\n",
    "- **Catalyst Optimizer** for query optimization\n",
    "- **Unified data access** across various data sources\n",
    "- **Hive compatibility** for existing Hive workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Spark SQL Examples\n",
    "# ============================================\n",
    "\n",
    "# Register DataFrame as temporary view\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "departments.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "# Basic SQL query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 70000\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation query\n",
    "dept_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as emp_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MAX(salary) as max_salary,\n",
    "        MIN(salary) as min_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "dept_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98156781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join query\n",
    "joined_data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.location,\n",
    "        d.budget_k\n",
    "    FROM employees e\n",
    "    LEFT JOIN departments d ON e.department = d.dept_name\n",
    "    ORDER BY e.salary DESC\n",
    "\"\"\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf28336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions\n",
    "window_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank,\n",
    "        AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,\n",
    "        salary - AVG(salary) OVER (PARTITION BY department) as salary_diff_from_avg\n",
    "    FROM employees\n",
    "    ORDER BY department, dept_rank\n",
    "\"\"\")\n",
    "window_query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subquery and CTE (Common Table Expression)\n",
    "cte_query = spark.sql(\"\"\"\n",
    "    WITH dept_avg AS (\n",
    "        SELECT department, AVG(salary) as avg_sal\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    )\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        ROUND(da.avg_sal, 2) as dept_avg,\n",
    "        CASE \n",
    "            WHEN e.salary > da.avg_sal THEN 'Above Average'\n",
    "            ELSE 'Below Average'\n",
    "        END as salary_category\n",
    "    FROM employees e\n",
    "    JOIN dept_avg da ON e.department = da.department\n",
    "    ORDER BY e.department, e.salary DESC\n",
    "\"\"\")\n",
    "cte_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61d83f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Performance Tuning <a id='performance-tuning'></a>\n",
    "\n",
    "### Key Performance Concepts\n",
    "\n",
    "#### 1. Partitioning\n",
    "- **Number of partitions** affects parallelism\n",
    "- Rule of thumb: 2-4 partitions per CPU core\n",
    "- Use `repartition()` to increase or `coalesce()` to decrease partitions\n",
    "\n",
    "#### 2. Caching and Persistence\n",
    "- Cache frequently accessed data to avoid recomputation\n",
    "- Choose appropriate storage level based on memory availability\n",
    "\n",
    "#### 3. Broadcast Variables\n",
    "- Send small read-only data to all executors\n",
    "- Useful for small lookup tables in joins\n",
    "\n",
    "#### 4. Avoid Shuffles\n",
    "- Shuffles are expensive (network I/O, disk I/O)\n",
    "- Use `reduceByKey()` instead of `groupByKey()`\n",
    "- Use broadcast joins for small tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c16f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Partitioning\n",
    "# ============================================\n",
    "\n",
    "rdd = sc.parallelize(range(100), 10)  # 10 partitions\n",
    "print(f\"Initial partitions: {rdd.getNumPartitions()}\")\n",
    "\n",
    "# Increase partitions (triggers shuffle)\n",
    "rdd_repartitioned = rdd.repartition(20)\n",
    "print(f\"After repartition(20): {rdd_repartitioned.getNumPartitions()}\")\n",
    "\n",
    "# Decrease partitions (no shuffle - more efficient)\n",
    "rdd_coalesced = rdd.coalesce(5)\n",
    "print(f\"After coalesce(5): {rdd_coalesced.getNumPartitions()}\")\n",
    "\n",
    "# DataFrame partitioning\n",
    "print(f\"\\nDataFrame partitions: {employees.rdd.getNumPartitions()}\")\n",
    "df_repartitioned = employees.repartition(4, \"department\")  # Partition by column\n",
    "print(f\"After repartition by department: {df_repartitioned.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83587964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Caching and Persistence\n",
    "# ============================================\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Cache in memory (shorthand for persist(MEMORY_AND_DISK))\n",
    "employees.cache()\n",
    "\n",
    "# Trigger caching with an action\n",
    "employees.count()\n",
    "\n",
    "# Check if cached\n",
    "print(f\"Is cached: {employees.is_cached}\")\n",
    "\n",
    "# Persist with specific storage level\n",
    "# StorageLevel options:\n",
    "# - MEMORY_ONLY: Store as deserialized Java objects in memory\n",
    "# - MEMORY_AND_DISK: Spill to disk if memory is insufficient\n",
    "# - MEMORY_ONLY_SER: Store as serialized objects (more space-efficient)\n",
    "# - DISK_ONLY: Store only on disk\n",
    "# - MEMORY_AND_DISK_SER: Serialized objects with disk spillover\n",
    "\n",
    "# Example: persist with serialization\n",
    "# df.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "# Unpersist to free memory\n",
    "employees.unpersist()\n",
    "print(f\"Is cached after unpersist: {employees.is_cached}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b666d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Broadcast Variables\n",
    "# ============================================\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# RDD broadcast example\n",
    "lookup_dict = {\"Engineering\": \"ENG\", \"Marketing\": \"MKT\", \"Sales\": \"SAL\"}\n",
    "broadcast_lookup = sc.broadcast(lookup_dict)\n",
    "\n",
    "# Use broadcast variable in transformation\n",
    "def map_department(emp):\n",
    "    dept_code = broadcast_lookup.value.get(emp[2], \"UNK\")\n",
    "    return (emp[0], emp[1], emp[2], dept_code, emp[3])\n",
    "\n",
    "emp_rdd = employees.rdd\n",
    "result = emp_rdd.map(map_department).collect()[:3]\n",
    "print(\"With broadcast lookup:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78378886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame broadcast join (for small tables)\n",
    "# This avoids shuffling the large table\n",
    "\n",
    "# Hint Spark to broadcast the smaller table\n",
    "broadcast_join = employees.join(\n",
    "    broadcast(departments),\n",
    "    employees.department == departments.dept_name\n",
    ")\n",
    "\n",
    "# Explain the query plan\n",
    "print(\"Query Plan with Broadcast Join:\")\n",
    "broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Performance Configuration Tips\n",
    "# ============================================\n",
    "\n",
    "# Important Spark configurations for performance:\n",
    "spark_configs = {\n",
    "    \"spark.sql.shuffle.partitions\": \"Default 200, reduce for smaller datasets\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\": \"Max size (bytes) for auto-broadcast, default 10MB\",\n",
    "    \"spark.default.parallelism\": \"Default parallelism for RDD operations\",\n",
    "    \"spark.executor.memory\": \"Memory per executor (e.g., '4g')\",\n",
    "    \"spark.executor.cores\": \"Cores per executor\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"Enable dynamic executor allocation\",\n",
    "    \"spark.sql.adaptive.enabled\": \"Enable Adaptive Query Execution (AQE)\",\n",
    "    \"spark.serializer\": \"Use 'org.apache.spark.serializer.KryoSerializer' for better performance\"\n",
    "}\n",
    "\n",
    "for config, description in spark_configs.items():\n",
    "    current_value = spark.conf.get(config, \"Not set\")\n",
    "    print(f\"{config}:\")\n",
    "    print(f\"  Current: {current_value}\")\n",
    "    print(f\"  Tip: {description}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Query Execution Plans\n",
    "# ============================================\n",
    "\n",
    "# Understanding explain() output\n",
    "complex_query = employees.filter(col(\"salary\") > 70000) \\\n",
    "    .groupBy(\"department\") \\\n",
    "    .agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "\n",
    "# Simple explain\n",
    "print(\"=== Simple Explain ===\")\n",
    "complex_query.explain()\n",
    "\n",
    "# Extended explain (includes logical plans)\n",
    "print(\"\\n=== Extended Explain ===\")\n",
    "complex_query.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191c553",
   "metadata": {},
   "source": [
    "### Performance Best Practices Summary\n",
    "\n",
    "| Area | Best Practice |\n",
    "|------|---------------|\n",
    "| **Partitioning** | 2-4 partitions per core; use `coalesce()` for reducing |\n",
    "| **Caching** | Cache iteratively used DataFrames; use appropriate storage level |\n",
    "| **Joins** | Broadcast small tables; filter before joining |\n",
    "| **Shuffles** | Minimize; use `reduceByKey` over `groupByKey` |\n",
    "| **Serialization** | Use Kryo serializer for better performance |\n",
    "| **Data Format** | Use columnar formats (Parquet, ORC) |\n",
    "| **Filtering** | Push filters as early as possible (predicate pushdown) |\n",
    "| **AQE** | Enable Adaptive Query Execution in Spark 3.0+ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b06640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715b026",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways <a id='key-takeaways'></a>\n",
    "\n",
    "### Architecture\n",
    "- Spark uses a **master-worker architecture** with Driver, Cluster Manager, and Executors\n",
    "- **DAG execution model** enables optimization and fault tolerance\n",
    "- Jobs are divided into **stages** (at shuffle boundaries) and **tasks** (per partition)\n",
    "\n",
    "### Data Abstractions\n",
    "- **RDDs**: Low-level, unstructured, full control but no optimization\n",
    "- **DataFrames**: Structured, optimized by Catalyst, preferred for most use cases\n",
    "- **Datasets**: Type-safe DataFrames (Scala/Java only)\n",
    "\n",
    "### Operations\n",
    "- **Transformations are lazy** - they build the DAG but don't execute\n",
    "- **Actions trigger execution** - `collect()`, `count()`, `save()`, etc.\n",
    "- **Narrow transformations** (map, filter) don't require shuffle\n",
    "- **Wide transformations** (groupBy, join) require shuffle - expensive!\n",
    "\n",
    "### Spark SQL\n",
    "- Provides **SQL interface** to structured data\n",
    "- Supports **joins, aggregations, window functions, CTEs**\n",
    "- Seamlessly integrates with DataFrame API\n",
    "\n",
    "### Performance\n",
    "- **Partition data appropriately** for parallelism\n",
    "- **Cache frequently accessed data** to avoid recomputation\n",
    "- **Use broadcast joins** for small lookup tables\n",
    "- **Minimize shuffles** - they are the biggest performance bottleneck\n",
    "- **Use columnar formats** (Parquet, ORC) for storage efficiency\n",
    "- **Enable AQE** (Adaptive Query Execution) in Spark 3.0+\n",
    "\n",
    "---\n",
    "\n",
    "### Further Learning Resources\n",
    "- [Apache Spark Official Documentation](https://spark.apache.org/docs/latest/)\n",
    "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [Spark Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)\n",
    "- [Databricks Spark Knowledge Base](https://kb.databricks.com/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
