{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3d1d19",
   "metadata": {},
   "source": [
    "# Data Lake File Formats\n",
    "\n",
    "Understanding file formats is crucial for building efficient data lakes. The choice of format impacts:\n",
    "- **Query performance** - How fast can you read specific columns or rows?\n",
    "- **Storage costs** - How well does the format compress?\n",
    "- **Schema evolution** - Can you add/remove fields without breaking pipelines?\n",
    "- **Interoperability** - Which tools and engines support the format?\n",
    "\n",
    "This notebook compares the most common data lake file formats and provides practical guidance for selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4122c7",
   "metadata": {},
   "source": [
    "## 1. File Format Comparison\n",
    "\n",
    "| Format | Type | Schema | Compression | Best For |\n",
    "|--------|------|--------|-------------|----------|\n",
    "| **Parquet** | Columnar | Embedded | Snappy, Gzip, Zstd, LZ4 | Analytics, OLAP queries |\n",
    "| **ORC** | Columnar | Embedded | Zlib, Snappy, LZO, Zstd | Hive ecosystem, analytics |\n",
    "| **Avro** | Row-based | Embedded (JSON) | Snappy, Deflate, Bzip2 | Streaming, Kafka, schema evolution |\n",
    "| **JSON** | Row-based | Self-describing | Gzip, external | APIs, logs, flexibility |\n",
    "| **CSV** | Row-based | None | Gzip, external | Simple interchange, legacy systems |\n",
    "\n",
    "### Detailed Format Breakdown\n",
    "\n",
    "#### Parquet\n",
    "- **Origin**: Apache project, created by Twitter and Cloudera\n",
    "- **Structure**: Columnar with row groups, column chunks, and pages\n",
    "- **Strengths**: Excellent compression ratios, predicate pushdown, column pruning\n",
    "- **Ecosystem**: Spark, Pandas, Dask, Presto, Athena, BigQuery\n",
    "\n",
    "#### ORC (Optimized Row Columnar)\n",
    "- **Origin**: Apache Hive project\n",
    "- **Structure**: Columnar with stripes, row indexes, and bloom filters\n",
    "- **Strengths**: ACID support in Hive, built-in indexes, excellent for Hive\n",
    "- **Ecosystem**: Hive, Presto, Spark (with some limitations)\n",
    "\n",
    "#### Avro\n",
    "- **Origin**: Apache project, developed for Hadoop\n",
    "- **Structure**: Row-based with schema stored in header\n",
    "- **Strengths**: Schema evolution, compact binary format, RPC support\n",
    "- **Ecosystem**: Kafka, Spark, Flink, schema registries\n",
    "\n",
    "#### JSON (JavaScript Object Notation)\n",
    "- **Structure**: Human-readable, self-describing, nested support\n",
    "- **Strengths**: Universal support, flexible schema, debugging-friendly\n",
    "- **Weaknesses**: Verbose, slow parsing, no native compression\n",
    "\n",
    "#### CSV (Comma-Separated Values)\n",
    "- **Structure**: Plain text, delimiter-separated\n",
    "- **Strengths**: Universal compatibility, human-readable\n",
    "- **Weaknesses**: No schema, no types, poor compression, escaping issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cf0df",
   "metadata": {},
   "source": [
    "## 2. Column vs Row-Based Storage\n",
    "\n",
    "Understanding the fundamental difference between columnar and row-based storage is essential.\n",
    "\n",
    "### Row-Based Storage (CSV, JSON, Avro)\n",
    "\n",
    "```\n",
    "Row 1: [id=1, name=\"Alice\", age=30, salary=75000]\n",
    "Row 2: [id=2, name=\"Bob\",   age=25, salary=65000]\n",
    "Row 3: [id=3, name=\"Carol\", age=35, salary=85000]\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Fast writes (append entire rows)\n",
    "- ✅ Efficient for `SELECT *` queries\n",
    "- ✅ Better for OLTP workloads\n",
    "- ✅ Natural for streaming data\n",
    "\n",
    "**Disadvantages:**\n",
    "- ❌ Must read entire row even for single column\n",
    "- ❌ Poor compression (mixed data types)\n",
    "- ❌ Slow aggregations across large datasets\n",
    "\n",
    "---\n",
    "\n",
    "### Columnar Storage (Parquet, ORC)\n",
    "\n",
    "```\n",
    "Column 'id':     [1, 2, 3]\n",
    "Column 'name':   [\"Alice\", \"Bob\", \"Carol\"]\n",
    "Column 'age':    [30, 25, 35]\n",
    "Column 'salary': [75000, 65000, 85000]\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Read only needed columns (column pruning)\n",
    "- ✅ Excellent compression (similar values together)\n",
    "- ✅ Fast aggregations (SUM, AVG, COUNT)\n",
    "- ✅ Predicate pushdown support\n",
    "\n",
    "**Disadvantages:**\n",
    "- ❌ Slower writes (must organize by column)\n",
    "- ❌ Inefficient for `SELECT *` on few rows\n",
    "- ❌ More complex file structure\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Use Case | Recommended Format |\n",
    "|----------|--------------------|\n",
    "| Analytics/BI queries | Columnar (Parquet/ORC) |\n",
    "| Data warehouse tables | Columnar (Parquet/ORC) |\n",
    "| Streaming ingestion | Row-based (Avro/JSON) |\n",
    "| Log processing | Row-based → Columnar |\n",
    "| Machine learning features | Columnar (Parquet) |\n",
    "| API responses | JSON |\n",
    "| Data exchange | CSV/JSON (compatibility) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d474d",
   "metadata": {},
   "source": [
    "## 3. Compression Codecs\n",
    "\n",
    "Compression reduces storage costs and can improve read performance by reducing I/O.\n",
    "\n",
    "### Codec Comparison\n",
    "\n",
    "| Codec | Compression Ratio | Speed | CPU Usage | Best For |\n",
    "|-------|-------------------|-------|-----------|----------|\n",
    "| **Snappy** | Medium | Very Fast | Low | Hot data, interactive queries |\n",
    "| **LZ4** | Medium | Very Fast | Low | Similar to Snappy, slightly faster |\n",
    "| **Gzip** | High | Slow | High | Cold storage, archival |\n",
    "| **Zstd** | High | Fast | Medium | Best balance of ratio vs speed |\n",
    "| **Brotli** | Very High | Very Slow | Very High | Web assets, rarely for data lakes |\n",
    "\n",
    "### Compression Levels\n",
    "\n",
    "Most codecs support compression levels:\n",
    "\n",
    "```\n",
    "Zstd Levels:\n",
    "  Level 1:  Fast compression, lower ratio\n",
    "  Level 3:  Default, good balance\n",
    "  Level 9:  Slower, better ratio\n",
    "  Level 19: Maximum compression\n",
    "\n",
    "Gzip Levels:\n",
    "  Level 1:  Fastest\n",
    "  Level 6:  Default\n",
    "  Level 9:  Best compression\n",
    "```\n",
    "\n",
    "### Recommendations by Use Case\n",
    "\n",
    "| Scenario | Recommended Codec |\n",
    "|----------|-------------------|\n",
    "| Interactive queries | Snappy or LZ4 |\n",
    "| Batch processing | Zstd (level 3) |\n",
    "| Long-term archival | Gzip or Zstd (high level) |\n",
    "| Streaming pipelines | Snappy (low latency) |\n",
    "| Storage-constrained | Zstd (level 9+) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123975c1",
   "metadata": {},
   "source": [
    "## 4. Python: Reading and Writing Parquet\n",
    "\n",
    "Parquet is the de facto standard for data lakes. Let's explore how to work with it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88574d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed416c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "n_rows = 10000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_rows + 1),\n",
    "    'customer_id': np.random.randint(1, 1000, n_rows),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], n_rows),\n",
    "    'amount': np.round(np.random.uniform(10, 500, n_rows), 2),\n",
    "    'transaction_date': [datetime(2024, 1, 1) + timedelta(days=int(x)) for x in np.random.randint(0, 365, n_rows)],\n",
    "    'is_online': np.random.choice([True, False], n_rows)\n",
    "})\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16971287",
   "metadata": {},
   "source": [
    "### 4.1 Writing Parquet with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29362c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple write with pandas (uses pyarrow by default)\n",
    "output_dir = 'sample_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Basic write\n",
    "df.to_parquet(f'{output_dir}/transactions_basic.parquet')\n",
    "\n",
    "# Write with compression options\n",
    "df.to_parquet(f'{output_dir}/transactions_snappy.parquet', compression='snappy')\n",
    "df.to_parquet(f'{output_dir}/transactions_gzip.parquet', compression='gzip')\n",
    "df.to_parquet(f'{output_dir}/transactions_zstd.parquet', compression='zstd')\n",
    "\n",
    "# Compare file sizes\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.endswith('.parquet'):\n",
    "        size = os.path.getsize(f'{output_dir}/{filename}')\n",
    "        print(f\"{filename}: {size:,} bytes ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b07c65",
   "metadata": {},
   "source": [
    "### 4.2 Writing Parquet with PyArrow (More Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfea677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write with detailed options\n",
    "pq.write_table(\n",
    "    table,\n",
    "    f'{output_dir}/transactions_advanced.parquet',\n",
    "    compression='zstd',\n",
    "    compression_level=3,  # Zstd compression level\n",
    "    row_group_size=5000,  # Rows per row group\n",
    "    use_dictionary=True,  # Dictionary encoding for strings\n",
    "    write_statistics=True  # Column statistics for predicate pushdown\n",
    ")\n",
    "\n",
    "print(\"Advanced parquet file written successfully!\")\n",
    "\n",
    "# View schema\n",
    "print(f\"\\nSchema:\\n{table.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9fd85",
   "metadata": {},
   "source": [
    "### 4.3 Partitioned Parquet (Common in Data Lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72334224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add partition columns\n",
    "df['year'] = df['transaction_date'].dt.year\n",
    "df['month'] = df['transaction_date'].dt.month\n",
    "\n",
    "# Write partitioned dataset\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "pq.write_to_dataset(\n",
    "    table,\n",
    "    root_path=f'{output_dir}/transactions_partitioned',\n",
    "    partition_cols=['year', 'month'],\n",
    "    compression='snappy'\n",
    ")\n",
    "\n",
    "# Show partition structure\n",
    "print(\"Partition structure created:\")\n",
    "for root, dirs, files in os.walk(f'{output_dir}/transactions_partitioned'):\n",
    "    level = root.replace(f'{output_dir}/transactions_partitioned', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:2]:  # Show first 2 files per directory\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c078c50",
   "metadata": {},
   "source": [
    "### 4.4 Reading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac39011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple read with pandas\n",
    "df_read = pd.read_parquet(f'{output_dir}/transactions_basic.parquet')\n",
    "print(f\"Read {len(df_read):,} rows\")\n",
    "\n",
    "# Read specific columns only (column pruning)\n",
    "df_subset = pd.read_parquet(\n",
    "    f'{output_dir}/transactions_basic.parquet',\n",
    "    columns=['customer_id', 'amount', 'product_category']\n",
    ")\n",
    "print(f\"\\nSubset columns: {df_subset.columns.tolist()}\")\n",
    "\n",
    "# Read with filters (predicate pushdown with pyarrow)\n",
    "df_filtered = pd.read_parquet(\n",
    "    f'{output_dir}/transactions_basic.parquet',\n",
    "    filters=[('product_category', '==', 'Electronics')]\n",
    ")\n",
    "print(f\"\\nFiltered rows (Electronics only): {len(df_filtered):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72868d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partitioned dataset\n",
    "df_partitioned = pd.read_parquet(f'{output_dir}/transactions_partitioned')\n",
    "print(f\"Read partitioned dataset: {len(df_partitioned):,} rows\")\n",
    "\n",
    "# Read specific partition only\n",
    "df_jan = pd.read_parquet(\n",
    "    f'{output_dir}/transactions_partitioned',\n",
    "    filters=[('year', '==', 2024), ('month', '==', 1)]\n",
    ")\n",
    "print(f\"January 2024 only: {len(df_jan):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99efa64",
   "metadata": {},
   "source": [
    "### 4.5 Inspecting Parquet Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file metadata (without loading data)\n",
    "parquet_file = pq.ParquetFile(f'{output_dir}/transactions_advanced.parquet')\n",
    "\n",
    "print(\"=== Parquet File Metadata ===\")\n",
    "print(f\"Number of row groups: {parquet_file.metadata.num_row_groups}\")\n",
    "print(f\"Number of columns: {parquet_file.metadata.num_columns}\")\n",
    "print(f\"Number of rows: {parquet_file.metadata.num_rows:,}\")\n",
    "print(f\"Created by: {parquet_file.metadata.created_by}\")\n",
    "\n",
    "print(\"\\n=== Schema ===\")\n",
    "print(parquet_file.schema_arrow)\n",
    "\n",
    "print(\"\\n=== Row Group 0 Info ===\")\n",
    "row_group = parquet_file.metadata.row_group(0)\n",
    "print(f\"Rows: {row_group.num_rows:,}\")\n",
    "print(f\"Total byte size: {row_group.total_byte_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02105aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-level statistics (useful for query optimization)\n",
    "print(\"=== Column Statistics ===\")\n",
    "for i in range(parquet_file.metadata.num_columns):\n",
    "    col = parquet_file.metadata.row_group(0).column(i)\n",
    "    print(f\"\\nColumn: {col.path_in_schema}\")\n",
    "    print(f\"  Compression: {col.compression}\")\n",
    "    print(f\"  Encodings: {col.encodings}\")\n",
    "    print(f\"  Total compressed size: {col.total_compressed_size:,} bytes\")\n",
    "    if col.statistics:\n",
    "        print(f\"  Has nulls: {col.statistics.has_null_count}\")\n",
    "        print(f\"  Distinct count: {col.statistics.distinct_count if col.statistics.has_distinct_count else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup sample files\n",
    "import shutil\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "    print(\"Sample data cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824f000",
   "metadata": {},
   "source": [
    "## 5. Format Selection Guide\n",
    "\n",
    "Use this decision tree to select the right file format:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    FORMAT SELECTION GUIDE                       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "              ┌───────────────────────────────┐\n",
    "              │  What is your primary use?    │\n",
    "              └───────────────────────────────┘\n",
    "                    │                │\n",
    "            Analytics/BI      Streaming/Events\n",
    "                    │                │\n",
    "                    ▼                ▼\n",
    "              ┌─────────┐      ┌─────────┐\n",
    "              │ PARQUET │      │  AVRO   │\n",
    "              └─────────┘      └─────────┘\n",
    "                    │                │\n",
    "                    ▼                ▼\n",
    "              Best for:        Best for:\n",
    "              • Data lakes     • Kafka events\n",
    "              • Spark/Presto   • Schema registry\n",
    "              • Athena/BQ      • Change data capture\n",
    "```\n",
    "\n",
    "### Quick Reference Matrix\n",
    "\n",
    "| Requirement | Best Format | Compression |\n",
    "|-------------|-------------|-------------|\n",
    "| Data warehouse/lake storage | Parquet | Zstd |\n",
    "| Hive-based analytics | ORC | Zlib |\n",
    "| Kafka streaming | Avro | Snappy |\n",
    "| API data exchange | JSON | Gzip |\n",
    "| Legacy system integration | CSV | Gzip |\n",
    "| Schema evolution needed | Avro | Snappy |\n",
    "| Maximum query performance | Parquet | Snappy |\n",
    "| Minimum storage cost | Parquet | Zstd (high) |\n",
    "| Human debugging | JSON | None |\n",
    "\n",
    "### Migration Path\n",
    "\n",
    "Common pattern in data lakes:\n",
    "\n",
    "```\n",
    "Raw Zone (Landing)     →    Curated Zone (Processed)    →    Consumption Zone\n",
    "─────────────────────       ───────────────────────         ──────────────────\n",
    "JSON/CSV/Avro               Parquet (partitioned)            Parquet (optimized)\n",
    "As-is from source           Cleaned, typed                   Aggregated, denormalized\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c874c",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "### Format Selection\n",
    "\n",
    "| Format | Choose When |\n",
    "|--------|-------------|\n",
    "| **Parquet** | Default for data lakes, analytics, and ML pipelines |\n",
    "| **ORC** | Hive-centric environments with ACID requirements |\n",
    "| **Avro** | Streaming with schema evolution (Kafka) |\n",
    "| **JSON** | APIs, logs, or when human readability matters |\n",
    "| **CSV** | Legacy systems or simple data exchange |\n",
    "\n",
    "### Storage Layout\n",
    "\n",
    "- **Columnar** (Parquet/ORC): Analytics queries, aggregations, column-specific access\n",
    "- **Row-based** (Avro/JSON/CSV): Streaming, full-row access, simple writes\n",
    "\n",
    "### Compression Strategy\n",
    "\n",
    "- **Snappy/LZ4**: Interactive queries, low latency\n",
    "- **Zstd**: Best overall balance (use level 3 as default)\n",
    "- **Gzip**: Cold storage, archival data\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Partition wisely** - Use date/time or high-cardinality columns\n",
    "2. **Right-size files** - Target 128MB-1GB per file for optimal parallelism\n",
    "3. **Enable statistics** - Helps query engines with predicate pushdown\n",
    "4. **Use dictionary encoding** - Excellent for low-cardinality string columns\n",
    "5. **Consider schema evolution** - Avro for frequent changes, Parquet for stability\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- ❌ Using CSV for large-scale analytics (slow, no types)\n",
    "- ❌ Over-partitioning (too many small files)\n",
    "- ❌ Ignoring compression (wastes storage and I/O)\n",
    "- ❌ Using JSON for large datasets (verbose, slow)\n",
    "- ❌ Not considering query patterns when choosing format"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
