{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33e1f45",
   "metadata": {},
   "source": [
    "# Modern Table Formats: Delta Lake, Iceberg, Hudi\n",
    "\n",
    "Modern data lakes have evolved beyond simple file storage to support **ACID transactions**, **schema evolution**, and **time travel**. This notebook explores three leading open table formats that bring data warehouse-like reliability to data lakes.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [The Problem: ACID on Data Lakes](#acid-problem)\n",
    "2. [Delta Lake](#delta-lake)\n",
    "3. [Apache Iceberg](#apache-iceberg)\n",
    "4. [Apache Hudi](#apache-hudi)\n",
    "5. [Comparison of Table Formats](#comparison)\n",
    "6. [Key Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e475b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Problem: ACID on Data Lakes <a id=\"acid-problem\"></a>\n",
    "\n",
    "### Traditional Data Lake Challenges\n",
    "\n",
    "Traditional data lakes (storing raw Parquet, ORC, or JSON files) suffer from several critical limitations:\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| **No ACID Transactions** | Concurrent writes can corrupt data; partial failures leave inconsistent state |\n",
    "| **No Schema Enforcement** | Schema drift leads to data quality issues |\n",
    "| **No Time Travel** | Cannot query historical versions of data |\n",
    "| **Inefficient Updates/Deletes** | Entire partitions must be rewritten for a single row change |\n",
    "| **Small File Problem** | Streaming workloads create many small files, degrading read performance |\n",
    "\n",
    "### ACID Properties Explained\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        ACID PROPERTIES                             │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  A - Atomicity     │ All operations succeed or all fail together   │\n",
    "│  C - Consistency   │ Data remains valid after transaction          │\n",
    "│  I - Isolation     │ Concurrent transactions don't interfere       │\n",
    "│  D - Durability    │ Committed changes persist despite failures    │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### How Table Formats Solve This\n",
    "\n",
    "Modern table formats add a **metadata layer** on top of data files:\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│                   Query Engine                       │\n",
    "│            (Spark, Trino, Flink, etc.)              │\n",
    "└───────────────────────┬──────────────────────────────┘\n",
    "                        │\n",
    "┌───────────────────────▼──────────────────────────────┐\n",
    "│              Table Format Layer                      │\n",
    "│       (Delta Lake / Iceberg / Hudi)                 │\n",
    "│  • Transaction Log    • Schema Management           │\n",
    "│  • Version Control    • File Statistics             │\n",
    "└───────────────────────┬──────────────────────────────┘\n",
    "                        │\n",
    "┌───────────────────────▼──────────────────────────────┐\n",
    "│              Data Files (Parquet/ORC)                │\n",
    "│                 on Object Storage                    │\n",
    "│            (S3, ADLS, GCS, HDFS)                    │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b98544",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Delta Lake <a id=\"delta-lake\"></a>\n",
    "\n",
    "**Delta Lake** is an open-source storage layer developed by Databricks that brings ACID transactions to Apache Spark and big data workloads.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **ACID Transactions** | Serializable isolation; concurrent reads/writes safely handled |\n",
    "| **Time Travel** | Query any historical version using version numbers or timestamps |\n",
    "| **Schema Evolution** | Add, rename, or reorder columns without rewriting data |\n",
    "| **Schema Enforcement** | Prevent bad data from entering the table |\n",
    "| **MERGE (Upserts)** | Efficient insert, update, delete in a single operation |\n",
    "| **Z-Ordering** | Multi-dimensional clustering for faster queries |\n",
    "| **Change Data Feed** | Track row-level changes for CDC pipelines |\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Delta Table Structure:\n",
    "──────────────────────────────────────────\n",
    "my_delta_table/\n",
    "├── _delta_log/                  # Transaction log\n",
    "│   ├── 00000000000000000000.json\n",
    "│   ├── 00000000000000000001.json\n",
    "│   ├── 00000000000000000002.json\n",
    "│   └── 00000000000000000010.checkpoint.parquet\n",
    "├── part-00000-xxx.parquet       # Data files\n",
    "├── part-00001-xxx.parquet\n",
    "└── part-00002-xxx.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Lake Example with PySpark\n",
    "# Note: Requires PySpark with delta-spark package\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Initialize Spark with Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeDemo\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d284eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Delta Table\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000),\n",
    "    (2, \"Bob\", \"Marketing\", 65000),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write as Delta format\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/employees_delta\")\n",
    "\n",
    "print(\"Delta table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0215a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel - Query Historical Versions\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Make some updates to create versions\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/employees_delta\")\n",
    "\n",
    "# Update: Give Engineering a raise\n",
    "delta_table.update(\n",
    "    condition=\"department = 'Engineering'\",\n",
    "    set={\"salary\": \"salary * 1.1\"}\n",
    ")\n",
    "\n",
    "# Query current version\n",
    "print(\"Current Version:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/employees_delta\").show()\n",
    "\n",
    "# Query previous version (version 0)\n",
    "print(\"Version 0 (Original):\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/employees_delta\").show()\n",
    "\n",
    "# Query by timestamp\n",
    "# spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-15\").load(\"/tmp/employees_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d808aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE Operation (Upsert)\n",
    "\n",
    "# New/updated employee data\n",
    "updates = [\n",
    "    (2, \"Bob\", \"Sales\", 70000),      # Update: Bob moved to Sales\n",
    "    (4, \"Diana\", \"Engineering\", 85000),  # Insert: New employee\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates, columns)\n",
    "\n",
    "# Perform MERGE operation\n",
    "delta_table.alias(\"target\").merge(\n",
    "    updates_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "print(\"After MERGE:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/employees_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Evolution\n",
    "\n",
    "# Add a new column by enabling schema evolution\n",
    "new_data = [\n",
    "    (5, \"Eve\", \"HR\", 60000, \"2024-01-15\"),  # New column: hire_date\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, columns + [\"hire_date\"])\n",
    "\n",
    "# Write with schema merge enabled\n",
    "new_df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"/tmp/employees_delta\")\n",
    "\n",
    "print(\"Schema after evolution:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/employees_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Table History\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/employees_delta\")\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2c04f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Apache Iceberg <a id=\"apache-iceberg\"></a>\n",
    "\n",
    "**Apache Iceberg** is an open table format originally developed at Netflix for massive-scale analytics. It's designed for **engine-agnostic** usage and **hidden partitioning**.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Hidden Partitioning** | Users don't need to know partition columns; Iceberg handles it |\n",
    "| **Partition Evolution** | Change partitioning scheme without rewriting data |\n",
    "| **Schema Evolution** | Full schema evolution with column ID tracking |\n",
    "| **Time Travel** | Query snapshots by ID or timestamp |\n",
    "| **Engine Agnostic** | Works with Spark, Flink, Trino, Dremio, and more |\n",
    "| **Branching & Tagging** | Git-like data management (experimental) |\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Iceberg Table Structure:\n",
    "──────────────────────────────────────────\n",
    "Catalog (e.g., Hive Metastore, AWS Glue)\n",
    "    │\n",
    "    └── Points to → metadata/v1.metadata.json (current version pointer)\n",
    "                            │\n",
    "┌───────────────────────────┴────────────────────────────┐\n",
    "│                   Metadata Layer                       │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  metadata/                                             │\n",
    "│  ├── v1.metadata.json      # Table metadata           │\n",
    "│  ├── v2.metadata.json      # Updated metadata         │\n",
    "│  ├── snap-xxx.avro         # Snapshot manifests       │\n",
    "│  └── manifest-xxx.avro     # File manifests           │\n",
    "└───────────────────────────┬────────────────────────────┘\n",
    "                            │\n",
    "┌───────────────────────────▼────────────────────────────┐\n",
    "│                    Data Layer                          │\n",
    "├────────────────────────────────────────────────────────┤\n",
    "│  data/                                                 │\n",
    "│  ├── dt=2024-01-15/                                    │\n",
    "│  │   ├── file1.parquet                                 │\n",
    "│  │   └── file2.parquet                                 │\n",
    "│  └── dt=2024-01-16/                                    │\n",
    "│      └── file3.parquet                                 │\n",
    "└────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Iceberg with PySpark\n",
    "# Requires: spark with iceberg-spark-runtime package\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergDemo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/tmp/iceberg-warehouse\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Iceberg table with partitioning\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS local.db.events (\n",
    "        event_id BIGINT,\n",
    "        event_type STRING,\n",
    "        event_time TIMESTAMP,\n",
    "        user_id BIGINT,\n",
    "        payload STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(event_time), event_type)\n",
    "\"\"\")\n",
    "\n",
    "# Note: Hidden partitioning! Users query by event_time, not by partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO local.db.events VALUES\n",
    "        (1, 'click', timestamp'2024-01-15 10:30:00', 100, '{\"page\": \"home\"}'),\n",
    "        (2, 'purchase', timestamp'2024-01-15 11:00:00', 100, '{\"amount\": 99.99}'),\n",
    "        (3, 'click', timestamp'2024-01-16 09:00:00', 101, '{\"page\": \"product\"}')\n",
    "\"\"\")\n",
    "\n",
    "# Query - no need to specify partition columns!\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM local.db.events \n",
    "    WHERE event_time > timestamp'2024-01-15 00:00:00'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Evolution - Change partitioning without rewriting data!\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.db.events \n",
    "    ADD PARTITION FIELD bucket(16, user_id)\n",
    "\"\"\")\n",
    "\n",
    "# New data will use the new partitioning; old data remains unchanged\n",
    "print(\"Partition evolution applied! New writes will use additional bucketing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel with Iceberg\n",
    "\n",
    "# View table snapshots\n",
    "spark.sql(\"SELECT * FROM local.db.events.snapshots\").show(truncate=False)\n",
    "\n",
    "# Query a specific snapshot\n",
    "# spark.sql(\"SELECT * FROM local.db.events VERSION AS OF 123456789\").show()\n",
    "\n",
    "# Query by timestamp\n",
    "# spark.sql(\"SELECT * FROM local.db.events TIMESTAMP AS OF '2024-01-15 12:00:00'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iceberg Maintenance Operations\n",
    "\n",
    "# Expire old snapshots (cleanup)\n",
    "spark.sql(\"\"\"\n",
    "    CALL local.system.expire_snapshots(\n",
    "        table => 'db.events',\n",
    "        older_than => TIMESTAMP '2024-01-14 00:00:00',\n",
    "        retain_last => 5\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Rewrite data files (compaction)\n",
    "spark.sql(\"\"\"\n",
    "    CALL local.system.rewrite_data_files(\n",
    "        table => 'db.events'\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cca3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Apache Hudi <a id=\"apache-hudi\"></a>\n",
    "\n",
    "**Apache Hudi** (Hadoop Upserts Deletes and Incrementals) was developed at Uber for **streaming data lake** use cases and is optimized for **incremental processing**.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Upserts/Deletes** | First-class support for record-level updates |\n",
    "| **Incremental Queries** | Efficiently query only changed data |\n",
    "| **Two Table Types** | Copy-on-Write (CoW) and Merge-on-Read (MoR) |\n",
    "| **Time Travel** | Query historical versions via timeline |\n",
    "| **Streaming Ingestion** | Native integration with Spark Streaming, Flink |\n",
    "| **Automatic Compaction** | Background compaction for MoR tables |\n",
    "\n",
    "### Table Types\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                      HUDI TABLE TYPES                                   │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│   Copy-on-Write (CoW)              Merge-on-Read (MoR)                 │\n",
    "│   ─────────────────────            ─────────────────────               │\n",
    "│   • Rewrites files on update       • Logs updates separately           │\n",
    "│   • Higher write latency           • Lower write latency               │\n",
    "│   • Faster reads                   • Slower reads (merge required)     │\n",
    "│   • Best for: batch, infrequent    • Best for: streaming, frequent     │\n",
    "│     updates                          updates                            │\n",
    "│                                                                         │\n",
    "│   ┌─────────────┐                  ┌─────────────┐  ┌────────────┐     │\n",
    "│   │  Parquet    │                  │  Parquet    │  │ Log Files  │     │\n",
    "│   │  (base)     │                  │  (base)     │  │ (updates)  │     │\n",
    "│   └─────────────┘                  └─────────────┘  └────────────┘     │\n",
    "│         │                                   \\           /              │\n",
    "│         ▼                                    \\         /               │\n",
    "│   ┌─────────────┐                         ┌──────────────┐             │\n",
    "│   │  Parquet    │                         │ Merge at     │             │\n",
    "│   │  (updated)  │                         │ Read Time    │             │\n",
    "│   └─────────────┘                         └──────────────┘             │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Hudi with PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HudiDemo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hudi:hudi-spark3.5-bundle_2.12:0.14.1\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hudi table (Copy-on-Write)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "data = [\n",
    "    (1, \"order_001\", \"Alice\", 150.00, datetime(2024, 1, 15, 10, 30)),\n",
    "    (2, \"order_002\", \"Bob\", 250.00, datetime(2024, 1, 15, 11, 0)),\n",
    "    (3, \"order_003\", \"Charlie\", 100.00, datetime(2024, 1, 15, 12, 0)),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"order_id\", \"customer\", \"amount\", \"order_time\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Hudi table configuration\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": \"orders\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"order_time\",\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.upsert.shuffle.parallelism\": 2,\n",
    "}\n",
    "\n",
    "# Write to Hudi\n",
    "df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_options) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/hudi_orders\")\n",
    "\n",
    "print(\"Hudi table created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert operation\n",
    "updates = [\n",
    "    (2, \"order_002\", \"Bob\", 300.00, datetime(2024, 1, 15, 14, 0)),  # Update amount\n",
    "    (4, \"order_004\", \"Diana\", 500.00, datetime(2024, 1, 15, 15, 0)),  # New order\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates, columns)\n",
    "\n",
    "updates_df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_options) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"/tmp/hudi_orders\")\n",
    "\n",
    "# Read current state\n",
    "print(\"After Upsert:\")\n",
    "spark.read.format(\"hudi\").load(\"/tmp/hudi_orders\").select(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03746c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Query - Get only changed records\n",
    "\n",
    "# Get commits timeline\n",
    "commits_df = spark.read.format(\"hudi\").load(\"/tmp/hudi_orders\") \\\n",
    "    .select(\"_hoodie_commit_time\").distinct().orderBy(\"_hoodie_commit_time\")\n",
    "commits_df.show()\n",
    "\n",
    "# Incremental query from a specific commit\n",
    "# begin_time = \"20240115100000\"  # Format: yyyyMMddHHmmss\n",
    "# incremental_df = spark.read.format(\"hudi\") \\\n",
    "#     .option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    "#     .option(\"hoodie.datasource.read.begin.instanttime\", begin_time) \\\n",
    "#     .load(\"/tmp/hudi_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7de0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel with Hudi\n",
    "\n",
    "# Query as of a specific timestamp\n",
    "# historical_df = spark.read.format(\"hudi\") \\\n",
    "#     .option(\"as.of.instant\", \"20240115120000\") \\\n",
    "#     .load(\"/tmp/hudi_orders\")\n",
    "\n",
    "# View timeline\n",
    "timeline_df = spark.read.format(\"hudi\").load(\"/tmp/hudi_orders\") \\\n",
    "    .select(\"_hoodie_commit_time\", \"_hoodie_record_key\", \"customer\", \"amount\")\n",
    "timeline_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0774052",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Comparison of Table Formats <a id=\"comparison\"></a>\n",
    "\n",
    "### Feature Comparison Matrix\n",
    "\n",
    "| Feature | Delta Lake | Apache Iceberg | Apache Hudi |\n",
    "|---------|------------|----------------|-------------|\n",
    "| **Origin** | Databricks | Netflix | Uber |\n",
    "| **First Release** | 2019 | 2018 | 2016 |\n",
    "| **License** | Apache 2.0 | Apache 2.0 | Apache 2.0 |\n",
    "| **ACID Transactions** | ✅ | ✅ | ✅ |\n",
    "| **Time Travel** | ✅ | ✅ | ✅ |\n",
    "| **Schema Evolution** | ✅ | ✅ (best) | ✅ |\n",
    "| **Partition Evolution** | ❌ | ✅ | ✅ (limited) |\n",
    "| **Hidden Partitioning** | ❌ | ✅ | ❌ |\n",
    "| **Incremental Processing** | Change Data Feed | ✅ | ✅ (best) |\n",
    "| **Engine Support** | Spark-centric | Multi-engine | Spark/Flink |\n",
    "| **Small File Handling** | OPTIMIZE + Z-Order | Compaction | Auto-compaction |\n",
    "| **Best For** | Spark/Databricks | Multi-engine analytics | Streaming CDC |\n",
    "\n",
    "### When to Use Each Format\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    CHOOSING A TABLE FORMAT                              │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  Use DELTA LAKE when:                                                   │\n",
    "│  • Using Databricks or primarily Spark                                  │\n",
    "│  • Need tight integration with MLflow                                   │\n",
    "│  • Want simplest getting-started experience                             │\n",
    "│  • Batch-heavy workloads with occasional updates                        │\n",
    "│                                                                         │\n",
    "│  Use ICEBERG when:                                                      │\n",
    "│  • Multi-engine environment (Spark + Trino + Flink)                    │\n",
    "│  • Need hidden partitioning for user simplicity                        │\n",
    "│  • Want partition evolution without rewrites                            │\n",
    "│  • Large-scale analytics with complex schemas                           │\n",
    "│                                                                         │\n",
    "│  Use HUDI when:                                                         │\n",
    "│  • Heavy streaming/CDC workloads                                        │\n",
    "│  • Need efficient incremental processing                                │\n",
    "│  • Frequent upserts with low latency requirements                       │\n",
    "│  • Database replication to data lake                                    │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "| Workload | Best Format | Reason |\n",
    "|----------|-------------|--------|\n",
    "| Batch analytics | Iceberg | Hidden partitioning, excellent query planning |\n",
    "| Streaming CDC | Hudi (MoR) | Optimized for frequent small updates |\n",
    "| ML Pipelines | Delta Lake | MLflow integration, feature store support |\n",
    "| Multi-engine | Iceberg | Broadest engine compatibility |\n",
    "| Databricks ecosystem | Delta Lake | Native integration, optimized performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c50f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: Creating tables in each format\n",
    "\n",
    "sample_data = [\n",
    "    (1, \"product_a\", 100),\n",
    "    (2, \"product_b\", 200),\n",
    "    (3, \"product_c\", 150),\n",
    "]\n",
    "columns = [\"id\", \"product\", \"quantity\"]\n",
    "\n",
    "# This is conceptual - each requires its own Spark configuration\n",
    "\n",
    "# Delta Lake\n",
    "delta_write = \"\"\"\n",
    "df.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/path/to/delta_table\")\n",
    "\"\"\"\n",
    "\n",
    "# Apache Iceberg\n",
    "iceberg_write = \"\"\"\n",
    "df.writeTo(\"catalog.db.iceberg_table\")\n",
    "    .using(\"iceberg\")\n",
    "    .createOrReplace()\n",
    "\"\"\"\n",
    "\n",
    "# Apache Hudi\n",
    "hudi_write = \"\"\"\n",
    "df.write.format(\"hudi\")\n",
    "    .option(\"hoodie.table.name\", \"hudi_table\")\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"id\")\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"quantity\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/path/to/hudi_table\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Delta Lake Write:\")\n",
    "print(delta_write)\n",
    "print(\"\\nIceberg Write:\")\n",
    "print(iceberg_write)\n",
    "print(\"\\nHudi Write:\")\n",
    "print(hudi_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e37429",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Key Takeaways <a id=\"takeaways\"></a>\n",
    "\n",
    "### Summary\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         KEY TAKEAWAYS                                   │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  1. MODERN TABLE FORMATS SOLVE CRITICAL DATA LAKE LIMITATIONS          │\n",
    "│     • ACID transactions prevent data corruption                         │\n",
    "│     • Time travel enables debugging and regulatory compliance           │\n",
    "│     • Schema evolution allows flexible data modeling                    │\n",
    "│                                                                         │\n",
    "│  2. ALL THREE FORMATS PROVIDE SIMILAR CORE CAPABILITIES                │\n",
    "│     • ACID guarantees                                                   │\n",
    "│     • Time travel / versioning                                          │\n",
    "│     • Efficient updates and deletes                                     │\n",
    "│     • Schema management                                                 │\n",
    "│                                                                         │\n",
    "│  3. CHOOSE BASED ON YOUR ECOSYSTEM AND WORKLOAD                        │\n",
    "│     • Delta Lake → Databricks/Spark-centric environments               │\n",
    "│     • Iceberg → Multi-engine, complex analytics                         │\n",
    "│     • Hudi → Streaming and CDC-heavy workloads                          │\n",
    "│                                                                         │\n",
    "│  4. CONSIDER OPERATIONAL ASPECTS                                        │\n",
    "│     • All require maintenance (compaction, cleanup)                     │\n",
    "│     • Cloud integration varies (AWS/Azure/GCP support)                 │\n",
    "│     • Tooling ecosystem and community support matter                   │\n",
    "│                                                                         │\n",
    "│  5. THE INDUSTRY IS CONVERGING                                          │\n",
    "│     • UniForm (Delta) can read Iceberg/Hudi metadata                   │\n",
    "│     • Apache XTable enables cross-format translation                    │\n",
    "│     • Future: more interoperability between formats                    │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Quick Decision Matrix\n",
    "\n",
    "| If you need... | Choose |\n",
    "|----------------|--------|\n",
    "| Easiest Spark integration | **Delta Lake** |\n",
    "| Best multi-engine support | **Apache Iceberg** |\n",
    "| Streaming/CDC optimization | **Apache Hudi** |\n",
    "| Partition evolution | **Apache Iceberg** |\n",
    "| Databricks compatibility | **Delta Lake** |\n",
    "| AWS native integration | All (Iceberg has best Athena/EMR support) |\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Delta Lake Documentation](https://docs.delta.io/)\n",
    "- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)\n",
    "- [Apache Hudi Documentation](https://hudi.apache.org/docs/overview/)\n",
    "- [Apache XTable (OneTable)](https://xtable.apache.org/) - Cross-format translation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
