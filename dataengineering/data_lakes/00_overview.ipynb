{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d9338c",
   "metadata": {},
   "source": [
    "# Data Lakes — Overview\n",
    "\n",
    "## Purpose\n",
    "Understand data lake architecture, design patterns, and how they differ from traditional data warehouses. Learn about zone architectures, file formats, and the evolution toward lakehouse architectures.\n",
    "\n",
    "## Key Questions\n",
    "- What is a data lake and how does it differ from a data warehouse?\n",
    "- How should data be organized within a data lake (zone architecture)?\n",
    "- Which file formats are best suited for different use cases?\n",
    "- What is a data lakehouse and why is it gaining adoption?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bed323",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Lake vs Data Warehouse\n",
    "\n",
    "### What is a Data Lake?\n",
    "A **data lake** is a centralized repository that stores **raw data in its native format** — structured, semi-structured, and unstructured — at any scale. It uses a **schema-on-read** approach, meaning data structure is applied when the data is accessed, not when it's stored.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Data Lake | Data Warehouse |\n",
    "|--------|-----------|----------------|\n",
    "| **Schema** | Schema-on-read | Schema-on-write |\n",
    "| **Data Types** | Raw, unstructured, semi-structured, structured | Structured, processed |\n",
    "| **Storage Cost** | Low (object storage) | Higher (optimized databases) |\n",
    "| **Processing** | ELT (Extract, Load, Transform) | ETL (Extract, Transform, Load) |\n",
    "| **Users** | Data scientists, engineers | Business analysts, BI users |\n",
    "| **Flexibility** | High — store everything | Lower — predefined schema |\n",
    "| **Query Performance** | Variable (depends on optimization) | Optimized for fast queries |\n",
    "| **Data Quality** | Can include raw/dirty data | Curated, validated data |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Data Lake:**\n",
    "- Exploratory analytics and data science\n",
    "- Machine learning model training\n",
    "- Storing diverse data formats (logs, IoT, images)\n",
    "- Future-proofing — store now, analyze later\n",
    "\n",
    "**Data Warehouse:**\n",
    "- Business intelligence and reporting\n",
    "- Consistent, reliable metrics\n",
    "- Ad-hoc SQL queries by analysts\n",
    "- Regulatory compliance requiring structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc391f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Zone Architecture (Medallion Architecture)\n",
    "\n",
    "Data lakes organize data into **zones** (or layers) to manage data quality and access. The most common pattern is the **medallion architecture**: Bronze → Silver → Gold.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         DATA LAKE                              │\n",
    "├───────────────┬───────────────────┬─────────────────────────────┤\n",
    "│   RAW/BRONZE  │  STAGING/SILVER   │      CURATED/GOLD           │\n",
    "│               │                   │                             │\n",
    "│  • Raw ingestion  │  • Cleaned      │  • Business-ready         │\n",
    "│  • No transforms  │  • Validated    │  • Aggregated             │\n",
    "│  • Full history   │  • Deduplicated │  • Denormalized           │\n",
    "│  • Immutable      │  • Standardized │  • Feature stores         │\n",
    "└───────────────┴───────────────────┴─────────────────────────────┘\n",
    "     ↓                    ↓                      ↓\n",
    "  Landing Zone      Transformation Zone     Consumption Zone\n",
    "```\n",
    "\n",
    "### Zone Details\n",
    "\n",
    "| Zone | Also Called | Purpose | Data Quality | Users |\n",
    "|------|-------------|---------|--------------|-------|\n",
    "| **Raw/Bronze** | Landing, Ingestion | Store data exactly as received | Low — raw, unvalidated | Data Engineers |\n",
    "| **Staging/Silver** | Refined, Cleansed | Clean, validate, standardize | Medium — cleaned | Data Engineers, Scientists |\n",
    "| **Curated/Gold** | Consumption, Trusted | Business-ready, aggregated | High — production quality | Analysts, Applications |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Keep raw data immutable** — never modify Bronze layer\n",
    "2. **Partition data** — by date, region, or business key\n",
    "3. **Apply data contracts** — define schemas between zones\n",
    "4. **Track lineage** — document transformations between layers\n",
    "5. **Implement access controls** — restrict Gold to authorized users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21600f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. File Formats for Data Lakes\n",
    "\n",
    "Choosing the right file format significantly impacts query performance, storage costs, and compatibility.\n",
    "\n",
    "### Comparison of Columnar Formats\n",
    "\n",
    "| Format | Type | Compression | Schema Evolution | Best For |\n",
    "|--------|------|-------------|------------------|----------|\n",
    "| **Parquet** | Columnar | Excellent (Snappy, GZIP) | Limited | Analytics, Spark, general purpose |\n",
    "| **ORC** | Columnar | Excellent (ZLIB, Snappy) | Good | Hive, heavy read workloads |\n",
    "| **Avro** | Row-based | Good | Excellent | Streaming, schema evolution |\n",
    "| **Delta Lake** | Columnar + ACID | Excellent | Excellent | Lakehouse, ACID transactions |\n",
    "| **Iceberg** | Columnar + ACID | Excellent | Excellent | Multi-engine, large tables |\n",
    "| **Hudi** | Columnar + ACID | Excellent | Excellent | Incremental updates, CDC |\n",
    "\n",
    "### Format Deep Dive\n",
    "\n",
    "#### Parquet\n",
    "- **Columnar storage** — reads only required columns\n",
    "- **Predicate pushdown** — filters at storage level\n",
    "- **Widely supported** — Spark, Presto, Athena, etc.\n",
    "- **Best for**: Analytical queries, data warehousing\n",
    "\n",
    "#### ORC (Optimized Row Columnar)\n",
    "- Originally developed for Hive\n",
    "- **Better compression** than Parquet in some cases\n",
    "- Built-in **indexes** and **bloom filters**\n",
    "- **Best for**: Hive ecosystems, read-heavy workloads\n",
    "\n",
    "#### Avro\n",
    "- **Row-based** — efficient for write-heavy workloads\n",
    "- **Schema stored with data** — self-describing\n",
    "- **Excellent schema evolution** — add/remove fields easily\n",
    "- **Best for**: Kafka, streaming, data exchange\n",
    "\n",
    "#### Delta Lake\n",
    "- **ACID transactions** on top of Parquet\n",
    "- **Time travel** — query historical versions\n",
    "- **Schema enforcement** — prevent bad data\n",
    "- **Merge/Upsert** operations (MERGE INTO)\n",
    "- **Best for**: Lakehouse architecture, reliable pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0071f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading/Writing Different Formats with PySpark\n",
    "\n",
    "# Note: This is illustrative code - requires PySpark environment\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark (with Delta Lake support)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLakeFormats\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Alice\", 100.0, \"2024-01-01\"),\n",
    "    (2, \"Bob\", 200.0, \"2024-01-02\"),\n",
    "    (3, \"Charlie\", 150.0, \"2024-01-03\")\n",
    "], [\"id\", \"name\", \"amount\", \"date\"])\n",
    "\n",
    "# Write as Parquet (partitioned by date)\n",
    "df.write.partitionBy(\"date\").parquet(\"/data/bronze/transactions_parquet\")\n",
    "\n",
    "# Write as ORC\n",
    "df.write.orc(\"/data/bronze/transactions_orc\")\n",
    "\n",
    "# Write as Avro\n",
    "df.write.format(\"avro\").save(\"/data/bronze/transactions_avro\")\n",
    "\n",
    "# Write as Delta Lake (with ACID support)\n",
    "df.write.format(\"delta\").save(\"/data/silver/transactions_delta\")\n",
    "\n",
    "# Delta Lake: Time Travel (query previous version)\n",
    "# df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/data/silver/transactions_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74a446",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Lakehouse Architecture\n",
    "\n",
    "A **data lakehouse** combines the best features of data lakes and data warehouses:\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────┐\n",
    "│                     DATA LAKEHOUSE                               │\n",
    "├──────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│   ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │\n",
    "│   │      BI      │  │    ML/AI     │  │  Streaming   │          │\n",
    "│   │  Analytics   │  │  Workloads   │  │  Analytics   │  ← USE   │\n",
    "│   └──────┬───────┘  └──────┬───────┘  └──────┬───────┘   CASES  │\n",
    "│          │                 │                 │                   │\n",
    "│   ┌──────┴─────────────────┴─────────────────┴───────┐          │\n",
    "│   │              UNIFIED QUERY ENGINE                │          │\n",
    "│   │         (Spark, Presto, Trino, Dremio)           │          │\n",
    "│   └──────────────────────┬───────────────────────────┘          │\n",
    "│                          │                                       │\n",
    "│   ┌──────────────────────┴───────────────────────────┐          │\n",
    "│   │           METADATA & GOVERNANCE LAYER            │          │\n",
    "│   │    (Delta Lake, Iceberg, Hudi, Unity Catalog)    │          │\n",
    "│   │  • ACID Transactions  • Schema Enforcement       │          │\n",
    "│   │  • Time Travel        • Data Lineage             │          │\n",
    "│   └──────────────────────┬───────────────────────────┘          │\n",
    "│                          │                                       │\n",
    "│   ┌──────────────────────┴───────────────────────────┐          │\n",
    "│   │              OPEN FILE FORMATS                   │          │\n",
    "│   │           (Parquet, ORC on Object Storage)       │          │\n",
    "│   └──────────────────────┬───────────────────────────┘          │\n",
    "│                          │                                       │\n",
    "│   ┌──────────────────────┴───────────────────────────┐          │\n",
    "│   │            CLOUD OBJECT STORAGE                  │          │\n",
    "│   │         (S3, ADLS, GCS, MinIO)                   │  ← BASE  │\n",
    "│   └──────────────────────────────────────────────────┘          │\n",
    "│                                                                  │\n",
    "└──────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Benefits of Lakehouse\n",
    "\n",
    "| Feature | Traditional Lake | Lakehouse |\n",
    "|---------|------------------|----------|\n",
    "| ACID Transactions | ❌ | ✅ |\n",
    "| Schema Enforcement | ❌ | ✅ |\n",
    "| Time Travel | ❌ | ✅ |\n",
    "| BI Tool Support | Limited | Full SQL support |\n",
    "| Data Quality | Manual | Built-in constraints |\n",
    "| Concurrent Writes | Problematic | Supported |\n",
    "| Cost | Low | Low (same storage) |\n",
    "\n",
    "### Popular Lakehouse Technologies\n",
    "\n",
    "1. **Delta Lake** (Databricks) — Most mature, tight Spark integration\n",
    "2. **Apache Iceberg** (Netflix) — Engine-agnostic, great for multi-engine\n",
    "3. **Apache Hudi** (Uber) — Best for incremental/CDC workloads\n",
    "\n",
    "### Lakehouse vs Two-Tier Architecture\n",
    "\n",
    "**Traditional (Lake + Warehouse):**\n",
    "```\n",
    "Data Sources → Data Lake → ETL → Data Warehouse → BI Tools\n",
    "                   ↓\n",
    "               ML/Data Science\n",
    "```\n",
    "- Data duplication\n",
    "- Complex pipelines\n",
    "- Data staleness\n",
    "\n",
    "**Lakehouse:**\n",
    "```\n",
    "Data Sources → Data Lakehouse → BI Tools + ML + Streaming\n",
    "```\n",
    "- Single source of truth\n",
    "- Reduced complexity\n",
    "- Real-time capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc230b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Delta Lake ACID Operations\n",
    "\n",
    "# Note: Requires Delta Lake environment\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# MERGE (Upsert) - Update existing, insert new\n",
    "delta_table = DeltaTable.forPath(spark, \"/data/silver/customers\")\n",
    "\n",
    "new_data = spark.createDataFrame([\n",
    "    (1, \"Alice Smith\", \"alice@new.com\"),  # Update\n",
    "    (4, \"Diana\", \"diana@new.com\")          # Insert\n",
    "], [\"id\", \"name\", \"email\"])\n",
    "\n",
    "# MERGE operation (UPSERT)\n",
    "delta_table.alias(\"target\").merge(\n",
    "    new_data.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\"name\": \"source.name\", \"email\": \"source.email\"}\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Time Travel - Query historical version\n",
    "df_yesterday = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2024-01-01\") \\\n",
    "    .load(\"/data/silver/customers\")\n",
    "\n",
    "# View history\n",
    "delta_table.history().show()\n",
    "\n",
    "# Rollback to previous version\n",
    "# spark.sql(\"RESTORE TABLE delta.`/data/silver/customers` TO VERSION AS OF 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ff39f",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Lake Governance & Catalog\n",
    "\n",
    "### Essential Governance Components\n",
    "\n",
    "| Component | Purpose | Tools |\n",
    "|-----------|---------|-------|\n",
    "| **Data Catalog** | Discover and understand data | AWS Glue, Unity Catalog, Hive Metastore |\n",
    "| **Access Control** | Secure data access | IAM, Ranger, Unity Catalog |\n",
    "| **Data Lineage** | Track data flow | OpenLineage, Marquez, Purview |\n",
    "| **Data Quality** | Validate data | Great Expectations, Deequ, Soda |\n",
    "| **Schema Registry** | Manage schemas | Confluent Schema Registry, AWS Glue |\n",
    "\n",
    "### Data Lake Anti-Patterns (\"Data Swamp\")\n",
    "\n",
    "| Anti-Pattern | Problem | Solution |\n",
    "|--------------|---------|----------|\n",
    "| No metadata | Can't find or understand data | Implement data catalog |\n",
    "| No governance | Data quality degrades | Data contracts, validation |\n",
    "| Dump everything | Storage bloat, no value | Define retention policies |\n",
    "| No access controls | Security risks | Role-based access |\n",
    "| No documentation | Tribal knowledge | Schema docs, lineage tracking |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe9d32",
   "metadata": {},
   "source": [
    "---\n",
    "## Takeaways\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Data Lake** = centralized storage for raw data in native formats (schema-on-read)\n",
    "2. **Zone Architecture** = Bronze (raw) → Silver (cleaned) → Gold (curated)\n",
    "3. **File Formats** = Use Parquet/ORC for analytics, Avro for streaming, Delta/Iceberg for ACID\n",
    "4. **Lakehouse** = Lake + Warehouse features (ACID, time travel, schema enforcement)\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "```\n",
    "Need ACID transactions?          → Use Delta Lake/Iceberg/Hudi\n",
    "Heavy schema evolution?          → Use Avro (streaming) or Iceberg\n",
    "Pure analytics workloads?        → Parquet is sufficient\n",
    "Multi-engine environment?        → Consider Apache Iceberg\n",
    "Need warehouse + lake?           → Adopt Lakehouse architecture\n",
    "```\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "- [ ] Implement zone/medallion architecture\n",
    "- [ ] Keep Bronze layer immutable\n",
    "- [ ] Partition data by common query patterns\n",
    "- [ ] Use columnar formats (Parquet/ORC) for analytics\n",
    "- [ ] Implement data catalog and governance\n",
    "- [ ] Consider lakehouse formats (Delta/Iceberg) for reliability\n",
    "- [ ] Define data retention and lifecycle policies\n",
    "- [ ] Monitor data quality across zones"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
