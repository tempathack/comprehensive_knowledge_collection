{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f422e5",
   "metadata": {},
   "source": [
    "# GCP Data Engineering Services\n",
    "\n",
    "This notebook provides a comprehensive overview of Google Cloud Platform's data engineering services. GCP offers a robust ecosystem of managed services for building scalable, reliable, and cost-effective data pipelines.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Google Cloud Storage (GCS)](#google-cloud-storage)\n",
    "2. [Dataflow](#dataflow)\n",
    "3. [Dataproc](#dataproc)\n",
    "4. [BigQuery](#bigquery)\n",
    "5. [Pub/Sub](#pubsub)\n",
    "6. [Best Practices & Architecture Patterns](#best-practices)\n",
    "7. [Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e2993",
   "metadata": {},
   "source": [
    "---\n",
    "## GCP Data Engineering Ecosystem Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         GCP Data Engineering Stack                          │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  INGESTION          PROCESSING           STORAGE           ANALYTICS        │\n",
    "│  ─────────          ──────────           ───────           ─────────        │\n",
    "│  • Pub/Sub          • Dataflow           • GCS             • BigQuery       │\n",
    "│  • Cloud IoT        • Dataproc           • BigQuery        • Looker         │\n",
    "│  • Transfer Svc     • Cloud Functions    • Bigtable        • Data Studio    │\n",
    "│  • Datastream       • Cloud Run          • Firestore       • Vertex AI      │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f296b6",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='google-cloud-storage'></a>\n",
    "## 1. Google Cloud Storage (GCS) - Data Lake Foundation\n",
    "\n",
    "Google Cloud Storage is an object storage service that serves as the foundation for building data lakes on GCP. It provides unified storage for structured, semi-structured, and unstructured data.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Storage Classes** | Standard, Nearline, Coldline, Archive |\n",
    "| **Durability** | 99.999999999% (11 9's) annual durability |\n",
    "| **Availability** | Up to 99.99% for multi-region |\n",
    "| **Consistency** | Strong global consistency |\n",
    "| **Object Size** | Up to 5 TiB per object |\n",
    "\n",
    "### Storage Classes Comparison\n",
    "\n",
    "```\n",
    "┌───────────────┬─────────────────┬─────────────────┬─────────────────┐\n",
    "│   Standard    │    Nearline     │    Coldline     │    Archive      │\n",
    "├───────────────┼─────────────────┼─────────────────┼─────────────────┤\n",
    "│ Hot data      │ Monthly access  │ Quarterly       │ Yearly access   │\n",
    "│ Frequent      │ 30-day min      │ 90-day min      │ 365-day min     │\n",
    "│ $0.020/GB/mo  │ $0.010/GB/mo    │ $0.004/GB/mo    │ $0.0012/GB/mo   │\n",
    "└───────────────┴─────────────────┴─────────────────┴─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Storage - Python SDK Examples\n",
    "from google.cloud import storage\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize client\n",
    "client = storage.Client(project='your-project-id')\n",
    "\n",
    "# Create a bucket with lifecycle management\n",
    "def create_data_lake_bucket(bucket_name: str, location: str = 'US') -> storage.Bucket:\n",
    "    \"\"\"Create a GCS bucket configured for data lake usage.\"\"\"\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    bucket.storage_class = 'STANDARD'\n",
    "    bucket.location = location\n",
    "    \n",
    "    # Enable versioning for data protection\n",
    "    bucket.versioning_enabled = True\n",
    "    \n",
    "    # Set lifecycle rules for cost optimization\n",
    "    bucket.lifecycle_rules = [\n",
    "        {\n",
    "            'action': {'type': 'SetStorageClass', 'storageClass': 'NEARLINE'},\n",
    "            'condition': {'age': 30, 'matchesPrefix': ['raw/']}  # Move raw data after 30 days\n",
    "        },\n",
    "        {\n",
    "            'action': {'type': 'SetStorageClass', 'storageClass': 'COLDLINE'},\n",
    "            'condition': {'age': 90, 'matchesPrefix': ['raw/']}\n",
    "        },\n",
    "        {\n",
    "            'action': {'type': 'Delete'},\n",
    "            'condition': {'age': 365, 'matchesPrefix': ['temp/']}  # Delete temp data after 1 year\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    new_bucket = client.create_bucket(bucket, location=location)\n",
    "    print(f\"Created bucket {new_bucket.name} in {new_bucket.location}\")\n",
    "    return new_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Lake Directory Structure Best Practice\n",
    "def create_data_lake_structure(bucket_name: str):\n",
    "    \"\"\"\n",
    "    Create a medallion architecture directory structure.\n",
    "    \n",
    "    Structure:\n",
    "    ├── raw/              # Bronze layer - raw ingested data\n",
    "    │   ├── source1/\n",
    "    │   └── source2/\n",
    "    ├── processed/        # Silver layer - cleaned, validated\n",
    "    │   ├── domain1/\n",
    "    │   └── domain2/\n",
    "    ├── curated/          # Gold layer - business-ready\n",
    "    │   ├── analytics/\n",
    "    │   └── ml_features/\n",
    "    └── temp/             # Temporary processing data\n",
    "    \"\"\"\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    directories = [\n",
    "        'raw/.keep',\n",
    "        'processed/.keep',\n",
    "        'curated/.keep',\n",
    "        'temp/.keep'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        blob = bucket.blob(directory)\n",
    "        blob.upload_from_string('')\n",
    "        print(f\"Created: gs://{bucket_name}/{directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794abb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient Data Upload with Parallel Composite Uploads\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "\n",
    "def upload_files_parallel(\n",
    "    bucket_name: str,\n",
    "    local_directory: str,\n",
    "    gcs_prefix: str,\n",
    "    max_workers: int = 10\n",
    "):\n",
    "    \"\"\"Upload multiple files to GCS in parallel.\"\"\"\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    local_path = Path(local_directory)\n",
    "    files = list(local_path.rglob('*'))\n",
    "    files = [f for f in files if f.is_file()]\n",
    "    \n",
    "    def upload_file(file_path: Path):\n",
    "        relative_path = file_path.relative_to(local_path)\n",
    "        blob_name = f\"{gcs_prefix}/{relative_path}\"\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.upload_from_filename(str(file_path))\n",
    "        return blob_name\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(upload_file, f): f for f in files}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                blob_name = future.result()\n",
    "                print(f\"Uploaded: {blob_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {futures[future]}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee33ea3",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='dataflow'></a>\n",
    "## 2. Dataflow - Unified Stream & Batch Processing\n",
    "\n",
    "Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines. It provides **unified programming model** for both batch and streaming data processing.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Pipeline** | A complete data processing task (DAG) |\n",
    "| **PCollection** | Distributed dataset (immutable) |\n",
    "| **Transform** | Operations on PCollections (Map, Filter, GroupBy) |\n",
    "| **Runner** | Execution engine (Dataflow, Direct, Spark, Flink) |\n",
    "| **Windowing** | Grouping elements by time for streaming |\n",
    "\n",
    "### Dataflow Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        Dataflow Pipeline                            │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│   Source        Transform 1       Transform 2        Sink          │\n",
    "│  ┌──────┐       ┌─────────┐       ┌─────────┐      ┌──────┐        │\n",
    "│  │Pub/Sub│ ───► │ Parse   │ ───► │ Enrich  │ ───► │  BQ  │        │\n",
    "│  │ GCS   │      │ Filter  │      │ Agg     │      │ GCS  │        │\n",
    "│  └──────┘       └─────────┘       └─────────┘      └──────┘        │\n",
    "│                                                                     │\n",
    "│  ◄──────────────── Autoscaling Workers ─────────────────►          │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Beam / Dataflow - Batch Processing Example\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery, BigQueryDisposition\n",
    "\n",
    "# Configure pipeline options for Dataflow\n",
    "def get_dataflow_options(project: str, region: str, temp_location: str) -> PipelineOptions:\n",
    "    \"\"\"Configure Dataflow runner options.\"\"\"\n",
    "    options = PipelineOptions([\n",
    "        f'--project={project}',\n",
    "        f'--region={region}',\n",
    "        f'--temp_location={temp_location}',\n",
    "        '--runner=DataflowRunner',\n",
    "        '--streaming=False',  # Batch mode\n",
    "        '--autoscaling_algorithm=THROUGHPUT_BASED',\n",
    "        '--max_num_workers=10',\n",
    "        '--disk_size_gb=50',\n",
    "        '--worker_machine_type=n1-standard-4'\n",
    "    ])\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch ETL Pipeline Example\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ParseJsonFn(beam.DoFn):\n",
    "    \"\"\"Parse JSON records and handle errors.\"\"\"\n",
    "    def process(self, element):\n",
    "        try:\n",
    "            record = json.loads(element)\n",
    "            # Add processing timestamp\n",
    "            record['processed_at'] = datetime.utcnow().isoformat()\n",
    "            yield beam.pvalue.TaggedOutput('valid', record)\n",
    "        except json.JSONDecodeError as e:\n",
    "            yield beam.pvalue.TaggedOutput('invalid', {'raw': element, 'error': str(e)})\n",
    "\n",
    "class EnrichDataFn(beam.DoFn):\n",
    "    \"\"\"Enrich records with additional computed fields.\"\"\"\n",
    "    def process(self, record):\n",
    "        # Add derived fields\n",
    "        if 'amount' in record:\n",
    "            record['amount_category'] = 'high' if record['amount'] > 1000 else 'low'\n",
    "        yield record\n",
    "\n",
    "def run_batch_etl_pipeline(\n",
    "    input_path: str,\n",
    "    output_table: str,\n",
    "    dead_letter_path: str,\n",
    "    options: PipelineOptions\n",
    "):\n",
    "    \"\"\"Run a batch ETL pipeline from GCS to BigQuery.\"\"\"\n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        # Read from GCS\n",
    "        raw_data = pipeline | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "        \n",
    "        # Parse with error handling (fan-out pattern)\n",
    "        parsed = raw_data | 'ParseJSON' >> beam.ParDo(ParseJsonFn()).with_outputs('valid', 'invalid')\n",
    "        \n",
    "        # Process valid records\n",
    "        enriched = (\n",
    "            parsed.valid\n",
    "            | 'Enrich' >> beam.ParDo(EnrichDataFn())\n",
    "            | 'FilterNulls' >> beam.Filter(lambda x: x.get('user_id') is not None)\n",
    "        )\n",
    "        \n",
    "        # Write to BigQuery\n",
    "        enriched | 'WriteToBigQuery' >> WriteToBigQuery(\n",
    "            output_table,\n",
    "            write_disposition=BigQueryDisposition.WRITE_APPEND,\n",
    "            create_disposition=BigQueryDisposition.CREATE_IF_NEEDED\n",
    "        )\n",
    "        \n",
    "        # Write errors to dead letter queue\n",
    "        (parsed.invalid\n",
    "         | 'FormatErrors' >> beam.Map(json.dumps)\n",
    "         | 'WriteDeadLetter' >> beam.io.WriteToText(dead_letter_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Pipeline Example with Windowing\n",
    "from apache_beam import window\n",
    "from apache_beam.transforms.trigger import AfterWatermark, AfterProcessingTime, AccumulationMode\n",
    "\n",
    "def run_streaming_pipeline(\n",
    "    subscription: str,\n",
    "    output_table: str,\n",
    "    options: PipelineOptions\n",
    "):\n",
    "    \"\"\"Real-time streaming pipeline with windowing.\"\"\"\n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        events = (\n",
    "            pipeline\n",
    "            # Read from Pub/Sub\n",
    "            | 'ReadPubSub' >> beam.io.ReadFromPubSub(\n",
    "                subscription=subscription,\n",
    "                with_attributes=True,\n",
    "                timestamp_attribute='event_time'  # Use event time\n",
    "            )\n",
    "            | 'DecodeMessages' >> beam.Map(lambda x: json.loads(x.data.decode('utf-8')))\n",
    "        )\n",
    "        \n",
    "        # Apply windowing for aggregations\n",
    "        windowed_events = (\n",
    "            events\n",
    "            | 'AddTimestamp' >> beam.Map(\n",
    "                lambda x: beam.window.TimestampedValue(x, x['timestamp'])\n",
    "            )\n",
    "            # 5-minute tumbling windows\n",
    "            | 'Window' >> beam.WindowInto(\n",
    "                window.FixedWindows(5 * 60),  # 5 minutes\n",
    "                trigger=AfterWatermark(\n",
    "                    early=AfterProcessingTime(60),  # Early firings every minute\n",
    "                    late=AfterProcessingTime(300)   # Late data up to 5 min\n",
    "                ),\n",
    "                accumulation_mode=AccumulationMode.ACCUMULATING,\n",
    "                allowed_lateness=beam.Duration(seconds=3600)  # 1 hour late data\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Aggregate by key\n",
    "        aggregated = (\n",
    "            windowed_events\n",
    "            | 'KeyByUser' >> beam.Map(lambda x: (x['user_id'], x['amount']))\n",
    "            | 'SumPerUser' >> beam.CombinePerKey(sum)\n",
    "            | 'FormatOutput' >> beam.Map(\n",
    "                lambda x: {'user_id': x[0], 'total_amount': x[1]}\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Write to BigQuery with streaming inserts\n",
    "        aggregated | 'StreamToBQ' >> WriteToBigQuery(\n",
    "            output_table,\n",
    "            method='STREAMING_INSERTS'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3307c",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='dataproc'></a>\n",
    "## 3. Dataproc - Managed Spark & Hadoop\n",
    "\n",
    "Google Cloud Dataproc is a fully managed service for running Apache Spark, Hadoop, Presto, and other open-source tools. It's ideal for existing Spark/Hadoop workloads and complex data transformations.\n",
    "\n",
    "### Dataproc vs Dataflow\n",
    "\n",
    "| Aspect | Dataproc | Dataflow |\n",
    "|--------|----------|----------|\n",
    "| **Use Case** | Existing Spark/Hadoop jobs | New pipelines, Beam-based |\n",
    "| **Scaling** | Manual cluster sizing | Fully auto-scaling |\n",
    "| **Pricing** | Per-cluster (VM hours) | Per-job (data processed) |\n",
    "| **Startup Time** | ~90 seconds | Near-instant (streaming) |\n",
    "| **Flexibility** | Full Spark API access | Beam abstractions |\n",
    "| **Best For** | ML, complex transformations | ETL pipelines, streaming |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927dc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataproc Cluster Management\n",
    "from google.cloud import dataproc_v1\n",
    "from google.cloud.dataproc_v1.types import Cluster, ClusterConfig, InstanceGroupConfig\n",
    "\n",
    "def create_dataproc_cluster(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    cluster_name: str,\n",
    "    master_machine_type: str = 'n1-standard-4',\n",
    "    worker_machine_type: str = 'n1-standard-4',\n",
    "    num_workers: int = 2\n",
    ") -> str:\n",
    "    \"\"\"Create a Dataproc cluster with autoscaling.\"\"\"\n",
    "    \n",
    "    cluster_client = dataproc_v1.ClusterControllerClient(\n",
    "        client_options={'api_endpoint': f'{region}-dataproc.googleapis.com:443'}\n",
    "    )\n",
    "    \n",
    "    cluster_config = {\n",
    "        'project_id': project_id,\n",
    "        'cluster_name': cluster_name,\n",
    "        'config': {\n",
    "            'master_config': {\n",
    "                'num_instances': 1,\n",
    "                'machine_type_uri': master_machine_type,\n",
    "                'disk_config': {'boot_disk_size_gb': 500}\n",
    "            },\n",
    "            'worker_config': {\n",
    "                'num_instances': num_workers,\n",
    "                'machine_type_uri': worker_machine_type,\n",
    "                'disk_config': {'boot_disk_size_gb': 500}\n",
    "            },\n",
    "            # Enable component gateway for web UIs\n",
    "            'endpoint_config': {'enable_http_port_access': True},\n",
    "            # Optional components\n",
    "            'software_config': {\n",
    "                'image_version': '2.1-debian11',\n",
    "                'optional_components': ['JUPYTER', 'ZEPPELIN'],\n",
    "                'properties': {\n",
    "                    'spark:spark.jars.packages': 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.0'\n",
    "                }\n",
    "            },\n",
    "            # Autoscaling policy\n",
    "            'autoscaling_config': {\n",
    "                'policy_uri': f'projects/{project_id}/regions/{region}/autoscalingPolicies/default-policy'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    operation = cluster_client.create_cluster(\n",
    "        request={'project_id': project_id, 'region': region, 'cluster': cluster_config}\n",
    "    )\n",
    "    \n",
    "    result = operation.result()\n",
    "    print(f\"Cluster created: {result.cluster_name}\")\n",
    "    return result.cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit PySpark Job to Dataproc\n",
    "def submit_pyspark_job(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    cluster_name: str,\n",
    "    main_python_file: str,\n",
    "    args: list = None\n",
    ") -> str:\n",
    "    \"\"\"Submit a PySpark job to Dataproc cluster.\"\"\"\n",
    "    \n",
    "    job_client = dataproc_v1.JobControllerClient(\n",
    "        client_options={'api_endpoint': f'{region}-dataproc.googleapis.com:443'}\n",
    "    )\n",
    "    \n",
    "    job = {\n",
    "        'placement': {'cluster_name': cluster_name},\n",
    "        'pyspark_job': {\n",
    "            'main_python_file_uri': main_python_file,\n",
    "            'args': args or [],\n",
    "            'properties': {\n",
    "                'spark.executor.memory': '4g',\n",
    "                'spark.executor.cores': '2',\n",
    "                'spark.dynamicAllocation.enabled': 'true'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    operation = job_client.submit_job_as_operation(\n",
    "        request={'project_id': project_id, 'region': region, 'job': job}\n",
    "    )\n",
    "    \n",
    "    result = operation.result()\n",
    "    print(f\"Job finished: {result.status.state.name}\")\n",
    "    return result.reference.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example PySpark Job: ETL from GCS to BigQuery\n",
    "# This would be saved as a .py file and submitted to Dataproc\n",
    "\n",
    "PYSPARK_ETL_SCRIPT = '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, current_timestamp\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"GCS-to-BigQuery-ETL\") \\\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Read from GCS (Parquet)\n",
    "    df = spark.read.parquet(\"gs://your-bucket/raw/events/\")\n",
    "    \n",
    "    # Transformations\n",
    "    transformed = df \\\\\n",
    "        .filter(col(\"event_type\").isNotNull()) \\\\\n",
    "        .withColumn(\n",
    "            \"event_category\",\n",
    "            when(col(\"event_type\").isin([\"click\", \"view\"]), \"engagement\")\n",
    "            .when(col(\"event_type\") == \"purchase\", \"conversion\")\n",
    "            .otherwise(\"other\")\n",
    "        ) \\\\\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    \n",
    "    # Write to BigQuery using Spark BigQuery connector\n",
    "    transformed.write \\\\\n",
    "        .format(\"bigquery\") \\\\\n",
    "        .option(\"table\", \"project.dataset.events_processed\") \\\\\n",
    "        .option(\"temporaryGcsBucket\", \"your-temp-bucket\") \\\\\n",
    "        .mode(\"append\") \\\\\n",
    "        .save()\n",
    "    \n",
    "    print(f\"Processed {transformed.count()} records\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(\"PySpark ETL script template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3125e16",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='bigquery'></a>\n",
    "## 4. BigQuery - Serverless Data Warehouse\n",
    "\n",
    "BigQuery is Google's fully managed, serverless data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Separation of Storage & Compute** | Pay for what you use independently |\n",
    "| **Columnar Storage** | Optimized for analytical queries |\n",
    "| **Partitioning** | Time-based and integer-range partitions |\n",
    "| **Clustering** | Sort data by specified columns |\n",
    "| **Streaming Inserts** | Real-time data ingestion |\n",
    "| **BigQuery ML** | ML models using SQL |\n",
    "| **BI Engine** | In-memory analysis for dashboards |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26979bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery Python SDK Examples\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import SchemaField, Table, TimePartitioning\n",
    "\n",
    "bq_client = bigquery.Client(project='your-project-id')\n",
    "\n",
    "def create_partitioned_table(\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    schema: list,\n",
    "    partition_field: str = 'event_date',\n",
    "    clustering_fields: list = None\n",
    ") -> Table:\n",
    "    \"\"\"Create a partitioned and clustered BigQuery table.\"\"\"\n",
    "    \n",
    "    table_ref = f\"{bq_client.project}.{dataset_id}.{table_id}\"\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Time-based partitioning\n",
    "    table.time_partitioning = TimePartitioning(\n",
    "        type_=bigquery.TimePartitioningType.DAY,\n",
    "        field=partition_field,\n",
    "        expiration_ms=365 * 24 * 60 * 60 * 1000  # 1 year retention\n",
    "    )\n",
    "    \n",
    "    # Clustering for query optimization\n",
    "    if clustering_fields:\n",
    "        table.clustering_fields = clustering_fields\n",
    "    \n",
    "    created_table = bq_client.create_table(table, exists_ok=True)\n",
    "    print(f\"Created table: {created_table.full_table_id}\")\n",
    "    return created_table\n",
    "\n",
    "# Example schema\n",
    "events_schema = [\n",
    "    SchemaField('event_id', 'STRING', mode='REQUIRED'),\n",
    "    SchemaField('user_id', 'STRING', mode='REQUIRED'),\n",
    "    SchemaField('event_type', 'STRING', mode='REQUIRED'),\n",
    "    SchemaField('event_date', 'DATE', mode='REQUIRED'),\n",
    "    SchemaField('event_timestamp', 'TIMESTAMP', mode='REQUIRED'),\n",
    "    SchemaField('properties', 'JSON', mode='NULLABLE'),\n",
    "    SchemaField('amount', 'FLOAT64', mode='NULLABLE'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data into BigQuery\n",
    "from google.cloud.bigquery import LoadJobConfig, SourceFormat, WriteDisposition\n",
    "\n",
    "def load_gcs_to_bigquery(\n",
    "    source_uri: str,\n",
    "    destination_table: str,\n",
    "    source_format: str = 'PARQUET'\n",
    ") -> bigquery.LoadJob:\n",
    "    \"\"\"Load data from GCS to BigQuery.\"\"\"\n",
    "    \n",
    "    job_config = LoadJobConfig(\n",
    "        source_format=getattr(SourceFormat, source_format),\n",
    "        write_disposition=WriteDisposition.WRITE_APPEND,\n",
    "        # Schema auto-detection for Parquet\n",
    "        autodetect=True if source_format == 'PARQUET' else False,\n",
    "        # Hive partitioning detection\n",
    "        hive_partitioning=bigquery.HivePartitioningOptions(\n",
    "            mode='AUTO',\n",
    "            source_uri_prefix=source_uri.rsplit('/', 1)[0]\n",
    "        ) if 'year=' in source_uri or 'date=' in source_uri else None\n",
    "    )\n",
    "    \n",
    "    load_job = bq_client.load_table_from_uri(\n",
    "        source_uri,\n",
    "        destination_table,\n",
    "        job_config=job_config\n",
    "    )\n",
    "    \n",
    "    result = load_job.result()  # Wait for completion\n",
    "    print(f\"Loaded {result.output_rows} rows to {destination_table}\")\n",
    "    return load_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery SQL Best Practices\n",
    "\n",
    "# Optimized query with partition pruning and clustering benefits\n",
    "OPTIMIZED_QUERY = \"\"\"\n",
    "-- Use partition filters to reduce data scanned\n",
    "SELECT\n",
    "    user_id,\n",
    "    event_type,\n",
    "    COUNT(*) as event_count,\n",
    "    SUM(amount) as total_amount,\n",
    "    AVG(amount) as avg_amount\n",
    "FROM `project.dataset.events`\n",
    "WHERE \n",
    "    -- Partition filter (reduces data scanned)\n",
    "    event_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY) AND CURRENT_DATE()\n",
    "    -- Clustering column filter (further optimization)\n",
    "    AND event_type IN ('purchase', 'subscription')\n",
    "GROUP BY user_id, event_type\n",
    "HAVING event_count > 5\n",
    "ORDER BY total_amount DESC\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "# Incremental processing pattern\n",
    "INCREMENTAL_MERGE = \"\"\"\n",
    "-- Merge pattern for incremental updates\n",
    "MERGE `project.dataset.dim_users` AS target\n",
    "USING (\n",
    "    SELECT DISTINCT\n",
    "        user_id,\n",
    "        FIRST_VALUE(email) OVER (PARTITION BY user_id ORDER BY updated_at DESC) as email,\n",
    "        FIRST_VALUE(name) OVER (PARTITION BY user_id ORDER BY updated_at DESC) as name,\n",
    "        MAX(updated_at) as last_updated\n",
    "    FROM `project.dataset.user_events_staging`\n",
    "    WHERE _PARTITIONDATE = CURRENT_DATE()\n",
    ") AS source\n",
    "ON target.user_id = source.user_id\n",
    "WHEN MATCHED AND source.last_updated > target.last_updated THEN\n",
    "    UPDATE SET\n",
    "        email = source.email,\n",
    "        name = source.name,\n",
    "        last_updated = source.last_updated\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (user_id, email, name, last_updated)\n",
    "    VALUES (source.user_id, source.email, source.name, source.last_updated)\n",
    "\"\"\"\n",
    "\n",
    "print(\"BigQuery SQL patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e47b4",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='pubsub'></a>\n",
    "## 5. Pub/Sub - Messaging & Event Streaming\n",
    "\n",
    "Google Cloud Pub/Sub is a fully managed real-time messaging service that allows you to send and receive messages between independent applications.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        Pub/Sub Architecture                         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│   Publishers              Topic                 Subscribers         │\n",
    "│   ──────────             ───────                ───────────         │\n",
    "│   ┌────────┐            ┌───────┐              ┌──────────┐        │\n",
    "│   │ App 1  │──┐         │       │──────────────│ Sub A    │        │\n",
    "│   └────────┘  │         │       │              │ (Push)   │        │\n",
    "│               ├────────►│ Topic │              └──────────┘        │\n",
    "│   ┌────────┐  │         │       │              ┌──────────┐        │\n",
    "│   │ App 2  │──┘         │       │──────────────│ Sub B    │        │\n",
    "│   └────────┘            └───────┘              │ (Pull)   │        │\n",
    "│                                                └──────────┘        │\n",
    "│                                                                     │\n",
    "│   • Messages are retained for 7 days (configurable)                │\n",
    "│   • At-least-once delivery guaranteed                              │\n",
    "│   • Exactly-once processing available                               │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Subscription Types\n",
    "\n",
    "| Type | Use Case | Delivery |\n",
    "|------|----------|----------|\n",
    "| **Pull** | Batch processing, custom rate control | Subscriber pulls messages |\n",
    "| **Push** | Serverless, webhooks | Pub/Sub pushes to endpoint |\n",
    "| **BigQuery** | Direct analytics ingestion | Auto-writes to BQ table |\n",
    "| **Cloud Storage** | Data lake ingestion | Auto-writes to GCS bucket |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ef002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pub/Sub Publisher Example\n",
    "from google.cloud import pubsub_v1\n",
    "from google.cloud.pubsub_v1 import PublisherClient\n",
    "from google.api_core import retry\n",
    "import json\n",
    "from concurrent import futures\n",
    "from typing import Callable\n",
    "\n",
    "class PubSubPublisher:\n",
    "    \"\"\"High-performance Pub/Sub publisher with batching.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id: str):\n",
    "        # Configure batching settings\n",
    "        batch_settings = pubsub_v1.types.BatchSettings(\n",
    "            max_messages=100,        # Max messages per batch\n",
    "            max_bytes=1024 * 1024,   # 1 MB max batch size\n",
    "            max_latency=0.01         # 10ms max wait time\n",
    "        )\n",
    "        \n",
    "        self.publisher = PublisherClient(\n",
    "            batch_settings=batch_settings\n",
    "        )\n",
    "        self.project_id = project_id\n",
    "        self.futures = []\n",
    "    \n",
    "    def get_topic_path(self, topic_id: str) -> str:\n",
    "        return self.publisher.topic_path(self.project_id, topic_id)\n",
    "    \n",
    "    def publish(\n",
    "        self,\n",
    "        topic_id: str,\n",
    "        data: dict,\n",
    "        ordering_key: str = None,\n",
    "        **attributes\n",
    "    ) -> futures.Future:\n",
    "        \"\"\"Publish a message with optional ordering key.\"\"\"\n",
    "        topic_path = self.get_topic_path(topic_id)\n",
    "        \n",
    "        # Serialize data\n",
    "        message_bytes = json.dumps(data).encode('utf-8')\n",
    "        \n",
    "        # Publish with retry\n",
    "        future = self.publisher.publish(\n",
    "            topic_path,\n",
    "            data=message_bytes,\n",
    "            ordering_key=ordering_key or '',\n",
    "            **attributes  # Custom attributes for filtering\n",
    "        )\n",
    "        \n",
    "        self.futures.append(future)\n",
    "        return future\n",
    "    \n",
    "    def flush(self):\n",
    "        \"\"\"Wait for all pending publishes to complete.\"\"\"\n",
    "        for future in self.futures:\n",
    "            try:\n",
    "                message_id = future.result(timeout=30)\n",
    "            except Exception as e:\n",
    "                print(f\"Publish failed: {e}\")\n",
    "        self.futures.clear()\n",
    "\n",
    "# Usage example\n",
    "# publisher = PubSubPublisher('my-project')\n",
    "# publisher.publish('events-topic', {'user_id': '123', 'event': 'click'}, event_type='click')\n",
    "# publisher.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pub/Sub Subscriber with Exactly-Once Processing\n",
    "from google.cloud.pubsub_v1 import SubscriberClient\n",
    "from google.cloud.pubsub_v1.subscriber.message import Message\n",
    "import time\n",
    "\n",
    "class PubSubSubscriber:\n",
    "    \"\"\"Pub/Sub subscriber with exactly-once semantics.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id: str):\n",
    "        self.subscriber = SubscriberClient()\n",
    "        self.project_id = project_id\n",
    "        self.streaming_pull_future = None\n",
    "    \n",
    "    def get_subscription_path(self, subscription_id: str) -> str:\n",
    "        return self.subscriber.subscription_path(self.project_id, subscription_id)\n",
    "    \n",
    "    def subscribe(\n",
    "        self,\n",
    "        subscription_id: str,\n",
    "        callback: Callable[[Message], None],\n",
    "        max_messages: int = 100,\n",
    "        ack_deadline: int = 60\n",
    "    ):\n",
    "        \"\"\"Start streaming pull subscription.\"\"\"\n",
    "        subscription_path = self.get_subscription_path(subscription_id)\n",
    "        \n",
    "        # Flow control to prevent overwhelming the subscriber\n",
    "        flow_control = pubsub_v1.types.FlowControl(\n",
    "            max_messages=max_messages,\n",
    "            max_bytes=10 * 1024 * 1024  # 10 MB\n",
    "        )\n",
    "        \n",
    "        def wrapped_callback(message: Message):\n",
    "            try:\n",
    "                # Process message\n",
    "                callback(message)\n",
    "                # Acknowledge on success\n",
    "                message.ack()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing message: {e}\")\n",
    "                # Negative acknowledge - message will be redelivered\n",
    "                message.nack()\n",
    "        \n",
    "        self.streaming_pull_future = self.subscriber.subscribe(\n",
    "            subscription_path,\n",
    "            callback=wrapped_callback,\n",
    "            flow_control=flow_control\n",
    "        )\n",
    "        \n",
    "        print(f\"Listening on {subscription_path}...\")\n",
    "        return self.streaming_pull_future\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the subscriber.\"\"\"\n",
    "        if self.streaming_pull_future:\n",
    "            self.streaming_pull_future.cancel()\n",
    "            self.streaming_pull_future.result()\n",
    "\n",
    "# Example callback\n",
    "def process_message(message: Message):\n",
    "    data = json.loads(message.data.decode('utf-8'))\n",
    "    print(f\"Received: {data}\")\n",
    "    print(f\"Attributes: {message.attributes}\")\n",
    "    print(f\"Message ID: {message.message_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BigQuery Subscription for Direct Analytics\n",
    "from google.cloud.pubsub_v1 import SubscriberClient\n",
    "from google.pubsub_v1.types import BigQueryConfig, Subscription\n",
    "\n",
    "def create_bigquery_subscription(\n",
    "    project_id: str,\n",
    "    topic_id: str,\n",
    "    subscription_id: str,\n",
    "    bigquery_table: str\n",
    "):\n",
    "    \"\"\"Create a subscription that writes directly to BigQuery.\"\"\"\n",
    "    \n",
    "    subscriber = SubscriberClient()\n",
    "    topic_path = subscriber.topic_path(project_id, topic_id)\n",
    "    subscription_path = subscriber.subscription_path(project_id, subscription_id)\n",
    "    \n",
    "    bigquery_config = BigQueryConfig(\n",
    "        table=bigquery_table,  # Format: project:dataset.table\n",
    "        use_topic_schema=True,  # Use Pub/Sub schema\n",
    "        write_metadata=True,    # Include message metadata\n",
    "        drop_unknown_fields=True\n",
    "    )\n",
    "    \n",
    "    subscription = Subscription(\n",
    "        name=subscription_path,\n",
    "        topic=topic_path,\n",
    "        bigquery_config=bigquery_config\n",
    "    )\n",
    "    \n",
    "    result = subscriber.create_subscription(request=subscription)\n",
    "    print(f\"Created BigQuery subscription: {result.name}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c554035",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='best-practices'></a>\n",
    "## 6. Best Practices & Architecture Patterns\n",
    "\n",
    "### Reference Architecture: Real-Time Analytics Pipeline\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────────────┐\n",
    "│                    Real-Time Analytics Architecture                          │\n",
    "├──────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│   DATA SOURCES              INGESTION              PROCESSING                │\n",
    "│   ────────────              ─────────              ──────────                │\n",
    "│                                                                              │\n",
    "│   ┌─────────┐              ┌─────────┐            ┌──────────┐               │\n",
    "│   │ Web App │───────┐      │         │            │          │               │\n",
    "│   └─────────┘       │      │         │   Stream   │ Dataflow │               │\n",
    "│                     ├─────►│ Pub/Sub │───────────►│ Streaming│               │\n",
    "│   ┌─────────┐       │      │         │            │ Pipeline │               │\n",
    "│   │ Mobile  │───────┘      │         │            │          │               │\n",
    "│   └─────────┘              └─────────┘            └────┬─────┘               │\n",
    "│                                                        │                     │\n",
    "│   ┌─────────┐              ┌─────────┐            ┌────▼─────┐               │\n",
    "│   │ Files   │─────────────►│   GCS   │───────────►│ Dataflow │               │\n",
    "│   │ (Batch) │              │(Landing)│   Batch    │  Batch   │               │\n",
    "│   └─────────┘              └─────────┘            └────┬─────┘               │\n",
    "│                                                        │                     │\n",
    "│   STORAGE                  SERVING                  CONSUMPTION              │\n",
    "│   ───────                  ───────                  ───────────              │\n",
    "│                                                                              │\n",
    "│               ┌────────────────────────────────────────┐                     │\n",
    "│               │                                        │                     │\n",
    "│   ┌───────────▼──┐        ┌───────────┐         ┌──────▼──────┐             │\n",
    "│   │   BigQuery   │◄──────►│  BI Engine│────────►│   Looker    │             │\n",
    "│   │  (Warehouse) │        │ (In-Memory)│        │ Data Studio │             │\n",
    "│   └──────────────┘        └───────────┘         └─────────────┘             │\n",
    "│                                                                              │\n",
    "└──────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d3e12",
   "metadata": {},
   "source": [
    "### Best Practices by Service\n",
    "\n",
    "#### Google Cloud Storage\n",
    "- ✅ Use lifecycle policies for automatic tiering\n",
    "- ✅ Enable Object Versioning for critical data\n",
    "- ✅ Use regional buckets for latency, multi-region for durability\n",
    "- ✅ Implement naming conventions with partitions (e.g., `year=2024/month=01/`)\n",
    "- ❌ Avoid small files - aggregate into larger objects\n",
    "- ❌ Don't use GCS for low-latency key-value lookups\n",
    "\n",
    "#### Dataflow\n",
    "- ✅ Use Flex Templates for production pipelines\n",
    "- ✅ Implement dead-letter queues for error handling\n",
    "- ✅ Use side inputs for small lookup tables\n",
    "- ✅ Set appropriate windowing and triggering strategies\n",
    "- ❌ Avoid stateful operations when possible\n",
    "- ❌ Don't use global windows for unbounded data\n",
    "\n",
    "#### Dataproc\n",
    "- ✅ Use ephemeral clusters (create → run → delete)\n",
    "- ✅ Store data in GCS, not HDFS\n",
    "- ✅ Enable autoscaling policies\n",
    "- ✅ Use initialization actions for customization\n",
    "- ❌ Avoid keeping clusters running when idle\n",
    "- ❌ Don't use local disk for persistent data\n",
    "\n",
    "#### BigQuery\n",
    "- ✅ Always use partition filters in queries\n",
    "- ✅ Cluster tables on high-cardinality filter columns\n",
    "- ✅ Use `MERGE` for incremental updates\n",
    "- ✅ Consider BI Engine for dashboard workloads\n",
    "- ❌ Avoid `SELECT *` - specify needed columns\n",
    "- ❌ Don't use streaming inserts for batch loads\n",
    "\n",
    "#### Pub/Sub\n",
    "- ✅ Use ordering keys for in-order processing\n",
    "- ✅ Implement idempotent subscribers\n",
    "- ✅ Set appropriate acknowledgment deadlines\n",
    "- ✅ Use BigQuery subscriptions for simple analytics\n",
    "- ❌ Avoid very long-running message processing\n",
    "- ❌ Don't use Pub/Sub for request-response patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infrastructure as Code: Terraform Example for GCP Data Platform\n",
    "\n",
    "TERRAFORM_CONFIG = '''\n",
    "# main.tf - GCP Data Engineering Infrastructure\n",
    "\n",
    "terraform {\n",
    "  required_providers {\n",
    "    google = {\n",
    "      source  = \"hashicorp/google\"\n",
    "      version = \"~> 5.0\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "variable \"project_id\" {}\n",
    "variable \"region\" { default = \"us-central1\" }\n",
    "\n",
    "# Data Lake Bucket\n",
    "resource \"google_storage_bucket\" \"data_lake\" {\n",
    "  name                        = \"${var.project_id}-data-lake\"\n",
    "  location                    = var.region\n",
    "  uniform_bucket_level_access = true\n",
    "  versioning { enabled = true }\n",
    "  \n",
    "  lifecycle_rule {\n",
    "    condition { age = 30 }\n",
    "    action {\n",
    "      type          = \"SetStorageClass\"\n",
    "      storage_class = \"NEARLINE\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# BigQuery Dataset\n",
    "resource \"google_bigquery_dataset\" \"analytics\" {\n",
    "  dataset_id    = \"analytics\"\n",
    "  friendly_name = \"Analytics Dataset\"\n",
    "  location      = var.region\n",
    "  \n",
    "  default_partition_expiration_ms = 7776000000  # 90 days\n",
    "}\n",
    "\n",
    "# Pub/Sub Topic\n",
    "resource \"google_pubsub_topic\" \"events\" {\n",
    "  name = \"events\"\n",
    "  \n",
    "  message_retention_duration = \"604800s\"  # 7 days\n",
    "}\n",
    "\n",
    "# Pub/Sub to BigQuery Subscription\n",
    "resource \"google_pubsub_subscription\" \"events_to_bq\" {\n",
    "  name  = \"events-to-bigquery\"\n",
    "  topic = google_pubsub_topic.events.name\n",
    "  \n",
    "  bigquery_config {\n",
    "    table            = \"${var.project_id}:${google_bigquery_dataset.analytics.dataset_id}.events_raw\"\n",
    "    use_topic_schema = true\n",
    "    write_metadata   = true\n",
    "  }\n",
    "}\n",
    "\n",
    "# Dataproc Autoscaling Policy\n",
    "resource \"google_dataproc_autoscaling_policy\" \"default\" {\n",
    "  policy_id = \"default-policy\"\n",
    "  location  = var.region\n",
    "\n",
    "  basic_algorithm {\n",
    "    yarn_config {\n",
    "      graceful_decommission_timeout = \"30s\"\n",
    "      scale_up_factor               = 0.5\n",
    "      scale_down_factor             = 0.5\n",
    "    }\n",
    "  }\n",
    "\n",
    "  worker_config {\n",
    "    min_instances = 2\n",
    "    max_instances = 10\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"Terraform configuration template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a359e4",
   "metadata": {},
   "source": [
    "### Cost Optimization Strategies\n",
    "\n",
    "| Service | Strategy | Potential Savings |\n",
    "|---------|----------|-------------------|\n",
    "| **GCS** | Use lifecycle policies for auto-tiering | 60-80% on cold data |\n",
    "| **GCS** | Compress files (gzip, snappy) | 50-70% storage |\n",
    "| **Dataflow** | Use Dataflow Prime for autoscaling | 20-40% compute |\n",
    "| **Dataproc** | Use preemptible/spot VMs for workers | 60-80% compute |\n",
    "| **Dataproc** | Ephemeral clusters (delete when idle) | 30-50% |\n",
    "| **BigQuery** | Partition and cluster tables | 50-90% query costs |\n",
    "| **BigQuery** | Use flat-rate pricing for predictable workloads | Varies |\n",
    "| **Pub/Sub** | Use BigQuery subscription (vs. processing) | 50%+ |\n",
    "| **All** | Reserve capacity with CUDs | 25-55% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba89069",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='takeaways'></a>\n",
    "## 7. Key Takeaways\n",
    "\n",
    "### Service Selection Guide\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│               When to Use Which Service?                            │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  Need to store files/objects?          ───► Google Cloud Storage   │\n",
    "│                                                                     │\n",
    "│  Need real-time messaging?             ───► Pub/Sub                 │\n",
    "│                                                                     │\n",
    "│  Need to run SQL analytics?            ───► BigQuery                │\n",
    "│                                                                     │\n",
    "│  Need to build new ETL pipelines?      ───► Dataflow                │\n",
    "│                                                                     │\n",
    "│  Have existing Spark/Hadoop jobs?      ───► Dataproc                │\n",
    "│                                                                     │\n",
    "│  Need ML on structured data?           ───► BigQuery ML             │\n",
    "│                                                                     │\n",
    "│  Need ML on unstructured data?         ───► Vertex AI + Dataproc    │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Service | Primary Use Case | Key Benefit |\n",
    "|---------|------------------|-------------|\n",
    "| **GCS** | Data lake storage | Scalable, durable, cost-effective |\n",
    "| **Dataflow** | Stream/batch ETL | Unified programming, auto-scaling |\n",
    "| **Dataproc** | Spark/Hadoop workloads | Managed clusters, ecosystem compat |\n",
    "| **BigQuery** | Data warehouse | Serverless, fast analytics |\n",
    "| **Pub/Sub** | Event streaming | Real-time, reliable messaging |\n",
    "\n",
    "### Integration Patterns\n",
    "\n",
    "1. **Streaming Analytics**: Pub/Sub → Dataflow → BigQuery\n",
    "2. **Batch ETL**: GCS → Dataflow/Dataproc → BigQuery\n",
    "3. **Data Lake**: GCS (landing) → GCS (processed) → BigQuery (serving)\n",
    "4. **ML Pipeline**: BigQuery → Vertex AI → GCS (models)\n",
    "5. **Log Analytics**: Cloud Logging → Pub/Sub → BigQuery\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "- [GCP Data Engineering Documentation](https://cloud.google.com/architecture/data-engineering)\n",
    "- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices-performance-overview)\n",
    "- [Dataflow Programming Guide](https://cloud.google.com/dataflow/docs/concepts/beam-programming-model)\n",
    "- [GCP Professional Data Engineer Certification](https://cloud.google.com/certification/data-engineer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
