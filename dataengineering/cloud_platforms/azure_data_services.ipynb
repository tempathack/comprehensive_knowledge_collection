{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e03437a",
   "metadata": {},
   "source": [
    "# Azure Data Engineering Services\n",
    "\n",
    "This notebook provides a comprehensive overview of Azure's data engineering services, covering storage, orchestration, processing, analytics, and streaming capabilities.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Azure Data Lake Storage Gen2](#azure-data-lake-storage-gen2)\n",
    "2. [Azure Data Factory](#azure-data-factory)\n",
    "3. [Azure Databricks](#azure-databricks)\n",
    "4. [Azure Synapse Analytics](#azure-synapse-analytics)\n",
    "5. [Azure Event Hubs](#azure-event-hubs)\n",
    "6. [Best Practices & Architecture Patterns](#best-practices--architecture-patterns)\n",
    "7. [Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f64d2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Azure Data Lake Storage Gen2\n",
    "\n",
    "**Azure Data Lake Storage Gen2 (ADLS Gen2)** is a highly scalable and cost-effective data lake solution for big data analytics. It combines the capabilities of Azure Blob Storage with a hierarchical file system.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Hierarchical Namespace** | True directory structure for efficient file operations |\n",
    "| **POSIX-compliant ACLs** | Fine-grained access control at file/folder level |\n",
    "| **Azure Blob API Compatible** | Works with existing blob storage tools and SDKs |\n",
    "| **Cost-Effective** | Hot, Cool, and Archive tiers for cost optimization |\n",
    "| **Hadoop Compatible** | ABFS driver for seamless Hadoop ecosystem integration |\n",
    "\n",
    "### Storage Hierarchy\n",
    "\n",
    "```\n",
    "Storage Account\n",
    "└── Container (File System)\n",
    "    └── Directory\n",
    "        └── Sub-Directory\n",
    "            └── Files (Blobs)\n",
    "```\n",
    "\n",
    "### Access Patterns\n",
    "\n",
    "- **Storage Account Key**: Full access (avoid in production)\n",
    "- **Shared Access Signature (SAS)**: Time-limited, scoped access\n",
    "- **Azure AD + RBAC**: Role-based access control\n",
    "- **ACLs**: POSIX-style permissions on files/directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Connecting to ADLS Gen2 with Azure SDK\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Using Azure AD authentication (recommended)\n",
    "account_name = \"your_storage_account\"\n",
    "account_url = f\"https://{account_name}.dfs.core.windows.net\"\n",
    "\n",
    "# DefaultAzureCredential supports multiple auth methods\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(account_url, credential=credential)\n",
    "\n",
    "# Create a file system (container)\n",
    "file_system_client = service_client.create_file_system(file_system=\"raw-data\")\n",
    "\n",
    "# Create a directory\n",
    "directory_client = file_system_client.create_directory(\"sales/2024\")\n",
    "\n",
    "# Upload a file\n",
    "file_client = directory_client.create_file(\"transactions.parquet\")\n",
    "with open(\"local_transactions.parquet\", \"rb\") as data:\n",
    "    file_client.upload_data(data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading from ADLS Gen2 with PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ADLS Gen2 Example\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.account.oauth.provider.type\", \n",
    "            \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ABFS URI format for ADLS Gen2\n",
    "storage_account = \"your_storage_account\"\n",
    "container = \"raw-data\"\n",
    "path = \"sales/2024/transactions.parquet\"\n",
    "\n",
    "# Read data using abfss:// protocol (secure)\n",
    "df = spark.read.parquet(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{path}\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb841ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Azure Data Factory\n",
    "\n",
    "**Azure Data Factory (ADF)** is a cloud-based ETL/ELT and data integration service for orchestrating data movement and transformation at scale.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Pipelines** | Logical grouping of activities that perform a unit of work |\n",
    "| **Activities** | Processing steps within a pipeline (Copy, Transform, Control) |\n",
    "| **Datasets** | Named views of data pointing to data stores |\n",
    "| **Linked Services** | Connection strings to data stores and compute resources |\n",
    "| **Triggers** | Units that determine when a pipeline should execute |\n",
    "| **Integration Runtime** | Compute infrastructure for data movement and dispatch |\n",
    "\n",
    "### Integration Runtime Types\n",
    "\n",
    "1. **Azure IR**: Managed compute in Azure regions\n",
    "2. **Self-Hosted IR**: On-premises or private network connectivity\n",
    "3. **Azure-SSIS IR**: Run SSIS packages in the cloud\n",
    "\n",
    "### Activity Types\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    ADF Activities                        │\n",
    "├─────────────────┬─────────────────┬─────────────────────┤\n",
    "│ Data Movement   │ Data Transform  │ Control Flow        │\n",
    "├─────────────────┼─────────────────┼─────────────────────┤\n",
    "│ Copy Activity   │ Data Flow       │ If Condition        │\n",
    "│                 │ Databricks      │ ForEach / Until     │\n",
    "│                 │ HDInsight       │ Switch              │\n",
    "│                 │ Stored Procedure│ Wait / Web          │\n",
    "│                 │ Azure Functions │ Execute Pipeline    │\n",
    "└─────────────────┴─────────────────┴─────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8551617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating ADF Pipeline programmatically\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "from azure.mgmt.datafactory.models import *\n",
    "\n",
    "# Initialize client\n",
    "credential = DefaultAzureCredential()\n",
    "subscription_id = \"your-subscription-id\"\n",
    "resource_group = \"your-resource-group\"\n",
    "factory_name = \"your-data-factory\"\n",
    "\n",
    "adf_client = DataFactoryManagementClient(credential, subscription_id)\n",
    "\n",
    "# Create a Linked Service to ADLS Gen2\n",
    "ls_adls = LinkedServiceResource(\n",
    "    properties=AzureBlobFSLinkedService(\n",
    "        url=f\"https://yourstorageaccount.dfs.core.windows.net\"\n",
    "    )\n",
    ")\n",
    "adf_client.linked_services.create_or_update(\n",
    "    resource_group, factory_name, \"ls_adls_gen2\", ls_adls\n",
    ")\n",
    "\n",
    "# Create a Dataset\n",
    "ds_parquet = DatasetResource(\n",
    "    properties=ParquetDataset(\n",
    "        linked_service_name=LinkedServiceReference(\n",
    "            reference_name=\"ls_adls_gen2\"\n",
    "        ),\n",
    "        location=AzureBlobFSLocation(\n",
    "            file_system=\"raw-data\",\n",
    "            folder_path=\"sales/2024\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "adf_client.datasets.create_or_update(\n",
    "    resource_group, factory_name, \"ds_sales_parquet\", ds_parquet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a Copy Pipeline\n",
    "copy_activity = CopyActivity(\n",
    "    name=\"CopyFromBlobToADLS\",\n",
    "    inputs=[DatasetReference(reference_name=\"ds_source_blob\")],\n",
    "    outputs=[DatasetReference(reference_name=\"ds_sales_parquet\")],\n",
    "    source=BlobSource(),\n",
    "    sink=ParquetSink()\n",
    ")\n",
    "\n",
    "# Create pipeline with activities\n",
    "pipeline = PipelineResource(\n",
    "    activities=[copy_activity],\n",
    "    parameters={\n",
    "        \"inputPath\": ParameterSpecification(type=\"String\"),\n",
    "        \"outputPath\": ParameterSpecification(type=\"String\")\n",
    "    }\n",
    ")\n",
    "\n",
    "adf_client.pipelines.create_or_update(\n",
    "    resource_group, factory_name, \"pl_copy_sales_data\", pipeline\n",
    ")\n",
    "\n",
    "# Create a Schedule Trigger\n",
    "trigger = TriggerResource(\n",
    "    properties=ScheduleTrigger(\n",
    "        pipelines=[\n",
    "            TriggerPipelineReference(\n",
    "                pipeline_reference=PipelineReference(reference_name=\"pl_copy_sales_data\"),\n",
    "                parameters={\"inputPath\": \"source/\", \"outputPath\": \"dest/\"}\n",
    "            )\n",
    "        ],\n",
    "        recurrence=ScheduleTriggerRecurrence(\n",
    "            frequency=\"Day\",\n",
    "            interval=1,\n",
    "            start_time=\"2024-01-01T00:00:00Z\",\n",
    "            time_zone=\"UTC\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "adf_client.triggers.create_or_update(\n",
    "    resource_group, factory_name, \"tr_daily_sales_copy\", trigger\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014db26a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Azure Databricks\n",
    "\n",
    "**Azure Databricks** is a fast, easy, and collaborative Apache Spark-based analytics platform optimized for Azure.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Workspace** | Environment for accessing Databricks assets |\n",
    "| **Clusters** | Managed Spark compute resources |\n",
    "| **Notebooks** | Collaborative code documents |\n",
    "| **Jobs** | Scheduled or triggered notebook/JAR execution |\n",
    "| **Delta Lake** | ACID transactions on data lakes |\n",
    "| **Unity Catalog** | Unified governance for data and AI |\n",
    "\n",
    "### Cluster Types\n",
    "\n",
    "1. **All-Purpose Clusters**: Interactive analysis, shared among users\n",
    "2. **Job Clusters**: Ephemeral, created for job runs, terminated after\n",
    "3. **SQL Warehouses**: Optimized for SQL analytics workloads\n",
    "\n",
    "### Delta Lake Benefits\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│                      Delta Lake                           │\n",
    "├──────────────────────────────────────────────────────────┤\n",
    "│  ✓ ACID Transactions    ✓ Schema Enforcement             │\n",
    "│  ✓ Time Travel          ✓ Unified Batch + Streaming      │\n",
    "│  ✓ Scalable Metadata    ✓ Data Versioning                │\n",
    "│  ✓ Z-Order Indexing     ✓ Auto-Optimization              │\n",
    "└──────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f55efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Delta Lake Operations in Azure Databricks\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake Example\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a Delta table\n",
    "delta_path = \"abfss://curated@storage.dfs.core.windows.net/sales_delta\"\n",
    "\n",
    "df = spark.read.parquet(\"abfss://raw@storage.dfs.core.windows.net/sales/\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Register as a table in metastore\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS sales_delta\n",
    "    USING DELTA\n",
    "    LOCATION '{delta_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Delta Lake Merge (Upsert) Operation\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load existing Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# New/updated records\n",
    "updates_df = spark.read.parquet(\"abfss://raw@storage.dfs.core.windows.net/sales_updates/\")\n",
    "\n",
    "# Merge operation (upsert)\n",
    "delta_table.alias(\"target\") \\\n",
    "    .merge(\n",
    "        updates_df.alias(\"source\"),\n",
    "        \"target.transaction_id = source.transaction_id\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set={\n",
    "        \"amount\": col(\"source.amount\"),\n",
    "        \"updated_at\": current_timestamp()\n",
    "    }) \\\n",
    "    .whenNotMatchedInsert(values={\n",
    "        \"transaction_id\": col(\"source.transaction_id\"),\n",
    "        \"amount\": col(\"source.amount\"),\n",
    "        \"created_at\": current_timestamp(),\n",
    "        \"updated_at\": current_timestamp()\n",
    "    }) \\\n",
    "    .execute()\n",
    "\n",
    "# Time Travel: Query previous version\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "\n",
    "# View table history\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Structured Streaming with Delta Lake\n",
    "# Read streaming data\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"abfss://config@storage.dfs.core.windows.net/schema/\") \\\n",
    "    .load(\"abfss://landing@storage.dfs.core.windows.net/events/\")\n",
    "\n",
    "# Write to Delta table with checkpointing\n",
    "query = streaming_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"abfss://config@storage.dfs.core.windows.net/checkpoints/events/\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start(\"abfss://curated@storage.dfs.core.windows.net/events_delta/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af2f5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Azure Synapse Analytics\n",
    "\n",
    "**Azure Synapse Analytics** is an integrated analytics service that combines enterprise data warehousing and big data analytics.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Synapse Workspace** | Unified experience for data engineering and analytics |\n",
    "| **Dedicated SQL Pool** | Enterprise data warehouse (formerly SQL DW) |\n",
    "| **Serverless SQL Pool** | Query data lake without loading |\n",
    "| **Apache Spark Pool** | Big data processing and ML |\n",
    "| **Synapse Pipelines** | Data orchestration (ADF-compatible) |\n",
    "| **Synapse Link** | Real-time analytics on operational data |\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────────────────┐\n",
    "                    │        Azure Synapse Studio         │\n",
    "                    │    (Unified Development Experience) │\n",
    "                    └─────────────────────────────────────┘\n",
    "                                      │\n",
    "        ┌─────────────────────────────┼─────────────────────────────┐\n",
    "        │                             │                             │\n",
    "        ▼                             ▼                             ▼\n",
    "┌───────────────┐           ┌───────────────┐           ┌───────────────┐\n",
    "│ Dedicated SQL │           │ Serverless    │           │ Apache Spark  │\n",
    "│ Pool (MPP)    │           │ SQL Pool      │           │ Pool          │\n",
    "├───────────────┤           ├───────────────┤           ├───────────────┤\n",
    "│ - EDW         │           │ - Ad-hoc      │           │ - Big Data    │\n",
    "│ - PolyBase    │           │ - Pay per TB  │           │ - ML/AI       │\n",
    "│ - Materialized│           │ - No loading  │           │ - Delta Lake  │\n",
    "│   Views       │           │ - OPENROWSET  │           │ - Notebooks   │\n",
    "└───────────────┘           └───────────────┘           └───────────────┘\n",
    "        │                             │                             │\n",
    "        └─────────────────────────────┴─────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "                    ┌─────────────────────────────────────┐\n",
    "                    │     Azure Data Lake Storage Gen2    │\n",
    "                    │         (Primary Storage)           │\n",
    "                    └─────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Serverless SQL Pool - Query Parquet in Data Lake\n",
    "\n",
    "# SQL script to query data lake files directly\n",
    "serverless_sql_query = \"\"\"\n",
    "-- Query Parquet files in ADLS Gen2 using OPENROWSET\n",
    "SELECT \n",
    "    customer_id,\n",
    "    product_category,\n",
    "    SUM(amount) AS total_amount,\n",
    "    COUNT(*) AS transaction_count\n",
    "FROM OPENROWSET(\n",
    "    BULK 'https://storage.dfs.core.windows.net/curated/sales/**/*.parquet',\n",
    "    FORMAT = 'PARQUET'\n",
    ") AS sales\n",
    "GROUP BY customer_id, product_category\n",
    "ORDER BY total_amount DESC;\n",
    "\n",
    "-- Create an external table for reusable access\n",
    "CREATE EXTERNAL TABLE sales_external (\n",
    "    transaction_id VARCHAR(50),\n",
    "    customer_id VARCHAR(50),\n",
    "    product_category VARCHAR(100),\n",
    "    amount DECIMAL(18,2),\n",
    "    transaction_date DATE\n",
    ")\n",
    "WITH (\n",
    "    LOCATION = 'curated/sales/',\n",
    "    DATA_SOURCE = adls_curated,\n",
    "    FILE_FORMAT = parquet_format\n",
    ");\n",
    "\"\"\"\n",
    "print(serverless_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Dedicated SQL Pool - Data Warehouse Patterns\n",
    "\n",
    "dedicated_sql_patterns = \"\"\"\n",
    "-- Create dimension table with proper distribution\n",
    "CREATE TABLE dim_customer\n",
    "WITH (\n",
    "    DISTRIBUTION = REPLICATE,  -- Small dimension tables\n",
    "    CLUSTERED COLUMNSTORE INDEX\n",
    ")\n",
    "AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    customer_segment,\n",
    "    region\n",
    "FROM staging.customers;\n",
    "\n",
    "-- Create fact table with hash distribution\n",
    "CREATE TABLE fact_sales\n",
    "WITH (\n",
    "    DISTRIBUTION = HASH(customer_id),  -- Large fact tables\n",
    "    CLUSTERED COLUMNSTORE INDEX,\n",
    "    PARTITION (transaction_date RANGE RIGHT FOR VALUES (\n",
    "        '2024-01-01', '2024-02-01', '2024-03-01', '2024-04-01'\n",
    "    ))\n",
    ")\n",
    "AS\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    amount,\n",
    "    transaction_date\n",
    "FROM staging.sales;\n",
    "\n",
    "-- Create materialized view for common aggregations\n",
    "CREATE MATERIALIZED VIEW mv_daily_sales\n",
    "WITH (DISTRIBUTION = HASH(customer_id))\n",
    "AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    CAST(transaction_date AS DATE) AS sale_date,\n",
    "    SUM(amount) AS daily_total,\n",
    "    COUNT(*) AS transaction_count\n",
    "FROM fact_sales\n",
    "GROUP BY customer_id, CAST(transaction_date AS DATE);\n",
    "\"\"\"\n",
    "print(dedicated_sql_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00513082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Synapse Spark Pool - Integration with SQL Pools\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# In Synapse Spark notebook\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read from Dedicated SQL Pool using Synapse connector\n",
    "df_from_sql = spark.read \\\n",
    "    .synapsesql(\"dedicated_pool.dbo.fact_sales\") \\\n",
    "    .filter(\"transaction_date >= '2024-01-01'\")\n",
    "\n",
    "# Process with Spark\n",
    "df_aggregated = df_from_sql.groupBy(\"customer_id\") \\\n",
    "    .agg({\"amount\": \"sum\", \"transaction_id\": \"count\"})\n",
    "\n",
    "# Write results back to SQL Pool\n",
    "df_aggregated.write \\\n",
    "    .synapsesql(\n",
    "        \"dedicated_pool.dbo.customer_summary\",\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "\n",
    "# Read from Serverless SQL Pool external table\n",
    "df_external = spark.read \\\n",
    "    .synapsesql(\"serverless_pool.dbo.sales_external\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3e6af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Azure Event Hubs\n",
    "\n",
    "**Azure Event Hubs** is a big data streaming platform and event ingestion service capable of receiving and processing millions of events per second.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Partitions** | Ordered sequence of events, enables parallelism |\n",
    "| **Consumer Groups** | View (state, position, offset) of entire event hub |\n",
    "| **Capture** | Automatic data capture to ADLS or Blob Storage |\n",
    "| **Schema Registry** | Centralized schema management with Avro support |\n",
    "| **Kafka Compatible** | Use Kafka clients/protocols with Event Hubs |\n",
    "\n",
    "### Pricing Tiers\n",
    "\n",
    "| Tier | Throughput Units | Use Case |\n",
    "|------|------------------|----------|\n",
    "| Basic | 1 TU (1 MB/s in, 2 MB/s out) | Development/Testing |\n",
    "| Standard | Up to 40 TUs | Production workloads |\n",
    "| Premium | Processing Units (PUs) | High-throughput, isolation |\n",
    "| Dedicated | Capacity Units (CUs) | Enterprise, guaranteed capacity |\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Producers                   Event Hub                    Consumers\n",
    "─────────                   ─────────                    ─────────\n",
    "                     ┌────────────────────┐\n",
    " IoT Devices    ────►│   Partition 0      │────►  Spark Streaming\n",
    "                     ├────────────────────┤\n",
    " Applications   ────►│   Partition 1      │────►  Azure Functions\n",
    "                     ├────────────────────┤\n",
    " Services       ────►│   Partition 2      │────►  Stream Analytics\n",
    "                     ├────────────────────┤\n",
    " Kafka Clients  ────►│   Partition N      │────►  Databricks\n",
    "                     └────────────────────┘\n",
    "                              │\n",
    "                              ▼ (Capture)\n",
    "                     ┌────────────────────┐\n",
    "                     │   ADLS Gen2        │\n",
    "                     │   (Avro format)    │\n",
    "                     └────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19261bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sending Events to Event Hubs\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import json\n",
    "\n",
    "# Using Azure AD authentication\n",
    "credential = DefaultAzureCredential()\n",
    "eventhub_namespace = \"your-namespace.servicebus.windows.net\"\n",
    "eventhub_name = \"sales-events\"\n",
    "\n",
    "producer = EventHubProducerClient(\n",
    "    fully_qualified_namespace=eventhub_namespace,\n",
    "    eventhub_name=eventhub_name,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Send batch of events\n",
    "async def send_events():\n",
    "    async with producer:\n",
    "        # Create a batch\n",
    "        event_batch = await producer.create_batch()\n",
    "        \n",
    "        events = [\n",
    "            {\"event_type\": \"sale\", \"customer_id\": \"C001\", \"amount\": 150.00},\n",
    "            {\"event_type\": \"sale\", \"customer_id\": \"C002\", \"amount\": 299.99},\n",
    "            {\"event_type\": \"return\", \"customer_id\": \"C001\", \"amount\": -50.00}\n",
    "        ]\n",
    "        \n",
    "        for event in events:\n",
    "            event_batch.add(EventData(json.dumps(event)))\n",
    "        \n",
    "        await producer.send_batch(event_batch)\n",
    "        print(f\"Sent {len(events)} events\")\n",
    "\n",
    "# import asyncio\n",
    "# asyncio.run(send_events())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3206d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Consuming Events from Event Hubs\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "from azure.eventhub.extensions.checkpointstoreblob import BlobCheckpointStore\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Checkpoint store for tracking progress\n",
    "checkpoint_store = BlobCheckpointStore(\n",
    "    blob_account_url=\"https://storage.blob.core.windows.net\",\n",
    "    container_name=\"eventhub-checkpoints\",\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "consumer = EventHubConsumerClient(\n",
    "    fully_qualified_namespace=\"your-namespace.servicebus.windows.net\",\n",
    "    eventhub_name=\"sales-events\",\n",
    "    consumer_group=\"$Default\",\n",
    "    credential=credential,\n",
    "    checkpoint_store=checkpoint_store\n",
    ")\n",
    "\n",
    "async def on_event(partition_context, event):\n",
    "    \"\"\"Process incoming events\"\"\"\n",
    "    print(f\"Partition: {partition_context.partition_id}\")\n",
    "    print(f\"Event: {event.body_as_str()}\")\n",
    "    \n",
    "    # Checkpoint after processing\n",
    "    await partition_context.update_checkpoint(event)\n",
    "\n",
    "async def receive_events():\n",
    "    async with consumer:\n",
    "        await consumer.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\"  # From beginning\n",
    "        )\n",
    "\n",
    "# import asyncio\n",
    "# asyncio.run(receive_events())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Spark Structured Streaming from Event Hubs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EventHubs Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Connection configuration\n",
    "connection_string = \"Endpoint=sb://namespace.servicebus.windows.net/;...\" \n",
    "eh_conf = {\n",
    "    \"eventhubs.connectionString\": sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "}\n",
    "\n",
    "# Define schema for incoming events\n",
    "event_schema = StructType() \\\n",
    "    .add(\"event_type\", StringType()) \\\n",
    "    .add(\"customer_id\", StringType()) \\\n",
    "    .add(\"amount\", DoubleType())\n",
    "\n",
    "# Read from Event Hubs\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**eh_conf) \\\n",
    "    .load()\n",
    "\n",
    "# Parse event body\n",
    "df_parsed = df_stream \\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\")) \\\n",
    "    .withColumn(\"event\", from_json(col(\"body\"), event_schema)) \\\n",
    "    .select(\n",
    "        col(\"event.event_type\"),\n",
    "        col(\"event.customer_id\"),\n",
    "        col(\"event.amount\"),\n",
    "        col(\"enqueuedTime\").alias(\"event_time\")\n",
    "    )\n",
    "\n",
    "# Write to Delta Lake\n",
    "query = df_parsed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"abfss://checkpoints@storage.dfs.core.windows.net/eventhubs/\") \\\n",
    "    .start(\"abfss://curated@storage.dfs.core.windows.net/streaming_events/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f7327",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices & Architecture Patterns\n",
    "\n",
    "### 1. Medallion Architecture (Bronze-Silver-Gold)\n",
    "\n",
    "A data design pattern used to organize data in a lakehouse.\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│                         Medallion Architecture                            │\n",
    "├──────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                           │\n",
    "│   ┌─────────────┐      ┌─────────────┐      ┌─────────────┐              │\n",
    "│   │   BRONZE    │      │   SILVER    │      │    GOLD     │              │\n",
    "│   │   (Raw)     │ ───► │  (Cleansed) │ ───► │  (Curated)  │              │\n",
    "│   └─────────────┘      └─────────────┘      └─────────────┘              │\n",
    "│                                                                           │\n",
    "│   • Raw ingestion       • Cleansed        • Business-level               │\n",
    "│   • Schema-on-read      • Deduplicated    • Aggregated                   │\n",
    "│   • Append-only         • Validated       • Star schema                  │\n",
    "│   • Full history        • Conformed       • Consumption-ready            │\n",
    "│                                                                           │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 2. Lambda vs Kappa Architecture\n",
    "\n",
    "| Aspect | Lambda | Kappa |\n",
    "|--------|--------|-------|\n",
    "| **Processing** | Batch + Stream (separate) | Stream only |\n",
    "| **Complexity** | Higher (two systems) | Lower (single system) |\n",
    "| **Reprocessing** | Batch layer | Replay stream |\n",
    "| **Use Case** | Legacy + Real-time | Streaming-first |\n",
    "| **Azure Services** | ADF + Event Hubs + Spark | Event Hubs + Spark Streaming |\n",
    "\n",
    "### 3. Security Best Practices\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│                        Security Layers                                    │\n",
    "├──────────────────────────────────────────────────────────────────────────┤\n",
    "│  1. Network Security                                                      │\n",
    "│     • Private Endpoints for all services                                  │\n",
    "│     • VNet integration                                                    │\n",
    "│     • Network Security Groups (NSGs)                                      │\n",
    "│                                                                           │\n",
    "│  2. Identity & Access                                                     │\n",
    "│     • Azure AD + Managed Identities                                       │\n",
    "│     • RBAC for resource access                                            │\n",
    "│     • ACLs for fine-grained data access                                   │\n",
    "│                                                                           │\n",
    "│  3. Data Protection                                                       │\n",
    "│     • Encryption at rest (Azure-managed or CMK)                           │\n",
    "│     • Encryption in transit (TLS 1.2+)                                    │\n",
    "│     • Data masking for sensitive fields                                   │\n",
    "│                                                                           │\n",
    "│  4. Governance                                                            │\n",
    "│     • Unity Catalog / Microsoft Purview                                   │\n",
    "│     • Data lineage tracking                                               │\n",
    "│     • Audit logging                                                       │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: End-to-End Data Pipeline Architecture\n",
    "\n",
    "architecture_diagram = \"\"\"\n",
    "                                AZURE DATA PLATFORM ARCHITECTURE\n",
    "══════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "    DATA SOURCES                    INGESTION                      STORAGE\n",
    "    ───────────                    ─────────                      ───────\n",
    "                                                                    \n",
    "    ┌───────────┐                ┌─────────────┐              ┌──────────────┐\n",
    "    │ Databases │───────────────►│             │              │   ADLS Gen2  │\n",
    "    └───────────┘                │   Azure     │              │              │\n",
    "                                 │   Data      │──────────────►  ┌────────┐  │\n",
    "    ┌───────────┐                │   Factory   │              │  │ Bronze │  │\n",
    "    │   APIs    │───────────────►│             │              │  └────────┘  │\n",
    "    └───────────┘                └─────────────┘              │       │      │\n",
    "                                                              │       ▼      │\n",
    "    ┌───────────┐                ┌─────────────┐              │  ┌────────┐  │\n",
    "    │ IoT/Events│───────────────►│ Event Hubs  │──────────────►  │ Silver │  │\n",
    "    └───────────┘                └─────────────┘              │  └────────┘  │\n",
    "                                                              │       │      │\n",
    "    ┌───────────┐                ┌─────────────┐              │       ▼      │\n",
    "    │   Files   │───────────────►│   Logic     │──────────────►  ┌────────┐  │\n",
    "    └───────────┘                │   Apps      │              │  │  Gold  │  │\n",
    "                                 └─────────────┘              │  └────────┘  │\n",
    "                                                              └──────────────┘\n",
    "                                                                     │\n",
    "    ═══════════════════════════════════════════════════════════════════════════\n",
    "    \n",
    "    PROCESSING                                                SERVING\n",
    "    ──────────                                                ───────\n",
    "                                                                     │\n",
    "    ┌──────────────────────────────┐                                 ▼\n",
    "    │       Azure Databricks       │◄────────────────┬───────────────┤\n",
    "    │  • ETL/ELT Processing        │                 │               │\n",
    "    │  • ML/AI Workloads           │                 │               │\n",
    "    │  • Delta Lake                │                 │               ▼\n",
    "    └──────────────────────────────┘                 │        ┌─────────────┐\n",
    "                                                     │        │  Synapse    │\n",
    "    ┌──────────────────────────────┐                 │        │  Analytics  │\n",
    "    │     Azure Synapse Spark      │◄────────────────┤        │  Dedicated  │\n",
    "    │  • Big Data Processing       │                 │        │  SQL Pool   │\n",
    "    │  • Synapse Link              │                 │        └─────────────┘\n",
    "    └──────────────────────────────┘                 │               │\n",
    "                                                     │               ▼\n",
    "    ═══════════════════════════════════════════════════════════════════════════\n",
    "    \n",
    "    CONSUMPTION\n",
    "    ───────────\n",
    "                                 │                               │\n",
    "                                 ▼                               ▼\n",
    "                          ┌─────────────┐                 ┌─────────────┐\n",
    "                          │  Power BI   │                 │  Custom     │\n",
    "                          │  Dashboards │                 │  Apps/APIs  │\n",
    "                          └─────────────┘                 └─────────────┘\n",
    "\"\"\"\n",
    "print(architecture_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b48158",
   "metadata": {},
   "source": [
    "### 4. Cost Optimization Strategies\n",
    "\n",
    "| Service | Strategy |\n",
    "|---------|----------|\n",
    "| **ADLS Gen2** | Lifecycle policies, tiered storage (Hot/Cool/Archive) |\n",
    "| **ADF** | Use mapping data flows only when needed, optimize IR |\n",
    "| **Databricks** | Auto-scaling, spot instances, cluster pools |\n",
    "| **Synapse** | Pause dedicated pools, use serverless for ad-hoc |\n",
    "| **Event Hubs** | Right-size throughput units, enable auto-inflate |\n",
    "\n",
    "### 5. Monitoring & Observability\n",
    "\n",
    "- **Azure Monitor**: Centralized metrics and logs\n",
    "- **Log Analytics**: Query and analyze telemetry\n",
    "- **Application Insights**: End-to-end tracing\n",
    "- **Azure Alerts**: Proactive notifications\n",
    "- **Databricks Observability**: Ganglia, Spark UI, Query History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41f346",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "### Key Points Summary\n",
    "\n",
    "| Service | Primary Use | Key Benefit |\n",
    "|---------|-------------|-------------|\n",
    "| **ADLS Gen2** | Data Lake Storage | Hierarchical namespace + cost-effective |\n",
    "| **Azure Data Factory** | Orchestration & ETL | 90+ connectors, visual pipelines |\n",
    "| **Azure Databricks** | Big Data Processing | Unified analytics, Delta Lake |\n",
    "| **Azure Synapse** | Analytics Hub | SQL + Spark + Pipelines unified |\n",
    "| **Azure Event Hubs** | Event Streaming | Millions of events/second |\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                          Decision Guide                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                          │\n",
    "│  Need batch orchestration?          → Azure Data Factory                 │\n",
    "│                                                                          │\n",
    "│  Need complex transformations?      → Databricks or Synapse Spark        │\n",
    "│                                                                          │\n",
    "│  Need real-time streaming?          → Event Hubs + Spark Streaming       │\n",
    "│                                                                          │\n",
    "│  Need SQL-based analytics?          → Synapse SQL (Serverless/Dedicated) │\n",
    "│                                                                          │\n",
    "│  Need ML/AI workloads?              → Azure Databricks                   │\n",
    "│                                                                          │\n",
    "│  Need enterprise data warehouse?    → Synapse Dedicated SQL Pool         │\n",
    "│                                                                          │\n",
    "│  Need ad-hoc data exploration?      → Synapse Serverless SQL Pool        │\n",
    "│                                                                          │\n",
    "│  Need ACID on data lake?            → Delta Lake (Databricks/Synapse)    │\n",
    "│                                                                          │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Essential Best Practices\n",
    "\n",
    "1. **Security First**: Use Managed Identities, Private Endpoints, and proper RBAC\n",
    "2. **Medallion Architecture**: Organize data in Bronze → Silver → Gold layers\n",
    "3. **Delta Lake**: Enable ACID transactions and time travel on your data lake\n",
    "4. **Cost Management**: Use lifecycle policies, auto-pause, and right-sizing\n",
    "5. **Monitoring**: Implement comprehensive observability from day one\n",
    "6. **Governance**: Use Unity Catalog or Microsoft Purview for data governance\n",
    "7. **Idempotency**: Design pipelines to be safely re-runnable\n",
    "8. **Partition Strategy**: Plan partitioning based on query patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
