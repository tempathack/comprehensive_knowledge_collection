{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de1548c",
   "metadata": {},
   "source": [
    "# AWS Data Engineering Services\n",
    "\n",
    "This notebook provides a comprehensive overview of AWS services commonly used in data engineering workflows. We'll explore storage, ETL, big data processing, streaming, and serverless computing services that form the backbone of modern data platforms on AWS.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Amazon S3 - Data Lake Storage](#1-amazon-s3---data-lake-storage)\n",
    "2. [AWS Glue - ETL and Data Catalog](#2-aws-glue---etl-and-data-catalog)\n",
    "3. [Amazon EMR - Big Data Processing](#3-amazon-emr---big-data-processing)\n",
    "4. [Amazon Kinesis - Real-Time Streaming](#4-amazon-kinesis---real-time-streaming)\n",
    "5. [AWS Lambda - Serverless Processing](#5-aws-lambda---serverless-processing)\n",
    "6. [Best Practices and Architecture Patterns](#6-best-practices-and-architecture-patterns)\n",
    "7. [Key Takeaways](#7-key-takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92efb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Amazon S3 - Data Lake Storage\n",
    "\n",
    "**Amazon Simple Storage Service (S3)** is the foundation of most AWS data engineering architectures, serving as a highly scalable, durable, and cost-effective object storage service.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Durability** | 99.999999999% (11 9's) durability |\n",
    "| **Availability** | 99.99% availability SLA |\n",
    "| **Scalability** | Virtually unlimited storage |\n",
    "| **Storage Classes** | Standard, Intelligent-Tiering, Glacier, etc. |\n",
    "| **Security** | Encryption at rest and in transit, IAM policies, bucket policies |\n",
    "\n",
    "### S3 Storage Classes for Data Lakes\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      S3 Storage Classes                         │\n",
    "├─────────────────────┬───────────────────┬───────────────────────┤\n",
    "│     Hot Data        │   Warm Data       │      Cold Data        │\n",
    "├─────────────────────┼───────────────────┼───────────────────────┤\n",
    "│  S3 Standard        │  S3 Standard-IA   │  S3 Glacier Instant   │\n",
    "│  (Frequent access)  │  (Infrequent)     │  S3 Glacier Flexible  │\n",
    "│                     │  S3 One Zone-IA   │  S3 Glacier Deep      │\n",
    "└─────────────────────┴───────────────────┴───────────────────────┘\n",
    "```\n",
    "\n",
    "### Data Lake Organization Pattern\n",
    "\n",
    "```\n",
    "s3://my-data-lake/\n",
    "├── raw/                    # Landing zone (Bronze)\n",
    "│   ├── source1/\n",
    "│   │   └── year=2024/month=01/day=15/\n",
    "│   └── source2/\n",
    "├── processed/              # Cleaned data (Silver)\n",
    "│   ├── domain1/\n",
    "│   └── domain2/\n",
    "├── curated/                # Business-ready (Gold)\n",
    "│   ├── analytics/\n",
    "│   └── ml-features/\n",
    "└── archive/                # Historical data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc56047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Working with S3 using boto3\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = 'my-data-lake'\n",
    "RAW_PREFIX = 'raw/'\n",
    "PROCESSED_PREFIX = 'processed/'\n",
    "\n",
    "def upload_to_raw_zone(local_file: str, source_name: str) -> str:\n",
    "    \"\"\"Upload file to raw zone with date partitioning.\"\"\"\n",
    "    now = datetime.now()\n",
    "    s3_key = f\"{RAW_PREFIX}{source_name}/year={now.year}/month={now.month:02d}/day={now.day:02d}/{local_file}\"\n",
    "    \n",
    "    s3_client.upload_file(\n",
    "        Filename=local_file,\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=s3_key,\n",
    "        ExtraArgs={'ServerSideEncryption': 'AES256'}\n",
    "    )\n",
    "    return f\"s3://{BUCKET_NAME}/{s3_key}\"\n",
    "\n",
    "def list_objects_with_prefix(prefix: str, max_keys: int = 100) -> list:\n",
    "    \"\"\"List objects in S3 with a given prefix.\"\"\"\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Prefix=prefix,\n",
    "        MaxKeys=max_keys\n",
    "    )\n",
    "    return [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "# Example usage\n",
    "# s3_path = upload_to_raw_zone('data.csv', 'sales_system')\n",
    "# print(f\"Uploaded to: {s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ed538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Lifecycle Policy Example (JSON configuration)\n",
    "lifecycle_policy = {\n",
    "    \"Rules\": [\n",
    "        {\n",
    "            \"ID\": \"MoveToGlacierAfter90Days\",\n",
    "            \"Status\": \"Enabled\",\n",
    "            \"Filter\": {\"Prefix\": \"raw/\"},\n",
    "            \"Transitions\": [\n",
    "                {\n",
    "                    \"Days\": 30,\n",
    "                    \"StorageClass\": \"STANDARD_IA\"\n",
    "                },\n",
    "                {\n",
    "                    \"Days\": 90,\n",
    "                    \"StorageClass\": \"GLACIER\"\n",
    "                }\n",
    "            ],\n",
    "            \"Expiration\": {\n",
    "                \"Days\": 365\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"ID\": \"DeleteIncompleteMultipartUploads\",\n",
    "            \"Status\": \"Enabled\",\n",
    "            \"Filter\": {\"Prefix\": \"\"},\n",
    "            \"AbortIncompleteMultipartUpload\": {\n",
    "                \"DaysAfterInitiation\": 7\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Apply lifecycle policy\n",
    "# s3_client.put_bucket_lifecycle_configuration(\n",
    "#     Bucket=BUCKET_NAME,\n",
    "#     LifecycleConfiguration=lifecycle_policy\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140bb6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. AWS Glue - ETL and Data Catalog\n",
    "\n",
    "**AWS Glue** is a fully managed ETL (Extract, Transform, Load) service that also provides a centralized metadata repository called the **Glue Data Catalog**.\n",
    "\n",
    "### AWS Glue Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                         AWS Glue Architecture                        │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  ┌──────────────┐    ┌──────────────┐    ┌──────────────────────┐   │\n",
    "│  │   Crawlers   │───▶│ Data Catalog │◀───│   External Tools     │   │\n",
    "│  │              │    │  (Databases, │    │  (Athena, Redshift,  │   │\n",
    "│  │ Auto-discover│    │   Tables,    │    │   EMR, QuickSight)   │   │\n",
    "│  │   schemas    │    │  Partitions) │    │                      │   │\n",
    "│  └──────────────┘    └──────────────┘    └──────────────────────┘   │\n",
    "│                              │                                       │\n",
    "│                              ▼                                       │\n",
    "│  ┌───────────────────────────────────────────────────────────────┐  │\n",
    "│  │                      Glue ETL Jobs                            │  │\n",
    "│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐   │  │\n",
    "│  │  │ Spark Jobs  │  │ Python Shell│  │ Glue Studio (Visual)│   │  │\n",
    "│  │  │ (PySpark)   │  │   Jobs      │  │                     │   │  │\n",
    "│  │  └─────────────┘  └─────────────┘  └─────────────────────┘   │  │\n",
    "│  └───────────────────────────────────────────────────────────────┘  │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Component | Description | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| **Crawlers** | Automatically discover schema and populate catalog | Schema discovery, partition detection |\n",
    "| **Data Catalog** | Centralized metadata repository | Schema management, data discovery |\n",
    "| **ETL Jobs** | Serverless Spark jobs for transformation | Data transformation, cleansing |\n",
    "| **Glue Studio** | Visual ETL job authoring | Low-code ETL development |\n",
    "| **DataBrew** | Visual data preparation | Data profiling, cleansing |\n",
    "| **Workflows** | Orchestrate ETL jobs | Complex pipeline orchestration |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275244f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: AWS Glue ETL Job (PySpark)\n",
    "# This script would run as an AWS Glue job\n",
    "\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, to_date, year, month\n",
    "\n",
    "# Initialize Glue context\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Read from Glue Data Catalog\n",
    "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"sales_database\",\n",
    "    table_name=\"raw_transactions\",\n",
    "    transformation_ctx=\"datasource\"\n",
    ")\n",
    "\n",
    "# Apply mappings (rename and cast columns)\n",
    "mapped_df = ApplyMapping.apply(\n",
    "    frame=datasource,\n",
    "    mappings=[\n",
    "        (\"transaction_id\", \"string\", \"transaction_id\", \"string\"),\n",
    "        (\"amount\", \"string\", \"amount\", \"decimal(10,2)\"),\n",
    "        (\"transaction_date\", \"string\", \"transaction_date\", \"date\"),\n",
    "        (\"customer_id\", \"string\", \"customer_id\", \"string\")\n",
    "    ],\n",
    "    transformation_ctx=\"mapped_df\"\n",
    ")\n",
    "\n",
    "# Convert to Spark DataFrame for complex transformations\n",
    "df = mapped_df.toDF()\n",
    "\n",
    "# Add partition columns\n",
    "df_partitioned = df.withColumn(\"year\", year(col(\"transaction_date\"))) \\\n",
    "                   .withColumn(\"month\", month(col(\"transaction_date\")))\n",
    "\n",
    "# Filter out invalid records\n",
    "df_clean = df_partitioned.filter(col(\"amount\") > 0)\n",
    "\n",
    "# Convert back to DynamicFrame\n",
    "output_dyf = DynamicFrame.fromDF(df_clean, glueContext, \"output_dyf\")\n",
    "\n",
    "# Write to processed zone with partitioning\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=output_dyf,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://my-data-lake/processed/transactions/\",\n",
    "        \"partitionKeys\": [\"year\", \"month\"]\n",
    "    },\n",
    "    transformation_ctx=\"write_output\"\n",
    ")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a85d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managing Glue Catalog with boto3\n",
    "import boto3\n",
    "\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "def create_glue_database(database_name: str, description: str = \"\") -> dict:\n",
    "    \"\"\"Create a Glue Data Catalog database.\"\"\"\n",
    "    return glue_client.create_database(\n",
    "        DatabaseInput={\n",
    "            'Name': database_name,\n",
    "            'Description': description\n",
    "        }\n",
    "    )\n",
    "\n",
    "def create_glue_crawler(crawler_name: str, database_name: str, \n",
    "                        s3_path: str, iam_role: str) -> dict:\n",
    "    \"\"\"Create a Glue Crawler to discover schema.\"\"\"\n",
    "    return glue_client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=iam_role,\n",
    "        DatabaseName=database_name,\n",
    "        Targets={\n",
    "            'S3Targets': [\n",
    "                {\n",
    "                    'Path': s3_path,\n",
    "                    'Exclusions': ['**/_SUCCESS', '**/_temporary/**']\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        SchemaChangePolicy={\n",
    "            'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "            'DeleteBehavior': 'LOG'\n",
    "        },\n",
    "        RecrawlPolicy={\n",
    "            'RecrawlBehavior': 'CRAWL_NEW_FOLDERS_ONLY'\n",
    "        },\n",
    "        Configuration='''{\"Version\": 1.0, \"Grouping\": {\"TableGroupingPolicy\": \"CombineCompatibleSchemas\"}}'''\n",
    "    )\n",
    "\n",
    "def start_crawler(crawler_name: str) -> dict:\n",
    "    \"\"\"Start a Glue Crawler.\"\"\"\n",
    "    return glue_client.start_crawler(Name=crawler_name)\n",
    "\n",
    "def get_table_schema(database_name: str, table_name: str) -> list:\n",
    "    \"\"\"Get schema of a table from Glue Catalog.\"\"\"\n",
    "    response = glue_client.get_table(\n",
    "        DatabaseName=database_name,\n",
    "        Name=table_name\n",
    "    )\n",
    "    return response['Table']['StorageDescriptor']['Columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e936235",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Amazon EMR - Big Data Processing\n",
    "\n",
    "**Amazon Elastic MapReduce (EMR)** is a managed cluster platform for running big data frameworks like Apache Spark, Hadoop, Hive, Presto, and Flink.\n",
    "\n",
    "### EMR Deployment Options\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                     EMR Deployment Options                          │\n",
    "├─────────────────────┬─────────────────────┬─────────────────────────┤\n",
    "│    EMR on EC2       │   EMR on EKS        │    EMR Serverless       │\n",
    "├─────────────────────┼─────────────────────┼─────────────────────────┤\n",
    "│ • Traditional       │ • Run on existing   │ • No cluster mgmt       │\n",
    "│   managed clusters  │   Kubernetes        │ • Auto-scaling          │\n",
    "│ • Full control      │ • Share resources   │ • Pay per use           │\n",
    "│ • Custom AMIs       │ • Container-based   │ • Quick startup         │\n",
    "│ • Spot instances    │ • Multi-tenant      │ • Spark & Hive only     │\n",
    "└─────────────────────┴─────────────────────┴─────────────────────────┘\n",
    "```\n",
    "\n",
    "### EMR Cluster Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                      EMR Cluster Architecture                        │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│    ┌──────────────────┐                                             │\n",
    "│    │   Master Node    │  HDFS NameNode, YARN ResourceManager        │\n",
    "│    │                  │  Spark Driver, Hive Metastore               │\n",
    "│    └────────┬─────────┘                                             │\n",
    "│             │                                                        │\n",
    "│    ┌────────┴─────────────────────────────────────────────┐         │\n",
    "│    │                                                       │         │\n",
    "│    ▼                                                       ▼         │\n",
    "│  ┌───────────────────────┐     ┌───────────────────────────┐        │\n",
    "│  │ Core Nodes (2-100+)   │     │ Task Nodes (Optional)     │        │\n",
    "│  │ • HDFS DataNodes      │     │ • Compute only            │        │\n",
    "│  │ • YARN NodeManagers   │     │ • No HDFS storage         │        │\n",
    "│  │ • Always running      │     │ • Can use Spot instances  │        │\n",
    "│  └───────────────────────┘     └───────────────────────────┘        │\n",
    "│                                                                      │\n",
    "│                         ┌───────────────────┐                        │\n",
    "│                         │  Amazon S3        │                        │\n",
    "│                         │  (EMRFS)          │                        │\n",
    "│                         │  Persistent Store │                        │\n",
    "│                         └───────────────────┘                        │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating an EMR Cluster with boto3\n",
    "import boto3\n",
    "\n",
    "emr_client = boto3.client('emr')\n",
    "\n",
    "def create_emr_cluster(\n",
    "    cluster_name: str,\n",
    "    log_uri: str,\n",
    "    ec2_key_name: str,\n",
    "    subnet_id: str\n",
    ") -> str:\n",
    "    \"\"\"Create an EMR cluster for Spark processing.\"\"\"\n",
    "    \n",
    "    response = emr_client.run_job_flow(\n",
    "        Name=cluster_name,\n",
    "        LogUri=log_uri,\n",
    "        ReleaseLabel='emr-7.0.0',\n",
    "        Applications=[\n",
    "            {'Name': 'Spark'},\n",
    "            {'Name': 'Hive'},\n",
    "            {'Name': 'JupyterEnterpriseGateway'}\n",
    "        ],\n",
    "        Instances={\n",
    "            'MasterInstanceType': 'm5.xlarge',\n",
    "            'SlaveInstanceType': 'm5.xlarge',\n",
    "            'InstanceCount': 3,\n",
    "            'Ec2KeyName': ec2_key_name,\n",
    "            'Ec2SubnetId': subnet_id,\n",
    "            'KeepJobFlowAliveWhenNoSteps': True,\n",
    "            'TerminationProtected': False\n",
    "        },\n",
    "        Configurations=[\n",
    "            {\n",
    "                'Classification': 'spark-defaults',\n",
    "                'Properties': {\n",
    "                    'spark.dynamicAllocation.enabled': 'true',\n",
    "                    'spark.sql.adaptive.enabled': 'true',\n",
    "                    'spark.serializer': 'org.apache.spark.serializer.KryoSerializer'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'Classification': 'spark-hive-site',\n",
    "                'Properties': {\n",
    "                    'hive.metastore.client.factory.class': \n",
    "                        'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        ServiceRole='EMR_DefaultRole',\n",
    "        JobFlowRole='EMR_EC2_DefaultRole',\n",
    "        VisibleToAllUsers=True,\n",
    "        Tags=[\n",
    "            {'Key': 'Environment', 'Value': 'production'},\n",
    "            {'Key': 'Project', 'Value': 'data-pipeline'}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response['JobFlowId']\n",
    "\n",
    "def add_spark_step(cluster_id: str, step_name: str, \n",
    "                   script_path: str, args: list = None) -> str:\n",
    "    \"\"\"Add a Spark step to running EMR cluster.\"\"\"\n",
    "    \n",
    "    step_args = [\n",
    "        'spark-submit',\n",
    "        '--deploy-mode', 'cluster',\n",
    "        '--master', 'yarn',\n",
    "        script_path\n",
    "    ]\n",
    "    if args:\n",
    "        step_args.extend(args)\n",
    "    \n",
    "    response = emr_client.add_job_flow_steps(\n",
    "        JobFlowId=cluster_id,\n",
    "        Steps=[\n",
    "            {\n",
    "                'Name': step_name,\n",
    "                'ActionOnFailure': 'CONTINUE',\n",
    "                'HadoopJarStep': {\n",
    "                    'Jar': 'command-runner.jar',\n",
    "                    'Args': step_args\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response['StepIds'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR Serverless Example\n",
    "import boto3\n",
    "\n",
    "emr_serverless = boto3.client('emr-serverless')\n",
    "\n",
    "def create_emr_serverless_application(app_name: str) -> str:\n",
    "    \"\"\"Create an EMR Serverless application.\"\"\"\n",
    "    response = emr_serverless.create_application(\n",
    "        name=app_name,\n",
    "        releaseLabel='emr-7.0.0',\n",
    "        type='SPARK',\n",
    "        autoStartConfiguration={\n",
    "            'enabled': True\n",
    "        },\n",
    "        autoStopConfiguration={\n",
    "            'enabled': True,\n",
    "            'idleTimeoutMinutes': 15\n",
    "        },\n",
    "        maximumCapacity={\n",
    "            'cpu': '200 vCPU',\n",
    "            'memory': '400 GB'\n",
    "        }\n",
    "    )\n",
    "    return response['applicationId']\n",
    "\n",
    "def submit_serverless_job(\n",
    "    application_id: str,\n",
    "    execution_role: str,\n",
    "    script_path: str,\n",
    "    spark_config: dict = None\n",
    ") -> str:\n",
    "    \"\"\"Submit a Spark job to EMR Serverless.\"\"\"\n",
    "    \n",
    "    spark_submit_params = {\n",
    "        'entryPoint': script_path,\n",
    "        'sparkSubmitParameters': '--conf spark.executor.cores=4 --conf spark.executor.memory=8g'\n",
    "    }\n",
    "    \n",
    "    response = emr_serverless.start_job_run(\n",
    "        applicationId=application_id,\n",
    "        executionRoleArn=execution_role,\n",
    "        jobDriver={\n",
    "            'sparkSubmit': spark_submit_params\n",
    "        },\n",
    "        configurationOverrides={\n",
    "            'monitoringConfiguration': {\n",
    "                's3MonitoringConfiguration': {\n",
    "                    'logUri': 's3://my-emr-logs/serverless/'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response['jobRunId']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0c027",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Amazon Kinesis - Real-Time Streaming\n",
    "\n",
    "**Amazon Kinesis** is a platform for collecting, processing, and analyzing real-time streaming data. It consists of multiple services designed for different streaming use cases.\n",
    "\n",
    "### Kinesis Services Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                       Amazon Kinesis Family                          │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐  │\n",
    "│  │ Kinesis Data    │    │ Kinesis Data    │    │ Kinesis Data    │  │\n",
    "│  │ Streams         │───▶│ Analytics       │───▶│ Firehose        │  │\n",
    "│  │                 │    │                 │    │                 │  │\n",
    "│  │ • Custom apps   │    │ • SQL on streams│    │ • Auto delivery │  │\n",
    "│  │ • Full control  │    │ • Flink apps    │    │ • S3, Redshift  │  │\n",
    "│  │ • Sub-second    │    │ • Aggregations  │    │ • Elasticsearch │  │\n",
    "│  └─────────────────┘    └─────────────────┘    └─────────────────┘  │\n",
    "│                                                                      │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐    │\n",
    "│  │                   Kinesis Video Streams                      │    │\n",
    "│  │     Capture, process, and store video streams               │    │\n",
    "│  └─────────────────────────────────────────────────────────────┘    │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Kinesis Data Streams Architecture\n",
    "\n",
    "```\n",
    "Producers                    Kinesis Stream                    Consumers\n",
    "                            ┌──────────────┐\n",
    "┌─────────┐                 │  Shard 1     │                 ┌─────────────┐\n",
    "│ App 1   │──┐              │  ────────    │              ┌──│ Lambda      │\n",
    "└─────────┘  │              │  Partition   │              │  └─────────────┘\n",
    "             │              │  Key Hash    │              │\n",
    "┌─────────┐  │  PutRecord   ├──────────────┤  GetRecords  │  ┌─────────────┐\n",
    "│ App 2   │──┼─────────────▶│  Shard 2     │──────────────┼──│ EMR         │\n",
    "└─────────┘  │              │  ────────    │              │  └─────────────┘\n",
    "             │              │              │              │\n",
    "┌─────────┐  │              ├──────────────┤              │  ┌─────────────┐\n",
    "│ IoT     │──┘              │  Shard N     │              └──│ Custom App  │\n",
    "└─────────┘                 │  ────────    │                 └─────────────┘\n",
    "                            └──────────────┘\n",
    "                            \n",
    "Data Retention: 24 hours (default) to 365 days\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis Data Streams - Producer Example\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "kinesis_client = boto3.client('kinesis')\n",
    "\n",
    "STREAM_NAME = 'user-events-stream'\n",
    "\n",
    "def put_record(stream_name: str, data: dict, partition_key: str) -> dict:\n",
    "    \"\"\"Put a single record to Kinesis stream.\"\"\"\n",
    "    response = kinesis_client.put_record(\n",
    "        StreamName=stream_name,\n",
    "        Data=json.dumps(data).encode('utf-8'),\n",
    "        PartitionKey=partition_key\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def put_records_batch(stream_name: str, records: list) -> dict:\n",
    "    \"\"\"Put multiple records to Kinesis stream (batch).\"\"\"\n",
    "    kinesis_records = [\n",
    "        {\n",
    "            'Data': json.dumps(record['data']).encode('utf-8'),\n",
    "            'PartitionKey': record['partition_key']\n",
    "        }\n",
    "        for record in records\n",
    "    ]\n",
    "    \n",
    "    response = kinesis_client.put_records(\n",
    "        StreamName=stream_name,\n",
    "        Records=kinesis_records\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Example: Send user click events\n",
    "def send_click_event(user_id: str, page: str, action: str):\n",
    "    \"\"\"Send a user click event to Kinesis.\"\"\"\n",
    "    event = {\n",
    "        'event_id': str(uuid.uuid4()),\n",
    "        'user_id': user_id,\n",
    "        'page': page,\n",
    "        'action': action,\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    return put_record(\n",
    "        stream_name=STREAM_NAME,\n",
    "        data=event,\n",
    "        partition_key=user_id  # Ensures same user's events go to same shard\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "# send_click_event('user-123', '/products', 'view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis Data Streams - Consumer Example\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "kinesis_client = boto3.client('kinesis')\n",
    "\n",
    "def get_shard_iterator(stream_name: str, shard_id: str, \n",
    "                       iterator_type: str = 'LATEST') -> str:\n",
    "    \"\"\"Get shard iterator for reading from stream.\"\"\"\n",
    "    response = kinesis_client.get_shard_iterator(\n",
    "        StreamName=stream_name,\n",
    "        ShardId=shard_id,\n",
    "        ShardIteratorType=iterator_type  # LATEST, TRIM_HORIZON, AT_TIMESTAMP\n",
    "    )\n",
    "    return response['ShardIterator']\n",
    "\n",
    "def consume_stream(stream_name: str, process_func: callable, \n",
    "                   max_iterations: int = 100):\n",
    "    \"\"\"Consume records from all shards in a Kinesis stream.\"\"\"\n",
    "    \n",
    "    # Describe stream to get shards\n",
    "    response = kinesis_client.describe_stream(StreamName=stream_name)\n",
    "    shards = response['StreamDescription']['Shards']\n",
    "    \n",
    "    shard_iterators = {}\n",
    "    for shard in shards:\n",
    "        shard_id = shard['ShardId']\n",
    "        shard_iterators[shard_id] = get_shard_iterator(\n",
    "            stream_name, shard_id, 'LATEST'\n",
    "        )\n",
    "    \n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        for shard_id, iterator in list(shard_iterators.items()):\n",
    "            if iterator is None:\n",
    "                continue\n",
    "                \n",
    "            response = kinesis_client.get_records(\n",
    "                ShardIterator=iterator,\n",
    "                Limit=100\n",
    "            )\n",
    "            \n",
    "            for record in response['Records']:\n",
    "                data = json.loads(record['Data'].decode('utf-8'))\n",
    "                process_func(data)\n",
    "            \n",
    "            shard_iterators[shard_id] = response['NextShardIterator']\n",
    "        \n",
    "        iteration += 1\n",
    "        time.sleep(0.2)  # Avoid throttling\n",
    "\n",
    "# Example processor\n",
    "def process_event(event: dict):\n",
    "    \"\"\"Process a single event from the stream.\"\"\"\n",
    "    print(f\"Processing: {event['event_id']} - User: {event['user_id']}\")\n",
    "\n",
    "# consume_stream('user-events-stream', process_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decba49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis Data Firehose Example\n",
    "import boto3\n",
    "\n",
    "firehose_client = boto3.client('firehose')\n",
    "\n",
    "def create_s3_delivery_stream(\n",
    "    stream_name: str,\n",
    "    bucket_arn: str,\n",
    "    role_arn: str,\n",
    "    prefix: str = 'data/'\n",
    ") -> dict:\n",
    "    \"\"\"Create a Kinesis Firehose delivery stream to S3.\"\"\"\n",
    "    \n",
    "    return firehose_client.create_delivery_stream(\n",
    "        DeliveryStreamName=stream_name,\n",
    "        DeliveryStreamType='DirectPut',\n",
    "        ExtendedS3DestinationConfiguration={\n",
    "            'RoleARN': role_arn,\n",
    "            'BucketARN': bucket_arn,\n",
    "            'Prefix': f'{prefix}year=!{{timestamp:yyyy}}/month=!{{timestamp:MM}}/day=!{{timestamp:dd}}/',\n",
    "            'ErrorOutputPrefix': 'errors/',\n",
    "            'BufferingHints': {\n",
    "                'SizeInMBs': 128,\n",
    "                'IntervalInSeconds': 300\n",
    "            },\n",
    "            'CompressionFormat': 'GZIP',\n",
    "            'DataFormatConversionConfiguration': {\n",
    "                'Enabled': True,\n",
    "                'SchemaConfiguration': {\n",
    "                    'DatabaseName': 'analytics_db',\n",
    "                    'TableName': 'events',\n",
    "                    'RoleARN': role_arn\n",
    "                },\n",
    "                'InputFormatConfiguration': {\n",
    "                    'Deserializer': {\n",
    "                        'OpenXJsonSerDe': {}\n",
    "                    }\n",
    "                },\n",
    "                'OutputFormatConfiguration': {\n",
    "                    'Serializer': {\n",
    "                        'ParquetSerDe': {\n",
    "                            'Compression': 'SNAPPY'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "def put_firehose_record(stream_name: str, data: dict) -> dict:\n",
    "    \"\"\"Put a record directly to Firehose.\"\"\"\n",
    "    return firehose_client.put_record(\n",
    "        DeliveryStreamName=stream_name,\n",
    "        Record={\n",
    "            'Data': json.dumps(data).encode('utf-8')\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9928eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. AWS Lambda - Serverless Processing\n",
    "\n",
    "**AWS Lambda** is a serverless compute service that runs code in response to events without provisioning or managing servers. It's ideal for event-driven data processing, real-time file processing, and lightweight ETL tasks.\n",
    "\n",
    "### Lambda Use Cases in Data Engineering\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│               Lambda in Data Engineering Workflows                   │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  1. S3 Event Processing          2. Stream Processing               │\n",
    "│  ┌─────┐    ┌────────┐          ┌─────────┐    ┌────────┐          │\n",
    "│  │ S3  │───▶│ Lambda │          │ Kinesis │───▶│ Lambda │          │\n",
    "│  └─────┘    └────┬───┘          └─────────┘    └────┬───┘          │\n",
    "│                  │                                   │              │\n",
    "│                  ▼                                   ▼              │\n",
    "│           ┌────────────┐                      ┌────────────┐        │\n",
    "│           │ Transform  │                      │ DynamoDB   │        │\n",
    "│           │ & Load     │                      │ Aggregation│        │\n",
    "│           └────────────┘                      └────────────┘        │\n",
    "│                                                                      │\n",
    "│  3. API Data Ingestion           4. Scheduled ETL                   │\n",
    "│  ┌───────────┐    ┌────────┐    ┌───────────┐    ┌────────┐        │\n",
    "│  │ API GW    │───▶│ Lambda │    │ EventBrg  │───▶│ Lambda │        │\n",
    "│  └───────────┘    └────┬───┘    │ Schedule  │    └────┬───┘        │\n",
    "│                        │        └───────────┘         │             │\n",
    "│                        ▼                              ▼             │\n",
    "│                 ┌────────────┐                 ┌────────────┐       │\n",
    "│                 │ Kinesis    │                 │ Glue Job   │       │\n",
    "│                 │ Firehose   │                 │ Trigger    │       │\n",
    "│                 └────────────┘                 └────────────┘       │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Lambda Limits and Considerations\n",
    "\n",
    "| Resource | Limit |\n",
    "|----------|-------|\n",
    "| Memory | 128 MB - 10,240 MB |\n",
    "| Timeout | 15 minutes max |\n",
    "| Payload | 6 MB (sync), 256 KB (async) |\n",
    "| Ephemeral Storage | 512 MB - 10,240 MB |\n",
    "| Concurrent Executions | 1,000 (default) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743874fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Function: S3 Event Processor\n",
    "# This would be deployed as a Lambda function\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function triggered by S3 PutObject events.\n",
    "    Processes incoming files and triggers downstream workflows.\n",
    "    \"\"\"\n",
    "    processed_files = []\n",
    "    \n",
    "    for record in event['Records']:\n",
    "        # Extract bucket and key from event\n",
    "        bucket = record['s3']['bucket']['name']\n",
    "        key = urllib.parse.unquote_plus(record['s3']['object']['key'])\n",
    "        \n",
    "        print(f\"Processing: s3://{bucket}/{key}\")\n",
    "        \n",
    "        # Skip non-data files\n",
    "        if key.endswith('_SUCCESS') or key.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        # Get file metadata\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        file_size = response['ContentLength']\n",
    "        \n",
    "        # Validate file\n",
    "        if file_size == 0:\n",
    "            print(f\"Skipping empty file: {key}\")\n",
    "            continue\n",
    "        \n",
    "        # Determine processing based on file type\n",
    "        if key.endswith('.csv'):\n",
    "            process_csv_file(bucket, key)\n",
    "        elif key.endswith('.json'):\n",
    "            process_json_file(bucket, key)\n",
    "        elif key.endswith('.parquet'):\n",
    "            trigger_glue_crawler(bucket, key)\n",
    "        \n",
    "        processed_files.append({\n",
    "            'bucket': bucket,\n",
    "            'key': key,\n",
    "            'size': file_size,\n",
    "            'processed_at': datetime.utcnow().isoformat()\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps({\n",
    "            'message': f'Processed {len(processed_files)} files',\n",
    "            'files': processed_files\n",
    "        })\n",
    "    }\n",
    "\n",
    "def process_csv_file(bucket: str, key: str):\n",
    "    \"\"\"Process CSV file and convert to Parquet.\"\"\"\n",
    "    # Trigger Glue job for CSV to Parquet conversion\n",
    "    glue_client.start_job_run(\n",
    "        JobName='csv-to-parquet-converter',\n",
    "        Arguments={\n",
    "            '--source_bucket': bucket,\n",
    "            '--source_key': key\n",
    "        }\n",
    "    )\n",
    "\n",
    "def process_json_file(bucket: str, key: str):\n",
    "    \"\"\"Process JSON file.\"\"\"\n",
    "    pass  # Implementation depends on use case\n",
    "\n",
    "def trigger_glue_crawler(bucket: str, key: str):\n",
    "    \"\"\"Trigger Glue crawler to update catalog.\"\"\"\n",
    "    crawler_name = 'data-lake-crawler'\n",
    "    try:\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "    except glue_client.exceptions.CrawlerRunningException:\n",
    "        print(f\"Crawler {crawler_name} is already running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Function: Kinesis Stream Processor\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('event_aggregations')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function triggered by Kinesis Data Streams.\n",
    "    Aggregates events and writes to DynamoDB.\n",
    "    \"\"\"\n",
    "    aggregations = defaultdict(lambda: {'count': 0, 'events': []})\n",
    "    \n",
    "    for record in event['Records']:\n",
    "        # Decode Kinesis record\n",
    "        payload = base64.b64decode(record['kinesis']['data'])\n",
    "        data = json.loads(payload)\n",
    "        \n",
    "        # Aggregate by user_id\n",
    "        user_id = data.get('user_id', 'unknown')\n",
    "        aggregations[user_id]['count'] += 1\n",
    "        aggregations[user_id]['events'].append({\n",
    "            'event_id': data.get('event_id'),\n",
    "            'action': data.get('action'),\n",
    "            'timestamp': data.get('timestamp')\n",
    "        })\n",
    "    \n",
    "    # Write aggregations to DynamoDB\n",
    "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H')\n",
    "    \n",
    "    with table.batch_writer() as batch:\n",
    "        for user_id, agg_data in aggregations.items():\n",
    "            batch.put_item(Item={\n",
    "                'pk': f'USER#{user_id}',\n",
    "                'sk': f'HOUR#{timestamp}',\n",
    "                'event_count': agg_data['count'],\n",
    "                'events': agg_data['events'][:10],  # Keep last 10 events\n",
    "                'updated_at': datetime.utcnow().isoformat()\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'processedRecords': len(event['Records']),\n",
    "        'uniqueUsers': len(aggregations)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Lambda Functions with boto3\n",
    "import boto3\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "def create_lambda_function(\n",
    "    function_name: str,\n",
    "    handler: str,\n",
    "    role_arn: str,\n",
    "    code_path: str,\n",
    "    runtime: str = 'python3.11',\n",
    "    memory: int = 256,\n",
    "    timeout: int = 60,\n",
    "    environment: dict = None\n",
    ") -> dict:\n",
    "    \"\"\"Create a Lambda function.\"\"\"\n",
    "    \n",
    "    # Create deployment package\n",
    "    zip_buffer = io.BytesIO()\n",
    "    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(code_path, 'lambda_function.py')\n",
    "    zip_buffer.seek(0)\n",
    "    \n",
    "    return lambda_client.create_function(\n",
    "        FunctionName=function_name,\n",
    "        Runtime=runtime,\n",
    "        Role=role_arn,\n",
    "        Handler=handler,\n",
    "        Code={'ZipFile': zip_buffer.read()},\n",
    "        MemorySize=memory,\n",
    "        Timeout=timeout,\n",
    "        Environment={'Variables': environment or {}},\n",
    "        TracingConfig={'Mode': 'Active'}  # Enable X-Ray\n",
    "    )\n",
    "\n",
    "def add_s3_trigger(\n",
    "    function_name: str,\n",
    "    bucket_name: str,\n",
    "    prefix: str = '',\n",
    "    suffix: str = ''\n",
    ") -> dict:\n",
    "    \"\"\"Add S3 trigger to Lambda function.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Add permission for S3 to invoke Lambda\n",
    "    lambda_client.add_permission(\n",
    "        FunctionName=function_name,\n",
    "        StatementId=f's3-trigger-{bucket_name}',\n",
    "        Action='lambda:InvokeFunction',\n",
    "        Principal='s3.amazonaws.com',\n",
    "        SourceArn=f'arn:aws:s3:::{bucket_name}'\n",
    "    )\n",
    "    \n",
    "    # Get Lambda ARN\n",
    "    lambda_arn = lambda_client.get_function(\n",
    "        FunctionName=function_name\n",
    "    )['Configuration']['FunctionArn']\n",
    "    \n",
    "    # Configure S3 bucket notification\n",
    "    notification_config = {\n",
    "        'LambdaFunctionConfigurations': [\n",
    "            {\n",
    "                'LambdaFunctionArn': lambda_arn,\n",
    "                'Events': ['s3:ObjectCreated:*'],\n",
    "                'Filter': {\n",
    "                    'Key': {\n",
    "                        'FilterRules': []\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if prefix:\n",
    "        notification_config['LambdaFunctionConfigurations'][0]['Filter']['Key']['FilterRules'].append(\n",
    "            {'Name': 'prefix', 'Value': prefix}\n",
    "        )\n",
    "    if suffix:\n",
    "        notification_config['LambdaFunctionConfigurations'][0]['Filter']['Key']['FilterRules'].append(\n",
    "            {'Name': 'suffix', 'Value': suffix}\n",
    "        )\n",
    "    \n",
    "    return s3_client.put_bucket_notification_configuration(\n",
    "        Bucket=bucket_name,\n",
    "        NotificationConfiguration=notification_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafa463",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Best Practices and Architecture Patterns\n",
    "\n",
    "### Modern Data Lake Architecture on AWS\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                      Modern Data Lake Architecture                               │\n",
    "├─────────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                                  │\n",
    "│  ┌──────────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                           DATA SOURCES                                    │   │\n",
    "│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────────────┐ │   │\n",
    "│  │  │Databases│  │   APIs  │  │   IoT   │  │  Logs   │  │ SaaS Apps       │ │   │\n",
    "│  │  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘  └────────┬────────┘ │   │\n",
    "│  └───────┼────────────┼────────────┼────────────┼────────────────┼──────────┘   │\n",
    "│          │            │            │            │                │              │\n",
    "│          ▼            ▼            ▼            ▼                ▼              │\n",
    "│  ┌──────────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                         INGESTION LAYER                                   │   │\n",
    "│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │   │\n",
    "│  │  │    Kinesis   │  │   Lambda     │  │    DMS       │  │   AppFlow    │  │   │\n",
    "│  │  │   Firehose   │  │  (API GW)    │  │ (Databases)  │  │   (SaaS)     │  │   │\n",
    "│  │  └──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘  │   │\n",
    "│  └───────────────────────────────────────────────────────────────────────────┘   │\n",
    "│                                      │                                           │\n",
    "│                                      ▼                                           │\n",
    "│  ┌──────────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                         STORAGE LAYER (S3)                                │   │\n",
    "│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐                    │   │\n",
    "│  │  │    Bronze    │  │    Silver    │  │     Gold     │                    │   │\n",
    "│  │  │  (Raw Data)  │──│  (Cleaned)   │──│  (Curated)   │                    │   │\n",
    "│  │  └──────────────┘  └──────────────┘  └──────────────┘                    │   │\n",
    "│  └───────────────────────────────────────────────────────────────────────────┘   │\n",
    "│                                      │                                           │\n",
    "│                                      ▼                                           │\n",
    "│  ┌──────────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                       PROCESSING LAYER                                    │   │\n",
    "│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │   │\n",
    "│  │  │  AWS Glue    │  │     EMR      │  │   Lambda     │  │    Step      │  │   │\n",
    "│  │  │  (ETL)       │  │  (Spark)     │  │ (Serverless) │  │  Functions   │  │   │\n",
    "│  │  └──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘  │   │\n",
    "│  └───────────────────────────────────────────────────────────────────────────┘   │\n",
    "│                                      │                                           │\n",
    "│                                      ▼                                           │\n",
    "│  ┌──────────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                       CONSUMPTION LAYER                                   │   │\n",
    "│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │   │\n",
    "│  │  │   Athena     │  │   Redshift   │  │  QuickSight  │  │  SageMaker   │  │   │\n",
    "│  │  │  (Ad-hoc)    │  │  (DW)        │  │   (BI)       │  │    (ML)      │  │   │\n",
    "│  │  └──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘  │   │\n",
    "│  └───────────────────────────────────────────────────────────────────────────┘   │\n",
    "│                                                                                  │\n",
    "│  ┌──────────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                    GOVERNANCE & SECURITY                                  │   │\n",
    "│  │  Lake Formation │ IAM │ KMS │ CloudTrail │ Glue Catalog │ DataZone       │   │\n",
    "│  └──────────────────────────────────────────────────────────────────────────┘   │\n",
    "│                                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8079ca7",
   "metadata": {},
   "source": [
    "### Best Practices by Service\n",
    "\n",
    "#### Amazon S3 Best Practices\n",
    "\n",
    "| Category | Best Practice |\n",
    "|----------|---------------|\n",
    "| **Partitioning** | Use date-based partitioning (year/month/day) for time-series data |\n",
    "| **File Format** | Use columnar formats (Parquet, ORC) for analytics workloads |\n",
    "| **File Size** | Target 128MB - 1GB files for optimal performance |\n",
    "| **Naming** | Use consistent, meaningful prefixes for efficient listing |\n",
    "| **Lifecycle** | Implement lifecycle policies to manage storage costs |\n",
    "| **Security** | Enable default encryption, block public access |\n",
    "\n",
    "#### AWS Glue Best Practices\n",
    "\n",
    "| Category | Best Practice |\n",
    "|----------|---------------|\n",
    "| **Job Bookmarks** | Enable to process only new data incrementally |\n",
    "| **Partitioning** | Push down predicates to reduce data scanned |\n",
    "| **Worker Type** | Choose appropriate worker type (G.1X, G.2X) based on workload |\n",
    "| **Error Handling** | Implement proper error handling and dead letter queues |\n",
    "| **Catalog** | Keep catalog organized with meaningful database/table names |\n",
    "\n",
    "#### Amazon EMR Best Practices\n",
    "\n",
    "| Category | Best Practice |\n",
    "|----------|---------------|\n",
    "| **Instance Types** | Use Spot instances for task nodes (up to 90% savings) |\n",
    "| **Storage** | Use EMRFS (S3) for persistent data, local HDFS for temp |\n",
    "| **Scaling** | Configure managed scaling for dynamic workloads |\n",
    "| **Security** | Enable encryption at rest and in transit |\n",
    "| **Monitoring** | Use CloudWatch and Spark UI for monitoring |\n",
    "\n",
    "#### Amazon Kinesis Best Practices\n",
    "\n",
    "| Category | Best Practice |\n",
    "|----------|---------------|\n",
    "| **Sharding** | Choose partition key that distributes data evenly |\n",
    "| **Batching** | Use PutRecords for batch ingestion (up to 500 records) |\n",
    "| **Enhanced Fan-Out** | Use for low-latency consumption with multiple consumers |\n",
    "| **Error Handling** | Implement retry logic with exponential backoff |\n",
    "| **Monitoring** | Monitor IteratorAge to detect consumer lag |\n",
    "\n",
    "#### AWS Lambda Best Practices\n",
    "\n",
    "| Category | Best Practice |\n",
    "|----------|---------------|\n",
    "| **Memory** | Tune memory based on workload (CPU scales with memory) |\n",
    "| **Cold Starts** | Use Provisioned Concurrency for latency-sensitive workloads |\n",
    "| **Connections** | Reuse connections (DB, HTTP) across invocations |\n",
    "| **Packaging** | Minimize deployment package size for faster cold starts |\n",
    "| **Idempotency** | Design functions to be idempotent for retry safety |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete Data Pipeline Orchestration with Step Functions\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sfn_client = boto3.client('stepfunctions')\n",
    "\n",
    "# Step Functions state machine definition for a data pipeline\n",
    "state_machine_definition = {\n",
    "    \"Comment\": \"Data Pipeline: Ingest, Transform, and Load\",\n",
    "    \"StartAt\": \"CheckNewData\",\n",
    "    \"States\": {\n",
    "        \"CheckNewData\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:check-new-data\",\n",
    "            \"Next\": \"HasNewData\"\n",
    "        },\n",
    "        \"HasNewData\": {\n",
    "            \"Type\": \"Choice\",\n",
    "            \"Choices\": [\n",
    "                {\n",
    "                    \"Variable\": \"$.hasNewData\",\n",
    "                    \"BooleanEquals\": True,\n",
    "                    \"Next\": \"StartGlueCrawler\"\n",
    "                }\n",
    "            ],\n",
    "            \"Default\": \"NoNewData\"\n",
    "        },\n",
    "        \"NoNewData\": {\n",
    "            \"Type\": \"Succeed\"\n",
    "        },\n",
    "        \"StartGlueCrawler\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::glue:startCrawler.sync\",\n",
    "            \"Parameters\": {\n",
    "                \"Name\": \"data-lake-crawler\"\n",
    "            },\n",
    "            \"Next\": \"RunGlueETLJob\"\n",
    "        },\n",
    "        \"RunGlueETLJob\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\",\n",
    "            \"Parameters\": {\n",
    "                \"JobName\": \"transform-to-curated\",\n",
    "                \"Arguments\": {\n",
    "                    \"--source_database.$\": \"$.database\",\n",
    "                    \"--source_table.$\": \"$.table\"\n",
    "                }\n",
    "            },\n",
    "            \"Catch\": [\n",
    "                {\n",
    "                    \"ErrorEquals\": [\"States.ALL\"],\n",
    "                    \"Next\": \"HandleETLError\"\n",
    "                }\n",
    "            ],\n",
    "            \"Next\": \"NotifySuccess\"\n",
    "        },\n",
    "        \"HandleETLError\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:handle-etl-error\",\n",
    "            \"Next\": \"NotifyFailure\"\n",
    "        },\n",
    "        \"NotifySuccess\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::sns:publish\",\n",
    "            \"Parameters\": {\n",
    "                \"TopicArn\": \"arn:aws:sns:us-east-1:123456789:pipeline-notifications\",\n",
    "                \"Message\": \"Data pipeline completed successfully\"\n",
    "            },\n",
    "            \"End\": True\n",
    "        },\n",
    "        \"NotifyFailure\": {\n",
    "            \"Type\": \"Task\",\n",
    "            \"Resource\": \"arn:aws:states:::sns:publish\",\n",
    "            \"Parameters\": {\n",
    "                \"TopicArn\": \"arn:aws:sns:us-east-1:123456789:pipeline-notifications\",\n",
    "                \"Message.$\": \"States.Format('Pipeline failed: {}', $.error)\"\n",
    "            },\n",
    "            \"End\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create state machine\n",
    "# sfn_client.create_state_machine(\n",
    "#     name='data-pipeline-orchestrator',\n",
    "#     definition=json.dumps(state_machine_definition),\n",
    "#     roleArn='arn:aws:iam::123456789:role/StepFunctionsExecutionRole'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5fde9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "### Summary of AWS Data Engineering Services\n",
    "\n",
    "| Service | Primary Use Case | When to Use |\n",
    "|---------|-----------------|-------------|\n",
    "| **S3** | Data Lake Storage | Always - foundation of AWS data architecture |\n",
    "| **Glue** | ETL & Catalog | Serverless ETL, schema management, data discovery |\n",
    "| **EMR** | Big Data Processing | Large-scale Spark/Hadoop, complex analytics |\n",
    "| **Kinesis** | Real-time Streaming | Event-driven, real-time analytics, IoT |\n",
    "| **Lambda** | Serverless Compute | Event-driven processing, lightweight ETL |\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    Choosing the Right Service                        │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  Data Volume?                                                        │\n",
    "│  ├── Small (< 1GB)        → Lambda + S3                             │\n",
    "│  ├── Medium (1-100GB)     → Glue ETL                                │\n",
    "│  └── Large (> 100GB)      → EMR or Glue with Spark                  │\n",
    "│                                                                      │\n",
    "│  Processing Type?                                                    │\n",
    "│  ├── Batch                → Glue or EMR                             │\n",
    "│  ├── Streaming            → Kinesis + Lambda/Flink                  │\n",
    "│  └── Micro-batch          → Kinesis Firehose                        │\n",
    "│                                                                      │\n",
    "│  Latency Requirements?                                               │\n",
    "│  ├── Real-time (< 1s)     → Kinesis Data Streams + Lambda           │\n",
    "│  ├── Near real-time       → Kinesis Firehose                        │\n",
    "│  └── Batch (> 1 min)      → Glue or EMR                             │\n",
    "│                                                                      │\n",
    "│  Operational Overhead?                                               │\n",
    "│  ├── Minimal              → Glue, Lambda, Kinesis Firehose          │\n",
    "│  ├── Moderate             → EMR Serverless                          │\n",
    "│  └── Full Control         → EMR on EC2                              │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Architecture Principles\n",
    "\n",
    "1. **Decouple Compute and Storage**: Use S3 as the central data lake, separate from processing engines\n",
    "2. **Use the Right Tool**: Match service to workload (streaming vs batch, simple vs complex)\n",
    "3. **Design for Failure**: Implement retry logic, dead letter queues, and monitoring\n",
    "4. **Optimize Costs**: Use lifecycle policies, Spot instances, and right-size resources\n",
    "5. **Security First**: Encrypt data at rest and in transit, use least privilege IAM policies\n",
    "6. **Automate Everything**: Use IaC (CloudFormation, CDK, Terraform) for reproducibility\n",
    "7. **Monitor and Alert**: Set up CloudWatch dashboards, alarms, and log analysis\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [AWS Data Analytics Lens](https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/welcome.html)\n",
    "- [AWS Big Data Blog](https://aws.amazon.com/blogs/big-data/)\n",
    "- [AWS Data Lake Whitepaper](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html)\n",
    "- [Modern Data Architecture on AWS](https://aws.amazon.com/big-data/datalakes-and-analytics/modern-data-architecture/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
