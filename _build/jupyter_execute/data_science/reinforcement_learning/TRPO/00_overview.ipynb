{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0c6b4f",
   "metadata": {},
   "source": [
    "# TRPO (Trust Region Policy Optimization) — low-level PyTorch implementation\n",
    "\n",
    "TRPO is an on-policy policy-gradient method that makes **monotonic-ish**, stable updates by constraining how much the policy is allowed to change each iteration via a **KL-divergence trust region**.\n",
    "\n",
    "In this notebook you will:\n",
    "- Derive the **KL constraint** (LaTeX) and how it leads to a **natural-gradient** step\n",
    "- Implement TRPO \"from scratch\" with **PyTorch autograd** + **conjugate gradient** + **backtracking line search**\n",
    "- Visualize **policy updates**, **KL per update**, and **episodic returns** with **Plotly**\n",
    "- See a reference **Stable-Baselines TRPO** implementation and understand its hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c778ba9",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. TRPO objective + the KL-divergence constraint (math)\n",
    "2. A tiny offline-friendly continuous-control environment (no downloads)\n",
    "3. Gaussian policy + value baseline (PyTorch)\n",
    "4. GAE advantages + value function fit\n",
    "5. TRPO update step (Fisher-vector product, conjugate gradient, line search)\n",
    "6. Plotly: episodic rewards, KL constraint, policy update snapshots\n",
    "7. Stable-Baselines TRPO: usage + hyperparameters (end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1249134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "import plotly\n",
    "\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc258a",
   "metadata": {},
   "source": [
    "## 1) TRPO objective and the KL-divergence constraint\n",
    "\n",
    "TRPO is usually presented as the constrained optimization problem:\n",
    "\n",
    "\\[\n",
    "\\max_\\theta\\; \\mathbb{E}_{s,a\\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_{\\text{old}}}(a\\mid s)}\\,\\hat A_{\\theta_{\\text{old}}}(s,a)\\right]\n",
    "\\qquad\\text{s.t.}\\qquad\n",
    "\\mathbb{E}_{s\\sim \\pi_{\\theta_{\\text{old}}}}\\left[D_{\\mathrm{KL}}\\!\\left(\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid s)\\,\\|\\,\\pi_\\theta(\\cdot\\mid s)\\right)\\right] \\le \\delta.\n",
    "\\]\n",
    "\n",
    "The trust region is **average KL divergence** (under states visited by the old policy). Intuition: *\"move in a direction that increases the objective, but don't move too far in policy space.\"*\n",
    "\n",
    "We use the standard definition:\n",
    "\n",
    "\\[\n",
    "D_{\\mathrm{KL}}(p\\|q) = \\mathbb{E}_{x\\sim p}\\left[\\log\\frac{p(x)}{q(x)}\\right].\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25419df8",
   "metadata": {},
   "source": [
    "### 1.1) Why this leads to a natural-gradient step\n",
    "\n",
    "Let \\(\\theta\\) be the policy parameters and \\(\\theta_{\\text{old}}\\) the pre-update parameters.\n",
    "\n",
    "TRPO uses two approximations around \\(\\theta_{\\text{old}}\\):\n",
    "\n",
    "- **First-order** (linear) approximation of the surrogate objective:\n",
    "\n",
    "\\[\n",
    "L(\\theta) \\approx L(\\theta_{\\text{old}}) + g^\\top (\\theta - \\theta_{\\text{old}})\n",
    "\\quad\\text{where}\\quad g = \\nabla_\\theta L(\\theta)\\big\\rvert_{\\theta=\\theta_{\\text{old}}}.\n",
    "\\]\n",
    "\n",
    "- **Second-order** (quadratic) approximation of the KL constraint:\n",
    "\n",
    "\\[\n",
    "\\bar D_{\\mathrm{KL}}(\\theta_{\\text{old}},\\theta)\n",
    "\\approx \\tfrac12 (\\theta - \\theta_{\\text{old}})^\\top H (\\theta - \\theta_{\\text{old}}),\n",
    "\\]\n",
    "\n",
    "where \\(H\\) is the Hessian of the average KL at \\(\\theta_{\\text{old}}\\) (equivalently, the policy's **Fisher information matrix** for common exponential-family policies).\n",
    "\n",
    "Define the step \\(p = \\theta - \\theta_{\\text{old}}\\). The constrained problem becomes:\n",
    "\n",
    "\\[\n",
    "\\max_p\\; g^\\top p\n",
    "\\qquad\\text{s.t.}\\qquad\n",
    "\\tfrac12 p^\\top H p \\le \\delta.\n",
    "\\]\n",
    "\n",
    "The solution is:\n",
    "\n",
    "\\[\n",
    "p^\\* = \\sqrt{\\frac{2\\delta}{g^\\top H^{-1} g}}\\; H^{-1} g.\n",
    "\\]\n",
    "\n",
    "So we need:\n",
    "1. The policy-gradient \\(g\\)\n",
    "2. The product \\(H^{-1} g\\) (without forming \\(H\\) explicitly) → **conjugate gradient** + **Hessian-vector products**\n",
    "3. A step scaling + **backtracking line search** to satisfy the *true* KL constraint and improve the surrogate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9a60e",
   "metadata": {},
   "source": [
    "## 2) A tiny offline-friendly continuous-control environment\n",
    "\n",
    "To keep the notebook self-contained (no Gym downloads), we use a 1D point-mass with state \\(s=(x,v)\\) and action \\(a\\in[-1,1]\\):\n",
    "\n",
    "- Dynamics: small acceleration changes velocity, velocity changes position\n",
    "- Goal: reach \\(x=0\\) with small velocity\n",
    "- Reward: negative quadratic cost (plus a small terminal bonus when reaching the goal)\n",
    "\n",
    "This is *not* meant to be a benchmark; it's just enough to show that TRPO learns and that the KL trust region stabilizes updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointMass1DEnv:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dt: float = 0.05,\n",
    "        max_steps: int = 150,\n",
    "        x_init_range: float = 2.0,\n",
    "        v_init_range: float = 0.5,\n",
    "        action_max: float = 1.0,\n",
    "        goal_x: float = 0.0,\n",
    "        goal_tol: float = 0.05,\n",
    "        goal_bonus: float = 5.0,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        self.dt = float(dt)\n",
    "        self.max_steps = int(max_steps)\n",
    "        self.x_init_range = float(x_init_range)\n",
    "        self.v_init_range = float(v_init_range)\n",
    "        self.action_max = float(action_max)\n",
    "        self.goal_x = float(goal_x)\n",
    "        self.goal_tol = float(goal_tol)\n",
    "        self.goal_bonus = float(goal_bonus)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.x = 0.0\n",
    "        self.v = 0.0\n",
    "\n",
    "    @property\n",
    "    def obs_dim(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def act_dim(self):\n",
    "        return 1\n",
    "\n",
    "    def reset(self, seed: int | None = None):\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        self.steps = 0\n",
    "        self.x = self.rng.uniform(-self.x_init_range, self.x_init_range)\n",
    "        self.v = self.rng.uniform(-self.v_init_range, self.v_init_range)\n",
    "        return np.array([self.x, self.v], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        a = float(np.clip(action, -self.action_max, self.action_max))\n",
    "\n",
    "        # simple damped dynamics\n",
    "        self.v = 0.99 * self.v + a * self.dt\n",
    "        self.x = self.x + self.v * self.dt\n",
    "        self.steps += 1\n",
    "\n",
    "        # quadratic cost around the goal\n",
    "        cost = (self.x - self.goal_x) ** 2 + 0.1 * (self.v**2) + 0.001 * (a**2)\n",
    "        reward = -float(cost)\n",
    "\n",
    "        done = False\n",
    "        if abs(self.x - self.goal_x) < self.goal_tol and abs(self.v) < self.goal_tol:\n",
    "            done = True\n",
    "            reward += float(self.goal_bonus)\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        obs = np.array([self.x, self.v], dtype=np.float32)\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PointMass1DEnv(seed=SEED)\n",
    "obs = env.reset()\n",
    "\n",
    "xs, vs, acts, rews = [obs[0]], [obs[1]], [], []\n",
    "done = False\n",
    "while not done:\n",
    "    a = rng.uniform(-1.0, 1.0)\n",
    "    obs, r, done, _ = env.step(a)\n",
    "    xs.append(obs[0])\n",
    "    vs.append(obs[1])\n",
    "    acts.append(a)\n",
    "    rews.append(r)\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "fig.add_trace(go.Scatter(y=xs, mode=\"lines\", name=\"x\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=vs, mode=\"lines\", name=\"v\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=acts, mode=\"lines\", name=\"a\"), row=3, col=1)\n",
    "fig.update_layout(\n",
    "    title=\"One random rollout in the toy env\",\n",
    "    height=650,\n",
    "    showlegend=True,\n",
    ")\n",
    "fig.update_yaxes(title_text=\"position x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"velocity v\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"action a\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"time step\", row=3, col=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"Return (sum reward):\", float(np.sum(rews)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab8477",
   "metadata": {},
   "source": [
    "## 3) Policy and value function (PyTorch)\n",
    "\n",
    "We'll use:\n",
    "- A **Gaussian policy** \\(\\pi_\\theta(a\\mid s)=\\mathcal{N}(\\mu_\\theta(s),\\sigma_\\theta(s)^2)\\) with diagonal covariance (here 1D)\n",
    "- A **value network** \\(V_\\phi(s)\\) as a baseline\n",
    "\n",
    "For TRPO we need:\n",
    "- \\(\\log \\pi_\\theta(a\\mid s)\\) to compute the surrogate objective\n",
    "- The **KL** between old and new Gaussian policies to build the trust region (and its Hessian-vector product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c302e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        act = activation if i < len(sizes) - 2 else output_activation\n",
    "        layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes=(64, 64)):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim, *hidden_sizes, act_dim], activation=nn.Tanh)\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, obs: torch.Tensor):\n",
    "        mean = self.net(obs)\n",
    "        log_std = self.log_std.expand_as(mean)\n",
    "        return mean, log_std\n",
    "\n",
    "    def dist(self, obs: torch.Tensor):\n",
    "        mean, log_std = self.forward(obs)\n",
    "        return torch.distributions.Normal(mean, torch.exp(log_std))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, obs: torch.Tensor):\n",
    "        dist = self.dist(obs)\n",
    "        action = dist.sample()\n",
    "        logp = dist.log_prob(action).sum(-1)\n",
    "        return action, logp\n",
    "\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, obs_dim: int, hidden_sizes=(64, 64)):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim, *hidden_sizes, 1], activation=nn.Tanh)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor):\n",
    "        return self.net(obs).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19264e9",
   "metadata": {},
   "source": [
    "## 4) TRPO building blocks (low-level)\n",
    "\n",
    "We implement:\n",
    "- GAE(\\(\\gamma,\\lambda\\)) for advantages\n",
    "- Value function regression\n",
    "- Conjugate gradient for solving \\(H x = g\\)\n",
    "- Fisher/Hessian-vector product via autograd on the mean KL\n",
    "- Backtracking line search enforcing the KL constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005825ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kl(mean_old, log_std_old, mean_new, log_std_new):\n",
    "    \"\"\"KL( N_old || N_new ) for diagonal Gaussians; returns shape (batch,).\"\"\"\n",
    "    var_old = torch.exp(2.0 * log_std_old)\n",
    "    var_new = torch.exp(2.0 * log_std_new)\n",
    "    kl_per_dim = (\n",
    "        log_std_new\n",
    "        - log_std_old\n",
    "        + (var_old + (mean_old - mean_new) ** 2) / (2.0 * var_new)\n",
    "        - 0.5\n",
    "    )\n",
    "    return kl_per_dim.sum(dim=-1)\n",
    "\n",
    "\n",
    "def flat_params(model: nn.Module):\n",
    "    return torch.cat([p.data.view(-1) for p in model.parameters()])\n",
    "\n",
    "\n",
    "def set_flat_params(model: nn.Module, flat: torch.Tensor):\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.copy_(flat[idx : idx + n].view_as(p))\n",
    "            idx += n\n",
    "\n",
    "\n",
    "def flat_grad(grads, params):\n",
    "    out = []\n",
    "    for g, p in zip(grads, params):\n",
    "        if g is None:\n",
    "            out.append(torch.zeros_like(p).view(-1))\n",
    "        else:\n",
    "            out.append(g.contiguous().view(-1))\n",
    "    return torch.cat(out)\n",
    "\n",
    "\n",
    "def conjugate_gradient(fvp_fn, b, cg_iters=10, residual_tol=1e-10):\n",
    "    x = torch.zeros_like(b)\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "\n",
    "    for _ in range(cg_iters):\n",
    "        Avp = fvp_fn(p)\n",
    "        alpha = rdotr / (torch.dot(p, Avp) + 1e-8)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Avp\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        if new_rdotr < residual_tol:\n",
    "            break\n",
    "        beta = new_rdotr / (rdotr + 1e-8)\n",
    "        p = r + beta * p\n",
    "        rdotr = new_rdotr\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def trpo_update(\n",
    "    policy: GaussianPolicy,\n",
    "    obs: torch.Tensor,\n",
    "    act: torch.Tensor,\n",
    "    adv: torch.Tensor,\n",
    "    logp_old: torch.Tensor,\n",
    "    max_kl: float = 0.01,\n",
    "    cg_iters: int = 10,\n",
    "    cg_damping: float = 1e-2,\n",
    "    backtrack_iters: int = 10,\n",
    "    backtrack_coeff: float = 0.8,\n",
    "):\n",
    "    \"\"\"One TRPO policy update step.\"\"\"\n",
    "\n",
    "    params = list(policy.parameters())\n",
    "    old_params = flat_params(policy)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_old, log_std_old = policy.forward(obs)\n",
    "        mean_old = mean_old.detach()\n",
    "        log_std_old = log_std_old.detach()\n",
    "\n",
    "    def surrogate():\n",
    "        dist = policy.dist(obs)\n",
    "        logp = dist.log_prob(act).sum(-1)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        return (ratio * adv).mean()\n",
    "\n",
    "    def mean_kl():\n",
    "        mean_new, log_std_new = policy.forward(obs)\n",
    "        return gaussian_kl(mean_old, log_std_old, mean_new, log_std_new).mean()\n",
    "\n",
    "    surr = surrogate()\n",
    "    g = torch.autograd.grad(surr, params, retain_graph=True, allow_unused=True)\n",
    "    g_flat = flat_grad(g, params).detach()\n",
    "\n",
    "    def fvp(v):\n",
    "        kl = mean_kl()\n",
    "        grads = torch.autograd.grad(kl, params, create_graph=True, allow_unused=True)\n",
    "        flat_kl_grad = flat_grad(grads, params)\n",
    "        kl_v = torch.dot(flat_kl_grad, v)\n",
    "        grads2 = torch.autograd.grad(kl_v, params, allow_unused=True)\n",
    "        hvp = flat_grad(grads2, params).detach()\n",
    "        return hvp + cg_damping * v\n",
    "\n",
    "    step_dir = conjugate_gradient(fvp, g_flat, cg_iters=cg_iters)\n",
    "    shs = torch.dot(step_dir, fvp(step_dir))\n",
    "    step_size = torch.sqrt(torch.tensor(2.0 * max_kl, dtype=shs.dtype) / (shs + 1e-8))\n",
    "    full_step = step_dir * step_size\n",
    "\n",
    "    def eval_surr_and_kl():\n",
    "        with torch.no_grad():\n",
    "            s = surrogate().item()\n",
    "            k = mean_kl().item()\n",
    "        return s, k\n",
    "\n",
    "    surr_old_val, _ = eval_surr_and_kl()\n",
    "\n",
    "    step_frac = 1.0\n",
    "    accepted = False\n",
    "    surr_new_val = surr_old_val\n",
    "    kl_new_val = 0.0\n",
    "\n",
    "    for _ in range(backtrack_iters):\n",
    "        new_params = old_params + step_frac * full_step\n",
    "        set_flat_params(policy, new_params)\n",
    "\n",
    "        surr_new_val, kl_new_val = eval_surr_and_kl()\n",
    "\n",
    "        if (surr_new_val > surr_old_val) and (kl_new_val <= max_kl):\n",
    "            accepted = True\n",
    "            break\n",
    "        step_frac *= backtrack_coeff\n",
    "\n",
    "    if not accepted:\n",
    "        set_flat_params(policy, old_params)\n",
    "\n",
    "    return {\n",
    "        \"surr_old\": float(surr_old_val),\n",
    "        \"surr_new\": float(surr_new_val),\n",
    "        \"kl\": float(kl_new_val),\n",
    "        \"step_frac\": float(step_frac if accepted else 0.0),\n",
    "        \"accepted\": bool(accepted),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_batch(env, policy, value_net, steps_per_batch, gamma=0.99, lam=0.98):\n",
    "    obs_buf = np.zeros((steps_per_batch, env.obs_dim), dtype=np.float32)\n",
    "    act_buf = np.zeros((steps_per_batch, env.act_dim), dtype=np.float32)\n",
    "    rew_buf = np.zeros(steps_per_batch, dtype=np.float32)\n",
    "    done_buf = np.zeros(steps_per_batch, dtype=np.float32)\n",
    "    val_buf = np.zeros(steps_per_batch, dtype=np.float32)\n",
    "    logp_buf = np.zeros(steps_per_batch, dtype=np.float32)\n",
    "\n",
    "    ep_returns = []\n",
    "    ep_ret = 0.0\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    for t in range(steps_per_batch):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            a_t, logp_t = policy.act(obs_t)\n",
    "            v_t = value_net(obs_t)\n",
    "\n",
    "        a = a_t.squeeze(0).cpu().numpy()\n",
    "        logp = float(logp_t.item())\n",
    "        v = float(v_t.item())\n",
    "\n",
    "        next_obs, r, done, _ = env.step(a)\n",
    "\n",
    "        obs_buf[t] = obs\n",
    "        act_buf[t] = a\n",
    "        rew_buf[t] = r\n",
    "        done_buf[t] = float(done)\n",
    "        val_buf[t] = v\n",
    "        logp_buf[t] = logp\n",
    "\n",
    "        ep_ret += float(r)\n",
    "\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            ep_returns.append(ep_ret)\n",
    "            ep_ret = 0.0\n",
    "            obs = env.reset()\n",
    "\n",
    "    # bootstrap value for the last state (if last transition wasn't terminal)\n",
    "    with torch.no_grad():\n",
    "        last_val = value_net(\n",
    "            torch.as_tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        ).item()\n",
    "\n",
    "    adv_buf = np.zeros(steps_per_batch, dtype=np.float32)\n",
    "    last_gae = 0.0\n",
    "\n",
    "    for t in reversed(range(steps_per_batch)):\n",
    "        if t == steps_per_batch - 1:\n",
    "            next_nonterminal = 1.0 - done_buf[t]\n",
    "            next_value = last_val\n",
    "        else:\n",
    "            next_nonterminal = 1.0 - done_buf[t]\n",
    "            next_value = val_buf[t + 1]\n",
    "\n",
    "        delta = rew_buf[t] + gamma * next_value * next_nonterminal - val_buf[t]\n",
    "        last_gae = delta + gamma * lam * next_nonterminal * last_gae\n",
    "        adv_buf[t] = last_gae\n",
    "\n",
    "    ret_buf = adv_buf + val_buf\n",
    "\n",
    "    # normalize advantages (very common and usually helpful)\n",
    "    adv_buf = (adv_buf - adv_buf.mean()) / (adv_buf.std() + 1e-8)\n",
    "\n",
    "    batch = {\n",
    "        \"obs\": torch.as_tensor(obs_buf, dtype=torch.float32, device=DEVICE),\n",
    "        \"act\": torch.as_tensor(act_buf, dtype=torch.float32, device=DEVICE),\n",
    "        \"logp_old\": torch.as_tensor(logp_buf, dtype=torch.float32, device=DEVICE),\n",
    "        \"adv\": torch.as_tensor(adv_buf, dtype=torch.float32, device=DEVICE),\n",
    "        \"ret\": torch.as_tensor(ret_buf, dtype=torch.float32, device=DEVICE),\n",
    "        \"ep_returns\": ep_returns,\n",
    "    }\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run configuration ---\n",
    "FAST_RUN = True  # set False for a longer run\n",
    "\n",
    "TOTAL_ITERS = 25 if FAST_RUN else 150\n",
    "STEPS_PER_BATCH = 1024 if FAST_RUN else 4096\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.98\n",
    "\n",
    "MAX_KL = 0.01\n",
    "CG_ITERS = 10\n",
    "CG_DAMPING = 1e-2\n",
    "BACKTRACK_ITERS = 10\n",
    "BACKTRACK_COEFF = 0.8\n",
    "\n",
    "VF_LR = 3e-4\n",
    "VF_ITERS = 10 if FAST_RUN else 80\n",
    "VF_BATCH = 128\n",
    "\n",
    "SNAPSHOT_EVERY = 5\n",
    "\n",
    "env = PointMass1DEnv(seed=SEED)\n",
    "policy = GaussianPolicy(env.obs_dim, env.act_dim, hidden_sizes=(64, 64)).to(DEVICE)\n",
    "value_net = ValueNet(env.obs_dim, hidden_sizes=(64, 64)).to(DEVICE)\n",
    "\n",
    "vf_optim = torch.optim.Adam(value_net.parameters(), lr=VF_LR)\n",
    "\n",
    "x_grid = np.linspace(-env.x_init_range, env.x_init_range, 101, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"iter\": [],\n",
    "    \"ep_ret_mean\": [],\n",
    "    \"ep_ret_p10\": [],\n",
    "    \"ep_ret_p90\": [],\n",
    "    \"kl\": [],\n",
    "    \"surr_old\": [],\n",
    "    \"surr_new\": [],\n",
    "    \"step_frac\": [],\n",
    "    \"policy_std\": [],\n",
    "}\n",
    "\n",
    "policy_snapshots = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for it in range(TOTAL_ITERS):\n",
    "    batch = collect_batch(\n",
    "        env,\n",
    "        policy,\n",
    "        value_net,\n",
    "        steps_per_batch=STEPS_PER_BATCH,\n",
    "        gamma=GAMMA,\n",
    "        lam=LAMBDA,\n",
    "    )\n",
    "\n",
    "    # --- Fit value function ---\n",
    "    for _ in range(VF_ITERS):\n",
    "        n = batch[\"obs\"].shape[0]\n",
    "        bs = min(VF_BATCH, n)\n",
    "        idx = torch.as_tensor(rng.choice(n, size=bs, replace=False), device=DEVICE)\n",
    "        v_pred = value_net(batch[\"obs\"][idx])\n",
    "        v_loss = F.mse_loss(v_pred, batch[\"ret\"][idx])\n",
    "        vf_optim.zero_grad()\n",
    "        v_loss.backward()\n",
    "        vf_optim.step()\n",
    "\n",
    "    # --- TRPO policy update ---\n",
    "    stats = trpo_update(\n",
    "        policy,\n",
    "        obs=batch[\"obs\"],\n",
    "        act=batch[\"act\"],\n",
    "        adv=batch[\"adv\"],\n",
    "        logp_old=batch[\"logp_old\"],\n",
    "        max_kl=MAX_KL,\n",
    "        cg_iters=CG_ITERS,\n",
    "        cg_damping=CG_DAMPING,\n",
    "        backtrack_iters=BACKTRACK_ITERS,\n",
    "        backtrack_coeff=BACKTRACK_COEFF,\n",
    "    )\n",
    "\n",
    "    # --- Metrics ---\n",
    "    ep_returns = batch[\"ep_returns\"]\n",
    "    if len(ep_returns) > 0:\n",
    "        ep_mean = float(np.mean(ep_returns))\n",
    "        ep_p10 = float(np.percentile(ep_returns, 10))\n",
    "        ep_p90 = float(np.percentile(ep_returns, 90))\n",
    "    else:\n",
    "        ep_mean, ep_p10, ep_p90 = float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        policy_std = float(torch.exp(policy.log_std).mean().item())\n",
    "\n",
    "    history[\"iter\"].append(it)\n",
    "    history[\"ep_ret_mean\"].append(ep_mean)\n",
    "    history[\"ep_ret_p10\"].append(ep_p10)\n",
    "    history[\"ep_ret_p90\"].append(ep_p90)\n",
    "    history[\"kl\"].append(stats[\"kl\"])\n",
    "    history[\"surr_old\"].append(stats[\"surr_old\"])\n",
    "    history[\"surr_new\"].append(stats[\"surr_new\"])\n",
    "    history[\"step_frac\"].append(stats[\"step_frac\"])\n",
    "    history[\"policy_std\"].append(policy_std)\n",
    "\n",
    "    # snapshot policy mean(action|x,v=0) over a grid\n",
    "    if (it == 0) or (it % SNAPSHOT_EVERY == 0) or (it == TOTAL_ITERS - 1):\n",
    "        obs_grid = np.stack([x_grid, np.zeros_like(x_grid)], axis=1)\n",
    "        with torch.no_grad():\n",
    "            mu, _ = policy.forward(torch.as_tensor(obs_grid, dtype=torch.float32, device=DEVICE))\n",
    "        policy_snapshots.append({\"iter\": it, \"mu\": mu.squeeze(-1).cpu().numpy()})\n",
    "\n",
    "    if (it + 1) % max(1, TOTAL_ITERS // 5) == 0 or it == 0:\n",
    "        print(\n",
    "            f\"iter {it:03d} | ep_ret_mean {ep_mean:8.2f} | KL {stats['kl']:.4f} | \"\n",
    "            f\"step_frac {stats['step_frac']:.3f} | std {policy_std:.3f}\"\n",
    "        )\n",
    "\n",
    "print(f\"Done in {time.time() - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eac9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly: learning curves and trust-region diagnostics\n",
    "\n",
    "iters = history[\"iter\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.08,\n",
    "    subplot_titles=(\n",
    "        \"Episodic return (mean + 10/90 percentile band)\",\n",
    "        \"Mean KL(old || new) per update (should be ≤ max_kl)\",\n",
    "        \"Policy std (exp(log_std))\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# return band\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iters, y=history[\"ep_ret_p90\"], mode=\"lines\", line=dict(width=0), showlegend=False),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=iters,\n",
    "        y=history[\"ep_ret_p10\"],\n",
    "        mode=\"lines\",\n",
    "        fill=\"tonexty\",\n",
    "        line=dict(width=0),\n",
    "        name=\"p10–p90\",\n",
    "        opacity=0.25,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iters, y=history[\"ep_ret_mean\"], mode=\"lines+markers\", name=\"mean\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# KL curve\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iters, y=history[\"kl\"], mode=\"lines+markers\", name=\"KL\"),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_hline(y=MAX_KL, line_dash=\"dash\", line_color=\"black\", row=2, col=1)\n",
    "\n",
    "# policy std\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iters, y=history[\"policy_std\"], mode=\"lines+markers\", name=\"std\"),\n",
    "    row=3,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.update_layout(height=850, title=\"TRPO learning diagnostics\")\n",
    "fig.update_xaxes(title_text=\"iteration\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"return\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"KL\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"std\", row=3, col=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa200bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly: how the policy mean changes over iterations\n",
    "\n",
    "fig = go.Figure()\n",
    "for snap in policy_snapshots:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_grid,\n",
    "            y=snap[\"mu\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"iter {snap['iter']}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Policy mean action μ(x, v=0) snapshots\",\n",
    "    xaxis_title=\"position x (with v fixed at 0)\",\n",
    "    yaxis_title=\"mean action μ\",\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa94b68",
   "metadata": {},
   "source": [
    "## 5) Stable-Baselines TRPO (reference implementation)\n",
    "\n",
    "TRPO **does exist** in the original `stable-baselines` (TensorFlow) project via `stable_baselines.trpo_mpi.TRPO` (and is re-exported as `stable_baselines.TRPO` if `mpi4py` is installed).\n",
    "\n",
    "Example usage (not executed here):\n",
    "\n",
    "```python\n",
    "import gym\n",
    "\n",
    "# Requires the original stable-baselines (TensorFlow) + mpi4py.\n",
    "from stable_baselines import TRPO\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = TRPO(\n",
    "    MlpPolicy,\n",
    "    env,\n",
    "    gamma=0.99,\n",
    "    timesteps_per_batch=1024,\n",
    "    max_kl=0.01,\n",
    "    cg_iters=10,\n",
    "    lam=0.98,\n",
    "    entcoeff=0.0,\n",
    "    cg_damping=1e-2,\n",
    "    vf_stepsize=3e-4,\n",
    "    vf_iters=3,\n",
    "    verbose=1,\n",
    ")\n",
    "model.learn(total_timesteps=200_000)\n",
    "```\n",
    "\n",
    "Source used to verify signature and defaults:\n",
    "- https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/trpo_mpi/trpo_mpi.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c44c3d",
   "metadata": {},
   "source": [
    "### Stable-Baselines TRPO hyperparameters (what they mean)\n",
    "\n",
    "From the upstream `TRPO.__init__` signature:\n",
    "\n",
    "- `gamma` — discount factor \\(\\gamma\\)\n",
    "- `timesteps_per_batch` — on-policy batch size (number of environment steps collected before each TRPO update)\n",
    "- `max_kl` — trust-region radius \\(\\delta\\): target/upper bound on mean KL(old \\|\\| new)\n",
    "- `cg_iters` — number of conjugate-gradient iterations used to approximately solve \\(H x = g\\)\n",
    "- `lam` — GAE parameter \\(\\lambda\\) controlling bias/variance tradeoff in advantages\n",
    "- `entcoeff` — entropy bonus coefficient (encourages exploration by penalizing low entropy)\n",
    "- `cg_damping` — adds a small multiple of the identity to the Fisher/Hessian-vector product for numerical stability\n",
    "- `vf_stepsize` — learning rate for the value function optimizer\n",
    "- `vf_iters` — number of value-function optimization iterations per update\n",
    "- `tensorboard_log` / `full_tensorboard_log` — logging configuration\n",
    "- `policy_kwargs` — extra arguments passed to the policy network constructor\n",
    "- `seed` — RNG seed\n",
    "- `n_cpu_tf_sess` — TensorFlow session CPU threading configuration\n",
    "\n",
    "A good way to tune TRPO is to start with:\n",
    "- `max_kl` around `0.01` and adjust up/down for faster learning vs stability\n",
    "- `timesteps_per_batch` larger for smoother updates (at higher compute cost)\n",
    "- `cg_damping` slightly larger if updates become numerically unstable\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}