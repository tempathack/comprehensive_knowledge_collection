{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d4f4b2",
   "metadata": {},
   "source": [
    "# ACKTR (Actor–Critic using Kronecker-Factored Trust Region)\n",
    "\n",
    "ACKTR is an **actor–critic** algorithm that updates the policy using a **trust region** (a KL-divergence constraint) and computes the update direction using **K-FAC** — a Kronecker-factored approximation of the curvature (Fisher / Gauss–Newton).\n",
    "\n",
    "This notebook focuses on the *math and intuition* behind:\n",
    "\n",
    "- **Trust regions** (why we constrain policy updates with KL)\n",
    "- **Natural gradients** and the **Fisher information matrix**\n",
    "- **K-FAC** (how we approximate the Fisher cheaply, layer-wise)\n",
    "\n",
    "---\n",
    "\n",
    "For a full **low-level PyTorch implementation** (including Plotly learning curves), see `01_acktr_from_scratch.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8b3c1",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- write the **KL-constrained** policy improvement objective used by trust-region methods\n",
    "- derive the **natural gradient** step: $\\Delta \\theta \\propto F^{-1} g$\n",
    "- explain **why** $F$ is the right geometry for policy updates (invariance)\n",
    "- explain **K-FAC** as a Kronecker factorization of per-layer curvature\n",
    "- connect **ACKTR** to TRPO-style updates but with a cheaper curvature approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e6f87",
   "metadata": {},
   "source": [
    "## 1) Trust regions: KL-constrained policy improvement\n",
    "\n",
    "In policy gradient methods, we want to improve a policy $\\pi_\\theta(a\\mid s)$ by changing parameters $\\theta$.\n",
    "\n",
    "A naive step $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$ can be **destructive**: it may change the policy too much and collapse performance.\n",
    "\n",
    "Trust-region methods instead solve (conceptually):\n",
    "\n",
    "$$\n",
    "\\max_{\\theta'}\\; \\mathbb{E}_{s\\sim d_{\\pi_\\theta},\\;a\\sim\\pi_\\theta}\\left[\\frac{\\pi_{\\theta'}(a\\mid s)}{\\pi_{\\theta}(a\\mid s)}\\,\\hat A_\\theta(s,a)\\right]\n",
    "+    \\quad\\text{s.t.}\\quad\n",
    "+    \\mathbb{E}_{s\\sim d_{\\pi_\\theta}}\\left[\\mathrm{KL}\\big(\\pi_{\\theta}(\\cdot\\mid s)\\;\\|\\;\\pi_{\\theta'}(\\cdot\\mid s)\\big)\\right] \\le \\delta.\n",
    "+$$\n",
    "\n",
    "- $\\hat A_\\theta(s,a)$ is an advantage estimate.\n",
    "- The constraint upper-bounds the **average KL** change (a trust region radius $\\delta$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c4ef8",
   "metadata": {},
   "source": [
    "## 2) Natural gradient and the Fisher information matrix\n",
    "\n",
    "If we locally approximate the objective and the KL constraint, the constrained problem yields a step in the **natural gradient** direction.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $g = \\nabla_\\theta J(\\theta)$ be the (vanilla) policy gradient.\n",
    "- $F$ be the **Fisher information matrix** of the policy:\n",
    "\n",
    "$$\n",
    "F = \\mathbb{E}_{s\\sim d_{\\pi_\\theta},\\;a\\sim\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\;\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)^\\top\\right].\n",
    "+$$\n",
    "\n",
    "The natural gradient direction is:\n",
    "\n",
    "$$\n",
    "\\Delta\\theta_{\\mathrm{nat}} = F^{-1} g.\n",
    "+$$\n",
    "\n",
    "A trust region step size can be chosen by the quadratic KL approximation:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(\\pi_{\\theta}\\|\\pi_{\\theta+\\alpha\\Delta\\theta}) \\approx \\tfrac{1}{2}\\,\\alpha^2\\, g^\\top F^{-1} g.\n",
    "+$$\n",
    "\n",
    "So, one principled choice is:\n",
    "\n",
    "$$\n",
    "\\alpha = \\sqrt{\\frac{2\\delta}{g^\\top F^{-1} g}}.\n",
    "+$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e4ad4",
   "metadata": {},
   "source": [
    "## 3) K-FAC: Kronecker-factored curvature (layer-wise)\n",
    "\n",
    "Directly forming/inverting $F$ is infeasible for neural networks (millions of parameters).\n",
    "\n",
    "**K-FAC** exploits structure of feed-forward layers to approximate curvature **per layer**.\n",
    "\n",
    "For a linear layer:\n",
    "\n",
    "$$\n",
    "h = W a + b\n",
    "+$$\n",
    "\n",
    "where $a$ is the input activation and $h$ the pre-activation output. Define $g = \\nabla_h \\mathcal{L}$ (the backprop signal into the layer).\n",
    "\n",
    "K-FAC approximates the layer Fisher block as a Kronecker product:\n",
    "\n",
    "$$\n",
    "F_W \\approx \\mathbb{E}[a a^\\top] \\otimes \\mathbb{E}[g g^\\top] \\;=\\; A \\otimes G.\n",
    "+$$\n",
    "\n",
    "This makes the inverse cheap:\n",
    "\n",
    "$$\n",
    "(A \\otimes G)^{-1} = A^{-1} \\otimes G^{-1}.\n",
    "+$$\n",
    "\n",
    "The Kronecker-factored natural gradient for the weight matrix becomes:\n",
    "\n",
    "$$\n",
    "\\Delta W \\approx G^{-1}\\,\\nabla_W \\mathcal{L}\\,A^{-1}.\n",
    "+$$\n",
    "\n",
    "In practice we use:\n",
    "\n",
    "- **exponential moving averages** for $A$ and $G$\n",
    "- **damping** for numerical stability, e.g. $A \\leftarrow A + \\lambda I$, $G \\leftarrow G + \\lambda I$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df5fd8",
   "metadata": {},
   "source": [
    "## 4) What makes it “ACKTR”\n",
    "\n",
    "ACKTR combines:\n",
    "\n",
    "- **actor–critic** losses (policy loss + value loss + entropy bonus)\n",
    "- a **K-FAC preconditioner** (approximate $F^{-1}$)\n",
    "- a **trust region / KL clip** that scales steps to avoid large policy shifts\n",
    "\n",
    "Conceptually the update looks like:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha\\,F^{-1}\\,\\nabla_\\theta \\mathcal{L}(\\theta)\n",
    "+$$\n",
    "\n",
    "with $\\alpha$ chosen (or clipped) so that the predicted KL stays below $\\delta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "    _TORCH_IMPORT_ERROR = e\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    GYMNASIUM_AVAILABLE = True\n",
    "except Exception:\n",
    "    GYMNASIUM_AVAILABLE = False\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print('Python', platform.python_version())\n",
    "print('NumPy', np.__version__)\n",
    "print('Plotly', plotly.__version__)\n",
    "print('Torch', torch.__version__ if TORCH_AVAILABLE else _TORCH_IMPORT_ERROR)\n",
    "print('Gymnasium', gym.__version__ if GYMNASIUM_AVAILABLE else 'not installed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9624b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny Plotly sketch: trust region + K-FAC (conceptual)\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_shape(type='rect', x0=0.05, x1=0.45, y0=0.55, y1=0.85, line=dict(width=2))\n",
    "fig.add_annotation(x=0.25, y=0.70, text='Policy update\\n(natural gradient)', showarrow=False, font=dict(size=14))\n",
    "\n",
    "fig.add_shape(type='rect', x0=0.55, x1=0.95, y0=0.55, y1=0.85, line=dict(width=2))\n",
    "fig.add_annotation(x=0.75, y=0.70, text='K-FAC\\n(Fisher approx.)', showarrow=False, font=dict(size=14))\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.55,\n",
    "    y=0.70,\n",
    "    ax=0.45,\n",
    "    ay=0.70,\n",
    "    xref='paper',\n",
    "    yref='paper',\n",
    "    axref='paper',\n",
    "    ayref='paper',\n",
    "    text='',\n",
    "    showarrow=True,\n",
    "    arrowhead=3,\n",
    "    arrowsize=1.2,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.50,\n",
    "    y=0.40,\n",
    "    text='$\\\\mathbb{E}[\\\\mathrm{KL}(\\\\pi_{old}\\\\|\\\\pi_{new})] \\\\le \\\\delta$\\ncontrols step size',\n",
    "    showarrow=False,\n",
    "    font=dict(size=13),\n",
    ")\n",
    "\n",
    "fig.update_xaxes(visible=False, range=[0, 1])\n",
    "fig.update_yaxes(visible=False, range=[0, 1])\n",
    "fig.update_layout(title='ACKTR: trust region (KL) + K-FAC curvature', height=310)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f2d2c",
   "metadata": {},
   "source": [
    "## Next notebook\n",
    "\n",
    "- `01_acktr_from_scratch.ipynb`: full ACKTR **low-level PyTorch** implementation on a Gymnasium environment + Plotly learning dynamics, including **episodic reward progression**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}