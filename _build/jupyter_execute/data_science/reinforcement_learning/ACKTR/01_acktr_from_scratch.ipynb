{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d1b9fd",
   "metadata": {},
   "source": [
    "# ACKTR from scratch (low-level PyTorch) — CartPole-v1\n",
    "\n",
    "This notebook implements **ACKTR-style** policy optimization in **low-level PyTorch**:\n",
    "\n",
    "- Actor update uses a **K-FAC preconditioner** (approx. Fisher inverse) + **trust-region clipping**.\n",
    "- Critic is trained as a baseline (value function) with a simple first-order optimizer for stability.\n",
    "\n",
    "We log training dynamics and visualize them with **Plotly**, including **episodic reward progression**.\n",
    "\n",
    "Prereqs:\n",
    "\n",
    "- PyTorch\n",
    "- Gymnasium\n",
    "- Plotly\n",
    "\n",
    "Theory reference: see `00_overview.ipynb` in this folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba00d5",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Setup + environment\n",
    "2. Actor/Critic networks\n",
    "3. Rollout collection + GAE\n",
    "4. K-FAC optimizer (Linear layers)\n",
    "5. Training loop (ACKTR update)\n",
    "6. Plotly diagnostics (reward + KL + losses)\n",
    "7. Stable-Baselines ACKTR reference + hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print('NumPy', np.__version__)\n",
    "print('Pandas', pd.__version__)\n",
    "print('Plotly', plotly.__version__)\n",
    "print('Gymnasium', gym.__version__)\n",
    "print('Torch', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Keep the implementation CPU-friendly and deterministic-ish.\n",
    "DEVICE = torch.device('cpu')\n",
    "print('DEVICE', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f71e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run configuration ---\n",
    "FAST_RUN = True  # set False for a longer, smoother curve\n",
    "\n",
    "# Environment\n",
    "ENV_ID = 'CartPole-v1'\n",
    "\n",
    "# Rollout / training\n",
    "TOTAL_TIMESTEPS = 40_000 if FAST_RUN else 200_000\n",
    "ROLLOUT_STEPS = 128\n",
    "\n",
    "# Discounting / advantage\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "# Loss weights\n",
    "ENT_COEF = 0.00\n",
    "\n",
    "# Critic optimizer\n",
    "CRITIC_LR = 1e-3\n",
    "\n",
    "# K-FAC / ACKTR knobs (actor)\n",
    "ACTOR_LR = 0.10\n",
    "KFAC_DAMPING = 0.03\n",
    "KFAC_STATS_DECAY = 0.95\n",
    "KFAC_CLIP = 0.01  # trust region / KL clip (see theory)\n",
    "INVERSE_UPDATE_INTERVAL = 1\n",
    "\n",
    "print('TOTAL_TIMESTEPS', TOTAL_TIMESTEPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a2dbc",
   "metadata": {},
   "source": [
    "## 1) Environment\n",
    "\n",
    "`CartPole-v1` is a classic discrete-action benchmark:\n",
    "\n",
    "- state $s \\in \\mathbb{R}^4$\n",
    "- actions $a \\in \\{0,1\\}$\n",
    "- reward $r_t = 1$ per step until termination\n",
    "\n",
    "It’s a good fit for a minimal ACKTR demonstration because the policy is a **categorical distribution**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_ID)\n",
    "obs_dim = int(env.observation_space.shape[0])\n",
    "act_dim = int(env.action_space.n)\n",
    "\n",
    "obs, _ = env.reset(seed=SEED)\n",
    "print('obs_dim', obs_dim, 'act_dim', act_dim)\n",
    "print('first obs', obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3a4b6",
   "metadata": {},
   "source": [
    "## 2) Actor–critic parameterization\n",
    "\n",
    "We use two networks:\n",
    "\n",
    "- Actor: logits for a categorical policy $\\pi_\\theta(a\\mid s)$.\n",
    "- Critic: a value function baseline $V_\\phi(s)$.\n",
    "\n",
    "The actor loss (policy gradient with entropy bonus) is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\pi}(\\theta)\n",
    "+    = -\\mathbb{E}\\left[\\log \\pi_\\theta(a\\mid s)\\,\\hat A(s,a)\\right]\n",
    "+    - \\beta\\,\\mathbb{E}\\left[\\mathcal{H}(\\pi_\\theta(\\cdot\\mid s))\\right].\n",
    "+$$\n",
    "\n",
    "The critic trains by regression to (bootstrapped) returns:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_V(\\phi) = \\tfrac{1}{2}\\,\\mathbb{E}\\left[(V_\\phi(s) - \\hat R)^2\\right].\n",
    "+$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes=(64, 64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.Tanh())\n",
    "            last = h\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.logits = nn.Linear(last, act_dim)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.logits(self.net(obs))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, hidden_sizes=(64, 64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.Tanh())\n",
    "            last = h\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.v = nn.Linear(last, 1)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.v(self.net(obs)).squeeze(-1)\n",
    "\n",
    "\n",
    "actor = Actor(obs_dim, act_dim).to(DEVICE)\n",
    "critic = Critic(obs_dim).to(DEVICE)\n",
    "\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "print('actor params', sum(p.numel() for p in actor.parameters()))\n",
    "print('critic params', sum(p.numel() for p in critic.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5cf8a2",
   "metadata": {},
   "source": [
    "## 3) Rollouts + GAE\n",
    "\n",
    "We collect on-policy rollouts of length $T$ and compute **generalized advantage estimation** (GAE):\n",
    "\n",
    "$$\n",
    "\\delta_t = r_t + \\gamma (1-d_t) V(s_{t+1}) - V(s_t)\n",
    "+$$\n",
    "\n",
    "$$\n",
    "\\hat A_t = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l\\,\\delta_{t+l}\n",
    "+$$\n",
    "\n",
    "with $d_t \\in \\{0,1\\}$ indicating episode termination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf45e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, last_value, gamma: float, lam: float):\n",
    "    \"\"\"NumPy GAE for a single rollout segment.\"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = np.zeros(T, dtype=np.float32)\n",
    "    gae = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        next_value = last_value if t == T - 1 else values[t + 1]\n",
    "        next_nonterminal = 1.0 - dones[t]\n",
    "        delta = rewards[t] + gamma * next_value * next_nonterminal - values[t]\n",
    "        gae = delta + gamma * lam * next_nonterminal * gae\n",
    "        advantages[t] = gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f6e8b",
   "metadata": {},
   "source": [
    "## 4) K-FAC optimizer (Linear layers)\n",
    "\n",
    "ACKTR replaces a vanilla gradient step with a **(preconditioned) natural gradient** step.\n",
    "\n",
    "For the policy parameters $\\theta$, the natural gradient direction is:\n",
    "\n",
    "$$\n",
    "\\Delta\\theta = F^{-1} g,\\qquad g = \\nabla_\\theta J(\\theta).\n",
    "+$$\n",
    "\n",
    "K-FAC approximates $F$ block-wise per layer using Kronecker factors:\n",
    "\n",
    "$$\n",
    "F_{\\ell} \\approx G_{\\ell} \\otimes A_{\\ell},\n",
    "+\\quad A_{\\ell}=\\mathbb{E}[a a^\\top],\\quad G_{\\ell}=\\mathbb{E}[g g^\\top].\n",
    "+$$\n",
    "\n",
    "For a linear layer, this yields the matrix-form update (with damping):\n",
    "\n",
    "$$\n",
    "\\Delta W_{\\ell} \\approx G_{\\ell}^{-1}\\,\\nabla_{W_{\\ell}}\\mathcal{L}\\,A_{\\ell}^{-1}.\n",
    "+$$\n",
    "\n",
    "We also apply a trust-region-style scaling so the policy does not change too much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFACOptimizer:\n",
    "    \"\"\"Minimal K-FAC for nn.Linear modules (actor only).\n",
    "\n",
    "    - Collects factor stats (A,G) via forward/backward hooks on a Fisher-like loss.\n",
    "    - Preconditions parameter gradients with G^{-1} @ grad @ A^{-1}.\n",
    "    - Scales the step using a trust-region clip.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        lr: float,\n",
    "        damping: float,\n",
    "        stats_decay: float,\n",
    "        kfac_clip: float,\n",
    "        inverse_update_interval: int = 1,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.lr = float(lr)\n",
    "        self.damping = float(damping)\n",
    "        self.stats_decay = float(stats_decay)\n",
    "        self.kfac_clip = float(kfac_clip)\n",
    "        self.inverse_update_interval = int(inverse_update_interval)\n",
    "\n",
    "        self._collect_stats = False\n",
    "        self._step = 0\n",
    "\n",
    "        self.modules = []\n",
    "        self.state = {}\n",
    "\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                self.modules.append(module)\n",
    "                self.state[module] = {\n",
    "                    'A': None,\n",
    "                    'G': None,\n",
    "                    'A_inv': None,\n",
    "                    'G_inv': None,\n",
    "                }\n",
    "                module.register_forward_hook(self._forward_hook)\n",
    "                module.register_full_backward_hook(self._backward_hook)\n",
    "\n",
    "    def set_collect_stats(self, collect: bool):\n",
    "        self._collect_stats = bool(collect)\n",
    "\n",
    "    def _forward_hook(self, module, inputs, output):\n",
    "        if not self._collect_stats:\n",
    "            return\n",
    "        module._kfac_input = inputs[0].detach()\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        if not self._collect_stats:\n",
    "            return\n",
    "        module._kfac_grad_output = grad_output[0].detach()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_stats(self):\n",
    "        for module in self.modules:\n",
    "            if not hasattr(module, '_kfac_input') or not hasattr(module, '_kfac_grad_output'):\n",
    "                continue\n",
    "\n",
    "            a = module._kfac_input\n",
    "            g = module._kfac_grad_output\n",
    "\n",
    "            if a.dim() != 2 or g.dim() != 2:\n",
    "                continue\n",
    "\n",
    "            batch = a.shape[0]\n",
    "            ones = torch.ones(batch, 1, device=a.device, dtype=a.dtype)\n",
    "            a_aug = torch.cat([a, ones], dim=1)\n",
    "\n",
    "            A_new = (a_aug.t() @ a_aug) / batch\n",
    "            G_new = (g.t() @ g) / batch\n",
    "\n",
    "            st = self.state[module]\n",
    "            if st['A'] is None:\n",
    "                st['A'] = A_new\n",
    "                st['G'] = G_new\n",
    "            else:\n",
    "                d = self.stats_decay\n",
    "                st['A'] = d * st['A'] + (1 - d) * A_new\n",
    "                st['G'] = d * st['G'] + (1 - d) * G_new\n",
    "\n",
    "        self._step += 1\n",
    "        if self._step % self.inverse_update_interval == 0:\n",
    "            self._update_inverses()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_inverses(self):\n",
    "        for module in self.modules:\n",
    "            st = self.state[module]\n",
    "            if st['A'] is None or st['G'] is None:\n",
    "                continue\n",
    "\n",
    "            A = st['A'] + self.damping * torch.eye(st['A'].shape[0], device=st['A'].device, dtype=st['A'].dtype)\n",
    "            G = st['G'] + self.damping * torch.eye(st['G'].shape[0], device=st['G'].device, dtype=st['G'].dtype)\n",
    "\n",
    "            st['A_inv'] = torch.linalg.inv(A)\n",
    "            st['G_inv'] = torch.linalg.inv(G)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        eps = 1e-8\n",
    "\n",
    "        shs = 0.0  # proxy for g^T F^{-1} g\n",
    "        updates = []\n",
    "\n",
    "        for module in self.modules:\n",
    "            st = self.state[module]\n",
    "            if st['A_inv'] is None or st['G_inv'] is None:\n",
    "                continue\n",
    "\n",
    "            if module.weight.grad is None:\n",
    "                continue\n",
    "            if module.bias is None or module.bias.grad is None:\n",
    "                continue\n",
    "\n",
    "            grad_w = module.weight.grad\n",
    "            grad_b = module.bias.grad\n",
    "            grad_wb = torch.cat([grad_w, grad_b.unsqueeze(1)], dim=1)\n",
    "\n",
    "            nat_wb = st['G_inv'] @ grad_wb @ st['A_inv']\n",
    "            nat_w = nat_wb[:, :-1]\n",
    "            nat_b = nat_wb[:, -1]\n",
    "\n",
    "            shs += float((grad_w * nat_w).sum().item() + (grad_b * nat_b).sum().item())\n",
    "            updates.append((module.weight, nat_w))\n",
    "            updates.append((module.bias, nat_b))\n",
    "\n",
    "        # Trust-region / KL clip: only scale down.\n",
    "        # (Theory: predicted KL \\approx 0.5 * alpha^2 * g^T F^{-1} g)\n",
    "        nu = 1.0\n",
    "        if shs > 0:\n",
    "            predicted_kl = 0.5 * shs\n",
    "            nu = float(min(1.0, np.sqrt(self.kfac_clip / (predicted_kl + eps))))\n",
    "        else:\n",
    "            predicted_kl = 0.0\n",
    "\n",
    "        for param, nat_grad in updates:\n",
    "            param.add_(nat_grad, alpha=-self.lr * nu)\n",
    "\n",
    "        return {\n",
    "            'shs': shs,\n",
    "            'predicted_kl': predicted_kl,\n",
    "            'nu': nu,\n",
    "        }\n",
    "\n",
    "\n",
    "actor_kfac = KFACOptimizer(\n",
    "    actor,\n",
    "    lr=ACTOR_LR,\n",
    "    damping=KFAC_DAMPING,\n",
    "    stats_decay=KFAC_STATS_DECAY,\n",
    "    kfac_clip=KFAC_CLIP,\n",
    "    inverse_update_interval=INVERSE_UPDATE_INTERVAL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68842f5",
   "metadata": {},
   "source": [
    "## 5) Training loop (ACKTR update)\n",
    "\n",
    "Each update:\n",
    "\n",
    "1. Collect $T$ on-policy transitions.\n",
    "2. Compute $\\hat A_t$ and $\\hat R_t$ with GAE.\n",
    "3. Update critic by minimizing $\\mathcal{L}_V$.\n",
    "4. For the actor:\n",
    "   - build a Fisher-like loss (to collect K-FAC stats)\n",
    "   - backprop that loss to update $A$ and $G$\n",
    "   - backprop the policy loss and take a K-FAC-preconditioned step\n",
    "\n",
    "We log:\n",
    "\n",
    "- episodic returns\n",
    "- actor loss, critic loss, entropy\n",
    "- estimated KL (before/after update)\n",
    "- trust-region scale factor $\\nu$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = TOTAL_TIMESTEPS // ROLLOUT_STEPS\n",
    "print('num_updates', num_updates)\n",
    "\n",
    "obs, _ = env.reset(seed=SEED)\n",
    "\n",
    "episode_return = 0.0\n",
    "episode_len = 0\n",
    "episode_returns = []\n",
    "episode_lengths = []\n",
    "\n",
    "logs = []\n",
    "start = time.time()\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "    # --- Rollout buffers ---\n",
    "    obs_buf = np.zeros((ROLLOUT_STEPS, obs_dim), dtype=np.float32)\n",
    "    act_buf = np.zeros((ROLLOUT_STEPS,), dtype=np.int64)\n",
    "    rew_buf = np.zeros((ROLLOUT_STEPS,), dtype=np.float32)\n",
    "    done_buf = np.zeros((ROLLOUT_STEPS,), dtype=np.float32)\n",
    "    val_buf = np.zeros((ROLLOUT_STEPS,), dtype=np.float32)\n",
    "\n",
    "    for t in range(ROLLOUT_STEPS):\n",
    "        obs_buf[t] = obs\n",
    "\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = actor(obs_t)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            value = critic(obs_t)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(int(action.item()))\n",
    "        done = bool(terminated or truncated)\n",
    "\n",
    "        act_buf[t] = int(action.item())\n",
    "        rew_buf[t] = float(reward)\n",
    "        done_buf[t] = float(done)\n",
    "        val_buf[t] = float(value.item())\n",
    "\n",
    "        episode_return += float(reward)\n",
    "        episode_len += 1\n",
    "\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            episode_returns.append(episode_return)\n",
    "            episode_lengths.append(episode_len)\n",
    "            episode_return = 0.0\n",
    "            episode_len = 0\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if done_buf[-1] == 1.0:\n",
    "            last_value = 0.0\n",
    "        else:\n",
    "            last_obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            last_value = float(critic(last_obs_t).item())\n",
    "\n",
    "    advantages, returns = compute_gae(\n",
    "        rewards=rew_buf,\n",
    "        values=val_buf,\n",
    "        dones=done_buf,\n",
    "        last_value=last_value,\n",
    "        gamma=GAMMA,\n",
    "        lam=GAE_LAMBDA,\n",
    "    )\n",
    "\n",
    "    obs_batch = torch.tensor(obs_buf, dtype=torch.float32, device=DEVICE)\n",
    "    act_batch = torch.tensor(act_buf, dtype=torch.int64, device=DEVICE)\n",
    "    adv_batch = torch.tensor(advantages, dtype=torch.float32, device=DEVICE)\n",
    "    ret_batch = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    adv_batch = (adv_batch - adv_batch.mean()) / (adv_batch.std() + 1e-8)\n",
    "\n",
    "    # --- Critic update (first-order) ---\n",
    "    critic_optim.zero_grad(set_to_none=True)\n",
    "    v_pred = critic(obs_batch)\n",
    "    critic_loss = 0.5 * (ret_batch - v_pred).pow(2).mean()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "\n",
    "    # --- Actor update (ACKTR-style via K-FAC) ---\n",
    "    actor_kfac.set_collect_stats(True)\n",
    "    logits_old = actor(obs_batch).detach()\n",
    "    dist_old = Categorical(logits=logits_old)\n",
    "\n",
    "    logits = actor(obs_batch)\n",
    "    dist = Categorical(logits=logits)\n",
    "    logp = dist.log_prob(act_batch)\n",
    "    entropy = dist.entropy().mean()\n",
    "\n",
    "    actor_loss = -(logp * adv_batch.detach()).mean() - ENT_COEF * entropy\n",
    "\n",
    "    # Fisher-like loss: E[-log pi(a|s)]\n",
    "    fisher_loss = -logp.mean()\n",
    "\n",
    "    actor.zero_grad(set_to_none=True)\n",
    "    fisher_loss.backward(retain_graph=True)\n",
    "    actor_kfac.set_collect_stats(False)\n",
    "    actor_kfac.update_stats()\n",
    "\n",
    "    actor.zero_grad(set_to_none=True)\n",
    "    actor_loss.backward()\n",
    "    step_info = actor_kfac.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_new = actor(obs_batch)\n",
    "        dist_new = Categorical(logits=logits_new)\n",
    "        approx_kl = torch.distributions.kl_divergence(dist_old, dist_new).mean().item()\n",
    "\n",
    "    logs.append(\n",
    "        {\n",
    "            'update': update,\n",
    "            'timesteps': update * ROLLOUT_STEPS,\n",
    "            'episodes': len(episode_returns),\n",
    "            'actor_loss': float(actor_loss.item()),\n",
    "            'critic_loss': float(critic_loss.item()),\n",
    "            'entropy': float(entropy.item()),\n",
    "            'approx_kl': float(approx_kl),\n",
    "            **step_info,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if update % 25 == 0:\n",
    "        recent = episode_returns[-20:]\n",
    "        mean_20 = float(np.mean(recent)) if recent else float('nan')\n",
    "        elapsed = time.time() - start\n",
    "        print(\n",
    "            f'update {update:4d}/{num_updates} | episodes {len(episode_returns):4d} '\n",
    "            f'| mean_return_20 {mean_20:7.2f} | kl {approx_kl:9.2e} | nu {step_info[\"nu\"]:7.3f} '\n",
    "            f'| elapsed {elapsed:6.1f}s'\n",
    "        )\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ebc1a",
   "metadata": {},
   "source": [
    "## 6) Plotly: learning dynamics\n",
    "\n",
    "We visualize:\n",
    "\n",
    "- episodic reward progression (raw + smoothed)\n",
    "- estimated KL per update\n",
    "- actor/critic losses\n",
    "- trust-region scaling factor $\\nu$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs = pd.DataFrame(logs)\n",
    "df_eps = pd.DataFrame({'episode': np.arange(1, len(episode_returns) + 1), 'return': episode_returns})\n",
    "df_eps['return_smooth'] = df_eps['return'].rolling(window=20, min_periods=1).mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_eps['episode'], y=df_eps['return'], mode='lines', name='return', line=dict(width=1)))\n",
    "fig.add_trace(go.Scatter(x=df_eps['episode'], y=df_eps['return_smooth'], mode='lines', name='return (20-ep mean)', line=dict(width=3)))\n",
    "fig.update_layout(\n",
    "    title='Episodic reward progression (CartPole-v1)',\n",
    "    xaxis_title='Episode',\n",
    "    yaxis_title='Return',\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig2 = px.line(df_logs, x='timesteps', y=['approx_kl', 'predicted_kl'], title='KL diagnostics per update')\n",
    "fig2.update_layout(height=380)\n",
    "fig2.show()\n",
    "\n",
    "fig3 = px.line(df_logs, x='timesteps', y=['actor_loss', 'critic_loss'], title='Losses per update')\n",
    "fig3.update_layout(height=380)\n",
    "fig3.show()\n",
    "\n",
    "fig4 = px.line(df_logs, x='timesteps', y=['nu'], title='Trust-region scaling (nu)')\n",
    "fig4.update_layout(height=320)\n",
    "fig4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4715c1",
   "metadata": {},
   "source": [
    "## 7) Stable-Baselines ACKTR (reference)\n",
    "\n",
    "We’ll include a reference snippet for the (TensorFlow-based) Stable-Baselines implementation of ACKTR, plus an explanation of its key hyperparameters.\n",
    "\n",
    "This section is **reference only** — the implementation above is the main deliverable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7237ae",
   "metadata": {},
   "source": [
    "### Stable-Baselines usage (snippet)\n",
    "\n",
    "```python\n",
    "# pip install stable-baselines==2.*  (TensorFlow 1.x based)\n",
    "from stable_baselines import ACKTR\n",
    "\n",
    "model = ACKTR(\n",
    "    policy='MlpPolicy',\n",
    "    env='CartPole-v1',\n",
    "    n_steps=20,\n",
    "    gamma=0.99,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.25,\n",
    "    vf_fisher_coef=1.0,\n",
    "    learning_rate=0.25,\n",
    "    max_grad_norm=0.5,\n",
    "    kfac_clip=0.001,\n",
    "    lr_schedule='linear',\n",
    "    kfac_update=1,\n",
    "    gae_lambda=None,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=200_000)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05a6cc",
   "metadata": {},
   "source": [
    "### Hyperparameters (Stable-Baselines) explained\n",
    "\n",
    "Stable-Baselines (\"v2\", TensorFlow 1.x) includes an `ACKTR` implementation (see `stable_baselines/acktr/acktr.py`). The constructor signature is:\n",
    "\n",
    "```python\n",
    "ACKTR(\n",
    "  policy,\n",
    "  env,\n",
    "  gamma=0.99,\n",
    "  n_steps=20,\n",
    "  ent_coef=0.01,\n",
    "  vf_coef=0.25,\n",
    "  vf_fisher_coef=1.0,\n",
    "  learning_rate=0.25,\n",
    "  max_grad_norm=0.5,\n",
    "  kfac_clip=0.001,\n",
    "  lr_schedule='linear',\n",
    "  async_eigen_decomp=False,\n",
    "  kfac_update=1,\n",
    "  gae_lambda=None,\n",
    "  policy_kwargs=None,\n",
    "  seed=None,\n",
    "  n_cpu_tf_sess=1,\n",
    "  # + logging/boilerplate args\n",
    ")\n",
    "```\n",
    "\n",
    "**Core RL knobs**\n",
    "\n",
    "- `gamma`: discount factor.\n",
    "- `n_steps`: rollout length per environment before each update.\n",
    "- `gae_lambda`: if not `None`, Stable-Baselines computes GAE with parameter $\\lambda$; if `None`, it uses the classic advantage (no GAE).\n",
    "- `ent_coef`: entropy bonus weight (encourages exploration).\n",
    "- `vf_coef`: value loss weight in the joint loss.\n",
    "\n",
    "**ACKTR / K-FAC + trust region knobs**\n",
    "\n",
    "- `kfac_clip`: KL-based clip used inside the K-FAC optimizer (trust-region-like safeguard; called `clip_kl` in the underlying optimizer).\n",
    "- `vf_fisher_coef`: weight on the **value-function Fisher loss**. In the Stable-Baselines code, the value Fisher is constructed by adding noise to the value output and backpropagating a Gaussian negative log-likelihood; this lets K-FAC build curvature stats for the critic.\n",
    "- `learning_rate`: the step size used by the K-FAC optimizer (and scheduled by `lr_schedule`).\n",
    "- `lr_schedule`: learning-rate schedule string (`'linear'`, `'constant'`, `'double_linear_con'`, `'middle_drop'`, `'double_middle_drop'`).\n",
    "- `kfac_update`: update frequency for K-FAC statistics / eigen decompositions.\n",
    "- `async_eigen_decomp`: compute eigen decompositions asynchronously (speed/throughput trade-off).\n",
    "- `max_grad_norm`: global gradient clipping.\n",
    "\n",
    "**Practical / reproducibility knobs**\n",
    "\n",
    "- `policy`: policy network type (e.g. `MlpPolicy`, `CnnPolicy`, `CnnLstmPolicy`).\n",
    "- `env`: Gym env instance or env id string.\n",
    "- `policy_kwargs`: extra arguments forwarded to the policy.\n",
    "- `seed`: seeds python/NumPy/TensorFlow RNGs.\n",
    "- `n_cpu_tf_sess`: TensorFlow thread count (for determinism, set this to `1`).\n",
    "\n",
    "Note: Stable-Baselines wires `ACKTR` into `kfac.KfacOptimizer(...)` with additional internal defaults (e.g. `momentum=0.9`, `epsilon=0.01`, `stats_decay=0.99`, `cold_iter=10`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}