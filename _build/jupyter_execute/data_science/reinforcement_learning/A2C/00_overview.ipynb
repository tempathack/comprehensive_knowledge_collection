{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47957a4",
   "metadata": {},
   "source": [
    "# A2C (Advantage Actor-Critic) — Low-Level PyTorch Implementation (CartPole-v1)\n",
    "\n",
    "A2C is an **on-policy actor-critic** algorithm:\n",
    "\n",
    "- the **actor** learns a policy $\\pi_\\theta(a\\mid s)$ (how to act)\n",
    "- the **critic** learns a value function $V_\\phi(s)$ (how good a state is)\n",
    "- the actor is trained with **advantages** (\"better than expected\" signals)\n",
    "\n",
    "This notebook builds the math carefully, then implements A2C with **minimal PyTorch** (no RL libraries, no high-level training abstractions), using a **vectorized Gymnasium environment** for synchronous rollouts.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- derive the A2C update from the **policy gradient theorem**\n",
    "- explain why the **baseline** (critic) reduces variance\n",
    "- implement **GAE($\\gamma,\\lambda$)** and n-step bootstrapped returns\n",
    "- train an A2C agent on `CartPole-v1` and visualize learning with Plotly\n",
    "- map the concepts to **Stable-Baselines3** A2C hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e43739",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. A2C intuition + what “advantage” means\n",
    "2. Mathematical formulation (LaTeX)\n",
    "3. Low-level PyTorch implementation (actor + critic)\n",
    "4. Training on CartPole with vectorized rollouts\n",
    "5. Plotly diagnostics (returns, losses, policy/value slices)\n",
    "6. Stable-Baselines3 A2C reference + hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    GYMNASIUM_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    GYMNASIUM_AVAILABLE = False\n",
    "    _GYM_IMPORT_ERROR = e\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "    _TORCH_IMPORT_ERROR = e\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "assert GYMNASIUM_AVAILABLE, f\"gymnasium import failed: {_GYM_IMPORT_ERROR}\"\n",
    "assert TORCH_AVAILABLE, f\"torch import failed: {_TORCH_IMPORT_ERROR}\"\n",
    "\n",
    "print('gymnasium', gym.__version__)\n",
    "print('torch', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run configuration ---\n",
    "\n",
    "# Keep FAST_RUN=True for a quick demo.\n",
    "# For a more reliable \"solve\", set FAST_RUN=False.\n",
    "FAST_RUN = True\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"  # discrete actions, small continuous state\n",
    "SEED = 42\n",
    "\n",
    "# A2C is usually run with multiple envs in parallel.\n",
    "N_ENVS = 8 if FAST_RUN else 16\n",
    "\n",
    "# Rollout horizon per env (A2C commonly uses small n_steps).\n",
    "N_STEPS = 5\n",
    "\n",
    "# Total interaction budget\n",
    "TOTAL_TIMESTEPS = 30_000 if FAST_RUN else 200_000\n",
    "\n",
    "# Core RL hyperparameters\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 1.0  # 1.0 => classic advantage w/ n-step bootstrapping\n",
    "\n",
    "# Loss weights\n",
    "ENT_COEF = 0.01\n",
    "VF_COEF = 0.5\n",
    "\n",
    "# Optimization\n",
    "LR = 7e-4\n",
    "MAX_GRAD_NORM = 0.5\n",
    "RMSPROP_EPS = 1e-5\n",
    "\n",
    "# Optional: normalize advantage each update\n",
    "NORMALIZE_ADVANTAGE = True\n",
    "\n",
    "# Network\n",
    "HIDDEN_SIZES = (128, 128)\n",
    "\n",
    "# Logging\n",
    "LOG_EVERY_UPDATES = 50\n",
    "RETURN_SMOOTHING_WINDOW = 50\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72cb5b",
   "metadata": {},
   "source": [
    "## 1) A2C intuition: actor + critic + advantage\n",
    "\n",
    "### Actor\n",
    "The actor is a stochastic policy $\\pi_\\theta(a\\mid s)$.\n",
    "\n",
    "- It outputs a distribution over actions.\n",
    "- We sample actions from that distribution to explore.\n",
    "\n",
    "### Critic\n",
    "The critic is a value function $V_\\phi(s)$.\n",
    "\n",
    "- It predicts the **expected discounted return** from state $s$.\n",
    "- It is trained via regression to match a bootstrapped return target.\n",
    "\n",
    "### Advantage\n",
    "The advantage measures how much better an action did compared to what the critic expected:\n",
    "\n",
    "$$\n",
    "A(s_t, a_t) = Q(s_t, a_t) - V(s_t).\n",
    "$$\n",
    "\n",
    "If $A(s_t,a_t)$ is positive, the action was better than expected, and the actor should increase its probability.\n",
    "\n",
    "### Why “A2C”?\n",
    "A2C is the **synchronous** version of A3C:\n",
    "\n",
    "- A3C: many workers update parameters asynchronously.\n",
    "- A2C: many workers collect experience **in parallel**, then we do a **single synchronized** update.\n",
    "\n",
    "In practice, A2C typically uses a vectorized environment and batches data as:\n",
    "\n",
    "$$\n",
    "\\text{batch size} = n_{\\text{env}} \\times n_{\\text{steps}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9515a9",
   "metadata": {},
   "source": [
    "## 2) Mathematical formulation (policy gradient + baseline)\n",
    "\n",
    "We model the environment as an MDP $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$.\n",
    "\n",
    "### Return\n",
    "The discounted return from time $t$ is:\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}.\n",
    "$$\n",
    "\n",
    "### Objective\n",
    "We want to maximize expected return:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t\\right].\n",
    "$$\n",
    "\n",
    "### Policy gradient theorem\n",
    "A standard form is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\, Q^{\\pi_\\theta}(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "### Baseline (variance reduction)\n",
    "We can subtract a baseline $b(s_t)$ without changing the expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\, b(s_t)] = 0.\n",
    "$$\n",
    "\n",
    "Choosing $b(s_t)=V_\\phi(s_t)$ yields the advantage form:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\, A_t\\right],\n",
    "\\quad A_t \\approx \\hat{A}(s_t,a_t).\n",
    "$$\n",
    "\n",
    "### Bootstrapped n-step return\n",
    "With a rollout horizon $T$ (a.k.a. `n_steps`), we use a bootstrapped target:\n",
    "\n",
    "$$\n",
    "\\hat{R}_t = \\sum_{k=0}^{T-1-t} \\gamma^k r_{t+k} + \\gamma^{T-t} V_\\phi(s_T).\n",
    "$$\n",
    "\n",
    "### Generalized Advantage Estimation (GAE)\n",
    "GAE defines the TD residual:\n",
    "\n",
    "$$\n",
    "\\delta_t = r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)\n",
    "$$\n",
    "\n",
    "and computes advantages with an exponentially-weighted sum:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t^{\\mathrm{GAE}(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l\\, \\delta_{t+l}.\n",
    "$$\n",
    "\n",
    "- $\\lambda=1$ recovers the classic (higher-variance) advantage.\n",
    "- smaller $\\lambda$ reduces variance but increases bias.\n",
    "\n",
    "### Loss functions (minimization form)\n",
    "\n",
    "Actor loss (to maximize expected advantage):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{actor}}(\\theta) = -\\mathbb{E}\\left[\\log \\pi_\\theta(a_t\\mid s_t)\\, \\hat{A}_t\\right].\n",
    "$$\n",
    "\n",
    "Critic loss (value regression):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{critic}}(\\phi) = \\frac{1}{2}\\,\\mathbb{E}\\left[(\\hat{R}_t - V_\\phi(s_t))^2\\right].\n",
    "$$\n",
    "\n",
    "Entropy bonus (encourage exploration):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{entropy}}(\\theta) = -\\mathbb{E}\\left[\\mathcal{H}(\\pi_\\theta(\\cdot\\mid s_t))\\right].\n",
    "$$\n",
    "\n",
    "Total loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{actor}} + c_v\\,\\mathcal{L}_{\\text{critic}} + c_e\\,\\mathcal{L}_{\\text{entropy}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vec_env(env_id: str, n_envs: int, seed: int) -> gym.vector.SyncVectorEnv:\n",
    "    env_fns = [lambda: gym.make(env_id) for _ in range(n_envs)]\n",
    "    env = gym.vector.SyncVectorEnv(env_fns, autoreset_mode=gym.vector.AutoresetMode.SAME_STEP)\n",
    "    env.reset(seed=[seed + i for i in range(n_envs)])\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_vec_env(ENV_ID, N_ENVS, SEED)\n",
    "\n",
    "obs_space = env.single_observation_space\n",
    "act_space = env.single_action_space\n",
    "\n",
    "assert isinstance(act_space, gym.spaces.Discrete), \"This notebook's implementation uses discrete actions (Categorical).\"\n",
    "\n",
    "OBS_DIM = int(np.prod(obs_space.shape))\n",
    "N_ACTIONS = int(act_space.n)\n",
    "\n",
    "print('obs_space', obs_space)\n",
    "print('act_space', act_space)\n",
    "print('OBS_DIM', OBS_DIM, 'N_ACTIONS', N_ACTIONS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e873d2",
   "metadata": {},
   "source": [
    "## 3) Actor-Critic network (low-level PyTorch)\n",
    "\n",
    "We use a shared MLP trunk, then two heads:\n",
    "\n",
    "- **actor head** outputs logits for a categorical distribution\n",
    "- **critic head** outputs a scalar value $V(s)$\n",
    "\n",
    "This is not the only architecture (you can also use separate networks), but it’s a common and effective baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden_sizes: tuple[int, int] = (128, 128)):\n",
    "        super().__init__()\n",
    "\n",
    "        h1, h2 = hidden_sizes\n",
    "        self.fc1 = nn.Linear(obs_dim, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "\n",
    "        self.actor = nn.Linear(h2, n_actions)\n",
    "        self.critic = nn.Linear(h2, 1)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # obs: (B, obs_dim)\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        logits = self.actor(x)            # (B, n_actions)\n",
    "        values = self.critic(x).squeeze(-1)  # (B,)\n",
    "        return logits, values\n",
    "\n",
    "\n",
    "def sample_actions_and_logp(logits: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Low-level categorical sampling without torch.distributions.\n",
    "\n",
    "    Returns:\n",
    "      actions: (B,) int64\n",
    "      logp:    (B,) log-prob of sampled action\n",
    "      entropy: (B,) categorical entropy\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # (B, A)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    actions = torch.multinomial(probs, num_samples=1).squeeze(-1)  # (B,)\n",
    "    logp = log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "\n",
    "    return actions, logp, entropy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def policy_action_probs(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return F.softmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073191a2",
   "metadata": {},
   "source": [
    "## 4) GAE implementation\n",
    "\n",
    "We compute advantages backwards in time:\n",
    "\n",
    "$$\n",
    "\\delta_t = r_t + \\gamma (1-d_t) V(s_{t+1}) - V(s_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_t = \\delta_t + \\gamma\\lambda(1-d_t) A_{t+1}\n",
    "$$\n",
    "\n",
    "where $d_t\\in\\{0,1\\}$ is the done flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb51489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    rewards: torch.Tensor,   # (T, N)\n",
    "    dones: torch.Tensor,     # (T, N) float32 {0,1}\n",
    "    values: torch.Tensor,    # (T, N)\n",
    "    last_values: torch.Tensor,  # (N,)\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    T, N = rewards.shape\n",
    "    advantages = torch.zeros((T, N), device=rewards.device, dtype=torch.float32)\n",
    "    last_adv = torch.zeros((N,), device=rewards.device, dtype=torch.float32)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        mask = 1.0 - dones[t]\n",
    "        next_values = last_values if t == T - 1 else values[t + 1]\n",
    "        delta = rewards[t] + gamma * mask * next_values - values[t]\n",
    "        last_adv = delta + gamma * gae_lambda * mask * last_adv\n",
    "        advantages[t] = last_adv\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c5e67",
   "metadata": {},
   "source": [
    "## 5) Training loop (A2C)\n",
    "\n",
    "Key design choices in this minimal implementation:\n",
    "\n",
    "- **Vectorized envs** (`n_envs`) to match A2C’s synchronous batching.\n",
    "- Rollout buffer of shape `(n_steps, n_envs, ...)`.\n",
    "- Compute **GAE + bootstrapped returns**.\n",
    "- Single gradient update per rollout (no replay buffer, no off-policy corrections).\n",
    "\n",
    "We also record:\n",
    "\n",
    "- episodic return (score) whenever any env finishes an episode\n",
    "- actor loss, critic loss, entropy, explained variance (optional diagnostic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    var_y = np.var(y_true)\n",
    "    if var_y < 1e-12:\n",
    "        return float('nan')\n",
    "    return 1.0 - float(np.var(y_true - y_pred) / var_y)\n",
    "\n",
    "\n",
    "def train_a2c(\n",
    "    env_id: str,\n",
    "    seed: int,\n",
    "    device: torch.device,\n",
    "    n_envs: int,\n",
    "    n_steps: int,\n",
    "    total_timesteps: int,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    "    ent_coef: float,\n",
    "    vf_coef: float,\n",
    "    lr: float,\n",
    "    max_grad_norm: float,\n",
    "    rmsprop_eps: float,\n",
    "    hidden_sizes: tuple[int, int],\n",
    "    normalize_advantage: bool,\n",
    "    log_every_updates: int = 50,\n",
    "    ):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = make_vec_env(env_id, n_envs, seed)\n",
    "    obs_space = env.single_observation_space\n",
    "    act_space = env.single_action_space\n",
    "\n",
    "    obs_dim = int(np.prod(obs_space.shape))\n",
    "    n_actions = int(act_space.n)\n",
    "\n",
    "    model = ActorCritic(obs_dim, n_actions, hidden_sizes=hidden_sizes).to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, eps=rmsprop_eps)\n",
    "\n",
    "    # Rollout buffers\n",
    "    obs_buf = torch.zeros((n_steps, n_envs, obs_dim), device=device, dtype=torch.float32)\n",
    "    act_buf = torch.zeros((n_steps, n_envs), device=device, dtype=torch.int64)\n",
    "    rew_buf = torch.zeros((n_steps, n_envs), device=device, dtype=torch.float32)\n",
    "    done_buf = torch.zeros((n_steps, n_envs), device=device, dtype=torch.float32)\n",
    "    val_buf = torch.zeros((n_steps, n_envs), device=device, dtype=torch.float32)\n",
    "\n",
    "    obs, _ = env.reset(seed=[seed + i for i in range(n_envs)])\n",
    "\n",
    "    # Episode tracking across vector envs\n",
    "    ep_returns_running = np.zeros((n_envs,), dtype=np.float32)\n",
    "    ep_lengths_running = np.zeros((n_envs,), dtype=np.int32)\n",
    "    ep_returns: list[float] = []\n",
    "    ep_lengths: list[int] = []\n",
    "\n",
    "    updates = total_timesteps // (n_envs * n_steps)\n",
    "    history_updates: list[dict] = []\n",
    "    last_adv_flat = None\n",
    "\n",
    "    t0 = time.time()\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    for update in range(1, updates + 1):\n",
    "        # --- Collect rollout ---\n",
    "        for t in range(n_steps):\n",
    "            obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "            obs_buf[t] = obs_t\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, values = model(obs_t)\n",
    "                actions, _, _ = sample_actions_and_logp(logits)\n",
    "\n",
    "            act_buf[t] = actions\n",
    "            val_buf[t] = values\n",
    "\n",
    "            next_obs, rewards, terminated, truncated, _ = env.step(actions.cpu().numpy())\n",
    "            dones = np.logical_or(terminated, truncated)\n",
    "\n",
    "            rew_buf[t] = torch.as_tensor(rewards, dtype=torch.float32, device=device)\n",
    "            done_buf[t] = torch.as_tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Episode bookkeeping\n",
    "            ep_returns_running += rewards\n",
    "            ep_lengths_running += 1\n",
    "            for i in np.where(dones)[0]:\n",
    "                ep_returns.append(float(ep_returns_running[i]))\n",
    "                ep_lengths.append(int(ep_lengths_running[i]))\n",
    "                ep_returns_running[i] = 0.0\n",
    "                ep_lengths_running[i] = 0\n",
    "\n",
    "            obs = next_obs\n",
    "            global_step += n_envs\n",
    "\n",
    "        # Bootstrap value from last observation\n",
    "        with torch.no_grad():\n",
    "            obs_last = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "            _, last_values = model(obs_last)  # (N,)\n",
    "\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards=rew_buf,\n",
    "            dones=done_buf,\n",
    "            values=val_buf,\n",
    "            last_values=last_values,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "        )\n",
    "\n",
    "        # Flatten (T, N, ...) -> (T*N, ...)\n",
    "        b_obs = obs_buf.reshape(-1, obs_dim)\n",
    "        b_act = act_buf.reshape(-1)\n",
    "        b_adv = advantages.reshape(-1)\n",
    "        b_ret = returns.reshape(-1)\n",
    "\n",
    "        if normalize_advantage:\n",
    "            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
    "\n",
    "        # --- Compute losses ---\n",
    "        logits, values_pred = model(b_obs)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        probs = log_probs.exp()\n",
    "\n",
    "        b_logp = log_probs.gather(1, b_act.unsqueeze(1)).squeeze(1)\n",
    "        entropy = -(probs * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "        actor_loss = -(b_logp * b_adv.detach()).mean()\n",
    "        critic_loss = 0.5 * F.mse_loss(values_pred, b_ret.detach())\n",
    "\n",
    "        loss = actor_loss + vf_coef * critic_loss - ent_coef * entropy\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        last_adv_flat = b_adv.detach().cpu().numpy()\n",
    "\n",
    "        # Diagnostics\n",
    "        y_true = b_ret.detach().cpu().numpy()\n",
    "        y_pred = values_pred.detach().cpu().numpy()\n",
    "\n",
    "        mean_ep_return = float(np.mean(ep_returns[-RETURN_SMOOTHING_WINDOW:])) if len(ep_returns) else float('nan')\n",
    "\n",
    "        history_updates.append(\n",
    "            dict(\n",
    "                update=update,\n",
    "                timesteps=global_step,\n",
    "                actor_loss=float(actor_loss.detach().cpu().item()),\n",
    "                critic_loss=float(critic_loss.detach().cpu().item()),\n",
    "                entropy=float(entropy.detach().cpu().item()),\n",
    "                explained_variance=explained_variance(y_true, y_pred),\n",
    "                episodes=len(ep_returns),\n",
    "                mean_return_window=mean_ep_return,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if update % log_every_updates == 0 or update == 1 or update == updates:\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"update {update:>4d}/{updates} | steps {global_step:>7d} | episodes {len(ep_returns):>5d} | \"\n",
    "                f\"mean_return@{RETURN_SMOOTHING_WINDOW} {mean_ep_return:>7.1f} | \"\n",
    "                f\"loss {float(loss.detach().cpu()):>8.4f} | {elapsed:>6.1f}s\"\n",
    "            )\n",
    "\n",
    "    env.close()\n",
    "    hist_df = pd.DataFrame(history_updates)\n",
    "    return model, hist_df, np.array(ep_returns, dtype=np.float32), np.array(ep_lengths, dtype=np.int32), last_adv_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, hist_df, ep_returns, ep_lengths, last_adv_flat = train_a2c(\n",
    "    env_id=ENV_ID,\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    "    n_envs=N_ENVS,\n",
    "    n_steps=N_STEPS,\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    gamma=GAMMA,\n",
    "    gae_lambda=GAE_LAMBDA,\n",
    "    ent_coef=ENT_COEF,\n",
    "    vf_coef=VF_COEF,\n",
    "    lr=LR,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    rmsprop_eps=RMSPROP_EPS,\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    normalize_advantage=NORMALIZE_ADVANTAGE,\n",
    "    log_every_updates=LOG_EVERY_UPDATES,\n",
    ")\n",
    "\n",
    "hist_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f02b22",
   "metadata": {},
   "source": [
    "## 6) Plot: score (return) per episode\n",
    "\n",
    "CartPole gives reward $+1$ per time step, so **episode return = episode length** (up to 500).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1, len(ep_returns) + 1)\n",
    "\n",
    "roll_mean = pd.Series(ep_returns).rolling(RETURN_SMOOTHING_WINDOW).mean().to_numpy()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=episodes, y=ep_returns, mode='lines', name='return', line=dict(width=1)))\n",
    "fig.add_trace(go.Scatter(x=episodes, y=roll_mean, mode='lines', name=f'mean@{RETURN_SMOOTHING_WINDOW}', line=dict(width=3)))\n",
    "fig.update_layout(\n",
    "    title='A2C on CartPole-v1 — score (return) per episode',\n",
    "    xaxis_title='episode',\n",
    "    yaxis_title='return',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e567b5a",
   "metadata": {},
   "source": [
    "## 7) Plot: training diagnostics (losses, entropy, explained variance)\n",
    "\n",
    "- **Actor loss** becomes more negative when advantages are consistently positive for sampled actions.\n",
    "- **Critic loss** should generally decrease as the value function fits the returns.\n",
    "- **Entropy** typically decreases as the policy becomes more confident.\n",
    "- **Explained variance** (rough critic diagnostic) near 1 is good; near 0 means the critic explains little.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f84af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Actor loss\", \"Critic loss\", \"Entropy\", \"Explained variance\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=hist_df['timesteps'], y=hist_df['actor_loss'], name='actor_loss'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=hist_df['timesteps'], y=hist_df['critic_loss'], name='critic_loss'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=hist_df['timesteps'], y=hist_df['entropy'], name='entropy'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=hist_df['timesteps'], y=hist_df['explained_variance'], name='explained_variance'), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=700, title='A2C training diagnostics', showlegend=False)\n",
    "fig.update_xaxes(title_text='timesteps')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda318d",
   "metadata": {},
   "source": [
    "## 8) Plot: advantage distribution (last update)\n",
    "\n",
    "A2C pushes up the log-probability of actions with **positive advantage** and pushes down those with **negative advantage**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    x=last_adv_flat,\n",
    "    nbins=60,\n",
    "    title='Advantage histogram (last update)',\n",
    ")\n",
    "fig.update_layout(xaxis_title='advantage', yaxis_title='count')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2183e0",
   "metadata": {},
   "source": [
    "## 9) Visualize the learned policy + value function (2D slice)\n",
    "\n",
    "CartPole states are 4D:\n",
    "\n",
    "$$\n",
    "s = (x, \\dot{x}, \\theta, \\dot{\\theta}).\n",
    "$$\n",
    "\n",
    "To visualize something, we take a **2D slice** over pole angle $\\theta$ and pole angular velocity $\\dot{\\theta}$, while fixing $x=0$ and $\\dot{x}=0$.\n",
    "\n",
    "- Left plot: $\\pi(a=1\\mid s)$ (probability of pushing right)\n",
    "- Right plot: $V(s)$ (critic estimate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def policy_value_slice(model: nn.Module, device: torch.device, grid_n: int = 70):\n",
    "    model.eval()\n",
    "    angles = np.linspace(-0.21, 0.21, grid_n)  # roughly CartPole angle limits\n",
    "    ang_vels = np.linspace(-3.0, 3.0, grid_n)\n",
    "\n",
    "    theta, theta_dot = np.meshgrid(angles, ang_vels)\n",
    "    states = np.zeros((grid_n * grid_n, 4), dtype=np.float32)\n",
    "    states[:, 2] = theta.ravel()\n",
    "    states[:, 3] = theta_dot.ravel()\n",
    "\n",
    "    obs_t = torch.as_tensor(states, dtype=torch.float32, device=device)\n",
    "    logits, values = model(obs_t)\n",
    "    probs = policy_action_probs(logits)\n",
    "    p_right = probs[:, 1].reshape(grid_n, grid_n).cpu().numpy()\n",
    "    v = values.reshape(grid_n, grid_n).cpu().numpy()\n",
    "\n",
    "    return angles, ang_vels, p_right, v\n",
    "\n",
    "\n",
    "angles, ang_vels, p_right, v = policy_value_slice(model, DEVICE)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Policy: P(push right)\", \"Critic: V(s)\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=angles,\n",
    "        y=ang_vels,\n",
    "        z=p_right,\n",
    "        colorscale='RdBu',\n",
    "        zmin=0.0,\n",
    "        zmax=1.0,\n",
    "        colorbar=dict(title='P(right)'),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=angles,\n",
    "        y=ang_vels,\n",
    "        z=v,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='V(s)'),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=420,\n",
    "    title='Learned policy/value on a 2D state slice (x=0, xdot=0)',\n",
    ")\n",
    "fig.update_xaxes(title_text='pole angle θ', row=1, col=1)\n",
    "fig.update_yaxes(title_text='pole angular velocity θdot', row=1, col=1)\n",
    "fig.update_xaxes(title_text='pole angle θ', row=1, col=2)\n",
    "fig.update_yaxes(title_text='pole angular velocity θdot', row=1, col=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594e8ed9",
   "metadata": {},
   "source": [
    "## 10) Quick evaluation (deterministic actions)\n",
    "\n",
    "We evaluate by taking the greedy action $\\arg\\max_a \\pi(a\\mid s)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c483ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, env_id: str, n_episodes: int = 10, seed: int = 0):\n",
    "    env = gym.make(env_id)\n",
    "    returns = []\n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        done = False\n",
    "        ret = 0.0\n",
    "        while not done:\n",
    "            obs_t = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            logits, _ = model(obs_t)\n",
    "            action = int(torch.argmax(logits, dim=-1).item())\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = bool(terminated or truncated)\n",
    "            ret += float(reward)\n",
    "        returns.append(ret)\n",
    "    env.close()\n",
    "    return np.array(returns, dtype=np.float32)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "eval_returns = evaluate(model, ENV_ID, n_episodes=10, seed=SEED + 1000)\n",
    "print('eval returns:', eval_returns)\n",
    "print('mean ± std:', float(eval_returns.mean()), '±', float(eval_returns.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae199b43",
   "metadata": {},
   "source": [
    "## 11) Pitfalls + diagnostics\n",
    "\n",
    "- **On-policy constraint**: A2C uses data from the *current* policy. If you reuse old experience without correction, it becomes biased.\n",
    "- **Done handling**: You must stop bootstrapping across episode boundaries. Here we treat `terminated OR truncated` as terminal for simplicity.\n",
    "- **Entropy coefficient**: Too high keeps the policy random; too low can collapse exploration early.\n",
    "- **Critic collapse**: If the critic is too weak/strong relative to the actor, learning can become unstable.\n",
    "- **Parallel envs matter**: With too few envs you get higher-variance updates.\n",
    "\n",
    "Good quick checks:\n",
    "\n",
    "- returns increase over time\n",
    "- entropy decreases gradually (not instantly)\n",
    "- critic loss decreases and explained variance improves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfd2d5",
   "metadata": {},
   "source": [
    "## 12) Exercises\n",
    "\n",
    "1. Change $\\lambda$ in GAE (e.g. 0.9) and compare learning curves.\n",
    "2. Swap RMSprop for Adam and compare stability.\n",
    "3. Implement **continuous actions** by outputting a Gaussian policy (mean + log-std) and testing on `Pendulum-v1`.\n",
    "4. Add a learning-rate schedule.\n",
    "5. Add observation normalization and compare speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812dacf",
   "metadata": {},
   "source": [
    "## 13) Stable-Baselines3 A2C reference implementation (web research)\n",
    "\n",
    "Stable-Baselines3 (SB3) includes an A2C implementation.\n",
    "\n",
    "- Docs page: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html\n",
    "\n",
    "### Minimal usage\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import A2C\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = A2C(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=7e-4,\n",
    "    n_steps=5,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    rms_prop_eps=1e-5,\n",
    "    use_rms_prop=True,\n",
    "    normalize_advantage=False,\n",
    ")\n",
    "model.learn(total_timesteps=200_000)\n",
    "```\n",
    "\n",
    "### SB3 A2C hyperparameters (signature + meaning)\n",
    "\n",
    "From the SB3 docs, the constructor signature is:\n",
    "\n",
    "```\n",
    "A2C(policy, env, learning_rate=0.0007, n_steps=5, gamma=0.99, gae_lambda=1.0,\n",
    "    ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, rms_prop_eps=1e-05,\n",
    "    use_rms_prop=True, use_sde=False, sde_sample_freq=-1,\n",
    "    rollout_buffer_class=None, rollout_buffer_kwargs=None,\n",
    "    normalize_advantage=False, stats_window_size=100, tensorboard_log=None,\n",
    "    policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True)\n",
    "```\n",
    "\n",
    "Parameter meanings (SB3 docs):\n",
    "\n",
    "- `policy`: policy class (e.g. `MlpPolicy`, `CnnPolicy`)\n",
    "- `env`: environment (Gym env, VecEnv, or registered env id string)\n",
    "- `learning_rate`: float or schedule\n",
    "- `n_steps`: rollout length per env (batch size = `n_steps * n_env`)\n",
    "- `gamma`: discount factor\n",
    "- `gae_lambda`: bias/variance trade-off for GAE; `1.0` equals classic advantage\n",
    "- `ent_coef`: entropy coefficient\n",
    "- `vf_coef`: value loss coefficient\n",
    "- `max_grad_norm`: gradient clipping threshold\n",
    "- `rms_prop_eps`: RMSprop epsilon\n",
    "- `use_rms_prop`: use RMSprop (default) vs Adam\n",
    "- `use_sde`: generalized State Dependent Exploration (gSDE)\n",
    "- `sde_sample_freq`: resample gSDE noise every n steps (`-1` = only at rollout start)\n",
    "- `rollout_buffer_class`: custom rollout buffer class\n",
    "- `rollout_buffer_kwargs`: kwargs for rollout buffer\n",
    "- `normalize_advantage`: normalize advantages\n",
    "- `stats_window_size`: episodes window for logging averages\n",
    "- `tensorboard_log`: tensorboard log dir\n",
    "- `policy_kwargs`: kwargs for policy network/architecture\n",
    "- `verbose`: verbosity level\n",
    "- `seed`: random seed\n",
    "- `device`: `cpu`, `cuda`, or `auto`\n",
    "- `_init_setup_model`: build the network immediately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67176ade",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Mnih et al. (2016), *Asynchronous Methods for Deep Reinforcement Learning* (A3C)\n",
    "- Schulman et al. (2016), *High-Dimensional Continuous Control Using Generalized Advantage Estimation*\n",
    "- Stable-Baselines3 A2C docs: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}