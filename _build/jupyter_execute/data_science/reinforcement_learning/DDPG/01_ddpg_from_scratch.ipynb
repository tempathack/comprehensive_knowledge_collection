{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38638eff",
   "metadata": {},
   "source": [
    "# DDPG (Deep Deterministic Policy Gradient) — from scratch in PyTorch\n",
    "\n",
    "DDPG is an **off-policy actor–critic** algorithm for **continuous control**.\n",
    "\n",
    "This notebook implements DDPG at a **low level** in PyTorch:\n",
    "\n",
    "- replay buffer\n",
    "- actor + critic networks\n",
    "- target networks with **soft updates**\n",
    "- exploration noise for deterministic policies\n",
    "- a clean training loop + Plotly diagnostics\n",
    "\n",
    "We’ll train on a simple continuous environment (default: `Pendulum-v1`) and plot:\n",
    "\n",
    "- **score per episode** (learning curve)\n",
    "- **Q-values / TD targets** during learning\n",
    "- **policy evolution** on fixed probe states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf25b67",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain the **actor–critic** factorization in DDPG and what each network learns\n",
    "- write the **critic target** with target networks *precisely*\n",
    "- understand why DDPG needs (1) **experience replay** and (2) **target networks**\n",
    "- implement DDPG updates in **low-level PyTorch** (no RL libraries)\n",
    "- interpret common diagnostics: returns, losses, Q-values, and policy drift\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- basic PyTorch (`nn.Module`, optimizers)\n",
    "- Bellman equation / TD learning intuition\n",
    "- continuous action spaces (Box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747988a6",
   "metadata": {},
   "source": [
    "## 1) DDPG structure (actor–critic) and target networks\n",
    "\n",
    "### Actor (deterministic policy)\n",
    "\n",
    "The actor is a deterministic policy network:\n",
    "\n",
    "$$a = \\mu_\\theta(s)$$\n",
    "\n",
    "In practice we output a `tanh`-bounded action and then **scale** to match the environment’s action bounds.\n",
    "\n",
    "### Critic (action-value function)\n",
    "\n",
    "The critic estimates the Q-value for a state–action pair:\n",
    "\n",
    "$$Q_\\phi(s,a) \\approx Q^{\\mu}(s,a)$$\n",
    "\n",
    "### Target networks (the stabilizer)\n",
    "\n",
    "Bootstrapping makes the target depend on the current function approximators.\n",
    "To reduce moving-target instability, DDPG maintains slowly-updated copies:\n",
    "\n",
    "- target actor: $\\mu_{\\theta'}$\n",
    "- target critic: $Q_{\\phi'}$\n",
    "\n",
    "Soft-update them after each gradient step:\n",
    "\n",
    "$$\\theta' \\leftarrow \\tau\\,\\theta + (1-\\tau)\\,\\theta'$$\n",
    "$$\\phi' \\leftarrow \\tau\\,\\phi + (1-\\tau)\\,\\phi'$$\n",
    "\n",
    "### Critic target (precise)\n",
    "\n",
    "For a transition $(s,a,r,s',d)$ sampled from replay (where $d\\in\\{0,1\\}$ indicates terminal), the TD target is\n",
    "\n",
    "$$y = r + \\gamma(1-d)\\,Q_{\\phi'}\\big(s',\\mu_{\\theta'}(s')\\big)$$\n",
    "\n",
    "and we fit the critic via\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = \\mathbb{E}\\big[(Q_\\phi(s,a)-y)^2\\big].$$\n",
    "\n",
    "### Actor objective (deterministic policy gradient)\n",
    "\n",
    "The actor is trained to maximize the critic’s value under its actions:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{s\\sim\\mathcal{D}}\\big[Q_\\phi(s,\\mu_\\theta(s))\\big].$$\n",
    "\n",
    "In code we minimize the actor loss\n",
    "\n",
    "$$\\mathcal{L}_{actor}(\\theta) = -\\mathbb{E}\\big[Q_\\phi(s,\\mu_\\theta(s))\\big].$$\n",
    "\n",
    "The gradient is the deterministic policy gradient:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\nabla_a Q_\\phi(s,a)\\rvert_{a=\\mu_\\theta(s)}\\,\\nabla_\\theta \\mu_\\theta(s)\\right].$$\n",
    "\n",
    "PyTorch computes this automatically when we backprop through `Q(s, actor(s))`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c2ce3",
   "metadata": {},
   "source": [
    "## 2) Algorithm sketch (pseudocode)\n",
    "\n",
    "1. Initialize actor $\\mu_\\theta$, critic $Q_\\phi$\n",
    "2. Initialize target networks $\\mu_{\\theta'}\\leftarrow\\mu_\\theta$, $Q_{\\phi'}\\leftarrow Q_\\phi$\n",
    "3. Initialize replay buffer $\\mathcal{D}$\n",
    "4. For each environment step:\n",
    "   - act with exploration: $a=\\mu_\\theta(s)+\\epsilon$\n",
    "   - store $(s,a,r,s',d)$ in $\\mathcal{D}$\n",
    "   - sample minibatch from $\\mathcal{D}$\n",
    "   - critic: regress $Q_\\phi(s,a)$ to $y=r+\\gamma(1-d)Q_{\\phi'}(s',\\mu_{\\theta'}(s'))$\n",
    "   - actor: ascend $\\nabla_\\theta Q_\\phi(s,\\mu_\\theta(s))$\n",
    "   - soft update targets: $(\\theta',\\phi')\\leftarrow \\tau(\\theta,\\phi)+(1-\\tau)(\\theta',\\phi')$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import platform\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "    _TORCH_IMPORT_ERROR = e\n",
    "\n",
    "# Gymnasium first; fall back to gym\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    GYM_BACKEND = 'gymnasium'\n",
    "except Exception:\n",
    "    import gym\n",
    "    GYM_BACKEND = 'gym'\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print('Python', platform.python_version())\n",
    "print('NumPy', np.__version__)\n",
    "print('Pandas', pd.__version__)\n",
    "print('Plotly', plotly.__version__)\n",
    "print('Gym backend', GYM_BACKEND, 'version', gym.__version__)\n",
    "print('Torch', torch.__version__ if TORCH_AVAILABLE else _TORCH_IMPORT_ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840639ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run configuration ---\n",
    "FAST_RUN = True  # set False for longer training\n",
    "\n",
    "ENV_ID = 'Pendulum-v1'\n",
    "SEED = 42\n",
    "\n",
    "NUM_EPISODES = 40 if FAST_RUN else 250\n",
    "MAX_STEPS_PER_EPISODE = None  # None means use env default\n",
    "\n",
    "REPLAY_SIZE = 200_000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "\n",
    "ACTOR_LR = 1e-3\n",
    "CRITIC_LR = 1e-3\n",
    "\n",
    "START_STEPS = 2_000  # random actions before using the actor + noise\n",
    "UPDATE_AFTER = 1_000  # start gradient updates after this many steps\n",
    "UPDATES_PER_STEP = 1\n",
    "\n",
    "NOISE_SIGMA = 0.1  # exploration noise std (in action units after scaling)\n",
    "\n",
    "HIDDEN_SIZES = (256, 256)\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "PROBE_N = 32\n",
    "PROBE_EVERY_EPISODES = 5\n",
    "\n",
    "DEVICE = 'cuda' if TORCH_AVAILABLE and torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f00b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    if TORCH_AVAILABLE:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def env_reset(env, seed: int | None = None):\n",
    "    out = env.reset(seed=seed) if seed is not None else env.reset()\n",
    "    if isinstance(out, tuple):\n",
    "        obs, info = out\n",
    "    else:\n",
    "        obs, info = out, {}\n",
    "    return obs, info\n",
    "\n",
    "\n",
    "def env_step(env, action):\n",
    "    out = env.step(action)\n",
    "    if len(out) == 5:\n",
    "        next_obs, reward, terminated, truncated, info = out\n",
    "        done = bool(terminated or truncated)\n",
    "    else:\n",
    "        next_obs, reward, done, info = out\n",
    "        done = bool(done)\n",
    "    return next_obs, float(reward), done, info\n",
    "\n",
    "\n",
    "def make_env(env_id: str, seed: int):\n",
    "    env = gym.make(env_id)\n",
    "    _ = env_reset(env, seed=seed)\n",
    "    try:\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return env\n",
    "\n",
    "\n",
    "def action_scale_and_bias(action_space):\n",
    "    # Works for gymnasium.spaces.Box and gym.spaces.Box\n",
    "    high = np.asarray(action_space.high, dtype=np.float32)\n",
    "    low = np.asarray(action_space.low, dtype=np.float32)\n",
    "    scale = (high - low) / 2.0\n",
    "    bias = (high + low) / 2.0\n",
    "    return scale, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim: int, act_dim: int, size: int, seed: int):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "        self.done_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "\n",
    "        self.max_size = int(size)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def add(self, obs, act, rew: float, next_obs, done: bool):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr] = float(done)\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        idx = self.rng.integers(0, self.size, size=batch_size)\n",
    "        batch = dict(\n",
    "            obs=self.obs_buf[idx],\n",
    "            act=self.act_buf[idx],\n",
    "            rew=self.rew_buf[idx],\n",
    "            next_obs=self.next_obs_buf[idx],\n",
    "            done=self.done_buf[idx],\n",
    "        )\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f39dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        act = activation if i < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[i], sizes[i + 1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes, action_scale, action_bias):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim, *hidden_sizes, act_dim], activation=nn.ReLU, output_activation=nn.Tanh)\n",
    "        self.register_buffer('action_scale', torch.as_tensor(action_scale, dtype=torch.float32))\n",
    "        self.register_buffer('action_bias', torch.as_tensor(action_bias, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        a = self.net(obs)\n",
    "        return self.action_scale * a + self.action_bias\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim + act_dim, *hidden_sizes, 1], activation=nn.ReLU, output_activation=nn.Identity)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eccabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DDPGConfig:\n",
    "    gamma: float = GAMMA\n",
    "    tau: float = TAU\n",
    "    actor_lr: float = ACTOR_LR\n",
    "    critic_lr: float = CRITIC_LR\n",
    "    batch_size: int = BATCH_SIZE\n",
    "    grad_clip_norm: float | None = GRAD_CLIP_NORM\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, obs_dim: int, act_dim: int, action_scale, action_bias, hidden_sizes, device: str, cfg: DDPGConfig):\n",
    "        self.device = torch.device(device)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.actor = Actor(obs_dim, act_dim, hidden_sizes, action_scale, action_bias).to(self.device)\n",
    "        self.critic = Critic(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "\n",
    "        # Target networks start as exact copies\n",
    "        self.target_actor = Actor(obs_dim, act_dim, hidden_sizes, action_scale, action_bias).to(self.device)\n",
    "        self.target_critic = Critic(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=cfg.actor_lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=cfg.critic_lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, obs: np.ndarray, noise_sigma: float = 0.0):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        action = self.actor(obs_t).cpu().numpy().squeeze(0)\n",
    "        if noise_sigma > 0:\n",
    "            action = action + np.random.normal(0.0, noise_sigma, size=action.shape).astype(np.float32)\n",
    "        return action\n",
    "\n",
    "    def update(self, batch):\n",
    "        obs = torch.as_tensor(batch['obs'], dtype=torch.float32, device=self.device)\n",
    "        act = torch.as_tensor(batch['act'], dtype=torch.float32, device=self.device)\n",
    "        rew = torch.as_tensor(batch['rew'], dtype=torch.float32, device=self.device)\n",
    "        next_obs = torch.as_tensor(batch['next_obs'], dtype=torch.float32, device=self.device)\n",
    "        done = torch.as_tensor(batch['done'], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # --- Critic update ---\n",
    "        with torch.no_grad():\n",
    "            next_act = self.target_actor(next_obs)\n",
    "            target_q_next = self.target_critic(next_obs, next_act)\n",
    "            y = rew + self.cfg.gamma * (1.0 - done) * target_q_next\n",
    "\n",
    "        q = self.critic(obs, act)\n",
    "        critic_loss = F.mse_loss(q, y)\n",
    "\n",
    "        self.critic_opt.zero_grad(set_to_none=True)\n",
    "        critic_loss.backward()\n",
    "        if self.cfg.grad_clip_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.grad_clip_norm)\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        # --- Actor update ---\n",
    "        self.actor_opt.zero_grad(set_to_none=True)\n",
    "        actor_actions = self.actor(obs)\n",
    "        actor_loss = -self.critic(obs, actor_actions).mean()\n",
    "        actor_loss.backward()\n",
    "        if self.cfg.grad_clip_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.cfg.grad_clip_norm)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        # --- Soft update target networks ---\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "                p_targ.data.mul_(1.0 - self.cfg.tau)\n",
    "                p_targ.data.add_(self.cfg.tau * p.data)\n",
    "\n",
    "            for p, p_targ in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
    "                p_targ.data.mul_(1.0 - self.cfg.tau)\n",
    "                p_targ.data.add_(self.cfg.tau * p.data)\n",
    "\n",
    "        metrics = {\n",
    "            'critic_loss': float(critic_loss.item()),\n",
    "            'actor_loss': float(actor_loss.item()),\n",
    "            'q_mean': float(q.detach().mean().item()),\n",
    "            'y_mean': float(y.detach().mean().item()),\n",
    "        }\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, window: int):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if len(x) < window:\n",
    "        return x\n",
    "    kernel = np.ones(window) / window\n",
    "    return np.convolve(x, kernel, mode='valid')\n",
    "\n",
    "\n",
    "def train_ddpg(env_id: str, seed: int):\n",
    "    set_global_seeds(seed)\n",
    "    env = make_env(env_id, seed=seed)\n",
    "\n",
    "    obs_dim = int(np.prod(env.observation_space.shape))\n",
    "    act_dim = int(np.prod(env.action_space.shape))\n",
    "\n",
    "    act_scale, act_bias = action_scale_and_bias(env.action_space)\n",
    "\n",
    "    buf = ReplayBuffer(obs_dim, act_dim, size=REPLAY_SIZE, seed=seed)\n",
    "    agent = DDPGAgent(\n",
    "        obs_dim=obs_dim,\n",
    "        act_dim=act_dim,\n",
    "        action_scale=act_scale,\n",
    "        action_bias=act_bias,\n",
    "        hidden_sizes=HIDDEN_SIZES,\n",
    "        device=DEVICE,\n",
    "        cfg=DDPGConfig(),\n",
    "    )\n",
    "\n",
    "    max_steps = MAX_STEPS_PER_EPISODE or getattr(env, '_max_episode_steps', 200)\n",
    "\n",
    "    logs = {\n",
    "        'episode': [],\n",
    "        'episode_return': [],\n",
    "        'episode_length': [],\n",
    "        'global_step_end': [],\n",
    "        # per-update metrics\n",
    "        'update_step': [],\n",
    "        'actor_loss': [],\n",
    "        'critic_loss': [],\n",
    "        'q_mean': [],\n",
    "        'y_mean': [],\n",
    "        # probe snapshots\n",
    "        'probe_episode': [],\n",
    "        'probe_action_stat': [],\n",
    "        'probe_q': [],\n",
    "    }\n",
    "\n",
    "    probe_states = None\n",
    "\n",
    "    global_step = 0\n",
    "    update_step = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    for ep in range(1, NUM_EPISODES + 1):\n",
    "        obs, _ = env_reset(env, seed=seed + ep)\n",
    "        obs = np.asarray(obs, dtype=np.float32).reshape(-1)\n",
    "\n",
    "        ep_return = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            if global_step < START_STEPS:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.act(obs, noise_sigma=NOISE_SIGMA)\n",
    "\n",
    "            # clip to action bounds\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high).astype(np.float32)\n",
    "\n",
    "            next_obs, reward, done, _ = env_step(env, action)\n",
    "            next_obs = np.asarray(next_obs, dtype=np.float32).reshape(-1)\n",
    "\n",
    "            buf.add(obs, action, reward, next_obs, done)\n",
    "\n",
    "            obs = next_obs\n",
    "            ep_return += reward\n",
    "            ep_len += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # gradient updates\n",
    "            if global_step >= UPDATE_AFTER and buf.size >= BATCH_SIZE:\n",
    "                for _u in range(UPDATES_PER_STEP):\n",
    "                    batch = buf.sample(BATCH_SIZE)\n",
    "                    metrics = agent.update(batch)\n",
    "\n",
    "                    logs['update_step'].append(update_step)\n",
    "                    logs['actor_loss'].append(metrics['actor_loss'])\n",
    "                    logs['critic_loss'].append(metrics['critic_loss'])\n",
    "                    logs['q_mean'].append(metrics['q_mean'])\n",
    "                    logs['y_mean'].append(metrics['y_mean'])\n",
    "                    update_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        logs['episode'].append(ep)\n",
    "        logs['episode_return'].append(ep_return)\n",
    "        logs['episode_length'].append(ep_len)\n",
    "        logs['global_step_end'].append(global_step)\n",
    "\n",
    "        # Fix a set of probe states once replay has enough data\n",
    "        if probe_states is None and buf.size >= max(PROBE_N, BATCH_SIZE):\n",
    "            probe_states = buf.sample(PROBE_N)['obs']\n",
    "\n",
    "        # Snapshot policy + Q on probe states to visualize policy evolution\n",
    "        if probe_states is not None and (ep % PROBE_EVERY_EPISODES == 0 or ep == NUM_EPISODES):\n",
    "            with torch.no_grad():\n",
    "                ps = torch.as_tensor(probe_states, dtype=torch.float32, device=agent.device)\n",
    "                pa = agent.actor(ps).cpu().numpy()\n",
    "                pq = agent.critic(ps, agent.actor(ps)).cpu().numpy().reshape(-1)\n",
    "\n",
    "            if pa.shape[1] == 1:\n",
    "                policy_stat = pa[:, 0]  # 1D actions\n",
    "            else:\n",
    "                policy_stat = np.linalg.norm(pa, axis=1)  # multi-dim summary\n",
    "\n",
    "            logs['probe_episode'].append(ep)\n",
    "            logs['probe_action_stat'].append(policy_stat)\n",
    "            logs['probe_q'].append(pq)\n",
    "\n",
    "        if ep % 10 == 0 or ep == 1 or ep == NUM_EPISODES:\n",
    "            elapsed = time.time() - t0\n",
    "            print(f'Episode {ep:4d} | return {ep_return:8.1f} | len {ep_len:3d} | steps {global_step:6d} | elapsed {elapsed:6.1f}s')\n",
    "\n",
    "    env.close()\n",
    "    return logs\n",
    "\n",
    "\n",
    "logs = train_ddpg(ENV_ID, seed=SEED)\n",
    "print('Done. Episodes:', len(logs['episode']), 'Updates:', len(logs['update_step']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4e38e",
   "metadata": {},
   "source": [
    "## 3) Plotly diagnostics\n",
    "\n",
    "DDPG can *look like it’s learning* while the critic is quietly diverging, so we’ll monitor:\n",
    "\n",
    "- **episode return** (score)\n",
    "- **critic loss** and **actor loss**\n",
    "- **Q-values vs TD targets** (sanity check)\n",
    "- **policy evolution** on fixed probe states (is the policy drifting smoothly?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f63eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Learning curve: score per episode ---\n",
    "df_ep = pd.DataFrame({\n",
    "    'episode': logs['episode'],\n",
    "    'return': logs['episode_return'],\n",
    "    'length': logs['episode_length'],\n",
    "})\n",
    "\n",
    "ma_window = 10\n",
    "ma = moving_average(df_ep['return'].values, window=ma_window)\n",
    "ma_x = df_ep['episode'].values[ma_window - 1:]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_ep['episode'], y=df_ep['return'], mode='lines', name='Return'))\n",
    "if len(ma) == len(ma_x):\n",
    "    fig.add_trace(go.Scatter(x=ma_x, y=ma, mode='lines', name=f'Return (MA {ma_window})'))\n",
    "fig.update_layout(title='DDPG learning curve (score per episode)', xaxis_title='Episode', yaxis_title='Return')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7412702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q-values, TD targets, and losses over update steps ---\n",
    "df_up = pd.DataFrame({\n",
    "    'update_step': logs['update_step'],\n",
    "    'critic_loss': logs['critic_loss'],\n",
    "    'actor_loss': logs['actor_loss'],\n",
    "    'q_mean': logs['q_mean'],\n",
    "    'y_mean': logs['y_mean'],\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_up['update_step'], y=df_up['q_mean'], mode='lines', name='Q(s,a) mean'))\n",
    "fig.add_trace(go.Scatter(x=df_up['update_step'], y=df_up['y_mean'], mode='lines', name='TD target y mean'))\n",
    "fig.update_layout(title='Critic outputs vs TD targets (mean over minibatch)', xaxis_title='Update step', yaxis_title='Value')\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_up['update_step'], y=df_up['critic_loss'], mode='lines', name='Critic loss (MSE)'))\n",
    "fig.add_trace(go.Scatter(x=df_up['update_step'], y=df_up['actor_loss'], mode='lines', name='Actor loss (-Q)'))\n",
    "fig.update_layout(title='Actor/Critic losses', xaxis_title='Update step', yaxis_title='Loss')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12239be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Policy evolution on fixed probe states ---\n",
    "if len(logs['probe_episode']) > 0:\n",
    "    probe_eps = logs['probe_episode']\n",
    "    z_action = np.stack(logs['probe_action_stat'], axis=1)  # (PROBE_N, T)\n",
    "    z_q = np.stack(logs['probe_q'], axis=1)  # (PROBE_N, T)\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=z_action,\n",
    "        x=probe_eps,\n",
    "        y=list(range(z_action.shape[0])),\n",
    "        colorscale='RdBu',\n",
    "        zmid=0.0,\n",
    "        colorbar=dict(title='action (1D) or ||a||'),\n",
    "    ))\n",
    "    fig.update_layout(title='Policy evolution on fixed probe states', xaxis_title='Episode snapshot', yaxis_title='Probe state index')\n",
    "    fig.show()\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=z_q,\n",
    "        x=probe_eps,\n",
    "        y=list(range(z_q.shape[0])),\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Q(s, mu(s))'),\n",
    "    ))\n",
    "    fig.update_layout(title='Q-values on probe states (critic under current actor)', xaxis_title='Episode snapshot', yaxis_title='Probe state index')\n",
    "    fig.show()\n",
    "else:\n",
    "    print('No probe snapshots recorded (try increasing NUM_EPISODES or reducing PROBE_EVERY_EPISODES).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15c585",
   "metadata": {},
   "source": [
    "## 4) Stable-Baselines implementation (if you want a reference)\n",
    "\n",
    "If you want a battle-tested baseline, Stable-Baselines has DDPG implementations.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- `stable-baselines3` (PyTorch) and `stable-baselines` (TensorFlow) are different packages.\n",
    "- This repository’s environment may not have them installed; the code below is for reference.\n",
    "\n",
    "### Stable-Baselines3 (PyTorch)\n",
    "\n",
    "```python\n",
    "# pip install stable-baselines3\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "n_actions = env.action_space.shape[0]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG('MlpPolicy', env, action_noise=action_noise, verbose=1)\n",
    "model.learn(total_timesteps=200_000)\n",
    "```\n",
    "\n",
    "### Stable-Baselines (TensorFlow; older/archived)\n",
    "\n",
    "```python\n",
    "# pip install stable-baselines\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from stable_baselines import DDPG\n",
    "from stable_baselines.ddpg.policies import MlpPolicy\n",
    "from stable_baselines.common.noise import NormalActionNoise\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(MlpPolicy, env, action_noise=action_noise, verbose=1)\n",
    "model.learn(total_timesteps=200_000)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d8309",
   "metadata": {},
   "source": [
    "## 5) Pitfalls + diagnostics\n",
    "\n",
    "- **Exploration**: deterministic policies need explicit noise; too little noise → no learning.\n",
    "- **Q-value blow-up**: if $Q$ grows without bound, reduce learning rates, add gradient clipping, check reward scale.\n",
    "- **Action scaling**: always scale `tanh` outputs to environment bounds; otherwise the critic learns on invalid actions.\n",
    "- **Replay warm-up**: start updates only after enough diverse transitions exist.\n",
    "- **Overestimation bias**: DDPG can overestimate; TD3 addresses this with twin critics + target smoothing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a63eb",
   "metadata": {},
   "source": [
    "## 6) Hyperparameters explained (the ones that matter)\n",
    "\n",
    "### `GAMMA` ($\\gamma$)\n",
    "Discount factor in the TD target:\n",
    "\n",
    "$$y=r+\\gamma(1-d)Q_{\\phi'}(s',\\mu_{\\theta'}(s')).$$\n",
    "\n",
    "- closer to 1 → longer-horizon credit assignment, but bootstrapping is harder\n",
    "- smaller → more myopic, often more stable\n",
    "\n",
    "### `TAU` ($\\tau$)\n",
    "Soft-update rate for target networks:\n",
    "\n",
    "$$\\theta'\\leftarrow\\tau\\theta+(1-\\tau)\\theta'.$$\n",
    "\n",
    "- smaller (e.g. 0.001) → targets change slowly (stable, but may learn slower)\n",
    "- larger (e.g. 0.02) → targets track faster (less bias, potentially less stable)\n",
    "\n",
    "### `REPLAY_SIZE`\n",
    "Maximum transitions stored.\n",
    "\n",
    "- too small → poor diversity, correlated samples\n",
    "- very large → more diversity but older data (off-policy mismatch) and more memory\n",
    "\n",
    "### `BATCH_SIZE`\n",
    "Minibatch size for gradient updates.\n",
    "\n",
    "- larger → smoother gradients, higher compute\n",
    "- smaller → noisier updates (can help exploration but can destabilize critic)\n",
    "\n",
    "### `START_STEPS`\n",
    "How long to act randomly before relying on the actor.\n",
    "\n",
    "- helps fill replay with diverse transitions\n",
    "- if too short, early actor updates overfit to narrow experience\n",
    "\n",
    "### `UPDATE_AFTER`\n",
    "Delay before starting gradient updates.\n",
    "\n",
    "- ensures the critic’s first targets aren’t based on tiny replay buffers\n",
    "\n",
    "### `UPDATES_PER_STEP`\n",
    "How many gradient updates to do per environment step.\n",
    "\n",
    "- `1` is the standard simple choice\n",
    "- larger values increase sample reuse but can overfit to replay and amplify instability\n",
    "\n",
    "### `NOISE_SIGMA`\n",
    "Exploration noise standard deviation (added to the actor’s action).\n",
    "\n",
    "- too small → agent may not discover better actions\n",
    "- too large → behavior becomes too random; critic targets get noisy\n",
    "\n",
    "### `HIDDEN_SIZES`\n",
    "Network capacity for actor/critic.\n",
    "\n",
    "- bigger networks can fit complex Q-functions but may be harder to train\n",
    "\n",
    "### `GRAD_CLIP_NORM`\n",
    "Gradient norm clipping (optional).\n",
    "\n",
    "- helps prevent occasional exploding gradients in the critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f16d7f",
   "metadata": {},
   "source": [
    "## 7) Exercises + references\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Replace Gaussian exploration with **Ornstein–Uhlenbeck** noise and compare learning.\n",
    "2. Add **LayerNorm** to the actor/critic MLPs; does it stabilize training?\n",
    "3. Implement **TD3** changes (twin critics + target policy smoothing) and compare the Q-value diagnostics.\n",
    "\n",
    "### References\n",
    "\n",
    "- Lillicrap et al., *Continuous control with deep reinforcement learning* (DDPG): https://arxiv.org/abs/1509.02971\n",
    "- OpenAI Spinning Up (DDPG explanation + tips): https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "- Stable-Baselines (archived TF implementations): https://github.com/hill-a/stable-baselines\n",
    "- Stable-Baselines3 docs: https://stable-baselines3.readthedocs.io/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}