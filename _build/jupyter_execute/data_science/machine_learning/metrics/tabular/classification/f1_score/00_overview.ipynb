{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264def56",
   "metadata": {},
   "source": [
    "# F1 Score (`f1_score`)\n",
    "\n",
    "The **F1 score** (a.k.a. *F-measure*) summarizes performance on the **positive class** by combining:\n",
    "\n",
    "- **Precision**: *when we predict positive, how often are we correct?*\n",
    "- **Recall**: *of all actual positives, how many did we find?*\n",
    "\n",
    "It’s especially common when:\n",
    "- the positive class is **rare** (class imbalance)\n",
    "- **false positives** and **false negatives** both matter (roughly equally)\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Derive the F1 formula from the confusion matrix.\n",
    "- Build a from-scratch NumPy implementation (binary + multiclass averages).\n",
    "- Visualize how **thresholding** changes precision, recall, and F1.\n",
    "- Use F1 to **tune** a simple logistic regression classifier.\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score as sk_f1_score\n",
    "from sklearn.metrics import precision_score as sk_precision_score\n",
    "from sklearn.metrics import recall_score as sk_recall_score\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import plotly\n",
    "\n",
    "print('numpy :', np.__version__)\n",
    "print('sklearn:', sklearn.__version__)\n",
    "print('plotly:', plotly.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e7735",
   "metadata": {},
   "source": [
    "## 1) Confusion matrix → precision, recall\n",
    "\n",
    "Assume **binary** classification:\n",
    "\n",
    "- true label: $y \\in \\{0, 1\\}$ (1 = *positive*)\n",
    "- predicted label: $\\hat{y} \\in \\{0, 1\\}$\n",
    "\n",
    "The confusion matrix counts:\n",
    "\n",
    "|            | $\\hat{y}=1$ | $\\hat{y}=0$ |\n",
    "|------------|-------------|-------------|\n",
    "| $y=1$      | TP          | FN          |\n",
    "| $y=0$      | FP          | TN          |\n",
    "\n",
    "From these:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}},\\qquad\n",
    "\\text{recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\n",
    "$$\n",
    "\n",
    "- Precision asks: *how noisy are our positive predictions?*\n",
    "- Recall asks: *how many positives did we miss?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f40825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny example\n",
    "y_true = np.array([1, 1, 1, 0, 0, 0, 0, 0])\n",
    "y_pred = np.array([1, 0, 1, 1, 0, 0, 0, 0])\n",
    "\n",
    "tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "\n",
    "tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa3faf",
   "metadata": {},
   "source": [
    "## 2) The F1 score\n",
    "\n",
    "The **F1 score** is the **harmonic mean** of precision and recall:\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}\n",
    "= \\frac{2\\text{precision}\\,\\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "$$\n",
    "\n",
    "Substituting the confusion-matrix definitions gives a very useful form:\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2\\,\\mathrm{TP}}{2\\,\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}}\n",
    "$$\n",
    "\n",
    "Key intuition:\n",
    "- **Harmonic mean punishes imbalance**: if precision is high but recall is near zero (or vice versa), $F_1$ is near zero.\n",
    "- **True negatives do not appear** in the formula. That’s great when negatives are abundant (imbalance), but it can also hide poor performance on the negative class.\n",
    "\n",
    "A generalization is the $F_\\beta$ score:\n",
    "\n",
    "$$\n",
    "F_\\beta = (1+\\beta^2)\\,\\frac{\\text{precision}\\,\\text{recall}}{\\beta^2\\,\\text{precision}+\\text{recall}}\n",
    "$$\n",
    "\n",
    "- $\\beta>1$ emphasizes recall\n",
    "- $\\beta<1$ emphasizes precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90693567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonic mean vs arithmetic mean\n",
    "ps = np.linspace(0.001, 0.999, 400)\n",
    "r_fixed = 0.2\n",
    "\n",
    "f1 = 2 * ps * r_fixed / (ps + r_fixed)\n",
    "am = 0.5 * (ps + r_fixed)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ps, y=f1, mode='lines', name='F1 (harmonic mean)'))\n",
    "fig.add_trace(go.Scatter(x=ps, y=am, mode='lines', name='Arithmetic mean', line=dict(dash='dash')))\n",
    "fig.update_layout(\n",
    "    title='Same recall, changing precision: harmonic vs arithmetic mean',\n",
    "    xaxis_title='Precision',\n",
    "    yaxis_title='Score',\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='left', x=0),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ad946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 as a function of precision and recall\n",
    "precision_grid = np.linspace(0, 1, 201)\n",
    "recall_grid = np.linspace(0, 1, 201)\n",
    "P, R = np.meshgrid(precision_grid, recall_grid)\n",
    "\n",
    "den = P + R\n",
    "\n",
    "F1 = np.zeros_like(den, dtype=float)\n",
    "np.divide(2 * P * R, den, out=F1, where=den != 0)\n",
    "\n",
    "fig = px.imshow(\n",
    "    F1,\n",
    "    x=precision_grid,\n",
    "    y=recall_grid,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    labels={'x': 'Precision', 'y': 'Recall', 'color': 'F1'},\n",
    "    title='F1 surface (heatmap) over precision/recall',\n",
    ")\n",
    "fig.update_layout(coloraxis_colorbar=dict(tickformat='.2f'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a74af4",
   "metadata": {},
   "source": [
    "## 3) NumPy implementation (from scratch)\n",
    "\n",
    "Below is a minimal implementation that mirrors common `sklearn.metrics.f1_score` behavior:\n",
    "\n",
    "- **binary** F1 via confusion-matrix counts\n",
    "- safe handling of **zero division** (when there are no predicted positives, or no true positives)\n",
    "- multiclass averages: `macro`, `micro`, `weighted`\n",
    "\n",
    "Convention:\n",
    "\n",
    "- when a denominator is zero, we return `zero_division` (default `0.0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babf522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_1d(a):\n",
    "    a = np.asarray(a)\n",
    "    return a.ravel()\n",
    "\n",
    "\n",
    "def _safe_divide(num, den, zero_division=0.0):\n",
    "    num = np.asarray(num, dtype=float)\n",
    "    den = np.asarray(den, dtype=float)\n",
    "\n",
    "    out = np.full(np.broadcast(num, den).shape, float(zero_division), dtype=float)\n",
    "    np.divide(num, den, out=out, where=den != 0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def confusion_counts_binary(y_true, y_pred, *, pos_label=1):\n",
    "    y_true = _as_1d(y_true)\n",
    "    y_pred = _as_1d(y_pred)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true{y_true.shape} vs y_pred{y_pred.shape}\")\n",
    "\n",
    "    yt = y_true == pos_label\n",
    "    yp = y_pred == pos_label\n",
    "\n",
    "    tp = np.sum(yt & yp)\n",
    "    fp = np.sum(~yt & yp)\n",
    "    fn = np.sum(yt & ~yp)\n",
    "    tn = np.sum(~yt & ~yp)\n",
    "\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "\n",
    "def precision_recall_f1_from_counts(tp, fp, fn, *, zero_division=0.0):\n",
    "    precision = _safe_divide(tp, tp + fp, zero_division=zero_division)\n",
    "    recall = _safe_divide(tp, tp + fn, zero_division=zero_division)\n",
    "    f1 = _safe_divide(2 * tp, 2 * tp + fp + fn, zero_division=zero_division)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def f1_score_binary(y_true, y_pred, *, pos_label=1, zero_division=0.0):\n",
    "    tp, fp, fn, tn = confusion_counts_binary(y_true, y_pred, pos_label=pos_label)\n",
    "    _, _, f1 = precision_recall_f1_from_counts(tp, fp, fn, zero_division=zero_division)\n",
    "    return float(f1)\n",
    "\n",
    "\n",
    "def f1_score_multiclass(y_true, y_pred, *, labels=None, average='macro', zero_division=0.0):\n",
    "    '''Multiclass/single-label F1 via one-vs-rest counts.\n",
    "\n",
    "    average: {'macro','micro','weighted', None}\n",
    "    '''\n",
    "\n",
    "    y_true = _as_1d(y_true)\n",
    "    y_pred = _as_1d(y_pred)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true{y_true.shape} vs y_pred{y_pred.shape}\")\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    tps = []\n",
    "    fps = []\n",
    "    fns = []\n",
    "    supports = []\n",
    "\n",
    "    for lab in labels:\n",
    "        tp = np.sum((y_true == lab) & (y_pred == lab))\n",
    "        fp = np.sum((y_true != lab) & (y_pred == lab))\n",
    "        fn = np.sum((y_true == lab) & (y_pred != lab))\n",
    "\n",
    "        tps.append(tp)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        supports.append(np.sum(y_true == lab))\n",
    "\n",
    "    tps = np.asarray(tps)\n",
    "    fps = np.asarray(fps)\n",
    "    fns = np.asarray(fns)\n",
    "    supports = np.asarray(supports)\n",
    "\n",
    "    per_class_f1 = _safe_divide(2 * tps, 2 * tps + fps + fns, zero_division=zero_division)\n",
    "\n",
    "    if average is None:\n",
    "        return labels, per_class_f1\n",
    "\n",
    "    average = str(average).lower()\n",
    "    if average == 'macro':\n",
    "        return float(np.mean(per_class_f1))\n",
    "    if average == 'weighted':\n",
    "        w = _safe_divide(supports, supports.sum(), zero_division=0.0)\n",
    "        return float(np.sum(w * per_class_f1))\n",
    "    if average == 'micro':\n",
    "        tp = tps.sum()\n",
    "        fp = fps.sum()\n",
    "        fn = fns.sum()\n",
    "        return float(_safe_divide(2 * tp, 2 * tp + fp + fn, zero_division=zero_division))\n",
    "\n",
    "    raise ValueError(\"average must be one of: 'macro', 'micro', 'weighted', None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d15e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks vs sklearn\n",
    "y_true = rng.integers(0, 2, size=200)\n",
    "y_pred = rng.integers(0, 2, size=200)\n",
    "\n",
    "ours = f1_score_binary(y_true, y_pred)\n",
    "sk = sk_f1_score(y_true, y_pred, zero_division=0)\n",
    "print('binary  f1: ours=', ours, 'sklearn=', sk)\n",
    "\n",
    "y_true_mc = rng.integers(0, 3, size=300)\n",
    "y_pred_mc = rng.integers(0, 3, size=300)\n",
    "\n",
    "for avg in ['macro', 'micro', 'weighted']:\n",
    "    ours = f1_score_multiclass(y_true_mc, y_pred_mc, average=avg)\n",
    "    sk = sk_f1_score(y_true_mc, y_pred_mc, average=avg, zero_division=0)\n",
    "    print(f\"multiclass {avg:8s}: ours={ours:.6f} sklearn={sk:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c63af",
   "metadata": {},
   "source": [
    "## 4) Thresholding: why F1 depends on the decision rule\n",
    "\n",
    "Many classifiers output a **score** or a **probability** $\\hat{p}(y=1\\mid x)$.\n",
    "\n",
    "To produce hard labels we pick a threshold $t$:\n",
    "\n",
    "$$\n",
    "\\hat{y}(t) = \\mathbb{1}[\\hat{p} \\ge t]\n",
    "$$\n",
    "\n",
    "Changing $t$ changes FP/FN, therefore precision/recall, therefore F1.\n",
    "\n",
    "A common way to **use F1 for optimization** is to choose $t$ (and other hyperparameters) to maximize validation-set F1:\n",
    "\n",
    "$$\n",
    "t^* \\in \\arg\\max_{t\\in[0,1]} F_1\\bigl(y,\\ \\mathbb{1}[\\hat{p}\\ge t]\\bigr)\n",
    "$$\n",
    "\n",
    "This is practical because:\n",
    "- F1 is **not differentiable** in the model parameters (it jumps when a single point crosses the threshold)\n",
    "- but it’s easy to optimize over a 1D threshold via a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic imbalanced dataset (2D for visualization)\n",
    "X, y = make_classification(\n",
    "    n_samples=2500,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.9, 0.1],\n",
    "    class_sep=1.4,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=7\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=7\n",
    ")\n",
    "\n",
    "# Standardize using train statistics (low-level)\n",
    "mean_ = X_train.mean(axis=0)\n",
    "std_ = X_train.std(axis=0)\n",
    "std_ = np.where(std_ == 0, 1.0, std_)\n",
    "\n",
    "X_train_s = (X_train - mean_) / std_\n",
    "X_val_s = (X_val - mean_) / std_\n",
    "X_test_s = (X_test - mean_) / std_\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_train_s[:, 0],\n",
    "    y=X_train_s[:, 1],\n",
    "    color=y_train.astype(str),\n",
    "    opacity=0.7,\n",
    "    labels={'x': 'x1 (standardized)', 'y': 'x2 (standardized)', 'color': 'class'},\n",
    "    title='Training data (imbalanced)',\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print('class balance (train):', np.bincount(y_train) / y_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f644e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    out = np.empty_like(z)\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    ez = np.exp(z[~pos])\n",
    "    out[~pos] = ez / (1.0 + ez)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_loss_from_proba(y_true, p, eps=1e-15):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    p = np.clip(np.asarray(p, dtype=float), eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n",
    "\n",
    "\n",
    "def fit_logistic_regression_gd(\n",
    "    X,\n",
    "    y,\n",
    "    *,\n",
    "    lr=0.2,\n",
    "    max_iter=2000,\n",
    "    alpha=0.0,\n",
    "    tol=1e-8,\n",
    "):\n",
    "    '''Binary logistic regression with gradient descent + optional L2 penalty.'''\n",
    "\n",
    "    Xb = add_intercept(X)\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "\n",
    "    n, d = Xb.shape\n",
    "    w = np.zeros(d)\n",
    "    history = []\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        p = sigmoid(Xb @ w)\n",
    "        loss = log_loss_from_proba(y, p) + 0.5 * alpha * np.sum(w[1:] ** 2)\n",
    "        history.append(loss)\n",
    "\n",
    "        grad = (Xb.T @ (p - y)) / n\n",
    "        grad[1:] += alpha * w[1:]\n",
    "\n",
    "        w_new = w - lr * grad\n",
    "\n",
    "        if np.linalg.norm(w_new - w) < tol:\n",
    "            w = w_new\n",
    "            break\n",
    "        w = w_new\n",
    "\n",
    "    return w, np.asarray(history)\n",
    "\n",
    "\n",
    "def predict_proba_logreg(X, w):\n",
    "    Xb = add_intercept(X)\n",
    "    return sigmoid(Xb @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss_hist = fit_logistic_regression_gd(X_train_s, y_train, lr=0.2, max_iter=3000, alpha=0.05)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=loss_hist, mode='lines', name='train log-loss'))\n",
    "fig.update_layout(title='Training curve (log-loss)', xaxis_title='Iteration', yaxis_title='Log-loss')\n",
    "fig.show()\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27303bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1_at_thresholds(y_true, y_score, thresholds, *, zero_division=0.0):\n",
    "    y_true = np.asarray(y_true).astype(int).ravel()\n",
    "    y_score = np.asarray(y_score, dtype=float).ravel()\n",
    "    thresholds = np.asarray(thresholds, dtype=float)\n",
    "\n",
    "    y_true_pos = y_true == 1\n",
    "    pred_pos = y_score[:, None] >= thresholds[None, :]\n",
    "\n",
    "    tp = np.sum(pred_pos & y_true_pos[:, None], axis=0)\n",
    "    fp = np.sum(pred_pos & ~y_true_pos[:, None], axis=0)\n",
    "    fn = np.sum(~pred_pos & y_true_pos[:, None], axis=0)\n",
    "\n",
    "    precision = _safe_divide(tp, tp + fp, zero_division=zero_division)\n",
    "    recall = _safe_divide(tp, tp + fn, zero_division=zero_division)\n",
    "    f1 = _safe_divide(2 * tp, 2 * tp + fp + fn, zero_division=zero_division)\n",
    "\n",
    "    return precision, recall, f1, tp, fp, fn\n",
    "\n",
    "\n",
    "p_val = predict_proba_logreg(X_val_s, w)\n",
    "thresholds = np.linspace(0.0, 1.0, 401)\n",
    "\n",
    "prec_t, rec_t, f1_t, tp_t, fp_t, fn_t = precision_recall_f1_at_thresholds(\n",
    "    y_val, p_val, thresholds, zero_division=0.0\n",
    ")\n",
    "\n",
    "best_idx = int(np.argmax(f1_t))\n",
    "t_best = float(thresholds[best_idx])\n",
    "\n",
    "print('best threshold (val):', t_best)\n",
    "print('F1 at best threshold (val):', float(f1_t[best_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=prec_t, mode='lines', name='precision'))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=rec_t, mode='lines', name='recall'))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=f1_t, mode='lines', name='F1', line=dict(width=3)))\n",
    "\n",
    "fig.add_vline(x=t_best, line_width=2, line_dash='dash', line_color='black')\n",
    "fig.update_layout(\n",
    "    title='Precision / Recall / F1 vs threshold (validation set)',\n",
    "    xaxis_title='Threshold t',\n",
    "    yaxis_title='Score',\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='left', x=0),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03181400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_from_threshold(y_true, y_score, t):\n",
    "    y_pred = (np.asarray(y_score) >= t).astype(int)\n",
    "    tp, fp, fn, tn = confusion_counts_binary(y_true, y_pred, pos_label=1)\n",
    "    mat = np.array([[tn, fp], [fn, tp]])\n",
    "    return mat, (tp, fp, fn, tn)\n",
    "\n",
    "\n",
    "mat_05, counts_05 = confusion_matrix_from_threshold(y_val, p_val, 0.5)\n",
    "mat_best, counts_best = confusion_matrix_from_threshold(y_val, p_val, t_best)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        f't=0.50 (F1={f1_score_binary(y_val, (p_val>=0.5).astype(int)):.3f})',\n",
    "        f't={t_best:.2f} (F1={f1_score_binary(y_val, (p_val>=t_best).astype(int)):.3f})',\n",
    "    ),\n",
    ")\n",
    "\n",
    "for col, mat in enumerate([mat_05, mat_best], start=1):\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=mat,\n",
    "            x=['Pred 0', 'Pred 1'],\n",
    "            y=['True 0', 'True 1'],\n",
    "            text=mat,\n",
    "            texttemplate='%{text}',\n",
    "            colorscale='Blues',\n",
    "            showscale=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "fig.update_layout(title='Confusion matrices on validation set')\n",
    "fig.show()\n",
    "\n",
    "counts_05, counts_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve with iso-F1 lines\n",
    "# (each point corresponds to one threshold)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=rec_t, y=prec_t, mode='lines', name='PR curve'))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[rec_t[best_idx]],\n",
    "        y=[prec_t[best_idx]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, color='red'),\n",
    "        name=f'Best F1 (t={t_best:.2f})',\n",
    "    )\n",
    ")\n",
    "\n",
    "f_levels = [0.2, 0.4, 0.6, 0.8]\n",
    "p_line = np.linspace(0.001, 1.0, 400)\n",
    "for f in f_levels:\n",
    "    mask = p_line > (f / 2)\n",
    "    p = p_line[mask]\n",
    "    r = (f * p) / (2 * p - f)\n",
    "    r = np.clip(r, 0, 1)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=r,\n",
    "            y=p,\n",
    "            mode='lines',\n",
    "            line=dict(dash='dot', width=1),\n",
    "            name=f'F1={f}',\n",
    "            hoverinfo='skip',\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Precision–Recall curve (validation) with iso-F1 lines',\n",
    "    xaxis_title='Recall',\n",
    "    yaxis_title='Precision',\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the threshold changes the *linear* decision boundary\n",
    "# p = sigmoid(z) >= t  <=>  z >= log(t/(1-t))\n",
    "\n",
    "def boundary_line(w, t, x1):\n",
    "    z_thr = np.log(t / (1 - t))\n",
    "    if np.isclose(w[2], 0.0):\n",
    "        return None\n",
    "    x2 = (z_thr - w[0] - w[1] * x1) / w[2]\n",
    "    return x2\n",
    "\n",
    "\n",
    "x1 = np.linspace(X_train_s[:, 0].min() - 0.5, X_train_s[:, 0].max() + 0.5, 200)\n",
    "x2_05 = boundary_line(w, 0.5, x1)\n",
    "x2_best = boundary_line(w, t_best, x1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_train_s[:, 0],\n",
    "    y=X_train_s[:, 1],\n",
    "    color=y_train.astype(str),\n",
    "    opacity=0.6,\n",
    "    labels={'x': 'x1 (standardized)', 'y': 'x2 (standardized)', 'color': 'class'},\n",
    "    title='Logistic regression: threshold shifts the decision boundary',\n",
    ")\n",
    "\n",
    "if x2_05 is not None:\n",
    "    fig.add_trace(go.Scatter(x=x1, y=x2_05, mode='lines', name='t=0.50', line=dict(color='black')))\n",
    "if x2_best is not None:\n",
    "    fig.add_trace(go.Scatter(x=x1, y=x2_best, mode='lines', name=f't={t_best:.2f}', line=dict(color='red')))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da630b5a",
   "metadata": {},
   "source": [
    "### Evaluate on the test set\n",
    "\n",
    "We picked $t^*$ on the **validation** set to avoid overfitting the threshold.\n",
    "\n",
    "Now compare:\n",
    "\n",
    "- default $t=0.5$\n",
    "- tuned $t=t^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa097f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = predict_proba_logreg(X_test_s, w)\n",
    "\n",
    "def report_binary(y_true, p, t):\n",
    "    y_hat = (p >= t).astype(int)\n",
    "    tp, fp, fn, tn = confusion_counts_binary(y_true, y_hat)\n",
    "    prec, rec, f1 = precision_recall_f1_from_counts(tp, fp, fn)\n",
    "    return {\n",
    "        'threshold': float(t),\n",
    "        'precision': float(prec),\n",
    "        'recall': float(rec),\n",
    "        'f1': float(f1),\n",
    "        'tp': int(tp),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn),\n",
    "        'tn': int(tn),\n",
    "    }\n",
    "\n",
    "rep_05 = report_binary(y_test, p_test, 0.5)\n",
    "rep_best = report_binary(y_test, p_test, t_best)\n",
    "\n",
    "rep_05, rep_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29de70",
   "metadata": {},
   "source": [
    "## 5) Using F1 for model selection (simple “optimization” loop)\n",
    "\n",
    "F1 is typically used as a **selection criterion** rather than a differentiable training loss.\n",
    "\n",
    "Example: tune L2 strength $\\alpha$ for logistic regression by:\n",
    "\n",
    "1) fit the model for each $\\alpha$\n",
    "2) pick the threshold $t$ that maximizes validation F1\n",
    "3) choose the best $(\\alpha, t)$ pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0, 0.01, 0.05, 0.2, 1.0]\n",
    "thresholds = np.linspace(0.0, 1.0, 401)\n",
    "\n",
    "results = []\n",
    "for a in alphas:\n",
    "    w_a, _ = fit_logistic_regression_gd(X_train_s, y_train, lr=0.2, max_iter=3000, alpha=a)\n",
    "    p_val_a = predict_proba_logreg(X_val_s, w_a)\n",
    "\n",
    "    _, _, f1_a, _, _, _ = precision_recall_f1_at_thresholds(y_val, p_val_a, thresholds)\n",
    "    best_idx_a = int(np.argmax(f1_a))\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            'alpha': float(a),\n",
    "            't_best': float(thresholds[best_idx_a]),\n",
    "            'f1_val_best': float(f1_a[best_idx_a]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vals = np.array([r['alpha'] for r in results])\n",
    "f1_vals = np.array([r['f1_val_best'] for r in results])\n",
    "\n",
    "best = results[int(np.argmax(f1_vals))]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=alpha_vals, y=f1_vals, mode='lines+markers', name='best val F1'))\n",
    "fig.update_layout(\n",
    "    title='Validation F1 after threshold tuning vs L2 strength',\n",
    "    xaxis_title='alpha (L2 strength)',\n",
    "    yaxis_title='best validation F1',\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61450e",
   "metadata": {},
   "source": [
    "## 6) Multiclass F1: macro vs micro vs weighted\n",
    "\n",
    "For multiclass single-label classification, F1 is usually computed by turning each class into a one-vs-rest problem.\n",
    "\n",
    "- **macro**: average F1 across classes (treat each class equally)\n",
    "- **weighted**: average F1 across classes weighted by class support\n",
    "- **micro**: compute global TP/FP/FN across classes before computing F1\n",
    "\n",
    "Note: in *single-label* multiclass classification, **micro F1 equals accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc21629",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_mc = np.array([0, 0, 0, 1, 1, 2, 2, 2, 2])\n",
    "y_pred_mc = np.array([0, 2, 0, 1, 0, 2, 2, 1, 2])\n",
    "\n",
    "labels, per_class = f1_score_multiclass(y_true_mc, y_pred_mc, average=None)\n",
    "print('labels:', labels)\n",
    "print('per-class F1:', per_class)\n",
    "\n",
    "for avg in ['macro', 'micro', 'weighted']:\n",
    "    ours = f1_score_multiclass(y_true_mc, y_pred_mc, average=avg)\n",
    "    sk = sk_f1_score(y_true_mc, y_pred_mc, average=avg, zero_division=0)\n",
    "    print(f\"{avg:8s}: ours={ours:.6f} sklearn={sk:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9683e8",
   "metadata": {},
   "source": [
    "## Pros / cons and when to use F1\n",
    "\n",
    "**Pros**\n",
    "- Good default when the positive class is **rare** and you care about both FP and FN.\n",
    "- Single number that summarizes the precision–recall tradeoff.\n",
    "- Common in information retrieval, detection tasks, and many imbalanced classification settings.\n",
    "\n",
    "**Cons / limitations**\n",
    "- Ignores **true negatives**: can be misleading if performance on the negative class matters.\n",
    "- **Threshold-dependent**: you must pick a threshold (or compare across thresholds).\n",
    "- Not a **proper scoring rule** (unlike log-loss / Brier), so it’s not ideal for probability calibration.\n",
    "- Not differentiable in model parameters → usually not used as a direct training loss.\n",
    "- Can hide tradeoffs: the same F1 can come from very different (precision, recall) pairs.\n",
    "\n",
    "**Good use cases**\n",
    "- Highly imbalanced binary classification where the negative class is huge (fraud, churn, defect detection).\n",
    "- Search / ranking systems after choosing an operating point.\n",
    "- Segmentation / detection tasks (F1 is closely related to the Dice coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232dac0d",
   "metadata": {},
   "source": [
    "## Common pitfalls + diagnostics\n",
    "\n",
    "- **Undefined divisions**: if the model predicts no positives, precision is undefined. Decide a policy (`zero_division=0` is common).\n",
    "- **Wrong averaging** in multiclass: `macro` emphasizes minority classes; `weighted` tracks overall distribution.\n",
    "- **Class imbalance** doesn’t magically disappear: F1 helps compared to accuracy, but you still need proper validation and often threshold tuning.\n",
    "- If you need to compare models *as rankers*, prefer PR curves / average precision instead of a single F1 at one threshold.\n",
    "- If FP and FN have different costs, prefer $F_\\beta$ or an explicit cost-sensitive metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2745163",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Implement $F_\\beta$ in NumPy and verify it against `sklearn.metrics.fbeta_score`.\n",
    "2) For the logistic regression example, compare the threshold that maximizes F1 vs the threshold that maximizes accuracy.\n",
    "3) Create an extremely imbalanced dataset (e.g. 99.5% negatives) and compare accuracy vs F1.\n",
    "4) For multiclass, create a dataset with one rare class and compare `macro` vs `weighted` F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205cfde",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "- scikit-learn user guide (precision/recall/F-score): https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}