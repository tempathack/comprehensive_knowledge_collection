{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad8fe5a",
   "metadata": {},
   "source": [
    "# roc_auc_score (ROC AUC)\n",
    "\n",
    "`roc_auc_score` computes the **area under the ROC curve**. It evaluates how well a model **ranks** positive examples above negative examples, using *scores* (probabilities or decision values), not hard class labels.\n",
    "\n",
    "**You will learn**\n",
    "- How thresholds produce points on the ROC curve (TPR vs FPR)\n",
    "- Two equivalent AUC formulas: trapezoid area and Mann–Whitney (rank) view\n",
    "- A NumPy implementation of `roc_curve` + `roc_auc_score` (tie-safe)\n",
    "- How to optimize for AUC with a differentiable pairwise surrogate (NumPy)\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Binary classification labels (0/1)\n",
    "- Confusion matrix terms: TP / FP / TN / FN\n",
    "- Basic probability and calculus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfada83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score as skl_roc_auc_score\n",
    "from sklearn.metrics import roc_curve as skl_roc_curve\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9676d1",
   "metadata": {},
   "source": [
    "## 1) From scores to TPR/FPR (one threshold)\n",
    "\n",
    "Assume:\n",
    "- true labels: $y_i \\in \\{0,1\\}$\n",
    "- model scores (higher = more positive): $s_i \\in \\mathbb{R}$\n",
    "- threshold: $\\tau$\n",
    "\n",
    "We predict positive if:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i(\\tau)=\\mathbb{1}[s_i \\ge \\tau]\n",
    "$$\n",
    "\n",
    "From the confusion matrix at threshold $\\tau$:\n",
    "\n",
    "$$\n",
    "\\mathrm{TPR}(\\tau)=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}, \\qquad\n",
    "\\mathrm{FPR}(\\tau)=\\frac{\\mathrm{FP}}{\\mathrm{FP}+\\mathrm{TN}}\n",
    "$$\n",
    "\n",
    "- TPR = **recall / sensitivity**\n",
    "- FPR = **1 - specificity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b678fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_at_threshold(y_true, y_score, threshold, pos_label=1):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "    if y_true.shape[0] != y_score.shape[0]:\n",
    "        raise ValueError(\"y_true and y_score must have the same length.\")\n",
    "\n",
    "    pos = y_true == pos_label\n",
    "    pred_pos = y_score >= threshold\n",
    "\n",
    "    tp = np.sum(pos & pred_pos)\n",
    "    fp = np.sum(~pos & pred_pos)\n",
    "    fn = np.sum(pos & ~pred_pos)\n",
    "    tn = np.sum(~pos & ~pred_pos)\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "\n",
    "def tpr_fpr_from_confusion(tp, fp, tn, fn):\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "    return tpr, fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_small = np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0])\n",
    "y_score_small = np.array([0.95, 0.90, 0.80, 0.75, 0.60, 0.55, 0.52, 0.50, 0.40, 0.35, 0.30, 0.10])\n",
    "\n",
    "threshold = 0.50\n",
    "\n",
    "tp, fp, tn, fn = confusion_at_threshold(y_true_small, y_score_small, threshold=threshold)\n",
    "tpr, fpr = tpr_fpr_from_confusion(tp, fp, tn, fn)\n",
    "\n",
    "df_small = pd.DataFrame({\"y_true\": y_true_small, \"score\": y_score_small})\n",
    "df_small = df_small.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "df_small[f\"y_pred(score \\u2265 {threshold:.2f})\"] = (df_small[\"score\"] >= threshold).astype(int)\n",
    "\n",
    "df_small, {\"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn, \"TPR\": tpr, \"FPR\": fpr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa74cb",
   "metadata": {},
   "source": [
    "## 2) ROC curve: sweep the threshold\n",
    "\n",
    "The ROC curve plots $(\\mathrm{FPR}(\\tau), \\mathrm{TPR}(\\tau))$ as we move the threshold $\\tau$ from **very strict** to **very lenient**:\n",
    "\n",
    "- $\\tau = +\\infty$ ⇒ predict nothing positive ⇒ (FPR,TPR) = (0,0)\n",
    "- $\\tau$ decreases ⇒ more predicted positives ⇒ move up/right\n",
    "- $\\tau = -\\infty$ ⇒ predict everything positive ⇒ (1,1)\n",
    "\n",
    "A random ranking gives the diagonal line $\\mathrm{TPR} = \\mathrm{FPR}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_bruteforce(y_true, y_score, pos_label=1):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "\n",
    "    thresholds = np.r_[np.inf, np.sort(np.unique(y_score))[::-1]]\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "\n",
    "    for thr in thresholds:\n",
    "        tp, fp, tn, fn = confusion_at_threshold(y_true, y_score, thr, pos_label=pos_label)\n",
    "        tpr_i, fpr_i = tpr_fpr_from_confusion(tp, fp, tn, fn)\n",
    "        fpr.append(fpr_i)\n",
    "        tpr.append(tpr_i)\n",
    "\n",
    "    return np.asarray(fpr), np.asarray(tpr), thresholds\n",
    "\n",
    "\n",
    "fpr_b, tpr_b, thr_b = roc_curve_bruteforce(y_true_small, y_score_small)\n",
    "auc_b = np.trapz(tpr_b, fpr_b)\n",
    "\n",
    "df_roc_small = pd.DataFrame({\"threshold\": thr_b, \"fpr\": fpr_b, \"tpr\": tpr_b})\n",
    "df_roc_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_labels = [\"inf\" if np.isinf(t) else f\"{t:.2f}\" for t in thr_b]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"ROC curve (toy example)\", \"TPR/FPR vs threshold\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr_b,\n",
    "        y=tpr_b,\n",
    "        mode=\"lines+markers\",\n",
    "        name=f\"ROC (AUC={auc_b:.3f})\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"black\"),\n",
    "        name=\"random\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr_b,\n",
    "        y=tpr_b,\n",
    "        mode=\"markers+text\",\n",
    "        text=point_labels,\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(size=8),\n",
    "        name=\"thresholds\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "mask = np.isfinite(thr_b)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=thr_b[mask],\n",
    "        y=tpr_b[mask],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"TPR\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=thr_b[mask],\n",
    "        y=fpr_b[mask],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"FPR\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"FPR\", range=[0, 1], row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"TPR\", range=[0, 1], row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"threshold τ\", autorange=\"reversed\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"rate\", range=[0, 1], row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=950, height=430)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653554c9",
   "metadata": {},
   "source": [
    "## 3) AUC: \"area\" and \"probability of correct ranking\"\n",
    "\n",
    "The **ROC AUC** is the area under the ROC curve:\n",
    "\n",
    "$$\n",
    "\\mathrm{AUC} = \\int_0^1 \\mathrm{TPR}(u)\\,du\n",
    "$$\n",
    "\n",
    "where we integrate TPR as a function of FPR.\n",
    "\n",
    "A powerful equivalent view (binary case) is:\n",
    "\n",
    "$$\n",
    "\\mathrm{AUC} = \\mathbb{P}(s^+ > s^-) + \\frac{1}{2}\\mathbb{P}(s^+ = s^-)\n",
    "$$\n",
    "\n",
    "where $s^+$ is the score of a random positive example and $s^-$ is the score of a random negative example.\n",
    "\n",
    "So AUC is a **ranking metric**:\n",
    "- any strictly monotonic transform of the score (e.g. logits → probabilities) leaves AUC unchanged\n",
    "- AUC = 0.5 means random ranking, AUC = 1.0 means perfect ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores = y_score_small[y_true_small == 1]\n",
    "neg_scores = y_score_small[y_true_small == 0]\n",
    "\n",
    "auc_pairwise = (pos_scores[:, None] > neg_scores[None, :]).mean() + 0.5 * (\n",
    "    pos_scores[:, None] == neg_scores[None, :]\n",
    ").mean()\n",
    "\n",
    "auc_pairwise, auc_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pairs = 500\n",
    "pos_s = rng.choice(pos_scores, size=n_pairs, replace=True)\n",
    "neg_s = rng.choice(neg_scores, size=n_pairs, replace=True)\n",
    "\n",
    "df_pairs = pd.DataFrame({\"neg_score\": neg_s, \"pos_score\": pos_s})\n",
    "\n",
    "min_s = float(min(df_pairs[\"neg_score\"].min(), df_pairs[\"pos_score\"].min()))\n",
    "max_s = float(max(df_pairs[\"neg_score\"].max(), df_pairs[\"pos_score\"].max()))\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_pairs,\n",
    "    x=\"neg_score\",\n",
    "    y=\"pos_score\",\n",
    "    opacity=0.55,\n",
    "    title=(\n",
    "        \"Random positive/negative score pairs (above diagonal = correct ranking)\" f\"<br>AUC ≈ {auc_pairwise:.3f}\"\n",
    "    ),\n",
    ")\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=min_s,\n",
    "    y0=min_s,\n",
    "    x1=max_s,\n",
    "    y1=max_s,\n",
    "    line=dict(color=\"black\", dash=\"dash\"),\n",
    ")\n",
    "fig.update_xaxes(title=\"negative score s⁻\")\n",
    "fig.update_yaxes(title=\"positive score s⁺\")\n",
    "fig.update_layout(width=650, height=520)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81205767",
   "metadata": {},
   "source": [
    "## 4) NumPy implementation (ROC curve + ROC AUC)\n",
    "\n",
    "A direct implementation by scanning all thresholds can be $O(n^2)$.\n",
    "\n",
    "A faster approach:\n",
    "\n",
    "1. Sort examples by score (descending)\n",
    "2. Sweep the threshold from high to low\n",
    "3. Track cumulative TP and FP counts\n",
    "4. Record a ROC point only when the score changes (tie handling)\n",
    "\n",
    "This is $O(n \\log n)$ due to sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf61c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_np(y_true, y_score, pos_label=1):\n",
    "    \"\"\"Compute ROC curve points (FPR, TPR) for binary classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Binary labels. Anything equal to `pos_label` is treated as positive.\n",
    "    y_score : array-like of shape (n_samples,)\n",
    "        Scores where larger means \"more positive\".\n",
    "    pos_label : label (default=1)\n",
    "        Which label is considered positive.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "    if y_true.shape[0] != y_score.shape[0]:\n",
    "        raise ValueError(\"y_true and y_score must have the same length.\")\n",
    "\n",
    "    pos = y_true == pos_label\n",
    "    n_pos = int(pos.sum())\n",
    "    n_neg = int((~pos).sum())\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        raise ValueError(\"roc_curve is undefined with only one class present in y_true.\")\n",
    "\n",
    "    order = np.argsort(-y_score, kind=\"mergesort\")\n",
    "    y_score_sorted = y_score[order]\n",
    "    y_pos_sorted = pos[order].astype(int)\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(y_score_sorted))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_pos_sorted.size - 1]\n",
    "\n",
    "    tps = np.cumsum(y_pos_sorted)[threshold_idxs]\n",
    "    fps = 1 + threshold_idxs - tps\n",
    "\n",
    "    # Prepend the point at threshold +inf: (FPR,TPR) = (0,0)\n",
    "    tps = np.r_[0, tps]\n",
    "    fps = np.r_[0, fps]\n",
    "    thresholds = np.r_[np.inf, y_score_sorted[threshold_idxs]]\n",
    "\n",
    "    fpr = fps / n_neg\n",
    "    tpr = tps / n_pos\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "\n",
    "def roc_auc_score_np(y_true, y_score, pos_label=1):\n",
    "    fpr, tpr, _ = roc_curve_np(y_true, y_score, pos_label=pos_label)\n",
    "    return float(np.trapz(tpr, fpr))\n",
    "\n",
    "\n",
    "def rankdata_average_ties(x):\n",
    "    \"\"\"Ranks starting at 1, using average ranks for ties (NumPy-only).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    order = np.argsort(x, kind=\"mergesort\")\n",
    "    x_sorted = x[order]\n",
    "\n",
    "    ranks_sorted = np.empty_like(x_sorted, dtype=float)\n",
    "\n",
    "    n = len(x_sorted)\n",
    "    i = 0\n",
    "    rank = 1\n",
    "    while i < n:\n",
    "        j = i + 1\n",
    "        while j < n and x_sorted[j] == x_sorted[i]:\n",
    "            j += 1\n",
    "\n",
    "        # ranks for i..j-1 are rank..rank+(j-i)-1\n",
    "        avg_rank = 0.5 * (rank + (rank + (j - i) - 1))\n",
    "        ranks_sorted[i:j] = avg_rank\n",
    "\n",
    "        rank += j - i\n",
    "        i = j\n",
    "\n",
    "    ranks = np.empty_like(ranks_sorted)\n",
    "    ranks[order] = ranks_sorted\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def roc_auc_score_mann_whitney_np(y_true, y_score, pos_label=1):\n",
    "    \"\"\"AUC via Mann–Whitney U / Wilcoxon rank-sum (tie-safe).\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "    if y_true.shape[0] != y_score.shape[0]:\n",
    "        raise ValueError(\"y_true and y_score must have the same length.\")\n",
    "\n",
    "    pos = y_true == pos_label\n",
    "    n_pos = int(pos.sum())\n",
    "    n_neg = int((~pos).sum())\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        raise ValueError(\"roc_auc_score is undefined with only one class present in y_true.\")\n",
    "\n",
    "    ranks = rankdata_average_ties(y_score)\n",
    "    sum_ranks_pos = ranks[pos].sum()\n",
    "    u = sum_ranks_pos - n_pos * (n_pos + 1) / 2\n",
    "    return float(u / (n_pos * n_neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = rng.integers(0, 2, size=300)\n",
    "y_score = rng.normal(size=300)\n",
    "\n",
    "auc_np = roc_auc_score_np(y_true, y_score)\n",
    "auc_mw = roc_auc_score_mann_whitney_np(y_true, y_score)\n",
    "auc_skl = skl_roc_auc_score(y_true, y_score)\n",
    "\n",
    "auc_np, auc_mw, auc_skl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab76b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our curve matches sklearn when drop_intermediate=False (sklearn defaults to drop_intermediate=True)\n",
    "fpr_np, tpr_np, thr_np = roc_curve_np(y_true, y_score)\n",
    "fpr_skl, tpr_skl, thr_skl = skl_roc_curve(y_true, y_score, drop_intermediate=False)\n",
    "\n",
    "(\n",
    "    np.allclose(fpr_np, fpr_skl),\n",
    "    np.allclose(tpr_np, tpr_skl),\n",
    "    np.allclose(thr_np, thr_skl),\n",
    "    len(fpr_np),\n",
    "    len(skl_roc_curve(y_true, y_score)[0]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC is invariant to strictly monotonic transforms of the scores\n",
    "auc_logits = roc_auc_score_np(y_true, y_score)\n",
    "auc_prob = roc_auc_score_np(y_true, 1 / (1 + np.exp(-y_score)))\n",
    "\n",
    "auc_logits, auc_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb14d8",
   "metadata": {},
   "source": [
    "## 5) Visual intuition: distributions → thresholds → ROC points\n",
    "\n",
    "Below we draw score distributions for each class and place a few thresholds. Each threshold maps to a point on the ROC curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ead43",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos, n_neg = 250, 750\n",
    "scores_pos = rng.normal(loc=1.2, scale=1.0, size=n_pos)\n",
    "scores_neg = rng.normal(loc=0.0, scale=1.0, size=n_neg)\n",
    "\n",
    "y_true_big = np.r_[np.ones(n_pos, dtype=int), np.zeros(n_neg, dtype=int)]\n",
    "y_score_big = np.r_[scores_pos, scores_neg]\n",
    "\n",
    "perm = rng.permutation(len(y_true_big))\n",
    "y_true_big = y_true_big[perm]\n",
    "y_score_big = y_score_big[perm]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve_np(y_true_big, y_score_big)\n",
    "auc_val = roc_auc_score_np(y_true_big, y_score_big)\n",
    "\n",
    "thresholds_demo = np.quantile(y_score_big, [0.9, 0.5, 0.1])\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Score distributions\", f\"ROC curve (AUC={auc_val:.3f})\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_score_big[y_true_big == 0],\n",
    "        name=\"negative\",\n",
    "        opacity=0.6,\n",
    "        nbinsx=40,\n",
    "        marker_color=\"gray\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_score_big[y_true_big == 1],\n",
    "        name=\"positive\",\n",
    "        opacity=0.6,\n",
    "        nbinsx=40,\n",
    "        marker_color=\"crimson\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "for thr, c in zip(thresholds_demo, colors):\n",
    "    fig.add_vline(x=float(thr), line_dash=\"dash\", line_color=c, row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode=\"lines\", name=\"ROC\"), row=1, col=2)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"black\"),\n",
    "        name=\"random\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "for thr, c in zip(thresholds_demo, colors):\n",
    "    tp, fp, tn, fn = confusion_at_threshold(y_true_big, y_score_big, threshold=float(thr))\n",
    "    tpr_thr, fpr_thr = tpr_fpr_from_confusion(tp, fp, tn, fn)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[fpr_thr],\n",
    "            y=[tpr_thr],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10, color=c),\n",
    "            name=f\"τ={thr:.2f}\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_layout(barmode=\"overlay\", width=950, height=430)\n",
    "fig.update_xaxes(title_text=\"score\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"FPR\", range=[0, 1], row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"TPR\", range=[0, 1], row=1, col=2)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc2c08",
   "metadata": {},
   "source": [
    "## 6) Class imbalance: ROC AUC is prevalence-invariant (PR AUC is not)\n",
    "\n",
    "ROC uses rates (TPR/FPR), so duplicating every negative example (same scores) leaves the curve and AUC unchanged.\n",
    "\n",
    "Precision–recall metrics do change with prevalence, so PR AUC is often preferred for extreme imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate negatives 10x (same scores) to change prevalence\n",
    "y_true_imbal = np.r_[y_true_big[y_true_big == 1], np.repeat(y_true_big[y_true_big == 0], 10)]\n",
    "y_score_imbal = np.r_[y_score_big[y_true_big == 1], np.repeat(y_score_big[y_true_big == 0], 10)]\n",
    "\n",
    "auc_orig = roc_auc_score_np(y_true_big, y_score_big)\n",
    "auc_imbal = roc_auc_score_np(y_true_imbal, y_score_imbal)\n",
    "\n",
    "ap_orig = average_precision_score(y_true_big, y_score_big)\n",
    "ap_imbal = average_precision_score(y_true_imbal, y_score_imbal)\n",
    "\n",
    "auc_orig, auc_imbal, ap_orig, ap_imbal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_o, tpr_o, _ = roc_curve_np(y_true_big, y_score_big)\n",
    "fpr_i, tpr_i, _ = roc_curve_np(y_true_imbal, y_score_imbal)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr_o, y=tpr_o, mode=\"lines\", name=f\"original (AUC={auc_orig:.3f})\"))\n",
    "fig.add_trace(go.Scatter(x=fpr_i, y=tpr_i, mode=\"lines\", name=f\"negatives ×10 (AUC={auc_imbal:.3f})\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"black\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"ROC curves overlap under prevalence shift\",\n",
    "    xaxis_title=\"FPR\",\n",
    "    yaxis_title=\"TPR\",\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    width=720,\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7f4ee",
   "metadata": {},
   "source": [
    "## 7) Practical usage (scikit-learn)\n",
    "\n",
    "Key points:\n",
    "- Pass **scores**, not hard labels.\n",
    "  - `predict_proba(X)[:, 1]` (probabilities)\n",
    "  - `decision_function(X)` (raw scores / logits)\n",
    "- Any monotonic transform of scores gives the same AUC.\n",
    "- For multiclass you must choose `multi_class=\"ovr\"` or `\"ovo\"` and an averaging strategy.\n",
    "\n",
    "Docs: `sklearn.metrics.roc_auc_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa872b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    weights=[0.85, 0.15],\n",
    "    class_sep=1.0,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_logit = clf.decision_function(X_test)\n",
    "score_proba = clf.predict_proba(X_test)[:, 1]\n",
    "score_label = clf.predict(X_test)\n",
    "\n",
    "# (logits and probabilities have identical ranking → identical AUC)\n",
    "skl_roc_auc_score(y_test, score_logit), skl_roc_auc_score(y_test, score_proba), skl_roc_auc_score(y_test, score_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848206d",
   "metadata": {},
   "source": [
    "## 8) Optimizing for ROC AUC (NumPy)\n",
    "\n",
    "For binary labels, AUC can be written as an average over all positive–negative pairs:\n",
    "\n",
    "$$\n",
    "\\mathrm{AUC}(s) = \\frac{1}{|P||N|} \\sum_{i\\in P}\\sum_{j\\in N} \\Big(\\mathbb{1}[s_i > s_j] + \\tfrac{1}{2}\\mathbb{1}[s_i = s_j]\\Big)\n",
    "$$\n",
    "\n",
    "This depends on **pairwise orderings** (rankings), which makes it:\n",
    "- non-decomposable over single examples\n",
    "- non-differentiable because of the indicator\n",
    "\n",
    "A common workaround is to optimize a **smooth pairwise surrogate**. For a linear scoring model $s(x)=w^\\top x$ one choice is the pairwise logistic loss:\n",
    "\n",
    "$$\n",
    "L(w)=\\frac{1}{|P||N|}\\sum_{i\\in P}\\sum_{j\\in N} \\log\\big(1+\\exp\\big(-(s_i - s_j)\\big)\\big)\n",
    "$$\n",
    "\n",
    "Minimizing $L$ encourages $s_i > s_j$ for positive $i$ and negative $j$, i.e. better AUC.\n",
    "\n",
    "In practice we sample pairs (SGD) instead of enumerating all $|P||N|$ pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04504d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.asarray(z)\n",
    "    z = np.clip(z, -40, 40)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def add_bias(X):\n",
    "    X = np.asarray(X)\n",
    "    return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "\n",
    "def make_gaussian_binary(n_pos=250, n_neg=1250, seed=0):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    mean_pos = np.array([1.5, 1.5])\n",
    "    mean_neg = np.array([0.0, 0.0])\n",
    "    cov = np.array([[1.0, 0.3], [0.3, 1.0]])\n",
    "\n",
    "    X_pos = rng_local.multivariate_normal(mean_pos, cov, size=n_pos)\n",
    "    X_neg = rng_local.multivariate_normal(mean_neg, cov, size=n_neg)\n",
    "\n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.r_[np.ones(n_pos, dtype=int), np.zeros(n_neg, dtype=int)]\n",
    "\n",
    "    perm = rng_local.permutation(len(y))\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "\n",
    "def train_logistic_logloss_gd(X, y, lr=0.2, steps=2000, l2=1e-3, log_every=50):\n",
    "    Xb = add_bias(X)\n",
    "    y = y.astype(float)\n",
    "\n",
    "    w = np.zeros(Xb.shape[1])\n",
    "    hist = []\n",
    "\n",
    "    for step in range(steps + 1):\n",
    "        scores = Xb @ w\n",
    "        p = sigmoid(scores)\n",
    "\n",
    "        grad = (Xb.T @ (p - y)) / len(y)\n",
    "        reg_grad = l2 * np.r_[0.0, w[1:]]  # don't regularize bias\n",
    "        w -= lr * (grad + reg_grad)\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            logloss = -(y * np.log(p + 1e-12) + (1 - y) * np.log(1 - p + 1e-12)).mean()\n",
    "            auc = roc_auc_score_np(y.astype(int), scores)\n",
    "            hist.append({\"step\": step, \"logloss\": logloss, \"train_auc\": auc})\n",
    "\n",
    "    return w, pd.DataFrame(hist)\n",
    "\n",
    "\n",
    "def train_auc_pairwise_sgd(\n",
    "    X, y, lr=0.2, steps=4000, batch_pairs=512, l2=1e-3, log_every=50, seed=0\n",
    "):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    Xb = add_bias(X)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    pos_idx = np.flatnonzero(y == 1)\n",
    "    neg_idx = np.flatnonzero(y == 0)\n",
    "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "        raise ValueError(\"Need both classes to optimize AUC.\")\n",
    "\n",
    "    w = np.zeros(Xb.shape[1])\n",
    "    hist = []\n",
    "\n",
    "    for step in range(steps + 1):\n",
    "        i = rng_local.choice(pos_idx, size=batch_pairs, replace=True)\n",
    "        j = rng_local.choice(neg_idx, size=batch_pairs, replace=True)\n",
    "\n",
    "        delta = Xb[i] - Xb[j]  # x_i - x_j\n",
    "        d = delta @ w  # (w^T x_i) - (w^T x_j)\n",
    "\n",
    "        # loss = log(1 + exp(-d))\n",
    "        # dloss/dd = -sigmoid(-d)\n",
    "        grad = -(sigmoid(-d)[:, None] * delta).mean(axis=0)\n",
    "\n",
    "        reg_grad = l2 * np.r_[0.0, w[1:]]\n",
    "        w -= lr * (grad + reg_grad)\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            scores = Xb @ w\n",
    "            auc = roc_auc_score_np(y, scores)\n",
    "            pair_loss = np.log1p(np.exp(-d)).mean()\n",
    "            hist.append({\"step\": step, \"pair_loss\": pair_loss, \"train_auc\": auc})\n",
    "\n",
    "    return w, pd.DataFrame(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_gaussian_binary(seed=SEED)\n",
    "\n",
    "# manual split (stratified-ish via shuffling; dataset is large enough here)\n",
    "idx = rng.permutation(len(y))\n",
    "n_train = int(0.7 * len(y))\n",
    "train_idx, test_idx = idx[:n_train], idx[n_train:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "w_ce, hist_ce = train_logistic_logloss_gd(X_train, y_train, lr=0.3, steps=2000, log_every=50)\n",
    "w_auc, hist_auc = train_auc_pairwise_sgd(\n",
    "    X_train, y_train, lr=0.3, steps=3000, batch_pairs=1024, log_every=50, seed=SEED\n",
    ")\n",
    "\n",
    "scores_ce_test = add_bias(X_test) @ w_ce\n",
    "scores_auc_test = add_bias(X_test) @ w_auc\n",
    "\n",
    "auc_ce_test = roc_auc_score_np(y_test, scores_ce_test)\n",
    "auc_auc_test = roc_auc_score_np(y_test, scores_auc_test)\n",
    "\n",
    "auc_ce_test, auc_auc_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist_ce[\"step\"],\n",
    "        y=hist_ce[\"train_auc\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"log-loss GD (train AUC)\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist_auc[\"step\"],\n",
    "        y=hist_auc[\"train_auc\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"pairwise AUC surrogate (train AUC)\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Training AUC over iterations\",\n",
    "    xaxis_title=\"step\",\n",
    "    yaxis_title=\"ROC AUC\",\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    width=760,\n",
    "    height=430,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea498ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_ce, tpr_ce, _ = roc_curve_np(y_test, scores_ce_test)\n",
    "fpr_auc, tpr_auc, _ = roc_curve_np(y_test, scores_auc_test)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr_ce,\n",
    "        y=tpr_ce,\n",
    "        mode=\"lines\",\n",
    "        name=f\"log-loss GD (test AUC={auc_ce_test:.3f})\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr_auc,\n",
    "        y=tpr_auc,\n",
    "        mode=\"lines\",\n",
    "        name=f\"AUC surrogate (test AUC={auc_auc_test:.3f})\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"black\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Test ROC curves\",\n",
    "    xaxis_title=\"FPR\",\n",
    "    yaxis_title=\"TPR\",\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    width=760,\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da1f97",
   "metadata": {},
   "source": [
    "## Pros / cons / when to use\n",
    "\n",
    "**Pros**\n",
    "- Threshold-free: summarizes performance across all thresholds\n",
    "- Ranking-focused: $\\mathbb{P}(s^+ > s^-)$ interpretation is often intuitive\n",
    "- Invariant to monotonic score transforms (logits vs probabilities)\n",
    "- Less sensitive to class imbalance than accuracy (uses normalized rates)\n",
    "\n",
    "**Cons / pitfalls**\n",
    "- Not about calibration: probabilities can be badly calibrated and still yield high AUC\n",
    "- Weights all FPR regions equally; if you care about tiny FPR, consider **partial AUC**\n",
    "- For extreme imbalance, PR AUC can be more informative than ROC AUC\n",
    "- Undefined if `y_true` contains only one class; multiclass requires design choices (`ovr`/`ovo`, averaging)\n",
    "\n",
    "**Good for**\n",
    "- Model comparison when you care about ranking / screening\n",
    "- Imbalanced classification when you want a threshold-independent ranking metric\n",
    "\n",
    "**Less good for**\n",
    "- Picking a single operating threshold under asymmetric costs\n",
    "- Measuring probability quality (use log-loss, Brier score, calibration curves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c665f91",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Implement **partial AUC** for a max FPR (e.g. integrate only over $\\mathrm{FPR}\\in[0,0.1]$).\n",
    "\n",
    "2) Extend `roc_curve_np` to support **sample weights**.\n",
    "\n",
    "3) Show numerically that AUC is unchanged by any strictly monotonic transform (try `np.tanh`, `np.exp`, `sigmoid`).\n",
    "\n",
    "4) Multiclass: compute one-vs-rest AUC for each class and compare macro vs weighted averages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c3192",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn `roc_auc_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "- scikit-learn ROC user guide: https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n",
    "- T. Fawcett (2006), *An introduction to ROC analysis*\n",
    "- Hanley & McNeil (1982), *The meaning and use of the area under a ROC curve*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}