{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae99515",
   "metadata": {},
   "source": [
    "# matthews_corrcoef (Matthews Correlation Coefficient, MCC)\n",
    "\n",
    "The **Matthews correlation coefficient (MCC)** is a single-number summary of a classifier’s **confusion matrix**.\n",
    "It can be interpreted as the **Pearson correlation** between true and predicted labels, so it naturally lives in $[-1, 1]$:\n",
    "\n",
    "- $+1$: perfect predictions\n",
    "- $0$: no better than random (no correlation)\n",
    "- $-1$: perfectly wrong (systematic inversion)\n",
    "\n",
    "MCC is especially useful when classes are **imbalanced**, because it uses **all four** confusion-matrix entries (TP, TN, FP, FN).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "- derive MCC from the confusion matrix and from Pearson correlation\n",
    "- implement MCC from scratch in NumPy (binary + multiclass)\n",
    "- build intuition with Plotly visuals (imbalance + thresholding)\n",
    "- use MCC to **select a decision threshold** / tune a simple model\n",
    "\n",
    "## Table of contents\n",
    "1. Confusion matrix recap\n",
    "2. Binary MCC: definition + correlation view\n",
    "3. Multiclass MCC\n",
    "4. NumPy implementation (from scratch)\n",
    "5. Intuition plots (TPR/TNR surface + imbalance trap)\n",
    "6. Using MCC for optimization: threshold tuning for logistic regression\n",
    "7. Pros, cons, and when to use MCC\n",
    "8. Exercises + references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f96e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import matthews_corrcoef as sk_matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bc3b5",
   "metadata": {},
   "source": [
    "## 1) Confusion matrix recap\n",
    "\n",
    "For **binary classification**, assume the positive class is labeled $1$ and the negative class is labeled $0$.\n",
    "\n",
    "A confusion matrix counts outcomes:\n",
    "\n",
    "|                | predicted $1$ | predicted $0$ |\n",
    "|---             |---:           |---:           |\n",
    "| **true $1$**   | TP            | FN            |\n",
    "| **true $0$**   | FP            | TN            |\n",
    "\n",
    "With total sample size:\n",
    "\n",
    "$$\n",
    "N = \\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}.\n",
    "$$\n",
    "\n",
    "Useful rates:\n",
    "\n",
    "- **TPR / recall / sensitivity**: $\\text{TPR} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$\n",
    "- **TNR / specificity**: $\\text{TNR} = \\frac{\\text{TN}}{\\text{TN}+\\text{FP}}$\n",
    "\n",
    "MCC “wants” both TPR and TNR to be high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b11cb",
   "metadata": {},
   "source": [
    "## 2) Binary MCC: definition + correlation view\n",
    "\n",
    "### 2.1 Definition (confusion-matrix form)\n",
    "\n",
    "The (binary) Matthews correlation coefficient is\n",
    "\n",
    "$$\n",
    "\\mathrm{MCC} =\n",
    "\\frac{\\text{TP}\\,\\text{TN} - \\text{FP}\\,\\text{FN}}\n",
    "{\\sqrt{(\\text{TP}+\\text{FP})(\\text{TP}+\\text{FN})(\\text{TN}+\\text{FP})(\\text{TN}+\\text{FN})}}.\n",
    "$$\n",
    "\n",
    "- The **numerator** rewards agreement (TP·TN) and penalizes disagreement (FP·FN).\n",
    "- The **denominator** normalizes to keep the score in $[-1, 1]$.\n",
    "\n",
    "If the denominator is $0$ (e.g. constant predictions, or all labels are the same), MCC is mathematically undefined.\n",
    "In practice (and in scikit-learn), it is returned as **0.0**.\n",
    "\n",
    "### 2.2 MCC = Pearson correlation for 0/1 labels\n",
    "\n",
    "Let $Y, \\hat Y \\in \\{0,1\\}$ be the true and predicted labels. The Pearson correlation is\n",
    "\n",
    "$$\n",
    "\\rho(Y, \\hat Y) = \\frac{\\mathrm{Cov}(Y, \\hat Y)}{\\sqrt{\\mathrm{Var}(Y)\\,\\mathrm{Var}(\\hat Y)}}.\n",
    "$$\n",
    "\n",
    "Using the contingency table above:\n",
    "\n",
    "- $\\mathbb{E}[Y] = \\frac{\\text{TP}+\\text{FN}}{N}$\n",
    "- $\\mathbb{E}[\\hat Y] = \\frac{\\text{TP}+\\text{FP}}{N}$\n",
    "- $\\mathbb{E}[Y\\hat Y] = \\frac{\\text{TP}}{N}$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(Y,\\hat Y)\n",
    "= \\mathbb{E}[Y\\hat Y] - \\mathbb{E}[Y]\\,\\mathbb{E}[\\hat Y]\n",
    "= \\frac{\\text{TP}\\,\\text{TN} - \\text{FP}\\,\\text{FN}}{N^2}.\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(Y) = \\frac{(\\text{TP}+\\text{FN})(\\text{TN}+\\text{FP})}{N^2},\n",
    "\\quad\n",
    "\\mathrm{Var}(\\hat Y) = \\frac{(\\text{TP}+\\text{FP})(\\text{TN}+\\text{FN})}{N^2}.\n",
    "$$\n",
    "\n",
    "Plugging these into $\\rho$ yields the MCC formula.\n",
    "This is why MCC is also known as the **phi coefficient** (correlation for two binary variables).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba5310",
   "metadata": {},
   "source": [
    "## 3) Multiclass MCC\n",
    "\n",
    "MCC has a natural multiclass extension based on the full $K\\times K$ confusion matrix.\n",
    "\n",
    "Let $C\\in\\mathbb{N}^{K\\times K}$ with entries:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\#\\{n : y^{(n)}=i, \\; \\hat y^{(n)}=j\\}.\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "- $s = \\sum_{i,j} C_{ij}$ (total)\n",
    "- $c = \\sum_k C_{kk}$ (correct / trace)\n",
    "- $t_k = \\sum_j C_{k j}$ (true count per class; row sums)\n",
    "- $p_k = \\sum_i C_{i k}$ (predicted count per class; column sums)\n",
    "\n",
    "Then the multiclass MCC is:\n",
    "\n",
    "$$\n",
    "\\mathrm{MCC} =\n",
    "\\frac{c\\,s - \\sum_k t_k p_k}\n",
    "{\\sqrt{\\left(s^2 - \\sum_k p_k^2\\right)\\left(s^2 - \\sum_k t_k^2\\right)}}.\n",
    "$$\n",
    "\n",
    "It reduces to the binary formula when $K=2$, and can be viewed as a correlation between one-hot encodings of $y$ and $\\hat y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ed102",
   "metadata": {},
   "source": [
    "## 4) NumPy implementation (from scratch)\n",
    "\n",
    "We’ll implement:\n",
    "\n",
    "- a simple confusion matrix builder\n",
    "- MCC for binary and multiclass using the $K\\times K$ formula\n",
    "- (optionally) the binary closed-form as a sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_np(y_true, y_pred, labels=None):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape\")\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    else:\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "    label_to_index = {label: i for i, label in enumerate(labels.tolist())}\n",
    "\n",
    "    true_idx = np.fromiter((label_to_index.get(v, -1) for v in y_true), dtype=int, count=y_true.size)\n",
    "    pred_idx = np.fromiter((label_to_index.get(v, -1) for v in y_pred), dtype=int, count=y_pred.size)\n",
    "\n",
    "    if (true_idx < 0).any() or (pred_idx < 0).any():\n",
    "        raise ValueError(\"labels must contain all values appearing in y_true and y_pred\")\n",
    "\n",
    "    k = labels.size\n",
    "    cm = np.zeros((k, k), dtype=int)\n",
    "    np.add.at(cm, (true_idx, pred_idx), 1)\n",
    "    return cm, labels\n",
    "\n",
    "\n",
    "def mcc_from_counts(tp, tn, fp, fn):\n",
    "    tp = np.asarray(tp, dtype=float)\n",
    "    tn = np.asarray(tn, dtype=float)\n",
    "    fp = np.asarray(fp, dtype=float)\n",
    "    fn = np.asarray(fn, dtype=float)\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    denom = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    return np.where(denom == 0, 0.0, num / denom)\n",
    "\n",
    "\n",
    "def matthews_corrcoef_np(y_true, y_pred, labels=None) -> float:\n",
    "    cm, _ = confusion_matrix_np(y_true, y_pred, labels=labels)\n",
    "\n",
    "    t_sum = cm.sum(axis=1, dtype=float)  # true per class\n",
    "    p_sum = cm.sum(axis=0, dtype=float)  # predicted per class\n",
    "\n",
    "    s = float(cm.sum())\n",
    "    c = float(np.trace(cm))\n",
    "\n",
    "    num = c * s - float(np.dot(t_sum, p_sum))\n",
    "    denom = np.sqrt((s**2 - float(np.dot(p_sum, p_sum))) * (s**2 - float(np.dot(t_sum, t_sum))))\n",
    "\n",
    "    return 0.0 if denom == 0.0 else num / denom\n",
    "\n",
    "\n",
    "def confusion_counts_binary(y_true, y_pred, positive_label=1):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    true_pos = y_true == positive_label\n",
    "    pred_pos = y_pred == positive_label\n",
    "\n",
    "    tp = int(np.sum(true_pos & pred_pos))\n",
    "    tn = int(np.sum(~true_pos & ~pred_pos))\n",
    "    fp = int(np.sum(~true_pos & pred_pos))\n",
    "    fn = int(np.sum(true_pos & ~pred_pos))\n",
    "    return tp, tn, fp, fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check vs scikit-learn\n",
    "\n",
    "y_true = np.array([1, 1, 1, 0, 0, 0, 0, 0])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 0, 0])\n",
    "\n",
    "cm, labels = confusion_matrix_np(y_true, y_pred)\n",
    "tp, tn, fp, fn = confusion_counts_binary(y_true, y_pred, positive_label=1)\n",
    "\n",
    "print(\"labels:\", labels)\n",
    "print(\"confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "print(\"TP,TN,FP,FN:\", tp, tn, fp, fn)\n",
    "\n",
    "print(\"MCC (scratch, KxK):\", matthews_corrcoef_np(y_true, y_pred))\n",
    "print(\"MCC (scratch, binary counts):\", float(mcc_from_counts(tp, tn, fp, fn)))\n",
    "print(\"MCC (sklearn):\", sk_matthews_corrcoef(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63228c",
   "metadata": {},
   "source": [
    "### 4.1 Multiclass sanity check\n",
    "\n",
    "MCC supports multiclass via the confusion-matrix generalization. We’ll verify our NumPy implementation against scikit-learn on a simple 3-class example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59618bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass sanity check (K=3)\n",
    "\n",
    "y_true_mc = rng.integers(0, 3, size=500)\n",
    "\n",
    "y_pred_mc = y_true_mc.copy()\n",
    "noise = rng.random(size=y_true_mc.size) < 0.25\n",
    "# if noisy: replace with a random label in {0,1,2}\n",
    "y_pred_mc[noise] = rng.integers(0, 3, size=int(noise.sum()))\n",
    "\n",
    "mcc_mc = matthews_corrcoef_np(y_true_mc, y_pred_mc)\n",
    "\n",
    "cm_mc, labels_mc = confusion_matrix_np(y_true_mc, y_pred_mc)\n",
    "\n",
    "fig = px.imshow(\n",
    "    cm_mc,\n",
    "    x=[f\"pred {l}\" for l in labels_mc],\n",
    "    y=[f\"true {l}\" for l in labels_mc],\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    ")\n",
    "fig.update_layout(title=f\"Multiclass confusion matrix (MCC={mcc_mc:.3f})\")\n",
    "fig.show()\n",
    "\n",
    "print(\"MCC multiclass (scratch):\", mcc_mc)\n",
    "print(\"MCC multiclass (sklearn):\", sk_matthews_corrcoef(y_true_mc, y_pred_mc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17101e56",
   "metadata": {},
   "source": [
    "## 5) Intuition plots\n",
    "\n",
    "### 5.1 MCC as a function of TPR and TNR\n",
    "\n",
    "If we fix the **class prevalence** $\\pi = P(Y=1)$ and imagine a classifier with some $(\\text{TPR},\\text{TNR})$, the expected confusion counts (for large $N$) are:\n",
    "\n",
    "$$\n",
    "\\text{TP} = N\\,\\pi\\,\\text{TPR},\\quad\n",
    "\\text{FN} = N\\,\\pi\\,(1-\\text{TPR}),\\quad\n",
    "\\text{TN} = N\\,(1-\\pi)\\,\\text{TNR},\\quad\n",
    "\\text{FP} = N\\,(1-\\pi)\\,(1-\\text{TNR}).\n",
    "$$\n",
    "\n",
    "Plotting MCC over $(\\text{TPR},\\text{TNR})$ shows how **both** kinds of mistakes affect the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9901b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mcc_surface(pi: float, grid_steps: int = 101, title: str | None = None):\n",
    "    t = np.linspace(0.0, 1.0, grid_steps)\n",
    "    tpr, tnr = np.meshgrid(t, t, indexing=\"xy\")\n",
    "\n",
    "    n = 1.0  # scale cancels out in MCC\n",
    "    tp = n * pi * tpr\n",
    "    fn = n * pi * (1 - tpr)\n",
    "    tn = n * (1 - pi) * tnr\n",
    "    fp = n * (1 - pi) * (1 - tnr)\n",
    "\n",
    "    z = mcc_from_counts(tp, tn, fp, fn)\n",
    "\n",
    "    fig = px.imshow(\n",
    "        z,\n",
    "        x=t,\n",
    "        y=t,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        labels={\"x\": \"TPR (recall)\", \"y\": \"TNR (specificity)\", \"color\": \"MCC\"},\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=t, y=t, mode=\"lines\", name=\"TPR = TNR\", line=dict(color=\"black\", dash=\"dash\"))\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title or f\"MCC surface over (TPR, TNR) with prevalence π={pi:.2f}\",\n",
    "        coloraxis_colorbar=dict(title=\"MCC\"),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_mcc_surface(pi=0.10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50bcdd",
   "metadata": {},
   "source": [
    "### 5.2 The “accuracy trap” under imbalance\n",
    "\n",
    "Consider a dataset where the positive class is rare. A trivial classifier that predicts **always negative** can achieve very high **accuracy**, even though it’s useless.\n",
    "\n",
    "MCC exposes this: constant predictions lead to an MCC of **0**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c7d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = np.linspace(0.001, 0.999, 300)\n",
    "\n",
    "acc_always_negative = 1.0 - prevalence\n",
    "mcc_always_negative = np.zeros_like(prevalence)\n",
    "balanced_acc_always_negative = np.full_like(prevalence, 0.5)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=prevalence, y=acc_always_negative, name=\"accuracy (always predict 0)\"))\n",
    "fig.add_trace(go.Scatter(x=prevalence, y=balanced_acc_always_negative, name=\"balanced accuracy (always 0)\", line=dict(dash=\"dash\")))\n",
    "fig.add_trace(go.Scatter(x=prevalence, y=mcc_always_negative, name=\"MCC (always 0)\", line=dict(dash=\"dot\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Imbalance demo: accuracy can look great while MCC stays 0\",\n",
    "    xaxis_title=\"Positive prevalence π = P(Y=1)\",\n",
    "    yaxis_title=\"Metric value\",\n",
    "    yaxis=dict(range=[-0.05, 1.05]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd1a3b",
   "metadata": {},
   "source": [
    "## 6) Using MCC for optimization: threshold tuning for logistic regression\n",
    "\n",
    "MCC is **not differentiable** with respect to model parameters (it depends on discrete labels), so we typically:\n",
    "\n",
    "1) train a probabilistic model with a smooth loss (e.g. **log-loss**)\n",
    "2) choose a **decision threshold** (or hyperparameters) that maximizes MCC on a validation set\n",
    "\n",
    "Below is a minimal **from-scratch** logistic regression and an MCC-based threshold selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "\n",
    "    out = np.empty_like(z, dtype=float)\n",
    "    pos = z >= 0\n",
    "\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    ez = np.exp(z[~pos])\n",
    "    out[~pos] = ez / (1.0 + ez)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def binary_log_loss(y_true, p, eps: float = 1e-15) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "\n",
    "    p = np.clip(p, eps, 1.0 - eps)\n",
    "    return float(-np.mean(y_true * np.log(p) + (1.0 - y_true) * np.log(1.0 - p)))\n",
    "\n",
    "\n",
    "def standardize_fit(X_train: np.ndarray):\n",
    "    mu = X_train.mean(axis=0)\n",
    "    sigma = X_train.std(axis=0)\n",
    "    sigma = np.where(sigma == 0, 1.0, sigma)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def standardize_apply(X: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> np.ndarray:\n",
    "    return (X - mu) / sigma\n",
    "\n",
    "\n",
    "def fit_logistic_regression_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    lr: float = 0.1,\n",
    "    n_iter: int = 2000,\n",
    "    l2: float = 0.0,\n",
    "):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    losses = np.empty(n_iter)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        p = sigmoid(X @ w)\n",
    "\n",
    "        # average log-loss + L2 (skip intercept)\n",
    "        losses[i] = binary_log_loss(y, p) + 0.5 * l2 * float(np.dot(w[1:], w[1:]))\n",
    "\n",
    "        grad = (X.T @ (p - y)) / n\n",
    "        grad[1:] += l2 * w[1:]\n",
    "\n",
    "        w -= lr * grad\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "\n",
    "def safe_div(num, denom):\n",
    "    num = np.asarray(num, dtype=float)\n",
    "    denom = np.asarray(denom, dtype=float)\n",
    "    return np.where(denom == 0, 0.0, num / denom)\n",
    "\n",
    "\n",
    "def binary_metrics_from_counts(tp, tn, fp, fn):\n",
    "    tp = np.asarray(tp, dtype=float)\n",
    "    tn = np.asarray(tn, dtype=float)\n",
    "    fp = np.asarray(fp, dtype=float)\n",
    "    fn = np.asarray(fn, dtype=float)\n",
    "\n",
    "    acc = safe_div(tp + tn, tp + tn + fp + fn)\n",
    "\n",
    "    precision = safe_div(tp, tp + fp)\n",
    "    recall = safe_div(tp, tp + fn)\n",
    "    f1 = safe_div(2 * precision * recall, precision + recall)\n",
    "\n",
    "    tpr = recall\n",
    "    tnr = safe_div(tn, tn + fp)\n",
    "    bal_acc = 0.5 * (tpr + tnr)\n",
    "\n",
    "    mcc = mcc_from_counts(tp, tn, fp, fn)\n",
    "\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic, imbalanced dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=4000,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.90, 0.10],\n",
    "    class_sep=1.2,\n",
    "    flip_y=0.02,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=7)\n",
    "\n",
    "mu, sigma = standardize_fit(X_train)\n",
    "X_train_s = standardize_apply(X_train, mu, sigma)\n",
    "X_val_s = standardize_apply(X_val, mu, sigma)\n",
    "X_test_s = standardize_apply(X_test, mu, sigma)\n",
    "\n",
    "X_train_i = add_intercept(X_train_s)\n",
    "X_val_i = add_intercept(X_val_s)\n",
    "X_test_i = add_intercept(X_test_s)\n",
    "\n",
    "w, losses = fit_logistic_regression_gd(X_train_i, y_train, lr=0.15, n_iter=2500, l2=1e-2)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(losses.size), y=losses, name=\"train loss\"))\n",
    "fig.update_layout(title=\"From-scratch logistic regression (GD): training loss\", xaxis_title=\"iteration\", yaxis_title=\"loss\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f274a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep on validation set\n",
    "p_val = sigmoid(X_val_i @ w)\n",
    "\n",
    "thresholds = np.linspace(0.0, 1.0, 401)\n",
    "\n",
    "y_val_bool = y_val.astype(bool)\n",
    "pred_pos = p_val[:, None] >= thresholds[None, :]\n",
    "\n",
    "tp = np.sum(pred_pos & y_val_bool[:, None], axis=0)\n",
    "fp = np.sum(pred_pos & ~y_val_bool[:, None], axis=0)\n",
    "fn = np.sum(~pred_pos & y_val_bool[:, None], axis=0)\n",
    "tn = np.sum(~pred_pos & ~y_val_bool[:, None], axis=0)\n",
    "\n",
    "metrics = binary_metrics_from_counts(tp, tn, fp, fn)\n",
    "\n",
    "best_idx = int(np.argmax(metrics[\"mcc\"]))\n",
    "best_t = float(thresholds[best_idx])\n",
    "\n",
    "best_acc_idx = int(np.argmax(metrics[\"accuracy\"]))\n",
    "best_acc_t = float(thresholds[best_acc_idx])\n",
    "\n",
    "best_t, best_acc_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how metrics change with the decision threshold\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, values in metrics.items():\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=values, name=name))\n",
    "\n",
    "fig.add_vline(x=0.5, line_dash=\"dot\", line_color=\"gray\", annotation_text=\"t=0.5\")\n",
    "fig.add_vline(\n",
    "    x=best_acc_t,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"gray\",\n",
    "    annotation_text=f\"best accuracy t={best_acc_t:.3f}\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=best_t,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=f\"best MCC t={best_t:.3f}\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Validation curves vs threshold\",\n",
    "    xaxis_title=\"threshold\",\n",
    "    yaxis_title=\"metric value\",\n",
    "    yaxis=dict(range=[-0.05, 1.05]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set and compare thresholds\n",
    "p_test = sigmoid(X_test_i @ w)\n",
    "\n",
    "\n",
    "def metrics_at_threshold(t: float):\n",
    "    y_pred = (p_test >= t).astype(int)\n",
    "    tp, tn, fp, fn = confusion_counts_binary(y_test, y_pred, positive_label=1)\n",
    "    m = binary_metrics_from_counts(tp, tn, fp, fn)\n",
    "    return {k: float(v) for k, v in m.items()}, y_pred\n",
    "\n",
    "\n",
    "for t in [0.5, best_acc_t, best_t]:\n",
    "    m, _ = metrics_at_threshold(t)\n",
    "    print(\n",
    "        f\"t={t:.3f} | MCC={m['mcc']:.3f} | acc={m['accuracy']:.3f} | bal_acc={m['balanced_accuracy']:.3f} | F1={m['f1']:.3f}\"\n",
    "    )\n",
    "\n",
    "# Confusion matrix for the MCC-optimal threshold\n",
    "m_best, y_test_pred = metrics_at_threshold(best_t)\n",
    "\n",
    "cm_test, _ = confusion_matrix_np(y_test, y_test_pred, labels=np.array([0, 1]))\n",
    "\n",
    "fig = px.imshow(\n",
    "    cm_test,\n",
    "    x=[\"pred 0\", \"pred 1\"],\n",
    "    y=[\"true 0\", \"true 1\"],\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    ")\n",
    "fig.update_layout(title=f\"Test confusion matrix (threshold={best_t:.3f}, MCC={m_best['mcc']:.3f})\")\n",
    "fig.show()\n",
    "\n",
    "print(\"Test MCC (scratch):\", m_best[\"mcc\"])\n",
    "print(\"Test MCC (sklearn):\", sk_matthews_corrcoef(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc621a84",
   "metadata": {},
   "source": [
    "## 7) Pros, cons, and when to use MCC\n",
    "\n",
    "### Pros\n",
    "- **Uses all of TP/TN/FP/FN** (unlike precision/recall which ignore TN/TP)\n",
    "- **Robust under class imbalance** (unlike accuracy)\n",
    "- **Symmetric**: swapping positive/negative labels does not change the value\n",
    "- **Single interpretable scale** ($[-1,1]$) with a correlation meaning\n",
    "- **Works for multiclass** via the confusion-matrix generalization\n",
    "\n",
    "### Cons / caveats\n",
    "- Can be **undefined** when predictions (or labels) are constant; commonly returned as **0** by convention\n",
    "- **Non-differentiable** w.r.t. model parameters → not a direct gradient-descent loss\n",
    "- Threshold-dependent for probabilistic models; you often need **threshold tuning**\n",
    "- Can be **noisy/unstable** with very small sample sizes or extremely rare classes\n",
    "\n",
    "### When MCC shines\n",
    "- **Imbalanced binary classification** where both error types matter (FP and FN)\n",
    "- Model selection and threshold tuning when you want a single score that “respects” the full confusion matrix\n",
    "- Domains with strong imbalance and asymmetric costs where accuracy is misleading (bioinformatics, fraud, anomaly-ish settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c2c04",
   "metadata": {},
   "source": [
    "## 8) Exercises + references\n",
    "\n",
    "### Exercises\n",
    "1) Compute MCC by hand for a few confusion matrices and interpret the sign.\n",
    "2) Implement a multiclass demo: generate $K=3$ labels, perturb predictions, and verify your MCC matches scikit-learn.\n",
    "3) On the logistic regression demo above:\n",
    "   - compare the threshold that maximizes **accuracy** vs **MCC**\n",
    "   - try a more imbalanced dataset (e.g. 99/1) and re-run the threshold sweep\n",
    "4) Implement cross-validated model selection where the chosen hyperparameter maximizes validation MCC.\n",
    "\n",
    "### References\n",
    "- Matthews, B. W. (1975). *Comparison of the predicted and observed secondary structure of T4 phage lysozyme.*\n",
    "- scikit-learn docs: `sklearn.metrics.matthews_corrcoef`\n",
    "- The phi coefficient (binary correlation) and its relationship to MCC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}