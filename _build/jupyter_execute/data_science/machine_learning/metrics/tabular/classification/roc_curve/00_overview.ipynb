{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ef6e71",
   "metadata": {},
   "source": [
    "# ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "The ROC curve visualizes the tradeoff between:\n",
    "\n",
    "- **True Positive Rate (TPR / recall / sensitivity)**\n",
    "- **False Positive Rate (FPR = 1 - specificity)**\n",
    "\n",
    "as we sweep a decision threshold over a model's **scores** (probabilities, logits, or any ranking score).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- define TPR/FPR from the confusion matrix\n",
    "- compute ROC points by threshold-sweeping\n",
    "- implement `roc_curve` and AUC from scratch (NumPy)\n",
    "- pick an operating threshold with ROC constraints (e.g. \"FPR ≤ 5%\")\n",
    "- use AUC-ROC to pick a hyperparameter for logistic regression\n",
    "\n",
    "## Quick import (scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa94fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Reproducibility: record versions (nice to have in a knowledge base)\n",
    "import sys\n",
    "import plotly\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy :\", np.__version__)\n",
    "print(\"plotly:\", plotly.__version__)\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "\n",
    "    print(\"sklearn:\", sklearn.__version__)\n",
    "except Exception as e:\n",
    "    print(\"sklearn: not available ->\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bde9f2",
   "metadata": {},
   "source": [
    "## 1) From scores to decisions\n",
    "\n",
    "Let $y_i \\in \\{0,1\\}$ be the true label and $s_i \\in \\mathbb{R}$ be a score where **larger means more likely positive**.\n",
    "\n",
    "A threshold $t$ turns scores into hard predictions:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i(t) = \\mathbb{1}[s_i \\ge t]\n",
    "$$\n",
    "\n",
    "This creates the confusion-matrix counts:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{TP}(t) &= \\sum_i \\mathbb{1}[y_i=1 \\land s_i \\ge t] \\\\\n",
    "\\mathrm{FP}(t) &= \\sum_i \\mathbb{1}[y_i=0 \\land s_i \\ge t] \\\\\n",
    "\\mathrm{TN}(t) &= \\sum_i \\mathbb{1}[y_i=0 \\land s_i < t] \\\\\n",
    "\\mathrm{FN}(t) &= \\sum_i \\mathbb{1}[y_i=1 \\land s_i < t]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Two key rates (both in $[0,1]$):\n",
    "\n",
    "$$\n",
    "\\mathrm{TPR}(t) = \\frac{\\mathrm{TP}(t)}{\\mathrm{TP}(t)+\\mathrm{FN}(t)}\n",
    "\\qquad\n",
    "\\mathrm{FPR}(t) = \\frac{\\mathrm{FP}(t)}{\\mathrm{FP}(t)+\\mathrm{TN}(t)}\n",
    "$$\n",
    "\n",
    "- $\\mathrm{TPR}(t)$ is **sensitivity / recall**: $P(\\hat{y}=1\\mid y=1)$\n",
    "- $\\mathrm{FPR}(t)$ is $1-\\text{specificity}$: $P(\\hat{y}=1\\mid y=0)$\n",
    "\n",
    "**ROC curve**: the set of points $(\\mathrm{FPR}(t),\\mathrm{TPR}(t))$ as we sweep $t$ from $+\\infty$ down to $-\\infty$.\n",
    "\n",
    "- At $t=+\\infty$: predict everything negative → $(0,0)$\n",
    "- At $t=-\\infty$: predict everything positive → $(1,1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce36a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy scores: positives tend to have higher scores, but overlap with negatives.\n",
    "n_pos, n_neg = 250, 350\n",
    "scores_pos = rng.normal(loc=1.2, scale=1.0, size=n_pos)\n",
    "scores_neg = rng.normal(loc=0.0, scale=1.0, size=n_neg)\n",
    "\n",
    "y_true = np.r_[np.ones(n_pos, dtype=int), np.zeros(n_neg, dtype=int)]\n",
    "y_score = np.r_[scores_pos, scores_neg]\n",
    "\n",
    "threshold_example = 0.5\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_histogram(\n",
    "    x=scores_neg,\n",
    "    name=\"y=0 (negative)\",\n",
    "    nbinsx=50,\n",
    "    opacity=0.6,\n",
    "    histnorm=\"probability density\",\n",
    ")\n",
    "fig.add_histogram(\n",
    "    x=scores_pos,\n",
    "    name=\"y=1 (positive)\",\n",
    "    nbinsx=50,\n",
    "    opacity=0.6,\n",
    "    histnorm=\"probability density\",\n",
    ")\n",
    "fig.add_vline(x=threshold_example, line_width=2, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(\n",
    "    barmode=\"overlay\",\n",
    "    title=\"Toy scores: two overlapping distributions (a threshold splits predictions)\",\n",
    "    xaxis_title=\"score s (higher ⇒ more positive)\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_counts(y_true, y_score, threshold):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score)\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return int(tp), int(fp), int(tn), int(fn)\n",
    "\n",
    "\n",
    "tp, fp, tn, fn = confusion_counts(y_true, y_score, threshold_example)\n",
    "tpr_example = tp / (tp + fn)\n",
    "fpr_example = fp / (fp + tn)\n",
    "\n",
    "print(f\"threshold t = {threshold_example:.2f}\")\n",
    "print(f\"TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "print(f\"TPR={tpr_example:.3f}, FPR={fpr_example:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae029ea5",
   "metadata": {},
   "source": [
    "## 2) Computing the ROC curve (NumPy)\n",
    "\n",
    "A naive ROC implementation tries many thresholds and recomputes the confusion matrix each time.\n",
    "That works, but it can be slow.\n",
    "\n",
    "A standard efficient approach:\n",
    "\n",
    "1. Sort samples by score $s$ (descending).\n",
    "2. Sweep the threshold from high to low.\n",
    "3. Each time the threshold crosses a score value, that sample flips from predicted negative → positive.\n",
    "4. Cumulative sums give $\\mathrm{TP}(t)$ and $\\mathrm{FP}(t)$ at each **unique** score.\n",
    "\n",
    "This is $O(n\\log n)$ for the sort, then $O(n)$ for the sweep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_numpy(y_true, y_score):\n",
    "    \"\"\"Compute ROC curve points (FPR, TPR) and thresholds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape (n_samples,)\n",
    "        Binary labels {0,1}.\n",
    "    y_score : array-like, shape (n_samples,)\n",
    "        Scores where higher means more likely positive.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fpr : ndarray\n",
    "        False positive rates (non-decreasing).\n",
    "    tpr : ndarray\n",
    "        True positive rates (non-decreasing).\n",
    "    thresholds : ndarray\n",
    "        Thresholds in descending score order, starting with +inf.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "\n",
    "    if y_true.shape != y_score.shape:\n",
    "        raise ValueError(\"y_true and y_score must have the same shape.\")\n",
    "\n",
    "    y_true = y_true.astype(int)\n",
    "    unique = np.unique(y_true)\n",
    "    if unique.size != 2 or not np.array_equal(unique, [0, 1]):\n",
    "        raise ValueError(f\"y_true must contain both 0 and 1 labels; got {unique}.\")\n",
    "\n",
    "    # Sort by decreasing score (stable sort => deterministic tie handling)\n",
    "    order = np.argsort(-y_score, kind=\"mergesort\")\n",
    "    y_true_sorted = y_true[order]\n",
    "    y_score_sorted = y_score[order]\n",
    "\n",
    "    pos = y_true_sorted == 1\n",
    "    neg = ~pos\n",
    "\n",
    "    n_pos = pos.sum()\n",
    "    n_neg = neg.sum()\n",
    "\n",
    "    tps = np.cumsum(pos)\n",
    "    fps = np.cumsum(neg)\n",
    "\n",
    "    # ROC points only change when the threshold passes a distinct score value.\n",
    "    distinct_score_indices = np.where(np.diff(y_score_sorted) != 0)[0]\n",
    "    threshold_indices = np.r_[distinct_score_indices, y_true_sorted.size - 1]\n",
    "\n",
    "    thresholds = y_score_sorted[threshold_indices]\n",
    "    tpr = tps[threshold_indices] / n_pos\n",
    "    fpr = fps[threshold_indices] / n_neg\n",
    "\n",
    "    # Add the (0,0) start point at threshold = +inf\n",
    "    thresholds = np.r_[np.inf, thresholds]\n",
    "    tpr = np.r_[0.0, tpr]\n",
    "    fpr = np.r_[0.0, fpr]\n",
    "\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "\n",
    "def auc_trapz(x, y):\n",
    "    \"\"\"Area under a curve via the trapezoidal rule.\n",
    "\n",
    "    Assumes x is sorted in non-decreasing order.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if x.ndim != 1 or y.ndim != 1 or x.shape != y.shape:\n",
    "        raise ValueError(\"x and y must be 1D arrays of the same length.\")\n",
    "    if np.any(np.diff(x) < 0):\n",
    "        raise ValueError(\"x must be sorted in non-decreasing order.\")\n",
    "    return float(np.trapz(y, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51564531",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve_numpy(y_true, y_score)\n",
    "auc_roc = auc_trapz(fpr, tpr)\n",
    "\n",
    "print(\"AUC (NumPy trapezoid):\", auc_roc)\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score as sk_roc_auc_score, roc_curve as sk_roc_curve\n",
    "\n",
    "    fpr_sk, tpr_sk, thr_sk = sk_roc_curve(y_true, y_score)\n",
    "    auc_sk = sk_roc_auc_score(y_true, y_score)\n",
    "    auc_sk_trapz = auc_trapz(fpr_sk, tpr_sk)\n",
    "\n",
    "    print(\"sklearn roc_curve points:\", fpr_sk.size)\n",
    "    print(\"AUC (sklearn roc_auc_score):\", auc_sk)\n",
    "    print(\"AUC (sklearn roc_curve + trapz):\", auc_sk_trapz)\n",
    "    print(\"AUC abs diff (ours vs sklearn):\", abs(auc_roc - auc_sk))\n",
    "except Exception as e:\n",
    "    print(\"sklearn check skipped ->\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_text = [\n",
    "    \"t=+inf\" if not np.isfinite(t) else f\"t={t:.3f}\" for t in thresholds\n",
    "]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=fpr,\n",
    "    y=tpr,\n",
    "    mode=\"lines+markers\",\n",
    "    name=f\"ROC (AUC={auc_roc:.3f})\",\n",
    "    text=hover_text,\n",
    "    hovertemplate=\"FPR=%{x:.3f}<br>TPR=%{y:.3f}<br>%{text}<extra></extra>\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=[0, 1],\n",
    "    y=[0, 1],\n",
    "    mode=\"lines\",\n",
    "    name=\"random (AUC=0.5)\",\n",
    "    line=dict(dash=\"dash\", color=\"gray\"),\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=[fpr_example],\n",
    "    y=[tpr_example],\n",
    "    mode=\"markers\",\n",
    "    name=f\"example t={threshold_example:.2f}\",\n",
    "    marker=dict(size=11, symbol=\"star\", color=\"black\"),\n",
    "    hovertemplate=\"FPR=%{x:.3f}<br>TPR=%{y:.3f}<br>example threshold<extra></extra>\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC curve: sweep threshold to trade FPR vs TPR\",\n",
    "    xaxis_title=\"False Positive Rate (FPR)\",\n",
    "    yaxis_title=\"True Positive Rate (TPR)\",\n",
    "    xaxis=dict(range=[0, 1], constrain=\"domain\"),\n",
    "    yaxis=dict(range=[0, 1], scaleanchor=\"x\", scaleratio=1),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f151a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How TPR/FPR evolve as the threshold moves (note: thresholds are in descending score order).\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=thresholds[1:], y=tpr[1:], mode=\"lines+markers\", name=\"TPR\")\n",
    "fig.add_scatter(x=thresholds[1:], y=fpr[1:], mode=\"lines+markers\", name=\"FPR\")\n",
    "fig.add_vline(x=threshold_example, line_width=2, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(\n",
    "    title=\"TPR and FPR as functions of the decision threshold\",\n",
    "    xaxis_title=\"threshold t (higher ⇒ stricter)\",\n",
    "    yaxis_title=\"rate\",\n",
    ")\n",
    "fig.update_xaxes(autorange=\"reversed\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bc9ee",
   "metadata": {},
   "source": [
    "## 3) AUC: a single-number summary (and what it means)\n",
    "\n",
    "The ROC curve is a whole *family* of operating points.\n",
    "A common summary is the **Area Under the ROC Curve (AUC-ROC)**.\n",
    "\n",
    "A helpful interpretation (ranking view):\n",
    "\n",
    "$$\n",
    "\\mathrm{AUC} = P(s^+ > s^-) + \\frac{1}{2} P(s^+ = s^-)\n",
    "$$\n",
    "\n",
    "So AUC is about how well the model **ranks** positives above negatives (not about calibration).\n",
    "\n",
    "AUC is convenient, but it can hide important details (for example, you might only care about very low FPR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd17fcb",
   "metadata": {},
   "source": [
    "## 4) Using ROC/AUC to tune a classifier (logistic regression from scratch)\n",
    "\n",
    "ROC curves need **scores**.\n",
    "Logistic regression produces a probability score $\\hat{p}(y=1\\mid x)$, so it's a natural match.\n",
    "\n",
    "Important nuance:\n",
    "\n",
    "- We usually **fit** logistic regression by minimizing **log loss** (it is smooth and differentiable).\n",
    "- We often **select** hyperparameters (e.g., regularization strength) by maximizing a metric like **AUC-ROC** on a validation set.\n",
    "- We then choose an **operating threshold** based on business constraints, often using the ROC curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1405cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D binary classification dataset (logistic generative story)\n",
    "n = 1200\n",
    "X = rng.normal(size=(n, 2))\n",
    "\n",
    "w_true = np.array([1.5, -2.0])\n",
    "b_true = 0.2\n",
    "\n",
    "logits = X @ w_true + b_true + rng.normal(0, 0.8, size=n)\n",
    "p = 1.0 / (1.0 + np.exp(-logits))\n",
    "y = rng.binomial(1, p).astype(int)\n",
    "\n",
    "# Train/validation split\n",
    "perm = rng.permutation(n)\n",
    "n_train = int(0.7 * n)\n",
    "train_idx = perm[:n_train]\n",
    "val_idx = perm[n_train:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "# Standardize using training statistics\n",
    "mu = X_train.mean(axis=0)\n",
    "sigma = X_train.std(axis=0) + 1e-12\n",
    "\n",
    "X_train_s = (X_train - mu) / sigma\n",
    "X_val_s = (X_val - mu) / sigma\n",
    "\n",
    "\n",
    "def add_intercept(X):\n",
    "    return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "\n",
    "Xb_train = add_intercept(X_train_s)\n",
    "Xb_val = add_intercept(X_val_s)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_val_s[:, 0],\n",
    "    y=X_val_s[:, 1],\n",
    "    color=y_val.astype(str),\n",
    "    title=\"Validation split (standardized features)\",\n",
    "    labels={\"x\": \"x1 (standardized)\", \"y\": \"x2 (standardized)\", \"color\": \"y\"},\n",
    "    opacity=0.7,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe565e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    out = np.empty_like(z)\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    exp_z = np.exp(z[~pos])\n",
    "    out[~pos] = exp_z / (1.0 + exp_z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def fit_logreg_gd(X, y, l2=0.0, lr=0.2, n_iter=2500):\n",
    "    \"\"\"Logistic regression via batch gradient descent (L2 on weights, not intercept).\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    w = np.zeros(X.shape[1], dtype=float)\n",
    "    history = np.empty(n_iter, dtype=float)\n",
    "    eps = 1e-12\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        z = X @ w\n",
    "        p = sigmoid(z)\n",
    "\n",
    "        # Regularized log loss (average)\n",
    "        data_loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
    "        reg_loss = 0.5 * l2 * np.sum(w[1:] ** 2)\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        grad = (X.T @ (p - y)) / X.shape[0]\n",
    "        grad[1:] += l2 * w[1:]\n",
    "\n",
    "        w -= lr * grad\n",
    "        history[i] = loss\n",
    "\n",
    "    return w, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ddcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning: pick L2 strength that maximizes validation AUC-ROC\n",
    "l2_grid = np.logspace(-4, 1, 10)\n",
    "\n",
    "weights_by_l2 = {}\n",
    "auc_by_l2 = []\n",
    "\n",
    "for l2 in l2_grid:\n",
    "    w, _ = fit_logreg_gd(Xb_train, y_train, l2=l2, lr=0.2, n_iter=2500)\n",
    "    weights_by_l2[float(l2)] = w\n",
    "\n",
    "    scores_val = sigmoid(Xb_val @ w)\n",
    "    fpr_v, tpr_v, _ = roc_curve_numpy(y_val, scores_val)\n",
    "    auc_v = auc_trapz(fpr_v, tpr_v)\n",
    "    auc_by_l2.append(float(auc_v))\n",
    "\n",
    "auc_by_l2 = np.array(auc_by_l2)\n",
    "best_i = int(np.argmax(auc_by_l2))\n",
    "best_l2 = float(l2_grid[best_i])\n",
    "best_auc = float(auc_by_l2[best_i])\n",
    "\n",
    "print(f\"best l2: {best_l2:g}\")\n",
    "print(f\"best validation AUC: {best_auc:.4f}\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=l2_grid,\n",
    "    y=auc_by_l2,\n",
    "    mode=\"lines+markers\",\n",
    "    name=\"validation AUC\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=[best_l2],\n",
    "    y=[best_auc],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(size=12, symbol=\"star\", color=\"black\"),\n",
    "    name=\"best\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\", title=\"L2 regularization strength (λ)\")\n",
    "fig.update_yaxes(title=\"Validation AUC-ROC\", range=[0, 1])\n",
    "fig.update_layout(title=\"Model selection: choose λ that maximizes AUC-ROC\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit the best model to visualize training loss\n",
    "best_w, loss_hist = fit_logreg_gd(Xb_train, y_train, l2=best_l2, lr=0.2, n_iter=2500)\n",
    "\n",
    "fig = px.line(\n",
    "    y=loss_hist,\n",
    "    title=f\"Training loss (regularized log loss), best λ={best_l2:g}\",\n",
    "    labels={\"x\": \"iteration\", \"y\": \"loss\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acfc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves for a few regularization settings\n",
    "candidates = [float(l2_grid[0]), best_l2, float(l2_grid[-1])]\n",
    "\n",
    "fig = go.Figure()\n",
    "for l2 in candidates:\n",
    "    w = best_w if l2 == best_l2 else weights_by_l2[l2]\n",
    "    scores_val = sigmoid(Xb_val @ w)\n",
    "    fpr_v, tpr_v, _ = roc_curve_numpy(y_val, scores_val)\n",
    "    auc_v = auc_trapz(fpr_v, tpr_v)\n",
    "    fig.add_scatter(\n",
    "        x=fpr_v,\n",
    "        y=tpr_v,\n",
    "        mode=\"lines\",\n",
    "        name=f\"λ={l2:g} (AUC={auc_v:.3f})\",\n",
    "    )\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=[0, 1],\n",
    "    y=[0, 1],\n",
    "    mode=\"lines\",\n",
    "    name=\"random (AUC=0.5)\",\n",
    "    line=dict(dash=\"dash\", color=\"gray\"),\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"ROC curves on validation set (different regularization strengths)\",\n",
    "    xaxis_title=\"False Positive Rate (FPR)\",\n",
    "    yaxis_title=\"True Positive Rate (TPR)\",\n",
    "    xaxis=dict(range=[0, 1], constrain=\"domain\"),\n",
    "    yaxis=dict(range=[0, 1], scaleanchor=\"x\", scaleratio=1),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25baf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing an operating threshold from the ROC curve\n",
    "# Example constraint: keep FPR <= 5% while maximizing TPR.\n",
    "scores_val = sigmoid(Xb_val @ best_w)\n",
    "fpr_v, tpr_v, thr_v = roc_curve_numpy(y_val, scores_val)\n",
    "\n",
    "target_fpr = 0.05\n",
    "feasible = np.where(fpr_v <= target_fpr)[0]\n",
    "\n",
    "chosen_i = int(feasible[np.argmax(tpr_v[feasible])]) if feasible.size else 0\n",
    "chosen_thr = float(thr_v[chosen_i])\n",
    "\n",
    "tp, fp, tn, fn = confusion_counts(y_val, scores_val, chosen_thr)\n",
    "precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "print(f\"target FPR <= {target_fpr:.2f}\")\n",
    "print(f\"chosen threshold: {chosen_thr:.4f}\")\n",
    "print(f\"FPR={fpr_v[chosen_i]:.4f}, TPR={tpr_v[chosen_i]:.4f}\")\n",
    "print(f\"TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "print(f\"precision={precision:.4f}, recall={recall:.4f}\")\n",
    "\n",
    "default_thr = 0.5\n",
    "tp2, fp2, tn2, fn2 = confusion_counts(y_val, scores_val, default_thr)\n",
    "tpr2 = tp2 / (tp2 + fn2)\n",
    "fpr2 = fp2 / (fp2 + tn2)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(\n",
    "    x=fpr_v,\n",
    "    y=tpr_v,\n",
    "    mode=\"lines\",\n",
    "    name=\"ROC (best model)\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=[fpr_v[chosen_i]],\n",
    "    y=[tpr_v[chosen_i]],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(size=12, symbol=\"star\", color=\"black\"),\n",
    "    name=f\"chosen (FPR≤{target_fpr:.2f})\",\n",
    "    hovertemplate=\"FPR=%{x:.3f}<br>TPR=%{y:.3f}<br>chosen threshold<extra></extra>\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=[fpr2],\n",
    "    y=[tpr2],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(size=10, symbol=\"circle\", color=\"gray\"),\n",
    "    name=\"default t=0.5\",\n",
    "    hovertemplate=\"FPR=%{x:.3f}<br>TPR=%{y:.3f}<br>t=0.5<extra></extra>\",\n",
    ")\n",
    "fig.add_vline(x=target_fpr, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(\n",
    "    title=\"Picking a threshold from ROC constraints\",\n",
    "    xaxis_title=\"False Positive Rate (FPR)\",\n",
    "    yaxis_title=\"True Positive Rate (TPR)\",\n",
    "    xaxis=dict(range=[0, 1], constrain=\"domain\"),\n",
    "    yaxis=dict(range=[0, 1], scaleanchor=\"x\", scaleratio=1),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371490a8",
   "metadata": {},
   "source": [
    "## Pros, cons, and when ROC is a good choice\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Threshold-free view**: shows the entire tradeoff curve instead of committing to one $t$.\n",
    "- **Ranking-focused**: works naturally with scores (and AUC has a clean ranking interpretation).\n",
    "- **Model comparison**: curves make it easy to compare classifiers across operating points.\n",
    "- **Less sensitive to class imbalance than accuracy** (it uses rates, not raw counts).\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- **Can look overly optimistic on very imbalanced problems**: a small FPR can still mean many false positives in absolute count.\n",
    "- **AUC can hide the region you actually care about** (e.g., only FPR < 1%). Consider partial AUC or zooming.\n",
    "- **Not about calibration**: a perfectly calibrated model and a poorly calibrated model can have the same ROC/AUC.\n",
    "- **Needs scores**: if you pass hard labels, you'll get only a couple of ROC points.\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- You care about **ranking** (retrieval, screening, triage) and want a threshold later.\n",
    "- You have a constraint like **\"FPR must be below X\"** and want the best achievable TPR.\n",
    "- Comparing multiple models when operating conditions or cost ratios are not yet fixed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed653bae",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement a **sample-weighted** ROC curve (each point has weight $w_i$).\n",
    "2. Show a case where two models have similar AUC but very different performance for **FPR < 1%**.\n",
    "3. Create an extremely imbalanced dataset and compare ROC vs **precision-recall** curves (why might PR be more informative?).\n",
    "4. Derive the ranking interpretation of AUC from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d5300c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn: `roc_curve`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "- scikit-learn: `roc_auc_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "- Tom Fawcett (2006), *An introduction to ROC analysis*: https://doi.org/10.1016/j.patrec.2005.10.010\n",
    "- Wikipedia: Receiver operating characteristic: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}