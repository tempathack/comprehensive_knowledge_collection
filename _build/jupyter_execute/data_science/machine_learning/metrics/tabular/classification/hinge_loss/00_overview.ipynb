{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725e3fe8",
   "metadata": {},
   "source": [
    "# hinge_loss\n",
    "\n",
    "Hinge loss is a **margin-based** loss for classification. It‚Äôs the standard convex surrogate behind the (soft-margin) **Support Vector Machine (SVM)**.\n",
    "\n",
    "This notebook:\n",
    "- defines **binary** and **multiclass** hinge loss with consistent notation\n",
    "- builds intuition with Plotly plots\n",
    "- implements the loss (and a useful subgradient) from scratch in NumPy\n",
    "- uses hinge loss to optimize a simple **linear classifier** (primal SVM-style)\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import hinge_loss\n",
    "```\n",
    "\n",
    "> Important: `hinge_loss` expects **decision scores** (real-valued margins), not probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24124f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import hinge_loss as skl_hinge_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3e000",
   "metadata": {},
   "source": [
    "\\\n",
    "    ## 1) Binary hinge loss (definition)\n",
    "\n",
    "    Binary classification with a **real-valued score**:\n",
    "\n",
    "    - label: $y \\in \\{-1, +1\\}$\n",
    "    - model score: $s = f(x) \\in \\mathbb{R}$\n",
    "    - prediction: $\\hat{y} = \\mathrm{sign}(s)$\n",
    "\n",
    "    The key quantity is the **(signed) margin**:\n",
    "\n",
    "    $$\n",
    "     m = y\\,s.\n",
    "    $$\n",
    "\n",
    "    - If $m > 0$, the example is classified correctly.\n",
    "    - Larger $m$ means ‚Äúmore confident‚Äù (further from the decision boundary).\n",
    "\n",
    "    The **hinge loss** is:\n",
    "\n",
    "    $$\n",
    "    \\ell(y, s) = \\max(0, 1 - y s) = \\max(0, 1 - m).\n",
    "    $$\n",
    "\n",
    "    Average hinge loss over a dataset:\n",
    "\n",
    "    $$\n",
    "    L = \\frac{1}{n}\\sum_{i=1}^n \\max(0, 1 - y_i s_i).\n",
    "    $$\n",
    "\n",
    "    ### Relationship to 0‚Äì1 loss\n",
    "\n",
    "    The 0‚Äì1 loss is $\\mathbb{1}[m \\le 0]$ (wrong sign).\n",
    "\n",
    "    Hinge loss is a **convex upper bound**:\n",
    "\n",
    "    $$\n",
    "    \\mathbb{1}[m \\le 0] \\;\\le\\; \\max(0, 1 - m).\n",
    "    $$\n",
    "\n",
    "    So minimizing hinge loss tends to reduce classification errors while also encouraging a **margin** ($m \\ge 1$ gives zero loss).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5482410",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.linspace(-3, 3, 600)\n",
    "\n",
    "loss_01 = (m <= 0).astype(float)\n",
    "loss_hinge = np.maximum(0.0, 1.0 - m)\n",
    "loss_sq_hinge = np.maximum(0.0, 1.0 - m) ** 2\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=m, y=loss_01, name=\"0-1 loss  ùüô[m‚â§0]\", line=dict(dash=\"dash\")))\n",
    "fig.add_trace(go.Scatter(x=m, y=loss_hinge, name=\"hinge  max(0, 1-m)\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=m, y=loss_sq_hinge, name=\"squared hinge (variant)\", line=dict(dash=\"dot\")))\n",
    "\n",
    "fig.add_vline(x=0, line_dash=\"dot\", line_color=\"gray\")\n",
    "fig.add_vline(x=1, line_dash=\"dot\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Loss as a function of margin  m = y¬∑score\",\n",
    "    xaxis_title=\"margin m\",\n",
    "    yaxis_title=\"loss\",\n",
    "    legend_title=\"\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb1658",
   "metadata": {},
   "source": [
    "\\\n",
    "    ## 2) Intuition: which points are penalized?\n",
    "\n",
    "    Because $\\ell(m)=\\max(0, 1-m)$:\n",
    "\n",
    "    - **Misclassified** points ($m \\le 0$) get loss $\\ge 1$.\n",
    "    - **Correct but too close** to the boundary ($0 < m < 1$) still get *some* loss.\n",
    "    - **Confident** points ($m \\ge 1$) get **zero** loss.\n",
    "\n",
    "    This is why hinge-based models often end up depending heavily on a subset of points (those with $m \\le 1$), commonly called **support vectors** in the SVM context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_samples = np.linspace(-2.5, 2.5, 60)\n",
    "loss_samples = np.maximum(0.0, 1.0 - m_samples)\n",
    "\n",
    "category = np.where(\n",
    "    m_samples <= 0,\n",
    "    \"misclassified (m ‚â§ 0)\",\n",
    "    np.where(m_samples < 1, \"correct but within margin (0 < m < 1)\", \"confident (m ‚â• 1)\"),\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=m_samples,\n",
    "    y=loss_samples,\n",
    "    color=category,\n",
    "    title=\"Only points with margin m < 1 contribute to hinge loss\",\n",
    ")\n",
    "fig.add_vline(x=0, line_dash=\"dot\", line_color=\"gray\")\n",
    "fig.add_vline(x=1, line_dash=\"dot\", line_color=\"gray\")\n",
    "fig.update_layout(xaxis_title=\"margin m\", yaxis_title=\"hinge loss\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76df53e",
   "metadata": {},
   "source": [
    "\\\n",
    "    ## 3) Multiclass hinge loss (Crammer‚ÄìSinger)\n",
    "\n",
    "    For $K$ classes, assume a score vector:\n",
    "\n",
    "    $$\n",
    "     s(x) \\in \\mathbb{R}^K, \\quad s_k(x) = \\text{score for class } k.\n",
    "    $$\n",
    "\n",
    "    If the true class is $y \\in \\{0,\\dots,K-1\\}$, the multiclass hinge loss is:\n",
    "\n",
    "    $$\n",
    "    \\ell(y, s) = \\max\\big(0, 1 + \\max_{j \\ne y} s_j - s_y\\big).\n",
    "    $$\n",
    "\n",
    "    It enforces a **margin** between the true class score and the best competing score:\n",
    "\n",
    "    $$\n",
    "     s_y \\ge \\max_{j \\ne y} s_j + 1 \\quad \\Rightarrow \\quad \\ell = 0.\n",
    "    $$\n",
    "\n",
    "    This is the formulation used by `sklearn.metrics.hinge_loss` when `pred_decision` is shaped `(n_samples, n_classes)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_1d_float(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(f\"Expected a 1D array, got shape={x.shape}\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def binary_hinge_loss(\n",
    "    y_true: np.ndarray,\n",
    "    scores: np.ndarray,\n",
    "    *,\n",
    "    margin: float = 1.0,\n",
    "    sample_weight: np.ndarray | None = None,\n",
    "    reduction: str = \"mean\",\n",
    ") -> float:\n",
    "    \"\"\"Binary hinge loss: mean_i max(0, margin - y_i * score_i).\n",
    "\n",
    "    Accepts labels in {0,1} or {-1,+1}. `scores` are raw decision scores.\n",
    "    \"\"\"\n",
    "\n",
    "    y = _as_1d_float(y_true)\n",
    "    s = _as_1d_float(scores)\n",
    "    if y.shape[0] != s.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"y_true and scores must match in length, got {y.shape[0]} vs {s.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    uniques = set(np.unique(y).tolist())\n",
    "    if uniques.issubset({0.0, 1.0}):\n",
    "        y = np.where(y == 0.0, -1.0, 1.0)\n",
    "    elif not uniques.issubset({-1.0, 1.0}):\n",
    "        raise ValueError(\n",
    "            f\"For binary hinge loss, y_true must be in {{0,1}} or {{-1,1}}, got {sorted(uniques)}\"\n",
    "        )\n",
    "\n",
    "    loss = np.maximum(0.0, margin - y * s)\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        w = _as_1d_float(sample_weight)\n",
    "        if w.shape[0] != loss.shape[0]:\n",
    "            raise ValueError(\"sample_weight must have the same length as y_true\")\n",
    "        if reduction == \"mean\":\n",
    "            return float(np.sum(w * loss) / np.sum(w))\n",
    "        if reduction == \"sum\":\n",
    "            return float(np.sum(w * loss))\n",
    "        raise ValueError(\"reduction must be 'mean' or 'sum'\")\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        return float(np.mean(loss))\n",
    "    if reduction == \"sum\":\n",
    "        return float(np.sum(loss))\n",
    "    raise ValueError(\"reduction must be 'mean' or 'sum'\")\n",
    "\n",
    "\n",
    "def multiclass_hinge_loss(\n",
    "    y_true: np.ndarray,\n",
    "    scores: np.ndarray,\n",
    "    *,\n",
    "    margin: float = 1.0,\n",
    "    sample_weight: np.ndarray | None = None,\n",
    "    reduction: str = \"mean\",\n",
    ") -> float:\n",
    "    \"\"\"Multiclass hinge loss (Crammer‚ÄìSinger): mean_i max(0, margin + max_{j!=y} s_ij - s_i,y).\n",
    "\n",
    "    `y_true` are integer class labels in [0, K-1]. `scores` has shape (n, K).\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.asarray(y_true)\n",
    "    s = np.asarray(scores, dtype=float)\n",
    "    if y.ndim != 1:\n",
    "        raise ValueError(f\"y_true must be 1D, got shape={y.shape}\")\n",
    "    if s.ndim != 2:\n",
    "        raise ValueError(f\"scores must be 2D, got shape={s.shape}\")\n",
    "    n, k = s.shape\n",
    "    if y.shape[0] != n:\n",
    "        raise ValueError(\"y_true and scores must match in n_samples\")\n",
    "\n",
    "    y = y.astype(int)\n",
    "    if y.min() < 0 or y.max() >= k:\n",
    "        raise ValueError(f\"y_true values must be in [0, {k-1}]\")\n",
    "\n",
    "    true_scores = s[np.arange(n), y]\n",
    "\n",
    "    s_other = s.copy()\n",
    "    s_other[np.arange(n), y] = -np.inf\n",
    "    max_other = np.max(s_other, axis=1)\n",
    "\n",
    "    loss = np.maximum(0.0, margin + max_other - true_scores)\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        w = _as_1d_float(sample_weight)\n",
    "        if w.shape[0] != loss.shape[0]:\n",
    "            raise ValueError(\"sample_weight must have the same length as y_true\")\n",
    "        if reduction == \"mean\":\n",
    "            return float(np.sum(w * loss) / np.sum(w))\n",
    "        if reduction == \"sum\":\n",
    "            return float(np.sum(w * loss))\n",
    "        raise ValueError(\"reduction must be 'mean' or 'sum'\")\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        return float(np.mean(loss))\n",
    "    if reduction == \"sum\":\n",
    "        return float(np.sum(loss))\n",
    "    raise ValueError(\"reduction must be 'mean' or 'sum'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fd70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Binary: compare against sklearn.metrics.hinge_loss ---\n",
    "\n",
    "y_true_01 = np.array([0, 1, 0, 1])\n",
    "score = np.array([-0.2, 0.5, 0.3, 1.2])\n",
    "\n",
    "skl = skl_hinge_loss(y_true_01, score)\n",
    "ours = binary_hinge_loss(y_true_01, score)\n",
    "print(\"binary | sklearn:\", skl)\n",
    "print(\"binary | numpy :\", ours)\n",
    "\n",
    "# --- Multiclass: compare against sklearn.metrics.hinge_loss ---\n",
    "\n",
    "y_true_mc = np.array([0, 1, 2])\n",
    "scores_mc = np.array(\n",
    "    [\n",
    "        [2.0, 0.0, -1.0],\n",
    "        [0.1, 0.2, 0.0],\n",
    "        [-1.0, 0.0, 3.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "skl_mc = skl_hinge_loss(y_true_mc, scores_mc)\n",
    "ours_mc = multiclass_hinge_loss(y_true_mc, scores_mc)\n",
    "print(\"multiclass | sklearn:\", skl_mc)\n",
    "print(\"multiclass | numpy :\", ours_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f2d11",
   "metadata": {},
   "source": [
    "\\\n",
    "    ## 4) Using hinge loss to optimize a linear classifier (soft-margin SVM style)\n",
    "\n",
    "    A common choice is a linear score function:\n",
    "\n",
    "    $$\n",
    "     s_i = f(x_i) = w^T x_i + b.\n",
    "    $$\n",
    "\n",
    "    A soft-margin (primal) SVM objective is:\n",
    "\n",
    "    $$\n",
    "    J(w,b) = \\frac{1}{2}\\lVert w \\rVert^2 + C\\,\\frac{1}{n}\\sum_{i=1}^n \\max\\big(0, 1 - y_i(w^T x_i + b)\\big).\n",
    "    $$\n",
    "\n",
    "    - The $\\tfrac12\\lVert w \\rVert^2$ term is **L2 regularization** (prefers a wider margin).\n",
    "    - $C>0$ trades off margin size vs hinge penalties.\n",
    "\n",
    "    ### Subgradient (what we need for gradient descent)\n",
    "\n",
    "    The hinge part is **not differentiable** at $m_i = 1$.\n",
    "    But it‚Äôs convex, so we can use a **subgradient**.\n",
    "\n",
    "    Let $m_i = y_i(w^T x_i + b)$ and define the ‚Äúviolators‚Äù:\n",
    "\n",
    "    $$\n",
    "    \\mathcal{V} = \\{i : m_i < 1\\}.\n",
    "    $$\n",
    "\n",
    "    A convenient subgradient is:\n",
    "\n",
    "    $$\n",
    "    \\nabla_w J = w - \\frac{C}{n}\\sum_{i\\in\\mathcal{V}} y_i x_i,\n",
    "    \\qquad\n",
    "    \\nabla_b J = - \\frac{C}{n}\\sum_{i\\in\\mathcal{V}} y_i.\n",
    "    $$\n",
    "\n",
    "    We‚Äôll implement full-batch subgradient descent below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LinearSVMHistory:\n",
    "    objective: list[float]\n",
    "    mean_hinge: list[float]\n",
    "    accuracy: list[float]\n",
    "\n",
    "\n",
    "def linear_svm_objective(\n",
    "    w: np.ndarray, b: float, X: np.ndarray, y: np.ndarray, *, C: float = 1.0\n",
    ") -> tuple[float, float]:\n",
    "    scores = X @ w + b\n",
    "    hinge = np.maximum(0.0, 1.0 - y * scores)\n",
    "    obj = 0.5 * float(w @ w) + C * float(np.mean(hinge))\n",
    "    return obj, float(np.mean(hinge))\n",
    "\n",
    "\n",
    "def linear_svm_subgrad(\n",
    "    w: np.ndarray, b: float, X: np.ndarray, y: np.ndarray, *, C: float = 1.0\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    n = X.shape[0]\n",
    "    scores = X @ w + b\n",
    "    margins = y * scores\n",
    "    viol = margins < 1.0\n",
    "\n",
    "    grad_w = w.copy()\n",
    "    grad_b = 0.0\n",
    "\n",
    "    if np.any(viol):\n",
    "        grad_w -= (C / n) * (X[viol].T @ y[viol])\n",
    "        grad_b = -(C / n) * float(np.sum(y[viol]))\n",
    "\n",
    "    return grad_w, grad_b\n",
    "\n",
    "\n",
    "def train_linear_svm_subgradient_descent(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    C: float = 1.0,\n",
    "    lr: float = 0.2,\n",
    "    n_epochs: int = 200,\n",
    "    seed: int = 42,\n",
    ") -> tuple[np.ndarray, float, LinearSVMHistory]:\n",
    "    \"\"\"Train a linear classifier with L2 + hinge using full-batch subgradient descent.\"\"\"\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    w = rng_local.normal(scale=0.01, size=X.shape[1])\n",
    "    b = 0.0\n",
    "\n",
    "    hist = LinearSVMHistory(objective=[], mean_hinge=[], accuracy=[])\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        obj, mean_hinge = linear_svm_objective(w, b, X, y, C=C)\n",
    "        scores = X @ w + b\n",
    "        y_pred = np.where(scores >= 0.0, 1.0, -1.0)\n",
    "        acc = float(np.mean(y_pred == y))\n",
    "\n",
    "        hist.objective.append(obj)\n",
    "        hist.mean_hinge.append(mean_hinge)\n",
    "        hist.accuracy.append(acc)\n",
    "\n",
    "        grad_w, grad_b = linear_svm_subgrad(w, b, X, y, C=C)\n",
    "        w = w - lr * grad_w\n",
    "        b = b - lr * grad_b\n",
    "\n",
    "    return w, b, hist\n",
    "\n",
    "\n",
    "# --- Make a simple dataset ---\n",
    "X_raw, y01 = make_blobs(n_samples=250, centers=2, cluster_std=1.8, random_state=42)\n",
    "y_pm1 = np.where(y01 == 0, -1.0, 1.0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_raw)\n",
    "\n",
    "w, b, hist = train_linear_svm_subgradient_descent(X, y_pm1, C=2.0, lr=0.15, n_epochs=220)\n",
    "\n",
    "print(\"final objective:\", hist.objective[-1])\n",
    "print(\"final mean hinge:\", hist.mean_hinge[-1])\n",
    "print(\"final accuracy :\", hist.accuracy[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b14638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "epochs = np.arange(len(hist.objective))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.06,\n",
    "    subplot_titles=(\n",
    "        \"Objective  (0.5||w||^2 + C¬∑mean_hinge)\",\n",
    "        \"Mean hinge loss\",\n",
    "        \"Accuracy\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=epochs, y=hist.objective, name=\"objective\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=hist.mean_hinge, name=\"mean hinge\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=hist.accuracy, name=\"accuracy\"), row=3, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"value\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"\", row=3, col=1, range=[0, 1.02])\n",
    "fig.update_xaxes(title_text=\"epoch\", row=3, col=1)\n",
    "\n",
    "fig.update_layout(height=700, title=\"Training curves (full-batch subgradient descent)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary + margin band in 2D\n",
    "\n",
    "x1_min, x1_max = float(X[:, 0].min() - 1.0), float(X[:, 0].max() + 1.0)\n",
    "x2_min, x2_max = float(X[:, 1].min() - 1.0), float(X[:, 1].max() + 1.0)\n",
    "\n",
    "xs = np.linspace(x1_min, x1_max, 200)\n",
    "\n",
    "w0, w1 = float(w[0]), float(w[1])\n",
    "\n",
    "\n",
    "def boundary_line(level: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return points (x1, x2) satisfying w0*x1 + w1*x2 + b = level.\"\"\"\n",
    "    if abs(w1) > 1e-10:\n",
    "        x1 = xs\n",
    "        x2 = (level - b - w0 * x1) / w1\n",
    "        return x1, x2\n",
    "\n",
    "    # Vertical line fallback\n",
    "    x1 = np.full_like(xs, (level - b) / w0)\n",
    "    x2 = np.linspace(x2_min, x2_max, xs.shape[0])\n",
    "    return x1, x2\n",
    "\n",
    "\n",
    "margins = y_pm1 * (X @ w + b)\n",
    "support = margins <= 1.0 + 1e-12\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# points by class\n",
    "for cls, color in [(-1.0, \"#1f77b4\"), (1.0, \"#d62728\")]:\n",
    "    mask = y_pm1 == cls\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X[mask, 0],\n",
    "            y=X[mask, 1],\n",
    "            mode=\"markers\",\n",
    "            name=f\"y={int(cls)}\",\n",
    "            marker=dict(size=8, color=color, line=dict(width=0)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# highlight support vectors\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[support, 0],\n",
    "        y=X[support, 1],\n",
    "        mode=\"markers\",\n",
    "        name=\"support (m ‚â§ 1)\",\n",
    "        marker=dict(size=14, color=\"rgba(0,0,0,0)\", line=dict(width=2, color=\"black\")),\n",
    "    )\n",
    ")\n",
    "\n",
    "# decision boundary and margins\n",
    "for level, name, dash, width, color in [\n",
    "    (0.0, \"decision f(x)=0\", \"solid\", 3, \"black\"),\n",
    "    (1.0, \"+margin f(x)=+1\", \"dash\", 2, \"gray\"),\n",
    "    (-1.0, \"-margin f(x)=-1\", \"dash\", 2, \"gray\"),\n",
    "]:\n",
    "    x1, x2 = boundary_line(level)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x1,\n",
    "            y=x2,\n",
    "            mode=\"lines\",\n",
    "            name=name,\n",
    "            line=dict(dash=dash, width=width, color=color),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Learned linear classifier with hinge loss (support vectors highlighted)\",\n",
    "    xaxis_title=\"x1 (scaled)\",\n",
    "    yaxis_title=\"x2 (scaled)\",\n",
    ")\n",
    "fig.update_xaxes(range=[x1_min, x1_max])\n",
    "fig.update_yaxes(range=[x2_min, x2_max])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fcc69",
   "metadata": {},
   "source": [
    "\\\n",
    "    ## 5) The role of `C` (regularization trade-off)\n",
    "\n",
    "    In the objective\n",
    "\n",
    "    $$\n",
    "    \\tfrac12\\lVert w\\rVert^2 + C\\,\\text{mean hinge},\n",
    "    $$\n",
    "\n",
    "    - **small `C`**: regularization dominates ‚Üí wider margin, more tolerance for violations\n",
    "    - **large `C`**: hinge penalties dominate ‚Üí tries harder to fit training points (narrower margin)\n",
    "\n",
    "    Below we train three models with different `C` values and compare the resulting decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badd8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "Cs = [0.2, 2.0, 20.0]\n",
    "models: list[tuple[float, np.ndarray, float]] = []\n",
    "\n",
    "for C in Cs:\n",
    "    w_c, b_c, _ = train_linear_svm_subgradient_descent(\n",
    "        X, y_pm1, C=C, lr=0.15, n_epochs=220, seed=42\n",
    "    )\n",
    "    models.append((C, w_c, b_c))\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(Cs), subplot_titles=[f\"C={C}\" for C in Cs])\n",
    "\n",
    "for col, (C, w_c, b_c) in enumerate(models, start=1):\n",
    "    # data\n",
    "    for cls, color in [(-1.0, \"#1f77b4\"), (1.0, \"#d62728\")]:\n",
    "        mask = y_pm1 == cls\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X[mask, 0],\n",
    "                y=X[mask, 1],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=6, color=color),\n",
    "                showlegend=(col == 1),\n",
    "                name=f\"y={int(cls)}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "    # boundary (only f(x)=0 to keep it readable)\n",
    "    w0, w1 = float(w_c[0]), float(w_c[1])\n",
    "    if abs(w1) > 1e-10:\n",
    "        x1 = xs\n",
    "        x2 = (0.0 - b_c - w0 * x1) / w1\n",
    "    else:\n",
    "        x1 = np.full_like(xs, (0.0 - b_c) / w0)\n",
    "        x2 = np.linspace(x2_min, x2_max, xs.shape[0])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x1,\n",
    "            y=x2,\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=3, color=\"black\"),\n",
    "            showlegend=False,\n",
    "            name=\"boundary\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=420,\n",
    "    title=\"Effect of C on the learned decision boundary\",\n",
    ")\n",
    "for col in range(1, len(Cs) + 1):\n",
    "    fig.update_xaxes(title_text=\"x1\", range=[x1_min, x1_max], row=1, col=col)\n",
    "    fig.update_yaxes(title_text=\"x2\", range=[x2_min, x2_max], row=1, col=col)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6cc32",
   "metadata": {},
   "source": [
    "## 6) Practical usage: `sklearn.metrics.hinge_loss`\n",
    "\n",
    "`sklearn.metrics.hinge_loss(y_true, pred_decision, ...)` expects:\n",
    "\n",
    "- **binary**: `pred_decision.shape == (n_samples,)` (a real-valued decision score)\n",
    "- **multiclass**: `pred_decision.shape == (n_samples, n_classes)` (one score per class)\n",
    "\n",
    "A common workflow:\n",
    "\n",
    "1) train a classifier that exposes `decision_function`\n",
    "2) compute `pred_decision = model.decision_function(X)`\n",
    "3) evaluate with `hinge_loss(y_true, pred_decision)`\n",
    "\n",
    "Below we fit `LinearSVC` and compare `sklearn`‚Äôs hinge loss to our NumPy implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52eae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=2.0, dual=True, random_state=42)\n",
    "clf.fit(X, y01)\n",
    "\n",
    "dec = clf.decision_function(X)\n",
    "\n",
    "skl = skl_hinge_loss(y01, dec)\n",
    "ours = binary_hinge_loss(np.where(y01 == 0, -1.0, 1.0), dec)\n",
    "\n",
    "print(\"sklearn hinge_loss:\", skl)\n",
    "print(\"numpy  hinge_loss:\", ours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e24c7",
   "metadata": {},
   "source": [
    "\\\n",
    "    ## 7) Pros, cons, and when to use hinge loss\n",
    "\n",
    "    ### Pros\n",
    "\n",
    "    - **Convex** (for linear models): optimization is well-behaved (no local minima).\n",
    "    - **Margin-aware**: doesn‚Äôt just separate classes; encourages a safety buffer.\n",
    "    - **Sparse dependence on data** (SVM view): only points with $m \\le 1$ influence the solution.\n",
    "    - Often strong performance for **high-dimensional** classification (e.g., text with bag-of-words / TF-IDF).\n",
    "\n",
    "    ### Cons\n",
    "\n",
    "    - **Non-smooth** at $m=1$ (requires subgradients or a smoothed variant).\n",
    "    - Produces **uncalibrated scores** (unlike logistic loss, it‚Äôs not a log-likelihood).\n",
    "    - Not ideal when you need **probabilities** or well-calibrated uncertainty.\n",
    "    - Can be sensitive to **label noise** near the boundary (like most margin-based methods).\n",
    "\n",
    "    ### Good use cases\n",
    "\n",
    "    - Binary or multiclass classification when you care about **large margins**.\n",
    "    - Linear classification on large, sparse feature spaces (classic SVM territory).\n",
    "    - As a surrogate for the 0‚Äì1 loss when you need a convex objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5583a4f",
   "metadata": {},
   "source": [
    "## 8) Common pitfalls and diagnostics\n",
    "\n",
    "- **Use decision scores**: hinge loss needs raw scores (e.g., `decision_function`), not probabilities.\n",
    "- **Label encoding**: math is cleanest with $y\\in\\{-1,+1\\}$; many libraries accept `{0,1}` but be explicit.\n",
    "- **Feature scaling**: for linear models with L2 regularization, scaling can strongly affect the margin and the effective regularization.\n",
    "- **Class imbalance**: hinge loss itself doesn‚Äôt fix imbalance; consider class weights or re-sampling.\n",
    "- **Interpretation**: a lower hinge loss generally means larger margins, but it‚Äôs not a calibrated probability of correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21bb386",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Implement **squared hinge loss** and compare optimization behavior (smoother gradients).\n",
    "2) Add **L1 regularization** and see how it changes sparsity in `w`.\n",
    "3) Compare hinge vs logistic loss on the same dataset: decision boundary, calibration, and outliers.\n",
    "4) Implement **SGD** (mini-batches) for the hinge objective and compare convergence.\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html\n",
    "- Vapnik, *The Nature of Statistical Learning Theory*\n",
    "- Cortes & Vapnik (1995), *Support-vector networks*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}