{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bf98ba",
   "metadata": {},
   "source": [
    "# balanced_accuracy_score\n",
    "\n",
    "Balanced accuracy is the **macro-average of recall**: it computes recall separately for each class and then averages across classes.\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "```\n",
    "\n",
    "It is especially useful when:\n",
    "- the dataset is **imbalanced**\n",
    "- you want to treat classes **equally** (e.g., you care about minority recall as much as majority recall)\n",
    "\n",
    "**Goals**\n",
    "- Build intuition (why accuracy can be misleading)\n",
    "- Derive the metric for **binary** and **multiclass** classification\n",
    "- Implement `balanced_accuracy_score` **from scratch** in NumPy\n",
    "- Visualize per-class recall and **threshold effects** (Plotly)\n",
    "- Use balanced accuracy to guide a simple optimization loop (from-scratch logistic regression)\n",
    "\n",
    "**Prerequisites**\n",
    "- Confusion matrix, recall (TPR), specificity (TNR)\n",
    "- Probabilistic classifiers (logistic regression outputs probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63584b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score as sk_balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc9ea7",
   "metadata": {},
   "source": [
    "## 1) Why not plain accuracy?\n",
    "\n",
    "**Accuracy** is\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\#\\{i : \\hat{y}_i = y_i\\}}{n}.\n",
    "$$\n",
    "\n",
    "If one class dominates, a model can achieve high accuracy by mostly predicting the majority class.\n",
    "\n",
    "Example: 99% negatives, 1% positives.\n",
    "- A classifier that predicts **always negative** gets **99% accuracy**.\n",
    "- But it has **0% recall** on the positive class.\n",
    "\n",
    "Balanced accuracy fixes this by computing recall per class and averaging them, so each class contributes equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e7c88",
   "metadata": {},
   "source": [
    "## 2) Definition (binary classification)\n",
    "\n",
    "For $y \\in \\{0,1\\}$ and predictions $\\hat{y} \\in \\{0,1\\}$:\n",
    "\n",
    "|             | $\\hat{y}=0$ | $\\hat{y}=1$ |\n",
    "|-------------|-------------|-------------|\n",
    "| $y=0$       | TN          | FP          |\n",
    "| $y=1$       | FN          | TP          |\n",
    "\n",
    "Two key rates:\n",
    "\n",
    "- **Recall / sensitivity / TPR**\n",
    "\n",
    "$$\n",
    "\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "- **Specificity / TNR**\n",
    "\n",
    "$$\n",
    "\\text{TNR} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "**Balanced accuracy** is the mean of these two:\n",
    "\n",
    "$$\n",
    "\\text{BA} = \\frac{1}{2}(\\text{TPR} + \\text{TNR}).\n",
    "$$\n",
    "\n",
    "It is also related to the **balanced error rate** (BER):\n",
    "\n",
    "$$\n",
    "\\text{BER} = 1 - \\text{BA} = \\tfrac{1}{2}(\\text{FNR} + \\text{FPR}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bccfa3",
   "metadata": {},
   "source": [
    "## 3) Definition (multiclass + adjusted)\n",
    "\n",
    "For $K$ classes, balanced accuracy is the **average recall per class** (macro recall):\n",
    "\n",
    "$$\n",
    "\\text{BA} = \\frac{1}{K} \\sum_{k=1}^K \\text{Recall}_k\n",
    "\\qquad\\text{where}\\qquad\n",
    "\\text{Recall}_k = \\frac{\\text{TP}_k}{\\text{TP}_k + \\text{FN}_k}.\n",
    "$$\n",
    "\n",
    "So balanced accuracy is exactly:\n",
    "\n",
    "$$\n",
    "\\text{BA} = \\texttt{recall\\_score}(\\text{average}=\"macro\").\n",
    "$$\n",
    "\n",
    "### Adjusted balanced accuracy\n",
    "scikit-learn also offers a chance-corrected version:\n",
    "\n",
    "$$\n",
    "\\text{BA}_{\\text{adj}} = \\frac{\\text{BA} - 1/K}{1 - 1/K}.\n",
    "$$\n",
    "\n",
    "- A classifier that effectively behaves like random guessing tends toward $\\text{BA}_{\\text{adj}} \\approx 0$.\n",
    "- Perfect classification gives $\\text{BA}_{\\text{adj}} = 1$.\n",
    "- Worse-than-chance can be negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_np(y_true, y_pred, sample_weight=None) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    correct = (y_true == y_pred).astype(float)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return float(correct.mean())\n",
    "\n",
    "    w = np.asarray(sample_weight, dtype=float)\n",
    "    return float(np.sum(w * correct) / np.sum(w))\n",
    "\n",
    "\n",
    "def per_class_recall_np(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=None,\n",
    "    sample_weight=None,\n",
    "    zero_division: float = 0.0,\n",
    "):\n",
    "    # Per-class recall:\n",
    "    #   recall_k = (# predicted as k among true k) / (# true k)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_true)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones_like(y_true, dtype=float)\n",
    "    else:\n",
    "        sample_weight = np.asarray(sample_weight, dtype=float)\n",
    "\n",
    "    recalls = np.empty(len(labels), dtype=float)\n",
    "\n",
    "    for i, cls in enumerate(labels):\n",
    "        mask = y_true == cls\n",
    "        denom = float(sample_weight[mask].sum())\n",
    "        if denom == 0.0:\n",
    "            recalls[i] = zero_division\n",
    "        else:\n",
    "            num = float(sample_weight[mask & (y_pred == cls)].sum())\n",
    "            recalls[i] = num / denom\n",
    "\n",
    "    return recalls, labels\n",
    "\n",
    "\n",
    "def balanced_accuracy_score_np(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    labels=None,\n",
    "    sample_weight=None,\n",
    "    adjusted: bool = False,\n",
    "    zero_division: float = 0.0,\n",
    ") -> float:\n",
    "    recalls, labels_used = per_class_recall_np(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=labels,\n",
    "        sample_weight=sample_weight,\n",
    "        zero_division=zero_division,\n",
    "    )\n",
    "    score = float(np.mean(recalls))\n",
    "\n",
    "    if not adjusted:\n",
    "        return score\n",
    "\n",
    "    n_classes = len(labels_used)\n",
    "    if n_classes <= 1:\n",
    "        return 1.0\n",
    "\n",
    "    chance = 1.0 / n_classes\n",
    "    return float((score - chance) / (1.0 - chance))\n",
    "\n",
    "\n",
    "def confusion_matrix_np(y_true, y_pred, labels=None, sample_weight=None):\n",
    "    # Small confusion-matrix helper (mainly for plotting)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "    true_idx = np.array([label_to_index.get(v, -1) for v in y_true], dtype=int)\n",
    "    pred_idx = np.array([label_to_index.get(v, -1) for v in y_pred], dtype=int)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones_like(true_idx, dtype=float)\n",
    "    else:\n",
    "        sample_weight = np.asarray(sample_weight, dtype=float)\n",
    "\n",
    "    cm = np.zeros((len(labels), len(labels)), dtype=float)\n",
    "    valid = (true_idx >= 0) & (pred_idx >= 0)\n",
    "    np.add.at(cm, (true_idx[valid], pred_idx[valid]), sample_weight[valid])\n",
    "\n",
    "    return cm, labels\n",
    "\n",
    "\n",
    "# quick sanity check vs scikit-learn\n",
    "_y_true = np.array([0, 0, 0, 1, 1, 1])\n",
    "_y_pred = np.array([0, 0, 1, 0, 1, 1])\n",
    "print('ours:', balanced_accuracy_score_np(_y_true, _y_pred))\n",
    "print('sklearn:', sk_balanced_accuracy_score(_y_true, _y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d27718",
   "metadata": {},
   "source": [
    "## 4) Worked example: \"always predict the majority class\"\n",
    "\n",
    "Let’s build an extremely imbalanced dataset and evaluate a trivial classifier.\n",
    "\n",
    "- 990 negatives (class 0)\n",
    "- 10 positives (class 1)\n",
    "- predictions: always class 0\n",
    "\n",
    "This classifier gets excellent **accuracy**, but poor minority-class performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9344d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg, n_pos = 990, 10\n",
    "\n",
    "y_true = np.array([0] * n_neg + [1] * n_pos)\n",
    "y_pred = np.zeros_like(y_true)\n",
    "\n",
    "acc = accuracy_score_np(y_true, y_pred)\n",
    "bal = balanced_accuracy_score_np(y_true, y_pred)\n",
    "bal_adj = balanced_accuracy_score_np(y_true, y_pred, adjusted=True)\n",
    "recalls, labels = per_class_recall_np(y_true, y_pred)\n",
    "\n",
    "print(f\"accuracy:          {acc:.4f}\")\n",
    "print(f\"balanced accuracy: {bal:.4f}\")\n",
    "print(f\"adjusted BA:       {bal_adj:.4f}\")\n",
    "print(\"per-class recall:\", dict(zip(labels.tolist(), recalls.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74758288",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, cm_labels = confusion_matrix_np(y_true, y_pred)\n",
    "\n",
    "fig = px.imshow(\n",
    "    cm,\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    x=[f\"pred={l}\" for l in cm_labels],\n",
    "    y=[f\"true={l}\" for l in cm_labels],\n",
    ")\n",
    "fig.update_layout(title=\"Confusion matrix: always predicting class 0\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(\n",
    "            x=[str(l) for l in labels],\n",
    "            y=recalls,\n",
    "            text=[f\"{r:.2f}\" for r in recalls],\n",
    "            textposition=\"auto\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Per-class recall (balanced accuracy is the mean of these)\",\n",
    "    xaxis_title=\"class\",\n",
    "    yaxis_title=\"recall\",\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57e1d3",
   "metadata": {},
   "source": [
    "## 5) Threshold dependence (probabilities → labels)\n",
    "\n",
    "Balanced accuracy is defined on **hard** predictions ($\\hat{y}$).\n",
    "\n",
    "If your model outputs probabilities $p(x) = P(y=1\\mid x)$, you still need a **decision threshold** $t$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbb{1}[p(x) \\ge t].\n",
    "$$\n",
    "\n",
    "Changing $t$ changes the confusion matrix, hence recall per class, hence balanced accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5803fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple probability simulation (overlapping scores + class imbalance)\n",
    "\n",
    "n_neg, n_pos = 2000, 100\n",
    "\n",
    "y_true = np.array([0] * n_neg + [1] * n_pos)\n",
    "\n",
    "# Negatives tend to have lower predicted probabilities, positives higher, but overlapping.\n",
    "p_neg = rng.beta(2.0, 8.0, size=n_neg)\n",
    "p_pos = rng.beta(5.0, 5.0, size=n_pos)\n",
    "\n",
    "proba = np.concatenate([p_neg, p_pos])\n",
    "\n",
    "# Shuffle together\n",
    "perm = rng.permutation(len(y_true))\n",
    "y_true = y_true[perm]\n",
    "proba = proba[perm]\n",
    "\n",
    "thresholds = np.linspace(0.0, 1.0, 401)\n",
    "accs = np.empty_like(thresholds)\n",
    "bals = np.empty_like(thresholds)\n",
    "\n",
    "for i, t in enumerate(thresholds):\n",
    "    y_pred = (proba >= t).astype(int)\n",
    "    accs[i] = accuracy_score_np(y_true, y_pred)\n",
    "    bals[i] = balanced_accuracy_score_np(y_true, y_pred)\n",
    "\n",
    "best_t = float(thresholds[np.argmax(bals)])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=accs, name=\"accuracy\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=bals, name=\"balanced accuracy\", mode=\"lines\"))\n",
    "fig.add_vline(x=best_t, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(\n",
    "    title=f\"Accuracy vs balanced accuracy as a function of threshold (best BA at t={best_t:.3f})\",\n",
    "    xaxis_title=\"threshold t\",\n",
    "    yaxis_title=\"score\",\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec7b1e",
   "metadata": {},
   "source": [
    "## 6) Using balanced accuracy to guide an optimization loop (logistic regression)\n",
    "\n",
    "Balanced accuracy is **not differentiable** w.r.t. model parameters because it depends on discrete decisions (argmax / threshold).\n",
    "\n",
    "In practice, we typically:\n",
    "\n",
    "1) Train a probabilistic classifier with a differentiable loss (e.g., log loss)\n",
    "2) Use balanced accuracy as a **model selection** criterion:\n",
    "   - choose hyperparameters\n",
    "   - choose early-stopping epoch\n",
    "   - choose decision threshold\n",
    "\n",
    "A common surrogate that often improves balanced accuracy is to train with **class weights** (roughly: make each class contribute equally to the loss).\n",
    "\n",
    "Below we train logistic regression from scratch in two ways:\n",
    "- **Unweighted log loss**\n",
    "- **Class-weighted log loss** (\"balanced\" weights)\n",
    "\n",
    "…and monitor **validation balanced accuracy** for early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D imbalanced dataset (mild overlap)\n",
    "\n",
    "n0, n1 = 1200, 80\n",
    "\n",
    "X0 = rng.normal(loc=(0.0, 0.0), scale=1.0, size=(n0, 2))\n",
    "X1 = rng.normal(loc=(1.2, 1.2), scale=1.0, size=(n1, 2))\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.concatenate([np.zeros(n0, dtype=int), np.ones(n1, dtype=int)])\n",
    "\n",
    "perm = rng.permutation(len(y))\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y.astype(str),\n",
    "    opacity=0.7,\n",
    "    title=\"Synthetic imbalanced dataset\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"class\"},\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print('train class counts:', {0: int((y_train==0).sum()), 1: int((y_train==1).sum())})\n",
    "print('val class counts:  ', {0: int((y_val==0).sum()), 1: int((y_val==1).sum())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce19a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_fit(X):\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0) + 1e-12\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def standardize_transform(X, mean, std):\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def add_intercept(X):\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def predict_proba_logreg(X, w):\n",
    "    Xb = add_intercept(X)\n",
    "    return expit(Xb @ w)\n",
    "\n",
    "\n",
    "def log_loss_binary(y, p, sample_weight=None, eps: float = 1e-12) -> float:\n",
    "    y = np.asarray(y)\n",
    "    p = np.clip(np.asarray(p), eps, 1.0 - eps)\n",
    "\n",
    "    per_sample = -(y * np.log(p) + (1.0 - y) * np.log(1.0 - p))\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return float(per_sample.mean())\n",
    "\n",
    "    w = np.asarray(sample_weight, dtype=float)\n",
    "    return float(np.sum(w * per_sample) / np.sum(w))\n",
    "\n",
    "\n",
    "def fit_logreg_gd(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    *,\n",
    "    lr: float = 0.2,\n",
    "    n_epochs: int = 400,\n",
    "    l2: float = 1e-2,\n",
    "    sample_weight=None,\n",
    "):\n",
    "    # Binary logistic regression with (optional) sample weights + early stopping on val BA\n",
    "    Xb = add_intercept(X_train)\n",
    "    n, d = Xb.shape\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(n, dtype=float)\n",
    "    else:\n",
    "        sample_weight = np.asarray(sample_weight, dtype=float)\n",
    "\n",
    "    sw_sum = float(sample_weight.sum())\n",
    "    w = np.zeros(d, dtype=float)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_bal_acc\": [],\n",
    "    }\n",
    "\n",
    "    best = {\n",
    "        \"epoch\": -1,\n",
    "        \"val_bal_acc\": -np.inf,\n",
    "        \"w\": w.copy(),\n",
    "    }\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # forward + gradient on train\n",
    "        p_train = expit(Xb @ w)\n",
    "        grad = (Xb.T @ (sample_weight * (p_train - y_train))) / sw_sum\n",
    "        grad[1:] += l2 * w[1:]\n",
    "\n",
    "        w = w - lr * grad\n",
    "\n",
    "        # metrics\n",
    "        p_train = expit(Xb @ w)\n",
    "        train_loss = log_loss_binary(y_train, p_train, sample_weight=sample_weight) + 0.5 * l2 * float(\n",
    "            np.sum(w[1:] ** 2)\n",
    "        )\n",
    "\n",
    "        p_val = predict_proba_logreg(X_val, w)\n",
    "        y_val_hat = (p_val >= 0.5).astype(int)\n",
    "\n",
    "        val_acc = accuracy_score_np(y_val, y_val_hat)\n",
    "        val_bal_acc = balanced_accuracy_score_np(y_val, y_val_hat)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_bal_acc\"].append(val_bal_acc)\n",
    "\n",
    "        if val_bal_acc > best[\"val_bal_acc\"]:\n",
    "            best = {\"epoch\": epoch, \"val_bal_acc\": val_bal_acc, \"w\": w.copy()}\n",
    "\n",
    "    return best[\"w\"], history, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582245a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for GD stability)\n",
    "mean, std = standardize_fit(X_train)\n",
    "X_train_s = standardize_transform(X_train, mean, std)\n",
    "X_val_s = standardize_transform(X_val, mean, std)\n",
    "\n",
    "# Unweighted training\n",
    "w_unw, hist_unw, best_unw = fit_logreg_gd(X_train_s, y_train, X_val_s, y_val)\n",
    "\n",
    "# Balanced class weights: each class gets ~50% of total weight\n",
    "n_train = len(y_train)\n",
    "n_pos = int((y_train == 1).sum())\n",
    "n_neg = int((y_train == 0).sum())\n",
    "\n",
    "w_pos = n_train / (2.0 * n_pos)\n",
    "w_neg = n_train / (2.0 * n_neg)\n",
    "sw_bal = np.where(y_train == 1, w_pos, w_neg)\n",
    "\n",
    "w_wt, hist_wt, best_wt = fit_logreg_gd(X_train_s, y_train, X_val_s, y_val, sample_weight=sw_bal)\n",
    "\n",
    "print('best epoch (unweighted):', best_unw['epoch'], 'val BA:', f\"{best_unw['val_bal_acc']:.4f}\")\n",
    "print('best epoch (weighted):  ', best_wt['epoch'], 'val BA:', f\"{best_wt['val_bal_acc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b94dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(hist_unw[\"train_loss\"]) + 1)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\"Train log loss\", \"Validation accuracy\", \"Validation balanced accuracy\"),\n",
    ")\n",
    "\n",
    "for name, hist in [(\"unweighted\", hist_unw), (\"class-weighted\", hist_wt)]:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=hist[\"train_loss\"], name=f\"{name} loss\", mode=\"lines\"),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=hist[\"val_acc\"], name=f\"{name} acc\", mode=\"lines\"),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=hist[\"val_bal_acc\"], name=f\"{name} BA\", mode=\"lines\"),\n",
    "        row=1,\n",
    "        col=3,\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=350, width=1100, title=\"Training curves (early stopping uses validation BA)\")\n",
    "fig.update_yaxes(range=[0, 1], row=1, col=2)\n",
    "fig.update_yaxes(range=[0, 1], row=1, col=3)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f69683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold_for_balanced_accuracy(y_true, proba, thresholds):\n",
    "    best = {\"t\": None, \"ba\": -np.inf}\n",
    "    for t in thresholds:\n",
    "        y_pred = (proba >= t).astype(int)\n",
    "        ba = balanced_accuracy_score_np(y_true, y_pred)\n",
    "        if ba > best[\"ba\"]:\n",
    "            best = {\"t\": float(t), \"ba\": float(ba)}\n",
    "    return best\n",
    "\n",
    "\n",
    "thresholds = np.linspace(0.0, 1.0, 401)\n",
    "\n",
    "p_unw = predict_proba_logreg(X_val_s, w_unw)\n",
    "p_wt = predict_proba_logreg(X_val_s, w_wt)\n",
    "\n",
    "best_t_unw = best_threshold_for_balanced_accuracy(y_val, p_unw, thresholds)\n",
    "best_t_wt = best_threshold_for_balanced_accuracy(y_val, p_wt, thresholds)\n",
    "\n",
    "print('best threshold (unweighted):', best_t_unw)\n",
    "print('best threshold (weighted):  ', best_t_wt)\n",
    "\n",
    "# Visualize BA(t)\n",
    "ba_unw = [balanced_accuracy_score_np(y_val, (p_unw >= t).astype(int)) for t in thresholds]\n",
    "ba_wt = [balanced_accuracy_score_np(y_val, (p_wt >= t).astype(int)) for t in thresholds]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=ba_unw, name=\"unweighted\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=ba_wt, name=\"class-weighted\", mode=\"lines\"))\n",
    "fig.add_vline(x=best_t_unw[\"t\"], line_dash=\"dash\", line_color=\"#1f77b4\")\n",
    "fig.add_vline(x=best_t_wt[\"t\"], line_dash=\"dash\", line_color=\"#ff7f0e\")\n",
    "fig.update_layout(\n",
    "    title=\"Validation balanced accuracy as a function of the decision threshold\",\n",
    "    xaxis_title=\"threshold t\",\n",
    "    yaxis_title=\"balanced accuracy\",\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_threshold(y_true, proba, t):\n",
    "    y_pred = (proba >= t).astype(int)\n",
    "    acc = accuracy_score_np(y_true, y_pred)\n",
    "    ba = balanced_accuracy_score_np(y_true, y_pred)\n",
    "    recalls, labels = per_class_recall_np(y_true, y_pred)\n",
    "    cm, _ = confusion_matrix_np(y_true, y_pred, labels=np.array([0, 1]))\n",
    "    return {\n",
    "        \"t\": float(t),\n",
    "        \"acc\": float(acc),\n",
    "        \"ba\": float(ba),\n",
    "        \"recalls\": dict(zip(labels.tolist(), recalls.tolist())),\n",
    "        \"cm\": cm,\n",
    "    }\n",
    "\n",
    "\n",
    "summaries = {\n",
    "    \"unweighted @0.5\": summarize_threshold(y_val, p_unw, 0.5),\n",
    "    \"unweighted @t*\": summarize_threshold(y_val, p_unw, best_t_unw[\"t\"]),\n",
    "    \"weighted @0.5\": summarize_threshold(y_val, p_wt, 0.5),\n",
    "    \"weighted @t*\": summarize_threshold(y_val, p_wt, best_t_wt[\"t\"]),\n",
    "}\n",
    "\n",
    "for k, v in summaries.items():\n",
    "    print(k, {\"t\": v[\"t\"], \"acc\": v[\"acc\"], \"ba\": v[\"ba\"], \"recalls\": v[\"recalls\"]})\n",
    "\n",
    "# Confusion matrices (2x2): rows=methods, cols=threshold choice\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Unweighted @0.5\",\n",
    "        \"Unweighted @t*\",\n",
    "        \"Weighted @0.5\",\n",
    "        \"Weighted @t*\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "items = [\n",
    "    (1, 1, summaries[\"unweighted @0.5\"]),\n",
    "    (1, 2, summaries[\"unweighted @t*\"]),\n",
    "    (2, 1, summaries[\"weighted @0.5\"]),\n",
    "    (2, 2, summaries[\"weighted @t*\"]),\n",
    "]\n",
    "\n",
    "for r, c, s in items:\n",
    "    cm = s[\"cm\"]\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=cm,\n",
    "            x=[\"pred=0\", \"pred=1\"],\n",
    "            y=[\"true=0\", \"true=1\"],\n",
    "            colorscale=\"Blues\",\n",
    "            showscale=False,\n",
    "            text=cm.astype(int),\n",
    "            texttemplate=\"%{text}\",\n",
    "        ),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=650, width=900, title=\"Validation confusion matrices\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary visualization (in original feature space)\n",
    "\n",
    "def decision_boundary_figure(X_val, y_val, w, mean, std, threshold: float, title: str):\n",
    "    x1_min, x1_max = X_val[:, 0].min() - 1.0, X_val[:, 0].max() + 1.0\n",
    "    x2_min, x2_max = X_val[:, 1].min() - 1.0, X_val[:, 1].max() + 1.0\n",
    "\n",
    "    xs = np.linspace(x1_min, x1_max, 200)\n",
    "    ys = np.linspace(x2_min, x2_max, 200)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_s = standardize_transform(grid, mean, std)\n",
    "\n",
    "    p = predict_proba_logreg(grid_s, w).reshape(xx.shape)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=xs,\n",
    "            y=ys,\n",
    "            z=p,\n",
    "            contours=dict(start=threshold, end=threshold, size=1, coloring=\"lines\"),\n",
    "            line=dict(color=\"black\", width=3),\n",
    "            showscale=False,\n",
    "            name=\"decision boundary\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_val[:, 0],\n",
    "            y=X_val[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=y_val,\n",
    "                colorscale=[[0, \"#1f77b4\"], [1, \"#d62728\"]],\n",
    "                opacity=0.7,\n",
    "                line=dict(width=0),\n",
    "            ),\n",
    "            name=\"validation points\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"x1\",\n",
    "        yaxis_title=\"x2\",\n",
    "        height=450,\n",
    "        width=500,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig1 = decision_boundary_figure(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    w_unw,\n",
    "    mean,\n",
    "    std,\n",
    "    threshold=best_t_unw[\"t\"],\n",
    "    title=f\"Unweighted logistic regression (threshold t*={best_t_unw['t']:.2f})\",\n",
    ")\n",
    "fig2 = decision_boundary_figure(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    w_wt,\n",
    "    mean,\n",
    "    std,\n",
    "    threshold=best_t_wt[\"t\"],\n",
    "    title=f\"Class-weighted logistic regression (threshold t*={best_t_wt['t']:.2f})\",\n",
    ")\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(fig1.layout.title.text, fig2.layout.title.text))\n",
    "for tr in fig1.data:\n",
    "    fig.add_trace(tr, row=1, col=1)\n",
    "for tr in fig2.data:\n",
    "    fig.add_trace(tr, row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=450, width=1050, title=\"Decision boundary tuned for balanced accuracy\")\n",
    "fig.update_xaxes(title_text=\"x1\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"x2\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x1\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"x2\", row=1, col=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669e44c",
   "metadata": {},
   "source": [
    "## 7) Practical `scikit-learn` usage\n",
    "\n",
    "### Metric\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy_score(y_true, y_pred)\n",
    "balanced_accuracy_score(y_true, y_pred, adjusted=True)\n",
    "```\n",
    "\n",
    "### Model selection\n",
    "\n",
    "- In `GridSearchCV` / `cross_val_score`, use `scoring=\"balanced_accuracy\"`.\n",
    "- Many estimators support `class_weight=\"balanced\"`, which often improves balanced accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e271145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn comparison on the same dataset\n",
    "\n",
    "clf_unw = LogisticRegression(max_iter=2000)\n",
    "clf_wt = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "\n",
    "clf_unw.fit(X_train, y_train)\n",
    "clf_wt.fit(X_train, y_train)\n",
    "\n",
    "pred_unw = clf_unw.predict(X_val)\n",
    "pred_wt = clf_wt.predict(X_val)\n",
    "\n",
    "print('sklearn unweighted BA:', sk_balanced_accuracy_score(y_val, pred_unw))\n",
    "print('sklearn weighted BA:  ', sk_balanced_accuracy_score(y_val, pred_wt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9af049",
   "metadata": {},
   "source": [
    "## 8) Pros, cons, and when to use it\n",
    "\n",
    "### Pros\n",
    "- Handles **class imbalance** better than accuracy (each class contributes equally).\n",
    "- Easy to interpret: it is the **average recall per class**.\n",
    "- Works naturally for **multiclass** problems.\n",
    "\n",
    "### Cons / limitations\n",
    "- Ignores **precision**: you can increase recall (and BA) by predicting a class more often, possibly creating many false positives.\n",
    "- **Threshold-dependent**: with probabilistic outputs, you may need to tune the decision threshold.\n",
    "- Not differentiable → typically used for **evaluation/model selection**, not as a direct training loss.\n",
    "- Equal class weighting may not match real **costs** (some false negatives/positives may matter more than others).\n",
    "\n",
    "### Good use-cases\n",
    "- Imbalanced classification where you want **good recall for every class**.\n",
    "- Settings where the minority class is important and accuracy would be misleading.\n",
    "\n",
    "If you care about ranking probabilities rather than hard labels, consider threshold-free metrics such as AUROC or Average Precision (PR AUC).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a29e9",
   "metadata": {},
   "source": [
    "## 9) Exercises\n",
    "\n",
    "1) Compute balanced accuracy by hand for a binary confusion matrix.\n",
    "\n",
    "2) Implement `balanced_accuracy_score_np(..., sample_weight=...)` tests:\n",
    "   - give higher weight to a subset of samples\n",
    "   - verify it matches `sklearn.metrics.balanced_accuracy_score(..., sample_weight=...)`.\n",
    "\n",
    "3) On the synthetic dataset above:\n",
    "   - compare accuracy vs balanced accuracy as you vary the threshold\n",
    "   - find a threshold that maximizes balanced accuracy and report the per-class recalls.\n",
    "\n",
    "4) Extend the notebook to multiclass:\n",
    "   - generate 3 classes with imbalance\n",
    "   - compute per-class recalls and balanced accuracy\n",
    "   - visualize the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1342a3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
    "- scikit-learn user guide (model evaluation): https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}