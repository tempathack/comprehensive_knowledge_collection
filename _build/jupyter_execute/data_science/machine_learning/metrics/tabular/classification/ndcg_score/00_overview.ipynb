{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12345151",
   "metadata": {},
   "source": [
    "# `ndcg_score` — Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "NDCG is a **ranking metric**: it evaluates how well a model orders items so that the most relevant ones appear near the top.\n",
    "\n",
    "Typical use cases:\n",
    "- web search / information retrieval\n",
    "- recommender systems (ranked lists)\n",
    "- learning-to-rank models\n",
    "\n",
    "In this notebook you will:\n",
    "- build intuition for **DCG** (gain + position discount) and **NDCG** (normalization)\n",
    "- work through a concrete, hand-computed example with plots\n",
    "- implement `dcg@k` / `ndcg@k` from scratch in NumPy (and sanity-check vs scikit-learn)\n",
    "- see how NDCG is used when training a simple (linear) ranking model\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import ndcg_score\n",
    "```\n",
    "\n",
    "## 1) Ranking is not classification\n",
    "\n",
    "For a query $q$ (a search query, a user in a recommender system, …) you usually have **many candidate items**:\n",
    "\n",
    "- true (graded) relevance labels: $y_{q1}, \\dots, y_{qn}$, with $y_{qi} \\ge 0$\n",
    "- model scores: $s_{q1}, \\dots, s_{qn}$ (any real numbers)\n",
    "\n",
    "Sorting scores induces a ranking (a permutation) $\\pi_q$ such that:\n",
    "\n",
    "$$\n",
    "s_{q\\pi_q(1)} \\ge s_{q\\pi_q(2)} \\ge \\dots \\ge s_{q\\pi_q(n)}\n",
    "$$\n",
    "\n",
    "The **top positions matter most** (users rarely scroll), and labels can be **graded** (e.g., 0 = irrelevant, 3 = perfect).\n",
    "\n",
    "NDCG answers: *“How good is my ranked list compared to the best possible ranking?”*\n",
    "\n",
    "## 2) DCG: reward relevant items early\n",
    "\n",
    "Let $\\mathrm{rel}_i$ be the true relevance of the item placed at rank $i$.\n",
    "\n",
    "A common choice is:\n",
    "\n",
    "- **gain**: $g(\\mathrm{rel}) = 2^{\\mathrm{rel}} - 1$ (amplifies high relevance)\n",
    "- **discount**: $d(i) = \\frac{1}{\\log_2(i + 1)}$ (penalizes lower ranks)\n",
    "\n",
    "Then the **Discounted Cumulative Gain** at cutoff $k$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{DCG}@k = \\sum_{i=1}^{k} \\frac{2^{\\mathrm{rel}_i} - 1}{\\log_2(i + 1)}\n",
    "$$\n",
    "\n",
    "Notes:\n",
    "- Only the **order** of scores matters (any strictly monotonic transform of scores leaves DCG/NDCG unchanged, ignoring ties).\n",
    "- Some sources use **linear gain** $g(\\mathrm{rel}) = \\mathrm{rel}$ instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3bf0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34eb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the discount curve and the (exponential) gain function\n",
    "k_max = 10\n",
    "positions = np.arange(1, k_max + 1)\n",
    "discounts = 1.0 / np.log2(positions + 1)\n",
    "\n",
    "fig = px.line(\n",
    "    x=positions,\n",
    "    y=discounts,\n",
    "    markers=True,\n",
    "    title=\"Position discount used by DCG: 1 / log2(rank + 1)\",\n",
    "    labels={\"x\": \"rank (1 = top result)\", \"y\": \"discount weight\"},\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "rels = np.arange(0, 5)\n",
    "gains = 2**rels - 1\n",
    "\n",
    "fig = px.bar(\n",
    "    x=rels,\n",
    "    y=gains,\n",
    "    title=\"Common gain function: g(rel) = 2^rel - 1 (graded relevance)\",\n",
    "    labels={\"x\": \"relevance grade (rel)\", \"y\": \"gain\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e967ab",
   "metadata": {},
   "source": [
    "## 3) IDCG and NDCG: make DCG comparable across queries\n",
    "\n",
    "Raw DCG depends on the query’s “label mass” (how many relevant items exist, and how relevant they are).\n",
    "\n",
    "So we normalize by the **best possible DCG** for that query:\n",
    "\n",
    "- $\\mathrm{IDCG}@k$ (Ideal DCG@k): compute DCG@k after sorting items by true relevance (best-first)\n",
    "- $\\mathrm{NDCG}@k$: scale to $[0, 1]$\n",
    "\n",
    "$$\n",
    "\\mathrm{NDCG}@k = \\frac{\\mathrm{DCG}@k}{\\mathrm{IDCG}@k}\n",
    "$$\n",
    "\n",
    "Edge case: if $\\mathrm{IDCG}@k = 0$ (all items have relevance 0), we define $\\mathrm{NDCG}@k = 0$.\n",
    "\n",
    "With multiple queries, you typically report the **mean** NDCG@k over queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example: one query with 8 candidate items\n",
    "doc_ids = np.array(list(\"ABCDEFGH\"))\n",
    "y_true = np.array([3, 2, 3, 0, 1, 2, 0, 1])\n",
    "y_score = np.array([0.60, 0.20, 0.80, 0.40, 0.10, 0.30, 0.05, 0.70])\n",
    "k = 5\n",
    "\n",
    "order_pred = np.argsort(-y_score, kind=\"mergesort\")\n",
    "order_ideal = np.argsort(-y_true, kind=\"mergesort\")\n",
    "\n",
    "rank = np.arange(1, len(doc_ids) + 1)\n",
    "discount = 1.0 / np.log2(rank + 1)\n",
    "\n",
    "def gain_exp(rel):\n",
    "    return 2**rel - 1\n",
    "\n",
    "rel_pred = y_true[order_pred]\n",
    "gain_pred = gain_exp(rel_pred)\n",
    "contrib_pred = gain_pred * discount\n",
    "dcg_k = contrib_pred[:k].sum()\n",
    "\n",
    "rel_ideal = y_true[order_ideal]\n",
    "gain_ideal = gain_exp(rel_ideal)\n",
    "contrib_ideal = gain_ideal * discount\n",
    "idcg_k = contrib_ideal[:k].sum()\n",
    "\n",
    "ndcg_k = dcg_k / idcg_k if idcg_k > 0 else 0.0\n",
    "\n",
    "print(f\"Predicted order: {''.join(doc_ids[order_pred])}\")\n",
    "print(f\"Ideal order:     {''.join(doc_ids[order_ideal])}\")\n",
    "print(f\"DCG@{k}:  {dcg_k:.4f}\")\n",
    "print(f\"IDCG@{k}: {idcg_k:.4f}\")\n",
    "print(f\"NDCG@{k}: {ndcg_k:.4f}\")\n",
    "\n",
    "sk_ndcg = ndcg_score(y_true[None, :], y_score[None, :], k=k)\n",
    "print(f\"sklearn ndcg_score@{k}: {sk_ndcg:.4f}\")\n",
    "\n",
    "# Any strictly monotonic transform of scores keeps the ranking the same\n",
    "y_score_scaled = 10 * y_score + 5\n",
    "sk_scaled = ndcg_score(y_true[None, :], y_score_scaled[None, :], k=k)\n",
    "print(f\"sklearn ndcg_score@{k} after scaling scores: {sk_scaled:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b358300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where does DCG@k come from? Break it into per-rank contributions.\n",
    "\n",
    "docs_pred = doc_ids[order_pred]\n",
    "scores_pred = y_score[order_pred]\n",
    "rels_pred = y_true[order_pred]\n",
    "gains_pred = gain_exp(rels_pred)\n",
    "contribs_pred = gains_pred * discount\n",
    "\n",
    "docs_ideal = doc_ids[order_ideal]\n",
    "rels_ideal = y_true[order_ideal]\n",
    "gains_ideal = gain_exp(rels_ideal)\n",
    "contribs_ideal = gains_ideal * discount\n",
    "\n",
    "rows = slice(0, k)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=[\"rank\", \"doc\", \"y_true\", \"score\", \"gain\", \"discount\", \"gain*discount\"],\n",
    "                align=\"left\",\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    (np.arange(1, len(doc_ids) + 1)[rows]).tolist(),\n",
    "                    docs_pred[rows].tolist(),\n",
    "                    rels_pred[rows].tolist(),\n",
    "                    np.round(scores_pred[rows], 3).tolist(),\n",
    "                    gains_pred[rows].tolist(),\n",
    "                    np.round(discount[rows], 3).tolist(),\n",
    "                    np.round(contribs_pred[rows], 3).tolist(),\n",
    "                ],\n",
    "                align=\"left\",\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(title=f\"Top-{k} contributions to DCG@{k} (predicted ranking)\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=np.arange(1, k + 1),\n",
    "        y=contribs_pred[:k],\n",
    "        name=\"predicted\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=np.arange(1, k + 1),\n",
    "        y=contribs_ideal[:k],\n",
    "        name=\"ideal\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Per-rank contributions: DCG@{k} vs IDCG@{k}\",\n",
    "    barmode=\"group\",\n",
    "    xaxis_title=\"rank\",\n",
    "    yaxis_title=\"gain(rel) / log2(rank+1)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG as a function of k (for this one query)\n",
    "k_list = np.arange(1, len(doc_ids) + 1)\n",
    "ndcgs = []\n",
    "for kk in k_list:\n",
    "    dcg = contribs_pred[:kk].sum()\n",
    "    idcg = contribs_ideal[:kk].sum()\n",
    "    ndcgs.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "ndcgs = np.array(ndcgs)\n",
    "\n",
    "fig = px.line(\n",
    "    x=k_list,\n",
    "    y=ndcgs,\n",
    "    markers=True,\n",
    "    title=\"NDCG@k for a single query\",\n",
    "    labels={\"x\": \"k (cutoff)\", \"y\": \"NDCG@k\"},\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1.05])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a399561",
   "metadata": {},
   "source": [
    "## 4) From-scratch NumPy implementation\n",
    "\n",
    "scikit-learn expects:\n",
    "\n",
    "- `y_true`: array of shape `(n_queries, n_docs)` with non-negative relevance grades\n",
    "- `y_score`: array of shape `(n_queries, n_docs)` with arbitrary real-valued model scores\n",
    "\n",
    "We’ll implement:\n",
    "\n",
    "- `dcg_at_k_numpy(...)` → DCG per query\n",
    "- `ndcg_at_k_numpy(...)` → NDCG per query\n",
    "- `mean_ndcg_at_k_numpy(...)` → average NDCG across queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_2d_same_shape(y_true, y_score):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "\n",
    "    if y_true.shape != y_score.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true{y_true.shape} vs y_score{y_score.shape}\")\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true[None, :]\n",
    "        y_score = y_score[None, :]\n",
    "    elif y_true.ndim != 2:\n",
    "        raise ValueError(\"expected 1D or 2D arrays\")\n",
    "\n",
    "    if np.any(y_true < 0):\n",
    "        raise ValueError(\"y_true must be non-negative for DCG/NDCG\")\n",
    "\n",
    "    return y_true, y_score\n",
    "\n",
    "\n",
    "def _gain(relevance, scheme=\"exponential\"):\n",
    "    relevance = np.asarray(relevance, dtype=float)\n",
    "    if scheme == \"exponential\":\n",
    "        return np.power(2.0, relevance) - 1.0\n",
    "    if scheme == \"linear\":\n",
    "        return relevance\n",
    "    raise ValueError(\"scheme must be 'exponential' or 'linear'\")\n",
    "\n",
    "\n",
    "def dcg_at_k_numpy(y_true, y_score, k=None, gain_scheme=\"exponential\"):\n",
    "    y_true, y_score = _as_2d_same_shape(y_true, y_score)\n",
    "    n_queries, n_docs = y_true.shape\n",
    "\n",
    "    if k is None:\n",
    "        k = n_docs\n",
    "    k = int(k)\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be >= 1\")\n",
    "    k = min(k, n_docs)\n",
    "\n",
    "    order = np.argsort(-y_score, axis=1, kind=\"mergesort\")\n",
    "    y_sorted = np.take_along_axis(y_true, order, axis=1)\n",
    "\n",
    "    gains = _gain(y_sorted[:, :k], scheme=gain_scheme)\n",
    "    discounts = 1.0 / np.log2(np.arange(2, k + 2))\n",
    "    return np.sum(gains * discounts[None, :], axis=1)\n",
    "\n",
    "\n",
    "def idcg_at_k_numpy(y_true, k=None, gain_scheme=\"exponential\"):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true[None, :]\n",
    "    if y_true.ndim != 2:\n",
    "        raise ValueError(\"expected 1D or 2D arrays\")\n",
    "\n",
    "    n_queries, n_docs = y_true.shape\n",
    "    if k is None:\n",
    "        k = n_docs\n",
    "    k = int(k)\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be >= 1\")\n",
    "    k = min(k, n_docs)\n",
    "\n",
    "    ideal_order = np.argsort(-y_true, axis=1, kind=\"mergesort\")\n",
    "    y_ideal = np.take_along_axis(y_true, ideal_order, axis=1)\n",
    "\n",
    "    gains = _gain(y_ideal[:, :k], scheme=gain_scheme)\n",
    "    discounts = 1.0 / np.log2(np.arange(2, k + 2))\n",
    "    return np.sum(gains * discounts[None, :], axis=1)\n",
    "\n",
    "\n",
    "def ndcg_at_k_numpy(y_true, y_score, k=None, gain_scheme=\"exponential\"):\n",
    "    y_true, y_score = _as_2d_same_shape(y_true, y_score)\n",
    "    dcg = dcg_at_k_numpy(y_true, y_score, k=k, gain_scheme=gain_scheme)\n",
    "    idcg = idcg_at_k_numpy(y_true, k=k, gain_scheme=gain_scheme)\n",
    "    return np.divide(dcg, idcg, out=np.zeros_like(dcg), where=idcg > 0)\n",
    "\n",
    "\n",
    "def mean_ndcg_at_k_numpy(y_true, y_score, k=None, gain_scheme=\"exponential\"):\n",
    "    return float(ndcg_at_k_numpy(y_true, y_score, k=k, gain_scheme=gain_scheme).mean())\n",
    "\n",
    "\n",
    "def dcg_at_k_expected_ties_1d(y_true, y_score, k=None, gain_scheme=\"exponential\"):\n",
    "    \"\"\"Expected DCG@k under uniform random tie-breaking for equal scores (1 query).\n",
    "\n",
    "    Useful for understanding how ties can change DCG/NDCG.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "    if y_true.ndim != 1 or y_score.ndim != 1:\n",
    "        raise ValueError(\"expected 1D arrays\")\n",
    "    if y_true.shape != y_score.shape:\n",
    "        raise ValueError(\"shape mismatch\")\n",
    "    if np.any(y_true < 0):\n",
    "        raise ValueError(\"y_true must be non-negative\")\n",
    "\n",
    "    n = y_true.size\n",
    "    if k is None:\n",
    "        k = n\n",
    "    k = int(k)\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be >= 1\")\n",
    "    k = min(k, n)\n",
    "\n",
    "    order = np.argsort(-y_score, kind=\"mergesort\")\n",
    "    y_true_sorted = y_true[order]\n",
    "    y_score_sorted = y_score[order]\n",
    "\n",
    "    gains_sorted = _gain(y_true_sorted, scheme=gain_scheme)\n",
    "    discounts = 1.0 / np.log2(np.arange(2, n + 2))\n",
    "\n",
    "    dcg = 0.0\n",
    "    start = 0\n",
    "    while start < k:\n",
    "        end = start + 1\n",
    "        while end < n and y_score_sorted[end] == y_score_sorted[start]:\n",
    "            end += 1\n",
    "        group_size = end - start\n",
    "        sum_discounts_in_top_k = discounts[start : min(end, k)].sum()\n",
    "        dcg += gains_sorted[start:end].sum() * (sum_discounts_in_top_k / group_size)\n",
    "        start = end\n",
    "    return float(dcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check against scikit-learn (ties are rare here due to continuous scores)\n",
    "n_queries, n_docs = 30, 40\n",
    "y_true_rand = rng.integers(0, 4, size=(n_queries, n_docs))\n",
    "y_score_rand = rng.normal(size=(n_queries, n_docs))\n",
    "\n",
    "for k in [1, 3, 5, 10, None]:\n",
    "    sk = ndcg_score(y_true_rand, y_score_rand, k=k)\n",
    "    np_impl = mean_ndcg_at_k_numpy(y_true_rand, y_score_rand, k=k)\n",
    "    print(f\"k={str(k):>4} | sklearn={sk:.6f} | numpy={np_impl:.6f} | abs diff={abs(sk-np_impl):.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c117c",
   "metadata": {},
   "source": [
    "## 5) Ties and other pitfalls\n",
    "\n",
    "### Ties in `y_score`\n",
    "\n",
    "If multiple items share exactly the same score, the ranking is not uniquely defined.\n",
    "Different tie-breaking rules can lead to different DCG/NDCG values.\n",
    "\n",
    "Libraries often implement a **tie-aware** variant (expected value under random permutations) when `ignore_ties=False`.\n",
    "\n",
    "### All-zero relevance\n",
    "\n",
    "If all relevances are zero for a query, then $\\mathrm{IDCG}@k = 0$ and NDCG is defined as 0.\n",
    "\n",
    "### Label scaling\n",
    "\n",
    "With exponential gains $2^{\\mathrm{rel}} - 1$, going from relevance 2 to 3 matters more than 0 to 1.\n",
    "That’s often what you want in IR (highly relevant docs should dominate), but you should choose grades deliberately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how ties can create a range of possible NDCG values\n",
    "y_true_tie = np.array([3, 2, 1, 0, 0])\n",
    "y_score_tie = np.array([0.9, 0.8, 0.8, 0.8, 0.1])  # tie among 3 items\n",
    "k = 5\n",
    "\n",
    "# Stable tie-breaking (argsort with kind=\"mergesort\")\n",
    "ndcg_simple = ndcg_at_k_numpy(y_true_tie, y_score_tie, k=k)[0]\n",
    "\n",
    "# Best/worst tie-breaking inside the tied group (same primary score ordering)\n",
    "order_best = np.lexsort((-y_true_tie, -y_score_tie))\n",
    "order_worst = np.lexsort((y_true_tie, -y_score_tie))\n",
    "\n",
    "def ndcg_from_order(order):\n",
    "    rel_sorted = y_true_tie[order]\n",
    "    discounts = 1.0 / np.log2(np.arange(2, rel_sorted.size + 2))\n",
    "    gains = (2**rel_sorted - 1) * discounts\n",
    "    dcg = gains[:k].sum()\n",
    "    idcg = idcg_at_k_numpy(y_true_tie[None, :], k=k)[0]\n",
    "    return float(dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "ndcg_best = ndcg_from_order(order_best)\n",
    "ndcg_worst = ndcg_from_order(order_worst)\n",
    "\n",
    "dcg_expected = dcg_at_k_expected_ties_1d(y_true_tie, y_score_tie, k=k)\n",
    "idcg = idcg_at_k_numpy(y_true_tie[None, :], k=k)[0]\n",
    "ndcg_expected = dcg_expected / idcg if idcg > 0 else 0.0\n",
    "\n",
    "print(f\"NDCG@{k} with stable tie-breaking:       {ndcg_simple:.4f}\")\n",
    "print(f\"NDCG@{k} best-case within ties:          {ndcg_best:.4f}\")\n",
    "print(f\"NDCG@{k} worst-case within ties:         {ndcg_worst:.4f}\")\n",
    "print(f\"NDCG@{k} expected under random ties:     {ndcg_expected:.4f}\")\n",
    "\n",
    "sk_tie_aware = ndcg_score(y_true_tie[None, :], y_score_tie[None, :], k=k, ignore_ties=False)\n",
    "sk_ignore_ties = ndcg_score(y_true_tie[None, :], y_score_tie[None, :], k=k, ignore_ties=True)\n",
    "print(f\"sklearn ndcg_score@{k} (ignore_ties=False): {sk_tie_aware:.4f}\")\n",
    "print(f\"sklearn ndcg_score@{k} (ignore_ties=True):  {sk_ignore_ties:.4f}\")\n",
    "\n",
    "fig = px.bar(\n",
    "    x=[\"worst\", \"expected\", \"deterministic\", \"best\"],\n",
    "    y=[ndcg_worst, ndcg_expected, ndcg_simple, ndcg_best],\n",
    "    title=f\"Tie handling can change NDCG@{k}\",\n",
    "    labels={\"x\": \"tie handling\", \"y\": f\"NDCG@{k}\"},\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1.05])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2cf75",
   "metadata": {},
   "source": [
    "## 6) Using NDCG when optimizing a simple ranking model\n",
    "\n",
    "NDCG depends on **sorting**, so it is not a smooth, differentiable function of the model parameters.\n",
    "In practice, learning-to-rank systems usually optimize a **surrogate loss** and track NDCG for evaluation / model selection.\n",
    "\n",
    "We’ll compare two simple linear scoring approaches:\n",
    "\n",
    "### Pointwise regression (MSE)\n",
    "\n",
    "Treat each (query, doc) as an independent training example and predict the relevance grade:\n",
    "\n",
    "$$\n",
    "s_{qi} = x_{qi}^\\top w\n",
    "\\qquad\n",
    "\\min_w \\; \\frac{1}{N}\\sum_{q,i} (s_{qi} - y_{qi})^2\n",
    "$$\n",
    "\n",
    "### Pairwise logistic loss (RankNet-style)\n",
    "\n",
    "For each query, form pairs $(i, j)$ where $y_{qi} > y_{qj}$ and encourage $s_{qi} > s_{qj}$:\n",
    "\n",
    "$$\n",
    "\\min_w \\; \\mathbb{E}_{(q,i,j)}\\big[\\log(1 + \\exp(-(s_{qi} - s_{qj})))\\big]\n",
    "$$\n",
    "\n",
    "Both produce scores you can rank; we’ll track **mean NDCG@k** during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a19b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic learning-to-rank dataset\n",
    "n_queries = 300\n",
    "n_docs = 10\n",
    "n_features = 6\n",
    "\n",
    "X = rng.normal(size=(n_queries, n_docs, n_features))\n",
    "w_star = rng.normal(size=(n_features,))\n",
    "\n",
    "latent = X @ w_star + 0.2 * rng.normal(size=(n_queries, n_docs))\n",
    "\n",
    "# Convert latent scores to graded relevance 0..3 *within each query*\n",
    "q50 = np.quantile(latent, 0.50, axis=1, keepdims=True)\n",
    "q75 = np.quantile(latent, 0.75, axis=1, keepdims=True)\n",
    "q90 = np.quantile(latent, 0.90, axis=1, keepdims=True)\n",
    "\n",
    "y_rel = np.zeros_like(latent, dtype=int)\n",
    "y_rel += latent >= q50\n",
    "y_rel += latent >= q75\n",
    "y_rel += latent >= q90\n",
    "\n",
    "# Train/validation split by query\n",
    "perm = rng.permutation(n_queries)\n",
    "n_train = int(0.8 * n_queries)\n",
    "train_idx, val_idx = perm[:n_train], perm[n_train:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y_rel[train_idx]\n",
    "X_val, y_val = X[val_idx], y_rel[val_idx]\n",
    "\n",
    "k_eval = 5\n",
    "\n",
    "def score_linear(X, w):\n",
    "    # X: (n_queries, n_docs, n_features) -> scores: (n_queries, n_docs)\n",
    "    return np.tensordot(X, w, axes=[2, 0])\n",
    "\n",
    "w0 = rng.normal(scale=0.1, size=(n_features,))\n",
    "baseline_val = mean_ndcg_at_k_numpy(y_val, score_linear(X_val, w0), k=k_eval)\n",
    "print(f\"Baseline mean NDCG@{k_eval} on val (random weights): {baseline_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb54bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pointwise_mse(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    k=5,\n",
    "    lr=0.05,\n",
    "    l2=1e-3,\n",
    "    epochs=60,\n",
    "):\n",
    "    n_features = X_train.shape[2]\n",
    "    w = rng.normal(scale=0.1, size=n_features)\n",
    "\n",
    "    X_flat = X_train.reshape(-1, n_features)\n",
    "    y_flat = y_train.reshape(-1).astype(float)\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        preds = X_flat @ w\n",
    "        err = preds - y_flat\n",
    "        grad = (X_flat.T @ err) / y_flat.size + l2 * w\n",
    "        w -= lr * grad\n",
    "\n",
    "        train_ndcg = mean_ndcg_at_k_numpy(y_train, score_linear(X_train, w), k=k)\n",
    "        val_ndcg = mean_ndcg_at_k_numpy(y_val, score_linear(X_val, w), k=k)\n",
    "        mse = float(np.mean(err**2))\n",
    "        history.append({\"epoch\": epoch, \"train_ndcg\": train_ndcg, \"val_ndcg\": val_ndcg, \"mse\": mse})\n",
    "    return w, history\n",
    "\n",
    "\n",
    "w_mse, hist_mse = train_pointwise_mse(X_train, y_train, X_val, y_val, k=k_eval)\n",
    "print(f\"Final val mean NDCG@{k_eval} (pointwise MSE): {hist_mse[-1]['val_ndcg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e681f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def _sample_pairs(y_query, max_pairs=40):\n",
    "    # indices (i, j) such that y_i > y_j\n",
    "    i_idx, j_idx = np.where(y_query[:, None] > y_query[None, :])\n",
    "    if i_idx.size == 0:\n",
    "        return np.empty((0,), dtype=int), np.empty((0,), dtype=int)\n",
    "    if i_idx.size > max_pairs:\n",
    "        sel = rng.choice(i_idx.size, size=max_pairs, replace=False)\n",
    "        i_idx, j_idx = i_idx[sel], j_idx[sel]\n",
    "    return i_idx.astype(int), j_idx.astype(int)\n",
    "\n",
    "\n",
    "def make_pair_dataset(y_train, max_pairs_per_query=40):\n",
    "    qs, is_, js = [], [], []\n",
    "    for q in range(y_train.shape[0]):\n",
    "        i_idx, j_idx = _sample_pairs(y_train[q], max_pairs=max_pairs_per_query)\n",
    "        if i_idx.size:\n",
    "            qs.append(np.full(i_idx.size, q, dtype=int))\n",
    "            is_.append(i_idx)\n",
    "            js.append(j_idx)\n",
    "    if not qs:\n",
    "        return np.empty((0,), dtype=int), np.empty((0,), dtype=int), np.empty((0,), dtype=int)\n",
    "    return np.concatenate(qs), np.concatenate(is_), np.concatenate(js)\n",
    "\n",
    "\n",
    "pair_q, pair_i, pair_j = make_pair_dataset(y_train, max_pairs_per_query=40)\n",
    "print(f\"Pairwise dataset size: {pair_q.size} pairs\")\n",
    "\n",
    "\n",
    "def train_pairwise_logistic(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    pair_q,\n",
    "    pair_i,\n",
    "    pair_j,\n",
    "    k=5,\n",
    "    lr=0.2,\n",
    "    l2=1e-3,\n",
    "    epochs=60,\n",
    "):\n",
    "    n_features = X_train.shape[2]\n",
    "    w = rng.normal(scale=0.1, size=n_features)\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        # x_diff: (n_pairs, n_features)\n",
    "        x_diff = X_train[pair_q, pair_i] - X_train[pair_q, pair_j]\n",
    "        d = x_diff @ w\n",
    "\n",
    "        # log(1 + exp(-d)) in a stable way\n",
    "        loss = float(np.mean(np.logaddexp(0.0, -d)) + 0.5 * l2 * np.sum(w**2))\n",
    "\n",
    "        p = _sigmoid(d)\n",
    "        grad = (x_diff * (p - 1.0)[:, None]).mean(axis=0) + l2 * w\n",
    "        w -= lr * grad\n",
    "\n",
    "        train_ndcg = mean_ndcg_at_k_numpy(y_train, score_linear(X_train, w), k=k)\n",
    "        val_ndcg = mean_ndcg_at_k_numpy(y_val, score_linear(X_val, w), k=k)\n",
    "        history.append({\"epoch\": epoch, \"train_ndcg\": train_ndcg, \"val_ndcg\": val_ndcg, \"loss\": loss})\n",
    "\n",
    "    return w, history\n",
    "\n",
    "\n",
    "w_rank, hist_rank = train_pairwise_logistic(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    pair_q,\n",
    "    pair_i,\n",
    "    pair_j,\n",
    "    k=k_eval,\n",
    ")\n",
    "print(f\"Final val mean NDCG@{k_eval} (pairwise logistic): {hist_rank[-1]['val_ndcg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44dfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training curves (mean NDCG@k)\n",
    "epochs = [h[\"epoch\"] for h in hist_mse]\n",
    "mse_train = [h[\"train_ndcg\"] for h in hist_mse]\n",
    "mse_val = [h[\"val_ndcg\"] for h in hist_mse]\n",
    "\n",
    "rank_train = [h[\"train_ndcg\"] for h in hist_rank]\n",
    "rank_val = [h[\"val_ndcg\"] for h in hist_rank]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=epochs, y=mse_val, mode=\"lines\", name=\"val NDCG (pointwise MSE)\"))\n",
    "fig.add_trace(go.Scatter(x=epochs, y=rank_val, mode=\"lines\", name=\"val NDCG (pairwise logistic)\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=epochs, y=mse_train, mode=\"lines\", name=\"train NDCG (pointwise MSE)\", line=dict(dash=\"dot\")))\n",
    "fig.add_trace(go.Scatter(x=epochs, y=rank_train, mode=\"lines\", name=\"train NDCG (pairwise logistic)\", line=dict(dash=\"dot\")))\n",
    "\n",
    "fig.add_hline(y=baseline_val, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"random baseline (val)\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Learning-to-rank toy example: mean NDCG@{k_eval} during training\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=f\"mean NDCG@{k_eval}\",\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1.05])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9f587",
   "metadata": {},
   "source": [
    "## Pros / cons and when to use NDCG\n",
    "\n",
    "### Pros\n",
    "- Works with **graded relevance** (not just relevant/irrelevant)\n",
    "- Emphasizes the **top of the ranking** via the discount and cutoff $k$\n",
    "- Normalized to $[0, 1]$ so it’s **comparable across queries**\n",
    "- Depends only on ranking order (invariant to strictly monotonic score transforms, ignoring ties)\n",
    "\n",
    "### Cons\n",
    "- Non-smooth due to sorting → not directly optimized by standard gradient descent (use surrogates or black-box optimization)\n",
    "- Requires a **query → candidate set** structure; not a drop-in metric for standard single-label classification\n",
    "- Tie handling, gain definition, and choice of $k$ can change results\n",
    "- If $\\mathrm{IDCG}@k = 0$ the metric is defined by convention (commonly 0)\n",
    "\n",
    "### Good fits\n",
    "- Search ranking (documents / products)\n",
    "- Recommender systems where you evaluate a ranked slate\n",
    "- Learning-to-rank model evaluation (pairwise/listwise methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff3b8f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Järvelin, K. & Kekäläinen, J. (2002). *Cumulated gain-based evaluation of IR techniques.* ACM TOIS.\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html\n",
    "- Burges, C. et al. (2005). *Learning to Rank using Gradient Descent* (RankNet).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}