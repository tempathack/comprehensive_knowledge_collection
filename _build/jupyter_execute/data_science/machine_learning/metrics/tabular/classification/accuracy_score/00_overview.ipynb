{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11548bc",
   "metadata": {},
   "source": [
    "# `accuracy_score` (classification accuracy)\n",
    "\n",
    "Accuracy is the simplest classification metric: it’s the **fraction of samples you got exactly right**.\n",
    "\n",
    "**You will learn:**\n",
    "- the math definition (binary, multiclass, multilabel)\n",
    "- a from-scratch NumPy implementation\n",
    "- how **decision thresholds** change accuracy\n",
    "- how to use accuracy when training a simple **logistic regression** model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a334c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score as sk_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b37f5c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- You know what **labels** $y \\in \\{0,1\\}$ (binary) or $y \\in \\{1,\\dots,K\\}$ (multiclass) are.\n",
    "- You’re comfortable with the idea that a classifier may output either:\n",
    "  - **hard predictions** $\\hat{y}$ (a class label), or\n",
    "  - **scores/probabilities** (then you still need a rule to turn them into $\\hat{y}$).\n",
    "\n",
    "### Notation\n",
    "\n",
    "- True labels: $y_1,\\dots,y_n$\n",
    "- Predicted labels: $\\hat{y}_1,\\dots,\\hat{y}_n$\n",
    "- Indicator function: $\\mathbf{1}[\\text{statement}]$ equals 1 if the statement is true, else 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7567a",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "### Generic (binary or multiclass)\n",
    "\n",
    "Accuracy is the **average of “correct?” indicators**:\n",
    "\n",
    "$$\n",
    "\\operatorname{Acc}(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}[y_i = \\hat{y}_i]\n",
    "$$\n",
    "\n",
    "With per-sample weights $w_i \\ge 0$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Acc}_w(y,\\hat{y}) = \\frac{\\sum_{i=1}^n w_i\\,\\mathbf{1}[y_i = \\hat{y}_i]}{\\sum_{i=1}^n w_i}\n",
    "$$\n",
    "\n",
    "### Binary (via confusion matrix)\n",
    "\n",
    "If you define true positives/negatives and false positives/negatives:\n",
    "\n",
    "$$\n",
    "\\operatorname{Acc} = \\frac{\\mathrm{TP} + \\mathrm{TN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{TN} + \\mathrm{FN}}\n",
    "$$\n",
    "\n",
    "### Relation to 0–1 loss\n",
    "\n",
    "Define the 0–1 loss per sample:\n",
    "\n",
    "$$\n",
    "\\ell_{0/1}(y_i,\\hat{y}_i) = \\mathbf{1}[y_i \\ne \\hat{y}_i]\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\operatorname{Acc} = 1 - \\frac{1}{n} \\sum_{i=1}^n \\ell_{0/1}(y_i,\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### Multilabel (subset accuracy)\n",
    "\n",
    "If each sample has a **vector** of labels $\\mathbf{y}_i \\in \\{0,1\\}^L$, scikit-learn’s `accuracy_score` uses **subset accuracy**:\n",
    "\n",
    "$$\n",
    "\\operatorname{Acc}_{\\text{subset}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}[\\mathbf{y}_i = \\hat{\\mathbf{y}}_i]\n",
    "$$\n",
    "\n",
    "A sample counts as correct only if **all labels match exactly**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2979d7",
   "metadata": {},
   "source": [
    "## Intuition: accuracy is an average of 0/1 per-sample outcomes\n",
    "\n",
    "Each sample contributes either:\n",
    "- 1 (correct prediction)\n",
    "- 0 (incorrect prediction)\n",
    "\n",
    "Accuracy is just the mean of that vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610889c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 1, 0, 1, 0, 0, 1])\n",
    "y_pred = np.array([0, 1, 0, 0, 1, 1, 0, 1])\n",
    "\n",
    "correct = (y_true == y_pred).astype(int)\n",
    "acc = correct.mean()\n",
    "\n",
    "print('per-sample correct:', correct)\n",
    "print('accuracy:', acc)\n",
    "print('sklearn accuracy:', sk_accuracy_score(y_true, y_pred))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=np.arange(len(correct)),\n",
    "        y=correct,\n",
    "        marker_color=[\"#2ca02c\" if c == 1 else \"#d62728\" for c in correct],\n",
    "        name=\"correct (1) / wrong (0)\",\n",
    "    )\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=acc,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=f\"accuracy = {acc:.2f}\",\n",
    "    annotation_position=\"top left\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy = mean of per-sample correctness\",\n",
    "    xaxis_title=\"sample index\",\n",
    "    yaxis_title=\"correct?\",\n",
    "    yaxis=dict(tickmode=\"array\", tickvals=[0, 1]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310173f",
   "metadata": {},
   "source": [
    "## From-scratch NumPy implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_np(y_true, y_pred, *, sample_weight=None, normalize=True):\n",
    "    '''Compute accuracy (and multilabel subset accuracy) using NumPy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true, y_pred:\n",
    "        1D arrays (n_samples,) for standard classification, or\n",
    "        2D arrays (n_samples, n_labels) for multilabel subset accuracy.\n",
    "\n",
    "    sample_weight:\n",
    "        Optional array (n_samples,) of non-negative weights.\n",
    "\n",
    "    normalize:\n",
    "        If True, return a fraction in [0, 1]. If False, return the (weighted) count.\n",
    "    '''\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true {y_true.shape} vs y_pred {y_pred.shape}\")\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        correct = (y_true == y_pred)\n",
    "    elif y_true.ndim == 2:\n",
    "        # multilabel subset accuracy: all labels must match per sample\n",
    "        correct = np.all(y_true == y_pred, axis=1)\n",
    "    else:\n",
    "        raise ValueError(f\"expected 1D or 2D arrays, got ndim={y_true.ndim}\")\n",
    "\n",
    "    if sample_weight is None:\n",
    "        if normalize:\n",
    "            return float(np.mean(correct))\n",
    "        return float(np.sum(correct))\n",
    "\n",
    "    w = np.asarray(sample_weight)\n",
    "    if w.ndim != 1 or w.shape[0] != correct.shape[0]:\n",
    "        raise ValueError(f\"sample_weight must be shape (n_samples,), got {w.shape}\")\n",
    "\n",
    "    correct_f = correct.astype(float)\n",
    "    if normalize:\n",
    "        return float(np.average(correct_f, weights=w))\n",
    "    return float(np.sum(w * correct_f))\n",
    "\n",
    "\n",
    "def predict_labels_from_proba(p, threshold=0.5):\n",
    "    '''Turn probabilities into hard labels using a threshold.'''\n",
    "\n",
    "    p = np.asarray(p)\n",
    "    return (p >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9daf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks vs scikit-learn\n",
    "\n",
    "# 1) Binary / multiclass (1D)\n",
    "y_true = rng.integers(0, 3, size=200)\n",
    "y_pred = rng.integers(0, 3, size=200)\n",
    "w = rng.uniform(0.1, 2.0, size=200)\n",
    "\n",
    "for normalize in [True, False]:\n",
    "    ours = accuracy_score_np(y_true, y_pred, sample_weight=w, normalize=normalize)\n",
    "    theirs = sk_accuracy_score(y_true, y_pred, sample_weight=w, normalize=normalize)\n",
    "    print(normalize, ours, theirs, 'diff', abs(ours - theirs))\n",
    "\n",
    "# 2) Multilabel subset accuracy (2D)\n",
    "y_true_ml = rng.integers(0, 2, size=(50, 4))\n",
    "y_pred_ml = rng.integers(0, 2, size=(50, 4))\n",
    "print('multilabel:', accuracy_score_np(y_true_ml, y_pred_ml), sk_accuracy_score(y_true_ml, y_pred_ml))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2cb935",
   "metadata": {},
   "source": [
    "## Accuracy depends on the decision threshold\n",
    "\n",
    "Many binary classifiers output a **probability** $p_i = P(y_i=1\\mid x_i)$.\n",
    "To turn that into a predicted label, you choose a threshold $t$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i(t) = \\mathbf{1}[p_i \\ge t]\n",
    "$$\n",
    "\n",
    "So accuracy is really a function of $t$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Acc}(t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}[y_i = \\hat{y}_i(t)]\n",
    "$$\n",
    "\n",
    "**Key property:** $\\operatorname{Acc}(t)$ is a **step function** of $t$ (it only changes when $t$ crosses one of the predicted probabilities).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic probabilities where threshold choice matters\n",
    "n = 200\n",
    "\n",
    "y_true = rng.integers(0, 2, size=n)\n",
    "\n",
    "# Create probabilities correlated with y_true, but noisy\n",
    "logit = (y_true * 2 - 1) * 1.2 + rng.normal(0, 1.0, size=n)\n",
    "p = 1 / (1 + np.exp(-logit))\n",
    "\n",
    "thresholds = np.linspace(0, 1, 401)\n",
    "accs = np.array([accuracy_score_np(y_true, predict_labels_from_proba(p, t)) for t in thresholds])\n",
    "\n",
    "best_idx = int(np.argmax(accs))\n",
    "best_t = float(thresholds[best_idx])\n",
    "\n",
    "print('accuracy @ t=0.50:', accuracy_score_np(y_true, predict_labels_from_proba(p, 0.5)))\n",
    "print('best threshold:', best_t)\n",
    "print('best accuracy:', float(accs[best_idx]))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=accs, mode='lines', name='accuracy(t)'))\n",
    "fig.add_vline(x=0.5, line_dash='dash', line_color='gray', annotation_text='0.5', annotation_position='top')\n",
    "fig.add_vline(x=best_t, line_dash='dash', line_color='black', annotation_text=f'best={best_t:.2f}', annotation_position='top')\n",
    "fig.update_layout(\n",
    "    title='Accuracy as a function of the decision threshold',\n",
    "    xaxis_title='threshold t',\n",
    "    yaxis_title='accuracy',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=p,\n",
    "    color=y_true.astype(str),\n",
    "    nbins=30,\n",
    "    barmode='overlay',\n",
    "    opacity=0.6,\n",
    "    title='Predicted probability distribution by true class',\n",
    "    labels={'x': 'predicted probability p', 'color': 'true label'},\n",
    ")\n",
    "fig.add_vline(x=best_t, line_dash='dash', line_color='black')\n",
    "fig.add_vline(x=0.5, line_dash='dash', line_color='gray')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c61beb",
   "metadata": {},
   "source": [
    "## A classic pitfall: accuracy on imbalanced data (“accuracy paradox”)\n",
    "\n",
    "If one class dominates, a model can achieve high accuracy by **always predicting the majority class**.\n",
    "\n",
    "Example: 95% negatives, 5% positives.\n",
    "- Predicting “negative” for everyone gives **95% accuracy**.\n",
    "- But it completely fails to detect the positives.\n",
    "\n",
    "This is why it’s good practice to always look at the **confusion matrix** (and consider metrics like recall/precision/F1 or balanced accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced example: 95% of class 0\n",
    "n = 200\n",
    "n_pos = int(0.05 * n)\n",
    "\n",
    "y_true = np.array([1] * n_pos + [0] * (n - n_pos))\n",
    "rng.shuffle(y_true)\n",
    "\n",
    "y_pred_all0 = np.zeros_like(y_true)\n",
    "\n",
    "acc = accuracy_score_np(y_true, y_pred_all0)\n",
    "print('majority-class baseline accuracy:', acc)\n",
    "\n",
    "# Confusion matrix counts (binary)\n",
    "TN = int(np.sum((y_true == 0) & (y_pred_all0 == 0)))\n",
    "FP = int(np.sum((y_true == 0) & (y_pred_all0 == 1)))\n",
    "FN = int(np.sum((y_true == 1) & (y_pred_all0 == 0)))\n",
    "TP = int(np.sum((y_true == 1) & (y_pred_all0 == 1)))\n",
    "\n",
    "cm = np.array([[TN, FP], [FN, TP]])\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cm,\n",
    "        x=['pred 0', 'pred 1'],\n",
    "        y=['true 0', 'true 1'],\n",
    "        text=cm,\n",
    "        texttemplate='%{text}',\n",
    "        colorscale='Blues',\n",
    "        showscale=False,\n",
    "    )\n",
    ")\n",
    "fig.update_layout(title='Confusion matrix for the majority-class baseline')\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(\n",
    "    x=['class 0', 'class 1'],\n",
    "    y=[int(np.sum(y_true == 0)), int(np.sum(y_true == 1))],\n",
    "    title='Class imbalance in the data',\n",
    "    labels={'x': 'class', 'y': 'count'},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478278a9",
   "metadata": {},
   "source": [
    "## Using accuracy during optimization: logistic regression (NumPy)\n",
    "\n",
    "### Why we usually *don’t* optimize accuracy directly\n",
    "\n",
    "Accuracy corresponds to the **0–1 loss**, which is **non-differentiable** with respect to model parameters.\n",
    "Gradient-based methods (like gradient descent) need smooth objectives, so we typically optimize a surrogate such as **log loss**.\n",
    "\n",
    "Even if you train with log loss, accuracy is still useful for:\n",
    "- monitoring training (does performance improve?)\n",
    "- comparing models\n",
    "- choosing a **decision threshold** on a validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D dataset (two overlapping Gaussians)\n",
    "\n",
    "n0, n1 = 260, 140\n",
    "X0 = rng.normal(loc=(-1.0, -1.0), scale=(1.1, 1.1), size=(n0, 2))\n",
    "X1 = rng.normal(loc=(1.2, 1.0), scale=(1.3, 1.0), size=(n1, 2))\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * n0 + [1] * n1)\n",
    "\n",
    "# Shuffle and split\n",
    "idx = rng.permutation(len(y))\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.35, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_train[:, 0],\n",
    "    y=X_train[:, 1],\n",
    "    color=y_train.astype(str),\n",
    "    title='Training data (2D) — overlap makes errors unavoidable',\n",
    "    labels={'x': 'x1', 'y': 'x2', 'color': 'class'},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaee5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def add_intercept(X):\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def log_loss_np(y_true, p, eps=1e-15):\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    y_true = y_true.astype(float)\n",
    "    return float(-np.mean(y_true * np.log(p) + (1 - y_true) * np.log(1 - p)))\n",
    "\n",
    "\n",
    "def fit_logreg_gd(X, y, *, lr=0.2, n_iters=400, l2=0.0, verbose=False):\n",
    "    '''Logistic regression with batch gradient descent (binary).'''\n",
    "\n",
    "    Xb = add_intercept(X)\n",
    "    y = y.astype(float)\n",
    "\n",
    "    w = np.zeros(Xb.shape[1])\n",
    "\n",
    "    history = {\n",
    "        'iter': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc@0.5': [],\n",
    "    }\n",
    "\n",
    "    for it in range(1, n_iters + 1):\n",
    "        z = Xb @ w\n",
    "        p = sigmoid(z)\n",
    "\n",
    "        # Gradient of average log loss + L2 regularization (excluding intercept)\n",
    "        grad = (Xb.T @ (p - y)) / Xb.shape[0]\n",
    "        grad[1:] += l2 * w[1:]\n",
    "\n",
    "        w -= lr * grad\n",
    "\n",
    "        if it % 5 == 0 or it == 1:\n",
    "            y_pred = (p >= 0.5).astype(int)\n",
    "            history['iter'].append(it)\n",
    "            history['train_loss'].append(log_loss_np(y, p))\n",
    "            history['train_acc@0.5'].append(accuracy_score_np(y.astype(int), y_pred))\n",
    "\n",
    "            if verbose:\n",
    "                print(it, history['train_loss'][-1], history['train_acc@0.5'][-1])\n",
    "\n",
    "    return w, history\n",
    "\n",
    "\n",
    "w, hist = fit_logreg_gd(X_train, y_train, lr=0.15, n_iters=500, l2=0.01)\n",
    "\n",
    "# Evaluate on validation\n",
    "p_val = sigmoid(add_intercept(X_val) @ w)\n",
    "acc_val_05 = accuracy_score_np(y_val, predict_labels_from_proba(p_val, 0.5))\n",
    "print('validation accuracy @ 0.5:', acc_val_05)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist['iter'], y=hist['train_loss'], mode='lines', name='train log loss'))\n",
    "fig.update_layout(title='Training objective (log loss) decreases smoothly', xaxis_title='iteration', yaxis_title='log loss')\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist['iter'], y=hist['train_acc@0.5'], mode='lines', name='train accuracy @ 0.5'))\n",
    "fig.update_layout(title='Accuracy during training (often changes in jumps)', xaxis_title='iteration', yaxis_title='accuracy', yaxis=dict(range=[0, 1]))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a decision threshold that maximizes validation accuracy\n",
    "\n",
    "thresholds = np.linspace(0, 1, 401)\n",
    "accs_val = np.array([accuracy_score_np(y_val, predict_labels_from_proba(p_val, t)) for t in thresholds])\n",
    "\n",
    "best_idx = int(np.argmax(accs_val))\n",
    "best_t = float(thresholds[best_idx])\n",
    "\n",
    "print('best threshold:', best_t)\n",
    "print('validation accuracy @ best_t:', float(accs_val[best_idx]))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=accs_val, mode='lines', name='val accuracy(t)'))\n",
    "fig.add_vline(x=0.5, line_dash='dash', line_color='gray', annotation_text='0.5', annotation_position='top')\n",
    "fig.add_vline(x=best_t, line_dash='dash', line_color='black', annotation_text=f'best={best_t:.2f}', annotation_position='top')\n",
    "fig.update_layout(title='Validation accuracy vs threshold', xaxis_title='threshold', yaxis_title='accuracy', yaxis=dict(range=[0, 1]))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fe38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary visualization (threshold = best_t)\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "x1_grid = np.linspace(x1_min, x1_max, 200)\n",
    "x2_grid = np.linspace(x2_min, x2_max, 200)\n",
    "xx, yy = np.meshgrid(x1_grid, x2_grid)\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "p_grid = sigmoid(add_intercept(grid) @ w).reshape(xx.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Background probability field\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        z=p_grid,\n",
    "        colorscale='RdBu',\n",
    "        reversescale=True,\n",
    "        opacity=0.6,\n",
    "        contours=dict(showlines=False),\n",
    "        colorbar=dict(title='P(y=1)'),\n",
    "        name='P(y=1)',\n",
    "    )\n",
    ")\n",
    "\n",
    "# Decision boundary: p = best_t\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        z=p_grid,\n",
    "        showscale=False,\n",
    "        contours=dict(start=best_t, end=best_t, size=1, coloring='lines'),\n",
    "        line=dict(color='black', width=3),\n",
    "        name='boundary',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_val[:, 0],\n",
    "        y=X_val[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=7, color=y_val, colorscale='Viridis', line=dict(width=0.5, color='white')),\n",
    "        name='validation points',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Decision boundary for threshold t={best_t:.2f}',\n",
    "    xaxis_title='x1',\n",
    "    yaxis_title='x2',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29057a15",
   "metadata": {},
   "source": [
    "## Practical usage (scikit-learn)\n",
    "\n",
    "For most workflows you’ll use scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- For multiclass, pass integer class labels.\n",
    "- For multilabel, `accuracy_score` computes **subset accuracy** (all labels must match).\n",
    "- Use `sample_weight=` if some samples should count more than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with scikit-learn's LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = clf.predict(X_val)\n",
    "print('sklearn LogisticRegression val accuracy:', sk_accuracy_score(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59ed29",
   "metadata": {},
   "source": [
    "## Pros / Cons / When to use\n",
    "\n",
    "### Pros\n",
    "- **Simple and interpretable**: “percent correct”.\n",
    "- Works well when **classes are balanced** and **error costs are similar**.\n",
    "- Useful as a quick baseline and sanity check.\n",
    "\n",
    "### Cons\n",
    "- **Misleading on imbalanced datasets** (majority-class baseline can look “great”).\n",
    "- Hides *which* mistakes you make (use a confusion matrix / per-class metrics).\n",
    "- For probabilistic models it is **threshold-dependent**.\n",
    "- Hard to optimize directly with gradient methods (0–1 loss is non-smooth).\n",
    "- For multilabel, subset accuracy can be **too strict**.\n",
    "\n",
    "### Good fits\n",
    "- Balanced multiclass problems (top-1 correctness matters).\n",
    "- Settings where false positives and false negatives have comparable cost.\n",
    "\n",
    "### Consider alternatives when\n",
    "- Classes are imbalanced: `balanced_accuracy_score`, precision/recall/F1, PR-AUC.\n",
    "- You care about ranking/threshold tradeoffs: ROC curves, PR curves.\n",
    "- You need probability quality: log loss, Brier score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787fc04",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Build a classifier that outputs probabilities and show how the **best threshold** shifts when class imbalance increases.\n",
    "2. Construct two models with identical accuracy but very different confusion matrices. Which one would you deploy for a medical screening task?\n",
    "3. For multilabel data, compare subset accuracy with per-label accuracy (Hamming accuracy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67e72b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn `accuracy_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "- scikit-learn classification metrics user guide: https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}