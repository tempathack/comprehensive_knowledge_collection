{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b799e55c",
   "metadata": {},
   "source": [
    "# brier_score_loss (Brier score)\n",
    "\n",
    "The **Brier score** is a proper scoring rule for **probabilistic binary classification**.  \n",
    "It measures the mean squared error between predicted probabilities and the actual outcomes.\n",
    "\n",
    "**In this notebook you will:**\n",
    "- understand the definition and its range\n",
    "- build intuition with plots\n",
    "- implement `brier_score_loss` from scratch in NumPy\n",
    "- use the Brier score as a differentiable training objective for a simple logistic regression model\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b321554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebe577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    out = np.empty_like(z)\n",
    "\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "\n",
    "    exp_z = np.exp(z[~pos])\n",
    "    out[~pos] = exp_z / (1.0 + exp_z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def logit(p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = np.clip(p, eps, 1.0 - eps)\n",
    "    return np.log(p / (1.0 - p))\n",
    "\n",
    "\n",
    "def calibrate_logit_scale(p: np.ndarray, k: float, eps: float = 1e-12) -> np.ndarray:\n",
    "    # Monotone calibration distortion: p -> sigmoid(k * logit(p)).\n",
    "    return sigmoid(k * logit(p, eps=eps))\n",
    "\n",
    "\n",
    "def calibration_bins(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10):\n",
    "    # Uniform-bin calibration curve + counts.\n",
    "\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_prob = np.asarray(y_prob, dtype=float).ravel()\n",
    "\n",
    "    if y_true.shape != y_prob.shape:\n",
    "        raise ValueError(\"y_true and y_prob must have the same shape\")\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_id = np.digitize(y_prob, bins) - 1\n",
    "    bin_id = np.clip(bin_id, 0, n_bins - 1)\n",
    "\n",
    "    mean_pred = np.full(n_bins, np.nan)\n",
    "    frac_pos = np.full(n_bins, np.nan)\n",
    "    counts = np.zeros(n_bins, dtype=int)\n",
    "\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_id == b\n",
    "        counts[b] = int(mask.sum())\n",
    "        if counts[b] == 0:\n",
    "            continue\n",
    "        mean_pred[b] = float(y_prob[mask].mean())\n",
    "        frac_pos[b] = float(y_true[mask].mean())\n",
    "\n",
    "    return bins, mean_pred, frac_pos, counts\n",
    "\n",
    "\n",
    "def brier_decomposition(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10):\n",
    "    # Murphy (1973) style decomposition using bins as 'forecast categories'.\n",
    "\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_prob = np.asarray(y_prob, dtype=float).ravel()\n",
    "\n",
    "    bs = float(np.mean((y_prob - y_true) ** 2))\n",
    "\n",
    "    _, mean_pred, frac_pos, counts = calibration_bins(y_true, y_prob, n_bins=n_bins)\n",
    "    mask = counts > 0\n",
    "\n",
    "    n = y_true.size\n",
    "    w = counts[mask] / n\n",
    "    rel = float(np.sum(w * (mean_pred[mask] - frac_pos[mask]) ** 2))\n",
    "\n",
    "    y_bar = float(y_true.mean())\n",
    "    res = float(np.sum(w * (frac_pos[mask] - y_bar) ** 2))\n",
    "\n",
    "    unc = float(y_bar * (1.0 - y_bar))\n",
    "\n",
    "    return bs, rel, res, unc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc28f85",
   "metadata": {},
   "source": [
    "## 1) Definition\n",
    "\n",
    "For binary events we observe labels\n",
    "\n",
    "$$\n",
    " y_i \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "and we predict probabilities for the positive class\n",
    "\n",
    "$$\n",
    " p_i = \\mathbb{P}(y_i = 1 \\mid x_i), \\qquad p_i \\in [0,1].\n",
    "$$\n",
    "\n",
    "The **Brier score (loss)** is\n",
    "\n",
    "$$\n",
    "\\mathrm{BS}(y,p)\n",
    "= \\frac{1}{n}\\sum_{i=1}^n (p_i - y_i)^2.\n",
    "$$\n",
    "\n",
    "- Lower is better; **0** is perfect.\n",
    "- For binary classification with $p_i \\in [0,1]$, the score lies in **$[0,1]$**.\n",
    "\n",
    "In scikit-learn, `sklearn.metrics.brier_score_loss(y_true, y_prob, ...)` expects `y_prob = P(y=pos_label)` (not hard class predictions).\n",
    "\n",
    "### Multiclass note\n",
    "\n",
    "`sklearn.metrics.brier_score_loss` is binary-only. A common multiclass generalization uses the probability vector $p_i \\in \\mathbb{R}^K$ and a one-hot label $e_{y_i}$:\n",
    "\n",
    "$$\n",
    "\\mathrm{BS}_K\n",
    "= \\frac{1}{n}\\sum_{i=1}^n \\lVert p_i - e_{y_i} \\rVert_2^2\n",
    "\\quad\n",
    "(\\text{sometimes divided by }K).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a035976",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(0.0, 1.0, 501)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=p, y=(p - 1.0) ** 2, mode=\"lines\", name=\"y=1: (1-p)^2\"))\n",
    "fig.add_trace(go.Scatter(x=p, y=(p - 0.0) ** 2, mode=\"lines\", name=\"y=0: p^2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Per-example Brier loss as a function of predicted probability\",\n",
    "    xaxis_title=\"Predicted probability p = P(y=1)\",\n",
    "    yaxis_title=\"Loss (p - y)^2\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089a7a0",
   "metadata": {},
   "source": [
    "## 2) Proper scoring rule (why “probabilities” matter)\n",
    "\n",
    "Assume the true event probability is $q$ and\n",
    "\n",
    "$$\n",
    "Y \\sim \\mathrm{Bernoulli}(q).\n",
    "$$\n",
    "\n",
    "If we always predict the same probability $p$, the **expected** Brier score is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(p - Y)^2]\n",
    "= (p-q)^2 + q(1-q).\n",
    "$$\n",
    "\n",
    "- The second term $q(1-q)$ is the **irreducible uncertainty** in the labels.\n",
    "- The first term $(p-q)^2$ is the **penalty for miscalibration**.\n",
    "\n",
    "So the expected Brier score is minimized at:\n",
    "\n",
    "$$\n",
    "p^* = q.\n",
    "$$\n",
    "\n",
    "That’s the defining property of a **(strictly) proper scoring rule**: telling the truth (predicting the real probability) is optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.3  # true event probability\n",
    "n = 50_000\n",
    "\n",
    "y = rng.binomial(1, q, size=n).astype(float)\n",
    "p_grid = np.linspace(0.0, 1.0, 401)\n",
    "\n",
    "empirical = np.mean((p_grid[None, :] - y[:, None]) ** 2, axis=0)\n",
    "analytic = (p_grid - q) ** 2 + q * (1.0 - q)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=p_grid, y=empirical, mode=\"lines\", name=\"empirical E[(p-Y)^2]\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=p_grid,\n",
    "        y=analytic,\n",
    "        mode=\"lines\",\n",
    "        name=\"analytic (p-q)^2 + q(1-q)\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "fig.add_vline(x=q, line=dict(color=\"black\", dash=\"dot\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Expected Brier score is minimized at p = q (here q={q})\",\n",
    "    xaxis_title=\"Forecast p\",\n",
    "    yaxis_title=\"Expected Brier score\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6beb382",
   "metadata": {},
   "source": [
    "## 3) Best constant predictor + Brier skill score\n",
    "\n",
    "If you predict the *same* probability $p$ for every sample, the Brier objective is\n",
    "\n",
    "$$\n",
    "\\min_p \\frac{1}{n}\\sum_{i=1}^n (p - y_i)^2.\n",
    "$$\n",
    "\n",
    "Taking the derivative and setting it to zero gives:\n",
    "\n",
    "$$\n",
    "p^* = \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n",
    "$$\n",
    "\n",
    "So the best constant forecast is simply the **base rate**.\n",
    "\n",
    "A common normalization is the **Brier Skill Score (BSS)** against a reference forecast (often the base rate):\n",
    "\n",
    "$$\n",
    "\\mathrm{BSS} = 1 - \\frac{\\mathrm{BS}_{\\text{model}}}{\\mathrm{BS}_{\\text{ref}}}.\n",
    "$$\n",
    "\n",
    "- `BSS = 1` is perfect\n",
    "- `BSS = 0` matches the reference\n",
    "- `BSS < 0` is worse than the reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8924632",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rng.binomial(1, 0.2, size=3000).astype(float)\n",
    "p_star = y.mean()\n",
    "\n",
    "p_grid = np.linspace(0.0, 1.0, 201)\n",
    "losses = np.array([np.mean((p - y) ** 2) for p in p_grid])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=p_grid, y=losses, mode=\"lines\", name=\"Brier(p)\"))\n",
    "fig.add_vline(\n",
    "    x=p_star,\n",
    "    line=dict(color=\"black\", dash=\"dot\"),\n",
    "    annotation_text=f\"mean(y) = {p_star:.3f}\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Best constant probability equals the base rate\",\n",
    "    xaxis_title=\"Constant prediction p\",\n",
    "    yaxis_title=\"Brier score\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"Best constant p*: {p_star:.4f}\")\n",
    "print(\n",
    "    f\"Brier score at p*: {np.mean((p_star - y) ** 2):.4f} \"\n",
    "    f\"(≈ p*(1-p) = {p_star*(1-p_star):.4f})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de22c3",
   "metadata": {},
   "source": [
    "## 4) Calibration intuition (and the Murphy decomposition)\n",
    "\n",
    "**Accuracy** only uses a hard decision (e.g. `p >= 0.5`).  \n",
    "The Brier score uses the *full* probability values, so it can distinguish between:\n",
    "\n",
    "- a well-calibrated forecast: “when I say 0.8, it happens ~80% of the time”\n",
    "- an overconfident forecast: probabilities pushed too close to 0 or 1\n",
    "- an underconfident forecast: probabilities too close to 0.5\n",
    "\n",
    "For binary forecasts, the Brier score can be decomposed into three interpretable parts (Murphy, 1973):\n",
    "\n",
    "$$\n",
    "\\mathrm{BS} = \\underbrace{\\mathrm{Rel}}_{\\text{reliability}}\n",
    "- \\underbrace{\\mathrm{Res}}_{\\text{resolution}}\n",
    "+ \\underbrace{\\mathrm{Unc}}_{\\text{uncertainty}}.\n",
    "$$\n",
    "\n",
    "Using probability bins (or exact forecast categories), with:\n",
    "- $p_k$ = average predicted probability in bin $k$\n",
    "- $o_k$ = observed frequency in bin $k$\n",
    "- $n_k$ = number of samples in bin $k$\n",
    "- $\\bar{y}$ = base rate\n",
    "\n",
    "the components are:\n",
    "\n",
    "$$\n",
    "\\mathrm{Rel} = \\sum_k \\frac{n_k}{n}(p_k - o_k)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{Res} = \\sum_k \\frac{n_k}{n}(o_k - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{Unc} = \\bar{y}(1-\\bar{y}).\n",
    "$$\n",
    "\n",
    "Intuition:\n",
    "- **Reliability** is (binned) calibration error (smaller is better)\n",
    "- **Resolution** rewards separating cases with different true frequencies (bigger is better)\n",
    "- **Uncertainty** is a property of the dataset only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca27623",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "x = rng.normal(size=n)\n",
    "\n",
    "# A \"true\" probability model\n",
    "p_true = sigmoid(1.5 * x - 0.25)\n",
    "y = rng.binomial(1, p_true)\n",
    "\n",
    "# Same hard predictions (threshold at 0.5), but different calibration\n",
    "p_calibrated = p_true\n",
    "p_overconf = calibrate_logit_scale(p_true, k=2.0)\n",
    "p_underconf = calibrate_logit_scale(p_true, k=0.5)\n",
    "\n",
    "models = {\n",
    "    \"calibrated (truth)\": p_calibrated,\n",
    "    \"overconfident (k=2)\": p_overconf,\n",
    "    \"underconfident (k=0.5)\": p_underconf,\n",
    "}\n",
    "\n",
    "print(\"Model comparison (same threshold decisions, different probability quality)\")\n",
    "for name, p in models.items():\n",
    "    y_hat = (p >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y, y_hat)\n",
    "    bs = brier_score_loss(y, p)\n",
    "    _, rel, res, unc = brier_decomposition(y, p, n_bins=10)\n",
    "    approx = rel - res + unc\n",
    "    print(\n",
    "        f\"{name:22s}  accuracy={acc:.3f}  brier={bs:.4f}  \"\n",
    "        f\"rel={rel:.4f}  res={res:.4f}  unc={unc:.4f}  (rel-res+unc≈{approx:.4f})\"\n",
    "    )\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Reliability diagram (calibration curve)\", \"Predicted probability distribution\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        name=\"perfect calibration\",\n",
    "        line=dict(color=\"black\", dash=\"dash\"),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "for name, p in models.items():\n",
    "    _, mean_pred, frac_pos, counts = calibration_bins(y, p, n_bins=10)\n",
    "    mask = counts > 0\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=mean_pred[mask], y=frac_pos[mask], mode=\"markers+lines\", name=name),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(go.Histogram(x=p, name=name, opacity=0.45, nbinsx=30), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Mean predicted probability\", range=[0, 1], row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Observed frequency\", range=[0, 1], row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Predicted probability\", range=[0, 1], row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"overlay\",\n",
    "    template=\"plotly_white\",\n",
    "    title=\"Calibration affects Brier score even when accuracy is unchanged\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0c6db",
   "metadata": {},
   "source": [
    "## 5) From-scratch NumPy implementation\n",
    "\n",
    "For binary labels, the implementation is just mean squared error:\n",
    "\n",
    "$$\n",
    "\\mathrm{BS} = \\frac{1}{n}\\sum_{i=1}^n (p_i - y_i)^2.\n",
    "$$\n",
    "\n",
    "Two practical details (matching `sklearn.metrics.brier_score_loss`):\n",
    "\n",
    "1) `y_prob` should be a probability for the **positive** class.\n",
    "2) If your labels are not `{0,1}`, you must specify which value is positive (`pos_label`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17adbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score_loss_np(y_true, y_prob, sample_weight=None, pos_label=None) -> float:\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_prob = np.asarray(y_prob, dtype=float).ravel()\n",
    "\n",
    "    if y_true.shape != y_prob.shape:\n",
    "        raise ValueError(\"y_true and y_prob must have the same shape\")\n",
    "\n",
    "    if pos_label is None:\n",
    "        unique = np.unique(y_true)\n",
    "        if np.all(np.isin(unique, [0, 1])):\n",
    "            pos_label = 1\n",
    "        else:\n",
    "            raise ValueError(\"pos_label must be specified when y_true is not in {0,1}\")\n",
    "\n",
    "    y01 = (y_true == pos_label).astype(float)\n",
    "    y_prob = np.clip(y_prob, 0.0, 1.0)\n",
    "\n",
    "    sq = (y_prob - y01) ** 2\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return float(np.mean(sq))\n",
    "\n",
    "    w = np.asarray(sample_weight, dtype=float).ravel()\n",
    "    if w.shape != y_true.shape:\n",
    "        raise ValueError(\"sample_weight must have the same shape as y_true\")\n",
    "    if np.any(w < 0):\n",
    "        raise ValueError(\"sample_weight cannot contain negative weights\")\n",
    "\n",
    "    return float(np.average(sq, weights=w))\n",
    "\n",
    "\n",
    "# Quick sanity checks vs scikit-learn\n",
    "\n",
    "y = rng.integers(0, 2, size=1000)\n",
    "p = rng.random(1000)\n",
    "\n",
    "print(\"sklearn:\", brier_score_loss(y, p))\n",
    "print(\"numpy  :\", brier_score_loss_np(y, p))\n",
    "print(\"allclose:\", np.allclose(brier_score_loss(y, p), brier_score_loss_np(y, p)))\n",
    "\n",
    "# pos_label example\n",
    "\n",
    "y_pm = rng.choice([-1, 1], size=1000)\n",
    "p = rng.random(1000)\n",
    "print(\"pos_label=1 (sklearn):\", brier_score_loss(y_pm, p, pos_label=1))\n",
    "print(\"pos_label=1 (numpy)  :\", brier_score_loss_np(y_pm, p, pos_label=1))\n",
    "\n",
    "# sample_weight example\n",
    "\n",
    "w = rng.random(1000)\n",
    "print(\"weighted (sklearn):\", brier_score_loss(y, p, sample_weight=w))\n",
    "print(\"weighted (numpy)  :\", brier_score_loss_np(y, p, sample_weight=w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43704872",
   "metadata": {},
   "source": [
    "## 6) Brier vs log loss (cross-entropy)\n",
    "\n",
    "Both Brier score and log loss are **proper scoring rules**, but they penalize mistakes differently.\n",
    "\n",
    "For a single example:\n",
    "\n",
    "- Brier: $(p-y)^2$ (bounded in $[0,1]$)\n",
    "- Log loss: $-y\\log p -(1-y)\\log(1-p)$ (unbounded for confident wrong predictions)\n",
    "\n",
    "Log loss heavily punishes being confidently wrong (e.g. predicting $p \\approx 0$ when $y=1$), while the Brier score is more forgiving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ddc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(1e-3, 1.0 - 1e-3, 500)\n",
    "\n",
    "brier_y1 = (1.0 - p) ** 2\n",
    "brier_y0 = p**2\n",
    "\n",
    "logloss_y1 = -np.log(p)\n",
    "logloss_y0 = -np.log(1.0 - p)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"y=1\", \"y=0\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=p, y=brier_y1, mode=\"lines\", name=\"Brier\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=p, y=logloss_y1, mode=\"lines\", name=\"Log loss\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=p, y=brier_y0, mode=\"lines\", name=\"Brier\"), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=p, y=logloss_y0, mode=\"lines\", name=\"Log loss\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Predicted probability p = P(y=1)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Predicted probability p = P(y=1)\", row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"loss\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"loss\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Per-example loss: Brier vs log loss\", template=\"plotly_white\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fd29f",
   "metadata": {},
   "source": [
    "## 7) Optimizing a simple model with the Brier loss (logistic regression)\n",
    "\n",
    "Because the Brier score is differentiable in the predicted probabilities, you can use it as a training objective.\n",
    "\n",
    "Let a logistic model predict\n",
    "\n",
    "$$\n",
    " p_i = \\sigma(z_i), \\qquad z_i = w^\\top x_i + b,\n",
    "$$\n",
    "\n",
    "where $\\sigma(t) = 1/(1+e^{-t})$.\n",
    "\n",
    "The Brier objective is:\n",
    "\n",
    "$$\n",
    "L(w,b) = \\frac{1}{n}\\sum_{i=1}^n (p_i - y_i)^2.\n",
    "$$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "= \\frac{2}{n}(p_i - y_i)\\,p_i(1-p_i).\n",
    "$$\n",
    "\n",
    "So the gradients are:\n",
    "\n",
    "$$\n",
    "\\nabla_w L = X^\\top \\frac{\\partial L}{\\partial z},\n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^n \\frac{\\partial L}{\\partial z_i}.\n",
    "$$\n",
    "\n",
    "A key difference vs log loss: the extra factor $p(1-p) \\le 0.25$ makes gradients **smaller near 0/1**, which can slow learning on confidently wrong examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D dataset so we can visualize the learned probabilities\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.5,\n",
    "    flip_y=0.03,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize (important for gradient descent)\n",
    "mu = X_train.mean(axis=0)\n",
    "sigma = X_train.std(axis=0) + 1e-12\n",
    "X_train_s = (X_train - mu) / sigma\n",
    "X_test_s = (X_test - mu) / sigma\n",
    "\n",
    "\n",
    "def train_logreg_brier_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    lr: float = 1.0,\n",
    "    n_iter: int = 1000,\n",
    "    l2: float = 1e-3,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    n, d = X.shape\n",
    "\n",
    "    w = rng_local.normal(scale=0.01, size=d)\n",
    "    b = 0.0\n",
    "\n",
    "    history = {\"brier\": []}\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        z = X @ w + b\n",
    "        p = sigmoid(z)\n",
    "\n",
    "        loss = np.mean((p - y) ** 2) + 0.5 * l2 * np.sum(w**2)\n",
    "        history[\"brier\"].append(float(loss))\n",
    "\n",
    "        dL_dz = (2.0 / n) * (p - y) * p * (1.0 - p)\n",
    "        grad_w = X.T @ dL_dz + l2 * w\n",
    "        grad_b = float(dL_dz.sum())\n",
    "\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "w, b, history = train_logreg_brier_gd(X_train_s, y_train, lr=1.0, n_iter=1000, l2=1e-3, seed=0)\n",
    "\n",
    "p_test = sigmoid(X_test_s @ w + b)\n",
    "\n",
    "print(\"From-scratch logistic regression (trained with Brier loss)\")\n",
    "print(f\"test accuracy       : {accuracy_score(y_test, (p_test >= 0.5).astype(int)):.3f}\")\n",
    "print(f\"test brier (numpy)  : {brier_score_loss_np(y_test, p_test):.4f}\")\n",
    "print(f\"test brier (sklearn): {brier_score_loss(y_test, p_test):.4f}\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=history[\"brier\"], mode=\"lines\", name=\"train Brier loss\"))\n",
    "fig.update_layout(\n",
    "    title=\"Gradient descent minimizing Brier loss\",\n",
    "    xaxis_title=\"Iteration\",\n",
    "    yaxis_title=\"Brier loss (+ L2 if set)\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Visualize the learned probability surface (test set)\n",
    "x0_min, x0_max = X_test_s[:, 0].min() - 0.75, X_test_s[:, 0].max() + 0.75\n",
    "x1_min, x1_max = X_test_s[:, 1].min() - 0.75, X_test_s[:, 1].max() + 0.75\n",
    "\n",
    "grid_x0 = np.linspace(x0_min, x0_max, 220)\n",
    "grid_x1 = np.linspace(x1_min, x1_max, 220)\n",
    "xx, yy = np.meshgrid(grid_x0, grid_x1)\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "proba = sigmoid(grid @ w + b).reshape(xx.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=grid_x0,\n",
    "        y=grid_x1,\n",
    "        z=proba,\n",
    "        contours=dict(start=0.0, end=1.0, size=0.1),\n",
    "        colorscale=\"RdBu\",\n",
    "        opacity=0.75,\n",
    "        colorbar=dict(title=\"P(y=1)\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=grid_x0,\n",
    "        y=grid_x1,\n",
    "        z=proba,\n",
    "        contours=dict(start=0.5, end=0.5, coloring=\"lines\"),\n",
    "        line=dict(color=\"black\", width=2),\n",
    "        showscale=False,\n",
    "        name=\"p=0.5\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_test_s[:, 0],\n",
    "        y=X_test_s[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            color=y_test,\n",
    "            colorscale=\"Portland\",\n",
    "            size=6,\n",
    "            line=dict(width=0.5, color=\"white\"),\n",
    "        ),\n",
    "        name=\"test points\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Learned probability surface and decision boundary (test set)\",\n",
    "    xaxis_title=\"feature 1 (standardized)\",\n",
    "    yaxis_title=\"feature 2 (standardized)\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Calibration curve on the test set\n",
    "_, mean_pred, frac_pos, counts = calibration_bins(y_test, p_test, n_bins=10)\n",
    "mask = counts > 0\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        name=\"perfect calibration\",\n",
    "        line=dict(color=\"black\", dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=mean_pred[mask], y=frac_pos[mask], mode=\"markers+lines\", name=\"model\"))\n",
    "fig.update_layout(\n",
    "    title=\"Reliability diagram (test set)\",\n",
    "    xaxis_title=\"Mean predicted probability\",\n",
    "    yaxis_title=\"Observed frequency\",\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Practical usage: the same metric with a standard scikit-learn classifier\n",
    "clf = LogisticRegression(max_iter=2000)\n",
    "clf.fit(X_train_s, y_train)\n",
    "p_sklearn = clf.predict_proba(X_test_s)[:, 1]\n",
    "print(\"\\nscikit-learn LogisticRegression (log loss optimized)\")\n",
    "print(f\"test accuracy: {accuracy_score(y_test, (p_sklearn >= 0.5).astype(int)):.3f}\")\n",
    "print(f\"test brier   : {brier_score_loss(y_test, p_sklearn):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1a17b",
   "metadata": {},
   "source": [
    "## 8) Pros, cons, and when to use it\n",
    "\n",
    "**Pros**\n",
    "- Proper scoring rule for probabilistic forecasts (encourages honest probabilities)\n",
    "- Interpretable as “MSE on probabilities”; bounded for binary problems\n",
    "- Sensitive to calibration; supports the reliability/resolution/uncertainty decomposition\n",
    "- Common in risk prediction, weather forecasting, calibration evaluation\n",
    "\n",
    "**Cons**\n",
    "- Less punishing for confident wrong predictions than log loss (can matter for training)\n",
    "- Value depends on the base rate; comparing scores across datasets can be misleading\n",
    "- Binary-only in `sklearn.metrics.brier_score_loss` (multiclass requires an extension)\n",
    "- On rare events, raw Brier numbers can look deceptively small; use a skill score or compare to a baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00085f4f",
   "metadata": {},
   "source": [
    "## 9) Pitfalls, diagnostics, exercises\n",
    "\n",
    "**Pitfalls**\n",
    "- Passing class labels instead of probabilities (use `predict_proba` for probabilistic classifiers)\n",
    "- Mis-specified positive class (`pos_label`) when labels are not `{0,1}`\n",
    "- Interpreting Brier without a baseline on imbalanced data\n",
    "\n",
    "**Diagnostics**\n",
    "- Plot a reliability diagram (calibration curve)\n",
    "- Report a baseline Brier score (base rate) and optionally Brier Skill Score\n",
    "\n",
    "**Exercises**\n",
    "1) Implement the multiclass Brier score and test it on a 3-class toy problem.\n",
    "2) Compute Brier Skill Score using the base rate as reference.\n",
    "3) Compare Brier vs log loss for a deliberately overconfident model.\n",
    "\n",
    "## References\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/modules/model_evaluation.html#brier-score-loss\n",
    "- Glenn W. Brier (1950), *Verification of forecasts expressed in terms of probability*\n",
    "- Allan H. Murphy (1973), *A new vector partition of the probability score*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}