{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa75c18",
   "metadata": {},
   "source": [
    "# `fbeta_score` (Fβ score)\n",
    "\n",
    "The **Fβ score** measures the quality of a **binary classifier** by combining **precision** and **recall** into a single number.\n",
    "\n",
    "It generalizes the F1 score by letting you choose how much more you care about **recall** vs **precision**.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Define Fβ from the confusion matrix (math + intuition)\n",
    "- Implement `fbeta_score` from scratch in NumPy (with edge cases)\n",
    "- Visualize how **β** and the **decision threshold** change the score (Plotly)\n",
    "- Use Fβ to *optimize* a simple classifier (threshold tuning + a smooth surrogate)\n",
    "\n",
    "## Quick import (reference)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import fbeta_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142ca41",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Binary classification with labels in `{0, 1}` (we treat `1` as the **positive** class)\n",
    "- Confusion matrix terms: TP, FP, FN, TN\n",
    "- Basic NumPy\n",
    "\n",
    "This notebook focuses on **binary** Fβ. For multiclass/multilabel, most libraries compute Fβ via one-vs-rest + averaging (micro/macro/weighted).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a08e29",
   "metadata": {},
   "source": [
    "## Confusion matrix (binary)\n",
    "\n",
    "Let:\n",
    "\n",
    "- true labels: \\(y \\in \\{0, 1\\}\\)\n",
    "- predicted labels: \\(\\hat{y} \\in \\{0, 1\\}\\)\n",
    "- `1` is the **positive** class\n",
    "\n",
    "|              | \\(\\hat{y}=1\\) | \\(\\hat{y}=0\\) |\n",
    "|--------------|--------------|--------------|\n",
    "| \\(y=1\\)      | TP           | FN           |\n",
    "| \\(y=0\\)      | FP           | TN           |\n",
    "\n",
    "Important: **Fβ does not use TN**. That’s a feature (when you care mostly about the positive class), but also a limitation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c74af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    z = np.clip(z, -50.0, 50.0)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def safe_divide(numer, denom, *, zero_division=0.0):\n",
    "    \"\"\"Elementwise numer/denom with a configurable value when denom == 0.\"\"\"\n",
    "    numer = np.asarray(numer, dtype=float)\n",
    "    denom = np.asarray(denom, dtype=float)\n",
    "    out = np.full_like(numer + denom, fill_value=float(zero_division), dtype=float)\n",
    "    np.divide(numer, denom, out=out, where=denom != 0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def confusion_counts_binary(y_true, y_pred, *, pos_label=1):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true{y_true.shape} vs y_pred{y_pred.shape}\")\n",
    "\n",
    "    y_true_pos = y_true == pos_label\n",
    "    y_pred_pos = y_pred == pos_label\n",
    "\n",
    "    tp = int(np.sum(y_true_pos & y_pred_pos))\n",
    "    fp = int(np.sum(~y_true_pos & y_pred_pos))\n",
    "    fn = int(np.sum(y_true_pos & ~y_pred_pos))\n",
    "    tn = int(np.sum(~y_true_pos & ~y_pred_pos))\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "\n",
    "def precision_recall_fbeta_from_counts(tp, fp, fn, *, beta=1.0, zero_division=0.0):\n",
    "    if beta <= 0:\n",
    "        raise ValueError(\"beta must be > 0\")\n",
    "    beta2 = beta**2\n",
    "\n",
    "    precision = float(safe_divide(tp, tp + fp, zero_division=zero_division))\n",
    "    recall = float(safe_divide(tp, tp + fn, zero_division=zero_division))\n",
    "\n",
    "    fbeta = float(\n",
    "        safe_divide(\n",
    "            (1.0 + beta2) * tp,\n",
    "            (1.0 + beta2) * tp + beta2 * fn + fp,\n",
    "            zero_division=zero_division,\n",
    "        )\n",
    "    )\n",
    "    return precision, recall, fbeta\n",
    "\n",
    "\n",
    "def precision_recall_fbeta(y_true, y_pred, *, beta=1.0, pos_label=1, zero_division=0.0):\n",
    "    tp, fp, fn, tn = confusion_counts_binary(y_true, y_pred, pos_label=pos_label)\n",
    "    return precision_recall_fbeta_from_counts(tp, fp, fn, beta=beta, zero_division=zero_division)\n",
    "\n",
    "\n",
    "def fbeta_score_numpy(y_true, y_pred, *, beta=1.0, pos_label=1, zero_division=0.0):\n",
    "    _, _, fbeta = precision_recall_fbeta(\n",
    "        y_true, y_pred, beta=beta, pos_label=pos_label, zero_division=zero_division\n",
    "    )\n",
    "    return fbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a04da",
   "metadata": {},
   "source": [
    "## Precision, recall, and Fβ (math)\n",
    "\n",
    "Precision and recall are:\n",
    "\n",
    "$$\n",
    "P = \\frac{TP}{TP + FP}, \\qquad R = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "The **Fβ score** is a weighted harmonic mean of \\(P\\) and \\(R\\):\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1+\\beta^2)PR}{\\beta^2 P + R}\n",
    "$$\n",
    "\n",
    "A very useful confusion-matrix form is:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1+\\beta^2)\\,TP}{(1+\\beta^2)\\,TP + \\beta^2\\,FN + FP}\n",
    "$$\n",
    "\n",
    "**How β changes the trade-off**\n",
    "\n",
    "- \\(\\beta = 1\\) gives **F1** (precision and recall weighted equally)\n",
    "- \\(\\beta > 1\\) favors **recall** (it upweights FN by \\(\\beta^2\\))\n",
    "- \\(\\beta < 1\\) favors **precision**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d73989",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 1, 1, 0, 0, 0, 0])\n",
    "y_pred = np.array([1, 0, 1, 1, 0, 0, 0])\n",
    "\n",
    "for beta in [0.5, 1.0, 2.0]:\n",
    "    p, r, f = precision_recall_fbeta(y_true, y_pred, beta=beta)\n",
    "    print(f\"beta={beta:>3}: precision={p:.3f}, recall={r:.3f}, Fbeta={f:.3f}\")\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import fbeta_score as skl_fbeta_score\n",
    "\n",
    "    print(\"\\nscikit-learn check:\")\n",
    "    for beta in [0.5, 1.0, 2.0]:\n",
    "        print(f\"beta={beta:>3}: sklearn={skl_fbeta_score(y_true, y_pred, beta=beta):.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n(scikit-learn not available for comparison)\")\n",
    "    print(\"Reason:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a3e04",
   "metadata": {},
   "source": [
    "## Scores vs labels: the role of the decision threshold\n",
    "\n",
    "Many models output a **score** or **probability** \\(s(x) \\in [0,1]\\), then convert it to a label using a threshold \\(t\\):\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\mathbb{1}[s(x) \\ge t]\n",
    "$$\n",
    "\n",
    "- Increasing \\(t\\) usually increases **precision** (fewer predicted positives) but decreases **recall**.\n",
    "- Since **Fβ depends on TP/FP/FN**, it depends on the choice of \\(t\\).\n",
    "\n",
    "A very common workflow is:\n",
    "\n",
    "1. Train a model with a differentiable loss (e.g., log loss)\n",
    "2. Choose \\(t\\) on a validation set to maximize your target metric (e.g., F2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44375fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_fbeta_curve(y_true, y_score, *, beta=1.0, thresholds=None, zero_division=0.0):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.0, 1.0, 301)\n",
    "    thresholds = np.asarray(thresholds, dtype=float)\n",
    "\n",
    "    pred_pos = y_score[:, None] >= thresholds[None, :]\n",
    "    y_pos = (y_true == 1)[:, None]\n",
    "\n",
    "    tp = np.sum(pred_pos & y_pos, axis=0)\n",
    "    fp = np.sum(pred_pos & ~y_pos, axis=0)\n",
    "    fn = np.sum(~pred_pos & y_pos, axis=0)\n",
    "\n",
    "    precision = safe_divide(tp, tp + fp, zero_division=zero_division)\n",
    "    recall = safe_divide(tp, tp + fn, zero_division=zero_division)\n",
    "\n",
    "    beta2 = beta**2\n",
    "    fbeta = safe_divide(\n",
    "        (1.0 + beta2) * tp,\n",
    "        (1.0 + beta2) * tp + beta2 * fn + fp,\n",
    "        zero_division=zero_division,\n",
    "    )\n",
    "\n",
    "    return thresholds, precision, recall, fbeta, tp, fp, fn\n",
    "\n",
    "\n",
    "def best_threshold_for_fbeta(y_true, y_score, *, beta=1.0, thresholds=None, zero_division=0.0):\n",
    "    thresholds, precision, recall, fbeta, tp, fp, fn = pr_fbeta_curve(\n",
    "        y_true, y_score, beta=beta, thresholds=thresholds, zero_division=zero_division\n",
    "    )\n",
    "    i = int(np.nanargmax(fbeta))\n",
    "    return {\n",
    "        \"threshold\": float(thresholds[i]),\n",
    "        \"fbeta\": float(fbeta[i]),\n",
    "        \"precision\": float(precision[i]),\n",
    "        \"recall\": float(recall[i]),\n",
    "        \"tp\": int(tp[i]),\n",
    "        \"fp\": int(fp[i]),\n",
    "        \"fn\": int(fn[i]),\n",
    "        \"index\": i,\n",
    "        \"thresholds\": thresholds,\n",
    "        \"precision_curve\": precision,\n",
    "        \"recall_curve\": recall,\n",
    "        \"fbeta_curve\": fbeta,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ff2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example: probability-like scores with overlap + class imbalance\n",
    "n_pos, n_neg = 180, 820\n",
    "y_true = np.r_[np.ones(n_pos, dtype=int), np.zeros(n_neg, dtype=int)]\n",
    "y_score = np.r_[rng.beta(5, 2, size=n_pos), rng.beta(2, 5, size=n_neg)]\n",
    "\n",
    "perm = rng.permutation(len(y_true))\n",
    "y_true, y_score = y_true[perm], y_score[perm]\n",
    "\n",
    "thresholds = np.linspace(0.0, 1.0, 301)\n",
    "_, precision, recall, _, _, _, _ = pr_fbeta_curve(y_true, y_score, beta=1.0, thresholds=thresholds)\n",
    "\n",
    "best_05 = best_threshold_for_fbeta(y_true, y_score, beta=0.5, thresholds=thresholds)\n",
    "best_1 = best_threshold_for_fbeta(y_true, y_score, beta=1.0, thresholds=thresholds)\n",
    "best_2 = best_threshold_for_fbeta(y_true, y_score, beta=2.0, thresholds=thresholds)\n",
    "\n",
    "best_05[\"threshold\"], best_1[\"threshold\"], best_2[\"threshold\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae655a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.12,\n",
    "    subplot_titles=(\"Precision & recall vs threshold\", \"Fβ vs threshold\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=precision, mode=\"lines\", name=\"precision\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=recall, mode=\"lines\", name=\"recall\"), row=1, col=1)\n",
    "\n",
    "for beta, best in [(0.5, best_05), (1.0, best_1), (2.0, best_2)]:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=best[\"thresholds\"],\n",
    "            y=best[\"fbeta_curve\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"F{beta:g}\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_vline(\n",
    "        x=best[\"threshold\"],\n",
    "        line_width=1,\n",
    "        line_dash=\"dot\",\n",
    "        line_color=\"gray\",\n",
    "        row=\"all\",\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"threshold t\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Fβ\", row=2, col=1)\n",
    "fig.update_layout(height=700, legend_orientation=\"h\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c9626",
   "metadata": {},
   "source": [
    "### Precision–recall curve + iso-Fβ lines\n",
    "\n",
    "As you sweep the threshold \\(t\\), you trace out a curve in (recall, precision) space.\n",
    "\n",
    "For a fixed \\(\\beta\\), you can also draw **iso-Fβ** curves. Points on higher iso-curves have higher Fβ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81482d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall,\n",
    "        y=precision,\n",
    "        mode=\"markers+lines\",\n",
    "        marker=dict(\n",
    "            color=thresholds,\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"threshold\"),\n",
    "            size=6,\n",
    "        ),\n",
    "        name=\"PR (threshold sweep)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Iso-Fβ lines for beta=2\n",
    "beta_iso = 2.0\n",
    "beta2 = beta_iso**2\n",
    "p_grid = np.linspace(1e-3, 1.0, 400)\n",
    "for F in [0.2, 0.4, 0.6, 0.8]:\n",
    "    denom = (1.0 + beta2) * p_grid - F\n",
    "    r = (F * beta2 * p_grid) / denom\n",
    "    mask = (denom > 0) & (r >= 0) & (r <= 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=r[mask],\n",
    "            y=p_grid[mask],\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=1, dash=\"dot\"),\n",
    "            name=f\"iso-F{beta_iso:g}={F}\",\n",
    "            opacity=0.8,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Precision–Recall curve with iso-F2 lines\",\n",
    "    xaxis_title=\"recall\",\n",
    "    yaxis_title=\"precision\",\n",
    "    height=600,\n",
    ")\n",
    "fig.update_xaxes(range=[0, 1])\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebbdda4",
   "metadata": {},
   "source": [
    "## Using Fβ to optimize a simple classifier\n",
    "\n",
    "Two practical ways to “optimize for Fβ” are:\n",
    "\n",
    "1. **Train** a model with a standard loss (e.g., log loss), then **tune the threshold** \\(t\\) to maximize Fβ on a validation set.\n",
    "2. Optimize a **smooth surrogate** of Fβ (use probabilities instead of hard labels) with gradient-based methods.\n",
    "\n",
    "We’ll do both with a from-scratch logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2de83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bias_for_target_rate(logits, target_rate, *, iters=60):\n",
    "    lo, hi = -20.0, 20.0\n",
    "    for _ in range(iters):\n",
    "        mid = (lo + hi) / 2.0\n",
    "        rate = sigmoid(logits + mid).mean()\n",
    "        if rate > target_rate:\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return (lo + hi) / 2.0\n",
    "\n",
    "\n",
    "def make_synthetic_logistic_data(n=4000, *, target_pos_rate=0.15, seed=0):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    X = rng_local.normal(size=(n, 2))\n",
    "    true_w = np.array([2.0, -1.2])\n",
    "    base_logits = X @ true_w\n",
    "    true_b = find_bias_for_target_rate(base_logits, target_pos_rate)\n",
    "    probs = sigmoid(base_logits + true_b)\n",
    "    y = rng_local.binomial(1, probs).astype(int)\n",
    "    return X, y, probs, (true_w, true_b)\n",
    "\n",
    "\n",
    "def train_val_test_split(X, y, *, ratios=(0.6, 0.2, 0.2), seed=0):\n",
    "    if not np.isclose(sum(ratios), 1.0):\n",
    "        raise ValueError(\"ratios must sum to 1\")\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    n = X.shape[0]\n",
    "    perm = rng_local.permutation(n)\n",
    "    X, y = X[perm], y[perm]\n",
    "\n",
    "    n_train = int(ratios[0] * n)\n",
    "    n_val = int(ratios[1] * n)\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train : n_train + n_val], y[n_train : n_train + n_val]\n",
    "    X_test, y_test = X[n_train + n_val :], y[n_train + n_val :]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "X, y, _, (true_w, true_b) = make_synthetic_logistic_data(n=4000, target_pos_rate=0.12, seed=1)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y, seed=1)\n",
    "\n",
    "print(\"positive rate (train/val/test):\", y_train.mean(), y_val.mean(), y_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e394ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def log_loss_and_grad(w, Xb, y, *, l2=0.0, eps=1e-12):\n",
    "    z = Xb @ w\n",
    "    p = sigmoid(z)\n",
    "\n",
    "    y = y.astype(float)\n",
    "    loss = -np.mean(y * np.log(p + eps) + (1.0 - y) * np.log(1.0 - p + eps))\n",
    "    if l2:\n",
    "        loss += 0.5 * l2 * np.sum(w[1:] ** 2)\n",
    "\n",
    "    grad = (Xb.T @ (p - y)) / Xb.shape[0]\n",
    "    if l2:\n",
    "        grad[1:] += l2 * w[1:]\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "def fit_logistic_regression_ce(X, y, *, lr=0.2, steps=1500, l2=0.0, seed=0):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    Xb = add_intercept(X)\n",
    "    w = rng_local.normal(scale=0.01, size=Xb.shape[1])\n",
    "\n",
    "    history = []\n",
    "    for step in range(steps):\n",
    "        loss, grad = log_loss_and_grad(w, Xb, y, l2=l2)\n",
    "        w -= lr * grad\n",
    "        if step % 20 == 0 or step == steps - 1:\n",
    "            history.append((step, loss))\n",
    "    return w, np.array(history)\n",
    "\n",
    "\n",
    "w_ce, hist_ce = fit_logistic_regression_ce(X_train, y_train, lr=0.3, steps=1200, l2=1e-3, seed=1)\n",
    "hist_ce[:5], hist_ce[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist_ce[:, 0], y=hist_ce[:, 1], mode=\"lines\", name=\"log loss\"))\n",
    "fig.update_layout(title=\"Logistic regression training (cross-entropy)\", xaxis_title=\"step\", yaxis_title=\"log loss\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(w, X):\n",
    "    Xb = add_intercept(X)\n",
    "    return sigmoid(Xb @ w)\n",
    "\n",
    "\n",
    "def evaluate_thresholded(y_true, y_score, *, threshold, beta=1.0):\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    tp, fp, fn, tn = confusion_counts_binary(y_true, y_pred)\n",
    "    precision, recall, fbeta = precision_recall_fbeta_from_counts(tp, fp, fn, beta=beta)\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"beta\": float(beta),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"fbeta\": float(fbeta),\n",
    "        \"tp\": int(tp),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"tn\": int(tn),\n",
    "    }\n",
    "\n",
    "\n",
    "val_scores_ce = predict_proba(w_ce, X_val)\n",
    "test_scores_ce = predict_proba(w_ce, X_test)\n",
    "\n",
    "betas = [0.5, 1.0, 2.0]\n",
    "rows = []\n",
    "for beta in betas:\n",
    "    best = best_threshold_for_fbeta(y_val, val_scores_ce, beta=beta, thresholds=np.linspace(0, 1, 501))\n",
    "    test_eval = evaluate_thresholded(y_test, test_scores_ce, threshold=best[\"threshold\"], beta=beta)\n",
    "    rows.append({\n",
    "        \"beta\": beta,\n",
    "        \"best_val_threshold\": best[\"threshold\"],\n",
    "        \"val_fbeta\": best[\"fbeta\"],\n",
    "        \"test_precision\": test_eval[\"precision\"],\n",
    "        \"test_recall\": test_eval[\"recall\"],\n",
    "        \"test_fbeta\": test_eval[\"fbeta\"],\n",
    "    })\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.DataFrame(rows)\n",
    "except Exception:\n",
    "    rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0ba60",
   "metadata": {},
   "source": [
    "Notice how the **optimal threshold changes with β**.\n",
    "\n",
    "- With \\(\\beta > 1\\), you typically pick a **lower** threshold to increase recall.\n",
    "- With \\(\\beta < 1\\), you often pick a **higher** threshold to increase precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c02730",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.0, 1.0, 501)\n",
    "\n",
    "fig = go.Figure()\n",
    "for beta in [0.5, 1.0, 2.0]:\n",
    "    best = best_threshold_for_fbeta(y_val, val_scores_ce, beta=beta, thresholds=thresholds)\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=best[\"fbeta_curve\"], mode=\"lines\", name=f\"val F{beta:g}\"))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[best[\"threshold\"]],\n",
    "            y=[best[\"fbeta\"]],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10),\n",
    "            name=f\"best t for F{beta:g}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Validation: Fβ vs threshold (same model, different β)\",\n",
    "    xaxis_title=\"threshold\",\n",
    "    yaxis_title=\"Fβ\",\n",
    "    height=500,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca2bc9",
   "metadata": {},
   "source": [
    "## Direct optimization (optional): a differentiable “soft Fβ” surrogate\n",
    "\n",
    "Hard Fβ uses **thresholded** predictions \\(\\hat{y} \\in \\{0,1\\}\\), so it’s not differentiable in the model parameters.\n",
    "\n",
    "A common trick is to replace \\(\\hat{y}\\) with the model probability \\(p\\in[0,1]\\) and define “soft” counts:\n",
    "\n",
    "$$\n",
    "\\widetilde{TP} = \\sum_i y_i p_i, \\quad\n",
    "\\widetilde{FP} = \\sum_i (1-y_i)p_i, \\quad\n",
    "\\widetilde{FN} = \\sum_i y_i (1-p_i)\n",
    "$$\n",
    "\n",
    "Then plug them into the same formula:\n",
    "\n",
    "$$\n",
    "\\widetilde{F}_\\beta = \\frac{(1+\\beta^2)\\widetilde{TP}}{(1+\\beta^2)\\widetilde{TP} + \\beta^2\\widetilde{FN} + \\widetilde{FP}}\n",
    "$$\n",
    "\n",
    "This surrogate is smooth in \\(p\\), so we can do gradient ascent on a logistic regression model.\n",
    "\n",
    "Caveat: optimizing \\(\\widetilde{F}_\\beta\\) is **not identical** to optimizing the hard-thresholded Fβ, but it can be a useful demonstration (and sometimes a practical heuristic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_fbeta_and_grad(w, Xb, y, *, beta=2.0, l2=0.0, eps=1e-12):\n",
    "    \"\"\"Return (soft_fbeta, grad_w) for a logistic regression model p = sigmoid(Xb @ w).\"\"\"\n",
    "    if beta <= 0:\n",
    "        raise ValueError(\"beta must be > 0\")\n",
    "    beta2 = beta**2\n",
    "\n",
    "    z = Xb @ w\n",
    "    p = sigmoid(z)\n",
    "    y = y.astype(float)\n",
    "\n",
    "    tp = np.sum(y * p)\n",
    "    sp = np.sum(p)  # tp + fp\n",
    "    pos = np.sum(y)\n",
    "    denom = sp + beta2 * pos + eps\n",
    "    f = (1.0 + beta2) * tp / denom\n",
    "\n",
    "    # dF/dp_i\n",
    "    dF_dp = (1.0 + beta2) * (y * denom - tp) / (denom**2)\n",
    "    dF_dz = dF_dp * p * (1.0 - p)\n",
    "    grad = Xb.T @ dF_dz\n",
    "\n",
    "    if l2:\n",
    "        f -= 0.5 * l2 * np.sum(w[1:] ** 2)\n",
    "        grad[1:] -= l2 * w[1:]\n",
    "\n",
    "    return float(f), grad\n",
    "\n",
    "\n",
    "def fit_logistic_regression_soft_fbeta(X, y, *, beta=2.0, lr=1e-3, steps=4000, l2=1e-3, seed=0):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    Xb = add_intercept(X)\n",
    "    w = rng_local.normal(scale=0.01, size=Xb.shape[1])\n",
    "\n",
    "    history = []\n",
    "    for step in range(steps):\n",
    "        f, grad = soft_fbeta_and_grad(w, Xb, y, beta=beta, l2=l2)\n",
    "        w += lr * grad\n",
    "        if step % 50 == 0 or step == steps - 1:\n",
    "            history.append((step, f))\n",
    "    return w, np.array(history)\n",
    "\n",
    "\n",
    "beta_opt = 2.0\n",
    "w_soft, hist_soft = fit_logistic_regression_soft_fbeta(\n",
    "    X_train, y_train, beta=beta_opt, lr=2e-3, steps=6000, l2=1e-3, seed=2\n",
    ")\n",
    "hist_soft[:5], hist_soft[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist_soft[:, 0], y=hist_soft[:, 1], mode=\"lines\", name=f\"soft F{beta_opt:g}\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Logistic regression training (maximize soft F{beta_opt:g})\",\n",
    "    xaxis_title=\"step\",\n",
    "    yaxis_title=f\"soft F{beta_opt:g}\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores_soft = predict_proba(w_soft, X_val)\n",
    "test_scores_soft = predict_proba(w_soft, X_test)\n",
    "\n",
    "best_ce = best_threshold_for_fbeta(y_val, val_scores_ce, beta=beta_opt, thresholds=np.linspace(0, 1, 501))\n",
    "best_soft = best_threshold_for_fbeta(y_val, val_scores_soft, beta=beta_opt, thresholds=np.linspace(0, 1, 501))\n",
    "\n",
    "test_ce = evaluate_thresholded(y_test, test_scores_ce, threshold=best_ce[\"threshold\"], beta=beta_opt)\n",
    "test_soft = evaluate_thresholded(y_test, test_scores_soft, threshold=best_soft[\"threshold\"], beta=beta_opt)\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"model\": \"cross-entropy + threshold tuning\",\n",
    "        \"val_best_threshold\": best_ce[\"threshold\"],\n",
    "        \"val_Fbeta\": best_ce[\"fbeta\"],\n",
    "        \"test_precision\": test_ce[\"precision\"],\n",
    "        \"test_recall\": test_ce[\"recall\"],\n",
    "        \"test_Fbeta\": test_ce[\"fbeta\"],\n",
    "    },\n",
    "    {\n",
    "        \"model\": f\"maximize soft F{beta_opt:g} + threshold tuning\",\n",
    "        \"val_best_threshold\": best_soft[\"threshold\"],\n",
    "        \"val_Fbeta\": best_soft[\"fbeta\"],\n",
    "        \"test_precision\": test_soft[\"precision\"],\n",
    "        \"test_recall\": test_soft[\"recall\"],\n",
    "        \"test_Fbeta\": test_soft[\"fbeta\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.DataFrame(rows)\n",
    "except Exception:\n",
    "    rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd7a91",
   "metadata": {},
   "source": [
    "## Pros / cons and when to use Fβ\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Focuses on the positive class** (TP/FP/FN) — useful for imbalanced problems where TN is less informative\n",
    "- **Adjustable trade-off** via \\(\\beta\\): pick recall-heavy (\\(\\beta>1\\)) or precision-heavy (\\(\\beta<1\\))\n",
    "- **Single number** summarizing the precision–recall trade-off (easy to compare models)\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Threshold-dependent**: you must choose a threshold (or a policy) to get a meaningful number\n",
    "- **Not a proper scoring rule**: it does not reward well-calibrated probabilities the way log loss / Brier score do\n",
    "- **Ignores TN**: can be misleading when TN matters (e.g., overall error rate is critical)\n",
    "- **Not smooth** in the hard form (can’t be directly optimized with gradient descent without surrogates)\n",
    "\n",
    "### When it’s a good fit\n",
    "\n",
    "- Information retrieval / search / recommendation (relevant items are “positive”)\n",
    "- Medical screening or safety monitoring (often \\(\\beta>1\\) to favor recall)\n",
    "- Fraud / abuse detection (often \\(\\beta<1\\) if false positives are expensive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce5a5a",
   "metadata": {},
   "source": [
    "## Pitfalls + diagnostics\n",
    "\n",
    "- Pick \\(\\beta\\) based on **real costs** (FN vs FP), not after looking at the test set.\n",
    "- Always tune thresholds on a **validation** set (or use cross-validation).\n",
    "- For highly imbalanced data, look at the full **precision–recall curve**; a single Fβ can hide failure modes.\n",
    "- Be explicit about the **positive class** (`pos_label`).\n",
    "- Decide how to handle **zero-division** cases (no predicted positives, or no actual positives).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb2a62",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement macro-averaged Fβ for multiclass classification via one-vs-rest.\n",
    "2. For a fixed model, show how the best threshold changes when \\(\\beta\\in\\{0.25,0.5,1,2,4\\}\\).\n",
    "3. Compare optimizing (a) log loss + threshold tuning vs (b) a soft-Fβ surrogate on a more extreme imbalance (e.g., 1% positives).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304340a9",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn `fbeta_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n",
    "- scikit-learn metrics overview: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- C. J. van Rijsbergen, *Information Retrieval* (discussion of the \\(F_\\beta\\) measure)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}