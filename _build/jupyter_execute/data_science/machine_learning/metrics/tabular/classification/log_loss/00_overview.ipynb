{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0849b99e",
   "metadata": {},
   "source": [
    "# log_loss (cross-entropy / negative log-likelihood)\n",
    "\n",
    "`log_loss` measures how well **predicted probabilities** match the true labels.\n",
    "It is the standard objective for logistic regression / softmax classifiers, and a common evaluation metric for probabilistic models.\n",
    "\n",
    "## Learning goals\n",
    "- understand the binary and multiclass formulas (with notation)\n",
    "- build intuition for why confident mistakes are punished heavily\n",
    "- implement numerically-stable log loss in NumPy (from probabilities and from logits)\n",
    "- see how minimizing log loss trains logistic regression via gradient descent\n",
    "- know when log loss is the right metric (and when it is not)\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import log_loss\n",
    "```\n",
    "\n",
    "## Table of contents\n",
    "1. Definitions and notation\n",
    "2. Intuition (plots)\n",
    "3. NumPy implementation (binary + multiclass)\n",
    "4. Using log loss to optimize logistic regression\n",
    "5. Pros, cons, pitfalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import log_loss as sk_log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9e005",
   "metadata": {},
   "source": [
    "## 1) Definitions and notation\n",
    "\n",
    "Assume we have $n$ examples.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "- True label: $y_i \\in \\{0,1\\}$\n",
    "- Predicted probability of the positive class: $p_i = P(y_i=1 \\mid x_i)$\n",
    "\n",
    "Per-example log loss (Bernoulli negative log-likelihood) is:\n",
    "\n",
    "$$\n",
    "\\ell_i = -\\Big(y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\\Big)\n",
    "$$\n",
    "\n",
    "Average (optionally weighted) log loss:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n}\\sum_{i=1}^n \\ell_i\n",
    "$$\n",
    "\n",
    "### Multiclass classification ($K$ classes)\n",
    "\n",
    "- True label: $y_i \\in \\{0,1,\\dots,K-1\\}$\n",
    "- Predicted probabilities: $p_{ik} = P(y_i=k \\mid x_i)$ with $\\sum_{k=0}^{K-1} p_{ik}=1$\n",
    "\n",
    "If we write one-hot targets $y_{ik} \\in \\{0,1\\}$, then:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{k=0}^{K-1} y_{ik}\\log(p_{ik})\n",
    "$$\n",
    "\n",
    "Equivalently (using integer labels):\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{n}\\sum_{i=1}^n \\log\\big(p_{i, y_i}\\big)\n",
    "$$\n",
    "\n",
    "### Why the log?\n",
    "\n",
    "If a model assigns probability $p_{i,y_i}$ to the true class, the likelihood of the dataset is:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^n p_{i,y_i}\n",
    "$$\n",
    "\n",
    "Taking `-log` turns a product into a sum:\n",
    "\n",
    "$$\n",
    "-\\log\\Big(\\prod_{i=1}^n p_{i,y_i}\\Big) = -\\sum_{i=1}^n \\log(p_{i,y_i})\n",
    "$$\n",
    "\n",
    "So minimizing log loss is the same as **maximizing likelihood**.\n",
    "\n",
    "### From logits (numerical stability)\n",
    "\n",
    "Sometimes models output **logits** (real-valued scores) instead of probabilities.\n",
    "\n",
    "Binary: logit $z_i = w^\\top x_i + b$, $p_i = \\sigma(z_i)$ where $\\sigma$ is the sigmoid.\n",
    "\n",
    "A stable per-sample loss is:\n",
    "\n",
    "$$\n",
    "\\ell_i = \\operatorname{softplus}(z_i) - y_i z_i,\\quad \\operatorname{softplus}(z)=\\log(1+e^z)\n",
    "$$\n",
    "\n",
    "Multiclass: logits $z_{ik}$, softmax probabilities $p_{ik} = \\frac{e^{z_{ik}}}{\\sum_j e^{z_{ij}}}$.\n",
    "\n",
    "Stable loss:\n",
    "\n",
    "$$\n",
    "\\ell_i = \\log\\Big(\\sum_{k=0}^{K-1} e^{z_{ik}}\\Big) - z_{i,y_i}\n",
    "$$\n",
    "\n",
    "In practice we also **clip probabilities** with a small $\\varepsilon$ to avoid $\\log(0)$ (which would be $+\\infty$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return np.where(z >= 0, 1.0 / (1.0 + np.exp(-z)), np.exp(z) / (1.0 + np.exp(z)))\n",
    "\n",
    "\n",
    "def softplus(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return np.logaddexp(0.0, z)\n",
    "\n",
    "\n",
    "def logsumexp(a, axis=None, keepdims=False):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    a_max = np.max(a, axis=axis, keepdims=True)\n",
    "    out = np.log(np.sum(np.exp(a - a_max), axis=axis, keepdims=True)) + a_max\n",
    "    if keepdims:\n",
    "        return out\n",
    "    if axis is None:\n",
    "        return out.squeeze()\n",
    "    return np.squeeze(out, axis=axis)\n",
    "\n",
    "\n",
    "def log_softmax(z, axis=1):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return z - logsumexp(z, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def _weighted_mean(values, sample_weight=None):\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    if sample_weight is None:\n",
    "        return float(np.mean(values))\n",
    "\n",
    "    w = np.asarray(sample_weight, dtype=float)\n",
    "    if w.shape != values.shape:\n",
    "        raise ValueError('sample_weight must have the same shape as values')\n",
    "    w_sum = np.sum(w)\n",
    "    if w_sum <= 0:\n",
    "        raise ValueError('sample_weight must sum to a positive number')\n",
    "    return float(np.sum(w * values) / w_sum)\n",
    "\n",
    "\n",
    "def log_loss_binary(y_true, y_prob, *, eps=1e-15, sample_weight=None):\n",
    "    \"\"\"Binary log loss from probabilities.\n",
    "\n",
    "    Parameters\n",
    "    - y_true: shape (n,), values in {0,1}\n",
    "    - y_prob: shape (n,), predicted P(y=1|x)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "\n",
    "    if y_true.shape != y_prob.shape:\n",
    "        raise ValueError('y_true and y_prob must have the same shape')\n",
    "\n",
    "    if np.any((y_true != 0) & (y_true != 1)):\n",
    "        raise ValueError('y_true must contain only 0/1 labels')\n",
    "\n",
    "    p = np.clip(y_prob, eps, 1.0 - eps)\n",
    "    losses = -(y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n",
    "    return _weighted_mean(losses, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "def log_loss_multiclass(y_true, y_prob, *, eps=1e-15, sample_weight=None):\n",
    "    \"\"\"Multiclass log loss from probabilities.\n",
    "\n",
    "    Parameters\n",
    "    - y_true: shape (n,), integer labels in {0,1,...,K-1}\n",
    "    - y_prob: shape (n,K), predicted class probabilities (rows should sum to 1)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "\n",
    "    if y_prob.ndim != 2:\n",
    "        raise ValueError('y_prob must be a 2D array of shape (n_samples, n_classes)')\n",
    "\n",
    "    n_samples, n_classes = y_prob.shape\n",
    "    if y_true.shape != (n_samples,):\n",
    "        raise ValueError('y_true must have shape (n_samples,)')\n",
    "\n",
    "    if np.any((y_true < 0) | (y_true >= n_classes)):\n",
    "        raise ValueError('y_true contains labels outside [0, n_classes)')\n",
    "\n",
    "    p = np.clip(y_prob, eps, 1.0 - eps)\n",
    "    p = p / p.sum(axis=1, keepdims=True)\n",
    "    losses = -np.log(p[np.arange(n_samples), y_true])\n",
    "    return _weighted_mean(losses, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "def log_loss_binary_from_logits(y_true, logits, *, sample_weight=None):\n",
    "    \"\"\"Binary log loss from logits: softplus(z) - y*z.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    logits = np.asarray(logits, dtype=float)\n",
    "\n",
    "    if y_true.shape != logits.shape:\n",
    "        raise ValueError('y_true and logits must have the same shape')\n",
    "    if np.any((y_true != 0) & (y_true != 1)):\n",
    "        raise ValueError('y_true must contain only 0/1 labels')\n",
    "\n",
    "    losses = softplus(logits) - y_true * logits\n",
    "    return _weighted_mean(losses, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "def log_loss_multiclass_from_logits(y_true, logits, *, sample_weight=None):\n",
    "    \"\"\"Multiclass log loss from logits: -log softmax(true_class).\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    logits = np.asarray(logits, dtype=float)\n",
    "\n",
    "    if logits.ndim != 2:\n",
    "        raise ValueError('logits must be a 2D array of shape (n_samples, n_classes)')\n",
    "\n",
    "    n_samples, n_classes = logits.shape\n",
    "    if y_true.shape != (n_samples,):\n",
    "        raise ValueError('y_true must have shape (n_samples,)')\n",
    "    if np.any((y_true < 0) | (y_true >= n_classes)):\n",
    "        raise ValueError('y_true contains labels outside [0, n_classes)')\n",
    "\n",
    "    log_probs = log_softmax(logits, axis=1)\n",
    "    losses = -log_probs[np.arange(n_samples), y_true]\n",
    "    return _weighted_mean(losses, sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4e052",
   "metadata": {},
   "source": [
    "## 2) Intuition (plots)\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "- if the true label is 1, the loss is $-\\log(p)$\n",
    "- if the true label is 0, the loss is $-\\log(1-p)$\n",
    "\n",
    "So **being confidently wrong** is punished heavily (the loss goes to $+\\infty$ as the predicted probability goes to 0 for the true class).\n",
    "\n",
    "A key property: log loss is a **strictly proper scoring rule**.\n",
    "If the true label is Bernoulli with positive rate $q$, then the *expected* loss of predicting $p$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\ell(p)] = -q\\log(p) - (1-q)\\log(1-p)\n",
    "$$\n",
    "\n",
    "This is the **cross-entropy** $H(q,p)$ and it is minimized at $p=q$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "p = np.linspace(eps, 1 - eps, 800)\n",
    "\n",
    "loss_y1 = -np.log(p)\n",
    "loss_y0 = -np.log(1 - p)\n",
    "\n",
    "q = 0.7\n",
    "expected_loss = -(q * np.log(p) + (1 - q) * np.log(1 - p))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        'Per-sample log loss as a function of predicted probability',\n",
    "        'Expected log loss when the true positive rate is q',\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=p, y=loss_y1, name='y=1: -log(p)'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=p, y=loss_y0, name='y=0: -log(1-p)'), row=1, col=1)\n",
    "fig.update_xaxes(title_text='predicted probability p', row=1, col=1)\n",
    "fig.update_yaxes(title_text='loss', row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=p, y=expected_loss, name='E[loss]'), row=1, col=2)\n",
    "fig.add_vline(x=q, line_width=2, line_dash='dash', line_color='black', row=1, col=2)\n",
    "fig.add_annotation(\n",
    "    x=q,\n",
    "    y=float(expected_loss[np.argmin(np.abs(p - q))]),\n",
    "    text='minimum at p=q',\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    ax=40,\n",
    "    ay=-30,\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.update_xaxes(title_text='predicted probability p', row=1, col=2)\n",
    "fig.update_yaxes(title_text='expected loss', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=420, legend=dict(orientation='h', yanchor='bottom', y=1.02))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defe563",
   "metadata": {},
   "source": [
    "## 3) NumPy implementation: quick sanity checks\n",
    "\n",
    "A small example showing why log loss is sensitive to a single confident mistake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 1, 1, 0, 0, 0])\n",
    "\n",
    "p_good = np.array([0.9, 0.8, 0.7, 0.3, 0.2, 0.1])\n",
    "p_one_confident_mistake = np.array([0.9, 0.8, 0.01, 0.3, 0.2, 0.99])\n",
    "\n",
    "print('mean log loss (good):', log_loss_binary(y_true, p_good))\n",
    "print('mean log loss (one confident mistake):', log_loss_binary(y_true, p_one_confident_mistake))\n",
    "print('sklearn check:', sk_log_loss(y_true, p_good))\n",
    "\n",
    "eps = 1e-15\n",
    "losses_good = -(y_true * np.log(np.clip(p_good, eps, 1 - eps)) + (1 - y_true) * np.log(1 - np.clip(p_good, eps, 1 - eps)))\n",
    "losses_bad = -(y_true * np.log(np.clip(p_one_confident_mistake, eps, 1 - eps)) + (1 - y_true) * np.log(1 - np.clip(p_one_confident_mistake, eps, 1 - eps)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=np.arange(len(y_true)), y=losses_good, name='per-sample loss (good)'))\n",
    "fig.add_trace(go.Bar(x=np.arange(len(y_true)), y=losses_bad, name='per-sample loss (one confident mistake)'))\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='A single confident mistake can dominate mean log loss',\n",
    "    xaxis_title='sample index',\n",
    "    yaxis_title='per-sample loss',\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "p_baseline = np.full_like(y_true, y_true.mean(), dtype=float)\n",
    "print('baseline (predict base rate p=mean(y)):', log_loss_binary(y_true, p_baseline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41b595",
   "metadata": {},
   "source": [
    "### Multiclass example\n",
    "\n",
    "For multiclass problems you pass a probability matrix of shape `(n_samples, n_classes)`.\n",
    "The loss for each sample is simply `-log(probability_assigned_to_the_true_class)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_mc = np.array([0, 2, 1, 2])\n",
    "P = np.array(\n",
    "    [\n",
    "        [0.7, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.7],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.05, 0.05, 0.9],\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('multiclass log loss (numpy):', log_loss_multiclass(y_true_mc, P))\n",
    "print('multiclass log loss (sklearn):', sk_log_loss(y_true_mc, P, labels=[0, 1, 2]))\n",
    "\n",
    "# log-loss from logits should match log-loss from probabilities\n",
    "Z = np.log(P)\n",
    "print('multiclass log loss from logits (numpy):', log_loss_multiclass_from_logits(y_true_mc, Z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd976ef6",
   "metadata": {},
   "source": [
    "## 4) Using log loss to optimize logistic regression (NumPy)\n",
    "\n",
    "Binary logistic regression models:\n",
    "\n",
    "$$\n",
    "z_i = w^\\top x_i + b,\\quad p_i = \\sigma(z_i)\n",
    "$$\n",
    "\n",
    "and minimizes the average log loss:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{n}\\sum_{i=1}^n \\Big(-y_i\\log(p_i) - (1-y_i)\\log(1-p_i)\\Big)\n",
    "$$\n",
    "\n",
    "A very useful fact for optimization is that the derivative w.r.t. the logit is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_i} = p_i - y_i\n",
    "$$\n",
    "\n",
    "So the gradients are:\n",
    "\n",
    "$$\n",
    "\\nabla_w J = \\frac{1}{n} X^\\top (p - y),\\quad \\frac{\\partial J}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^n (p_i - y_i)\n",
    "$$\n",
    "\n",
    "Below is a simple gradient descent optimizer that learns `w` and `b` by directly minimizing log loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68282f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_fit_transform(X):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    std = np.where(std == 0, 1.0, std)\n",
    "    return (X - mean) / std, mean, std\n",
    "\n",
    "\n",
    "def standardize_transform(X, mean, std):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    std = np.where(std == 0, 1.0, std)\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def fit_logistic_regression_gd(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val=None,\n",
    "    y_val=None,\n",
    "    *,\n",
    "    lr=0.2,\n",
    "    n_steps=300,\n",
    "    l2=0.0,\n",
    "):\n",
    "    X_train = np.asarray(X_train, dtype=float)\n",
    "    y_train = np.asarray(y_train)\n",
    "    n_samples, n_features = X_train.shape\n",
    "\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "\n",
    "    history = {\n",
    "        'step': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "    }\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        logits = X_train @ w + b\n",
    "        p = sigmoid(logits)\n",
    "\n",
    "        loss = log_loss_binary(y_train, p)\n",
    "        grad_w = (X_train.T @ (p - y_train)) / n_samples + l2 * w\n",
    "        grad_b = float(np.mean(p - y_train))\n",
    "\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "        pred = (p >= 0.5).astype(int)\n",
    "        acc = float(np.mean(pred == y_train))\n",
    "\n",
    "        history['step'].append(step)\n",
    "        history['train_loss'].append(loss)\n",
    "        history['train_acc'].append(acc)\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            logits_val = X_val @ w + b\n",
    "            p_val = sigmoid(logits_val)\n",
    "            history['val_loss'].append(log_loss_binary(y_val, p_val))\n",
    "            history['val_acc'].append(float(np.mean((p_val >= 0.5).astype(int) == y_val)))\n",
    "\n",
    "    return w, b, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(\n",
    "    n_samples=800,\n",
    "    centers=2,\n",
    "    n_features=2,\n",
    "    cluster_std=2.2,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=0,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "X_train_s, mean, std = standardize_fit_transform(X_train)\n",
    "X_val_s = standardize_transform(X_val, mean, std)\n",
    "\n",
    "w, b, hist = fit_logistic_regression_gd(\n",
    "    X_train_s,\n",
    "    y_train,\n",
    "    X_val=X_val_s,\n",
    "    y_val=y_val,\n",
    "    lr=0.2,\n",
    "    n_steps=250,\n",
    ")\n",
    "\n",
    "fig = make_subplots(specs=[[{'secondary_y': True}]])\n",
    "fig.add_trace(go.Scatter(x=hist['step'], y=hist['train_loss'], name='train log loss'), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=hist['step'], y=hist['val_loss'], name='val log loss'), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=hist['step'], y=hist['train_acc'], name='train accuracy'), secondary_y=True)\n",
    "fig.add_trace(go.Scatter(x=hist['step'], y=hist['val_acc'], name='val accuracy'), secondary_y=True)\n",
    "\n",
    "fig.update_xaxes(title_text='gradient descent step')\n",
    "fig.update_yaxes(title_text='log loss (lower is better)', secondary_y=False)\n",
    "fig.update_yaxes(title_text='accuracy', range=[0, 1], secondary_y=True)\n",
    "fig.update_layout(title='Minimizing log loss trains logistic regression', height=420)\n",
    "fig.show()\n",
    "\n",
    "print('final train loss:', hist['train_loss'][-1])\n",
    "print('final val loss:', hist['val_loss'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4449ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_min, x0_max = X_train_s[:, 0].min() - 0.8, X_train_s[:, 0].max() + 0.8\n",
    "x1_min, x1_max = X_train_s[:, 1].min() - 0.8, X_train_s[:, 1].max() + 0.8\n",
    "\n",
    "x0 = np.linspace(x0_min, x0_max, 220)\n",
    "x1 = np.linspace(x1_min, x1_max, 220)\n",
    "xx0, xx1 = np.meshgrid(x0, x1)\n",
    "grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "\n",
    "prob_grid = sigmoid(grid @ w + b).reshape(xx0.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=x0,\n",
    "        y=x1,\n",
    "        z=prob_grid,\n",
    "        contours=dict(start=0.0, end=1.0, size=0.1),\n",
    "        colorscale='RdBu',\n",
    "        opacity=0.85,\n",
    "        colorbar=dict(title='P(y=1)'),\n",
    "        name='P(y=1)',\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=x0,\n",
    "        y=x1,\n",
    "        z=prob_grid,\n",
    "        contours=dict(start=0.5, end=0.5, size=0.5),\n",
    "        showscale=False,\n",
    "        line=dict(color='black', width=3),\n",
    "        hoverinfo='skip',\n",
    "        name='decision boundary (p=0.5)',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_train_s[:, 0],\n",
    "        y=X_train_s[:, 1],\n",
    "        mode='markers',\n",
    "        name='train',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=y_train,\n",
    "            cmin=0,\n",
    "            cmax=1,\n",
    "            colorscale=[[0, '#1f77b4'], [1, '#d62728']],\n",
    "            line=dict(width=0.5, color='black'),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_val_s[:, 0],\n",
    "        y=X_val_s[:, 1],\n",
    "        mode='markers',\n",
    "        name='val',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            symbol='x',\n",
    "            color=y_val,\n",
    "            cmin=0,\n",
    "            cmax=1,\n",
    "            colorscale=[[0, '#1f77b4'], [1, '#d62728']],\n",
    "            line=dict(width=1.0, color='black'),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Decision boundary after minimizing log loss (standardized feature space)',\n",
    "    xaxis_title='feature 1 (standardized)',\n",
    "    yaxis_title='feature 2 (standardized)',\n",
    "    height=520,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2bdf1",
   "metadata": {},
   "source": [
    "## 5) Pros, cons, pitfalls\n",
    "\n",
    "### Pros\n",
    "- **Uses probabilities**: rewards calibrated predictions, not just correct hard labels.\n",
    "- **Strictly proper scoring rule**: in expectation, you minimize it by predicting the true conditional probabilities.\n",
    "- **Differentiable**: works naturally as a training objective (logistic regression, neural nets, softmax models).\n",
    "- **Works for multiclass**: via categorical cross-entropy.\n",
    "\n",
    "### Cons / caveats\n",
    "- **Harder to interpret** than accuracy (units are nats if using natural logs).\n",
    "- **Unbounded above**: a few confidently wrong predictions can dominate the mean.\n",
    "- **Sensitive to label noise**: mislabeled points can produce very large losses if the model is confident.\n",
    "- Requires good probability estimates; models that only rank well (AUC) can still have poor log loss.\n",
    "\n",
    "### Common pitfalls\n",
    "- Passing **hard class labels** instead of probabilities (log loss expects probabilities).\n",
    "- For multiclass, probability rows must align with label order and sum to 1.\n",
    "- With scikit-learn, if `y_true` contains only one class, pass `labels=[...]` to define the full label set.\n",
    "- Not clipping probabilities leads to `log(0)` and infinite loss; use a small $\\varepsilon$.\n",
    "\n",
    "### Where it is a good fit\n",
    "- When you care about **probability quality**: risk estimation, triage systems, cost-sensitive decisions.\n",
    "- When you want an evaluation metric that matches the training objective for probabilistic classifiers.\n",
    "- When comparing calibrated models (often alongside calibration curves / Brier score).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42203817",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "- Derive $\\partial \\ell / \\partial z = p - y$ for the binary case.\n",
    "- Implement multiclass gradient descent for softmax regression using `log_loss_multiclass_from_logits`.\n",
    "- Compare log loss and accuracy on an imbalanced dataset; notice that accuracy can look good even with poor probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ad89d",
   "metadata": {},
   "source": [
    "## References\n",
    "- scikit-learn: `sklearn.metrics.log_loss` https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "- Cross-entropy and negative log-likelihood (NLL): https://en.wikipedia.org/wiki/Cross_entropy\n",
    "- Proper scoring rules: https://en.wikipedia.org/wiki/Scoring_rule\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}