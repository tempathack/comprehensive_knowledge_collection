{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9277cb16",
   "metadata": {},
   "source": [
    "# AUC / ROC AUC (Area Under the ROC Curve)\n",
    "\n",
    "AUC-ROC is a **ranking metric** for binary classification. It answers a simple question:\n",
    "\n",
    "> If we randomly pick one positive and one negative example, what’s the probability the model assigns a **higher score** to the positive one?\n",
    "\n",
    "This notebook focuses on **ROC AUC** (the most common meaning of “AUC” in classification).\n",
    "\n",
    "**Goals**\n",
    "- Build the ROC curve from the confusion matrix by sweeping a threshold\n",
    "- Compute AUC via the trapezoidal rule (what `sklearn.metrics.auc` does)\n",
    "- Implement `roc_curve` + `roc_auc_score` from scratch in NumPy\n",
    "- Visualize why AUC is threshold-free and how it relates to ranking\n",
    "- Use an AUC-inspired surrogate loss to train a simple linear model\n",
    "\n",
    "**Quick import (scikit-learn)**\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "```\n",
    "\n",
    "**Assumption throughout:** binary labels `y ∈ {0,1}` and a real-valued score `s(x)` where larger means “more positive”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefa2e6",
   "metadata": {},
   "source": [
    "## 1) Intuition: AUC is about *ordering*, not thresholds\n",
    "\n",
    "Suppose your model outputs a score $s(x)$ (a probability, logit, margin, etc.).\n",
    "\n",
    "- If we pick a threshold $\\tau$, we turn scores into hard predictions.\n",
    "- If we **sweep** $\\tau$ from $+\\infty$ down to $-\\infty$, we trace out the ROC curve.\n",
    "- AUC summarizes the entire ROC curve into one number in $[0,1]$.\n",
    "\n",
    "Two key takeaways:\n",
    "\n",
    "1. **Only ranking matters:** any strictly increasing transform $g$ preserves AUC.\n",
    "\n",
    "   $$\\mathrm{AUC}(s) = \\mathrm{AUC}(g\\circ s)$$\n",
    "\n",
    "2. **Pairwise probability interpretation:**\n",
    "\n",
    "   $$\n",
    "   \\mathrm{AUC}\n",
    "   = \\mathbb{P}(s(X^+) > s(X^-))\n",
    "   + \\tfrac{1}{2}\\,\\mathbb{P}(s(X^+) = s(X^-))\n",
    "   $$\n",
    "\n",
    "   where $X^+$ is a random positive example and $X^-$ is a random negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy \"score\" example: overlapping score distributions\n",
    "n_pos, n_neg = 400, 600\n",
    "\n",
    "pos_scores = rng.normal(loc=1.0, scale=1.0, size=n_pos)\n",
    "neg_scores = rng.normal(loc=0.0, scale=1.0, size=n_neg)\n",
    "\n",
    "y_true = np.r_[np.ones(n_pos, dtype=int), np.zeros(n_neg, dtype=int)]\n",
    "y_score = np.r_[pos_scores, neg_scores]\n",
    "\n",
    "toy_auc = roc_auc_score(y_true, y_score)\n",
    "print(f\"Toy ROC AUC = {toy_auc:.3f}\")\n",
    "\n",
    "fig = px.histogram(\n",
    "    {\"score\": y_score, \"class\": np.where(y_true == 1, \"positive\", \"negative\")},\n",
    "    x=\"score\",\n",
    "    color=\"class\",\n",
    "    opacity=0.6,\n",
    "    barmode=\"overlay\",\n",
    "    marginal=\"box\",\n",
    "    title=\"Toy scores: overlap between classes\",\n",
    "    labels={\"score\": \"model score s(x)\", \"class\": \"true class\"},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cff6dc",
   "metadata": {},
   "source": [
    "## 2) From thresholds to the ROC curve\n",
    "\n",
    "Given a threshold $\\tau$, convert scores to predicted labels:\n",
    "\n",
    "$$\\hat{y}_i(\\tau) = \\mathbb{1}[s_i \\ge \\tau]$$\n",
    "\n",
    "Confusion-matrix counts (as functions of $\\tau$):\n",
    "\n",
    "$$\n",
    "\\mathrm{TP}(\\tau) = \\sum_{i=1}^n \\mathbb{1}[y_i=1 \\land \\hat{y}_i(\\tau)=1]\n",
    "\\quad\n",
    "\\mathrm{FP}(\\tau) = \\sum_{i=1}^n \\mathbb{1}[y_i=0 \\land \\hat{y}_i(\\tau)=1]\n",
    "$$\n",
    "$$\n",
    "\\mathrm{TN}(\\tau) = \\sum_{i=1}^n \\mathbb{1}[y_i=0 \\land \\hat{y}_i(\\tau)=0]\n",
    "\\quad\n",
    "\\mathrm{FN}(\\tau) = \\sum_{i=1}^n \\mathbb{1}[y_i=1 \\land \\hat{y}_i(\\tau)=0]\n",
    "$$\n",
    "\n",
    "Normalize to rates:\n",
    "\n",
    "$$\n",
    "\\mathrm{TPR}(\\tau) = \\frac{\\mathrm{TP}(\\tau)}{P}\n",
    "\\qquad\n",
    "\\mathrm{FPR}(\\tau) = \\frac{\\mathrm{FP}(\\tau)}{N}\n",
    "$$\n",
    "\n",
    "where $P$ is the number of positives and $N$ the number of negatives.\n",
    "\n",
    "The ROC curve is the set of points:\n",
    "\n",
    "$$\\{(\\mathrm{FPR}(\\tau), \\mathrm{TPR}(\\tau)) : \\tau \\in \\mathbb{R}\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507616f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_counts_at_threshold(y_true, y_score, threshold):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score)\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "\n",
    "for thr in [2.0, 1.0, 0.0, -1.0]:\n",
    "    tp, fp, tn, fn = confusion_counts_at_threshold(y_true, y_score, threshold=thr)\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    print(f\"thr={thr:>4}: TP={tp:>3} FP={fp:>3} TN={tn:>3} FN={fn:>3} | TPR={tpr:.3f} FPR={fpr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16166a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode=\"lines\",\n",
    "        name=f\"ROC (AUC = {roc_auc:.3f})\",\n",
    "        line=dict(width=3),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        name=\"Random (AUC = 0.5)\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Color ROC points by threshold (skip the first threshold = +inf)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr[1:],\n",
    "        y=tpr[1:],\n",
    "        mode=\"markers\",\n",
    "        name=\"Threshold points\",\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=thresholds[1:],\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"threshold\"),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC curve (TPR vs FPR) from a threshold sweep\",\n",
    "    xaxis_title=\"False Positive Rate (FPR)\",\n",
    "    yaxis_title=\"True Positive Rate (TPR)\",\n",
    "    width=750,\n",
    "    height=520,\n",
    ")\n",
    "\n",
    "# Make the plot square-ish so the diagonal really looks like 45 degrees\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882a276",
   "metadata": {},
   "source": [
    "## 3) AUC = area under the ROC curve (trapezoidal rule)\n",
    "\n",
    "Once you have ROC points $(x_k, y_k)$ with $x_k$ = FPR and $y_k$ = TPR sorted by increasing FPR,\n",
    "you can approximate the integral using trapezoids:\n",
    "\n",
    "$$\n",
    "\\mathrm{AUC} \\approx \\sum_{k=0}^{m-2} (x_{k+1} - x_k)\\,\\frac{y_{k+1}+y_k}{2}\n",
    "$$\n",
    "\n",
    "This is exactly what `sklearn.metrics.auc(x, y)` computes (for any curve, not only ROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_trapezoid(x, y):\n",
    "    '''Compute area under a curve via the trapezoidal rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : 1D arrays of same length\n",
    "        x must be monotonically increasing.\n",
    "    '''\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if x.ndim != 1 or y.ndim != 1 or x.shape != y.shape:\n",
    "        raise ValueError(\"x and y must be 1D arrays with the same shape\")\n",
    "\n",
    "    dx = np.diff(x)\n",
    "    if np.any(dx < 0):\n",
    "        raise ValueError(\"x must be monotonically increasing\")\n",
    "\n",
    "    return float(np.sum(dx * (y[1:] + y[:-1]) / 2.0))\n",
    "\n",
    "\n",
    "print(f\"sklearn auc(fpr,tpr)          = {auc(fpr, tpr):.10f}\")\n",
    "print(f\"NumPy  auc_trapezoid(fpr,tpr) = {auc_trapezoid(fpr, tpr):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127a09a",
   "metadata": {},
   "source": [
    "## 4) From scratch: ROC curve and ROC AUC in NumPy\n",
    "\n",
    "A convenient way to build the ROC curve:\n",
    "\n",
    "1. Sort examples by score (descending).\n",
    "2. Sweep a threshold from high to low (equivalently: move down the sorted list).\n",
    "3. Track how many positives/negatives we’ve included so far.\n",
    "4. Record $(\\mathrm{FPR},\\mathrm{TPR})$ whenever the score changes.\n",
    "\n",
    "This produces the same step-shaped ROC curve you get from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cc6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_numpy(y_true, y_score):\n",
    "    '''Compute ROC curve (FPR, TPR, thresholds) for binary classification.\n",
    "\n",
    "    Matches the standard approach used by scikit-learn:\n",
    "    - thresholds are unique score values in descending order, with an initial +inf\n",
    "    - the returned curve starts at (0,0)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n,)\n",
    "        Binary labels in {0,1}\n",
    "    y_score : array-like of shape (n,)\n",
    "        Scores where larger means more positive\n",
    "    '''\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "    if y_true.shape != y_score.shape:\n",
    "        raise ValueError(\"y_true and y_score must have the same shape\")\n",
    "\n",
    "    unique = np.unique(y_true)\n",
    "    if not set(unique.tolist()).issubset({0, 1}):\n",
    "        raise ValueError(\"y_true must be binary with values in {0,1}\")\n",
    "\n",
    "    y_true = y_true.astype(int)\n",
    "    n_pos = int(np.sum(y_true == 1))\n",
    "    n_neg = int(np.sum(y_true == 0))\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        raise ValueError(\"ROC is undefined when only one class is present\")\n",
    "\n",
    "    # Sort by score descending (stable sort so ties are handled consistently)\n",
    "    order = np.argsort(-y_score, kind=\"mergesort\")\n",
    "    y_true_sorted = y_true[order]\n",
    "    y_score_sorted = y_score[order]\n",
    "\n",
    "    # Indices where score changes (each threshold is a distinct score value)\n",
    "    distinct = np.where(y_score_sorted[1:] != y_score_sorted[:-1])[0]\n",
    "    threshold_idxs = np.r_[distinct, y_true_sorted.size - 1]\n",
    "\n",
    "    tps = np.cumsum(y_true_sorted)[threshold_idxs]\n",
    "    fps = (threshold_idxs + 1) - tps\n",
    "\n",
    "    tpr = tps / n_pos\n",
    "    fpr = fps / n_neg\n",
    "    thresholds = y_score_sorted[threshold_idxs]\n",
    "\n",
    "    # Prepend the (0,0) point with threshold = +inf\n",
    "    tpr = np.r_[0.0, tpr]\n",
    "    fpr = np.r_[0.0, fpr]\n",
    "    thresholds = np.r_[np.inf, thresholds]\n",
    "\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "\n",
    "def roc_auc_score_numpy(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve_numpy(y_true, y_score)\n",
    "    return auc_trapezoid(fpr, tpr)\n",
    "\n",
    "\n",
    "fpr_np, tpr_np, thr_np = roc_curve_numpy(y_true, y_score)\n",
    "auc_np = roc_auc_score_numpy(y_true, y_score)\n",
    "\n",
    "print(f\"sklearn roc_auc_score = {roc_auc_score(y_true, y_score):.10f}\")\n",
    "print(f\"NumPy   roc_auc_score = {auc_np:.10f}\")\n",
    "\n",
    "# Sanity check: same AUC\n",
    "assert np.isclose(auc_np, roc_auc_score(y_true, y_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abe577",
   "metadata": {},
   "source": [
    "## 5) AUC as a rank statistic (Mann–Whitney / Wilcoxon)\n",
    "\n",
    "The “pairwise probability” view can be written explicitly as:\n",
    "\n",
    "$$\n",
    "\\mathrm{AUC}\n",
    "= \\frac{1}{PN}\\sum_{i\\in\\mathcal{P}}\\sum_{j\\in\\mathcal{N}}\n",
    "\\Big(\\mathbb{1}[s_i > s_j] + \\tfrac{1}{2}\\,\\mathbb{1}[s_i = s_j]\\Big)\n",
    "$$\n",
    "\n",
    "There’s an equivalent computation using **ranks**.\n",
    "Let $r_i$ be the rank of $s_i$ among all scores (rank 1 = smallest score), using **average ranks for ties**.\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathrm{AUC} = \\frac{\\sum_{i\\in\\mathcal{P}} r_i - \\frac{P(P+1)}{2}}{PN}\n",
    "$$\n",
    "\n",
    "This is useful because it can be computed in $O(n\\log n)$ via sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e085781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankdata_average_ties(x):\n",
    "    '''Rank data with average ranks for ties (1 = smallest).'''\n",
    "    x = np.asarray(x)\n",
    "    order = np.argsort(x, kind=\"mergesort\")\n",
    "    x_sorted = x[order]\n",
    "\n",
    "    ranks_sorted = np.empty(x_sorted.shape[0], dtype=float)\n",
    "    i = 0\n",
    "    n = x_sorted.shape[0]\n",
    "    while i < n:\n",
    "        j = i\n",
    "        while j + 1 < n and x_sorted[j + 1] == x_sorted[i]:\n",
    "            j += 1\n",
    "\n",
    "        # ranks are 1..n, positions i..j correspond to ranks (i+1)..(j+1)\n",
    "        avg_rank = 0.5 * ((i + 1) + (j + 1))\n",
    "        ranks_sorted[i : j + 1] = avg_rank\n",
    "        i = j + 1\n",
    "\n",
    "    ranks = np.empty_like(ranks_sorted)\n",
    "    ranks[order] = ranks_sorted\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def roc_auc_score_rank_numpy(y_true, y_score):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score)\n",
    "    n_pos = int(np.sum(y_true == 1))\n",
    "    n_neg = int(np.sum(y_true == 0))\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        raise ValueError(\"ROC AUC is undefined when only one class is present\")\n",
    "\n",
    "    ranks = rankdata_average_ties(y_score)\n",
    "    sum_pos_ranks = float(np.sum(ranks[y_true == 1]))\n",
    "    return (sum_pos_ranks - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n",
    "\n",
    "\n",
    "auc_rank = roc_auc_score_rank_numpy(y_true, y_score)\n",
    "print(f\"ROC AUC (rank formula) = {auc_rank:.10f}\")\n",
    "print(f\"ROC AUC (ROC+trapz)    = {roc_auc_score_numpy(y_true, y_score):.10f}\")\n",
    "assert np.isclose(auc_rank, roc_auc_score_numpy(y_true, y_score))\n",
    "\n",
    "# Direct pairwise computation: O(P*N), only feasible for small n\n",
    "pos_idx = np.flatnonzero(y_true == 1)\n",
    "neg_idx = np.flatnonzero(y_true == 0)\n",
    "\n",
    "sub_idx = np.r_[\n",
    "    rng.choice(pos_idx, size=60, replace=False),\n",
    "    rng.choice(neg_idx, size=60, replace=False),\n",
    "]\n",
    "sub_idx = rng.permutation(sub_idx)\n",
    "\n",
    "y_sub = y_true[sub_idx]\n",
    "s_sub = y_score[sub_idx]\n",
    "\n",
    "pos = s_sub[y_sub == 1]\n",
    "neg = s_sub[y_sub == 0]\n",
    "\n",
    "diffs = pos[:, None] - neg[None, :]\n",
    "auc_pairwise = (np.sum(diffs > 0) + 0.5 * np.sum(diffs == 0)) / (pos.size * neg.size)\n",
    "\n",
    "print(f\"ROC AUC (pairwise, small) = {auc_pairwise:.10f}\")\n",
    "assert np.isclose(auc_pairwise, roc_auc_score_rank_numpy(y_sub, s_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424e885",
   "metadata": {},
   "source": [
    "### Monotonic transforms don’t change AUC\n",
    "\n",
    "Because AUC depends only on the ordering of scores, any strictly increasing transform keeps the ROC curve (and AUC) the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(z):\n",
    "    # stable log(1 + exp(z))\n",
    "    return np.logaddexp(0.0, z)\n",
    "\n",
    "\n",
    "s = y_score\n",
    "s_transformed = softplus(3.0 * s)  # strictly increasing\n",
    "\n",
    "print(f\"AUC(s)            = {roc_auc_score_numpy(y_true, s):.10f}\")\n",
    "print(f\"AUC(softplus(3s)) = {roc_auc_score_numpy(y_true, s_transformed):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb27c95",
   "metadata": {},
   "source": [
    "## 6) A threshold view: how TPR and FPR change as you move the cutoff\n",
    "\n",
    "ROC plots *TPR vs FPR*, but sometimes it’s useful to see both rates as explicit functions of the threshold $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the NumPy ROC implementation so we have (fpr,tpr,threshold) aligned\n",
    "fpr_np, tpr_np, thr_np = roc_curve_numpy(y_true, y_score)\n",
    "\n",
    "# Drop the first threshold (+inf) for plotting\n",
    "thr_plot = thr_np[1:]\n",
    "tpr_plot = tpr_np[1:]\n",
    "fpr_plot = fpr_np[1:]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thr_plot, y=tpr_plot, mode=\"lines+markers\", name=\"TPR\"))\n",
    "fig.add_trace(go.Scatter(x=thr_plot, y=fpr_plot, mode=\"lines+markers\", name=\"FPR\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"TPR and FPR as functions of the threshold\",\n",
    "    xaxis_title=\"threshold τ (higher = stricter)\",\n",
    "    yaxis_title=\"rate\",\n",
    "    width=800,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "# thresholds are in descending score order; flip the x-axis so we read left→right as τ decreases\n",
    "fig.update_xaxes(autorange=\"reversed\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56a7be",
   "metadata": {},
   "source": [
    "## 7) Practical usage: ROC AUC for a classifier\n",
    "\n",
    "In practice, you usually:\n",
    "\n",
    "1. Train a model with a differentiable loss (log-loss, hinge, etc.).\n",
    "2. Evaluate **ROC AUC** on validation/test using *scores* (`predict_proba` or `decision_function`).\n",
    "\n",
    "**Important:** AUC expects *scores*, not hard class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=2500,\n",
    "    n_features=10,\n",
    "    n_informative=4,\n",
    "    n_redundant=0,\n",
    "    weights=[0.85, 0.15],\n",
    "    class_sep=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=2000),\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Probability scores for the positive class\n",
    "y_score_test = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_test = roc_auc_score(y_test, y_score_test)\n",
    "print(f\"Test ROC AUC = {auc_test:.3f}\")\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_score_test)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr_test, y=tpr_test, mode=\"lines\", name=f\"Test ROC (AUC={auc_test:.3f})\"))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", line=dict(dash=\"dash\", color=\"gray\"), name=\"Random\"))\n",
    "fig.update_layout(\n",
    "    title=\"ROC curve on a held-out test set\",\n",
    "    xaxis_title=\"False Positive Rate (FPR)\",\n",
    "    yaxis_title=\"True Positive Rate (TPR)\",\n",
    "    width=750,\n",
    "    height=520,\n",
    ")\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n",
    "\n",
    "# Same AUC via our NumPy implementation\n",
    "print(f\"NumPy ROC AUC = {roc_auc_score_numpy(y_test, y_score_test):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b22a4d",
   "metadata": {},
   "source": [
    "## 8) Using AUC for optimization (via a smooth surrogate)\n",
    "\n",
    "The “true” AUC objective is based on an indicator:\n",
    "\n",
    "$$\n",
    "\\max_w\\; \\frac{1}{PN}\\sum_{i\\in\\mathcal{P}}\\sum_{j\\in\\mathcal{N}} \\mathbb{1}[s_w(x_i) > s_w(x_j)]\n",
    "$$\n",
    "\n",
    "This is hard to optimize directly because the indicator is non-differentiable and the double sum over all pairs is $O(PN)$.\n",
    "\n",
    "A common trick: replace the indicator with a **pairwise surrogate loss**.\n",
    "For a linear scoring model $s_w(x)=w^\\top x$, define $d_{ij}=w^\\top(x_i-x_j)$ and use the pairwise logistic loss:\n",
    "\n",
    "$$\n",
    "\\ell(d) = \\log(1+e^{-d})\n",
    "$$\n",
    "\n",
    "Then minimize:\n",
    "\n",
    "$$\n",
    "L(w)=\\frac{1}{PN}\\sum_{i\\in\\mathcal{P}}\\sum_{j\\in\\mathcal{N}} \\ell\\big(w^\\top(x_i-x_j)\\big)\n",
    "$$\n",
    "\n",
    "The gradient of one pair is:\n",
    "\n",
    "$$\n",
    "\\nabla_w\\ell\\big(w^\\top(x_i-x_j)\\big)\n",
    "= -\\sigma\\big(-w^\\top(x_i-x_j)\\big)\\,(x_i-x_j)\n",
    "$$\n",
    "\n",
    "where $\\sigma(z)=\\frac{1}{1+e^{-z}}$ is the sigmoid.\n",
    "\n",
    "In code, we approximate the full double sum with **mini-batches of random positive-negative pairs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    out = np.empty_like(z)\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    exp_z = np.exp(z[~pos])\n",
    "    out[~pos] = exp_z / (1.0 + exp_z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def standardize_fit(X):\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    std = np.where(std == 0, 1.0, std)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def standardize_transform(X, mean, std):\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def add_intercept(X):\n",
    "    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "\n",
    "\n",
    "# Prepare a smaller dataset for fast training curves\n",
    "X_small, y_small = make_classification(\n",
    "    n_samples=1400,\n",
    "    n_features=8,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    weights=[0.9, 0.1],\n",
    "    class_sep=0.9,\n",
    "    flip_y=0.03,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X_small, y_small, test_size=0.35, stratify=y_small, random_state=7\n",
    ")\n",
    "\n",
    "mean, std = standardize_fit(X_tr)\n",
    "X_tr = standardize_transform(X_tr, mean, std)\n",
    "X_va = standardize_transform(X_va, mean, std)\n",
    "\n",
    "X_tr = add_intercept(X_tr)\n",
    "X_va = add_intercept(X_va)\n",
    "\n",
    "\n",
    "def train_logistic_ce(X, y, X_val, y_val, lr=0.1, reg=1e-3, epochs=30, batch_size=256, seed=0):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    auc_hist = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        idx = rng_local.permutation(n)\n",
    "        for start in range(0, n, batch_size):\n",
    "            batch = idx[start : start + batch_size]\n",
    "            xb = X[batch]\n",
    "            yb = y[batch]\n",
    "\n",
    "            logits = xb @ w\n",
    "            p = sigmoid(logits)\n",
    "\n",
    "            grad = (xb.T @ (p - yb)) / xb.shape[0] + reg * w\n",
    "            w -= lr * grad\n",
    "\n",
    "        auc_hist.append(roc_auc_score_numpy(y_val, X_val @ w))\n",
    "\n",
    "    return w, np.array(auc_hist)\n",
    "\n",
    "\n",
    "def train_auc_pairwise(\n",
    "    X,\n",
    "    y,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    lr=0.1,\n",
    "    reg=1e-3,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=200,\n",
    "    batch_pairs=512,\n",
    "    seed=0,\n",
    "):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    pos_idx = np.flatnonzero(y == 1)\n",
    "    neg_idx = np.flatnonzero(y == 0)\n",
    "    if pos_idx.size == 0 or neg_idx.size == 0:\n",
    "        raise ValueError(\"Need both classes for AUC training\")\n",
    "\n",
    "    auc_hist = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for _ in range(steps_per_epoch):\n",
    "            pi = rng_local.choice(pos_idx, size=batch_pairs, replace=True)\n",
    "            ni = rng_local.choice(neg_idx, size=batch_pairs, replace=True)\n",
    "\n",
    "            diff = X[pi] - X[ni]\n",
    "            d_scores = diff @ w\n",
    "\n",
    "            # loss per pair: log(1 + exp(-d)) = softplus(-d)\n",
    "            # grad: -sigmoid(-d) * diff\n",
    "            weight = sigmoid(-d_scores)\n",
    "            grad = -(weight[:, None] * diff).mean(axis=0) + reg * w\n",
    "            w -= lr * grad\n",
    "\n",
    "        auc_hist.append(roc_auc_score_numpy(y_val, X_val @ w))\n",
    "\n",
    "    return w, np.array(auc_hist)\n",
    "\n",
    "\n",
    "w_ce, auc_ce = train_logistic_ce(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_va,\n",
    "    y_va,\n",
    "    lr=0.2,\n",
    "    reg=1e-3,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    seed=1,\n",
    ")\n",
    "w_auc, auc_auc = train_auc_pairwise(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_va,\n",
    "    y_va,\n",
    "    lr=0.2,\n",
    "    reg=1e-3,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=150,\n",
    "    batch_pairs=512,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "print(f\"Final val AUC (cross-entropy training) = {auc_ce[-1]:.3f}\")\n",
    "print(f\"Final val AUC (pairwise AUC training)  = {auc_auc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=auc_ce, mode=\"lines+markers\", name=\"Train w/ log-loss (CE)\"))\n",
    "fig.add_trace(go.Scatter(y=auc_auc, mode=\"lines+markers\", name=\"Train w/ pairwise AUC surrogate\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Validation ROC AUC during training\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=\"ROC AUC\",\n",
    "    width=850,\n",
    "    height=450,\n",
    "    yaxis=dict(range=[0.45, 1.0]),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c031ab4",
   "metadata": {},
   "source": [
    "## 9) Pros, cons, and when to use ROC AUC\n",
    "\n",
    "**Pros**\n",
    "- **Threshold-free**: summarizes performance across all possible thresholds.\n",
    "- **Ranking interpretation**: $\\mathrm{AUC}\\approx \\mathbb{P}(s(X^+) > s(X^-))$ is intuitive.\n",
    "- **Insensitive to score calibration**: if your ranking is good but probabilities aren’t calibrated, AUC can still be high.\n",
    "- **Works well for model comparison** when you don’t yet know the operating threshold.\n",
    "\n",
    "**Cons / pitfalls**\n",
    "- **Not about calibration**: a model can have great AUC and terrible probability estimates.\n",
    "- **May be misleading under heavy class imbalance** if you care about the *precision* regime (consider **PR AUC**).\n",
    "- **Averages over regions you may not care about** (e.g., you might only tolerate FPR < 1%).\n",
    "- **Hard to optimize directly**: the exact AUC objective is non-smooth and pairwise.\n",
    "- **Depends on label meaning and score direction**: flipping the positive class changes AUC to $1-\\mathrm{AUC}$.\n",
    "\n",
    "**Good use cases**\n",
    "- When you care about **ranking** (who is more likely positive) more than a single fixed decision threshold.\n",
    "- When you’ll choose the operating threshold later (business constraints, costs, etc.).\n",
    "- As a model-selection metric for classifiers that output usable scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b7cb0",
   "metadata": {},
   "source": [
    "## 10) Exercises\n",
    "\n",
    "1. Create a highly imbalanced dataset (e.g. 1% positives) and compare ROC AUC vs PR AUC.\n",
    "2. Show two models with the same ROC AUC but very different TPR when FPR < 0.01.\n",
    "3. Extend the NumPy ROC code to support **sample weights**.\n",
    "4. For the pairwise AUC training, try a hinge loss $\\ell(d)=\\max(0, 1-d)$ and compare curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d462c7",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn: `roc_curve`, `roc_auc_score`, `auc`\n",
    "  - https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
    "- Tom Fawcett (2006): *An introduction to ROC analysis*\n",
    "- Wilcoxon / Mann–Whitney rank-sum statistic interpretation of AUC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}