{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c173ed01",
   "metadata": {},
   "source": [
    "# D² Pinball Score (`d2_pinball_score`)\n",
    "\n",
    "`d2_pinball_score` is a regression **score** that measures the fraction of **pinball loss** (quantile loss) explained by a model relative to a simple baseline that always predicts the empirical \\(\\alpha\\)-quantile of `y_true`.\n",
    "\n",
    "- Best possible score: **1.0**\n",
    "- Baseline score (constant \\(\\alpha\\)-quantile predictor): **0.0**\n",
    "- Can be negative (worse than the baseline)\n",
    "\n",
    "It’s the quantile-regression analogue of \\(R^2\\): “how much better than the best constant quantile prediction?”.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Define pinball loss and the D² pinball score (with math)\n",
    "- Build intuition for the asymmetric penalty and the quantile baseline\n",
    "- Implement `d2_pinball_score` from scratch in NumPy (weights + multioutput)\n",
    "- Fit a simple linear **quantile regression** model with subgradient descent\n",
    "- Know pros/cons and when to use D² pinball\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import d2_pinball_score\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- quantiles / percentiles\n",
    "- basic regression notation\n",
    "- piecewise functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, QuantileRegressor\n",
    "from sklearn.metrics import d2_pinball_score, mean_pinball_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a3f63",
   "metadata": {},
   "source": [
    "## 1) Pinball loss (quantile loss)\n",
    "\n",
    "For a quantile level \\(\\alpha \\in [0, 1]\\), define the **pinball loss** for one sample as:\n",
    "\n",
    "Let \\(u = y - \\hat{y}\\) (true minus prediction). Then\n",
    "\n",
    "$$\n",
    "\\rho_\\alpha(u) = u\\,\\big(\\alpha - \\mathbb{1}[u < 0]\\big)\n",
    "$$\n",
    "\n",
    "Equivalently (piecewise):\n",
    "\n",
    "$$\n",
    "\\rho_\\alpha(y, \\hat{y}) =\n",
    "\\begin{cases}\n",
    "\\alpha (y - \\hat{y}) & y \\ge \\hat{y} \\\\\n",
    "(1-\\alpha)(\\hat{y} - y) & y < \\hat{y}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The **mean pinball loss** over \\(n\\) samples is:\n",
    "\n",
    "$$\n",
    "L_\\alpha(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n} \\rho_\\alpha(y_i, \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- \\(\\alpha\\) controls asymmetry: for \\(\\alpha=0.9\\), **under-prediction** is penalized 9x as much as over-prediction.\n",
    "- \\(\\alpha=0.5\\) gives \\(\\rho_{0.5}(y,\\hat{y}) = \\tfrac12|y-\\hat{y}|\\), so `d2_pinball_score(alpha=0.5)` matches the MAE-based D² score (`d2_absolute_error_score`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaec922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinball_loss_residual(u, alpha):\n",
    "    # Pinball loss as a function of residual u = y - y_pred\n",
    "    u = np.asarray(u)\n",
    "    return np.where(u >= 0, alpha * u, (alpha - 1) * u)\n",
    "\n",
    "u = np.linspace(-4, 4, 400)\n",
    "alphas = [0.1, 0.5, 0.9]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in alphas:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=u,\n",
    "            y=pinball_loss_residual(u, a),\n",
    "            mode=\"lines\",\n",
    "            name=f\"alpha={a}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_vline(x=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(\n",
    "    title='Pinball loss is a \"tilted\" absolute value',\n",
    "    xaxis_title=\"residual u = y - y_pred\",\n",
    "    yaxis_title=\"rho_alpha(u)\",\n",
    "    height=380,\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffd1a1",
   "metadata": {},
   "source": [
    "## 2) The baseline and the D² pinball score\n",
    "\n",
    "### 2.1 Best constant predictor = empirical \\(\\alpha\\)-quantile\n",
    "\n",
    "Suppose you restrict yourself to a **constant** prediction \\(c\\) for every sample.\n",
    "You minimize:\n",
    "\n",
    "$$\n",
    "J(c) = \\sum_{i=1}^n \\rho_\\alpha(y_i, c)\n",
    "$$\n",
    "\n",
    "A subgradient w.r.t. \\(c\\) is:\n",
    "\n",
    "$$\n",
    "\\partial_c J(c) = \\sum_{i=1}^n (\\mathbb{1}[y_i < c] - \\alpha)\n",
    "$$\n",
    "\n",
    "So at an optimum, roughly an \\(\\alpha\\) fraction of samples lie **below** \\(c\\).\n",
    "That is exactly the definition of an empirical \\(\\alpha\\)-quantile.\n",
    "\n",
    "With sample weights \\(w_i \\ge 0\\), the condition becomes:\n",
    "\n",
    "$$\n",
    "\\partial_c J(c) = \\sum_{i=1}^n w_i(\\mathbb{1}[y_i < c] - \\alpha) = 0\n",
    "\\quad\\Rightarrow\\quad\n",
    "c = \\text{weighted }\\alpha\\text{-quantile of }y\n",
    "$$\n",
    "\n",
    "### 2.2 D² pinball score\n",
    "\n",
    "Let \\(L_\\alpha(y, \\hat{y})\\) be the mean pinball loss for your model predictions \\(\\hat{y}\\).\n",
    "Let \\(\\tilde{y}\\) be the **baseline** predictions that always equal the empirical \\(\\alpha\\)-quantile of \\(y\\).\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "D^2_\\alpha(y, \\hat{y})\n",
    "= 1 - \\frac{L_\\alpha(y, \\hat{y})}{L_\\alpha(y, \\tilde{y})}\n",
    "$$\n",
    "\n",
    "- \\(D^2=1\\) for perfect predictions (zero pinball loss)\n",
    "- \\(D^2=0\\) for the constant quantile baseline\n",
    "- \\(D^2<0\\) if your model is worse than the baseline\n",
    "\n",
    "Edge cases (matching scikit-learn):\n",
    "\n",
    "- with fewer than 2 samples, the score is undefined (`nan`)\n",
    "- if the baseline loss is 0 (e.g. constant targets), `sklearn` returns:\n",
    "  - 1.0 for perfect predictions\n",
    "  - 0.0 otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The empirical alpha-quantile minimizes pinball loss among constant predictors\n",
    "n = 250\n",
    "y = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "y[:15] += rng.exponential(scale=4.0, size=15)  # right-tail outliers\n",
    "\n",
    "alpha = 0.9\n",
    "q = float(np.percentile(y, alpha * 100))\n",
    "\n",
    "c_grid = np.linspace(y.min() - 1, y.max() + 1, 300)\n",
    "loss_grid = np.array([mean_pinball_loss(y, np.full_like(y, c), alpha=alpha) for c in c_grid])\n",
    "\n",
    "c_star = float(c_grid[np.argmin(loss_grid)])\n",
    "loss_star = float(loss_grid.min())\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Mean pinball loss vs constant prediction\",\n",
    "        \"y distribution\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=c_grid, y=loss_grid, mode=\"lines\"), row=1, col=1)\n",
    "fig.add_vline(x=q, line_dash=\"dash\", line_color=\"#4C78A8\", row=1, col=1)\n",
    "fig.add_vline(x=c_star, line_dash=\"dot\", line_color=\"#E45756\", row=1, col=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[c_star], y=[loss_star], mode=\"markers\", marker=dict(size=9, color=\"#E45756\"), showlegend=False),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.update_xaxes(title_text=\"constant prediction c\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"mean pinball loss\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=y, nbinsx=40, marker_color=\"rgba(0,0,0,0.65)\"), row=1, col=2)\n",
    "fig.add_vline(x=q, line_dash=\"dash\", line_color=\"#4C78A8\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"y\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=360,\n",
    "    title=f\"Baseline for D²: empirical {alpha:.1f}-quantile (q={q:.3f})\",\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "q, c_star\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f62b6a",
   "metadata": {},
   "source": [
    "## 3) Interpreting D²\n",
    "\n",
    "Think of \\(L_\\alpha(y,\\tilde{y})\\) as the loss of a “no-features” model: it only knows the labels and predicts a constant quantile.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "D^2 = 1 - \\frac{L_\\alpha(\\text{model})}{L_\\alpha(\\text{baseline})}\n",
    "$$\n",
    "\n",
    "So if \\(D^2=0.3\\), your model reduces pinball loss by **30%** compared to the constant-quantile baseline on that dataset.\n",
    "\n",
    "Like \\(R^2\\), D² is mainly for comparing models on the **same target/dataset**; it’s not a scale-free number you can compare across unrelated problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bfbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny examples\n",
    "\n",
    "y_true = np.array([1, 2, 3])\n",
    "y_pred = np.array([1, 3, 3])\n",
    "\n",
    "for a in [0.1, 0.5, 0.9]:\n",
    "    print(f\"alpha={a}: D2={d2_pinball_score(y_true, y_pred, alpha=a):.4f}\")\n",
    "\n",
    "alpha = 0.9\n",
    "q = np.percentile(y_true, alpha * 100)\n",
    "y_baseline = np.full_like(y_true, q, dtype=float)\n",
    "\n",
    "print()\n",
    "print(\"Baseline D2 (should be 0):\", d2_pinball_score(y_true, y_baseline, alpha=alpha))\n",
    "print(\"Perfect D2 (should be 1):\", d2_pinball_score(y_true, y_true, alpha=alpha))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32529f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D² as predictions degrade from perfect -> noisy\n",
    "n = 400\n",
    "y_true = rng.standard_t(df=3, size=n)  # heavy tails\n",
    "\n",
    "# fixed direction of noise so the curve is easier to read\n",
    "noise = rng.normal(size=n)\n",
    "\n",
    "sigmas = np.linspace(0, 2.5, 60)\n",
    "alphas = [0.1, 0.5, 0.9]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in alphas:\n",
    "    d2_vals = [d2_pinball_score(y_true, y_true + s * noise, alpha=a) for s in sigmas]\n",
    "    fig.add_trace(go.Scatter(x=sigmas, y=d2_vals, mode=\"lines\", name=f\"alpha={a}\"))\n",
    "\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.add_hline(y=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(\n",
    "    title=\"As predictions get noisier, D² pinball decreases (can go negative)\",\n",
    "    xaxis_title=\"noise scale added to perfect predictions\",\n",
    "    yaxis_title=\"D² pinball score\",\n",
    "    height=380,\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6b9b7",
   "metadata": {},
   "source": [
    "## 4) From-scratch NumPy implementation\n",
    "\n",
    "Below is a small NumPy implementation that mirrors scikit-learn’s behavior:\n",
    "\n",
    "- supports 1D targets \\(n\\) and multioutput targets \\((n, m)\\)\n",
    "- supports `sample_weight`\n",
    "- supports `multioutput` aggregation:\n",
    "  - `'raw_values'` (per-output scores)\n",
    "  - `'uniform_average'` (simple mean)\n",
    "  - array-like weights of shape \\((m,)\\)\n",
    "- uses the same baseline as scikit-learn:\n",
    "  - unweighted: `np.percentile(y_true, q=alpha*100, axis=0)`\n",
    "  - weighted: a **lower** weighted percentile (first value where the weight CDF reaches \\(\\alpha\\))\n",
    "\n",
    "Edge cases:\n",
    "\n",
    "- with fewer than 2 samples: returns `nan`\n",
    "- if the baseline loss is zero (e.g. constant targets):\n",
    "  - perfect predictions → 1.0\n",
    "  - imperfect predictions → 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a81d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_2d(y):\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 1:\n",
    "        return y.reshape(-1, 1)\n",
    "    if y.ndim != 2:\n",
    "        raise ValueError(f\"y must be 1D or 2D, got shape {y.shape}\")\n",
    "    return y\n",
    "\n",
    "\n",
    "def _weighted_percentile_lower(array, sample_weight, percentile):\n",
    "    # Lower weighted percentile (mirrors sklearn.utils.stats._weighted_percentile)\n",
    "    array = np.asarray(array)\n",
    "    sample_weight = np.asarray(sample_weight)\n",
    "\n",
    "    n_dim = array.ndim\n",
    "    if n_dim == 0:\n",
    "        return array[()]\n",
    "\n",
    "    if array.ndim == 1:\n",
    "        array = array.reshape((-1, 1))\n",
    "\n",
    "    if sample_weight.ndim == 1:\n",
    "        if sample_weight.shape[0] != array.shape[0]:\n",
    "            raise ValueError(\"sample_weight must have shape (n_samples,) when 1D\")\n",
    "        sample_weight = np.tile(sample_weight.reshape(-1, 1), (1, array.shape[1]))\n",
    "    elif sample_weight.shape != array.shape:\n",
    "        raise ValueError(\"sample_weight must have same shape as array, or shape (n_samples,)\")\n",
    "\n",
    "    if np.any(sample_weight < 0):\n",
    "        raise ValueError(\"sample_weight must be non-negative\")\n",
    "\n",
    "    sorted_idx = np.argsort(array, axis=0)\n",
    "    sorted_array = np.take_along_axis(array, sorted_idx, axis=0)\n",
    "    sorted_weights = np.take_along_axis(sample_weight, sorted_idx, axis=0)\n",
    "\n",
    "    weight_cdf = np.cumsum(sorted_weights, axis=0)\n",
    "    total_weight = weight_cdf[-1]\n",
    "    adjusted = percentile / 100.0 * total_weight\n",
    "\n",
    "    # For percentile=0, ignore leading observations with sample_weight=0 (sklearn behavior)\n",
    "    mask = adjusted == 0\n",
    "    if np.any(mask):\n",
    "        adjusted = adjusted.astype(float, copy=False)\n",
    "        adjusted[mask] = np.nextafter(adjusted[mask], adjusted[mask] + 1)\n",
    "\n",
    "    idx = np.array(\n",
    "        [np.searchsorted(weight_cdf[:, i], adjusted[i]) for i in range(weight_cdf.shape[1])],\n",
    "        dtype=int,\n",
    "    )\n",
    "    idx = np.clip(idx, 0, array.shape[0] - 1)\n",
    "\n",
    "    out = sorted_array[idx, np.arange(array.shape[1])]\n",
    "    return out[0] if n_dim == 1 else out\n",
    "\n",
    "\n",
    "def mean_pinball_loss_numpy(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    alpha=0.5,\n",
    "    sample_weight=None,\n",
    "    multioutput=\"uniform_average\",\n",
    "):\n",
    "    # NumPy implementation of mean pinball loss\n",
    "\n",
    "    if not (0.0 <= float(alpha) <= 1.0):\n",
    "        raise ValueError(\"alpha must be in [0, 1]\")\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"y_true and y_pred must have the same shape, got {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "    Y_true = _as_2d(y_true)\n",
    "    Y_pred = _as_2d(y_pred)\n",
    "    n_samples, n_outputs = Y_true.shape\n",
    "\n",
    "    u = Y_true - Y_pred  # residual = y - y_pred\n",
    "    loss = np.where(u >= 0, alpha * u, (alpha - 1) * u)  # always >= 0\n",
    "\n",
    "    if sample_weight is None:\n",
    "        out = loss.mean(axis=0)\n",
    "    else:\n",
    "        w = np.asarray(sample_weight).reshape(-1)\n",
    "        if w.shape[0] != n_samples:\n",
    "            raise ValueError(f\"sample_weight must have shape (n_samples,), got {w.shape}\")\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "        w_sum = w.sum()\n",
    "        if w_sum == 0:\n",
    "            raise ValueError(\"sum(sample_weight) must be positive\")\n",
    "        out = (loss * w.reshape(-1, 1)).sum(axis=0) / w_sum\n",
    "\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == \"raw_values\":\n",
    "            return out\n",
    "        if multioutput == \"uniform_average\":\n",
    "            return float(np.mean(out))\n",
    "        raise ValueError(\"multioutput must be 'raw_values' or 'uniform_average' or an array-like\")\n",
    "    else:\n",
    "        weights = np.asarray(multioutput).reshape(-1)\n",
    "        if weights.shape[0] != n_outputs:\n",
    "            raise ValueError(f\"multioutput weights must have shape (n_outputs,), got {weights.shape}\")\n",
    "        return float(np.average(out, weights=weights))\n",
    "\n",
    "\n",
    "def d2_pinball_score_numpy(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    alpha=0.5,\n",
    "    multioutput=\"uniform_average\",\n",
    "):\n",
    "    # NumPy implementation of scikit-learn's d2_pinball_score\n",
    "\n",
    "    if not (0.0 <= float(alpha) <= 1.0):\n",
    "        raise ValueError(\"alpha must be in [0, 1]\")\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"y_true and y_pred must have the same shape, got {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "    Y_true = _as_2d(y_true)\n",
    "    Y_pred = _as_2d(y_pred)\n",
    "    n_samples, n_outputs = Y_true.shape\n",
    "\n",
    "    if n_samples < 2:\n",
    "        warnings.warn(\"D^2 score is not well-defined with less than two samples.\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if sample_weight is None:\n",
    "        w = None\n",
    "    else:\n",
    "        w = np.asarray(sample_weight).reshape(-1)\n",
    "        if w.shape[0] != n_samples:\n",
    "            raise ValueError(f\"sample_weight must have shape (n_samples,), got {w.shape}\")\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "        if w.sum() == 0:\n",
    "            raise ValueError(\"sum(sample_weight) must be positive\")\n",
    "\n",
    "    numerator = mean_pinball_loss_numpy(\n",
    "        Y_true,\n",
    "        Y_pred,\n",
    "        sample_weight=w,\n",
    "        alpha=alpha,\n",
    "        multioutput=\"raw_values\",\n",
    "    )\n",
    "\n",
    "    if w is None:\n",
    "        yq = np.percentile(Y_true, q=alpha * 100, axis=0)\n",
    "    else:\n",
    "        yq = _weighted_percentile_lower(Y_true, sample_weight=w, percentile=alpha * 100)\n",
    "\n",
    "    y_quantile = np.tile(yq, (n_samples, 1))\n",
    "\n",
    "    denominator = mean_pinball_loss_numpy(\n",
    "        Y_true,\n",
    "        y_quantile,\n",
    "        sample_weight=w,\n",
    "        alpha=alpha,\n",
    "        multioutput=\"raw_values\",\n",
    "    )\n",
    "\n",
    "    nonzero_numerator = numerator != 0\n",
    "    nonzero_denominator = denominator != 0\n",
    "    valid_score = nonzero_numerator & nonzero_denominator\n",
    "\n",
    "    output_scores = np.ones(n_outputs, dtype=float)\n",
    "    output_scores[valid_score] = 1 - numerator[valid_score] / denominator[valid_score]\n",
    "    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.0\n",
    "\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == \"raw_values\":\n",
    "            return output_scores\n",
    "        if multioutput == \"uniform_average\":\n",
    "            return float(np.mean(output_scores))\n",
    "        raise ValueError(\"multioutput must be 'raw_values' or 'uniform_average' or an array-like\")\n",
    "    else:\n",
    "        weights = np.asarray(multioutput).reshape(-1)\n",
    "        if weights.shape[0] != n_outputs:\n",
    "            raise ValueError(f\"multioutput weights must have shape (n_outputs,), got {weights.shape}\")\n",
    "        return float(np.average(output_scores, weights=weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks vs scikit-learn\n",
    "\n",
    "alpha = 0.9\n",
    "\n",
    "# 1D\n",
    "\n",
    "y_true = rng.normal(size=80)\n",
    "y_pred = y_true + rng.normal(0, 0.8, size=80)\n",
    "\n",
    "print(\"1D\")\n",
    "print(\"numpy :\", d2_pinball_score_numpy(y_true, y_pred, alpha=alpha))\n",
    "print(\"sklearn:\", d2_pinball_score(y_true, y_pred, alpha=alpha))\n",
    "\n",
    "# Multioutput + weights\n",
    "Y_true = rng.normal(size=(120, 3))\n",
    "Y_pred = Y_true + rng.normal(0, 0.5, size=(120, 3))\n",
    "w = rng.uniform(0.2, 2.0, size=120)\n",
    "\n",
    "print()\n",
    "print(\"Multioutput (raw)\")\n",
    "print(\"numpy :\", d2_pinball_score_numpy(Y_true, Y_pred, sample_weight=w, alpha=alpha, multioutput=\"raw_values\"))\n",
    "print(\"sklearn:\", d2_pinball_score(Y_true, Y_pred, sample_weight=w, alpha=alpha, multioutput=\"raw_values\"))\n",
    "\n",
    "print()\n",
    "print(\"Multioutput (uniform_average)\")\n",
    "print(\"numpy :\", d2_pinball_score_numpy(Y_true, Y_pred, sample_weight=w, alpha=alpha, multioutput=\"uniform_average\"))\n",
    "print(\"sklearn:\", d2_pinball_score(Y_true, Y_pred, sample_weight=w, alpha=alpha, multioutput=\"uniform_average\"))\n",
    "\n",
    "# Constant-target edge case\n",
    "y_const = np.ones(10)\n",
    "\n",
    "print()\n",
    "print(\"Constant target:\")\n",
    "print(\n",
    "    \"perfect:\",\n",
    "    d2_pinball_score_numpy(y_const, y_const, alpha=alpha),\n",
    "    d2_pinball_score(y_const, y_const, alpha=alpha),\n",
    ")\n",
    "print(\n",
    "    \"imperfect:\",\n",
    "    d2_pinball_score_numpy(y_const, np.zeros_like(y_const), alpha=alpha),\n",
    "    d2_pinball_score(y_const, np.zeros_like(y_const), alpha=alpha),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca0320",
   "metadata": {},
   "source": [
    "## 5) Using D² pinball while optimizing a model (from scratch)\n",
    "\n",
    "On a fixed dataset \\(y\\), the baseline loss \\(L_\\alpha(y, \\tilde{y})\\) depends only on \\(y\\) (and \\(\\alpha\\)), not on the model parameters \\(\\theta\\).\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "D^2_\\alpha(\\theta) = 1 - \\frac{L_\\alpha(y, \\hat{y}_\\theta)}{L_\\alpha(y, \\tilde{y})}\n",
    "$$\n",
    "\n",
    "Maximizing \\(D^2_\\alpha\\) is equivalent to minimizing the pinball loss:\n",
    "\n",
    "$$\n",
    "\\arg\\max_\\theta D^2_\\alpha(\\theta)\n",
    "=\n",
    "\\arg\\min_\\theta L_\\alpha(y, \\hat{y}_\\theta)\n",
    "$$\n",
    "\n",
    "Their gradients differ only by a constant scale:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta D^2_\\alpha(\\theta)\n",
    "= -\\frac{1}{L_\\alpha(y, \\tilde{y})}\\nabla_\\theta L_\\alpha(y, \\hat{y}_\\theta)\n",
    "$$\n",
    "\n",
    "So in practice:\n",
    "\n",
    "- train by minimizing pinball loss (a proper training objective)\n",
    "- track D² pinball as a **score** on train/validation sets\n",
    "\n",
    "### Subgradient for a linear model\n",
    "\n",
    "Let \\(\\hat{y}_i = x_i^\\top w\\) and \\(u_i = y_i - \\hat{y}_i\\).\n",
    "For one sample, a valid subgradient of \\(\\rho_\\alpha(u_i)\\) w.r.t. \\(\\hat{y}_i\\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\rho_\\alpha}{\\partial \\hat{y}_i} =\n",
    "\\begin{cases}\n",
    "-\\alpha & u_i > 0 \\\\\n",
    "1-\\alpha & u_i < 0 \\\\\n",
    "0 & u_i = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This is enough to do (sub)gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf81e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data with skewed / heteroscedastic noise\n",
    "n = 350\n",
    "x = rng.uniform(-2, 3, size=n)\n",
    "\n",
    "noise = (0.6 + 0.3 * (x - x.min())) * rng.normal(size=n)\n",
    "noise += (rng.random(n) < 0.15) * rng.exponential(scale=3.0, size=n)  # positive outliers\n",
    "\n",
    "y = 1.0 + 2.0 * x + noise\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "alpha = 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0dc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_quantile_regression_sgd(X, y, *, alpha, lr=0.05, n_steps=600):\n",
    "    # Fit y ~= b + X w by minimizing mean pinball loss with subgradient descent\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y).reshape(-1)\n",
    "\n",
    "    n = X.shape[0]\n",
    "    X_design = np.column_stack([np.ones(n), X.reshape(n, -1)])  # intercept + features\n",
    "\n",
    "    w = np.zeros(X_design.shape[1])\n",
    "\n",
    "    # Baseline loss for D² tracking (constant alpha-quantile)\n",
    "    baseline_pred = np.full_like(y, np.percentile(y, alpha * 100), dtype=float)\n",
    "    baseline_loss = mean_pinball_loss_numpy(y, baseline_pred, alpha=alpha)\n",
    "\n",
    "    loss_hist = []\n",
    "    d2_hist = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        y_pred = X_design @ w\n",
    "        u = y - y_pred\n",
    "\n",
    "        # subgradient d rho / d y_pred\n",
    "        grad_pred = np.where(u > 0, -alpha, 1 - alpha)\n",
    "        grad_pred[u == 0] = 0.0\n",
    "\n",
    "        grad_w = (X_design.T @ grad_pred) / n\n",
    "        w = w - lr * grad_w\n",
    "\n",
    "        loss = mean_pinball_loss_numpy(y, y_pred, alpha=alpha)\n",
    "        d2 = 1 - loss / baseline_loss if baseline_loss != 0 else (1.0 if loss == 0 else 0.0)\n",
    "\n",
    "        loss_hist.append(loss)\n",
    "        d2_hist.append(d2)\n",
    "\n",
    "    return w, np.array(loss_hist), np.array(d2_hist)\n",
    "\n",
    "w_qr, loss_hist, d2_hist = fit_linear_quantile_regression_sgd(X_train, y_train, alpha=alpha, lr=0.05, n_steps=700)\n",
    "\n",
    "w_qr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization diagnostics\n",
    "iters = np.arange(len(loss_hist))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Train pinball loss vs iteration\", \"Train D² pinball vs iteration\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=iters, y=loss_hist, mode=\"lines\"), row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"iteration\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"L_alpha\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iters, y=d2_hist, mode=\"lines\", line=dict(color=\"#4C78A8\")),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"iteration\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"D²\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=360, title=f\"Subgradient descent for alpha={alpha}\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fca343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to OLS (mean regression) on the same alpha\n",
    "\n",
    "# Design matrices\n",
    "X_train_design = np.column_stack([np.ones(len(X_train)), X_train])\n",
    "X_test_design = np.column_stack([np.ones(len(X_test)), X_test])\n",
    "\n",
    "# Our quantile regressor\n",
    "\n",
    "y_test_pred_qr = X_test_design @ w_qr\n",
    "\n",
    "# OLS (targets conditional mean)\n",
    "ols = LinearRegression().fit(X_train, y_train)\n",
    "y_test_pred_ols = ols.predict(X_test)\n",
    "\n",
    "print(f\"Test D² pinball (alpha={alpha}):\")\n",
    "print(\"  quantile GD :\", d2_pinball_score(y_test, y_test_pred_qr, alpha=alpha))\n",
    "print(\"  OLS (mean)  :\", d2_pinball_score(y_test, y_test_pred_ols, alpha=alpha))\n",
    "\n",
    "print()\n",
    "print(\"Test mean pinball loss:\")\n",
    "print(\"  quantile GD :\", mean_pinball_loss(y_test, y_test_pred_qr, alpha=alpha))\n",
    "print(\"  OLS (mean)  :\", mean_pinball_loss(y_test, y_test_pred_ols, alpha=alpha))\n",
    "\n",
    "print()\n",
    "print(\"Coverage P(y <= y_pred):\")\n",
    "print(\"  quantile GD :\", float(np.mean(y_test <= y_test_pred_qr)))\n",
    "print(\"  OLS (mean)  :\", float(np.mean(y_test <= y_test_pred_ols)))\n",
    "\n",
    "# Fit visualization\n",
    "x_line = np.linspace(x.min(), x.max(), 250)\n",
    "X_line = x_line.reshape(-1, 1)\n",
    "X_line_design = np.column_stack([np.ones(len(x_line)), X_line])\n",
    "\n",
    "y_line_qr = X_line_design @ w_qr\n",
    "y_line_ols = ols.intercept_ + ols.coef_[0] * x_line\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode=\"markers\", name=\"data\", marker=dict(size=6, opacity=0.55)))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_ols, mode=\"lines\", name=\"OLS (mean)\", line=dict(color=\"gray\", dash=\"dash\")))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_qr, mode=\"lines\", name=f\"Quantile GD (alpha={alpha})\", line=dict(color=\"#4C78A8\", width=3)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Linear quantile regression: fitted line vs OLS\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029a03d",
   "metadata": {},
   "source": [
    "## 6) Practical usage (scikit-learn)\n",
    "\n",
    "`d2_pinball_score` is most useful when you care about **quantiles** rather than the mean:\n",
    "\n",
    "- prediction intervals / uncertainty bands (e.g. 10th and 90th percentile)\n",
    "- asymmetric costs (under-predicting is worse than over-predicting, or vice versa)\n",
    "- risk metrics like Value-at-Risk (VaR)\n",
    "\n",
    "In scikit-learn you typically pair it with a quantile model such as `QuantileRegressor`:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.metrics import d2_pinball_score\n",
    "\n",
    "alpha = 0.9\n",
    "model = QuantileRegressor(quantile=alpha, alpha=0.0).fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "d2_pinball_score(y_test, y_pred, alpha=alpha)\n",
    "```\n",
    "\n",
    "For cross-validation you can build a scorer:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import make_scorer\n",
    "scorer = make_scorer(d2_pinball_score, greater_is_better=True, alpha=0.9)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn QuantileRegressor (solves a convex optimization problem)\n",
    "alpha_level = 0.9\n",
    "\n",
    "qr = QuantileRegressor(quantile=alpha_level, alpha=0.0, solver=\"highs\").fit(X_train, y_train)\n",
    "y_test_pred_qr_sklearn = qr.predict(X_test)\n",
    "\n",
    "d2 = d2_pinball_score(y_test, y_test_pred_qr_sklearn, alpha=alpha_level)\n",
    "loss = mean_pinball_loss(y_test, y_test_pred_qr_sklearn, alpha=alpha_level)\n",
    "\n",
    "(d2, loss, qr.coef_, qr.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a0629",
   "metadata": {},
   "source": [
    "## Pros, cons, and when to use D² pinball\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Quantile-aware**: aligns evaluation with quantile regression and asymmetric costs\n",
    "- **Baseline-relative**: 0 means “no better than predicting the constant \\(\\alpha\\)-quantile”\n",
    "- **Robust-ish**: pinball loss grows linearly in the residual magnitude (less outlier-sensitive than squared loss)\n",
    "- **Works for intervals**: evaluate different quantiles (e.g. 0.1 and 0.9) separately\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- **Requires choosing** \\(\\alpha\\): different quantiles answer different questions\n",
    "- **Not scale-free across problems**: like MAE, the underlying loss depends on the units of \\(y\\)\n",
    "- **Can be negative**: models can be arbitrarily worse than the constant-quantile baseline\n",
    "- **Non-smooth objective**: training involves subgradients or specialized solvers (LP, coordinate descent, etc.)\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- forecasting with prediction intervals (demand, energy, latency, finance)\n",
    "- service-level problems (e.g. “allocate enough capacity so that 90% of days we’re covered”)\n",
    "- risk-sensitive settings (VaR-style quantiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62507c",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Plot mean pinball loss vs constant prediction \\(c\\) for several \\(\\alpha\\) values and verify the minimizer moves from low to high quantiles.\n",
    "2. Fit two models for \\(\\alpha=0.1\\) and \\(\\alpha=0.9\\); plot the resulting prediction interval and estimate empirical coverage.\n",
    "3. Create a dataset where D² pinball is strongly negative and interpret it in terms of numerator vs denominator.\n",
    "4. Show numerically that `alpha=0.5` makes pinball loss proportional to MAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedc1fa",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_pinball_score.html\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_pinball_loss.html\n",
    "- Koenker & Machado (1999): Goodness of fit for quantile regression (Eq. 7): https://doi.org/10.1080/01621459.1999.10473882\n",
    "- Hastie, Tibshirani, Wainwright (2015): Statistical Learning with Sparsity (pinball deviance discussion): https://hastie.su.domains/StatLearnSparsity/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}