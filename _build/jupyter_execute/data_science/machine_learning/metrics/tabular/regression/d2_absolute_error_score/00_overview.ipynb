{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9adddf",
   "metadata": {},
   "source": [
    "# D² Absolute Error Score (`d2_absolute_error_score`)\n",
    "\n",
    "`d2_absolute_error_score` is a **baseline-relative** regression score based on **absolute error** (L1). It is the L1 analogue of `r2_score`:\n",
    "\n",
    "- `R²` compares your squared error to the **mean** baseline (best constant under L2).\n",
    "- `D²_absolute` compares your absolute error to the **median** baseline (best constant under L1).\n",
    "\n",
    "**Best possible score is 1.0**.\n",
    "\n",
    "- `1.0` → perfect predictions.\n",
    "- `0.0` → no better than always predicting the (weighted) median of `y_true`.\n",
    "- `< 0` → worse than the median baseline (can be arbitrarily negative).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Write the metric in math notation and interpret its values.\n",
    "- See why the **median** is the baseline for absolute error.\n",
    "- Implement `d2_absolute_error_score` from scratch in NumPy (including sample weights).\n",
    "- Use the metric while optimizing a simple **L1 linear regression** model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17959f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import d2_absolute_error_score as sk_d2_absolute_error_score\n",
    "from sklearn.metrics import mean_absolute_error as sk_mean_absolute_error\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c553b",
   "metadata": {},
   "source": [
    "## 1) Definition (L1 analogue of R²)\n",
    "\n",
    "For targets \\(y_1,\\dots,y_n\\) and predictions \\(\\hat{y}_1,\\dots,\\hat{y}_n\\), define the **mean absolute error** (MAE):\n",
    "\n",
    "\\[\n",
    "\\mathrm{MAE}(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "\\]\n",
    "\n",
    "Let \\(m\\) be the **empirical median** of \\(y\\) (or the **weighted median** if `sample_weight` is provided). The **median baseline** is the constant predictor:\n",
    "\n",
    "\\[\n",
    "\\tilde{y}_i = m\\quad\\text{for all } i\n",
    "\\]\n",
    "\n",
    "Then the D² absolute error score is\n",
    "\n",
    "\\[\n",
    "D^2_{\\mathrm{AE}}(y, \\hat{y})\n",
    "= 1 - \\frac{\\mathrm{MAE}(y, \\hat{y})}{\\mathrm{MAE}(y, \\tilde{y})}.\n",
    "\\]\n",
    "\n",
    "This is exactly what scikit-learn implements:\n",
    "\n",
    "- `d2_absolute_error_score(y_true, y_pred)`\n",
    "- is the special case of `d2_pinball_score(..., alpha=0.5)`.\n",
    "\n",
    "### Multi-output\n",
    "\n",
    "If \\(y\\in\\mathbb{R}^{n\\times m}\\), scikit-learn computes a score per output and then averages them (uniformly by default).\n",
    "\n",
    "### Important edge case (constant targets)\n",
    "\n",
    "If all \\(y_i\\) are equal, then \\(\\mathrm{MAE}(y, \\tilde{y}) = 0\\).\n",
    "\n",
    "- perfect predictions → score `1.0`\n",
    "- otherwise → scikit-learn returns `0.0` (because the baseline has zero error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny example\n",
    "\n",
    "y_true = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n",
    "\n",
    "m = np.median(y_true)\n",
    "mae_model = np.mean(np.abs(y_true - y_pred))\n",
    "mae_baseline = np.mean(np.abs(y_true - m))\n",
    "\n",
    "print(\"median m      =\", m)\n",
    "print(\"MAE(model)    =\", mae_model)\n",
    "print(\"MAE(baseline) =\", mae_baseline)\n",
    "print(\"D²_AE manual  =\", 1 - mae_model / mae_baseline)\n",
    "print(\"D²_AE sklearn =\", sk_d2_absolute_error_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44fa98",
   "metadata": {},
   "source": [
    "## 2) Why the baseline is the median\n",
    "\n",
    "For an **L2** loss, the best constant prediction is the mean.\n",
    "\n",
    "For an **L1** loss, the best constant prediction is the median.\n",
    "\n",
    "Consider the optimization problem over constants \\(c\\):\n",
    "\n",
    "\\[\n",
    "\\min_c\\; f(c) = \\sum_{i=1}^n |y_i - c|.\n",
    "\\]\n",
    "\n",
    "- \\(f(c)\\) is convex and piecewise-linear.\n",
    "- A (sub)gradient of \\(|y_i - c|\\) w.r.t. \\(c\\) is \\(\\mathrm{sign}(c - y_i)\\) (with any value in \\([-1,1]\\) allowed when \\(c=y_i\\)).\n",
    "\n",
    "So a subgradient of \\(f\\) is:\n",
    "\n",
    "\\[\n",
    "\\partial f(c) = \\sum_{i=1}^n \\mathrm{sign}(c - y_i).\n",
    "\\]\n",
    "\n",
    "At an optimum \\(c^*\\), we need \\(0 \\in \\partial f(c^*)\\), which happens when roughly half the mass is on each side:\n",
    "\n",
    "- \\(\\#\\{i: y_i \\le c^*\\} \\ge n/2\\)\n",
    "- \\(\\#\\{i: y_i \\ge c^*\\} \\ge n/2\\)\n",
    "\n",
    "That’s exactly the defining property of a **median**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3744b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: MAE of a constant predictor c is minimized at the median\n",
    "\n",
    "y = rng.normal(loc=0.0, scale=1.0, size=101)\n",
    "\n",
    "c_grid = np.linspace(y.min() - 1.0, y.max() + 1.0, 400)\n",
    "mae_c = np.array([np.mean(np.abs(y - c)) for c in c_grid])\n",
    "\n",
    "m = np.median(y)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=c_grid, y=mae_c, mode=\"lines\", name=\"MAE(y, c)\"))\n",
    "fig.add_vline(x=float(m), line_dash=\"dash\", line_color=\"#E45756\", annotation_text=\"median\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"For L1, the best constant predictor is the median\",\n",
    "    xaxis_title=\"constant prediction c\",\n",
    "    yaxis_title=\"MAE(y, c)\",\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"median:\", m)\n",
    "print(\"argmin grid approx:\", c_grid[np.argmin(mae_c)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5eb4e",
   "metadata": {},
   "source": [
    "## 3) Interpreting values: 1, 0, negative\n",
    "\n",
    "Because D² is a *ratio to the median baseline*:\n",
    "\n",
    "- `1.0` means \\(\\mathrm{MAE}(y,\\hat{y}) = 0\\).\n",
    "- `0.0` means \\(\\mathrm{MAE}(y,\\hat{y}) = \\mathrm{MAE}(y,\\tilde{y})\\) (you match the baseline).\n",
    "- `-1.0` means your MAE is **twice** the baseline MAE.\n",
    "- More generally, if \\(\\mathrm{MAE}(y,\\hat{y}) = k\\,\\mathrm{MAE}(y,\\tilde{y})\\) then \\(D^2 = 1-k\\).\n",
    "\n",
    "Below we build a few predictors on the *same* `y_true` and compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208798a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 1.5 + rng.normal(size=80)\n",
    "\n",
    "m = np.median(y_true)\n",
    "baseline = np.full_like(y_true, m)\n",
    "\n",
    "models = {\n",
    "    \"Perfect\": y_true,\n",
    "    \"Noisy\": y_true + rng.normal(0, 0.4, size=y_true.shape),\n",
    "    \"Median baseline\": baseline,\n",
    "    \"Anti-median\": 2 * m - y_true,  # doubles every |y-m|, so D² = 1 - 2 = -1\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, y_pred in models.items():\n",
    "    mae_model = np.mean(np.abs(y_true - y_pred))\n",
    "    mae_base = np.mean(np.abs(y_true - baseline))\n",
    "    d2 = sk_d2_absolute_error_score(y_true, y_pred)\n",
    "    rows.append((name, mae_model, mae_base, d2))\n",
    "\n",
    "rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: MAE(model) vs MAE(baseline) and the resulting D²\n",
    "names = [r[0] for r in rows]\n",
    "mae_model = np.array([r[1] for r in rows])\n",
    "mae_base = np.array([r[2] for r in rows])\n",
    "d2 = np.array([r[3] for r in rows])\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Absolute errors\", \"D²_AE score\"))\n",
    "\n",
    "fig.add_trace(go.Bar(x=names, y=mae_model, name=\"MAE(model)\", marker_color=\"#4C78A8\"), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=names, y=mae_base, name=\"MAE(median baseline)\", marker_color=\"#F58518\"), row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"MAE\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=names, y=d2, name=\"D²_AE\", marker_color=\"#54A24B\"), row=1, col=2)\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"score\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(barmode=\"group\", height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3814aa",
   "metadata": {},
   "source": [
    "## 4) From-scratch NumPy implementation\n",
    "\n",
    "The implementation below mirrors scikit-learn’s behavior:\n",
    "\n",
    "- supports 1D and multi-output targets\n",
    "- supports `sample_weight`\n",
    "- supports `multioutput` aggregation:\n",
    "  - `'raw_values'` (per-output)\n",
    "  - `'uniform_average'` (default)\n",
    "  - array-like weights (length = `n_outputs`)\n",
    "\n",
    "We also reproduce scikit-learn’s convention for the constant-target edge case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_2d(y):\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 1:\n",
    "        return y.reshape(-1, 1)\n",
    "    if y.ndim == 2:\n",
    "        return y\n",
    "    raise ValueError(f\"y must be 1D or 2D, got shape {y.shape}\")\n",
    "\n",
    "\n",
    "def weighted_percentile_lower(array, sample_weight, percentile=50.0):\n",
    "    '''Lower weighted percentile (matches sklearn.utils.stats._weighted_percentile).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : (n,) or (n, m)\n",
    "    sample_weight : (n,) or same shape as array\n",
    "    percentile : float in [0, 100]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q : float or (m,) ndarray\n",
    "    '''\n",
    "    array = np.asarray(array)\n",
    "    w = np.asarray(sample_weight)\n",
    "\n",
    "    n_dim = array.ndim\n",
    "    if n_dim == 0:\n",
    "        return array.item()\n",
    "\n",
    "    if array.ndim == 1:\n",
    "        array = array.reshape(-1, 1)\n",
    "\n",
    "    if w.ndim == 1:\n",
    "        if w.shape[0] != array.shape[0]:\n",
    "            raise ValueError(\"sample_weight must have shape (n_samples,)\")\n",
    "        w = np.tile(w.reshape(-1, 1), (1, array.shape[1]))\n",
    "    elif w.shape != array.shape:\n",
    "        raise ValueError(\"sample_weight must have shape (n_samples,) or array.shape\")\n",
    "\n",
    "    if np.any(w < 0):\n",
    "        raise ValueError(\"sample_weight must be non-negative\")\n",
    "\n",
    "    sorted_idx = np.argsort(array, axis=0)\n",
    "    sorted_array = np.take_along_axis(array, sorted_idx, axis=0)\n",
    "    sorted_w = np.take_along_axis(w, sorted_idx, axis=0).astype(float)\n",
    "\n",
    "    cdf = np.cumsum(sorted_w, axis=0)\n",
    "    total = cdf[-1]\n",
    "\n",
    "    adj = (percentile / 100.0) * total\n",
    "    adj = np.asarray(adj, dtype=float)\n",
    "\n",
    "    # percentile=0: ignore leading zeros (sklearn GH20528 behavior)\n",
    "    mask = adj == 0\n",
    "    if np.any(mask):\n",
    "        adj[mask] = np.nextafter(adj[mask], adj[mask] + 1)\n",
    "\n",
    "    idx = np.array([np.searchsorted(cdf[:, j], adj[j]) for j in range(array.shape[1])])\n",
    "    idx = np.clip(idx, 0, array.shape[0] - 1)\n",
    "\n",
    "    q = sorted_array[idx, np.arange(array.shape[1])]\n",
    "    return q[0] if n_dim == 1 else q\n",
    "\n",
    "\n",
    "def mae_raw_values(y_true, y_pred, sample_weight=None):\n",
    "    '''Per-output MAE (returns shape (n_outputs,)).'''\n",
    "    y_true = _to_2d(y_true).astype(float)\n",
    "    y_pred = _to_2d(y_pred).astype(float)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true {y_true.shape}, y_pred {y_pred.shape}\")\n",
    "\n",
    "    abs_err = np.abs(y_true - y_pred)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return abs_err.mean(axis=0)\n",
    "\n",
    "    w = np.asarray(sample_weight, dtype=float).reshape(-1)\n",
    "    if w.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(\"sample_weight must have shape (n_samples,)\")\n",
    "    if np.any(w < 0):\n",
    "        raise ValueError(\"sample_weight must be non-negative\")\n",
    "\n",
    "    w_sum = w.sum()\n",
    "    if w_sum == 0:\n",
    "        raise ValueError(\"sample_weight sum must be > 0\")\n",
    "\n",
    "    return (abs_err * w[:, None]).sum(axis=0) / w_sum\n",
    "\n",
    "\n",
    "def d2_absolute_error_score_numpy(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput=\"uniform_average\",\n",
    "):\n",
    "    '''NumPy implementation of sklearn.metrics.d2_absolute_error_score.'''\n",
    "    y_true_2d = _to_2d(y_true)\n",
    "    y_pred_2d = _to_2d(y_pred)\n",
    "\n",
    "    if y_true_2d.shape != y_pred_2d.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true {y_true_2d.shape}, y_pred {y_pred_2d.shape}\")\n",
    "\n",
    "    n = y_true_2d.shape[0]\n",
    "    m = y_true_2d.shape[1]\n",
    "\n",
    "    if n < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    num = mae_raw_values(y_true_2d, y_pred_2d, sample_weight=sample_weight)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        med = np.percentile(y_true_2d, q=50, axis=0)\n",
    "    else:\n",
    "        med = weighted_percentile_lower(y_true_2d, sample_weight=sample_weight, percentile=50)\n",
    "\n",
    "    den = mae_raw_values(y_true_2d, np.tile(med, (n, 1)), sample_weight=sample_weight)\n",
    "\n",
    "    scores = np.ones(m, dtype=float)\n",
    "\n",
    "    nonzero_num = num != 0\n",
    "    nonzero_den = den != 0\n",
    "    valid = nonzero_num & nonzero_den\n",
    "\n",
    "    scores[valid] = 1.0 - (num[valid] / den[valid])\n",
    "    scores[nonzero_num & ~nonzero_den] = 0.0\n",
    "\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == \"raw_values\":\n",
    "            return scores\n",
    "        if multioutput != \"uniform_average\":\n",
    "            raise ValueError(\"multioutput must be 'raw_values', 'uniform_average', or array-like\")\n",
    "        weights = None\n",
    "    else:\n",
    "        weights = np.asarray(multioutput, dtype=float)\n",
    "        if weights.shape != (m,):\n",
    "            raise ValueError(f\"multioutput weights must have shape ({m},)\")\n",
    "\n",
    "    return float(np.average(scores, weights=weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7484c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks vs scikit-learn\n",
    "\n",
    "# 1D\n",
    "n = 120\n",
    "y_true = rng.normal(size=n)\n",
    "y_pred = y_true + rng.normal(0, 0.5, size=n)\n",
    "\n",
    "print(\"1D\")\n",
    "print(\"numpy :\", d2_absolute_error_score_numpy(y_true, y_pred))\n",
    "print(\"sklearn:\", sk_d2_absolute_error_score(y_true, y_pred))\n",
    "\n",
    "# Weighted\n",
    "w = rng.uniform(0.2, 2.0, size=n)\n",
    "print()\n",
    "print(\"Weighted\")\n",
    "print(\"numpy :\", d2_absolute_error_score_numpy(y_true, y_pred, sample_weight=w))\n",
    "print(\"sklearn:\", sk_d2_absolute_error_score(y_true, y_pred, sample_weight=w))\n",
    "\n",
    "# Multioutput\n",
    "Y_true = rng.normal(size=(80, 3))\n",
    "Y_pred = Y_true + rng.normal(0, 0.8, size=(80, 3))\n",
    "\n",
    "print()\n",
    "print(\"Multioutput (raw_values)\")\n",
    "print(\"numpy :\", d2_absolute_error_score_numpy(Y_true, Y_pred, multioutput=\"raw_values\"))\n",
    "print(\"sklearn:\", sk_d2_absolute_error_score(Y_true, Y_pred, multioutput=\"raw_values\"))\n",
    "\n",
    "print()\n",
    "print(\"Multioutput (weighted average)\")\n",
    "weights = np.array([0.2, 0.3, 0.5])\n",
    "print(\"numpy :\", d2_absolute_error_score_numpy(Y_true, Y_pred, multioutput=weights))\n",
    "print(\"sklearn:\", sk_d2_absolute_error_score(Y_true, Y_pred, multioutput=weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b27e49",
   "metadata": {},
   "source": [
    "## 5) Using D² while optimizing a model (from scratch)\n",
    "\n",
    "On a fixed dataset \\(\\{(x_i,y_i)\\}_{i=1}^n\\), the baseline term\n",
    "\n",
    "\\[\n",
    "\\mathrm{MAE}(y, \\tilde{y}) = \\mathrm{MAE}\\bigl(y,\\; \\text{median}(y)\\bigr)\n",
    "\\]\n",
    "\n",
    "depends **only** on \\(y\\), not on the model parameters \\(\\theta\\). Therefore:\n",
    "\n",
    "\\[\n",
    "D^2_{\\mathrm{AE}}(\\theta) = 1 - \\frac{\\mathrm{MAE}(\\theta)}{\\mathrm{MAE}_\\text{baseline}}\n",
    "\\]\n",
    "\n",
    "and maximizing \\(D^2_{\\mathrm{AE}}\\) is equivalent to minimizing MAE.\n",
    "\n",
    "### L1 linear regression\n",
    "\n",
    "With a linear model \\(\\hat{y} = Xw\\), the MAE objective is:\n",
    "\n",
    "\\[\n",
    "L(w) = \\frac{1}{n}\\sum_{i=1}^n |(Xw)_i - y_i|.\n",
    "\\]\n",
    "\n",
    "A subgradient is\n",
    "\n",
    "\\[\n",
    "\\nabla_w L(w) \\in \\frac{1}{n}X^\\top s,\\quad s_i \\in \\mathrm{sign}((Xw)_i - y_i),\n",
    "\\]\n",
    "\n",
    "where \\(\\mathrm{sign}(0)\\) can be any value in \\([-1,1]\\). In code we use `np.sign`, which returns 0 at exactly 0 residual.\n",
    "\n",
    "Below we fit an L1 regression model with **subgradient descent** and track both MAE and \\(D^2_{\\mathrm{AE}}\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de232c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with outliers: OLS (L2) can be pulled around; L1 is more robust.\n",
    "\n",
    "n = 250\n",
    "x = rng.uniform(-3, 3, size=n)\n",
    "y = 2.0 + 1.5 * x + rng.normal(0, 1.0, size=n)\n",
    "\n",
    "# Add a handful of strong outliers\n",
    "out_idx = rng.choice(n, size=18, replace=False)\n",
    "y[out_idx] += rng.normal(12, 4, size=out_idx.shape[0])\n",
    "\n",
    "X = np.column_stack([np.ones(n), x])\n",
    "\n",
    "# Baseline\n",
    "m = np.median(y)\n",
    "y_baseline = np.full_like(y, m)\n",
    "mae_baseline = np.mean(np.abs(y - y_baseline))\n",
    "\n",
    "# OLS (minimizes MSE, not MAE)\n",
    "w_ols, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "y_hat_ols = X @ w_ols\n",
    "\n",
    "print(\"MAE baseline:\", mae_baseline)\n",
    "print(\"D²_AE (OLS):\", sk_d2_absolute_error_score(y, y_hat_ols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca892330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_l1_linear_subgradient(X, y, *, lr0=0.4, n_steps=600):\n",
    "    '''Minimize MAE(y, Xw) with subgradient descent.'''\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    mae_hist = []\n",
    "    d2_hist = []\n",
    "\n",
    "    # Constant baseline for D²\n",
    "    m = np.median(y)\n",
    "    mae_base = np.mean(np.abs(y - m))\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        y_hat = X @ w\n",
    "        r = y_hat - y\n",
    "\n",
    "        # Subgradient of MAE\n",
    "        g = (X.T @ np.sign(r)) / n\n",
    "\n",
    "        lr = lr0 / np.sqrt(t + 1)  # simple diminishing step size\n",
    "        w = w - lr * g\n",
    "\n",
    "        mae = np.mean(np.abs(r))\n",
    "        d2 = 1.0 if mae == 0 else 1.0 - mae / mae_base\n",
    "\n",
    "        mae_hist.append(mae)\n",
    "        d2_hist.append(d2)\n",
    "\n",
    "    return w, np.array(mae_hist), np.array(d2_hist)\n",
    "\n",
    "\n",
    "w_l1, mae_hist, d2_hist = fit_l1_linear_subgradient(X, y)\n",
    "\n",
    "y_hat_l1 = X @ w_l1\n",
    "\n",
    "print(\"w_ols:\", w_ols)\n",
    "print(\"w_l1 :\", w_l1)\n",
    "\n",
    "print(\"D²_AE (L1):\", sk_d2_absolute_error_score(y, y_hat_l1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization diagnostics\n",
    "iters = np.arange(len(mae_hist))\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"D²_AE vs iteration\", \"MAE vs iteration\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=iters, y=d2_hist, mode=\"lines\", name=\"D²_AE\"), row=1, col=1)\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"iteration\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"score\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=iters, y=mae_hist, mode=\"lines\", name=\"MAE\"), row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"iteration\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"MAE\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit visualization\n",
    "x_line = np.linspace(x.min(), x.max(), 200)\n",
    "X_line = np.column_stack([np.ones_like(x_line), x_line])\n",
    "\n",
    "y_line_ols = X_line @ w_ols\n",
    "y_line_l1 = X_line @ w_l1\n",
    "\n",
    "y_line_baseline = np.full_like(x_line, m)\n",
    "\n",
    "colors = np.where(np.isin(np.arange(n), out_idx), \"#E45756\", \"#4C78A8\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode=\"markers\",\n",
    "        name=\"data\",\n",
    "        marker=dict(size=6, opacity=0.65, color=colors),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_baseline, mode=\"lines\", name=\"median baseline\", line=dict(dash=\"dot\")))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_ols, mode=\"lines\", name=\"OLS (L2)\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_l1, mode=\"lines\", name=\"L1 via MAE\", line=dict(width=3)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"L1 (MAE) fit is less sensitive to outliers (red points)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    height=520,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909d424",
   "metadata": {},
   "source": [
    "## 6) Pros, cons, and when to use\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Baseline-relative and interpretable**: `0` = “no better than predicting the median”.\n",
    "- **Robust to outliers** (compared to L2-based scores): absolute error grows linearly.\n",
    "- **Scale- and shift-invariant**: if you scale/shift both `y_true` and `y_pred`, the score stays the same.\n",
    "- **Connects to quantile regression**: it is `d2_pinball_score(alpha=0.5)`.\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- **Non-smooth objective**: optimizing MAE (and thus D²) needs subgradients / specialized solvers (or smooth approximations like Huber).\n",
    "- **Can be negative and unbounded below**: easy to misread if you expect a \\([0,1]\\) score.\n",
    "- **Not meaningful for a single sample**: returns `NaN` for `n_samples < 2`.\n",
    "- **Constant targets edge case**: if `y_true` is constant, the baseline error is 0; scikit-learn returns `1` for perfect predictions, otherwise `0`.\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- You care about *typical* error magnitude (median-like behavior), not rare extreme misses.\n",
    "- Data has heavy tails / outliers and you want a robustness-oriented score.\n",
    "- Model selection where a baseline-relative, unitless score is easier to compare than raw MAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203fb05",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a dataset where OLS has higher `R²` but lower `D²_AE` than an L1 fit. Explain why.\n",
    "2. Show numerically that `d2_absolute_error_score` is identical to `d2_pinball_score(alpha=0.5)`.\n",
    "3. Implement a **Huber** regression from scratch and compare its `D²_AE` to OLS and L1.\n",
    "4. With sample weights, build an example where the weighted median baseline shifts drastically and changes the score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6d136",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_absolute_error_score.html\n",
    "- scikit-learn user guide (D²): https://scikit-learn.org/stable/modules/model_evaluation.html#d2-score\n",
    "- Koenker & Machado (1999), quantile regression goodness-of-fit: https://doi.org/10.1080/01621459.1999.10473882\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}