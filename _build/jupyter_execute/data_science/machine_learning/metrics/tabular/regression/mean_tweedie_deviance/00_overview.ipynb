{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71bdf6a",
   "metadata": {},
   "source": [
    "# mean_tweedie_deviance (Mean Tweedie Deviance)\n",
    "\n",
    "`mean_tweedie_deviance` is a **regression** metric / loss from the *Tweedie* family of distributions.\n",
    "It is a great fit when targets are **non-negative**, often **skewed**, and may contain **many zeros**.\n",
    "\n",
    "Typical examples:\n",
    "\n",
    "- insurance pricing: claim cost per policy (many zeros + positive continuous)\n",
    "- rainfall amounts (many zeros + positive continuous)\n",
    "- event counts (Poisson special case)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Understand the Tweedie power parameter `p` and special cases (Normal / Poisson / Gamma)\n",
    "- Write the metric with clear math + domain constraints\n",
    "- Implement `mean_tweedie_deviance` from scratch in NumPy (incl. weights + edge cases)\n",
    "- Build intuition with plots: asymmetry, effect of `p`, and sensitivity to extreme errors\n",
    "- Use the deviance as a training loss: fit a simple Tweedie GLM with gradient descent\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic calculus (derivatives)\n",
    "- Familiarity with regression + linear models\n",
    "- Helpful: GLMs and link functions (we use a log link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52067db0",
   "metadata": {},
   "source": [
    "## 2.5) Probabilistic view: deviance as (scaled) negative log-likelihood\n",
    "\n",
    "For Tweedie GLMs, the deviance comes from the log-likelihood.\n",
    "You can view it as the extra negative log-likelihood you pay by predicting $\\hat{\\mu}$ instead of the (unrealistic) perfectly-fitting saturated model.\n",
    "\n",
    "For an exponential dispersion model, the (unit) deviance can be written as:\n",
    "\n",
    "$$\n",
    "d_p(y, \\hat{\\mu}) = 2\\,\\big( \\mathcal{L}(\\hat{\\mu}) - \\mathcal{L}(y) \\big)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}(\\mu)$ is the part of the negative log-likelihood that depends on $\\mu$.\n",
    "For $p\\notin\\{0,1,2\\}$ (ignoring constants that don't depend on $\\mu$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mu) = \\frac{\\mu^{2-p}}{2-p} - \\frac{y\\,\\mu^{1-p}}{1-p}\n",
    "$$\n",
    "\n",
    "and the saturated model corresponds to setting $\\mu=y$ for each sample.\n",
    "So minimizing mean Tweedie deviance is (up to a scale) the same as maximum likelihood for the Tweedie family.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313c014",
   "metadata": {},
   "source": [
    "## 1) The Tweedie family (why this metric exists)\n",
    "\n",
    "The Tweedie family is an **exponential dispersion model** with a mean–variance relationship:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(Y\\mid \\mu) = \\phi\\,\\mu^{p}\n",
    "$$\n",
    "\n",
    "- $\\mu = \\mathbb{E}[Y]$ is the mean\n",
    "- $\\phi>0$ is a dispersion (scale) parameter\n",
    "- $p$ is the **power parameter**: it determines the distribution type and how variance scales with the mean\n",
    "\n",
    "In scikit-learn, `mean_tweedie_deviance` is defined for `p \\le 0` or `p \\ge 1`.\n",
    "Common special cases:\n",
    "\n",
    "| power `p` | distribution (typical) | support |\n",
    "|---:|---|---|\n",
    "| `p = 0` | Normal | real-valued |\n",
    "| `p = 1` | Poisson | counts (0,1,2,...) |\n",
    "| `1 < p < 2` | Compound Poisson–Gamma | mass at 0 + positive continuous |\n",
    "| `p = 2` | Gamma | positive continuous |\n",
    "| `p = 3` | Inverse Gaussian | positive continuous |\n",
    "\n",
    "The main “wow” case is **`1 < p < 2`**: it models data that is *often zero* but otherwise *continuous and skewed*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712643d0",
   "metadata": {},
   "source": [
    "## 2) Definition\n",
    "\n",
    "Let $y_i$ be targets and $\\hat{\\mu}_i$ be predictions (think: *predicted mean*).\n",
    "The **per-sample Tweedie deviance** is:\n",
    "\n",
    "For $p \\notin \\{0,1,2\\}$:\n",
    "\n",
    "$$\n",
    "d_p(y, \\hat{\\mu}) =\n",
    "2\\left[\n",
    "\\frac{y^{2-p}}{(1-p)(2-p)}\n",
    "- \\frac{y\\,\\hat{\\mu}^{1-p}}{1-p}\n",
    "+ \\frac{\\hat{\\mu}^{2-p}}{2-p}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "- $p=0$ (Normal):\n",
    "\n",
    "  $$d_0(y, \\hat{\\mu}) = (y-\\hat{\\mu})^2$$\n",
    "\n",
    "- $p=1$ (Poisson):\n",
    "\n",
    "  $$d_1(y, \\hat{\\mu}) = 2\\left( y\\log\\frac{y}{\\hat{\\mu}} + \\hat{\\mu}-y \\right),\\quad 0\\log 0 := 0$$\n",
    "\n",
    "- $p=2$ (Gamma):\n",
    "\n",
    "  $$d_2(y, \\hat{\\mu}) = 2\\left( \\log\\frac{\\hat{\\mu}}{y} + \\frac{y}{\\hat{\\mu}} - 1 \\right)$$\n",
    "\n",
    "The scikit-learn metric is the **mean** deviance:\n",
    "\n",
    "$$\n",
    "\\mathrm{mean\\_tweedie\\_deviance}(y, \\hat{\\mu}) = \\frac{1}{n}\\sum_{i=1}^n d_p(y_i, \\hat{\\mu}_i)\n",
    "$$\n",
    "\n",
    "### Domain constraints (important)\n",
    "\n",
    "scikit-learn enforces these domains:\n",
    "\n",
    "- `p < 0`: $\\hat{\\mu} > 0$ (targets may be any real number)\n",
    "- `p = 0`: no constraints ($y,\\hat{\\mu}\\in\\mathbb{R}$)\n",
    "- `1 \\le p < 2`: $y \\ge 0$ and $\\hat{\\mu} > 0$\n",
    "- `p \\ge 2`: $y > 0$ and $\\hat{\\mu} > 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo (using scikit-learn)\n",
    "y_true = np.array([2.0, 0.0, 1.0, 4.0])\n",
    "y_pred = np.array([0.5, 0.5, 2.0, 2.0])\n",
    "\n",
    "print('power=1   (Poisson):', mean_tweedie_deviance(y_true, y_pred, power=1))\n",
    "print('power=1.5 (Tweedie):', mean_tweedie_deviance(y_true, y_pred, power=1.5))\n",
    "\n",
    "y_true_pos = np.array([2.0, 1.0, 4.0, 3.0])\n",
    "y_pred_pos = np.array([1.5, 0.7, 2.5, 3.4])\n",
    "\n",
    "print('power=0 (MSE):', mean_tweedie_deviance(y_true_pos, y_pred_pos, power=0))\n",
    "print('power=2 (Gamma):', mean_tweedie_deviance(y_true_pos, y_pred_pos, power=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0de53",
   "metadata": {},
   "source": [
    "## 3) NumPy implementation (from scratch)\n",
    "\n",
    "We'll implement two functions:\n",
    "\n",
    "- `tweedie_deviance_per_sample_np` → returns $d_p(y_i, \\hat{\\mu}_i)$ for each sample\n",
    "- `mean_tweedie_deviance_np` → averages the per-sample deviances (optionally weighted)\n",
    "\n",
    "Goal: match scikit-learn behavior, including input validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_1d_float(x, name):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(f'{name} must be 1D, got shape {x.shape!r}')\n",
    "    return x\n",
    "\n",
    "\n",
    "def _xlogy(x, y):\n",
    "    \"\"\"Compute x * log(y) with the convention 0 * log(y) = 0.\n",
    "\n",
    "    This avoids the RuntimeWarning you get from np.where(..., 0, x*np.log(y)).\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    x, y = np.broadcast_arrays(x, y)\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "\n",
    "    mask = x != 0\n",
    "    out[mask] = x[mask] * np.log(y[mask])\n",
    "    return out\n",
    "\n",
    "\n",
    "def tweedie_deviance_per_sample_np(y_true, y_pred, *, power=0.0):\n",
    "    y_true = _check_1d_float(y_true, 'y_true')\n",
    "    y_pred = _check_1d_float(y_pred, 'y_pred')\n",
    "    if y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError('y_true and y_pred must have the same length')\n",
    "\n",
    "    p = float(power)\n",
    "    if not (p <= 0 or p >= 1):\n",
    "        raise ValueError('power must be <= 0 or >= 1')\n",
    "\n",
    "    msg = f'Mean Tweedie deviance error with power={p} can only be used on '\n",
    "    if p < 0:\n",
    "        if np.any(y_pred <= 0):\n",
    "            raise ValueError(msg + 'strictly positive y_pred.')\n",
    "    elif p == 0:\n",
    "        pass\n",
    "    elif 1 <= p < 2:\n",
    "        if np.any(y_true < 0) or np.any(y_pred <= 0):\n",
    "            raise ValueError(msg + 'non-negative y and strictly positive y_pred.')\n",
    "    elif p >= 2:\n",
    "        if np.any(y_true <= 0) or np.any(y_pred <= 0):\n",
    "            raise ValueError(msg + 'strictly positive y and y_pred.')\n",
    "\n",
    "    if p < 0:\n",
    "        # Matches scikit-learn: use y_true^... only when y_true > 0.\n",
    "        y_pos = np.where(y_true > 0, y_true, 0.0)\n",
    "        dev = 2 * (\n",
    "            np.power(y_pos, 2 - p) / ((1 - p) * (2 - p))\n",
    "            - y_true * np.power(y_pred, 1 - p) / (1 - p)\n",
    "            + np.power(y_pred, 2 - p) / (2 - p)\n",
    "        )\n",
    "    elif p == 0:\n",
    "        dev = (y_true - y_pred) ** 2\n",
    "    elif p == 1:\n",
    "        # 2 * ( y*log(y/mu) - y + mu )\n",
    "        dev = 2 * (_xlogy(y_true, y_true / y_pred) - y_true + y_pred)\n",
    "    elif p == 2:\n",
    "        dev = 2 * (np.log(y_pred / y_true) + y_true / y_pred - 1)\n",
    "    else:\n",
    "        dev = 2 * (\n",
    "            np.power(y_true, 2 - p) / ((1 - p) * (2 - p))\n",
    "            - y_true * np.power(y_pred, 1 - p) / (1 - p)\n",
    "            + np.power(y_pred, 2 - p) / (2 - p)\n",
    "        )\n",
    "\n",
    "    return dev\n",
    "\n",
    "\n",
    "def mean_tweedie_deviance_np(y_true, y_pred, *, power=0.0, sample_weight=None):\n",
    "    dev = tweedie_deviance_per_sample_np(y_true, y_pred, power=power)\n",
    "    if sample_weight is None:\n",
    "        return float(np.mean(dev))\n",
    "\n",
    "    w = _check_1d_float(sample_weight, 'sample_weight')\n",
    "    if w.shape[0] != dev.shape[0]:\n",
    "        raise ValueError('sample_weight must have the same length as y')\n",
    "    if np.any(w < 0):\n",
    "        raise ValueError('sample_weight must be non-negative')\n",
    "    if float(np.sum(w)) == 0.0:\n",
    "        raise ValueError('sum of sample_weight must be > 0')\n",
    "\n",
    "    return float(np.sum(w * dev) / np.sum(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c76551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that our implementation matches scikit-learn\n",
    "for p in [0, 1, 1.5, 2, 3, -1]:\n",
    "    if p == 0:\n",
    "        y = rng.normal(size=300)\n",
    "        mu = rng.normal(size=300)\n",
    "    elif p < 2:\n",
    "        y = rng.poisson(lam=2.0, size=300).astype(float)\n",
    "        mu = np.exp(rng.normal(size=300))\n",
    "    else:\n",
    "        y = np.exp(rng.normal(size=300))\n",
    "        mu = np.exp(rng.normal(size=300))\n",
    "\n",
    "    w = rng.uniform(0.1, 2.0, size=y.shape[0])\n",
    "\n",
    "    sk = mean_tweedie_deviance(y, mu, power=p)\n",
    "    npv = mean_tweedie_deviance_np(y, mu, power=p)\n",
    "\n",
    "    sk_w = mean_tweedie_deviance(y, mu, power=p, sample_weight=w)\n",
    "    np_w = mean_tweedie_deviance_np(y, mu, power=p, sample_weight=w)\n",
    "\n",
    "    print(f'power={p:>4}: abs diff={abs(sk-npv):.3e}, abs diff (weighted)={abs(sk_w-np_w):.3e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e643a8",
   "metadata": {},
   "source": [
    "## 4) Intuition with plots\n",
    "\n",
    "Two important intuitions:\n",
    "\n",
    "1. **Asymmetry** (for `p >= 1`): underpredicting a positive target is usually punished much more than overpredicting.\n",
    "2. **Power `p` controls extreme sensitivity**: larger `p` tends to reduce the penalty for very large deviations.\n",
    "\n",
    "We'll visualize the deviance as a function of the multiplicative factor:\n",
    "\n",
    "$$\\hat{\\mu} = y \\cdot f$$\n",
    "\n",
    "where $f<1$ means underprediction and $f>1$ means overprediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = 10.0\n",
    "f = np.logspace(-2, 2, 600)  # multiplicative factor\n",
    "mu = y0 * f\n",
    "\n",
    "powers = [0, 1, 1.5, 2, 3]\n",
    "\n",
    "fig = go.Figure()\n",
    "for p in powers:\n",
    "    dev = tweedie_deviance_per_sample_np(np.full_like(mu, y0), mu, power=p)\n",
    "    fig.add_trace(go.Scatter(x=f, y=np.log1p(dev), mode='lines', name=f'p={p}'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Tweedie deviance vs multiplicative error (plotted as log(1 + deviance))',\n",
    "    xaxis_title='factor f in μ̂ = y · f (log scale)',\n",
    "    yaxis_title='log(1 + deviance)',\n",
    ")\n",
    "fig.update_xaxes(type='log')\n",
    "fig.add_vline(x=1.0, line_dash='dash', line_color='rgba(0,0,0,0.4)')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada76e45",
   "metadata": {},
   "source": [
    "### Why underprediction can hurt a lot\n",
    "\n",
    "For `p > 0`, the loss includes terms like $\\log\\hat{\\mu}$ (Poisson / Gamma special cases) or powers of $\\hat{\\mu}$.\n",
    "This makes the deviance blow up when:\n",
    "\n",
    "- $y>0$ and $\\hat{\\mu}\\to 0^+$  (severe underprediction)\n",
    "\n",
    "In optimization, this shows up directly in the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_grid = np.logspace(-3, 3, 600)\n",
    "y_fixed = 10.0\n",
    "\n",
    "fig = go.Figure()\n",
    "for p in [1, 1.5, 2, 3]:\n",
    "    # For p!=0, the derivative wrt μ is: ∂d/∂μ = 2 * μ^{-p} * (μ - y)\n",
    "    grad_mu = 2 * (mu_grid ** (-p)) * (mu_grid - y_fixed)\n",
    "    fig.add_trace(go.Scatter(x=mu_grid, y=np.abs(grad_mu), mode='lines', name=f'|∂d/∂μ|, p={p}'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Gradient magnitude vs prediction μ̂ (fixed y=10): small μ̂ can create huge gradients',\n",
    "    xaxis_title='μ̂ (log scale)',\n",
    "    yaxis_title='|∂d/∂μ| (log scale)',\n",
    ")\n",
    "fig.update_xaxes(type='log')\n",
    "fig.update_yaxes(type='log')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbafba",
   "metadata": {},
   "source": [
    "## 5) Using Tweedie deviance to fit a model (low-level optimization)\n",
    "\n",
    "`mean_tweedie_deviance` is not only an evaluation metric: it is also a natural **training loss** for Tweedie GLMs.\n",
    "\n",
    "We'll fit a 1D **log-link** model:\n",
    "\n",
    "$$\n",
    "\\eta_i = b_0 + b_1 x_i\n",
    "\\qquad\n",
    "\\hat{\\mu}_i = \\exp(\\eta_i)\n",
    "$$\n",
    "\n",
    "The log link guarantees $\\hat{\\mu}_i>0$, which is required for `p \\ne 0`.\n",
    "\n",
    "### Objective\n",
    "\n",
    "$$\n",
    "J(b_0,b_1) = \\frac{1}{n}\\sum_{i=1}^n d_p(y_i, \\hat{\\mu}_i)\n",
    "$$\n",
    "\n",
    "### Gradient (for `p != 0`)\n",
    "\n",
    "A useful identity (valid for `p != 0`) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d_p}{\\partial \\hat{\\mu}} = 2\\,\\hat{\\mu}^{-p}(\\hat{\\mu}-y)\n",
    "$$\n",
    "\n",
    "With the log link $\\hat{\\mu}=e^{\\eta}$, we have $\\frac{\\partial \\hat{\\mu}}{\\partial \\eta}=\\hat{\\mu}$, so:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d_p}{\\partial \\eta} = 2\\,\\hat{\\mu}^{1-p}(\\hat{\\mu}-y)\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_0} = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial d_p}{\\partial \\eta_i}\n",
    "\\qquad\n",
    "\\frac{\\partial J}{\\partial b_1} = \\frac{1}{n}\\sum_{i=1}^n x_i\\,\\frac{\\partial d_p}{\\partial \\eta_i}\n",
    "$$\n",
    "\n",
    "We'll use gradient descent to minimize $J$ for a synthetic dataset that looks like **compound Poisson** data (`1 < p < 2`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic compound Poisson–Gamma data (mass at 0 + positive continuous)\n",
    "n = 500\n",
    "x = rng.uniform(-2, 2, size=n)\n",
    "\n",
    "b0_true = 0.2\n",
    "b1_true = 0.8\n",
    "\n",
    "# True mean curve\n",
    "mu_true = np.exp(b0_true + b1_true * x)\n",
    "\n",
    "# Compound Poisson–Gamma construction:\n",
    "#   N ~ Poisson(lambda)\n",
    "#   Severity ~ Gamma(k, theta)\n",
    "#   Y = sum_{j=1..N} Severity_j\n",
    "# Then Y has a point mass at 0 and a continuous positive tail.\n",
    "\n",
    "severity_shape = 2.0\n",
    "severity_scale = 0.5  # mean severity = shape * scale = 1.0\n",
    "sev_mean = severity_shape * severity_scale\n",
    "\n",
    "lam = mu_true / sev_mean\n",
    "counts = rng.poisson(lam)\n",
    "\n",
    "y = np.zeros(n)\n",
    "mask = counts > 0\n",
    "y[mask] = rng.gamma(shape=counts[mask] * severity_shape, scale=severity_scale)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('y vs x', 'y distribution'))\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='markers', marker=dict(size=5, opacity=0.6), name='y'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=y, nbinsx=60, name='hist(y)'), row=1, col=2)\n",
    "fig.update_xaxes(title_text='x', row=1, col=1)\n",
    "fig.update_yaxes(title_text='y', row=1, col=1)\n",
    "fig.update_xaxes(title_text='y', row=1, col=2)\n",
    "fig.update_yaxes(title_text='count', row=1, col=2)\n",
    "fig.update_layout(title='Synthetic compound Poisson-like regression target', showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58422b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_clip(z, clip=20.0):\n",
    "    \"\"\"exp(z) with clipping to avoid overflow during training.\"\"\"\n",
    "\n",
    "    return np.exp(np.clip(z, -clip, clip))\n",
    "\n",
    "\n",
    "def tweedie_glm_loss_and_grad(b0, b1, x, y, *, power):\n",
    "    \"\"\"Return (loss, db0, db1) for μ̂ = exp(b0 + b1 x).\"\"\"\n",
    "\n",
    "    eta = b0 + b1 * x\n",
    "    mu = exp_clip(eta)\n",
    "\n",
    "    loss = mean_tweedie_deviance_np(y, mu, power=power)\n",
    "\n",
    "    p = float(power)\n",
    "    if p == 0:\n",
    "        raise ValueError('This helper assumes power != 0 (use MSE directly for p=0).')\n",
    "\n",
    "    # ∂d/∂η = 2 * (μ^(2-p) - y * μ^(1-p))\n",
    "    grad_eta = 2 * (mu ** (2 - p) - y * mu ** (1 - p))\n",
    "\n",
    "    db0 = float(np.mean(grad_eta))\n",
    "    db1 = float(np.mean(grad_eta * x))\n",
    "    return loss, db0, db1\n",
    "\n",
    "\n",
    "def fit_tweedie_glm_gd(x, y, *, power, lr=0.05, n_steps=600):\n",
    "    b0, b1 = 0.0, 0.0\n",
    "\n",
    "    history = {\n",
    "        'step': [],\n",
    "        'b0': [],\n",
    "        'b1': [],\n",
    "        'loss': [],\n",
    "    }\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        loss, db0, db1 = tweedie_glm_loss_and_grad(b0, b1, x, y, power=power)\n",
    "\n",
    "        history['step'].append(step)\n",
    "        history['b0'].append(b0)\n",
    "        history['b1'].append(b1)\n",
    "        history['loss'].append(loss)\n",
    "\n",
    "        b0 -= lr * db0\n",
    "        b1 -= lr * db1\n",
    "\n",
    "    return (b0, b1), history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43679abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "power = 1.5\n",
    "\n",
    "(b0_hat, b1_hat), hist = fit_tweedie_glm_gd(x, y, power=power, lr=0.05, n_steps=700)\n",
    "\n",
    "mu_hat = exp_clip(b0_hat + b1_hat * x)\n",
    "\n",
    "print(f'true params: b0={b0_true:.3f}, b1={b1_true:.3f}')\n",
    "print(f'GD   params: b0={b0_hat:.3f}, b1={b1_hat:.3f}, mean deviance={mean_tweedie_deviance_np(y, mu_hat, power=power):.4f}')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist['step'], y=hist['loss'], mode='lines', name='mean deviance'))\n",
    "fig.update_layout(\n",
    "    title='Gradient descent on mean Tweedie deviance',\n",
    "    xaxis_title='step',\n",
    "    yaxis_title='mean Tweedie deviance',\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Visualize fitted mean curve\n",
    "x_line = np.linspace(x.min(), x.max(), 300)\n",
    "mu_line_true = np.exp(b0_true + b1_true * x_line)\n",
    "mu_line_hat = np.exp(b0_hat + b1_hat * x_line)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='data', marker=dict(size=5, opacity=0.55)))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=mu_line_true, mode='lines', name='true E[y|x]', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=mu_line_hat, mode='lines', name='GD fitted mean'))\n",
    "fig.update_layout(title='Fitting the mean with Tweedie deviance (log link)', xaxis_title='x', yaxis_title='y')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9afa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss landscape over (b0, b1) and the GD path\n",
    "# (This is not a quadratic bowl like MSE, but is often nicely behaved for 1 <= p <= 2.)\n",
    "\n",
    "b0_grid = np.linspace(b0_hat - 1.4, b0_hat + 1.4, 120)\n",
    "b1_grid = np.linspace(b1_hat - 1.4, b1_hat + 1.4, 120)\n",
    "\n",
    "B0, B1 = np.meshgrid(b0_grid, b1_grid)\n",
    "ETA = B0[:, :, None] + B1[:, :, None] * x[None, None, :]\n",
    "MU = exp_clip(ETA)\n",
    "\n",
    "p = float(power)\n",
    "# p=1.5 here (general formula)\n",
    "const = np.power(y, 2 - p) / ((1 - p) * (2 - p))\n",
    "DEV = 2 * (\n",
    "    const[None, None, :]\n",
    "    - y[None, None, :] * np.power(MU, 1 - p) / (1 - p)\n",
    "    + np.power(MU, 2 - p) / (2 - p)\n",
    ")\n",
    "Z = np.mean(DEV, axis=2)\n",
    "\n",
    "stride = max(1, len(hist['step']) // 140)\n",
    "b0_path = np.array(hist['b0'])[::stride]\n",
    "b1_path = np.array(hist['b1'])[::stride]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=b0_grid,\n",
    "        y=b1_grid,\n",
    "        z=Z,\n",
    "        contours_coloring='heatmap',\n",
    "        colorbar=dict(title='mean deviance'),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=b0_path,\n",
    "        y=b1_path,\n",
    "        mode='lines+markers',\n",
    "        name='GD path',\n",
    "        marker=dict(size=4, color='black'),\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title='Mean Tweedie deviance landscape + gradient descent trajectory',\n",
    "    xaxis_title='b0 (intercept)',\n",
    "    yaxis_title='b1 (slope)',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea6fbc",
   "metadata": {},
   "source": [
    "## 6) Practical usage (scikit-learn)\n",
    "\n",
    "**As a metric**:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "mean_tweedie_deviance(y_true, y_pred, power=1.5)\n",
    "```\n",
    "\n",
    "**As a model loss**: scikit-learn provides `TweedieRegressor`, which minimizes a Tweedie deviance objective\n",
    "(optionally with L2 regularization via `alpha`).\n",
    "\n",
    "Choosing `power` is usually treated as a hyperparameter (cross-validate over a small grid like `[1.1, 1.3, 1.5, 1.7, 1.9]`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90678fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1)\n",
    "\n",
    "reg = TweedieRegressor(power=power, link='log', alpha=0.0, max_iter=5000)\n",
    "reg.fit(X, y)\n",
    "\n",
    "mu_sk = reg.predict(X)\n",
    "\n",
    "print('sklearn intercept_:', float(reg.intercept_))\n",
    "print('sklearn coef_     :', reg.coef_)\n",
    "print('mean deviance (sklearn fit):', mean_tweedie_deviance(y, mu_sk, power=power))\n",
    "print('mean deviance (our GD fit) :', mean_tweedie_deviance(y, exp_clip(b0_hat + b1_hat * x), power=power))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069649a8",
   "metadata": {},
   "source": [
    "## 7) Pros, cons, and when to use it\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Principled**: corresponds to (scaled) negative log-likelihood for Tweedie GLMs\n",
    "- **Handles zeros + positive continuous** when `1 < p < 2` (compound Poisson)\n",
    "- **Tunable** via `p`: bridges MSE (`p=0`), Poisson deviance (`p=1`), Gamma deviance (`p=2`)\n",
    "- **Naturally heteroscedastic**: matches data where variance grows with the mean\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Requires selecting a good **power parameter `p`** (mis-specification hurts)\n",
    "- **Domain constraints**: `y_pred` must be strictly positive for `p != 0` (and `y_true` must be positive for `p >= 2`)\n",
    "- Less “unit-interpretable” than MAE/MSE (involves powers/logs)\n",
    "- Can be **numerically harsh** if a model predicts values near 0 while targets are positive\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- `p=1` Poisson: count data (calls per hour, clicks per day, defects per batch)\n",
    "- `1 < p < 2` compound Poisson: claim cost, rainfall, “amount spent” with many zeros\n",
    "- `p=2` Gamma: positive continuous targets with multiplicative noise (severity, durations, rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71c86f",
   "metadata": {},
   "source": [
    "## 8) Common pitfalls + diagnostics\n",
    "\n",
    "- **Check your target support**: if you have zeros, avoid `p >= 2`.\n",
    "- **Ensure positivity of predictions** for `p != 0` (log link or clipping with a small epsilon).\n",
    "- **Watch for huge contributions**: plot per-sample deviances to spot underprediction near $\\hat{\\mu}\\approx 0$.\n",
    "- **Choose `p` deliberately**: cross-validate; `1 < p < 2` is often a strong starting point for zero-heavy positive targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc632a",
   "metadata": {},
   "source": [
    "## 9) Exercises\n",
    "\n",
    "1. Implement `d2_tweedie_score` and show how it relates to the mean deviance.\n",
    "2. On the synthetic dataset above, try `p in {1.1, 1.3, 1.5, 1.7, 1.9}` and compare fitted curves and final loss.\n",
    "3. Modify the data generator to increase the fraction of zeros. Does the best `p` shift?\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_tweedie_deviance.html\n",
    "- scikit-learn user guide (Tweedie regression): https://scikit-learn.org/stable/modules/linear_model.html#tweedie-regression\n",
    "- Jørgensen, B. (1997). *The Theory of Dispersion Models*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}