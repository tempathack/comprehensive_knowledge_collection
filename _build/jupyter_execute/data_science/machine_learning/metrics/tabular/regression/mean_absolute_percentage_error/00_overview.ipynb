{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc63b4b",
   "metadata": {},
   "source": [
    "# mean_absolute_percentage_error (MAPE)\n",
    "\n",
    "Mean Absolute Percentage Error (MAPE) measures the average **relative** size of the errors:\n",
    "\n",
    "> “On average, how far off am I, relative to the true value?”\n",
    "\n",
    "Because it is **unitless**, it’s common in forecasting and business settings.\n",
    "\n",
    "Important caveat: MAPE is undefined when a true target is 0 (and unstable when targets are near 0).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- define MAPE precisely (and relate it to MAE)\n",
    "- build intuition for “relative error” with plots\n",
    "- implement MAPE from scratch in NumPy (weights + multi-output + safe handling of zeros)\n",
    "- optimize a simple linear regression model by minimizing MAPE with subgradient descent\n",
    "- understand pros/cons and when to use MAPE\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- absolute error / residuals\n",
    "- basic linear regression notation\n",
    "- gradients / subgradients (helpful but not required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ce936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e625a",
   "metadata": {},
   "source": [
    "## 1) Definition\n",
    "\n",
    "Let $y \\in \\mathbb{R}^n$ be targets and $\\hat{y} \\in \\mathbb{R}^n$ be predictions.\n",
    "Define the per-sample **absolute percentage error** (APE):\n",
    "\n",
    "$$\n",
    "\\mathrm{APE}_i = \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\n",
    "$$\n",
    "\n",
    "Then the mean absolute percentage error is:\n",
    "\n",
    "$$\n",
    "\\mathrm{MAPE}(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{APE}_i\n",
    "$$\n",
    "\n",
    "In practice we usually guard against division by zero and sign ambiguity by using:\n",
    "\n",
    "$$\n",
    "\\mathrm{MAPE}(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{\\max(\\varepsilon, |y_i|)}\n",
    "$$\n",
    "\n",
    "This is the definition used by scikit-learn. Note:\n",
    "\n",
    "- scikit-learn returns a **relative** value in $[0, \\infty)$ (e.g. `0.23`), not a percentage in $[0, 100]$\n",
    "- to convert to “percent”, multiply by 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb56f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny example\n",
    "y_true = np.array([10.0, 100.0, 50.0, 25.0])\n",
    "y_pred = np.array([12.0, 102.0, 55.0, 20.0])\n",
    "\n",
    "ape = np.abs(y_true - y_pred) / np.abs(y_true)\n",
    "mape = float(ape.mean())  # relative value (0.0 = perfect)\n",
    "\n",
    "mape, 100 * mape, mean_absolute_percentage_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1b6c9",
   "metadata": {},
   "source": [
    "## 2) Intuition: “2 units” means different things\n",
    "\n",
    "MAPE normalizes each absolute error by the true value.\n",
    "\n",
    "- an absolute error of 2 when the true value is 10 is a 20% error\n",
    "- the same absolute error of 2 when the true value is 100 is a 2% error\n",
    "\n",
    "So MAPE tends to care a lot about getting **small targets** right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([10.0, 100.0])\n",
    "y_pred = np.array([12.0, 102.0])  # same absolute error (2) in both cases\n",
    "\n",
    "abs_err = np.abs(y_true - y_pred)\n",
    "ape_pct = 100 * abs_err / y_true\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Absolute error (units)\", \"Absolute percentage error (%)\"),\n",
    ")\n",
    "fig.add_trace(go.Bar(x=[\"y=10\", \"y=100\"], y=abs_err, name=\"abs error\"), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=[\"y=10\", \"y=100\"], y=ape_pct, name=\"% error\", marker_color=\"#E45756\"), row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"|y - ŷ|\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"100×|y - ŷ| / y\", row=1, col=2)\n",
    "fig.update_layout(height=350, title=\"Same absolute error, very different relative error\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7fe55",
   "metadata": {},
   "source": [
    "## 3) MAPE is a weighted MAE (and it’s scale-free)\n",
    "\n",
    "Rewrite the safe definition as:\n",
    "\n",
    "$$\n",
    "\\mathrm{MAPE}(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n} w_i\\,|y_i - \\hat{y}_i|\\qquad\\text{where}\\qquad\n",
    "w_i = \\frac{1}{\\max(\\varepsilon, |y_i|)}\n",
    "$$\n",
    "\n",
    "So MAPE is just **MAE with per-sample weights** that are larger when $|y_i|$ is small.\n",
    "\n",
    "A key property is **scale invariance**:\n",
    "\n",
    "$$\n",
    "\\mathrm{MAPE}(cy, c\\hat{y}) = \\mathrm{MAPE}(y, \\hat{y})\\qquad\\text{for any }c>0\n",
    "$$\n",
    "\n",
    "(While MAE scales linearly with $c$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = rng.lognormal(mean=2.0, sigma=0.8, size=200)\n",
    "y_pred = y_true * (1.0 + rng.normal(0.0, 0.15, size=y_true.size))\n",
    "\n",
    "scales = np.array([0.1, 1.0, 10.0, 100.0])\n",
    "mape_vals = []\n",
    "mae_vals = []\n",
    "\n",
    "for c in scales:\n",
    "    mape_vals.append(mean_absolute_percentage_error(c * y_true, c * y_pred))\n",
    "    mae_vals.append(mean_absolute_error(c * y_true, c * y_pred))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scales, y=mape_vals, mode=\"lines+markers\", name=\"MAPE (relative)\"))\n",
    "fig.add_trace(go.Scatter(x=scales, y=mae_vals, mode=\"lines+markers\", name=\"MAE (units)\", yaxis=\"y2\"))\n",
    "\n",
    "fig.update_xaxes(title_text=\"scale factor c\", type=\"log\")\n",
    "fig.update_yaxes(title_text=\"MAPE\", rangemode=\"tozero\")\n",
    "fig.update_layout(\n",
    "    title=\"Scale invariance: MAPE stays the same when you scale the target\",\n",
    "    height=380,\n",
    "    yaxis2=dict(title=\"MAE\", overlaying=\"y\", side=\"right\", showgrid=False),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f98a7",
   "metadata": {},
   "source": [
    "## 4) Loss shape: why small targets dominate\n",
    "\n",
    "For a **single** sample with $|y|>0$, the MAPE loss as a function of prediction $\\hat{y}$ is:\n",
    "\n",
    "$$\n",
    "\\ell(\\hat{y}) = \\frac{|y - \\hat{y}|}{|y|}\n",
    "$$\n",
    "\n",
    "This is a V-shape (like MAE), but its slope is $1/|y|$.\n",
    "\n",
    "- if $|y|$ is small, the V is steep → small absolute errors cause large percentage errors\n",
    "- if $|y|$ is large, the V is shallow → the same absolute error counts less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = [1.0, 10.0, 100.0]\n",
    "yhat = np.linspace(-20, 140, 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "for y in y_values:\n",
    "    loss = np.abs(y - yhat) / np.abs(y)\n",
    "    fig.add_trace(go.Scatter(x=yhat, y=loss, mode=\"lines\", name=f\"y={y:g}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Per-sample MAPE loss vs prediction (different true values)\",\n",
    "    xaxis_title=\"prediction ŷ\",\n",
    "    yaxis_title=\"|y - ŷ| / |y|\",\n",
    "    height=380,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b43df",
   "metadata": {},
   "source": [
    "## 5) Common pitfalls (and alternatives)\n",
    "\n",
    "### Pitfalls\n",
    "\n",
    "- **Zero targets**: if $y_i = 0$ then $|y_i - \\hat{y}_i|/|y_i|$ is undefined.\n",
    "  scikit-learn uses $\\max(\\varepsilon, |y_i|)$, which turns division-by-zero into a **huge** number.\n",
    "- **Near-zero targets**: values close to 0 can dominate the average (because they get very large weights).\n",
    "- **Negative targets**: you can still compute a “relative error” with $|y_i|$ in the denominator, but the word “percent” can be misleading.\n",
    "\n",
    "### Alternatives\n",
    "\n",
    "- **WAPE** (weighted absolute percentage error):\n",
    "  $$\\mathrm{WAPE} = \\frac{\\sum_i |y_i - \\hat{y}_i|}{\\sum_i |y_i|}$$\n",
    "  (behaves better with zeros unless *all* targets are zero).\n",
    "- **sMAPE** (symmetric MAPE):\n",
    "  $$\\mathrm{sMAPE} = \\frac{1}{n}\\sum_i \\frac{2|y_i - \\hat{y}_i|}{|y_i| + |\\hat{y}_i|}$$\n",
    "  (bounded, but still has edge cases and interpretability quirks).\n",
    "- **MAE/RMSE** when absolute error matters.\n",
    "- **RMSLE** when relative differences matter and $y \\ge 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How near-zero targets blow up the percentage error\n",
    "eps = np.finfo(np.float64).eps\n",
    "\n",
    "y = np.logspace(-6, 1, 500)  # from 1e-6 to 10\n",
    "abs_err = 1.0\n",
    "\n",
    "ape_no_guard = abs_err / y\n",
    "ape_with_eps = abs_err / np.maximum(y, eps)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y, y=ape_no_guard, mode=\"lines\", name=\"1/|y|\"))\n",
    "fig.add_trace(go.Scatter(x=y, y=ape_with_eps, mode=\"lines\", name=\"1/max(|y|, eps)\", line=dict(dash=\"dash\")))\n",
    "\n",
    "fig.update_xaxes(title_text=\"|y|\", type=\"log\")\n",
    "fig.update_yaxes(title_text=\"absolute percentage error\", type=\"log\")\n",
    "fig.update_layout(title=\"Fixed absolute error (1.0) becomes huge when |y| is tiny\", height=380)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc8a83",
   "metadata": {},
   "source": [
    "## 6) NumPy implementation from scratch\n",
    "\n",
    "Below is a NumPy implementation that mirrors scikit-learn’s behavior:\n",
    "\n",
    "- supports 1D and multioutput targets\n",
    "- supports `sample_weight` (weights per sample)\n",
    "- supports `multioutput` aggregation (`\"raw_values\"`, `\"uniform_average\"`, or per-output weights)\n",
    "- uses $\\varepsilon = \\mathrm{eps}(\\text{float64})$ to avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error_np(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput=\"uniform_average\",\n",
    "    epsilon=None,\n",
    "):\n",
    "    \"\"\"NumPy implementation of scikit-learn's MAPE.\n",
    "\n",
    "    Returns a relative value (e.g. 0.23), not a percent (23%).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"shape mismatch: y_true{y_true.shape} vs y_pred{y_pred.shape}\")\n",
    "\n",
    "    if epsilon is None:\n",
    "        epsilon = np.finfo(np.float64).eps\n",
    "\n",
    "    denom = np.maximum(np.abs(y_true), epsilon)\n",
    "    mape = np.abs(y_pred - y_true) / denom\n",
    "\n",
    "    # 1D\n",
    "    if y_true.ndim == 1:\n",
    "        if sample_weight is None:\n",
    "            return float(mape.mean())\n",
    "\n",
    "        w = np.asarray(sample_weight)\n",
    "        if w.shape != (y_true.shape[0],):\n",
    "            raise ValueError(f\"sample_weight must have shape {(y_true.shape[0],)}, got {w.shape}\")\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "        return float(np.sum(w * mape) / np.sum(w))\n",
    "\n",
    "    # 2D\n",
    "    if y_true.ndim != 2:\n",
    "        raise ValueError(\"y_true must be 1D or 2D\")\n",
    "\n",
    "    if sample_weight is None:\n",
    "        output_errors = mape.mean(axis=0)\n",
    "    else:\n",
    "        w = np.asarray(sample_weight)\n",
    "        if w.shape != (y_true.shape[0],):\n",
    "            raise ValueError(f\"sample_weight must have shape {(y_true.shape[0],)}, got {w.shape}\")\n",
    "        if np.any(w < 0):\n",
    "            raise ValueError(\"sample_weight must be non-negative\")\n",
    "        output_errors = np.average(mape, axis=0, weights=w)\n",
    "\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == \"raw_values\":\n",
    "            return output_errors\n",
    "        if multioutput == \"uniform_average\":\n",
    "            return float(np.mean(output_errors))\n",
    "        raise ValueError(\"multioutput must be 'raw_values', 'uniform_average', or array-like\")\n",
    "\n",
    "    w_out = np.asarray(multioutput, dtype=float)\n",
    "    if w_out.shape != (y_true.shape[1],):\n",
    "        raise ValueError(f\"multioutput weights must have shape {(y_true.shape[1],)}, got {w_out.shape}\")\n",
    "    if np.any(w_out < 0):\n",
    "        raise ValueError(\"multioutput weights must be non-negative\")\n",
    "\n",
    "    return float(np.sum(w_out * output_errors) / np.sum(w_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks vs scikit-learn\n",
    "\n",
    "# 1D\n",
    "y_true = rng.normal(size=60)\n",
    "y_pred = y_true + rng.normal(0, 0.4, size=60)\n",
    "y_true[[3, 15]] = 0.0  # include zeros to show eps-guard behavior\n",
    "\n",
    "print(\"1D\")\n",
    "print(\"numpy :\", mean_absolute_percentage_error_np(y_true, y_pred))\n",
    "print(\"sklearn:\", mean_absolute_percentage_error(y_true, y_pred))\n",
    "\n",
    "# Multioutput\n",
    "Y_true = rng.normal(size=(80, 3))\n",
    "Y_pred = Y_true + rng.normal(0, 0.5, size=(80, 3))\n",
    "w = rng.uniform(0.5, 2.0, size=80)\n",
    "\n",
    "print(\"\\nMultioutput (raw)\")\n",
    "print(\"numpy :\", mean_absolute_percentage_error_np(Y_true, Y_pred, sample_weight=w, multioutput=\"raw_values\"))\n",
    "print(\"sklearn:\", mean_absolute_percentage_error(Y_true, Y_pred, sample_weight=w, multioutput=\"raw_values\"))\n",
    "\n",
    "print(\"\\nMultioutput (weighted)\")\n",
    "weights = np.array([0.2, 0.3, 0.5])\n",
    "print(\"numpy :\", mean_absolute_percentage_error_np(Y_true, Y_pred, sample_weight=w, multioutput=weights))\n",
    "print(\"sklearn:\", mean_absolute_percentage_error(Y_true, Y_pred, sample_weight=w, multioutput=weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a81ad7",
   "metadata": {},
   "source": [
    "## 7) Using MAPE to *fit* a model (from scratch)\n",
    "\n",
    "MAPE is often used as an **evaluation** metric, but you *can* also use it as a training objective.\n",
    "\n",
    "For a linear model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = Xw + b\n",
    "$$\n",
    "\n",
    "the (safe) MAPE objective is:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{|(Xw + b)_i - y_i|}{\\max(\\varepsilon, |y_i|)}\n",
    "$$\n",
    "\n",
    "This is convex, but not differentiable everywhere (because of $|\\cdot|$). A simple low-level optimizer is **subgradient descent**.\n",
    "\n",
    "Let $r = Xw + b - y$ and $d_i = \\max(\\varepsilon, |y_i|)$. One valid subgradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_w J = \\frac{1}{n} X^\\top \\left(\\frac{\\mathrm{sign}(r)}{d}\\right)\\qquad\\qquad\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\mathrm{sign}(r_i)}{d_i}\n",
    "$$\n",
    "\n",
    "where division is elementwise.\n",
    "\n",
    "Interpretation: minimizing MAPE is equivalent to minimizing a **weighted MAE** with weights $1/d_i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c053d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic regression data with multiplicative (relative) noise\n",
    "# This is a setting where a relative-error metric like MAPE often makes sense.\n",
    "n = 500\n",
    "x = rng.uniform(0, 200, size=n)\n",
    "X = x[:, None]\n",
    "\n",
    "# True linear relationship (targets are strictly positive)\n",
    "y_clean = 20.0 + 5.0 * x  # range ~ [20, 1020]\n",
    "\n",
    "# Multiplicative lognormal noise with mean 1\n",
    "sigma = 0.35\n",
    "mult = np.exp(rng.normal(loc=-0.5 * sigma**2, scale=sigma, size=n))\n",
    "y = y_clean * mult\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3937613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: ordinary least squares (minimizes MSE)\n",
    "ols = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "\n",
    "mape_ols = mean_absolute_percentage_error(y_test, y_pred_ols)\n",
    "mae_ols = mean_absolute_error(y_test, y_pred_ols)\n",
    "mse_ols = mean_squared_error(y_test, y_pred_ols)\n",
    "\n",
    "(ols.intercept_, ols.coef_[0]), (mape_ols, mae_ols, mse_ols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression_mape_subgradient(X, y, *, lr0=500.0, n_iters=4000, epsilon=None):\n",
    "    \"\"\"Minimize MAPE for y_hat = X @ w + b using subgradient descent.\n",
    "\n",
    "    Uses a decaying learning rate: lr_t = lr0 / sqrt(t+1).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    if epsilon is None:\n",
    "        epsilon = np.finfo(np.float64).eps\n",
    "\n",
    "    # Good starting point when w=0 for L1-type losses\n",
    "    w = np.zeros(n_features)\n",
    "    b = float(np.median(y))\n",
    "\n",
    "    denom = np.maximum(np.abs(y), epsilon)\n",
    "    history = np.empty(n_iters)\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        y_hat = X @ w + b\n",
    "        r = y_hat - y\n",
    "        g = np.sign(r) / denom  # subgradient wrt y_hat\n",
    "\n",
    "        grad_w = (X.T @ g) / n_samples\n",
    "        grad_b = g.mean()\n",
    "\n",
    "        lr = lr0 / np.sqrt(t + 1)\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "        y_hat2 = X @ w + b\n",
    "        history[t] = np.mean(np.abs(y_hat2 - y) / denom)\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "# Feature scaling helps subgradient methods\n",
    "x_mean = X_train.mean(axis=0)\n",
    "x_std = X_train.std(axis=0)\n",
    "\n",
    "X_train_s = (X_train - x_mean) / x_std\n",
    "X_test_s = (X_test - x_mean) / x_std\n",
    "\n",
    "w_mape, b_mape, hist = fit_linear_regression_mape_subgradient(X_train_s, y_train)\n",
    "\n",
    "# Convert parameters back to the original x scale:\n",
    "# y_hat = w_s * ((x - mu)/sigma) + b_s = (w_s/sigma) * x + (b_s - w_s*mu/sigma)\n",
    "slope_mape = w_mape[0] / x_std[0]\n",
    "intercept_mape = b_mape - slope_mape * x_mean[0]\n",
    "\n",
    "y_pred_mape = intercept_mape + slope_mape * X_test[:, 0]\n",
    "\n",
    "mape_mape = mean_absolute_percentage_error(y_test, y_pred_mape)\n",
    "mae_mape = mean_absolute_error(y_test, y_pred_mape)\n",
    "mse_mape = mean_squared_error(y_test, y_pred_mape)\n",
    "\n",
    "(intercept_mape, slope_mape), (mape_mape, mae_mape, mse_mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c83e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    y=100 * hist,\n",
    "    title=\"Subgradient descent: MAPE objective vs iteration\",\n",
    "    labels={\"index\": \"iteration\", \"y\": \"train MAPE (%)\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit visualization: data + fitted lines\n",
    "x_line = np.linspace(X.min(), X.max(), 250)\n",
    "\n",
    "y_line_true = 20.0 + 5.0 * x_line\n",
    "y_line_ols = ols.intercept_ + ols.coef_[0] * x_line\n",
    "y_line_mape = intercept_mape + slope_mape * x_line\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_test[:, 0], y=y_test, mode=\"markers\", name=\"test data\", marker=dict(size=6, opacity=0.55)))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_true, mode=\"lines\", name=\"true line\", line=dict(color=\"green\", dash=\"dash\")))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_ols, mode=\"lines\", name=f\"OLS (MSE) | test MAPE={100*mape_ols:.1f}%\"))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_mape, mode=\"lines\", name=f\"MAPE-trained | test MAPE={100*mape_mape:.1f}%\", line=dict(color=\"#E45756\")))\n",
    "\n",
    "fig.update_layout(title=\"Optimizing MAPE shifts the fit toward relative accuracy\", height=420)\n",
    "fig.update_xaxes(title_text=\"x\")\n",
    "fig.update_yaxes(title_text=\"y\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of absolute percentage errors on the test set\n",
    "eps = np.finfo(np.float64).eps\n",
    "\n",
    "ape_ols = np.abs(y_test - y_pred_ols) / np.maximum(np.abs(y_test), eps)\n",
    "ape_mape = np.abs(y_test - y_pred_mape) / np.maximum(np.abs(y_test), eps)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=100 * ape_ols, name=\"OLS (MSE)\", boxpoints=\"outliers\"))\n",
    "fig.add_trace(go.Box(y=100 * ape_mape, name=\"MAPE-trained\", boxpoints=\"outliers\"))\n",
    "\n",
    "fig.update_yaxes(title_text=\"absolute % error\", type=\"log\")\n",
    "fig.update_layout(title=\"Test absolute percentage errors (%)\", height=380)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efb456",
   "metadata": {},
   "source": [
    "## 8) Practical usage (scikit-learn)\n",
    "\n",
    "MAPE is commonly used for **evaluation** and model selection.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "mean_absolute_percentage_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "For cross-validation, scikit-learn follows a “bigger is better” convention and exposes MAPE as **negative** MAPE:\n",
    "\n",
    "```python\n",
    "cross_val_score(model, X, y, scoring=\"neg_mean_absolute_percentage_error\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    LinearRegression(),\n",
    "    X,\n",
    "    y,\n",
    "    scoring=\"neg_mean_absolute_percentage_error\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "# Convert back to positive MAPE and percent\n",
    "mape_cv = -scores\n",
    "float(mape_cv.mean()), float((100 * mape_cv).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19ec6f",
   "metadata": {},
   "source": [
    "## Pros, cons, and when to use MAPE\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Interpretable**: “average relative error” is easy to communicate\n",
    "- **Unitless / scale-free**: comparable across targets measured in different units or at different scales\n",
    "- **Natural for multiplicative noise**: when you care about proportional errors (10% off is 10% off)\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Undefined at 0** and unstable near 0 (can explode and dominate the mean)\n",
    "- **Over-weights small targets**: optimization behaves like weighted MAE with weights $1/|y|$\n",
    "- **Awkward with negatives**: the “percent” interpretation breaks down if $y$ can be negative\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- forecasting demand/sales/traffic where values are strictly positive and not too close to 0\n",
    "- comparing performance across multiple series with different scales\n",
    "- business metrics where relative deviation matters more than absolute units\n",
    "\n",
    "### Avoid when\n",
    "\n",
    "- targets can be 0 or near 0 (consider MAE/RMSE, WAPE, or sMAPE)\n",
    "- you care about absolute units (e.g. “within ±5 kWh”)\n",
    "- targets can be negative and “percent” becomes ambiguous\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4eea34",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement WAPE and compare it to MAPE on a dataset with many zeros.\n",
    "2. Show that minimizing MAPE is equivalent to minimizing MAE with per-sample weights $w_i = 1/\\max(\\varepsilon, |y_i|)$.\n",
    "3. For a constant predictor $\\hat{y}_i \\equiv c$, experiment numerically with which $c$ minimizes MAPE (hint: weighted median).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1051b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html\n",
    "- Hyndman & Athanasopoulos, *Forecasting: Principles and Practice* (accuracy measures): https://otexts.com/fpp3/accuracy.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}