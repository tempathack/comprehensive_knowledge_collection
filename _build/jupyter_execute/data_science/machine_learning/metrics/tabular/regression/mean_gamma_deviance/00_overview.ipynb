{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310aed4e",
   "metadata": {},
   "source": [
    "# Mean Gamma Deviance (`mean_gamma_deviance`) — Regression Metric (From Scratch)\n",
    "\n",
    "Mean Gamma deviance is a **scale-invariant** regression loss for **strictly positive** targets.\n",
    "It measures error through the **ratio** between the true value and the prediction, making it a strong fit when **relative errors** matter (and variance tends to grow with the square of the mean).\n",
    "\n",
    "**Goals**\n",
    "- Build intuition with small numeric examples + Plotly visuals\n",
    "- Derive the formula + key properties (non-negativity, scale invariance, asymmetry)\n",
    "- Implement `mean_gamma_deviance` in NumPy (from scratch) and validate vs scikit-learn\n",
    "- Use the loss to fit a simple **Gamma regression (GLM)** with gradient descent (low-level NumPy)\n",
    "- Summarize pros/cons, good use cases, and common pitfalls\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_gamma_deviance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bea173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.linear_model import GammaRegressor\n",
    "from sklearn.metrics import mean_gamma_deviance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import sklearn\n",
    "\n",
    "print(\"numpy  :\", np.__version__)\n",
    "print(\"pandas :\", pd.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"plotly :\", plotly.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36534bae",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Regression setup: targets $y$ and predictions $\\hat y$\n",
    "- Comfort with logs and ratios\n",
    "- (Optional) basic derivatives for the gradient / optimization section\n",
    "\n",
    "**Domain constraints**\n",
    "- Requires **strictly positive** values: $y_i > 0$ and $\\hat y_i > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19112074",
   "metadata": {},
   "source": [
    "## 1) Definition and notation\n",
    "\n",
    "Let:\n",
    "\n",
    "- $y \\in \\mathbb{R}_{>0}^n$ be the true targets\n",
    "- $\\hat y \\in \\mathbb{R}_{>0}^n$ be the predictions\n",
    "\n",
    "The **Gamma deviance** per sample is:\n",
    "\n",
    "$$\n",
    "d_i(y_i, \\hat y_i)\n",
    "= 2\\left(\\log\\frac{\\hat y_i}{y_i} + \\frac{y_i}{\\hat y_i} - 1\\right)\n",
    "$$\n",
    "\n",
    "The **mean Gamma deviance** is:\n",
    "\n",
    "$$\n",
    "\\mathrm{MGD}(y, \\hat y)\n",
    "= \\frac{1}{n}\\sum_{i=1}^n d_i(y_i, \\hat y_i)\n",
    "$$\n",
    "\n",
    "In scikit-learn, `mean_gamma_deviance` is the Tweedie deviance with power $p=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaecb6d",
   "metadata": {},
   "source": [
    "### Where this comes from (deviance / likelihood view)\n",
    "\n",
    "In generalized linear models (GLMs), a **deviance** compares your model to a “perfect” *saturated* model:\n",
    "\n",
    "$$\n",
    "D(y, \\mu) = 2\\sum_{i=1}^n \\big[\\ell(y_i; y_i) - \\ell(y_i; \\mu_i)\\big]\n",
    "$$\n",
    "\n",
    "For the Gamma family (ignoring dispersion constants), the **negative log-likelihood** for one sample can be written as:\n",
    "\n",
    "$$\n",
    "-\\ell(y; \\mu) = \\log \\mu + \\frac{y}{\\mu} + \\text{const}\n",
    "$$\n",
    "\n",
    "So the deviance contribution is:\n",
    "\n",
    "$$\n",
    "d(y, \\mu)\n",
    "= 2\\Big[(\\log \\mu + y/\\mu) - (\\log y + 1)\\Big]\n",
    "= 2\\left(\\log\\frac{\\mu}{y} + \\frac{y}{\\mu} - 1\\right)\n",
    "$$\n",
    "\n",
    "This is exactly the formula used by `mean_gamma_deviance`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb843c",
   "metadata": {},
   "source": [
    "### Ratio form (why it measures *relative* error)\n",
    "\n",
    "Define the ratio\n",
    "\n",
    "$$\n",
    "r_i = \\frac{y_i}{\\hat y_i} \\quad (>0)\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "d_i\n",
    "= 2\\left(r_i - \\log r_i - 1\\right)\n",
    "$$\n",
    "\n",
    "So the loss depends only on the **ratio** $y/\\hat y$:\n",
    "\n",
    "- scaling both $y$ and $\\hat y$ by the same constant leaves $r$ unchanged → **same deviance**\n",
    "- a “2× overprediction” ($\\hat y = 2y$) is penalized the same whether $y=1$ or $y=1000$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57a64e",
   "metadata": {},
   "source": [
    "### Non-negativity (and when it is 0)\n",
    "\n",
    "A key inequality for $r>0$ is:\n",
    "\n",
    "$$\n",
    "\\log r \\le r - 1\n",
    "$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\n",
    "r - \\log r - 1 \\ge 0\n",
    "$$\n",
    "\n",
    "Therefore $d_i \\ge 0$ for every sample, and:\n",
    "\n",
    "- $d_i = 0$ **iff** $r_i = 1$ **iff** $y_i = \\hat y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18df83e",
   "metadata": {},
   "source": [
    "## 2) Intuition: shape of the penalty\n",
    "\n",
    "### Asymmetry\n",
    "\n",
    "Because $d_i$ depends on the ratio $r=y/\\hat y$:\n",
    "\n",
    "- **underprediction** ($\\hat y < y$) gives $r>1$ and grows roughly **linearly** in $r$\n",
    "- **overprediction** ($\\hat y > y$) gives $r<1$ and grows roughly like **$-\\log r$**\n",
    "\n",
    "So *big* underpredictions are typically penalized more strongly than equally-large overpredictions (in multiplicative terms).\n",
    "\n",
    "### Small-error approximation\n",
    "\n",
    "Let the **relative error** be\n",
    "\n",
    "$$\n",
    "\\varepsilon = \\frac{y-\\hat y}{\\hat y}\n",
    "\\quad\\Rightarrow\\quad\n",
    "r = \\frac{y}{\\hat y} = 1+\\varepsilon\n",
    "$$\n",
    "\n",
    "Using $\\log(1+\\varepsilon) \\approx \\varepsilon - \\varepsilon^2/2$ for small $\\varepsilon$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{2} = r - \\log r - 1\n",
    "\\approx (1+\\varepsilon) - (\\varepsilon - \\varepsilon^2/2) - 1\n",
    "= \\varepsilon^2/2\n",
    "$$\n",
    "\n",
    "So for small errors:\n",
    "\n",
    "$$\n",
    "d \\approx \\varepsilon^2 = \\left(\\frac{y-\\hat y}{\\hat y}\\right)^2\n",
    "$$\n",
    "\n",
    "Interpretation: **Gamma deviance behaves like squared relative error** near the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize d(r) = 2 * (r - log r - 1)\n",
    "r = np.logspace(-2, 2, 600)  # r = y / y_hat\n",
    "\n",
    "d = 2 * (r - np.log(r) - 1)\n",
    "\n",
    "fig = px.line(\n",
    "    x=r,\n",
    "    y=d,\n",
    "    log_x=True,\n",
    "    title=\"Gamma deviance as a function of ratio r = y / ŷ\",\n",
    "    labels={\"x\": \"r = y / ŷ (log scale)\", \"y\": \"d(r)\"},\n",
    ")\n",
    "fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ac351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few multiplicative errors side-by-side\n",
    "ratios = np.array([0.1, 0.5, 1.0, 2.0, 10.0])  # r = y / y_hat\n",
    "contrib = 2 * (ratios - np.log(ratios) - 1)\n",
    "\n",
    "pd.DataFrame({\"r = y/ŷ\": ratios, \"d(r)\": contrib}).style.format({\"d(r)\": \"{:.4f}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad257a",
   "metadata": {},
   "source": [
    "## 3) A tiny worked example\n",
    "\n",
    "We’ll compute mean Gamma deviance step-by-step and compare to scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([2.0, 0.5, 1.0, 4.0])\n",
    "y_pred = np.array([0.5, 0.5, 2.0, 2.0])\n",
    "\n",
    "per_sample = 2 * (np.log(y_pred / y_true) + y_true / y_pred - 1)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"ratio r=y/ŷ\": y_true / y_pred,\n",
    "        \"per-sample deviance d_i\": per_sample,\n",
    "    }\n",
    ")\n",
    "\n",
    "mgd_np = float(per_sample.mean())\n",
    "mgd_sklearn = mean_gamma_deviance(y_true, y_pred)\n",
    "\n",
    "(df.style.format({\"ratio r=y/ŷ\": \"{:.3f}\", \"per-sample deviance d_i\": \"{:.4f}\"}), mgd_np, mgd_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6b98e",
   "metadata": {},
   "source": [
    "## 4) NumPy implementation (from scratch)\n",
    "\n",
    "A minimal implementation is just the formula plus:\n",
    "\n",
    "- shape checks\n",
    "- positivity checks (or optional clipping)\n",
    "- optional sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c987b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_gamma_deviance_np(y_true, y_pred, sample_weight=None, *, eps=0.0):\n",
    "    '''Mean Gamma deviance (NumPy).\n",
    "\n",
    "    Matches scikit-learn's definition:\n",
    "        d_i = 2 * (log(y_pred / y_true) + y_true / y_pred - 1)\n",
    "        mean = average(d_i, weights=sample_weight)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Requires y_true > 0 and y_pred > 0.\n",
    "    - If eps > 0, values are clipped to [eps, +inf) to avoid log/div-by-zero.\n",
    "    '''\n",
    "\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true{y_true.shape} vs y_pred{y_pred.shape}\")\n",
    "\n",
    "    if eps > 0:\n",
    "        y_true = np.clip(y_true, eps, None)\n",
    "        y_pred = np.clip(y_pred, eps, None)\n",
    "\n",
    "    if np.any(y_true <= 0) or np.any(y_pred <= 0):\n",
    "        raise ValueError(\"mean_gamma_deviance requires strictly positive y_true and y_pred\")\n",
    "\n",
    "    dev = 2 * (np.log(y_pred / y_true) + y_true / y_pred - 1)\n",
    "    return float(np.average(dev, weights=sample_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ce638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate vs scikit-learn (including sample weights)\n",
    "y_true = rng.lognormal(mean=0.0, sigma=0.8, size=1_000)\n",
    "y_pred = rng.lognormal(mean=0.1, sigma=0.8, size=1_000)\n",
    "weights = rng.uniform(0.5, 2.0, size=1_000)\n",
    "\n",
    "print(\"unweighted np     :\", mean_gamma_deviance_np(y_true, y_pred))\n",
    "print(\"unweighted sklearn:\", mean_gamma_deviance(y_true, y_pred))\n",
    "print()\n",
    "print(\"weighted np       :\", mean_gamma_deviance_np(y_true, y_pred, sample_weight=weights))\n",
    "print(\"weighted sklearn  :\", mean_gamma_deviance(y_true, y_pred, sample_weight=weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5caa50",
   "metadata": {},
   "source": [
    "## 5) Relative vs absolute metrics (scale invariance in one picture)\n",
    "\n",
    "Consider a fixed multiplicative error: $\\hat y = c\\,y$.\n",
    "\n",
    "- Gamma deviance depends only on $r=y/\\hat y = 1/c$ → **constant** across scales.\n",
    "- Squared error $(y-\\hat y)^2$ grows with the scale of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.logspace(0, 3, 80)  # 1 .. 1000\n",
    "c = 1.5  # 50% overprediction\n",
    "\n",
    "y_hat = c * y\n",
    "\n",
    "per_sample_gamma = 2 * (np.log(y_hat / y) + y / y_hat - 1)\n",
    "per_sample_sq = (y - y_hat) ** 2\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y, y=per_sample_gamma, mode=\"lines\", name=\"Gamma deviance (per-sample)\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y,\n",
    "        y=per_sample_sq,\n",
    "        mode=\"lines\",\n",
    "        name=\"Squared error (per-sample)\",\n",
    "        yaxis=\"y2\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Same relative error (ŷ = 1.5·y) across scales\",\n",
    "    xaxis=dict(title=\"y (log scale)\", type=\"log\"),\n",
    "    yaxis=dict(title=\"Gamma deviance (scale-invariant)\"),\n",
    "    yaxis2=dict(title=\"Squared error (scale-dependent)\", overlaying=\"y\", side=\"right\", type=\"log\"),\n",
    "    legend=dict(x=0.02, y=0.98),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8223a560",
   "metadata": {},
   "source": [
    "## 6) Gradients (useful for optimization)\n",
    "\n",
    "For a single sample:\n",
    "\n",
    "$$\n",
    "d(y, \\hat y) = 2\\left(\\log\\frac{\\hat y}{y} + \\frac{y}{\\hat y} - 1\\right)\n",
    "$$\n",
    "\n",
    "Derivative w.r.t. the prediction $\\hat y$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial \\hat y}\n",
    "= 2\\left(\\frac{1}{\\hat y} - \\frac{y}{\\hat y^2}\\right)\n",
    "= 2\\,\\frac{\\hat y - y}{\\hat y^2}\n",
    "$$\n",
    "\n",
    "### Log-link parameterization (common in Gamma regression)\n",
    "\n",
    "If you model predictions as strictly positive via\n",
    "\n",
    "$$\n",
    "\\hat y = \\mu = \\exp(\\eta)\n",
    "$$\n",
    "\n",
    "then by the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial \\eta}\n",
    "= \\frac{\\partial d}{\\partial \\mu}\\,\\frac{\\partial \\mu}{\\partial \\eta}\n",
    "= 2\\,\\frac{\\mu - y}{\\mu^2}\\,\\mu\n",
    "= 2\\left(1 - \\frac{y}{\\mu}\\right)\n",
    "$$\n",
    "\n",
    "This form is often numerically nicer because it avoids $1/\\mu^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mean_gamma_deviance_wrt_mu(y_true, mu_pred):\n",
    "    # Gradient of mean Gamma deviance w.r.t. mu_pred (vector).\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    mu_pred = np.asarray(mu_pred, dtype=float)\n",
    "\n",
    "    if np.any(y_true <= 0) or np.any(mu_pred <= 0):\n",
    "        raise ValueError(\"y_true and mu_pred must be strictly positive\")\n",
    "\n",
    "    n = y_true.size\n",
    "    return (2.0 / n) * (mu_pred - y_true) / (mu_pred**2)\n",
    "\n",
    "\n",
    "def finite_diff_grad(f, x, eps=1e-6):\n",
    "    x = x.astype(float).copy()\n",
    "    g = np.zeros_like(x)\n",
    "    for i in range(x.size):\n",
    "        x1 = x.copy(); x1[i] += eps\n",
    "        x2 = x.copy(); x2[i] -= eps\n",
    "        g[i] = (f(x1) - f(x2)) / (2 * eps)\n",
    "    return g\n",
    "\n",
    "\n",
    "y = rng.lognormal(mean=0.0, sigma=0.7, size=7)\n",
    "mu = rng.lognormal(mean=0.2, sigma=0.7, size=7)\n",
    "\n",
    "f = lambda mu_vec: mean_gamma_deviance_np(y, mu_vec)\n",
    "\n",
    "g_analytic = grad_mean_gamma_deviance_wrt_mu(y, mu)\n",
    "g_numeric = finite_diff_grad(f, mu)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"mu\": mu, \"grad analytic\": g_analytic, \"grad numeric\": g_numeric, \"abs diff\": np.abs(g_analytic - g_numeric)}\n",
    ").style.format(\"{:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a37fc",
   "metadata": {},
   "source": [
    "## 7) Using mean Gamma deviance to optimize a model (Gamma regression)\n",
    "\n",
    "A common way to build a model that always predicts positive values is a **log link**:\n",
    "\n",
    "$$\n",
    "\\eta_i = x_i^\\top w\n",
    "\\qquad\\Rightarrow\\qquad\n",
    "\\mu_i = \\exp(\\eta_i)\n",
    "$$\n",
    "\n",
    "We fit $w$ by minimizing mean Gamma deviance:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{n}\\sum_{i=1}^n 2\\left(\\log\\frac{\\mu_i}{y_i} + \\frac{y_i}{\\mu_i} - 1\\right)\n",
    "$$\n",
    "\n",
    "Using the gradient w.r.t. $\\eta$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d_i}{\\partial \\eta_i} = 2\\left(1 - \\frac{y_i}{\\mu_i}\\right)\n",
    "$$\n",
    "\n",
    "and $\\eta = Xw$, the gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = \\frac{2}{n} X^\\top\\left(\\mathbf{1} - \\frac{y}{\\mu}\\right)\n",
    "$$\n",
    "\n",
    "We’ll implement gradient descent and fit a toy dataset generated from a Gamma model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Gamma-regression dataset (variance grows ~ mean^2)\n",
    "n = 600\n",
    "x1 = rng.normal(size=n)\n",
    "x2 = rng.normal(size=n)\n",
    "\n",
    "X_raw = np.column_stack([x1, x2])\n",
    "X_raw = (X_raw - X_raw.mean(axis=0)) / X_raw.std(axis=0)  # simple standardization\n",
    "\n",
    "X = np.column_stack([np.ones(n), X_raw])  # add intercept\n",
    "\n",
    "w_true = np.array([0.25, 0.9, -0.6])\n",
    "eta_true = X @ w_true\n",
    "mu_true = np.exp(eta_true)\n",
    "\n",
    "shape_k = 6.0  # larger k => less noise (var = mu^2 / k)\n",
    "y = rng.gamma(shape_k, scale=mu_true / shape_k)\n",
    "\n",
    "(mu_true.min(), mu_true.mean(), mu_true.max(), y.min(), y.mean(), y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize heteroscedasticity (log-log scatter)\n",
    "line = np.logspace(np.log10(mu_true.min()), np.log10(mu_true.max()), 200)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_true, y=y, mode=\"markers\", name=\"samples\", marker=dict(opacity=0.55)))\n",
    "fig.add_trace(go.Scatter(x=line, y=line, mode=\"lines\", name=\"y = μ\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Synthetic data: y ~ Gamma(mean=μ) (variance increases with μ)\",\n",
    "    xaxis=dict(title=\"true mean μ\", type=\"log\"),\n",
    "    yaxis=dict(title=\"observed y\", type=\"log\"),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, mu_train, mu_test = train_test_split(\n",
    "    X, y, mu_true, test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "def fit_gamma_regression_gd(X, y, *, lr=0.05, n_iter=2500, clip_eta=20.0):\n",
    "    # Fit log-link Gamma regression by minimizing mean Gamma deviance.\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    history = np.zeros(n_iter)\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        eta = np.clip(X @ w, -clip_eta, clip_eta)\n",
    "        mu = np.exp(eta)\n",
    "\n",
    "        history[t] = mean_gamma_deviance_np(y, mu)\n",
    "\n",
    "        # grad = (2/n) * X^T (1 - y/mu)\n",
    "        grad = (2.0 / n) * (X.T @ (1.0 - y / mu))\n",
    "        w -= lr * grad\n",
    "\n",
    "    return w, history\n",
    "\n",
    "\n",
    "w_gd, history = fit_gamma_regression_gd(X_train, y_train, lr=0.05, n_iter=2500)\n",
    "\n",
    "print(\"w_true:\", w_true)\n",
    "print(\"w_gd  :\", w_gd)\n",
    "print()\n",
    "print(\"train MGD:\", mean_gamma_deviance_np(y_train, np.exp(np.clip(X_train @ w_gd, -20, 20))))\n",
    "print(\"test  MGD:\", mean_gamma_deviance_np(y_test, np.exp(np.clip(X_test @ w_gd, -20, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a899d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    x=np.arange(len(history)),\n",
    "    y=history,\n",
    "    title=\"Training mean Gamma deviance during gradient descent\",\n",
    "    labels={\"x\": \"iteration\", \"y\": \"mean_gamma_deviance\"},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb01686",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_pred_test = np.exp(np.clip(X_test @ w_gd, -20, 20))\n",
    "\n",
    "line = np.logspace(np.log10(mu_test.min()), np.log10(mu_test.max()), 200)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_test, y=mu_pred_test, mode=\"markers\", name=\"pred\", marker=dict(opacity=0.6)))\n",
    "fig.add_trace(go.Scatter(x=line, y=line, mode=\"lines\", name=\"ideal\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Recovered mean μ on test set (gradient descent)\",\n",
    "    xaxis=dict(title=\"true μ\", type=\"log\"),\n",
    "    yaxis=dict(title=\"predicted μ\", type=\"log\"),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to scikit-learn's GammaRegressor\n",
    "# (it includes an intercept internally, so we pass features without the intercept column)\n",
    "\n",
    "gr = GammaRegressor(alpha=0.0, max_iter=5000)\n",
    "gr.fit(X_train[:, 1:], y_train)\n",
    "\n",
    "mu_pred_sk = gr.predict(X_test[:, 1:])\n",
    "\n",
    "print(\"sklearn [intercept, coef]:\", np.r_[gr.intercept_, gr.coef_])\n",
    "print(\"gd      w             :\", w_gd)\n",
    "print()\n",
    "print(\"test MGD (gd)     :\", mean_gamma_deviance(y_test, mu_pred_test))\n",
    "print(\"test MGD (sklearn):\", mean_gamma_deviance(y_test, mu_pred_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407e700",
   "metadata": {},
   "source": [
    "## 8) Pros, cons, and when to use mean Gamma deviance\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Scale-invariant / relative**: depends on $y/\\hat y$, so it treats multiplicative errors consistently across scales.\n",
    "- Well-matched to **Gamma-like data** (strictly positive, heteroscedastic with $\\mathrm{Var}(y) \\propto \\mu^2$).\n",
    "- For log-link linear models ($\\mu = \\exp(Xw)$), the objective is **convex in $w$** (good optimization properties).\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Requires **strictly positive** $y$ and $\\hat y$ (zeros/negatives break the log / division).\n",
    "- Can **explode** when predictions get very close to 0 (huge penalty).\n",
    "- Not in the same units as $y$ (less directly interpretable than MAE/RMSE).\n",
    "- Asymmetric in multiplicative error: large **underpredictions** tend to be punished more than equally-large overpredictions.\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- Modeling **positive continuous** quantities where **relative error** matters:\n",
    "  insurance claim severity, costs/prices, rainfall amounts, time-to-complete, demand with strictly positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236f3f2",
   "metadata": {},
   "source": [
    "## 9) Common pitfalls and diagnostics\n",
    "\n",
    "- **Zeros in the target**: Gamma deviance requires $y>0$.\n",
    "  - If you have many zeros + positive continuous values, consider a **Tweedie** model with $1<p<2$ (compound Poisson) or a two-part model.\n",
    "- **Model outputs can go negative**: if you use a plain linear model for $\\hat y$, it can produce invalid (≤0) predictions.\n",
    "  - Use a positive link function (e.g., $\\exp(\\cdot)$ or softplus) when training with Gamma deviance.\n",
    "- **Numerical stability**: clip predictions away from 0 (and sometimes clip the linear predictor before applying $\\exp$).\n",
    "- **Check relative residuals**: since the loss is relative, inspect $(y-\\hat y)/\\hat y$ vs $\\hat y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6eee17",
   "metadata": {},
   "source": [
    "## 10) Exercises\n",
    "\n",
    "1. Implement a **weighted** gradient descent version (use `sample_weight`) and verify it matches `np.average` weighting.\n",
    "2. Show empirically that, for small relative errors, Gamma deviance is close to **squared relative error**.\n",
    "3. Compare three approaches on the same positive dataset:\n",
    "   - minimize MSE on $y$\n",
    "   - minimize MSE on $\\log y$ (lognormal-ish)\n",
    "   - minimize Gamma deviance (Gamma-ish)\n",
    "   and inspect which residual pattern looks most appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d4db8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: `sklearn.metrics.mean_gamma_deviance`\n",
    "- scikit-learn User Guide: Tweedie deviance (`mean_tweedie_deviance`)\n",
    "- Generalized Linear Models (GLMs) and Gamma regression (log link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}