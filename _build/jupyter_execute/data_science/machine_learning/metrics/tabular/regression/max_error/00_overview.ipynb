{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48df2c1e",
   "metadata": {},
   "source": [
    "# Max Error (`max_error`)\n",
    "\n",
    "`max_error` (also called **maximum absolute error**) measures the **single worst absolute mistake** a regression model makes on a dataset.\n",
    "\n",
    "**Goals**\n",
    "- Define `max_error` precisely (and connect it to the $\\ell_\\infty$ norm)\n",
    "- Visualize what “worst-case error” means\n",
    "- Implement the metric from scratch in NumPy\n",
    "- Compare it to MAE and RMSE\n",
    "- Use a smooth surrogate to optimize a simple linear regression model for worst-case error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94caf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import max_error, mean_absolute_error, mean_squared_error\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc2178",
   "metadata": {},
   "source": [
    "## 1) Definition\n",
    "\n",
    "Let $y \\in \\mathbb{R}^n$ be targets and $\\hat{y} \\in \\mathbb{R}^n$ predictions. Define residuals $r_i = y_i - \\hat{y}_i$.\n",
    "\n",
    "The **max error** is the maximum absolute residual:\n",
    "\n",
    "$$\n",
    "\\mathrm{MaxError}(y, \\hat{y}) = \\max_{i \\in \\{1,\\ldots,n\\}} |y_i - \\hat{y}_i|\n",
    "= \\|y-\\hat{y}\\|_{\\infty}.\n",
    "$$\n",
    "\n",
    "- **Best possible value**: $0$ (perfect predictions)\n",
    "- **Units**: same as the target $y$\n",
    "- **Important behavior**: only the *worst* point matters\n",
    "\n",
    "### Relation to MAE / RMSE\n",
    "If we define absolute errors $e_i = |y_i-\\hat{y}_i| \\ge 0$, then:\n",
    "\n",
    "$$\n",
    "\\max_i e_i \\;\\ge\\; \\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\\;\\ge\\; \\frac{1}{n}\\sum_{i=1}^n e_i.\n",
    "$$\n",
    "\n",
    "So, for the same residuals: **MaxError $\\ge$ RMSE $\\ge$ MAE**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d902545",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([2.0, 0.0, 4.0, 1.0, 3.0])\n",
    "y_pred = np.array([1.7, 0.2, 3.9, 2.3, 2.8])\n",
    "\n",
    "residual = y_true - y_pred\n",
    "abs_error = np.abs(residual)\n",
    "\n",
    "max_err_np = float(np.max(abs_error))\n",
    "max_err_skl = float(max_error(y_true, y_pred))\n",
    "\n",
    "print(\"residuals:\", residual)\n",
    "print(\"|residuals|:\", abs_error)\n",
    "print(f\"max_error (NumPy):  {max_err_np:.4f}\")\n",
    "print(f\"max_error (sklearn): {max_err_skl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d70303",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(y_true))\n",
    "max_idx = int(np.argmax(abs_error))\n",
    "\n",
    "lo = min(y_true.min(), y_pred.min()) - 0.5\n",
    "hi = max(y_true.max(), y_pred.max()) + 0.5\n",
    "line = np.linspace(lo, hi, 200)\n",
    "\n",
    "colors = np.array([\"#1f77b4\"] * len(y_true), dtype=object)\n",
    "colors[max_idx] = \"crimson\"\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Predictions vs targets\", \"Absolute errors |y-ŷ|\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_true,\n",
    "        y=y_pred,\n",
    "        mode=\"markers\",\n",
    "        name=\"samples\",\n",
    "        marker=dict(size=10, color=colors),\n",
    "        customdata=np.column_stack([idx, abs_error]),\n",
    "        hovertemplate=\"i=%{customdata[0]}<br>y=%{x:.2f}<br>ŷ=%{y:.2f}<br>|e|=%{customdata[1]:.2f}<extra></extra>\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=line,\n",
    "        y=line,\n",
    "        mode=\"lines\",\n",
    "        name=\"y = ŷ\",\n",
    "        line=dict(color=\"gray\", dash=\"dash\"),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[y_true[max_idx]],\n",
    "        y=[y_pred[max_idx]],\n",
    "        mode=\"markers+text\",\n",
    "        name=\"worst point\",\n",
    "        marker=dict(size=14, color=\"crimson\", symbol=\"x\"),\n",
    "        text=[\"max\"],\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=idx,\n",
    "        y=abs_error,\n",
    "        marker_color=colors,\n",
    "        name=\"|error|\",\n",
    "        hovertemplate=\"i=%{x}<br>|e|=%{y:.3f}<extra></extra>\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[max_idx],\n",
    "        y=[abs_error[max_idx]],\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(size=10, color=\"crimson\"),\n",
    "        text=[f\"max = {abs_error[max_idx]:.2f}\"],\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False,\n",
    "        hoverinfo=\"skip\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"ŷ\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"sample index i\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"|y - ŷ|\", row=1, col=2)\n",
    "fig.update_layout(title=f\"max_error = {max_err_np:.3f}\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19e12c",
   "metadata": {},
   "source": [
    "## 2) Intuition: why `max_error` behaves differently\n",
    "\n",
    "Unlike MAE/RMSE (which average over points), `max_error` is a **minimax** statistic:\n",
    "\n",
    "- If you improve many *non-worst* points but the worst point stays the same, **`max_error` does not change**.\n",
    "- If a single point gets worse and becomes the new maximum, **`max_error` jumps**.\n",
    "\n",
    "A good way to see this is to vary the prediction for *one* sample while keeping all others fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 60\n",
    "\n",
    "y_true = rng.normal(0, 1.0, size=n)\n",
    "y_pred_base = y_true + rng.normal(0, 0.6, size=n)\n",
    "\n",
    "j = 0  # we'll perturb this one prediction\n",
    "\n",
    "deltas = np.linspace(-6, 6, 400)\n",
    "max_vals, mae_vals, rmse_vals = [], [], []\n",
    "\n",
    "for d in deltas:\n",
    "    y_pred = y_pred_base.copy()\n",
    "    y_pred[j] += d\n",
    "\n",
    "    r = y_true - y_pred\n",
    "    e = np.abs(r)\n",
    "\n",
    "    max_vals.append(float(np.max(e)))\n",
    "    mae_vals.append(float(np.mean(e)))\n",
    "    rmse_vals.append(float(np.sqrt(np.mean(r**2))))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=deltas, y=max_vals, mode=\"lines\", name=\"max_error\"))\n",
    "fig.add_trace(go.Scatter(x=deltas, y=mae_vals, mode=\"lines\", name=\"MAE\"))\n",
    "fig.add_trace(go.Scatter(x=deltas, y=rmse_vals, mode=\"lines\", name=\"RMSE\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Perturbing one prediction: max_error vs MAE vs RMSE\",\n",
    "    xaxis_title=\"delta added to one prediction\",\n",
    "    yaxis_title=\"metric value\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586d887",
   "metadata": {},
   "source": [
    "## 3) `max_error` from scratch (NumPy)\n",
    "\n",
    "In scikit-learn, `max_error` is defined for **single-output regression** (multioutput is not supported).\n",
    "\n",
    "Below is a minimal NumPy implementation with similar shape checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_error_np(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.ndim != 1 or y_pred.ndim != 1:\n",
    "        raise ValueError(\"Multioutput not supported in max_error\")\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "    return float(np.max(np.abs(y_true - y_pred)))\n",
    "\n",
    "\n",
    "# quick check\n",
    "for _ in range(3):\n",
    "    yt = rng.normal(size=20)\n",
    "    yp = yt + rng.normal(0, 0.5, size=20)\n",
    "    assert np.isclose(max_error_np(yt, yp), max_error(yt, yp))\n",
    "\n",
    "print(\"ok: max_error_np matches sklearn on random tests\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e60602",
   "metadata": {},
   "source": [
    "## 4) Example: one bad outlier dominates\n",
    "\n",
    "Because `max_error` is a worst-case statistic, a single extreme residual can dominate the score.\n",
    "\n",
    "This can be a *feature* (if the worst-case matters) or a *bug* (if the worst-case is just noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1aea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "\n",
    "y_true = rng.normal(0, 1.0, size=n)\n",
    "y_pred = y_true + rng.normal(0, 0.3, size=n)\n",
    "\n",
    "# Inject one huge mistake\n",
    "k = 7\n",
    "\n",
    "y_pred[k] += 6.0\n",
    "\n",
    "r = y_true - y_pred\n",
    "e = np.abs(r)\n",
    "\n",
    "mae = float(np.mean(e))\n",
    "rmse = float(np.sqrt(np.mean(r**2)))\n",
    "mx = float(np.max(e))\n",
    "\n",
    "print(f\"MAE  = {mae:.3f}\")\n",
    "print(f\"RMSE = {rmse:.3f}\")\n",
    "print(f\"max_error = {mx:.3f}\")\n",
    "\n",
    "colors = np.array([\"#1f77b4\"] * n, dtype=object)\n",
    "colors[int(np.argmax(e))] = \"crimson\"\n",
    "\n",
    "fig = px.bar(\n",
    "    x=np.arange(n),\n",
    "    y=e,\n",
    "    title=\"Absolute errors with a single outlier\",\n",
    "    labels={\"x\": \"sample index i\", \"y\": \"|y - ŷ|\"},\n",
    ")\n",
    "fig.update_traces(marker_color=colors)\n",
    "fig.add_hline(y=mx, line_dash=\"dash\", line_color=\"crimson\", annotation_text=f\"max = {mx:.2f}\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3aa886",
   "metadata": {},
   "source": [
    "## 5) Using `max_error` as an optimization objective (minimax regression)\n",
    "\n",
    "For a model $\\hat{y}_i = f(x_i;\\theta)$, minimizing max error means:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\; \\max_{i} \\; |y_i - f(x_i;\\theta)|.\n",
    "$$\n",
    "\n",
    "For a **line** $\\hat{y}_i = b_0 + b_1 x_i$ this is a classic problem: **$\\ell_\\infty$ (Chebyshev) regression**.\n",
    "\n",
    "### Linear-programming form (convex)\n",
    "Introduce a slack variable $t \\ge 0$ representing the maximum absolute residual:\n",
    "\n",
    "$$\n",
    "\\min_{b_0,b_1,t} \\; t\n",
    "\\quad\\text{s.t.}\\quad\n",
    "-t \\le y_i - (b_0 + b_1 x_i) \\le t \\;\\; \\forall i.\n",
    "$$\n",
    "\n",
    "This is convex, but the max/absolute value makes it **non-smooth**, so plain gradient descent on the exact objective is awkward.\n",
    "\n",
    "### Smooth surrogate (for gradient descent)\n",
    "We can build a differentiable approximation in two steps:\n",
    "\n",
    "1) Smooth absolute value\n",
    "\n",
    "$$\n",
    "|r| \\approx \\sqrt{r^2 + \\varepsilon}\n",
    "$$\n",
    "\n",
    "2) Smooth max via log-sum-exp\n",
    "\n",
    "$$\n",
    "\\max_i z_i \\approx \\frac{1}{\\alpha}\\log\\sum_i \\exp(\\alpha z_i)\n",
    "$$\n",
    "\n",
    "With $r_i = y_i - (b_0 + b_1 x_i)$ and $z_i = \\sqrt{r_i^2+\\varepsilon}$:\n",
    "\n",
    "$$\n",
    "\\tilde{J}(b_0,b_1) = \\frac{1}{\\alpha}\\log\\sum_{i=1}^n \\exp(\\alpha\\,\\sqrt{r_i^2+\\varepsilon}).\n",
    "$$\n",
    "\n",
    "As $\\alpha \\to \\infty$ and $\\varepsilon \\to 0$, $\\tilde{J}$ approaches the true max error.\n",
    "\n",
    "#### Gradients\n",
    "Let\n",
    "\n",
    "$$\n",
    "w_i = \\frac{\\exp(\\alpha z_i)}{\\sum_j \\exp(\\alpha z_j)} \\quad (\\text{a softmax over } z_i)\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{J}}{\\partial r_i} = w_i\\,\\frac{r_i}{z_i},\n",
    "\\qquad\n",
    "\\frac{\\partial \\tilde{J}}{\\partial b_0} = -\\sum_i w_i\\,\\frac{r_i}{z_i},\n",
    "\\qquad\n",
    "\\frac{\\partial \\tilde{J}}{\\partial b_1} = -\\sum_i w_i\\,\\frac{r_i}{z_i}\\,x_i.\n",
    "$$\n",
    "\n",
    "Interpretation: the gradient becomes a **weighted combination of signs**, and with large $\\alpha$ the weights concentrate on the current worst-error points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data with a couple of outliers\n",
    "n = 140\n",
    "x = rng.uniform(-3, 3, size=n)\n",
    "\n",
    "b0_true = 1.5\n",
    "b1_true = 2.0\n",
    "noise = rng.normal(0, 0.8, size=n)\n",
    "\n",
    "y = b0_true + b1_true * x + noise\n",
    "\n",
    "outlier_idx = rng.choice(n, size=2, replace=False)\n",
    "y[outlier_idx] += rng.normal(0, 6.0, size=2)\n",
    "\n",
    "fig = px.scatter(x=x, y=y, title=\"Synthetic regression data (with a couple outliers)\", labels={\"x\": \"x\", \"y\": \"y\"})\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x[outlier_idx],\n",
    "        y=y[outlier_idx],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"crimson\", symbol=\"x\"),\n",
    "        name=\"outliers\",\n",
    "    )\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_abs(r, eps=1e-6):\n",
    "    return np.sqrt(r * r + eps)\n",
    "\n",
    "\n",
    "def smooth_max(z, alpha=30.0):\n",
    "    # stable log-sum-exp\n",
    "    z_scaled = alpha * z\n",
    "    z_max = np.max(z_scaled)\n",
    "    return float((z_max + np.log(np.sum(np.exp(z_scaled - z_max)))) / alpha)\n",
    "\n",
    "\n",
    "def smooth_max_weights(z, alpha=30.0):\n",
    "    z_scaled = alpha * z\n",
    "    z_max = np.max(z_scaled)\n",
    "    w = np.exp(z_scaled - z_max)\n",
    "    w /= np.sum(w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def smooth_max_error_for_line(x, y, b0, b1, *, alpha=30.0, eps=1e-6):\n",
    "    y_hat = b0 + b1 * x\n",
    "    r = y - y_hat\n",
    "    e = smooth_abs(r, eps=eps)\n",
    "    return smooth_max(e, alpha=alpha)\n",
    "\n",
    "\n",
    "def smooth_max_error_gradients_for_line(x, y, b0, b1, *, alpha=30.0, eps=1e-6):\n",
    "    y_hat = b0 + b1 * x\n",
    "    r = y - y_hat\n",
    "    e = smooth_abs(r, eps=eps)\n",
    "\n",
    "    w = smooth_max_weights(e, alpha=alpha)\n",
    "    obj = smooth_max(e, alpha=alpha)\n",
    "\n",
    "    # d obj / d r_i = w_i * (r_i / e_i)\n",
    "    d_obj_dr = w * (r / e)\n",
    "\n",
    "    db0 = float(-np.sum(d_obj_dr))\n",
    "    db1 = float(-np.sum(d_obj_dr * x))\n",
    "    return float(obj), db0, db1, w\n",
    "\n",
    "\n",
    "def fit_line_minimax_gd(x, y, *, alpha=30.0, eps=1e-6, lr=0.1, n_steps=2000):\n",
    "    b0, b1 = 0.0, 0.0\n",
    "\n",
    "    history = {\n",
    "        \"step\": [],\n",
    "        \"b0\": [],\n",
    "        \"b1\": [],\n",
    "        \"smooth_obj\": [],\n",
    "        \"max_error\": [],\n",
    "    }\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        obj, db0, db1, _ = smooth_max_error_gradients_for_line(x, y, b0, b1, alpha=alpha, eps=eps)\n",
    "\n",
    "        if step % 10 == 0 or step == n_steps - 1:\n",
    "            y_hat = b0 + b1 * x\n",
    "            history[\"step\"].append(step)\n",
    "            history[\"b0\"].append(b0)\n",
    "            history[\"b1\"].append(b1)\n",
    "            history[\"smooth_obj\"].append(obj)\n",
    "            history[\"max_error\"].append(max_error_np(y, y_hat))\n",
    "\n",
    "        b0 -= lr * db0\n",
    "        b1 -= lr * db1\n",
    "\n",
    "    return (b0, b1), history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3722b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS fit (minimizes MSE)\n",
    "X = np.column_stack([np.ones_like(x), x])\n",
    "b0_ols, b1_ols = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Smooth-minimax fit (approximate max_error minimization)\n",
    "(alpha, lr, n_steps) = (40.0, 0.1, 2000)\n",
    "(b0_mm, b1_mm), hist = fit_line_minimax_gd(x, y, alpha=alpha, lr=lr, n_steps=n_steps)\n",
    "\n",
    "# Compare metrics on the training set\n",
    "\n",
    "def summarize_fit(name, b0, b1):\n",
    "    y_hat = b0 + b1 * x\n",
    "    r = y - y_hat\n",
    "    mae = float(np.mean(np.abs(r)))\n",
    "    rmse = float(np.sqrt(np.mean(r**2)))\n",
    "    mx = max_error_np(y, y_hat)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"b0\": float(b0),\n",
    "        \"b1\": float(b1),\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"max_error\": mx,\n",
    "    }\n",
    "\n",
    "rows = [\n",
    "    summarize_fit(\"OLS (MSE)\", b0_ols, b1_ols),\n",
    "    summarize_fit(f\"Smooth-minimax (alpha={alpha:g})\", b0_mm, b1_mm),\n",
    "]\n",
    "rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Smooth surrogate objective\", \"Actual max_error\"),\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=hist[\"step\"], y=hist[\"smooth_obj\"], mode=\"lines\", name=\"smooth obj\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=hist[\"step\"], y=hist[\"max_error\"], mode=\"lines\", name=\"max_error\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"step\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"step\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=2)\n",
    "fig.update_layout(title=\"Training progress\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf24d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fits + their worst-case bands\n",
    "x_line = np.linspace(x.min(), x.max(), 250)\n",
    "\n",
    "def yhat_line(b0, b1):\n",
    "    return b0 + b1 * x_line\n",
    "\n",
    "# Compute max_error (vertical band half-width) on the training points\n",
    "mx_ols = max_error_np(y, b0_ols + b1_ols * x)\n",
    "mx_mm = max_error_np(y, b0_mm + b1_mm * x)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode=\"markers\", name=\"data\", marker=dict(size=6, opacity=0.8)))\n",
    "\n",
    "# OLS line + band\n",
    "y_line_ols = yhat_line(b0_ols, b1_ols)\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_ols, mode=\"lines\", name=f\"OLS line (max={mx_ols:.2f})\", line=dict(color=\"#1f77b4\")))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.concatenate([x_line, x_line[::-1]]),\n",
    "        y=np.concatenate([y_line_ols + mx_ols, (y_line_ols - mx_ols)[::-1]]),\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(31, 119, 180, 0.12)\",\n",
    "        line=dict(color=\"rgba(31, 119, 180, 0)\"),\n",
    "        name=\"OLS band\",\n",
    "        hoverinfo=\"skip\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Minimax line + band\n",
    "y_line_mm = yhat_line(b0_mm, b1_mm)\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line_mm, mode=\"lines\", name=f\"Smooth-minimax line (max={mx_mm:.2f})\", line=dict(color=\"crimson\")))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.concatenate([x_line, x_line[::-1]]),\n",
    "        y=np.concatenate([y_line_mm + mx_mm, (y_line_mm - mx_mm)[::-1]]),\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(220, 20, 60, 0.12)\",\n",
    "        line=dict(color=\"rgba(220, 20, 60, 0)\"),\n",
    "        name=\"Minimax band\",\n",
    "        hoverinfo=\"skip\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"OLS vs minimax regression: max-error bands\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the log-sum-exp weights concentrate on large errors\n",
    "\n",
    "def weights_for_params(b0, b1, *, alpha=40.0, eps=1e-6):\n",
    "    r = y - (b0 + b1 * x)\n",
    "    e = smooth_abs(r, eps=eps)\n",
    "    w = smooth_max_weights(e, alpha=alpha)\n",
    "    return e, w\n",
    "\n",
    "# use the minimax solution\n",
    "errors, weights = weights_for_params(b0_mm, b1_mm, alpha=alpha)\n",
    "\n",
    "df = {\n",
    "    \"abs_error\": errors,\n",
    "    \"weight\": weights,\n",
    "}\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"abs_error\",\n",
    "    y=\"weight\",\n",
    "    title=f\"Softmax weights over errors (alpha={alpha:g})\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"|error| (smoothed)\", yaxis_title=\"weight (sums to 1)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f2d47",
   "metadata": {},
   "source": [
    "## 6) Pros, cons, and when to use `max_error`\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Direct worst-case control**: answers “what is the largest absolute miss?”\n",
    "- **Interpretable**: same units as $y$ and corresponds to a tight error band\n",
    "- **Useful for constraints / SLAs**: when you must keep every error under a threshold\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- **Dominated by one point**: improving many points doesn’t matter if the worst stays unchanged\n",
    "- **Outlier sensitive**: a single noisy label can dictate the score\n",
    "- **Non-smooth objective**: if you try to optimize it directly, gradients are unstable / undefined at ties and at zero\n",
    "- **May trade average accuracy for worst-case** (as seen in the minimax vs OLS example)\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- Safety- or reliability-critical regression where **worst-case** error matters more than average error\n",
    "- Systems with explicit tolerances (calibration, manufacturing, control)\n",
    "- Monitoring / evaluation alongside MAE/RMSE to catch “rare but disastrous” failures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c01c1be",
   "metadata": {},
   "source": [
    "## 7) Exercises\n",
    "\n",
    "1. Replace the log-sum-exp max with a $p$-norm approximation: $\\|e\\|_p = (\\sum_i e_i^p)^{1/p}$ and study the limit $p\\to\\infty$.\n",
    "2. Solve the exact minimax regression via linear programming and compare to the smooth approximation.\n",
    "3. Evaluate `max_error` **per group** (e.g., slices of your data) and compare worst-group vs overall worst-case.\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn: `sklearn.metrics.max_error`\n",
    "- Chebyshev (minimax) approximation / $\\ell_\\infty$ regression\n",
    "- Log-sum-exp as a smooth approximation to max\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}