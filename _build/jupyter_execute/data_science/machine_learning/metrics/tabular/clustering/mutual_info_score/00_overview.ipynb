{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ad1cd7",
   "metadata": {},
   "source": [
    "# Mutual Information Score (`mutual_info_score`)\n",
    "\n",
    "`mutual_info_score` measures the **mutual information (MI)** between two discrete labelings (often: two clusterings).\n",
    "\n",
    "Think of it as: *\"How much does knowing clustering A reduce uncertainty about clustering B?\"*\n",
    "\n",
    "**Goals**\n",
    "- Define MI using entropy and the joint distribution\n",
    "- Compute MI from a contingency table (counts)\n",
    "- Implement `mutual_info_score` from scratch in NumPy (and verify vs scikit-learn)\n",
    "- Visualize how MI behaves under label permutation, random independence, and label noise\n",
    "- Use MI as a non-differentiable objective to tune a simple logistic-regression decision threshold\n",
    "- Understand pros/cons and when to prefer NMI/AMI\n",
    "\n",
    "**Quick import (scikit-learn)**\n",
    "```python\n",
    "from sklearn.metrics import mutual_info_score\n",
    "```\n",
    "\n",
    "**Units:** scikit-learn uses the natural log, so MI is measured in **nats** (use `log2` for bits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import (\n",
    "    adjusted_mutual_info_score,\n",
    "    mutual_info_score,\n",
    "    normalized_mutual_info_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d32c5",
   "metadata": {},
   "source": [
    "## 1) What does `mutual_info_score` measure?\n",
    "\n",
    "We observe two label arrays of length $n$:\n",
    "\n",
    "- `labels_true`: a partition $U$ (e.g., ground-truth classes or clustering A)\n",
    "- `labels_pred`: a partition $V$ (e.g., predicted clusters or clustering B)\n",
    "\n",
    "Mutual information $I(U;V)$ is:\n",
    "\n",
    "- **0** when $U$ and $V$ are independent (knowing one tells you nothing about the other)\n",
    "- **large** when $U$ and $V$ are strongly dependent (knowing one reveals a lot about the other)\n",
    "\n",
    "In clustering, MI is popular because it is **permutation-invariant**: renaming clusters (0 ↔ 2, etc.) does not change the score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce08af8",
   "metadata": {},
   "source": [
    "## 2) Mutual information for discrete variables\n",
    "\n",
    "Let $U$ and $V$ be discrete random variables with joint pmf $p(u,v)$ and marginals $p(u)$, $p(v)$.\n",
    "\n",
    "### Entropy (uncertainty)\n",
    "\n",
    "$$\n",
    "H(U) = -\\sum_u p(u)\\,\\log p(u)\n",
    "$$\n",
    "\n",
    "### Mutual information (shared information)\n",
    "\n",
    "Equivalent definitions:\n",
    "\n",
    "$$\n",
    "I(U;V)\n",
    "= \\sum_{u}\\sum_{v} p(u,v)\\,\\log\\frac{p(u,v)}{p(u)p(v)}\n",
    "= H(U) + H(V) - H(U,V)\n",
    "= H(U) - H(U\\mid V)\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- $p(u)p(v)$ is what the joint would look like **if $U$ and $V$ were independent**\n",
    "- the log-ratio compares **observed** co-occurrence vs **independence**\n",
    "- cells where $p(u,v) > p(u)p(v)$ contribute positively; cells where $p(u,v) < p(u)p(v)$ contribute negatively, but the total $I(U;V)\\ge 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c33dc",
   "metadata": {},
   "source": [
    "## 3) From label arrays to MI: the contingency matrix\n",
    "\n",
    "Given $n$ paired observations $(u_i, v_i)$, define the contingency table counts:\n",
    "\n",
    "- $n_{ij}$ = number of samples with $u=i$ and $v=j$\n",
    "- $n_{i*} = \\sum_j n_{ij}$ (row sums)\n",
    "- $n_{*j} = \\sum_i n_{ij}$ (column sums)\n",
    "- $n = \\sum_{i,j} n_{ij}$ (total)\n",
    "\n",
    "A common \"plug-in\" estimator replaces probabilities by empirical frequencies:\n",
    "\n",
    "$$\n",
    "\\widehat{I}(U;V)\n",
    "= \\sum_{i}\\sum_{j} \\frac{n_{ij}}{n}\\,\\log\\left( \\frac{n\\,n_{ij}}{n_{i*}n_{*j}} \\right)\n",
    "$$\n",
    "\n",
    "By convention, terms with $n_{ij}=0$ contribute $0$ (since $0\\log 0 := 0$).\n",
    "\n",
    "This is what `sklearn.metrics.mutual_info_score` computes (using the natural log).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98334958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_matrix_numpy(labels_true, labels_pred):\n",
    "    \"\"\"Build the contingency matrix N where N[i, j] counts samples with\n",
    "    true label i and predicted label j (after re-indexing labels to 0..K-1).\n",
    "    \"\"\"\n",
    "    labels_true = np.asarray(labels_true)\n",
    "    labels_pred = np.asarray(labels_pred)\n",
    "    if labels_true.shape[0] != labels_pred.shape[0]:\n",
    "        raise ValueError(\"labels_true and labels_pred must have the same length\")\n",
    "\n",
    "    true_values, true_inv = np.unique(labels_true, return_inverse=True)\n",
    "    pred_values, pred_inv = np.unique(labels_pred, return_inverse=True)\n",
    "\n",
    "    n_true = true_values.shape[0]\n",
    "    n_pred = pred_values.shape[0]\n",
    "\n",
    "    flat = true_inv * n_pred + pred_inv\n",
    "    counts = np.bincount(flat, minlength=n_true * n_pred)\n",
    "    contingency = counts.reshape(n_true, n_pred)\n",
    "\n",
    "    return contingency, true_values, pred_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_score_numpy(labels_true, labels_pred):\n",
    "    \"\"\"NumPy implementation of mutual information between two labelings.\n",
    "\n",
    "    Matches sklearn.metrics.mutual_info_score (natural log, result in nats).\n",
    "    \"\"\"\n",
    "    contingency, _, _ = contingency_matrix_numpy(labels_true, labels_pred)\n",
    "    contingency = contingency.astype(float)\n",
    "\n",
    "    n = contingency.sum()\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    row_sum = contingency.sum(axis=1)\n",
    "    col_sum = contingency.sum(axis=0)\n",
    "\n",
    "    i_idx, j_idx = np.nonzero(contingency)\n",
    "    nij = contingency[i_idx, j_idx]\n",
    "\n",
    "    mi = (nij / n) * np.log((nij * n) / (row_sum[i_idx] * col_sum[j_idx]))\n",
    "    return float(mi.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification vs scikit-learn\n",
    "for k_true, k_pred in [(3, 3), (4, 2), (10, 10)]:\n",
    "    labels_true_rand = rng.integers(0, k_true, size=500)\n",
    "    labels_pred_rand = rng.integers(0, k_pred, size=500)\n",
    "\n",
    "    mi_sklearn = mutual_info_score(labels_true_rand, labels_pred_rand)\n",
    "    mi_numpy = mutual_info_score_numpy(labels_true_rand, labels_pred_rand)\n",
    "\n",
    "    print(\n",
    "        f\"k_true={k_true:>2}, k_pred={k_pred:>2} | sklearn={mi_sklearn:.6f} numpy={mi_numpy:.6f} | diff={abs(mi_sklearn - mi_numpy):.2e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c8830",
   "metadata": {},
   "source": [
    "## 4) Intuition: permutation, independence, and noise\n",
    "\n",
    "We'll build a small toy labeling with 3 clusters and compare three predicted labelings:\n",
    "\n",
    "1. **Perfect but permuted**: same partition, different label ids (MI should be maximal)\n",
    "2. **Independent random labels**: no relationship (MI near 0)\n",
    "3. **Noisy labels**: flip some fraction of labels at random (MI decreases with noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_cluster = 150\n",
    "labels_true = np.repeat(np.arange(3), n_per_cluster)\n",
    "rng.shuffle(labels_true)\n",
    "\n",
    "# 1) perfect but permuted\n",
    "perm = np.array([2, 0, 1])\n",
    "labels_perm = perm[labels_true]\n",
    "\n",
    "# 2) independent random labels (same number of clusters)\n",
    "labels_random = rng.integers(0, 3, size=labels_true.shape[0])\n",
    "\n",
    "\n",
    "# 3) noisy labels: with prob p, replace by a random label\n",
    "def add_label_noise(labels, n_labels, p_noise, rng):\n",
    "    labels = np.asarray(labels).copy()\n",
    "    mask = rng.random(labels.shape[0]) < p_noise\n",
    "    labels[mask] = rng.integers(0, n_labels, size=mask.sum())\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels_noisy = add_label_noise(labels_true, n_labels=3, p_noise=0.3, rng=rng)\n",
    "\n",
    "for name, lab in [\n",
    "    (\"perfect (permuted)\", labels_perm),\n",
    "    (\"random (independent)\", labels_random),\n",
    "    (\"noisy (p=0.3)\", labels_noisy),\n",
    "]:\n",
    "    print(f\"{name:>18}: MI={mutual_info_score_numpy(labels_true, lab):.4f} nats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88552775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_heatmap_trace(contingency, true_vals, pred_vals, showscale=False):\n",
    "    contingency = np.asarray(contingency)\n",
    "    return go.Heatmap(\n",
    "        z=contingency,\n",
    "        x=[str(v) for v in pred_vals],\n",
    "        y=[str(v) for v in true_vals],\n",
    "        colorscale=\"Blues\",\n",
    "        showscale=showscale,\n",
    "        text=contingency,\n",
    "        texttemplate=\"%{text}\",\n",
    "        hovertemplate=\"true=%{y}<br>pred=%{x}<br>count=%{z}<extra></extra>\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d86bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_perm, true_vals, pred_perm_vals = contingency_matrix_numpy(labels_true, labels_perm)\n",
    "cont_rand, _, pred_rand_vals = contingency_matrix_numpy(labels_true, labels_random)\n",
    "cont_noisy, _, pred_noisy_vals = contingency_matrix_numpy(labels_true, labels_noisy)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=[\n",
    "        \"Perfect (permuted)\",\n",
    "        \"Random (independent)\",\n",
    "        \"Noisy (p=0.3)\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.add_trace(contingency_heatmap_trace(cont_perm, true_vals, pred_perm_vals), row=1, col=1)\n",
    "fig.add_trace(contingency_heatmap_trace(cont_rand, true_vals, pred_rand_vals), row=1, col=2)\n",
    "fig.add_trace(contingency_heatmap_trace(cont_noisy, true_vals, pred_noisy_vals), row=1, col=3)\n",
    "\n",
    "fig.update_xaxes(title_text=\"pred label\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"pred label\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"pred label\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"true label\", row=1, col=1)\n",
    "\n",
    "fig.update_layout(title=\"Contingency tables (counts)\", height=350)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2057f0",
   "metadata": {},
   "source": [
    "A quick read:\n",
    "\n",
    "- A near **diagonal** contingency table means each true cluster mostly maps to one predicted cluster ⇒ high MI.\n",
    "- A near **uniform** table means predicted labels don't depend on true labels ⇒ MI near 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.linspace(0.0, 1.0, 41)\n",
    "n_repeats = 30\n",
    "\n",
    "mi_means = []\n",
    "mi_stds = []\n",
    "\n",
    "for p_noise in ps:\n",
    "    vals = []\n",
    "    for _ in range(n_repeats):\n",
    "        lab = add_label_noise(labels_true, n_labels=3, p_noise=float(p_noise), rng=rng)\n",
    "        vals.append(mutual_info_score_numpy(labels_true, lab))\n",
    "    vals = np.array(vals)\n",
    "    mi_means.append(vals.mean())\n",
    "    mi_stds.append(vals.std())\n",
    "\n",
    "mi_means = np.array(mi_means)\n",
    "mi_stds = np.array(mi_stds)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=ps, y=mi_means, mode=\"lines\", name=\"mean MI\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.r_[ps, ps[::-1]],\n",
    "        y=np.r_[mi_means - mi_stds, (mi_means + mi_stds)[::-1]],\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(0,0,0,0.10)\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        hoverinfo=\"skip\",\n",
    "        name=\"±1 std\",\n",
    "    )\n",
    ")\n",
    "\n",
    "max_mi = np.log(3)\n",
    "fig.add_hline(y=max_mi, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"log(3) max\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Mutual information vs label noise\",\n",
    "    xaxis_title=\"noise probability p (replace label with random label)\",\n",
    "    yaxis_title=\"MI (nats)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28191512",
   "metadata": {},
   "source": [
    "## 5) What MI is \"adding up\": observed vs expected co-occurrences\n",
    "\n",
    "Each contingency cell $(i,j)$ compares:\n",
    "\n",
    "- observed count: $n_{ij}$\n",
    "- expected count under independence: $\\frac{n_{i*} n_{*j}}{n}$\n",
    "\n",
    "The per-cell contribution is:\n",
    "\n",
    "$$\n",
    "\\frac{n_{ij}}{n}\\log\\left(\\frac{n_{ij}}{\\frac{n_{i*}n_{*j}}{n}}\\right)\n",
    "$$\n",
    "\n",
    "Let's visualize those contributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_contribution_matrix(contingency):\n",
    "    contingency = np.asarray(contingency, dtype=float)\n",
    "    n = contingency.sum()\n",
    "    if n == 0:\n",
    "        return np.zeros_like(contingency, dtype=float)\n",
    "\n",
    "    row_sum = contingency.sum(axis=1, keepdims=True)\n",
    "    col_sum = contingency.sum(axis=0, keepdims=True)\n",
    "    denom = row_sum * col_sum\n",
    "\n",
    "    ratio = np.ones_like(contingency, dtype=float)\n",
    "    np.divide(contingency * n, denom, out=ratio, where=(contingency > 0))\n",
    "\n",
    "    contrib = np.zeros_like(contingency, dtype=float)\n",
    "    mask = contingency > 0\n",
    "    contrib[mask] = (contingency[mask] / n) * np.log(ratio[mask])\n",
    "    return contrib\n",
    "\n",
    "\n",
    "contrib_noisy = mi_contribution_matrix(cont_noisy)\n",
    "\n",
    "fig = px.imshow(\n",
    "    contrib_noisy,\n",
    "    text_auto=\".3f\",\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0.0,\n",
    "    labels=dict(x=\"pred label\", y=\"true label\", color=\"contrib (nats)\"),\n",
    "    x=[str(v) for v in pred_noisy_vals],\n",
    "    y=[str(v) for v in true_vals],\n",
    "    title=\"Per-cell MI contributions (noisy example, p=0.3)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"Sum of contributions = {contrib_noisy.sum():.4f} nats (should equal MI)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f694f",
   "metadata": {},
   "source": [
    "## 6) Using MI in a simple optimization loop: tune a logistic-regression threshold\n",
    "\n",
    "`mutual_info_score` needs **discrete labels**. For a probabilistic classifier (like logistic regression) we usually have scores/probabilities $\\hat{p}(x)$.\n",
    "\n",
    "A common way to use MI is as a **validation objective** over a non-differentiable choice, e.g. the decision threshold $\\tau$:\n",
    "\n",
    "$$\n",
    "\\hat{y}(\\tau) = \\mathbb{1}[\\hat{p}(x) \\ge \\tau],\n",
    "\\qquad\n",
    "\\tau^* = \\arg\\max_{\\tau\\in[0,1]} I(Y;\\hat{Y}(\\tau)).\n",
    "$$\n",
    "\n",
    "We'll:\n",
    "1. fit logistic regression from scratch (gradient descent on log-loss),\n",
    "2. sweep thresholds on a validation set,\n",
    "3. pick the threshold that maximizes MI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1600,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.8, 0.2],\n",
    "    class_sep=1.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.35, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "X_train_s = (X_train - mean) / std\n",
    "X_val_s = (X_val - mean) / std\n",
    "\n",
    "# add intercept column\n",
    "X_train_b = np.c_[np.ones(X_train_s.shape[0]), X_train_s]\n",
    "X_val_b = np.c_[np.ones(X_val_s.shape[0]), X_val_s]\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_val_s[:, 0],\n",
    "    y=X_val_s[:, 1],\n",
    "    color=y_val.astype(int),\n",
    "    title=\"Validation data (standardized features)\",\n",
    "    labels={\"x\": \"x1 (standardized)\", \"y\": \"x2 (standardized)\", \"color\": \"class\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip(z, -50, 50)  # numerical stability\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def fit_logistic_regression_gd(X, y, lr=0.2, n_iter=2500, l2=0.0):\n",
    "    \"\"\"Binary logistic regression with gradient descent on log-loss.\n",
    "\n",
    "    X: (n, d) including intercept column if desired.\n",
    "    y: (n,) in {0,1}.\n",
    "    l2: L2 regularization strength (applied to weights except intercept).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d, dtype=float)\n",
    "\n",
    "    eps = 1e-12\n",
    "    losses = np.empty(n_iter, dtype=float)\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        p = sigmoid(X @ w)\n",
    "\n",
    "        loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
    "        loss += 0.5 * l2 * np.sum(w[1:] ** 2)\n",
    "        losses[t] = loss\n",
    "\n",
    "        grad = (X.T @ (p - y)) / n\n",
    "        grad[1:] += l2 * w[1:]\n",
    "\n",
    "        w -= lr * grad\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "\n",
    "w, losses = fit_logistic_regression_gd(X_train_b, y_train, lr=0.2, n_iter=2500, l2=0.1)\n",
    "print(\"w =\", w)\n",
    "\n",
    "fig = px.line(\n",
    "    y=losses,\n",
    "    title=\"Training curve (log-loss)\",\n",
    "    labels={\"x\": \"iteration\", \"y\": \"log-loss\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    return float(np.mean(y_true == y_pred))\n",
    "\n",
    "\n",
    "def confusion_counts(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "\n",
    "p_val = sigmoid(X_val_b @ w)\n",
    "\n",
    "taus = np.linspace(0.01, 0.99, 99)\n",
    "mi_vals = np.empty_like(taus)\n",
    "acc_vals = np.empty_like(taus)\n",
    "\n",
    "for i, tau in enumerate(taus):\n",
    "    y_pred_tau = (p_val >= tau).astype(int)\n",
    "    mi_vals[i] = mutual_info_score_numpy(y_val, y_pred_tau)\n",
    "    acc_vals[i] = accuracy(y_val, y_pred_tau)\n",
    "\n",
    "best_i = int(np.argmax(mi_vals))\n",
    "best_tau = float(taus[best_i])\n",
    "\n",
    "y_pred_best = (p_val >= best_tau).astype(int)\n",
    "y_pred_05 = (p_val >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Best threshold (by MI): tau* = {best_tau:.2f}\")\n",
    "print(f\"MI(tau*)  = {mutual_info_score_numpy(y_val, y_pred_best):.4f} nats\")\n",
    "print(f\"MI(0.50)  = {mutual_info_score_numpy(y_val, y_pred_05):.4f} nats\")\n",
    "print(f\"Acc(tau*) = {accuracy(y_val, y_pred_best):.3f}\")\n",
    "print(f\"Acc(0.50) = {accuracy(y_val, y_pred_05):.3f}\")\n",
    "\n",
    "tp, fp, tn, fn = confusion_counts(y_val, y_pred_best)\n",
    "print(f\"Confusion@tau*: TP={tp} FP={fp} TN={tn} FN={fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=taus, y=mi_vals, mode=\"lines\", name=\"MI (nats)\"))\n",
    "fig.add_trace(go.Scatter(x=taus, y=acc_vals, mode=\"lines\", name=\"Accuracy\", yaxis=\"y2\"))\n",
    "\n",
    "fig.add_vline(x=best_tau, line_dash=\"dash\", line_color=\"black\", annotation_text=\"tau*\")\n",
    "fig.add_vline(x=0.5, line_dash=\"dot\", line_color=\"gray\", annotation_text=\"0.5\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Threshold sweep on validation set\",\n",
    "    xaxis_title=\"threshold tau\",\n",
    "    yaxis=dict(title=\"MI (nats)\"),\n",
    "    yaxis2=dict(title=\"Accuracy\", overlaying=\"y\", side=\"right\", range=[0, 1]),\n",
    "    legend=dict(x=0.01, y=0.99),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8840d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    p = np.clip(p, 1e-12, 1 - 1e-12)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "\n",
    "def decision_boundary_x2(w, tau, x1_grid):\n",
    "    # w0 + w1*x1 + w2*x2 = logit(tau)\n",
    "    w0, w1, w2 = w\n",
    "    if abs(w2) < 1e-12:\n",
    "        raise ValueError(\"w2 is ~0; boundary would be vertical in (x1,x2)\")\n",
    "    return (logit(tau) - w0 - w1 * x1_grid) / w2\n",
    "\n",
    "\n",
    "x1_min, x1_max = X_val_s[:, 0].min() - 0.5, X_val_s[:, 0].max() + 0.5\n",
    "x1_grid = np.linspace(x1_min, x1_max, 200)\n",
    "\n",
    "x2_tau05 = decision_boundary_x2(w, tau=0.5, x1_grid=x1_grid)\n",
    "x2_best = decision_boundary_x2(w, tau=best_tau, x1_grid=x1_grid)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_val_s[:, 0],\n",
    "        y=X_val_s[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            color=y_val.astype(int),\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"class\"),\n",
    "            size=6,\n",
    "            opacity=0.8,\n",
    "        ),\n",
    "        name=\"val points\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x1_grid,\n",
    "        y=x2_tau05,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", dash=\"dot\", width=3),\n",
    "        name=\"boundary (tau=0.5)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x1_grid,\n",
    "        y=x2_best,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"crimson\", dash=\"dash\", width=3),\n",
    "        name=f\"boundary (tau*={best_tau:.2f})\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Decision boundary shifts when you change the threshold\",\n",
    "    xaxis_title=\"x1 (standardized)\",\n",
    "    yaxis_title=\"x2 (standardized)\",\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b1606",
   "metadata": {},
   "source": [
    "### Bonus: MI is a non-smooth objective (toy grid search)\n",
    "\n",
    "If we hard-threshold predictions, MI changes only when the decision boundary flips some points.\n",
    "That makes $I(Y;\\hat{Y})$ a **piecewise-constant** function of model parameters.\n",
    "\n",
    "To make this visible, we'll take a **1D linear classifier** on the validation set:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbb{1}[w_0 + w_1 x_1 \\ge 0]\n",
    "$$\n",
    "\n",
    "and grid-search $(w_0, w_1)$ to maximize MI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe91d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_val_s[:, 0]\n",
    "y_bin = y_val.astype(int)\n",
    "\n",
    "w0_grid = np.linspace(-4.0, 4.0, 81)\n",
    "w1_grid = np.linspace(-4.0, 4.0, 81)\n",
    "\n",
    "mi_grid = np.empty((w0_grid.size, w1_grid.size), dtype=float)\n",
    "\n",
    "for i, w0 in enumerate(w0_grid):\n",
    "    for j, w1 in enumerate(w1_grid):\n",
    "        y_pred = (w0 + w1 * x1 >= 0).astype(int)  # tau=0.5 boundary (logit=0)\n",
    "        mi_grid[i, j] = mutual_info_score_numpy(y_bin, y_pred)\n",
    "\n",
    "best_idx = np.unravel_index(np.argmax(mi_grid), mi_grid.shape)\n",
    "w0_best = float(w0_grid[best_idx[0]])\n",
    "w1_best = float(w1_grid[best_idx[1]])\n",
    "\n",
    "print(\n",
    "    f\"Best (w0,w1) on this grid: ({w0_best:.2f}, {w1_best:.2f}) | MI={mi_grid[best_idx]:.4f} nats\"\n",
    ")\n",
    "\n",
    "fig = px.imshow(\n",
    "    mi_grid,\n",
    "    x=w1_grid,\n",
    "    y=w0_grid,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    labels=dict(x=\"w1\", y=\"w0\", color=\"MI (nats)\"),\n",
    "    title=\"MI landscape for a 1D hard-threshold classifier (piecewise constant)\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=[w1_best],\n",
    "    y=[w0_best],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(color=\"red\", size=10),\n",
    "    name=\"best\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558a357",
   "metadata": {},
   "source": [
    "**Takeaway:** MI can be used to tune *discrete* choices (thresholds, model selection, number of clusters) via search.\n",
    "With hard predictions it becomes step-like (non-smooth) in typical model parameters, so it is rarely used as a direct gradient-based training loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e1f3f",
   "metadata": {},
   "source": [
    "## 7) Pitfalls (and why NMI/AMI exist)\n",
    "\n",
    "Two common gotchas:\n",
    "\n",
    "1. **MI is not normalized**: values depend on the entropies of the labelings (number of clusters and their balance). Comparing MI across datasets can be misleading.\n",
    "2. **MI does not penalize over-segmentation**: a clustering with many tiny clusters can still have a high MI with the ground truth.\n",
    "\n",
    "Let's see the second pitfall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true_small = labels_true\n",
    "labels_unique = np.arange(labels_true_small.shape[0])  # each sample its own cluster\n",
    "\n",
    "for name, pred in [\n",
    "    (\"perfect (permuted)\", labels_perm),\n",
    "    (\"unique cluster per sample\", labels_unique),\n",
    "    (\"random labels (3 clusters)\", labels_random),\n",
    "]:\n",
    "    mi = mutual_info_score(labels_true_small, pred)\n",
    "    nmi = normalized_mutual_info_score(labels_true_small, pred)\n",
    "    ami = adjusted_mutual_info_score(labels_true_small, pred)\n",
    "\n",
    "    print(f\"{name:>24}: MI={mi:.4f} | NMI={nmi:.4f} | AMI={ami:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa357a23",
   "metadata": {},
   "source": [
    "## 8) Pros, cons, and where MI shines\n",
    "\n",
    "**Pros**\n",
    "- **Permutation-invariant**: relabeling clusters doesn't change the score.\n",
    "- **Captures general dependence**: not limited to linear relationships (for discrete variables).\n",
    "- **Uses full contingency table**: considers all overlaps, not only \"matching\" pairs.\n",
    "\n",
    "**Cons / caveats**\n",
    "- **Unbounded / not directly interpretable across problems** (depends on label entropies).\n",
    "- **Can reward over-segmentation** (many clusters) → consider `normalized_mutual_info_score` or `adjusted_mutual_info_score`.\n",
    "- **Requires discrete variables**: continuous features need discretization or different MI estimators.\n",
    "- **Non-differentiable for hard labels**: not a convenient loss for gradient-based training; better as a validation metric / search objective.\n",
    "\n",
    "**Common use cases**\n",
    "- External **clustering evaluation** when ground truth labels exist.\n",
    "- Comparing two different clusterings/segmentations of the same data.\n",
    "- **Feature selection** (MI between a discrete target and a discretized feature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16bbe2",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement **normalized mutual information** (NMI) from scratch using entropies.\n",
    "2. Show how MI changes when clusters are highly imbalanced (e.g., 95/5 split).\n",
    "3. Use MI to select the best of several clustering hyperparameters (e.g., choose $k$ in k-means when ground truth exists).\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn: `sklearn.metrics.mutual_info_score`\n",
    "- Cover & Thomas, *Elements of Information Theory* (entropy, mutual information)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}