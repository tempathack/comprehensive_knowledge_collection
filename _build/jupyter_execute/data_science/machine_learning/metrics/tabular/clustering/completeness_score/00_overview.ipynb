{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414cd0b3",
   "metadata": {},
   "source": [
    "# Completeness score (clustering)\n",
    "\n",
    "Completeness answers:\n",
    "\n",
    "> “Do all samples of each true class end up in the same predicted cluster?”\n",
    "\n",
    "- If a class is **split** across multiple clusters → completeness goes down.\n",
    "- If multiple classes are **merged** into one cluster → completeness can stay high.\n",
    "\n",
    "This is why completeness is typically reported together with **homogeneity** (or combined as **V-measure**).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- interpret completeness in plain language\n",
    "- derive it from entropy / conditional entropy\n",
    "- implement it from scratch in NumPy\n",
    "- visualize how different labelings affect the score\n",
    "- use it to tune a simple clustering model (with a caveat)\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import completeness_score\n",
    "```\n",
    "\n",
    "## Notation (quick)\n",
    "\n",
    "- True class labels: $C$ (a categorical variable)\n",
    "- Predicted cluster labels: $K$ (a categorical variable)\n",
    "- $n$ = number of samples\n",
    "- Contingency table: $n_{ck}$ = #samples of class $c$ assigned to cluster $k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677164b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.metrics import completeness_score as skl_completeness_score\n",
    "from sklearn.metrics import homogeneity_score as skl_homogeneity_score\n",
    "from sklearn.metrics import v_measure_score as skl_v_measure_score\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be4ff4",
   "metadata": {},
   "source": [
    "## 1) Intuition: completeness cares about splitting classes\n",
    "\n",
    "A good clustering is often described by two complementary properties:\n",
    "\n",
    "- **Homogeneity**: each cluster contains (mostly) one class.\n",
    "- **Completeness**: each class is contained (mostly) in one cluster.\n",
    "\n",
    "Completeness ignores cluster names: relabeling clusters (e.g. swapping cluster IDs 0 and 1) does **not** change the score.\n",
    "\n",
    "A useful diagnostic is the **contingency table** (a.k.a. class–cluster confusion matrix): rows = true classes, columns = predicted clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90faca96",
   "metadata": {},
   "source": [
    "## 2) The math (entropy view)\n",
    "\n",
    "Let $n_{ck}$ be the contingency table and $n$ the total number of samples.\n",
    "\n",
    "Define probabilities:\n",
    "\n",
    "- $p(c,k) = n_{ck}/n$\n",
    "- $p(c) = n_c/n$ where $n_c = \\sum_k n_{ck}$\n",
    "- $p(k) = n_k/n$ where $n_k = \\sum_c n_{ck}$\n",
    "\n",
    "Entropy of the cluster labeling:\n",
    "\n",
    "$$\n",
    "H(K) = -\\sum_{k} p(k)\\log p(k)\n",
    "$$\n",
    "\n",
    "Conditional entropy of clusters given the true classes:\n",
    "\n",
    "$$\n",
    "H(K\\mid C) = -\\sum_{c} p(c) \\sum_k p(k\\mid c)\\log p(k\\mid c)\n",
    "$$\n",
    "\n",
    "with $p(k\\mid c) = n_{ck}/n_c$.\n",
    "\n",
    "**Completeness** is:\n",
    "\n",
    "$$\n",
    "\\text{completeness}(C,K) =\n",
    "\\begin{cases}\n",
    "1 - \\frac{H(K\\mid C)}{H(K)} & \\text{if } H(K) > 0 \\\\\n",
    "1 & \\text{if } H(K)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The log base does not matter (it cancels in the ratio).\n",
    "- $H(K)=0$ happens when all samples are put in a single cluster (a degenerate but “complete” labeling).\n",
    "\n",
    "Since mutual information is $I(C;K) = H(K) - H(K\\mid C)$, you can also write:\n",
    "\n",
    "$$\n",
    "\\text{completeness}(C,K) = \\frac{I(C;K)}{H(K)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc245d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_matrix_np(y_true, y_pred):\n",
    "    \"\"\"Contingency table n_{ck} with rows=classes, cols=clusters.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(\"y_true and y_pred must have the same length\")\n",
    "    if y_true.ndim != 1 or y_pred.ndim != 1:\n",
    "        raise ValueError(\"y_true and y_pred must be 1D arrays\")\n",
    "\n",
    "    classes, class_idx = np.unique(y_true, return_inverse=True)\n",
    "    clusters, cluster_idx = np.unique(y_pred, return_inverse=True)\n",
    "\n",
    "    contingency = np.zeros((classes.size, clusters.size), dtype=np.int64)\n",
    "    np.add.at(contingency, (class_idx, cluster_idx), 1)\n",
    "    return contingency, classes, clusters\n",
    "\n",
    "\n",
    "def entropy_from_counts(counts):\n",
    "    \"\"\"Entropy H(X) from counts (natural log).\"\"\"\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    total = counts.sum()\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    probs = counts[counts > 0] / total\n",
    "    return float(-np.sum(probs * np.log(probs)))\n",
    "\n",
    "\n",
    "def conditional_entropy_from_contingency(contingency, *, given=\"rows\"):\n",
    "    \"\"\"Conditional entropy from a contingency table.\n",
    "\n",
    "    - given=\"rows\": H(columns | rows)   -> H(K | C)\n",
    "    - given=\"cols\": H(rows | columns)  -> H(C | K)\n",
    "    \"\"\"\n",
    "    contingency = np.asarray(contingency, dtype=float)\n",
    "    n = contingency.sum()\n",
    "    if n <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    if given == \"rows\":\n",
    "        weights = contingency.sum(axis=1)\n",
    "        entropies = [entropy_from_counts(row) for row in contingency]\n",
    "    elif given == \"cols\":\n",
    "        weights = contingency.sum(axis=0)\n",
    "        entropies = [entropy_from_counts(col) for col in contingency.T]\n",
    "    else:\n",
    "        raise ValueError(\"given must be 'rows' or 'cols'\")\n",
    "\n",
    "    weights = weights / n\n",
    "    return float(np.sum(weights * np.array(entropies)))\n",
    "\n",
    "\n",
    "def completeness_score_np(y_true, y_pred):\n",
    "    contingency, _, _ = contingency_matrix_np(y_true, y_pred)\n",
    "\n",
    "    h_k = entropy_from_counts(contingency.sum(axis=0))\n",
    "    if h_k == 0.0:\n",
    "        return 1.0\n",
    "\n",
    "    h_k_given_c = conditional_entropy_from_contingency(contingency, given=\"rows\")\n",
    "    return 1.0 - (h_k_given_c / h_k)\n",
    "\n",
    "\n",
    "def homogeneity_score_np(y_true, y_pred):\n",
    "    contingency, _, _ = contingency_matrix_np(y_true, y_pred)\n",
    "\n",
    "    h_c = entropy_from_counts(contingency.sum(axis=1))\n",
    "    if h_c == 0.0:\n",
    "        return 1.0\n",
    "\n",
    "    h_c_given_k = conditional_entropy_from_contingency(contingency, given=\"cols\")\n",
    "    return 1.0 - (h_c_given_k / h_c)\n",
    "\n",
    "\n",
    "def v_measure_score_np(y_true, y_pred, *, beta=1.0):\n",
    "    \"\"\"V-measure = harmonic mean of homogeneity and completeness.\"\"\"\n",
    "    h = homogeneity_score_np(y_true, y_pred)\n",
    "    c = completeness_score_np(y_true, y_pred)\n",
    "\n",
    "    denom = beta * h + c\n",
    "    if denom == 0.0:\n",
    "        return 0.0\n",
    "    return (1.0 + beta) * h * c / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9dd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy labelings to build intuition\n",
    "y_true = np.repeat([0, 1, 2], repeats=50)\n",
    "\n",
    "# A) Perfect clustering, but cluster IDs are permuted\n",
    "y_pred_permuted = np.repeat([1, 0, 2], repeats=50)\n",
    "\n",
    "# B) Split class 0 across two clusters (hurts completeness)\n",
    "y_pred_split = np.concatenate(\n",
    "    [np.repeat([0, 1], repeats=[25, 25]), np.repeat(2, 50), np.repeat(3, 50)]\n",
    ")\n",
    "\n",
    "# C) Put everything into one cluster (degenerate, but completeness==1 by convention)\n",
    "y_pred_one_cluster = np.zeros_like(y_true)\n",
    "\n",
    "cases = {\n",
    "    \"A) permuted (perfect)\": y_pred_permuted,\n",
    "    \"B) split one class\": y_pred_split,\n",
    "    \"C) one cluster\": y_pred_one_cluster,\n",
    "}\n",
    "\n",
    "for name, y_pred in cases.items():\n",
    "    c_np = completeness_score_np(y_true, y_pred)\n",
    "    c_skl = skl_completeness_score(y_true, y_pred)\n",
    "    h_np = homogeneity_score_np(y_true, y_pred)\n",
    "    h_skl = skl_homogeneity_score(y_true, y_pred)\n",
    "    v_np = v_measure_score_np(y_true, y_pred)\n",
    "    v_skl = skl_v_measure_score(y_true, y_pred)\n",
    "\n",
    "    print(name)\n",
    "    print(f\"  completeness  numpy={c_np:.4f}  sklearn={c_skl:.4f}\")\n",
    "    print(f\"  homogeneity   numpy={h_np:.4f}  sklearn={h_skl:.4f}\")\n",
    "    print(f\"  v-measure     numpy={v_np:.4f}  sklearn={v_skl:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9486f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contingency_heatmap(y_true, y_pred, *, title):\n",
    "    contingency, classes, clusters = contingency_matrix_np(y_true, y_pred)\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=contingency,\n",
    "            x=[str(k) for k in clusters],\n",
    "            y=[str(c) for c in classes],\n",
    "            colorscale=\"Blues\",\n",
    "            showscale=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(classes):\n",
    "        for j, k in enumerate(clusters):\n",
    "            fig.add_annotation(\n",
    "                x=str(k),\n",
    "                y=str(c),\n",
    "                text=str(int(contingency[i, j])),\n",
    "                showarrow=False,\n",
    "                font=dict(color=\"black\"),\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"predicted cluster\",\n",
    "        yaxis_title=\"true class\",\n",
    "        margin=dict(l=40, r=40, t=70, b=40),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "for name, y_pred in cases.items():\n",
    "    c = completeness_score_np(y_true, y_pred)\n",
    "    fig = plot_contingency_heatmap(y_true, y_pred, title=f\"{name} (completeness={c:.3f})\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the score reacts when one class is gradually split\n",
    "\n",
    "n0, n1 = 200, 200\n",
    "y_true_2 = np.array([0] * n0 + [1] * n1)\n",
    "base_pred = np.array([0] * n0 + [1] * n1)\n",
    "\n",
    "fractions = np.linspace(0.0, 1.0, 41)\n",
    "completenesses = []\n",
    "homogeneities = []\n",
    "v_measures = []\n",
    "\n",
    "for f in fractions:\n",
    "    y_pred = base_pred.copy()\n",
    "    move = int(round(f * n0))\n",
    "    idx = np.arange(n0)\n",
    "    y_pred[idx[:move]] = 1  # move some of class 0 into cluster 1\n",
    "\n",
    "    completenesses.append(completeness_score_np(y_true_2, y_pred))\n",
    "    homogeneities.append(homogeneity_score_np(y_true_2, y_pred))\n",
    "    v_measures.append(v_measure_score_np(y_true_2, y_pred))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fractions, y=completenesses, mode=\"lines+markers\", name=\"completeness\"))\n",
    "fig.add_trace(go.Scatter(x=fractions, y=homogeneities, mode=\"lines+markers\", name=\"homogeneity\"))\n",
    "fig.add_trace(go.Scatter(x=fractions, y=v_measures, mode=\"lines+markers\", name=\"v-measure\"))\n",
    "fig.update_layout(\n",
    "    title=\"Splitting one class across clusters: completeness vs homogeneity\",\n",
    "    xaxis_title=\"fraction of class 0 moved into the other cluster\",\n",
    "    yaxis_title=\"score\",\n",
    "    yaxis=dict(range=[-0.02, 1.02]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a2bc7",
   "metadata": {},
   "source": [
    "### What the plot should make you notice\n",
    "\n",
    "- When you start moving part of class 0 into another cluster, that class becomes **split** → completeness drops.\n",
    "- As you move *all* of class 0 into the other cluster, you end up with **one effective cluster** → completeness returns to 1.\n",
    "\n",
    "So **completeness alone** can be maximized by collapsing everything into a single cluster.\n",
    "\n",
    "That’s why you typically report completeness together with homogeneity (or V-measure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cce6fa",
   "metadata": {},
   "source": [
    "## 3) Using completeness to optimize a simple algorithm (k-means model selection)\n",
    "\n",
    "Completeness is an **external** clustering metric: it requires ground-truth class labels.\n",
    "\n",
    "So you usually use it for **evaluation** or for **model selection** when you *do* have labels (e.g. benchmarking different clustering methods, tuning the number of clusters $k$, etc.).\n",
    "\n",
    "Important caveat:\n",
    "\n",
    "- If you blindly maximize completeness over $k$, the best answer can be $k=1$ (the degenerate “everything in one cluster” solution).\n",
    "- Maximizing completeness tends to favor **fewer** clusters, because merging classes is not penalized.\n",
    "- In practice you either:\n",
    "  - constrain $k$ to a sensible range (e.g. $k \\ge 2$, or $k$ near the expected number of classes), and/or\n",
    "  - optimize/report **V-measure** instead of completeness alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D dataset with known classes\n",
    "\n",
    "centers = np.array([[-3.0, 0.0], [3.0, 0.0], [0.0, 3.5]])\n",
    "n_per_class = 200\n",
    "std = 0.8\n",
    "\n",
    "X = np.vstack([rng.normal(loc=center, scale=std, size=(n_per_class, 2)) for center in centers])\n",
    "y_true_blobs = np.repeat(np.arange(centers.shape[0]), repeats=n_per_class)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y_true_blobs.astype(str),\n",
    "    title=\"Synthetic data (true classes)\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"true class\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72677151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_fit_predict_np(X, k, *, n_init=10, max_iter=100, tol=1e-6, rng=rng):\n",
    "    \"\"\"A small NumPy k-means (Lloyd's algorithm) implementation.\n",
    "\n",
    "    Returns: labels, centroids, inertia\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be 2D\")\n",
    "    n_samples = X.shape[0]\n",
    "    if not (1 <= k <= n_samples):\n",
    "        raise ValueError(\"k must be in [1, n_samples]\")\n",
    "\n",
    "    best_inertia = np.inf\n",
    "    best_labels = None\n",
    "    best_centroids = None\n",
    "\n",
    "    for _ in range(n_init):\n",
    "        centroids = X[rng.choice(n_samples, size=k, replace=False)].copy()\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            # squared distances: shape (n_samples, k)\n",
    "            d2 = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n",
    "            labels = np.argmin(d2, axis=1)\n",
    "\n",
    "            new_centroids = centroids.copy()\n",
    "            for j in range(k):\n",
    "                mask = labels == j\n",
    "                if not np.any(mask):\n",
    "                    new_centroids[j] = X[rng.integers(0, n_samples)]\n",
    "                else:\n",
    "                    new_centroids[j] = X[mask].mean(axis=0)\n",
    "\n",
    "            shift = np.max(np.linalg.norm(new_centroids - centroids, axis=1))\n",
    "            centroids = new_centroids\n",
    "            if shift < tol:\n",
    "                break\n",
    "\n",
    "        d2 = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n",
    "        inertia = float(np.sum(np.min(d2, axis=1)))\n",
    "\n",
    "        if inertia < best_inertia:\n",
    "            best_inertia = inertia\n",
    "            best_labels = labels\n",
    "            best_centroids = centroids\n",
    "\n",
    "    return best_labels, best_centroids, best_inertia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Optimize\" k-means by selecting k that maximizes completeness (and compare to v-measure)\n",
    "\n",
    "ks = np.arange(1, 9)\n",
    "results = []\n",
    "\n",
    "for k in ks:\n",
    "    labels, _, inertia = kmeans_fit_predict_np(X, k, n_init=8, max_iter=60)\n",
    "    c = completeness_score_np(y_true_blobs, labels)\n",
    "    v = v_measure_score_np(y_true_blobs, labels)\n",
    "    results.append((k, c, v, inertia))\n",
    "\n",
    "results = np.array(results, dtype=float)\n",
    "k_vals = results[:, 0]\n",
    "c_vals = results[:, 1]\n",
    "v_vals = results[:, 2]\n",
    "inertia_vals = results[:, 3]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=k_vals, y=c_vals, mode=\"lines+markers\", name=\"completeness\"))\n",
    "fig.add_trace(go.Scatter(x=k_vals, y=v_vals, mode=\"lines+markers\", name=\"v-measure\"))\n",
    "fig.update_layout(\n",
    "    title=\"Model selection for k-means: completeness vs k (and v-measure)\",\n",
    "    xaxis_title=\"k (number of clusters)\",\n",
    "    yaxis_title=\"score\",\n",
    "    yaxis=dict(range=[-0.02, 1.02]),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    x=k_vals,\n",
    "    y=inertia_vals,\n",
    "    markers=True,\n",
    "    title=\"k-means inertia vs k (lower is better for the k-means objective)\",\n",
    "    labels={\"x\": \"k\", \"y\": \"inertia\"},\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "best_k_unconstrained = int(k_vals[np.argmax(c_vals)])\n",
    "best_k_k_ge_2 = int(k_vals[1:][np.argmax(c_vals[1:])])\n",
    "best_k_by_v_measure = int(k_vals[np.argmax(v_vals)])\n",
    "\n",
    "print(f\"Best k by completeness (unconstrained): k={best_k_unconstrained}\")\n",
    "print(f\"Best k by completeness (restrict k>=2): k={best_k_k_ge_2}\")\n",
    "print(f\"Best k by v-measure: k={best_k_by_v_measure}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ff8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the completeness-selected solution (restrict k>=2)\n",
    "\n",
    "k_star = best_k_k_ge_2\n",
    "labels_star, centroids_star, _ = kmeans_fit_predict_np(X, k_star, n_init=12, max_iter=80)\n",
    "\n",
    "c_star = completeness_score_np(y_true_blobs, labels_star)\n",
    "h_star = homogeneity_score_np(y_true_blobs, labels_star)\n",
    "v_star = v_measure_score_np(y_true_blobs, labels_star)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=labels_star.astype(str),\n",
    "    title=f\"k-means clustering (k={k_star})\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"cluster\"},\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centroids_star[:, 0],\n",
    "        y=centroids_star[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(symbol=\"x\", size=12, color=\"black\"),\n",
    "        name=\"centroids\",\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"completeness={c_star:.4f}  homogeneity={h_star:.4f}  v-measure={v_star:.4f}\")\n",
    "\n",
    "fig = plot_contingency_heatmap(\n",
    "    y_true_blobs,\n",
    "    labels_star,\n",
    "    title=f\"Contingency table for k-means (k={k_star}, completeness={c_star:.3f})\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fc8d8",
   "metadata": {},
   "source": [
    "## 4) Pros, cons, and when to use it\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Interpretable**: directly measures “class splitting”.\n",
    "- **Label-invariant**: relabeling cluster IDs does not change the score.\n",
    "- **Normalized**: outputs a value in $[0,1]$.\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- **Doesn’t penalize merging classes**: a single-cluster solution gets completeness $=1$.\n",
    "- **Needs ground truth labels**: not usable as a purely unsupervised internal metric.\n",
    "- **Not smooth/differentiable**: hard to use as a direct training objective; it’s mainly for evaluation/model selection.\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- Benchmarking clustering methods when you have ground truth (e.g. synthetic data, annotated datasets).\n",
    "- Model selection for clustering (choose hyperparameters) when labels are available.\n",
    "- As part of **V-measure** (together with homogeneity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb04d45",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Modify the toy examples to create a case with **high completeness but low homogeneity**.\n",
    "2. Implement `homogeneity_score_np` by swapping the roles of classes and clusters and verify it matches the formula.\n",
    "3. Compare completeness to other external clustering metrics: ARI (Adjusted Rand Index) and NMI (Normalized Mutual Information).\n",
    "\n",
    "## References\n",
    "\n",
    "- scikit-learn docs: https://scikit-learn.org/stable/api/sklearn.metrics.html\n",
    "- Rosenberg & Hirschberg (2007): “V-measure: A conditional entropy-based external cluster evaluation measure”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}