{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8962b8e5",
   "metadata": {},
   "source": [
    "# Homogeneity Score (`homogeneity_score`)\n",
    "\n",
    "Homogeneity is an **external clustering metric**: it scores how *pure* each predicted cluster is with respect to **ground-truth** class labels.\n",
    "\n",
    "> Intuition: *If I open a cluster, do I mostly see one class?*\n",
    "\n",
    "- Perfectly pure clusters → score = **1.0**\n",
    "- Completely mixed clusters (clusters don’t help predict the class) → score ≈ **0.0**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- explain homogeneity in terms of **entropy**\n",
    "- compute it from a **contingency matrix** (class × cluster counts)\n",
    "- implement `homogeneity_score` from scratch in NumPy\n",
    "- visualize what increases / decreases the score\n",
    "- use it to **tune** a simple clustering algorithm (with caveats)\n",
    "\n",
    "---\n",
    "\n",
    "## Quick import\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import homogeneity_score\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Intuition: purity vs completeness\n",
    "2. The math: entropy & conditional entropy\n",
    "3. NumPy implementation (from scratch)\n",
    "4. Worked toy example + plots\n",
    "5. How mixing affects homogeneity\n",
    "6. Pitfall: over-segmentation\n",
    "7. Using homogeneity to tune k-means (grid search)\n",
    "8. Pros/cons + when to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a64f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import (\n",
    "    completeness_score as sk_completeness_score,\n",
    "    homogeneity_score as sk_homogeneity_score,\n",
    "    v_measure_score as sk_v_measure_score,\n",
    ")\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376f814",
   "metadata": {},
   "source": [
    "## 1) Intuition: purity vs completeness\n",
    "\n",
    "Homogeneity cares about **purity inside each predicted cluster**.\n",
    "\n",
    "- If a cluster contains multiple ground-truth classes, it’s *impure* → homogeneity goes down.\n",
    "- If a ground-truth class gets split across many clusters, homogeneity **does not** complain.\n",
    "\n",
    "That second point is why homogeneity is often paired with **completeness**:\n",
    "\n",
    "- **Homogeneity**: *each cluster contains only members of a single class*.\n",
    "- **Completeness**: *all members of a given class are assigned to the same cluster*.\n",
    "\n",
    "Both together are summarized by the **V-measure** (harmonic mean).\n",
    "\n",
    "A key property: homogeneity is **label-permutation invariant**. If you relabel clusters (e.g., swap cluster `0` and `1`), the score doesn’t change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0486c9",
   "metadata": {},
   "source": [
    "## 2) The math: entropy & conditional entropy\n",
    "\n",
    "We have:\n",
    "\n",
    "- ground-truth class labels: $c \\in \\{1,\\dots,C\\}$ (random variable $C$)\n",
    "- predicted cluster labels: $k \\in \\{1,\\dots,K\\}$ (random variable $K$)\n",
    "\n",
    "### 2.1 Contingency matrix\n",
    "\n",
    "Let the contingency matrix $N \\in \\mathbb{N}^{C\\times K}$ count co-occurrences:\n",
    "\n",
    "$$\n",
    "N_{c,k} = \\#\\{i: y_i = c, \\; \\hat y_i = k\\}.\n",
    "$$\n",
    "\n",
    "Define totals:\n",
    "\n",
    "- $n = \\sum_{c,k} N_{c,k}$\n",
    "- class counts: $n_c = \\sum_k N_{c,k}$\n",
    "- cluster counts: $n_k = \\sum_c N_{c,k}$\n",
    "\n",
    "### 2.2 Entropy\n",
    "\n",
    "The entropy of the class variable is\n",
    "\n",
    "$$\n",
    "H(C) = -\\sum_{c=1}^C p(c)\\,\\log p(c),\n",
    "\\qquad p(c)=\\frac{n_c}{n}.\n",
    "$$\n",
    "\n",
    "### 2.3 Conditional entropy\n",
    "\n",
    "The conditional entropy of classes *given clusters* is\n",
    "\n",
    "$$\n",
    "H(C\\mid K)\n",
    "= \\sum_{k=1}^K p(k)\\,H(C\\mid K=k)\n",
    "= -\\sum_{k=1}^K\\sum_{c=1}^C p(c,k)\\,\\log p(c\\mid k),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "p(k)=\\frac{n_k}{n},\\quad\n",
    "p(c,k)=\\frac{N_{c,k}}{n},\\quad\n",
    "p(c\\mid k)=\\frac{N_{c,k}}{n_k}.\n",
    "$$\n",
    "\n",
    "### 2.4 Homogeneity score\n",
    "\n",
    "Homogeneity is defined as\n",
    "\n",
    "$$\n",
    "h = 1 - \\frac{H(C\\mid K)}{H(C)}.\n",
    "$$\n",
    "\n",
    "Edge case: if $H(C)=0$ (all points belong to one class), homogeneity is defined as **1.0**.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- $H(C\\mid K)=0$ ⇒ each cluster determines the class perfectly ⇒ **$h=1$**\n",
    "- $H(C\\mid K)=H(C)$ ⇒ clusters tell you nothing about the class ⇒ **$h=0$**\n",
    "\n",
    "Note: the log base cancels in the ratio, so you can use natural log.\n",
    "\n",
    "A nice identity (using mutual information $I(C;K)$):\n",
    "\n",
    "$$\n",
    "h = \\frac{I(C;K)}{H(C)}.\n",
    "$$\n",
    "\n",
    "So homogeneity is the **fraction of class entropy explained by the clustering**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438ea22",
   "metadata": {},
   "source": [
    "## 3) NumPy implementation (from scratch)\n",
    "\n",
    "We’ll implement:\n",
    "\n",
    "- a contingency matrix builder (any label types)\n",
    "- entropy + conditional entropy from counts\n",
    "- `homogeneity_score` using the definition above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y):\n",
    "    '''Map arbitrary labels to integer ids 0..(m-1).'''\n",
    "    y = np.asarray(y)\n",
    "    classes, y_idx = np.unique(y, return_inverse=True)\n",
    "    return classes, y_idx\n",
    "\n",
    "\n",
    "def contingency_matrix_np(y_true, y_pred):\n",
    "    '''Contingency matrix N with N[c,k] = count(true=c, pred=k).'''\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape\")\n",
    "\n",
    "    true_labels, true_idx = encode_labels(y_true)\n",
    "    pred_labels, pred_idx = encode_labels(y_pred)\n",
    "\n",
    "    n_classes = true_labels.size\n",
    "    n_clusters = pred_labels.size\n",
    "\n",
    "    N = np.zeros((n_classes, n_clusters), dtype=int)\n",
    "    np.add.at(N, (true_idx, pred_idx), 1)\n",
    "\n",
    "    return N, true_labels, pred_labels\n",
    "\n",
    "\n",
    "def entropy_from_counts(counts: np.ndarray) -> float:\n",
    "    '''Shannon entropy of a discrete distribution given counts.'''\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    total = counts.sum()\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    p = counts[counts > 0] / total\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "\n",
    "def conditional_entropy_C_given_K_from_contingency(N: np.ndarray) -> float:\n",
    "    '''Compute H(C|K) from contingency matrix N (classes x clusters).'''\n",
    "    N = np.asarray(N, dtype=float)\n",
    "    n = N.sum()\n",
    "    if n <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    n_k = N.sum(axis=0, keepdims=True)  # (1, K)\n",
    "\n",
    "    # H(C|K) = - sum_{c,k} p(c,k) log p(c|k)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        p_ck = N / n\n",
    "        p_c_given_k = np.divide(N, n_k, where=n_k > 0)\n",
    "        terms = np.where(N > 0, p_ck * np.log(p_c_given_k), 0.0)\n",
    "\n",
    "    return float(-terms.sum())\n",
    "\n",
    "\n",
    "def homogeneity_score_np(y_true, y_pred) -> float:\n",
    "    '''Homogeneity score in [0,1]. Matches sklearn's definition.'''\n",
    "    N, _, _ = contingency_matrix_np(y_true, y_pred)\n",
    "\n",
    "    H_C = entropy_from_counts(N.sum(axis=1))\n",
    "    if H_C == 0.0:\n",
    "        return 1.0\n",
    "\n",
    "    H_C_given_K = conditional_entropy_C_given_K_from_contingency(N)\n",
    "    h = 1.0 - H_C_given_K / H_C\n",
    "\n",
    "    # Numerical safety\n",
    "    return float(np.clip(h, 0.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d98608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check vs scikit-learn\n",
    "\n",
    "y_true = rng.integers(0, 4, size=500)\n",
    "y_pred = rng.integers(0, 7, size=500)\n",
    "\n",
    "h_np = homogeneity_score_np(y_true, y_pred)\n",
    "h_sk = sk_homogeneity_score(y_true, y_pred)\n",
    "\n",
    "print(\"homogeneity (numpy): \", h_np)\n",
    "print(\"homogeneity (sklearn):\", h_sk)\n",
    "print(\"abs diff:\", abs(h_np - h_sk))\n",
    "\n",
    "# Edge case: one true class -> defined as 1.0\n",
    "print(\"one-class edge case:\", homogeneity_score_np(np.zeros(20), rng.integers(0, 3, size=20)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593e964",
   "metadata": {},
   "source": [
    "## 4) Worked toy example + plots\n",
    "\n",
    "Let’s build a small example and look at:\n",
    "\n",
    "- the contingency matrix\n",
    "- per-cluster class proportions\n",
    "- per-cluster class entropy (how “mixed” each cluster is)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cee481",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_toy = np.array([\n",
    "    \"A\", \"A\", \"A\", \"A\", \"A\",\n",
    "    \"B\", \"B\", \"B\", \"B\",\n",
    "    \"C\", \"C\", \"C\", \"C\",\n",
    "])\n",
    "\n",
    "# Clusters are somewhat mixed:\n",
    "# - cluster 0: mostly A\n",
    "# - cluster 1: mix of A and B\n",
    "# - cluster 2: pure C\n",
    "# - cluster 3: mix of B and C\n",
    "y_pred_toy = np.array([\n",
    "    0, 0, 0, 1, 1,\n",
    "    1, 1, 3, 3,\n",
    "    2, 2, 2, 3,\n",
    "])\n",
    "\n",
    "N_toy, classes_toy, clusters_toy = contingency_matrix_np(y_true_toy, y_pred_toy)\n",
    "\n",
    "h_toy = homogeneity_score_np(y_true_toy, y_pred_toy)\n",
    "\n",
    "print(\"classes:\", classes_toy)\n",
    "print(\"clusters:\", clusters_toy)\n",
    "print(\"contingency N (rows=class, cols=cluster):\")\n",
    "print(N_toy)\n",
    "print(\"homogeneity:\", h_toy)\n",
    "\n",
    "fig = px.imshow(\n",
    "    N_toy,\n",
    "    x=[f\"cluster {k}\" for k in clusters_toy],\n",
    "    y=[f\"class {c}\" for c in classes_toy],\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    title=f\"Toy contingency matrix (homogeneity={h_toy:.3f})\",\n",
    "    labels={\"x\": \"predicted cluster\", \"y\": \"true class\", \"color\": \"count\"},\n",
    ")\n",
    "fig.update_layout(coloraxis_showscale=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-cluster class proportions and per-cluster entropy\n",
    "\n",
    "cluster_sizes = N_toy.sum(axis=0)\n",
    "proportions = np.divide(N_toy, cluster_sizes, where=cluster_sizes > 0)\n",
    "\n",
    "cluster_entropies = np.array([entropy_from_counts(N_toy[:, k]) for k in range(N_toy.shape[1])])\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Class proportions within each cluster\", \"Entropy within each cluster\"),\n",
    ")\n",
    "\n",
    "# stacked bars (proportions)\n",
    "for i, c in enumerate(classes_toy):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[f\"cluster {k}\" for k in clusters_toy],\n",
    "            y=proportions[i],\n",
    "            name=f\"class {c}\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_yaxes(title_text=\"proportion\", range=[0, 1], row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"cluster\", row=1, col=1)\n",
    "\n",
    "# entropies\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f\"cluster {k}\" for k in clusters_toy],\n",
    "        y=cluster_entropies,\n",
    "        name=\"entropy\",\n",
    "        marker_color=\"gray\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"H(C | K=k)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"cluster\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(barmode=\"stack\", title_text=\"What makes homogeneity go up/down\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2915f",
   "metadata": {},
   "source": [
    "## 5) How mixing affects homogeneity\n",
    "\n",
    "Consider a **binary** problem with two equally common classes.\n",
    "\n",
    "We’ll create cluster labels by copying the true labels and then **flipping** a fraction $\\varepsilon$ of them.\n",
    "\n",
    "- $\\varepsilon = 0$ ⇒ perfectly pure clusters ⇒ homogeneity = 1\n",
    "- larger $\\varepsilon$ ⇒ more mixing inside clusters ⇒ homogeneity drops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_fraction(y, eps: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    if not (0.0 <= eps <= 1.0):\n",
    "        raise ValueError(\"eps must be in [0,1]\")\n",
    "\n",
    "    y_pred = y.copy()\n",
    "    flip = rng.random(size=y.size) < eps\n",
    "    y_pred[flip] = 1 - y_pred[flip]\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "n = 2000\n",
    "# perfectly balanced classes\n",
    "true_bin = np.r_[np.zeros(n // 2, dtype=int), np.ones(n // 2, dtype=int)]\n",
    "rng.shuffle(true_bin)\n",
    "\n",
    "eps_grid = np.linspace(0.0, 0.5, 51)\n",
    "h_values = []\n",
    "\n",
    "for eps in eps_grid:\n",
    "    pred_bin = flip_fraction(true_bin, eps=float(eps), rng=rng)\n",
    "    h_values.append(homogeneity_score_np(true_bin, pred_bin))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=eps_grid, y=h_values, mode=\"lines+markers\", name=\"homogeneity\"))\n",
    "fig.update_layout(\n",
    "    title=\"Homogeneity vs label mixing (binary flip noise)\",\n",
    "    xaxis_title=\"flip fraction ε\",\n",
    "    yaxis_title=\"homogeneity\",\n",
    "    yaxis_range=[0, 1.02],\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fdcd02",
   "metadata": {},
   "source": [
    "## 6) Pitfall: over-segmentation can reach 1.0\n",
    "\n",
    "Homogeneity ignores whether a class is split across many clusters.\n",
    "\n",
    "If each class is divided into multiple *sub-clusters* (all pure), homogeneity stays **1.0**, even though the clustering is often less useful.\n",
    "\n",
    "We’ll demonstrate this by taking $C=3$ classes and splitting each class into $m$ pure clusters.\n",
    "\n",
    "We’ll also show **completeness** and **V-measure** for contrast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c32cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 3\n",
    "n_per_class = 400\n",
    "\n",
    "y_true = np.repeat(np.arange(C), n_per_class)\n",
    "rng.shuffle(y_true)\n",
    "\n",
    "\n",
    "def split_each_class_into_m_clusters(y_true, m: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.empty_like(y_true)\n",
    "\n",
    "    for c in range(np.max(y_true) + 1):\n",
    "        idx = np.where(y_true == c)[0]\n",
    "        sub = rng.integers(0, m, size=idx.size)\n",
    "        y_pred[idx] = c * m + sub\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "m_grid = np.arange(1, 21)\n",
    "\n",
    "h_list = []\n",
    "comp_list = []\n",
    "v_list = []\n",
    "\n",
    "for m in m_grid:\n",
    "    y_pred = split_each_class_into_m_clusters(y_true, m=int(m), rng=rng)\n",
    "    h_list.append(homogeneity_score_np(y_true, y_pred))\n",
    "    comp_list.append(sk_completeness_score(y_true, y_pred))\n",
    "    v_list.append(sk_v_measure_score(y_true, y_pred))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=m_grid, y=h_list, mode=\"lines+markers\", name=\"homogeneity\"))\n",
    "fig.add_trace(go.Scatter(x=m_grid, y=comp_list, mode=\"lines+markers\", name=\"completeness\"))\n",
    "fig.add_trace(go.Scatter(x=m_grid, y=v_list, mode=\"lines+markers\", name=\"v-measure\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Over-segmentation: splitting each class into m pure clusters\",\n",
    "    xaxis_title=\"m (clusters per true class)\",\n",
    "    yaxis_title=\"score\",\n",
    "    yaxis_range=[0, 1.02],\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd7a12",
   "metadata": {},
   "source": [
    "## 7) Using homogeneity to tune k-means (grid search)\n",
    "\n",
    "Homogeneity is **not differentiable** w.r.t. model parameters (it depends on discrete assignments), so you normally use it for:\n",
    "\n",
    "- comparing clustering algorithms\n",
    "- selecting hyperparameters (like number of clusters $k$)\n",
    "\n",
    "Below is a tiny **NumPy k-means** implementation and a grid search over $k$.\n",
    "\n",
    "We’ll see an important behavior:\n",
    "\n",
    "- as $k$ increases, homogeneity often increases (sometimes monotonically)\n",
    "\n",
    "So *optimizing for homogeneity alone* tends to push toward larger $k$ unless you constrain $k$ or pair it with completeness / V-measure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_fit_predict_np(X: np.ndarray, k: int, n_iters: int = 50, seed: int = 0):\n",
    "    '''Simple k-means (Lloyd) implementation. Returns labels and centroids.'''\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n, d = X.shape\n",
    "\n",
    "    if not (1 <= k <= n):\n",
    "        raise ValueError(\"k must be in [1, n]\")\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    # init: choose k random points as centroids\n",
    "    centroids = X[rng_local.choice(n, size=k, replace=False)].copy()\n",
    "\n",
    "    labels = np.full(n, -1, dtype=int)\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        # squared distances to each centroid (n, k)\n",
    "        d2 = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n",
    "        new_labels = np.argmin(d2, axis=1)\n",
    "\n",
    "        if np.array_equal(new_labels, labels):\n",
    "            break\n",
    "\n",
    "        labels = new_labels\n",
    "\n",
    "        # update step\n",
    "        for j in range(k):\n",
    "            mask = labels == j\n",
    "            if np.any(mask):\n",
    "                centroids[j] = X[mask].mean(axis=0)\n",
    "            else:\n",
    "                # empty cluster: re-seed to a random point\n",
    "                centroids[j] = X[rng_local.integers(0, n)]\n",
    "\n",
    "    return labels, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with known classes (so we can compute external metrics)\n",
    "\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=1500,\n",
    "    centers=3,\n",
    "    n_features=2,\n",
    "    cluster_std=[1.0, 1.2, 0.9],\n",
    "    random_state=3,\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y_true.astype(str),\n",
    "    title=\"Ground-truth classes (for evaluation)\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"true class\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ecbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different k-means clusterings visually\n",
    "\n",
    "def plot_clustering(X, labels, title: str):\n",
    "    fig = px.scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        color=labels.astype(str),\n",
    "        title=title,\n",
    "        labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"cluster\"},\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for k in [2, 3, 6]:\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=0)\n",
    "    y_pred = km.fit_predict(X)\n",
    "\n",
    "    h = homogeneity_score_np(y_true, y_pred)\n",
    "    plot_clustering(X, y_pred, title=f\"KMeans k={k} (homogeneity={h:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4320d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over k and random seeds (using the NumPy k-means above)\n",
    "\n",
    "k_values = np.arange(2, 13)\n",
    "seeds = np.arange(0, 15)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for k in k_values:\n",
    "    for seed in seeds:\n",
    "        labels, _ = kmeans_fit_predict_np(X, k=int(k), n_iters=80, seed=int(seed))\n",
    "        rows.append(\n",
    "            {\n",
    "                \"k\": int(k),\n",
    "                \"seed\": int(seed),\n",
    "                \"homogeneity\": homogeneity_score_np(y_true, labels),\n",
    "                \"completeness\": sk_completeness_score(y_true, labels),\n",
    "                \"v_measure\": sk_v_measure_score(y_true, labels),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# best seed per k (by homogeneity)\n",
    "best_by_k = {}\n",
    "for r in rows:\n",
    "    k = r[\"k\"]\n",
    "    if (k not in best_by_k) or (r[\"homogeneity\"] > best_by_k[k][\"homogeneity\"]):\n",
    "        best_by_k[k] = r\n",
    "\n",
    "best_rows = [best_by_k[k] for k in k_values]\n",
    "\n",
    "best_k_by_h = max(best_rows, key=lambda r: r[\"homogeneity\"])[\"k\"]\n",
    "print(\"best k by homogeneity:\", best_k_by_h)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# scatter all runs\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[r[\"k\"] for r in rows],\n",
    "        y=[r[\"homogeneity\"] for r in rows],\n",
    "        mode=\"markers\",\n",
    "        name=\"homogeneity (all seeds)\",\n",
    "        marker=dict(size=6, opacity=0.35),\n",
    "    )\n",
    ")\n",
    "\n",
    "# line: best homogeneity per k\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[r[\"k\"] for r in best_rows],\n",
    "        y=[r[\"homogeneity\"] for r in best_rows],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"best homogeneity per k\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# lines: completeness and v-measure for the same best runs\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[r[\"k\"] for r in best_rows],\n",
    "        y=[r[\"completeness\"] for r in best_rows],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"completeness (same best-by-h runs)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[r[\"k\"] for r in best_rows],\n",
    "        y=[r[\"v_measure\"] for r in best_rows],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"v-measure (same best-by-h runs)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=best_k_by_h,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"gray\",\n",
    "    annotation_text=f\"best k by homogeneity: {best_k_by_h}\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Selecting k by homogeneity (watch the over-segmentation bias)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"score\",\n",
    "    yaxis_range=[0, 1.02],\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f80cc7",
   "metadata": {},
   "source": [
    "## 8) Pros/cons + when to use\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Interpretable**: “cluster purity” aligned with many real use cases\n",
    "- **Scale [0, 1]** and **label-permutation invariant**\n",
    "- Works for **multiclass** and **imbalanced** class distributions\n",
    "- Information-theoretic: connects to **entropy** and **mutual information**\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- Requires **ground-truth labels** (so it’s not usable for truly unsupervised evaluation)\n",
    "- **Ignores completeness** → can be **artificially high** with many clusters (over-segmentation)\n",
    "- Not a smooth/differentiable objective (used for evaluation / selection, not gradient training)\n",
    "- Can hide issues if small impure clusters exist but are tiny (weighted by cluster size)\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- Benchmarking clustering when you have a gold standard (topics, categories, known segments)\n",
    "- Situations where mixing classes inside a cluster is especially harmful (you need “clean buckets”)\n",
    "- As part of **V-measure** (homogeneity + completeness) or alongside other external metrics (ARI, AMI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638e43b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Rosenberg, A., & Hirschberg, J. (2007). *V-measure: A conditional entropy-based external cluster evaluation measure.*\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html\n",
    "- Related metrics: `completeness_score`, `v_measure_score`, `adjusted_rand_score`, `adjusted_mutual_info_score`\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Create a clustering with **high homogeneity but low completeness**. Verify with plots.\n",
    "2. Modify the toy example so one small cluster is very impure. How much does homogeneity change?\n",
    "3. Implement `completeness_score` from scratch and reproduce V-measure.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}