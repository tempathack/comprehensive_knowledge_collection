{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2c3e1a",
   "metadata": {},
   "source": [
    "# ARIMA (Autoregressive Integrated Moving Average)\n",
    "\n",
    "ARIMA models a (possibly **non-stationary**) univariate time series by:\n",
    "1) **differencing** it until it is approximately stationary (the **I**),\n",
    "2) fitting an **ARMA** model to the differenced series (the **AR** + **MA**),\n",
    "3) **integrating** forecasts back to the original scale.\n",
    "\n",
    "## What you’ll learn\n",
    "- what “integration” means in ARIMA (differencing)\n",
    "- why differencing helps with trends / unit roots (non-stationarity)\n",
    "- how to choose $(p, d, q)$ (intuition + ACF/PACF)\n",
    "- a low-level NumPy implementation (fit + forecast + uncertainty)\n",
    "- Plotly visuals: raw vs. differenced series, forecasts, and confidence intuition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d16c99",
   "metadata": {},
   "source": [
    "## ARIMA in one equation (with backshift notation)\n",
    "\n",
    "Let $B$ be the **backshift operator**: $B y_t = y_{t-1}$.\n",
    "Define the differencing operator $\\nabla = 1 - B$.\n",
    "\n",
    "An **ARIMA$(p,d,q)$** model can be written compactly as:\n",
    "\n",
    "$$\n",
    "\\phi(B)\\, \\nabla^d y_t = c + \\theta(B)\\, \\varepsilon_t, \\qquad \\varepsilon_t \\sim \\mathrm{WN}(0,\\sigma^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\phi(B) = 1 - \\phi_1 B - \\cdots - \\phi_p B^p$ (**AR polynomial**)  \n",
    "- $\\theta(B) = 1 + \\theta_1 B + \\cdots + \\theta_q B^q$ (**MA polynomial**)  \n",
    "- $\\nabla^d y_t = (1-B)^d y_t$ is the **$d$-th differenced** series  \n",
    "- $\\varepsilon_t$ is (ideally) **white noise** (uncorrelated, constant variance)\n",
    "\n",
    "Interpretation:\n",
    "- **AR$(p)$**: “today relates to the last $p$ values”\n",
    "- **I$(d)$**: “work on changes, not levels, to remove trends / unit roots”\n",
    "- **MA$(q)$**: “today relates to the last $q$ shocks / errors”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c3ee2",
   "metadata": {},
   "source": [
    "## Integration = differencing (precisely)\n",
    "\n",
    "ARIMA’s “**Integrated**” part is historical terminology: instead of modeling $y_t$ directly, we model its differences, then **sum (integrate) back** to get predictions on the original scale.\n",
    "\n",
    "### First difference\n",
    "$$\n",
    "\\Delta y_t = y_t - y_{t-1} = (1 - B) y_t.\n",
    "$$\n",
    "\n",
    "### Second difference\n",
    "$$\n",
    "\\Delta^2 y_t = \\Delta(\\Delta y_t) = y_t - 2y_{t-1} + y_{t-2}.\n",
    "$$\n",
    "\n",
    "### $d$-th difference (general form)\n",
    "$$\n",
    "\\Delta^d y_t = (1 - B)^d y_t = \\sum_{k=0}^{d} (-1)^k \\binom{d}{k} y_{t-k}.\n",
    "$$\n",
    "\n",
    "This is a discrete analog of taking derivatives: each differencing step removes one degree of polynomial trend (roughly speaking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b40b7",
   "metadata": {},
   "source": [
    "## Why ARIMA handles non-stationarity\n",
    "\n",
    "ARMA models assume (weak) **stationarity**: mean and autocovariances are time-invariant.\n",
    "Many real series aren’t stationary because they contain a **stochastic trend** (a *unit root*), e.g. a random walk.\n",
    "\n",
    "### Unit-root example (random walk)\n",
    "$$\n",
    "y_t = y_{t-1} + \\varepsilon_t \\quad \\Rightarrow \\quad \\Delta y_t = \\varepsilon_t.\n",
    "$$\n",
    "\n",
    "The level series $y_t$ is non-stationary, but the increment series $\\Delta y_t$ is stationary (white noise). ARIMA uses $d$ differences to reduce the series to something ARMA can model.\n",
    "\n",
    "### Deterministic trend intuition\n",
    "If $y_t = a + bt + u_t$ with stationary $u_t$, then:\n",
    "$$\n",
    "\\Delta y_t = b + (u_t - u_{t-1}),\n",
    "$$\n",
    "so differencing removes the linear trend component.\n",
    "\n",
    "### Practical warning: over-differencing\n",
    "If you difference too much (choose $d$ larger than needed), you can introduce extra moving-average structure, increase variance, and harm forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_arma(n: int, *, c: float, phi: np.ndarray, theta: np.ndarray, sigma: float, burn_in: int = 300, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Simulate a stationary ARMA(p,q): y_t = c + sum(phi_i y_{t-i}) + eps_t + sum(theta_j eps_{t-j}).\"\"\"\n",
    "    phi = np.asarray(phi, dtype=float)\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "    p, q = len(phi), len(theta)\n",
    "\n",
    "    m = n + burn_in\n",
    "    eps = rng.normal(0.0, sigma, size=m)\n",
    "    y = np.zeros(m)\n",
    "\n",
    "    for t in range(m):\n",
    "        ar = 0.0\n",
    "        for i in range(1, p + 1):\n",
    "            if t - i >= 0:\n",
    "                ar += phi[i - 1] * y[t - i]\n",
    "\n",
    "        ma = 0.0\n",
    "        for j in range(1, q + 1):\n",
    "            if t - j >= 0:\n",
    "                ma += theta[j - 1] * eps[t - j]\n",
    "\n",
    "        y[t] = c + ar + eps[t] + ma\n",
    "\n",
    "    return y[burn_in:]\n",
    "\n",
    "\n",
    "# Build a non-stationary series by integrating a stationary ARMA increment process.\n",
    "n = 320\n",
    "dates = pd.date_range(\"2020-01-01\", periods=n, freq=\"D\")\n",
    "\n",
    "# Increment process: ARMA(2,1)\n",
    "phi_true = np.array([0.6, -0.2])\n",
    "theta_true = np.array([0.5])\n",
    "drift = 0.10\n",
    "sigma = 1.0\n",
    "\n",
    "dy = simulate_arma(n, c=drift, phi=phi_true, theta=theta_true, sigma=sigma, rng=rng)\n",
    "y = 100 + np.cumsum(dy)  # integrated => non-stationary level series\n",
    "\n",
    "d1 = np.diff(y, n=1)\n",
    "\n",
    "raw_p = adfuller(y)[1]\n",
    "diff_p = adfuller(d1)[1]\n",
    "print(f\"ADF p-value (raw y):        {raw_p:.3g}\")\n",
    "print(f\"ADF p-value (diff Δy):      {diff_p:.3g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0cc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.08,\n",
    "    subplot_titles=(\"Raw series (level)\", \"First difference (Δy)\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=dates, y=y, mode=\"lines\", name=\"y\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=dates[1:], y=d1, mode=\"lines\", name=\"Δy\"), row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Value\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "fig.update_layout(height=550, title=\"Raw vs. differenced series\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40a7e6",
   "metadata": {},
   "source": [
    "## Choosing $(p, d, q)$ (intuition + common workflow)\n",
    "\n",
    "### 1) Choose $d$ (integration order)\n",
    "- Use **plots**: trend / changing level often suggests differencing.\n",
    "- Use **unit-root / stationarity tests** (imperfect but useful): ADF, KPSS.\n",
    "- Difference the *minimum* number of times needed to make the series look approximately stationary.\n",
    "\n",
    "Rules of thumb:\n",
    "- Many business series: $d \\in \\{0,1\\}$.\n",
    "- If the series has strong seasonality: you often need **seasonal differencing** too (that’s SARIMA).\n",
    "\n",
    "### 2) Choose $p$ and $q$ on the differenced series\n",
    "On $x_t = \\Delta^d y_t$ (now assumed stationary), use:\n",
    "- **PACF** shape to guess $p$ (AR order)\n",
    "- **ACF** shape to guess $q$ (MA order)\n",
    "- then refine with **AIC/BIC** or rolling validation.\n",
    "\n",
    "Heuristics (idealized patterns):\n",
    "- AR$(p)$: PACF cuts off after lag $p$, ACF tails off\n",
    "- MA$(q)$: ACF cuts off after lag $q$, PACF tails off\n",
    "- ARMA: both tail off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf_np(x: np.ndarray, nlags: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x - x.mean()\n",
    "    denom = np.dot(x, x)\n",
    "    out = np.empty(nlags + 1)\n",
    "    out[0] = 1.0\n",
    "    for k in range(1, nlags + 1):\n",
    "        out[k] = np.dot(x[k:], x[:-k]) / denom\n",
    "    return out\n",
    "\n",
    "\n",
    "def pacf_ols(x: np.ndarray, nlags: int) -> np.ndarray:\n",
    "    \"\"\"Partial autocorrelation via OLS: pacf(k) = coeff on lag k in an AR(k) regression.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x - x.mean()\n",
    "    out = np.empty(nlags + 1)\n",
    "    out[0] = 1.0\n",
    "\n",
    "    for k in range(1, nlags + 1):\n",
    "        y_reg = x[k:]\n",
    "        X_lags = np.column_stack([x[k - i - 1 : -(i + 1)] for i in range(k)])\n",
    "        X = np.column_stack([np.ones(len(y_reg)), X_lags])\n",
    "        beta, *_ = np.linalg.lstsq(X, y_reg, rcond=None)\n",
    "        out[k] = beta[-1]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "nlags = 24\n",
    "acf_vals = acf_np(d1, nlags=nlags)\n",
    "pacf_vals = pacf_ols(d1, nlags=nlags)\n",
    "lags = np.arange(nlags + 1)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"ACF (Δy)\", \"PACF (Δy)\"), horizontal_spacing=0.12)\n",
    "fig.add_trace(go.Bar(x=lags, y=acf_vals, name=\"ACF\"), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=lags, y=pacf_vals, name=\"PACF\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Lag\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Lag\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Correlation\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Correlation\", row=1, col=2)\n",
    "fig.update_layout(height=350, title=\"Autocorrelation diagnostics on differenced series\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8409bb",
   "metadata": {},
   "source": [
    "## Low-level NumPy ARIMA implementation (educational)\n",
    "\n",
    "We’ll implement a simple **ARIMA$(p,d,q)$** fit using **conditional sum of squares (CSS)**:\n",
    "\n",
    "1) Difference: $x_t = \\Delta^d y_t$  \n",
    "2) Fit ARMA on $x_t$ by minimizing squared one-step-ahead residuals\n",
    "\n",
    "### ARMA residual recursion\n",
    "For $x_t$ modeled as:\n",
    "$$\n",
    "x_t = c + \\sum_{i=1}^p \\phi_i x_{t-i} + \\varepsilon_t + \\sum_{j=1}^q \\theta_j \\varepsilon_{t-j},\n",
    "$$\n",
    "the residuals satisfy:\n",
    "$$\n",
    "\\varepsilon_t = x_t - c - \\sum_{i=1}^p \\phi_i x_{t-i} - \\sum_{j=1}^q \\theta_j \\varepsilon_{t-j}.\n",
    "$$\n",
    "\n",
    "We minimize $\\sum_t \\varepsilon_t^2$ over $(c, \\phi, \\theta)$.\n",
    "\n",
    "This is not a full maximum-likelihood fit (which handles initial conditions and constraints more carefully), but it’s a good “from scratch” view of how ARIMA works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bcf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_levels(y: np.ndarray, d: int) -> tuple[np.ndarray, list[float]]:\n",
    "    \"\"\"Return Δ^d y and the last values needed to invert the differencing.\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if d < 0:\n",
    "        raise ValueError(\"d must be >= 0\")\n",
    "    if d == 0:\n",
    "        return y.copy(), []\n",
    "\n",
    "    levels = [y]\n",
    "    for _ in range(d):\n",
    "        levels.append(levels[-1][1:] - levels[-1][:-1])\n",
    "\n",
    "    last_values = [levels[i][-1] for i in range(d)]\n",
    "    return levels[-1], last_values\n",
    "\n",
    "\n",
    "def undifference_forecast(diff_forecast: np.ndarray, last_values: list[float]) -> np.ndarray:\n",
    "    \"\"\"Invert differencing for forecasts; last_values are [last y, last Δy, ..., last Δ^(d-1)y].\"\"\"\n",
    "    diff_forecast = np.asarray(diff_forecast, dtype=float)\n",
    "    d = len(last_values)\n",
    "    if d == 0:\n",
    "        return diff_forecast.copy()\n",
    "\n",
    "    states = np.array(last_values, dtype=float)\n",
    "    out = np.empty(len(diff_forecast))\n",
    "\n",
    "    for t, delta_d in enumerate(diff_forecast):\n",
    "        delta = delta_d\n",
    "        for k in range(d - 1, -1, -1):\n",
    "            states[k] = states[k] + delta\n",
    "            delta = states[k]\n",
    "        out[t] = states[0]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def arma_residuals(x: np.ndarray, c: float, phi: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    phi = np.asarray(phi, dtype=float)\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "    p, q = len(phi), len(theta)\n",
    "\n",
    "    eps = np.zeros_like(x)\n",
    "    for t in range(len(x)):\n",
    "        ar = 0.0\n",
    "        for i in range(1, p + 1):\n",
    "            if t - i >= 0:\n",
    "                ar += phi[i - 1] * x[t - i]\n",
    "\n",
    "        ma = 0.0\n",
    "        for j in range(1, q + 1):\n",
    "            if t - j >= 0:\n",
    "                ma += theta[j - 1] * eps[t - j]\n",
    "\n",
    "        eps[t] = x[t] - c - ar - ma\n",
    "\n",
    "    return eps\n",
    "\n",
    "\n",
    "def _arma_css_sse(params: np.ndarray, x: np.ndarray, p: int, q: int) -> float:\n",
    "    c = params[0]\n",
    "    phi = params[1 : 1 + p]\n",
    "    theta = params[1 + p : 1 + p + q]\n",
    "    eps = arma_residuals(x, c=c, phi=phi, theta=theta)\n",
    "    start = max(p, q)\n",
    "    return float(np.sum(eps[start:] ** 2))\n",
    "\n",
    "\n",
    "def fit_arma_css(x: np.ndarray, p: int, q: int) -> dict:\n",
    "    \"\"\"Fit ARMA(p,q) to x via conditional sum of squares (CSS).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    # Initial guess: AR(p) via OLS (ignore MA), theta = 0.\n",
    "    if p > 0:\n",
    "        y_reg = x[p:]\n",
    "        X_lags = np.column_stack([x[p - i - 1 : -(i + 1)] for i in range(p)])\n",
    "        X = np.column_stack([np.ones(len(y_reg)), X_lags])\n",
    "        beta, *_ = np.linalg.lstsq(X, y_reg, rcond=None)\n",
    "        c0 = float(beta[0])\n",
    "        phi0 = beta[1:]\n",
    "    else:\n",
    "        c0 = float(np.mean(x))\n",
    "        phi0 = np.array([])\n",
    "\n",
    "    theta0 = np.zeros(q)\n",
    "    x0 = np.concatenate([[c0], phi0, theta0]).astype(float)\n",
    "\n",
    "    bounds = [(None, None)] + [(-0.99, 0.99)] * (p + q)\n",
    "    res = minimize(_arma_css_sse, x0=x0, args=(x, p, q), method=\"L-BFGS-B\", bounds=bounds)\n",
    "\n",
    "    params = res.x\n",
    "    c = float(params[0])\n",
    "    phi = params[1 : 1 + p]\n",
    "    theta = params[1 + p : 1 + p + q]\n",
    "    eps = arma_residuals(x, c=c, phi=phi, theta=theta)\n",
    "\n",
    "    start = max(p, q)\n",
    "    sigma2 = float(np.mean(eps[start:] ** 2))\n",
    "\n",
    "    return {\n",
    "        \"success\": bool(res.success),\n",
    "        \"message\": str(res.message),\n",
    "        \"c\": c,\n",
    "        \"phi\": phi,\n",
    "        \"theta\": theta,\n",
    "        \"eps\": eps,\n",
    "        \"sigma\": float(np.sqrt(sigma2)),\n",
    "        \"sse\": float(np.sum(eps[start:] ** 2)),\n",
    "        \"nobs\": int(len(x)),\n",
    "    }\n",
    "\n",
    "\n",
    "def arma_forecast_mean(x_hist: np.ndarray, eps_hist: np.ndarray, *, c: float, phi: np.ndarray, theta: np.ndarray, h: int) -> np.ndarray:\n",
    "    x_hist = np.asarray(x_hist, dtype=float)\n",
    "    eps_hist = np.asarray(eps_hist, dtype=float)\n",
    "    phi = np.asarray(phi, dtype=float)\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "    p, q = len(phi), len(theta)\n",
    "\n",
    "    x = x_hist.tolist()\n",
    "    eps = eps_hist.tolist()\n",
    "    out = []\n",
    "\n",
    "    for _ in range(h):\n",
    "        ar = sum(phi[i] * x[-1 - i] for i in range(p)) if p else 0.0\n",
    "        ma = sum(theta[j] * eps[-1 - j] for j in range(q)) if q else 0.0\n",
    "        pred = c + ar + ma\n",
    "        x.append(pred)\n",
    "        eps.append(0.0)  # E[eps_{t+h}] = 0 for mean forecasts\n",
    "        out.append(pred)\n",
    "\n",
    "    return np.asarray(out)\n",
    "\n",
    "\n",
    "def arma_simulate_future(\n",
    "    x_hist: np.ndarray,\n",
    "    eps_hist: np.ndarray,\n",
    "    *,\n",
    "    c: float,\n",
    "    phi: np.ndarray,\n",
    "    theta: np.ndarray,\n",
    "    sigma: float,\n",
    "    h: int,\n",
    "    n_sims: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Simulate future paths of x_t conditional on history (for uncertainty bands).\"\"\"\n",
    "    x_hist = np.asarray(x_hist, dtype=float)\n",
    "    eps_hist = np.asarray(eps_hist, dtype=float)\n",
    "    phi = np.asarray(phi, dtype=float)\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "    p, q = len(phi), len(theta)\n",
    "\n",
    "    sims = np.zeros((n_sims, h))\n",
    "    base_len = len(x_hist)\n",
    "\n",
    "    for s in range(n_sims):\n",
    "        x = np.concatenate([x_hist, np.zeros(h)])\n",
    "        eps = np.concatenate([eps_hist, np.zeros(h)])\n",
    "\n",
    "        for step in range(h):\n",
    "            t = base_len + step\n",
    "            ar = 0.0\n",
    "            for i in range(1, p + 1):\n",
    "                ar += phi[i - 1] * x[t - i]\n",
    "            ma = 0.0\n",
    "            for j in range(1, q + 1):\n",
    "                ma += theta[j - 1] * eps[t - j]\n",
    "\n",
    "            eps[t] = rng.normal(0.0, sigma)\n",
    "            x[t] = c + ar + ma + eps[t]\n",
    "            sims[s, step] = x[t]\n",
    "\n",
    "    return sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a08c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "n_train = 260\n",
    "y_train = y[:n_train]\n",
    "y_test = y[n_train:]\n",
    "h = len(y_test)\n",
    "\n",
    "order = (2, 1, 1)\n",
    "p, d, q = order\n",
    "\n",
    "x_train, last_values = difference_levels(y_train, d=d)\n",
    "fit = fit_arma_css(x_train, p=p, q=q)\n",
    "\n",
    "print(\"CSS optimization success:\", fit[\"success\"], \"|\", fit[\"message\"])\n",
    "print(\"Estimated c:\", fit[\"c\"])\n",
    "print(\"Estimated phi:\", fit[\"phi\"])\n",
    "print(\"Estimated theta:\", fit[\"theta\"])\n",
    "print(\"Estimated sigma:\", fit[\"sigma\"])\n",
    "\n",
    "# Mean forecast on differenced scale, then integrate back to original scale\n",
    "x_fc_mean = arma_forecast_mean(x_train, fit[\"eps\"], c=fit[\"c\"], phi=fit[\"phi\"], theta=fit[\"theta\"], h=h)\n",
    "y_fc_mean = undifference_forecast(x_fc_mean, last_values=last_values)\n",
    "\n",
    "# Uncertainty via conditional simulation\n",
    "n_sims = 2000\n",
    "x_fc_sims = arma_simulate_future(\n",
    "    x_train,\n",
    "    fit[\"eps\"],\n",
    "    c=fit[\"c\"],\n",
    "    phi=fit[\"phi\"],\n",
    "    theta=fit[\"theta\"],\n",
    "    sigma=fit[\"sigma\"],\n",
    "    h=h,\n",
    "    n_sims=n_sims,\n",
    "    rng=rng,\n",
    ")\n",
    "y_fc_sims = np.vstack([undifference_forecast(x_fc_sims[i], last_values=last_values) for i in range(n_sims)])\n",
    "\n",
    "q05 = np.quantile(y_fc_sims, 0.05, axis=0)\n",
    "q50 = np.quantile(y_fc_sims, 0.50, axis=0)\n",
    "q95 = np.quantile(y_fc_sims, 0.95, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ae8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dates = dates[:n_train]\n",
    "x_test_dates = dates[n_train:]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_train_dates, y=y_train, mode=\"lines\", name=\"Train\"))\n",
    "fig.add_trace(go.Scatter(x=x_test_dates, y=y_test, mode=\"lines\", name=\"Test (actual)\", line=dict(color=\"#444\")))\n",
    "\n",
    "# Confidence band\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_test_dates,\n",
    "        y=q95,\n",
    "        mode=\"lines\",\n",
    "        name=\"90% interval\",\n",
    "        line=dict(width=0),\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_test_dates,\n",
    "        y=q05,\n",
    "        mode=\"lines\",\n",
    "        line=dict(width=0),\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"rgba(31, 119, 180, 0.20)\",\n",
    "        name=\"90% interval\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_test_dates, y=y_fc_mean, mode=\"lines\", name=\"Forecast (mean)\", line=dict(color=\"#1f77b4\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    height=450,\n",
    "    title=f\"ARIMA{order} forecast (NumPy CSS) + simulated uncertainty\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Value\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence intuition: show a few sampled future paths + the widening band.\n",
    "n_paths = 40\n",
    "path_idx = rng.choice(n_sims, size=n_paths, replace=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=dates[:n_train], y=y_train, mode=\"lines\", name=\"Train\"))\n",
    "\n",
    "for i in path_idx:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_test_dates,\n",
    "            y=y_fc_sims[i],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"rgba(31,119,180,0.15)\", width=1),\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_test_dates, y=q50, mode=\"lines\", name=\"Median forecast\", line=dict(color=\"#1f77b4\", width=3)))\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_test_dates, y=q95, mode=\"lines\", line=dict(width=0), showlegend=False)\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_test_dates,\n",
    "        y=q05,\n",
    "        mode=\"lines\",\n",
    "        line=dict(width=0),\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"rgba(31, 119, 180, 0.20)\",\n",
    "        name=\"90% band\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=450,\n",
    "    title=\"Forecast uncertainty intuition (fan of simulated futures)\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Value\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d9e62",
   "metadata": {},
   "source": [
    "## Typical real-world use cases\n",
    "\n",
    "ARIMA is a strong baseline when:\n",
    "- you have a **single** time series (univariate forecasting)\n",
    "- dynamics are mostly **linear** and captured by autocorrelation\n",
    "- non-stationarity is mostly **trend / unit-root-like** (handled by differencing)\n",
    "\n",
    "Common examples:\n",
    "- demand / sales forecasting (after removing seasonality or using SARIMA)\n",
    "- call volume / ticket volume / website traffic forecasting\n",
    "- energy load and simple sensor forecasting (with careful outlier handling)\n",
    "- macroeconomic indicators (often with differencing and log transforms)\n",
    "- as a baseline in finance for returns/volatility proxies (with strong caveats)\n",
    "\n",
    "When ARIMA may *not* be enough:\n",
    "- strong seasonality (use **SARIMA**: seasonal differencing + seasonal AR/MA)\n",
    "- external drivers (use **ARIMAX** / regression with ARMA errors)\n",
    "- multiple interacting series (VAR / state space)\n",
    "- non-linear patterns / regime shifts (consider richer models)\n",
    "\n",
    "## References\n",
    "- Box, Jenkins, Reinsel, Ljung — *Time Series Analysis: Forecasting and Control*\n",
    "- Hyndman & Athanasopoulos — *Forecasting: Principles and Practice*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}