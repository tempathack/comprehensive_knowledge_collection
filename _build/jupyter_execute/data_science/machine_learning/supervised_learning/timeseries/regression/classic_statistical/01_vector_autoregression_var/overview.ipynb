{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952945a4",
   "metadata": {},
   "source": [
    "# Vector Autoregression (VAR) from scratch (NumPy)\n",
    "\n",
    "## Goals\n",
    "- Model **multivariate time series** jointly (not one series at a time)\n",
    "- Understand and visualize **cross-dependencies** between variables\n",
    "- Understand **stationarity/stability** in multivariate systems\n",
    "- Select lag order `p` with information criteria\n",
    "- Implement VAR estimation + impulse response functions (IRFs) with **NumPy**\n",
    "- Visualize **variable interactions** and **IRFs** with **Plotly**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5a760",
   "metadata": {},
   "source": [
    "## 1) Multivariate time series modeling (what VAR is)\n",
    "\n",
    "A **multivariate** time series observes multiple variables at each time step.\n",
    "\n",
    "- Let the observation at time `t` be a vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\begin{bmatrix} y_{1,t}\\\\ y_{2,t}\\\\ \\vdots\\\\ y_{k,t}\\end{bmatrix} \\in \\mathbb{R}^k\n",
    "$$\n",
    "\n",
    "- A **VAR(p)** model explains current values using `p` past lags of *all* variables:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\mathbf{A}_2\\mathbf{y}_{t-2} + \\cdots + \\mathbf{A}_p\\mathbf{y}_{t-p} + \\mathbf{u}_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{c} \\in \\mathbb{R}^k$ is an intercept vector\n",
    "- $\\mathbf{A}_\\ell \\in \\mathbb{R}^{k\\times k}$ are coefficient matrices (one per lag)\n",
    "- $\\mathbf{u}_t \\in \\mathbb{R}^k$ is a zero-mean innovation (shock) with covariance $\\Sigma_u$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36ab71",
   "metadata": {},
   "source": [
    "## 2) Cross-dependencies between variables\n",
    "\n",
    "The key difference from fitting `k` separate AR models is that VAR includes **cross-lag terms**.\n",
    "\n",
    "For `k = 2` and `p = 1`:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_t \\\\ y_t \\end{bmatrix} =\n",
    "\\begin{bmatrix} c_x \\\\ c_y \\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x_{t-1} \\\\ y_{t-1} \\end{bmatrix} +\n",
    "\\begin{bmatrix} u_{x,t} \\\\ u_{y,t} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $a_{12}$ is the effect of **$y_{t-1}$ on $x_t$**\n",
    "- $a_{21}$ is the effect of **$x_{t-1}$ on $y_t$**\n",
    "\n",
    "These off-diagonal terms let VAR capture feedback loops (e.g., policy ↔ economy, supply ↔ price, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e104ae6",
   "metadata": {},
   "source": [
    "## 3) Stationarity / stability in multivariate systems\n",
    "\n",
    "For univariate AR models, stationarity relates to roots of a characteristic polynomial.\n",
    "For VAR, the analogous concept is **stability**: shocks should not explode over time.\n",
    "\n",
    "### VAR(1)\n",
    "$$\n",
    "\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\mathbf{u}_t\n",
    "$$\n",
    "A sufficient and (under mild assumptions) standard condition for stability is:\n",
    "\n",
    "$$\n",
    "\\rho(\\mathbf{A}_1) < 1\n",
    "$$\n",
    "\n",
    "where $\\rho(\\cdot)$ is the spectral radius (largest absolute eigenvalue).\n",
    "\n",
    "### VAR(p) companion form\n",
    "Define the **state vector**:\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_t = \\begin{bmatrix}\n",
    "\\mathbf{y}_t \\\\\n",
    "\\mathbf{y}_{t-1} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{y}_{t-p+1}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{kp}\n",
    "$$\n",
    "\n",
    "and the **companion matrix** $\\mathbf{F} \\in \\mathbb{R}^{kp\\times kp}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{A}_1 & \\mathbf{A}_2 & \\cdots & \\mathbf{A}_{p-1} & \\mathbf{A}_p \\\\\n",
    "\\mathbf{I}_k & \\mathbf{0} & \\cdots & \\mathbf{0} & \\mathbf{0} \\\\\n",
    "\\mathbf{0} & \\mathbf{I}_k & \\cdots & \\mathbf{0} & \\mathbf{0} \\\\\n",
    "\\vdots & & \\ddots & & \\vdots \\\\\n",
    "\\mathbf{0} & \\mathbf{0} & \\cdots & \\mathbf{I}_k & \\mathbf{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the system is stable/stationary if all eigenvalues of $\\mathbf{F}$ lie **inside the unit circle**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def companion_matrix(A_list: list[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Build the VAR(p) companion matrix F of shape (k*p, k*p).\"\"\"\n",
    "    if len(A_list) == 0:\n",
    "        raise ValueError(\"A_list must be non-empty\")\n",
    "\n",
    "    k = A_list[0].shape[0]\n",
    "    p = len(A_list)\n",
    "    if any(A.shape != (k, k) for A in A_list):\n",
    "        raise ValueError(\"All A_i must have shape (k, k)\")\n",
    "\n",
    "    F = np.zeros((k * p, k * p), dtype=float)\n",
    "    F[:k, : k * p] = np.concatenate(A_list, axis=1)\n",
    "    if p > 1:\n",
    "        F[k:, :-k] = np.eye(k * (p - 1))\n",
    "    return F\n",
    "\n",
    "\n",
    "def check_stationarity(A_list: list[np.ndarray]) -> dict:\n",
    "    \"\"\"Return eigenvalues and a stationarity flag based on the companion matrix.\"\"\"\n",
    "    F = companion_matrix(A_list)\n",
    "    eigvals = np.linalg.eigvals(F)\n",
    "    spectral_radius = float(np.max(np.abs(eigvals)))\n",
    "    return {\n",
    "        \"is_stationary\": spectral_radius < 1.0,\n",
    "        \"spectral_radius\": spectral_radius,\n",
    "        \"eigvals\": eigvals,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92a196",
   "metadata": {},
   "source": [
    "## 4) Low-level NumPy estimation (OLS)\n",
    "\n",
    "With observations $\\{\\mathbf{y}_t\\}_{t=1}^T$ and lag order `p`, define the stacked matrices:\n",
    "\n",
    "- Dependent block:\n",
    "$$\n",
    "\\mathbf{Y} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{y}_{p+1}^\\top \\\\\n",
    "\\mathbf{y}_{p+2}^\\top \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{y}_{T}^\\top\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{(T-p)\\times k}\n",
    "$$\n",
    "\n",
    "- Design matrix (constant + lags):\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "1 & \\mathbf{y}_{p}^\\top & \\mathbf{y}_{p-1}^\\top & \\cdots & \\mathbf{y}_{1}^\\top \\\\\n",
    "1 & \\mathbf{y}_{p+1}^\\top & \\mathbf{y}_{p}^\\top & \\cdots & \\mathbf{y}_{2}^\\top \\\\\n",
    "\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & \\mathbf{y}_{T-1}^\\top & \\mathbf{y}_{T-2}^\\top & \\cdots & \\mathbf{y}_{T-p}^\\top \n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{(T-p)\\times (1+kp)}\n",
    "$$\n",
    "\n",
    "Collect coefficients into:\n",
    "$$\n",
    "\\mathbf{B} = \\begin{bmatrix} \\mathbf{c} & \\mathbf{A}_1 & \\cdots & \\mathbf{A}_p \\end{bmatrix}^\\top \\in \\mathbb{R}^{(1+kp)\\times k}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X}\\mathbf{B} + \\mathbf{U}\n",
    "$$\n",
    "\n",
    "and the OLS estimator is:\n",
    "$$\n",
    "\\hat{\\mathbf{B}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y}\n",
    "$$\n",
    "\n",
    "We’ll compute it with `np.linalg.lstsq` (numerically safer than forming the inverse explicitly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_var_matrices(y: np.ndarray, p: int, include_const: bool = True) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Construct (X, Y) for VAR(p) with rows as time and columns as variables.\n",
    "\n",
    "    y: (T, k)\n",
    "    X: (T-p, (1 if include_const else 0) + k*p)\n",
    "    Y: (T-p, k)\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if y.ndim != 2:\n",
    "        raise ValueError(\"y must have shape (T, k)\")\n",
    "    if p < 1:\n",
    "        raise ValueError(\"p must be >= 1\")\n",
    "\n",
    "    n_steps, k = y.shape\n",
    "    if n_steps <= p:\n",
    "        raise ValueError(\"Need more observations than p\")\n",
    "\n",
    "    Y = y[p:, :]\n",
    "    X_parts = []\n",
    "    if include_const:\n",
    "        X_parts.append(np.ones((n_steps - p, 1)))\n",
    "    for lag in range(1, p + 1):\n",
    "        X_parts.append(y[p - lag : n_steps - lag, :])\n",
    "    X = np.concatenate(X_parts, axis=1)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def fit_var_ols(y: np.ndarray, p: int, include_const: bool = True) -> dict:\n",
    "    \"\"\"Fit VAR(p) with equation-by-equation OLS (equivalently multivariate least squares).\"\"\"\n",
    "    X, Y = build_var_matrices(y=y, p=p, include_const=include_const)\n",
    "    beta_hat, *_ = np.linalg.lstsq(X, Y, rcond=None)  # (n_features, k)\n",
    "\n",
    "    Y_hat = X @ beta_hat\n",
    "    residuals = Y - Y_hat\n",
    "\n",
    "    effective_T = Y.shape[0]\n",
    "    Sigma_u = (residuals.T @ residuals) / effective_T\n",
    "\n",
    "    k = Y.shape[1]\n",
    "    start = 0\n",
    "    if include_const:\n",
    "        intercept = beta_hat[0, :]\n",
    "        start = 1\n",
    "    else:\n",
    "        intercept = np.zeros(k)\n",
    "\n",
    "    A_list = []\n",
    "    for lag_index in range(p):\n",
    "        block = beta_hat[start + lag_index * k : start + (lag_index + 1) * k, :]  # (k, k)\n",
    "        A_list.append(block.T)\n",
    "\n",
    "    return {\n",
    "        \"p\": p,\n",
    "        \"k\": k,\n",
    "        \"include_const\": include_const,\n",
    "        \"beta\": beta_hat,\n",
    "        \"intercept\": intercept,\n",
    "        \"A\": A_list,\n",
    "        \"residuals\": residuals,\n",
    "        \"Sigma_u\": Sigma_u,\n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a41c82",
   "metadata": {},
   "source": [
    "## 5) Lag selection (choosing `p`)\n",
    "\n",
    "Increasing `p` makes the model more flexible, but it increases parameters quickly:\n",
    "- With `k` variables, each lag adds `k×k` coefficients.\n",
    "- With `p` lags, coefficients are `k^2 p` (plus `k` intercepts).\n",
    "\n",
    "A common approach is to fit VAR models for `p = 1..p_max` and pick the lag that minimizes an information criterion.\n",
    "\n",
    "Using the residual covariance estimate $\\hat{\\Sigma}_u(p)$ and parameter count $m(p) = k^2 p + k$ (if intercept included):\n",
    "\n",
    "$$\n",
    "\\mathrm{AIC}(p) = \\log\\det\\hat{\\Sigma}_u(p) + \\frac{2m(p)}{T_p}\n",
    "$$\n",
    "$$\n",
    "\\mathrm{BIC}(p) = \\log\\det\\hat{\\Sigma}_u(p) + \\frac{\\log(T_p)\\,m(p)}{T_p}\n",
    "$$\n",
    "\n",
    "where $T_p = T - p$ is the number of usable rows after lagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _slogdet_psd(matrix: np.ndarray, jitter: float = 1e-10, max_tries: int = 6) -> float:\n",
    "    \"\"\"Stable log(det(.)) for covariance-like matrices, with diagonal jitter if needed.\"\"\"\n",
    "    matrix = np.asarray(matrix, dtype=float)\n",
    "    if matrix.shape[0] != matrix.shape[1]:\n",
    "        raise ValueError(\"matrix must be square\")\n",
    "\n",
    "    jitter_value = float(jitter)\n",
    "    for _ in range(max_tries):\n",
    "        sign, logdet = np.linalg.slogdet(matrix)\n",
    "        if sign > 0:\n",
    "            return float(logdet)\n",
    "        matrix = matrix + jitter_value * np.eye(matrix.shape[0])\n",
    "        jitter_value *= 10.0\n",
    "\n",
    "    raise np.linalg.LinAlgError(\"slogdet failed: matrix not positive definite after jitter\")\n",
    "\n",
    "\n",
    "def information_criteria_var(y: np.ndarray, p: int, include_const: bool = True) -> dict:\n",
    "    model = fit_var_ols(y=y, p=p, include_const=include_const)\n",
    "    k = model[\"k\"]\n",
    "    T_p = model[\"Y\"].shape[0]\n",
    "\n",
    "    n_params = (k * k * p) + (k if include_const else 0)\n",
    "    logdet = _slogdet_psd(model[\"Sigma_u\"])\n",
    "\n",
    "    aic = logdet + 2.0 * n_params / T_p\n",
    "    bic = logdet + np.log(T_p) * n_params / T_p\n",
    "    return {\n",
    "        \"p\": p,\n",
    "        \"aic\": float(aic),\n",
    "        \"bic\": float(bic),\n",
    "        \"n_params\": int(n_params),\n",
    "        \"T_p\": int(T_p),\n",
    "    }\n",
    "\n",
    "\n",
    "def select_lag(y: np.ndarray, p_max: int, include_const: bool = True) -> tuple[pd.DataFrame, dict]:\n",
    "    records = [information_criteria_var(y=y, p=p, include_const=include_const) for p in range(1, p_max + 1)]\n",
    "    table = pd.DataFrame.from_records(records).set_index(\"p\").sort_index()\n",
    "    best = {\n",
    "        \"aic\": int(table[\"aic\"].idxmin()),\n",
    "        \"bic\": int(table[\"bic\"].idxmin()),\n",
    "    }\n",
    "    return table, best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41399a6",
   "metadata": {},
   "source": [
    "## 6) Intuition: an interacting system (simulate a stable VAR)\n",
    "\n",
    "Think of a VAR as a simple discrete-time dynamical system:\n",
    "\n",
    "- Each variable responds to its own past (**inertia / momentum**)\n",
    "- Each variable can respond to other variables’ past (**coupling / interaction**)\n",
    "\n",
    "We’ll simulate a 3-variable VAR(2) with cross-effects, then:\n",
    "1) choose a lag order\n",
    "2) fit the coefficients\n",
    "3) visualize interactions and IRFs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fce232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cholesky_with_jitter(matrix: np.ndarray, jitter: float = 1e-10, max_tries: int = 6) -> np.ndarray:\n",
    "    matrix = np.asarray(matrix, dtype=float)\n",
    "    jitter_value = float(jitter)\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            return np.linalg.cholesky(matrix)\n",
    "        except np.linalg.LinAlgError:\n",
    "            matrix = matrix + jitter_value * np.eye(matrix.shape[0])\n",
    "            jitter_value *= 10.0\n",
    "    raise\n",
    "\n",
    "\n",
    "def simulate_var(\n",
    "    A_list: list[np.ndarray],\n",
    "    Sigma_u: np.ndarray,\n",
    "    n_steps: int,\n",
    "    *,\n",
    "    intercept: np.ndarray | None = None,\n",
    "    burn_in: int = 200,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Simulate y_t = c + sum_i A_i y_{t-i} + u_t.\n",
    "\n",
    "    Returns y with shape (n_steps, k).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    k = A_list[0].shape[0]\n",
    "    p = len(A_list)\n",
    "    if intercept is None:\n",
    "        intercept = np.zeros(k)\n",
    "\n",
    "    chol = cholesky_with_jitter(Sigma_u)\n",
    "\n",
    "    y = np.zeros((n_steps + burn_in + p, k), dtype=float)\n",
    "    for t in range(p, y.shape[0]):\n",
    "        ar_part = np.zeros(k)\n",
    "        for lag_index, A in enumerate(A_list, start=1):\n",
    "            ar_part = ar_part + A @ y[t - lag_index]\n",
    "        shock = chol @ rng.standard_normal(k)\n",
    "        y[t] = intercept + ar_part + shock\n",
    "\n",
    "    return y[burn_in + p :, :]\n",
    "\n",
    "\n",
    "names = [\"Output\", \"Inflation\", \"PolicyRate\"]\n",
    "\n",
    "# A stable VAR(2) with cross-dependencies (off-diagonals)\n",
    "A1_true = np.array(\n",
    "    [\n",
    "        [0.55, 0.10, -0.15],\n",
    "        [0.15, 0.45, 0.05],\n",
    "        [-0.20, 0.25, 0.40],\n",
    "    ]\n",
    ")\n",
    "A2_true = np.array(\n",
    "    [\n",
    "        [-0.15, 0.00, 0.00],\n",
    "        [0.00, -0.10, 0.00],\n",
    "        [0.00, 0.00, -0.10],\n",
    "    ]\n",
    ")\n",
    "\n",
    "Sigma_u_true = np.array(\n",
    "    [\n",
    "        [1.00, 0.30, 0.15],\n",
    "        [0.30, 1.00, 0.10],\n",
    "        [0.15, 0.10, 1.00],\n",
    "    ]\n",
    ")\n",
    "\n",
    "stationarity_true = check_stationarity([A1_true, A2_true])\n",
    "stationarity_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = simulate_var([A1_true, A2_true], Sigma_u=Sigma_u_true, n_steps=800, rng=rng)\n",
    "df = pd.DataFrame(y, columns=names)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for name in names:\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df[name], mode=\"lines\", name=name))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Simulated interacting system (VAR(2))\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"value\",\n",
    "    legend_title=\"variable\",\n",
    "    height=350,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b65fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_table, best = select_lag(df.values, p_max=6, include_const=True)\n",
    "ic_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feeb579",
   "metadata": {},
   "outputs": [],
   "source": [
    "best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a10ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = best[\"bic\"]\n",
    "model = fit_var_ols(df.values, p=p_hat, include_const=True)\n",
    "\n",
    "stationarity_hat = check_stationarity(model[\"A\"])\n",
    "{k: stationarity_hat[k] for k in [\"is_stationary\", \"spectral_radius\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_var_coefficients(A_list: list[np.ndarray], names: list[str]) -> go.Figure:\n",
    "    k = A_list[0].shape[0]\n",
    "    p = len(A_list)\n",
    "    zmax = max(float(np.max(np.abs(A))) for A in A_list)\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=p,\n",
    "        shared_yaxes=True,\n",
    "        subplot_titles=[f\"Lag {i}\" for i in range(1, p + 1)],\n",
    "    )\n",
    "\n",
    "    for col, A in enumerate(A_list, start=1):\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=A,\n",
    "                x=names,\n",
    "                y=names,\n",
    "                colorscale=\"RdBu\",\n",
    "                zmin=-zmax,\n",
    "                zmax=zmax,\n",
    "                showscale=col == 1,\n",
    "                colorbar=dict(title=\"coef\"),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Estimated variable interactions (A matrices)\",\n",
    "        height=360,\n",
    "        width=320 * p,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Source (lagged variable)\")\n",
    "    fig.update_yaxes(title_text=\"Target (current variable)\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "plot_var_coefficients(model[\"A\"], names).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3e3d4",
   "metadata": {},
   "source": [
    "## 7) Impulse response functions (IRFs)\n",
    "\n",
    "IRFs answer: *“If I shock variable `j` today, how do variables respond over future horizons?”*\n",
    "\n",
    "A VAR has a moving-average representation:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\mu + \\sum_{h=0}^{\\infty} \\Psi_h\\,\\mathbf{u}_{t-h}\n",
    "$$\n",
    "\n",
    "where each $\\Psi_h \\in \\mathbb{R}^{k\\times k}$ maps a shock at horizon `h` to responses.\n",
    "\n",
    "- For stable VARs, $\\Psi_h \\to 0$ as $h \\to \\infty$.\n",
    "- If shocks are correlated ($\\Sigma_u$ not diagonal), IRFs are often **orthogonalized** via a Cholesky factorization of $\\Sigma_u$.\n",
    "  This introduces an ordering assumption (the variable order matters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e47074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irf_matrices(A_list: list[np.ndarray], horizons: int) -> np.ndarray:\n",
    "    \"\"\"Return Psi_h for h=0..H with shape (H+1, k, k).\"\"\"\n",
    "    k = A_list[0].shape[0]\n",
    "    p = len(A_list)\n",
    "    F = companion_matrix(A_list)\n",
    "\n",
    "    Psi = np.empty((horizons + 1, k, k), dtype=float)\n",
    "    F_power = np.eye(k * p)\n",
    "    for h in range(horizons + 1):\n",
    "        Psi[h] = F_power[:k, :k]\n",
    "        F_power = F_power @ F\n",
    "    return Psi\n",
    "\n",
    "\n",
    "def orthogonalize_irf(Psi: np.ndarray, Sigma_u: np.ndarray) -> np.ndarray:\n",
    "    chol = cholesky_with_jitter(Sigma_u)\n",
    "    return Psi @ chol\n",
    "\n",
    "\n",
    "def plot_irf_grid(Psi: np.ndarray, names: list[str], *, title: str) -> go.Figure:\n",
    "    horizons = Psi.shape[0] - 1\n",
    "    k = Psi.shape[1]\n",
    "    h = np.arange(horizons + 1)\n",
    "\n",
    "    subplot_titles = [f\"{names[i]} ← {names[j]}\" for i in range(k) for j in range(k)]\n",
    "    fig = make_subplots(rows=k, cols=k, shared_xaxes=True, subplot_titles=subplot_titles)\n",
    "\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=h,\n",
    "                    y=Psi[:, i, j],\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=2),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=h,\n",
    "                    y=np.zeros_like(h),\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(color=\"black\", width=1, dash=\"dot\"),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(title=title, height=240 * k, width=240 * k)\n",
    "    for row in range(1, k + 1):\n",
    "        fig.update_yaxes(title_text=\"response\", row=row, col=1)\n",
    "    for col in range(1, k + 1):\n",
    "        fig.update_xaxes(title_text=\"horizon\", row=k, col=col)\n",
    "    return fig\n",
    "\n",
    "\n",
    "H = 20\n",
    "Psi = irf_matrices(model[\"A\"], horizons=H)\n",
    "Psi_orth = orthogonalize_irf(Psi, Sigma_u=model[\"Sigma_u\"])\n",
    "\n",
    "plot_irf_grid(Psi_orth, names=names, title=\"Orthogonalized IRFs (Cholesky, order = column order)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9689593",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "- VAR models multiple time series jointly and captures **cross-dependencies** via off-diagonal coefficients in $\\mathbf{A}_\\ell$.\n",
    "- Multivariate stationarity is a **stability** condition: eigenvalues of the companion matrix must be inside the unit circle.\n",
    "- Lag selection trades bias/variance; AIC often prefers larger models, BIC is more conservative.\n",
    "- IRFs make dynamics interpretable; orthogonalization requires an identification assumption (ordering matters).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}