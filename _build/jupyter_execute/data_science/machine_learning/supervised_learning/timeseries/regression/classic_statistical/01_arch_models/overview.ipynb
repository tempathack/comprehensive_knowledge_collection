{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953683a6",
   "metadata": {},
   "source": [
    "# ARCH Models (Volatility Models)\n",
    "\n",
    "## Goals\n",
    "- Precisely define *conditional heteroskedasticity*.\n",
    "- Explain why we model **variance** (volatility) instead of the **mean** for many financial return series.\n",
    "- Build ARCH(q) variance equations in LaTeX and implement them from scratch in NumPy.\n",
    "- Visualize **volatility clustering** and **variance forecasts** with Plotly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e55b4",
   "metadata": {},
   "source": [
    "## Conditional heteroskedasticity (definition)\n",
    "\n",
    "Let $r_t$ be a return (or mean-zero residual) and $\\mathcal{F}_{t-1}$ the information available up to time $t-1$.\n",
    "\n",
    "A standard decomposition is:\n",
    "\n",
    "$$\n",
    "r_t = \\mu + \\varepsilon_t,\n",
    "\\qquad \\mathbb{E}[\\varepsilon_t\\mid\\mathcal{F}_{t-1}] = 0,\n",
    "\\qquad \\operatorname{Var}(\\varepsilon_t\\mid\\mathcal{F}_{t-1}) = h_t.\n",
    "$$\n",
    "\n",
    "- **Homoskedastic** models assume $h_t = \\sigma^2$ is constant.\n",
    "- **Conditionally heteroskedastic** models assume $h_t$ is **time-varying** and **depends on past information** $\\mathcal{F}_{t-1}$.\n",
    "\n",
    "ARCH/GARCH models explicitly specify a recursion for $h_t$ (the conditional variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192c1e9",
   "metadata": {},
   "source": [
    "## Why model variance (volatility) instead of the mean?\n",
    "\n",
    "For many liquid financial assets at daily (or higher) frequency:\n",
    "- The **conditional mean** $\\mathbb{E}[r_t\\mid\\mathcal{F}_{t-1}]$ is often small and hard to predict reliably.\n",
    "- The **conditional variance** $h_t$ shows strong, stable structure (persistence / clustering), making it forecastable.\n",
    "- Many decisions depend more on *risk* than *drift*: even if the expected return is near 0, the **uncertainty is not**.\n",
    "\n",
    "### Financial use cases\n",
    "- **Risk management**: Value-at-Risk (VaR), Expected Shortfall, stress testing, margin.\n",
    "- **Portfolio construction**: volatility targeting, risk parity, dynamic leverage.\n",
    "- **Derivatives**: volatility inputs/forecasts (and a bridge to option-implied volatility).\n",
    "- **Trading & execution**: position sizing, stop placement, liquidity/impact models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa04016",
   "metadata": {},
   "source": [
    "## Volatility clustering + market-shock intuition\n",
    "\n",
    "**Volatility clustering**: large moves tend to be followed by large moves (of either sign), and small moves tend to be followed by small moves.\n",
    "\n",
    "A simple shock story:\n",
    "- Suppose a big negative news event hits at time $t$.\n",
    "- The return shock $\\varepsilon_t$ is large in magnitude, so $\\varepsilon_t^2$ is large.\n",
    "- In ARCH models, the next conditional variance $h_{t+1}$ increases because it *loads on* past squared shocks.\n",
    "- With higher $h_{t+1}$, the next innovation $\\varepsilon_{t+1}$ is more likely to be large in magnitude, so clusters form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbd620",
   "metadata": {},
   "source": [
    "## ARCH(q) model (LaTeX)\n",
    "\n",
    "ARCH models specify the conditional variance as a weighted sum of recent squared innovations.\n",
    "\n",
    "$$\n",
    "\\varepsilon_t = z_t\\sqrt{h_t}, \\qquad z_t\\ \\text{i.i.d. with}\\ \\mathbb{E}[z_t]=0,\\ \\operatorname{Var}(z_t)=1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = \\omega + \\sum_{i=1}^{q}\\alpha_i\\varepsilon_{t-i}^2\n",
    "$$\n",
    "\n",
    "### Parameter constraints (to keep $h_t$ positive)\n",
    "- $\\omega > 0$\n",
    "- $\\alpha_i \\ge 0$ for all $i$\n",
    "\n",
    "### Stationarity / finite unconditional variance (common condition)\n",
    "A typical **second-order stationarity** condition is:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{q}\\alpha_i < 1\n",
    "$$\n",
    "\n",
    "which implies the long-run (unconditional) variance:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[h_t] = \\operatorname{Var}(\\varepsilon_t) = \\frac{\\omega}{1-\\sum_{i=1}^{q}\\alpha_i}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef8110",
   "metadata": {},
   "source": [
    "## Variance equation breakdown (what each term does)\n",
    "\n",
    "$$\n",
    "h_t = \\underbrace{\\omega}_{\\text{baseline variance}} + \\sum_{i=1}^{q}\\underbrace{\\alpha_i}_{\\text{news/shock weight}}\\underbrace{\\varepsilon_{t-i}^2}_{\\text{recent shock size}}\n",
    "$$\n",
    "\n",
    "- $\\omega$ sets the baseline scale (and pins down the long-run variance when combined with $\\alpha$'s).\n",
    "- $\\alpha_i$ controls how strongly a past shock affects current volatility.\n",
    "- Squaring removes the sign: both positive and negative shocks increase variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_arch(T: int, omega: float, alpha, mu: float = 0.0, burn: int = 300, seed: int = 7):\n",
    "    \"\"\"Simulate an ARCH(q) process with Gaussian innovations.\n",
    "\n",
    "    Model:\n",
    "      r_t = mu + eps_t\n",
    "      eps_t = z_t * sqrt(h_t)\n",
    "      h_t = omega + sum_{i=1..q} alpha_i * eps_{t-i}^2\n",
    "    \"\"\"\n",
    "    alpha = np.asarray(alpha, dtype=float)\n",
    "    q = int(alpha.size)\n",
    "    if q < 1:\n",
    "        raise ValueError(\"alpha must have at least one element (q>=1)\")\n",
    "    if omega <= 0:\n",
    "        raise ValueError(\"omega must be > 0\")\n",
    "    if np.any(alpha < 0):\n",
    "        raise ValueError(\"all alpha_i must be >= 0\")\n",
    "\n",
    "    alpha_sum = float(alpha.sum())\n",
    "    if alpha_sum >= 1:\n",
    "        raise ValueError(\"Need sum(alpha) < 1 for finite unconditional variance in this demo\")\n",
    "\n",
    "    h_bar = omega / (1.0 - alpha_sum)\n",
    "    n = int(T + burn)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    z = rng.standard_normal(n)\n",
    "\n",
    "    eps = np.zeros(n)\n",
    "    h = np.full(n, h_bar)\n",
    "\n",
    "    eps[:q] = np.sqrt(h_bar) * z[:q]\n",
    "    for t in range(q, n):\n",
    "        # h_t = omega + sum_i alpha_i * eps_{t-i}^2\n",
    "        lagged_eps_sq = eps[t - np.arange(1, q + 1)] ** 2\n",
    "        h[t] = omega + float(np.dot(alpha, lagged_eps_sq))\n",
    "        eps[t] = np.sqrt(h[t]) * z[t]\n",
    "\n",
    "    eps = eps[burn:]\n",
    "    h = h[burn:]\n",
    "    r = mu + eps\n",
    "    return r, eps, h\n",
    "\n",
    "\n",
    "def arch_conditional_variance(eps, omega: float, alpha, initial_variance: float | None = None):\n",
    "    \"\"\"Compute h_t for an ARCH(q) model given a residual series eps_t.\"\"\"\n",
    "    eps = np.asarray(eps, dtype=float)\n",
    "    alpha = np.asarray(alpha, dtype=float)\n",
    "    q = int(alpha.size)\n",
    "    if q < 1:\n",
    "        raise ValueError(\"alpha must have at least one element (q>=1)\")\n",
    "    if omega <= 0:\n",
    "        raise ValueError(\"omega must be > 0\")\n",
    "    if np.any(alpha < 0):\n",
    "        raise ValueError(\"all alpha_i must be >= 0\")\n",
    "\n",
    "    if initial_variance is None:\n",
    "        alpha_sum = float(alpha.sum())\n",
    "        if alpha_sum >= 1:\n",
    "            raise ValueError(\"Need sum(alpha) < 1 to use the unconditional variance as initialization\")\n",
    "        initial_variance = omega / (1.0 - alpha_sum)\n",
    "\n",
    "    h = np.full(eps.size, float(initial_variance))\n",
    "    for t in range(q, eps.size):\n",
    "        lagged_eps_sq = eps[t - np.arange(1, q + 1)] ** 2\n",
    "        h[t] = omega + float(np.dot(alpha, lagged_eps_sq))\n",
    "    return h\n",
    "\n",
    "\n",
    "def arch_forecast_variance(eps, omega: float, alpha, horizon: int):\n",
    "    \"\"\"Multi-step variance forecasts for ARCH(q).\n",
    "\n",
    "    Uses E[eps_{t+k}^2 | F_t] = h_{t+k|t} (since Var(z)=1).\n",
    "    \"\"\"\n",
    "    eps = np.asarray(eps, dtype=float)\n",
    "    alpha = np.asarray(alpha, dtype=float)\n",
    "    q = int(alpha.size)\n",
    "    if eps.size < q:\n",
    "        raise ValueError(\"Need at least q residuals to forecast\")\n",
    "\n",
    "    eps_sq_lags = eps[-np.arange(1, q + 1)] ** 2  # [eps_t^2, eps_{t-1}^2, ...]\n",
    "    forecasts = np.zeros(int(horizon), dtype=float)\n",
    "    for k in range(int(horizon)):\n",
    "        h_next = omega + float(np.dot(alpha, eps_sq_lags))\n",
    "        forecasts[k] = h_next\n",
    "        eps_sq_lags = np.concatenate([[h_next], eps_sq_lags[:-1]])\n",
    "    return forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: simulate an ARCH(2) return series to visualize volatility clustering\n",
    "T = 2000\n",
    "mu = 0.0\n",
    "omega = 0.05\n",
    "alpha = np.array([0.25, 0.15])  # sum < 1\n",
    "\n",
    "r, eps, h = simulate_arch(T=T, omega=omega, alpha=alpha, mu=mu, seed=42)\n",
    "\n",
    "view = 600\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"t\": np.arange(T),\n",
    "        \"return\": r,\n",
    "        \"eps_sq\": eps**2,\n",
    "        \"h\": h,\n",
    "        \"sigma\": np.sqrt(h),\n",
    "    }\n",
    ").tail(view)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.06,\n",
    "    subplot_titles=(\"Simulated returns\", \"Volatility clustering: squared returns vs conditional variance\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"t\"], y=df[\"return\"], mode=\"lines\", name=\"r_t\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df[\"t\"], y=df[\"eps_sq\"], mode=\"lines\", name=r\"$\\varepsilon_t^2$\", line=dict(width=1)),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df[\"t\"], y=df[\"h\"], mode=\"lines\", name=r\"$h_t$\", line=dict(width=2)),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"return\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"variance\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"time\", row=2, col=1)\n",
    "fig.update_layout(height=650, legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance forecasts: h_{T+k | T}\n",
    "horizon = 60\n",
    "h_fore = arch_forecast_variance(eps, omega=omega, alpha=alpha, horizon=horizon)\n",
    "\n",
    "alpha_sum = float(alpha.sum())\n",
    "h_bar = omega / (1.0 - alpha_sum)\n",
    "\n",
    "lookback = 250\n",
    "hist_x = np.arange(T - lookback, T)\n",
    "fore_x = np.arange(T, T + horizon)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist_x, y=h[-lookback:], mode=\"lines\", name=r\"historical $h_t$\"))\n",
    "fig.add_trace(go.Scatter(x=fore_x, y=h_fore, mode=\"lines\", name=r\"forecast $h_{T+k|T}$\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.concatenate([hist_x, fore_x]),\n",
    "        y=np.full(hist_x.size + fore_x.size, h_bar),\n",
    "        mode=\"lines\",\n",
    "        name=r\"long-run $\\bar h$\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ARCH variance forecast (mean reversion toward long-run variance)\",\n",
    "    xaxis_title=\"time\",\n",
    "    yaxis_title=\"variance\",\n",
    "    height=450,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e976d",
   "metadata": {},
   "source": [
    "## Notes / pitfalls\n",
    "\n",
    "- ARCH models react to *recent* shocks but can require a large $q$ to capture the long persistence seen in markets.\n",
    "- A frequent extension is **GARCH**, which adds lagged variance terms to get slow decay with few parameters.\n",
    "- Stationarity conditions above target *finite variance*; stricter stationarity conditions can be more nuanced.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}