{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7bf82a",
   "metadata": {},
   "source": [
    "# 01 Moving Average (MA) Model\n",
    "\n",
    "## Goals\n",
    "- Define the MA($q$) model (and how it differs from a rolling mean).\n",
    "- Explain the generative “algorithm” and the math behind it.\n",
    "- Understand assumptions, requirements, and *invertibility*.\n",
    "- Implement MA simulation/inversion in NumPy.\n",
    "- Visualize the effect of order, impulse response, and residual autocorrelation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cb44a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Basic probability (expectation, variance, covariance)\n",
    "- Time series basics (lags, autocorrelation)\n",
    "- Python: NumPy + Plotly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c722782",
   "metadata": {},
   "source": [
    "## Model definition (MA($q$))\n",
    "\n",
    "An MA($q$) model is a **finite linear filter of white-noise shocks**. It is *not* the same thing as the rolling/simple moving average used for smoothing.\n",
    "\n",
    "### Time-domain form\n",
    "\\[\n",
    "X_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots + \\theta_q \\varepsilon_{t-q},\n",
    "\\qquad \\varepsilon_t \\sim \\text{WN}(0,\\sigma^2).\n",
    "\\]\n",
    "\n",
    "Convenient shorthand: set \\(\\theta_0 = 1\\) so\n",
    "\\[\n",
    "X_t - \\mu = \\sum_{i=0}^q \\theta_i \\varepsilon_{t-i}.\n",
    "\\]\n",
    "\n",
    "### Backshift-operator form\n",
    "Let \\(B X_t = X_{t-1}\\). Then\n",
    "\\[\n",
    "X_t - \\mu = \\Theta(B)\\,\\varepsilon_t,\n",
    "\\qquad \\Theta(B) = 1 + \\theta_1 B + \\cdots + \\theta_q B^q.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b6df6",
   "metadata": {},
   "source": [
    "## What the “algorithm” is\n",
    "\n",
    "### 1) Generating (simulating) an MA($q$)\n",
    "For each time step \\(t\\):\n",
    "1. Draw a new shock \\(\\varepsilon_t\\).\n",
    "2. Output \\(X_t\\) as a weighted sum of the current and last \\(q\\) shocks.\n",
    "\n",
    "In other words, MA($q$) is an **FIR (finite impulse response) filter** applied to white noise.\n",
    "\n",
    "### 2) Using MA($q$) as a statistical model (Box–Jenkins view)\n",
    "- **Identify** \\(q\\): the ACF often shows a sharp cutoff after lag \\(q\\) (heuristic).\n",
    "- **Estimate** parameters: typically via (Gaussian) maximum likelihood / innovations algorithms.\n",
    "- **Diagnose** fit: residuals (estimated shocks) should look like white noise; residual ACF should be near 0.\n",
    "- **Forecast**: beyond \\(q\\) steps, the optimal forecast tends back to \\(\\mu\\) because future shocks have mean 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895613b",
   "metadata": {},
   "source": [
    "## How it works mathematically (step-by-step)\n",
    "\n",
    "Write the centered process \\(Y_t = X_t - \\mu = \\sum_{i=0}^q \\theta_i \\varepsilon_{t-i}\\).\n",
    "\n",
    "### Mean\n",
    "\\[\n",
    "\\mathbb{E}[X_t] = \\mu + \\sum_{i=0}^q \\theta_i\\,\\mathbb{E}[\\varepsilon_{t-i}] = \\mu.\n",
    "\\]\n",
    "\n",
    "### Autocovariance\n",
    "For lag \\(k \\ge 0\\):\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\gamma(k)\n",
    "&= \\text{Cov}(Y_t, Y_{t-k})\\\\\n",
    "&= \\text{Cov}\\Big(\\sum_{i=0}^q \\theta_i \\varepsilon_{t-i},\\ \\sum_{j=0}^q \\theta_j \\varepsilon_{t-k-j}\\Big)\\\\\n",
    "&= \\sum_{i=0}^q \\sum_{j=0}^q \\theta_i\\theta_j\\,\\text{Cov}(\\varepsilon_{t-i}, \\varepsilon_{t-k-j}).\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Because \\(\\{\\varepsilon_t\\}\\) is white noise,\n",
    "\\(\\text{Cov}(\\varepsilon_{t-i}, \\varepsilon_{t-k-j}) = \\sigma^2\\) only when \\(t-i = t-k-j\\) (i.e. \\(j=i-k\\)), otherwise it is 0.\n",
    "That leaves only overlapping terms:\n",
    "\\[\n",
    "\\gamma(k) = \\sigma^2 \\sum_{i=0}^{q-k} \\theta_i\\theta_{i+k},\n",
    "\\qquad k=0,1,\\dots,q.\n",
    "\\]\n",
    "For \\(|k|>q\\), there is no overlap in shocks, so \\(\\gamma(k)=0\\).\n",
    "\n",
    "### Autocorrelation (ACF)\n",
    "\\[\n",
    "\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}.\n",
    "\\]\n",
    "Key takeaway: **the theoretical ACF of MA($q$) is exactly 0 for lags \\(|k|>q\\)** (\"cutoff\").\n",
    "\n",
    "### Impulse response\n",
    "If a single unit shock happens at time \\(t\\) (\\(\\varepsilon_t=1\\), all other shocks 0), then\n",
    "\\[\n",
    "X_{t+h} - \\mu = \\theta_h\\ \\text{for } h=0,1,\\dots,q,\\quad \\text{and } 0 \\text{ thereafter},\n",
    "\\]\n",
    "where \\(\\theta_0=1\\). So MA models have **finite shock memory**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea29ea",
   "metadata": {},
   "source": [
    "## How and when it is used (and what it models)\n",
    "\n",
    "MA($q$) is used when **dependence comes from short-lived disturbances** rather than long-run persistence.\n",
    "\n",
    "Typical scenarios / incident and noise patterns:\n",
    "- **Measurement noise with short carryover** (sensor “ringing”, smoothing/filtering artifacts).\n",
    "- **One-off incidents** that affect a few subsequent observations (temporary outage recovery, delayed reporting).\n",
    "- **Aggregation / batching effects** (value at time \\(t\\) mixes several recent random contributions).\n",
    "- **Microstructure noise** in finance (e.g., bid–ask bounce creates short-range negative autocorrelation).\n",
    "\n",
    "Practical modeling:\n",
    "- Standalone MA models are common for short-memory series.\n",
    "- More often, MA terms appear inside **ARMA/ARIMA/SARIMA** models to capture short-run shock structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f6c8b",
   "metadata": {},
   "source": [
    "## Model assumptions and requirements\n",
    "\n",
    "Core assumptions (for the MA part):\n",
    "- Innovations \\(\\varepsilon_t\\) have **zero mean**, **constant variance** \\(\\sigma^2\\), and are **uncorrelated** over time (often assumed i.i.d.).\n",
    "- Parameters \\(\\mu,\\theta_1,\\dots,\\theta_q\\) are constant over the sample.\n",
    "- The relationship is **linear** and time-invariant.\n",
    "\n",
    "Notes:\n",
    "- **Stationarity**: any finite MA($q$) with finite-variance shocks is (weakly) stationary; no restrictions on \\(\\theta\\) are needed for stationarity.\n",
    "- For likelihood-based estimation and standard errors, people often assume **Gaussian** shocks; without Gaussianity, Gaussian MLE is still a common quasi-MLE.\n",
    "- Estimation typically needs a sample size meaningfully larger than \\(q\\), and diagnostics rely on having enough data to see residual autocorrelation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22dcff",
   "metadata": {},
   "source": [
    "## Invertibility\n",
    "\n",
    "MA parameters are not always uniquely identified from second-order moments unless we impose **invertibility**.\n",
    "\n",
    "Define the MA polynomial\n",
    "\\[\n",
    "\\Theta(z) = 1 + \\theta_1 z + \\cdots + \\theta_q z^q.\n",
    "\\]\n",
    "The MA($q$) is **invertible** if all roots of \\(\\Theta(z)\\) lie **outside** the unit circle:\n",
    "\\[\n",
    "|z_i| > 1 \\quad \\text{for every root } z_i.\n",
    "\\]\n",
    "\n",
    "Why it matters:\n",
    "- It guarantees a **stable inverse filter**, so shocks can be recovered from data:\n",
    "  \\(\\varepsilon_t = \\Theta(B)^{-1}(X_t-\\mu)\\), an AR($\\infty$) expansion with absolutely summable coefficients.\n",
    "- It makes the MA representation **unique** (no parameter aliases).\n",
    "\n",
    "### MA(1) intuition\n",
    "For MA(1): \\(X_t-\\mu = (1+\\theta B)\\varepsilon_t\\).\n",
    "If \\(|\\theta|<1\\), we can expand the inverse as a convergent series:\n",
    "\\[\n",
    "(1+\\theta B)^{-1} = 1 - \\theta B + \\theta^2 B^2 - \\theta^3 B^3 + \\cdots.\n",
    "\\]\n",
    "If \\(|\\theta|\\ge 1\\), this inverse does not converge, and computing shocks from observations becomes unstable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ee31f",
   "metadata": {},
   "source": [
    "## Intuition behind the error terms (innovations)\n",
    "\n",
    "The shocks \\(\\varepsilon_t\\) represent **new information** that was not predictable from the past.\n",
    "An MA model says: the observation today is the *current shock plus echoes of a few recent shocks*.\n",
    "\n",
    "Abstract examples:\n",
    "- **Echo / ringing**: a calibration event perturbs the next few readings.\n",
    "- **Delayed reporting**: one random reporting error spills into a couple of subsequent periods.\n",
    "- **Batching**: what you record at \\(t\\) includes several recent random contributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4351bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bced9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_filter(eps, theta, mu=0.0):\n",
    "    eps = np.asarray(eps, dtype=float)\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "\n",
    "    q = theta.size\n",
    "    n = eps.size\n",
    "    x = np.empty(n, dtype=float)\n",
    "\n",
    "    for t in range(n):\n",
    "        value = eps[t]\n",
    "        for k in range(1, q + 1):\n",
    "            if t - k >= 0:\n",
    "                value += theta[k - 1] * eps[t - k]\n",
    "        x[t] = mu + value\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def simulate_ma(\n",
    "    theta,\n",
    "    n,\n",
    "    mu=0.0,\n",
    "    sigma=1.0,\n",
    "    rng=None,\n",
    "    burnin=200,\n",
    "):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    eps = rng.normal(0.0, sigma, size=n + burnin)\n",
    "    x = ma_filter(eps, theta, mu=mu)\n",
    "    return x[burnin:], eps[burnin:]\n",
    "\n",
    "\n",
    "def innovations_from_ma(x, theta, mu=0.0):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "\n",
    "    q = theta.size\n",
    "    n = x.size\n",
    "    eps = np.zeros(n, dtype=float)\n",
    "\n",
    "    for t in range(n):\n",
    "        value = x[t] - mu\n",
    "        for k in range(1, q + 1):\n",
    "            if t - k >= 0:\n",
    "                value -= theta[k - 1] * eps[t - k]\n",
    "        eps[t] = value\n",
    "\n",
    "    return eps\n",
    "\n",
    "\n",
    "def sample_acf(x, max_lag):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x - x.mean()\n",
    "\n",
    "    denom = float(np.dot(x, x))\n",
    "    if denom == 0.0:\n",
    "        return np.r_[1.0, np.zeros(max_lag)]\n",
    "\n",
    "    acf = np.empty(max_lag + 1, dtype=float)\n",
    "    acf[0] = 1.0\n",
    "    for k in range(1, max_lag + 1):\n",
    "        acf[k] = float(np.dot(x[k:], x[:-k]) / denom)\n",
    "    return acf\n",
    "\n",
    "\n",
    "def theoretical_acf_ma(theta, max_lag):\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "    q = theta.size\n",
    "\n",
    "    theta_full = np.r_[1.0, theta]\n",
    "    gamma = np.zeros(max_lag + 1, dtype=float)\n",
    "    for k in range(0, max_lag + 1):\n",
    "        if k <= q:\n",
    "            gamma[k] = float(np.sum(theta_full[: q + 1 - k] * theta_full[k:]))\n",
    "\n",
    "    return gamma / gamma[0]\n",
    "\n",
    "\n",
    "def is_invertible(theta, tol=1e-12):\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "    if theta.size == 0:\n",
    "        return True\n",
    "\n",
    "    coeffs = np.r_[theta[::-1], 1.0]\n",
    "    roots = np.roots(coeffs)\n",
    "    return bool(np.all(np.abs(roots) > 1 + tol))\n",
    "\n",
    "\n",
    "def fit_ma1_via_acf(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    mu_hat = float(x.mean())\n",
    "    x0 = x - mu_hat\n",
    "\n",
    "    r1 = float(sample_acf(x0, 1)[1])\n",
    "    r1 = float(np.clip(r1, -0.499, 0.499))\n",
    "\n",
    "    if abs(r1) < 1e-12:\n",
    "        theta_hat = 0.0\n",
    "    else:\n",
    "        disc = max(0.0, 1.0 - 4.0 * r1 * r1)\n",
    "        s = disc ** 0.5\n",
    "        theta_a = (1.0 + s) / (2.0 * r1)\n",
    "        theta_b = (1.0 - s) / (2.0 * r1)\n",
    "        theta_hat = min([theta_a, theta_b], key=lambda v: abs(v))\n",
    "        theta_hat = float(np.clip(theta_hat, -0.99, 0.99))\n",
    "\n",
    "    var_hat = float(np.mean(x0 * x0))\n",
    "    sigma2_hat = var_hat / (1.0 + theta_hat * theta_hat)\n",
    "    sigma_hat = float(sigma2_hat ** 0.5)\n",
    "\n",
    "    return mu_hat, np.array([theta_hat]), sigma_hat\n",
    "\n",
    "\n",
    "def fit_ma_via_acf_random_search(\n",
    "    x,\n",
    "    q,\n",
    "    max_lag=20,\n",
    "    n_candidates=20000,\n",
    "    seed=0,\n",
    "):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    mu_hat = float(x.mean())\n",
    "    x0 = x - mu_hat\n",
    "    target = sample_acf(x0, max_lag)\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    best_theta = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for _ in range(n_candidates):\n",
    "        cand = rng_local.uniform(-0.99, 0.99, size=q)\n",
    "        if not is_invertible(cand):\n",
    "            continue\n",
    "        theo = theoretical_acf_ma(cand, max_lag)\n",
    "        loss = float(np.mean((target[1:] - theo[1:]) ** 2))\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_theta = cand\n",
    "\n",
    "    if best_theta is None:\n",
    "        best_theta = np.zeros(q, dtype=float)\n",
    "\n",
    "    var_hat = float(np.mean(x0 * x0))\n",
    "    sigma2_hat = var_hat / float(np.sum(np.r_[1.0, best_theta] ** 2))\n",
    "    sigma_hat = float(sigma2_hat ** 0.5)\n",
    "\n",
    "    return mu_hat, best_theta, sigma_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    \"MA(0)\": np.array([]),\n",
    "    \"MA(1)\": np.array([0.8]),\n",
    "    \"MA(2)\": np.array([0.8, 0.3]),\n",
    "    \"MA(5)\": np.array([0.8, 0.3, 0.1, 0.05, 0.02]),\n",
    "}\n",
    "\n",
    "burnin = 50\n",
    "n = 250\n",
    "max_lag = 20\n",
    "\n",
    "eps = rng.normal(0.0, 1.0, size=n + burnin)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=len(orders),\n",
    "    column_titles=[\n",
    "        f\"{name} (invertible={is_invertible(theta)})\" for name, theta in orders.items()\n",
    "    ],\n",
    "    row_titles=[\"sample path\", \"sample ACF\"],\n",
    "    vertical_spacing=0.15,\n",
    ")\n",
    "\n",
    "for col, (name, theta) in enumerate(orders.items(), start=1):\n",
    "    x = ma_filter(eps, theta)[burnin:]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=x, mode=\"lines\", name=name, showlegend=False),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    acf = sample_acf(x, max_lag)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=np.arange(max_lag + 1), y=acf, showlegend=False),\n",
    "        row=2,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"t\", row=1, col=col)\n",
    "    fig.update_xaxes(title_text=\"lag\", row=2, col=col)\n",
    "    fig.update_yaxes(title_text=\"X_t\", row=1, col=col)\n",
    "    fig.update_yaxes(title_text=\"ACF\", row=2, col=col, range=[-1, 1])\n",
    "\n",
    "fig.update_layout(\n",
    "    height=650,\n",
    "    width=1150,\n",
    "    title_text=\"Effect of MA order: sample paths and ACF cutoff\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390fc544",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = orders[\"MA(5)\"]\n",
    "impulse = np.r_[1.0, theta]\n",
    "lags = np.arange(impulse.size)\n",
    "\n",
    "fig = go.Figure(go.Bar(x=lags, y=impulse))\n",
    "fig.update_layout(\n",
    "    title=\"MA impulse response (finite-memory filter coefficients)\",\n",
    "    xaxis_title=\"lag h\",\n",
    "    yaxis_title=\"response weight (θ_h, with θ_0=1)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c981b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_true = np.array([0.8, 0.3])\n",
    "x, _eps_true = simulate_ma(theta_true, n=1000, sigma=1.0, rng=rng, burnin=200)\n",
    "\n",
    "mu_1, theta_1, sigma_1 = fit_ma1_via_acf(x)\n",
    "mu_2, theta_2, sigma_2 = fit_ma_via_acf_random_search(\n",
    "    x, q=2, max_lag=20, n_candidates=25000, seed=123\n",
    ")\n",
    "\n",
    "resid_1 = innovations_from_ma(x, theta_1, mu=mu_1)\n",
    "resid_2 = innovations_from_ma(x, theta_2, mu=mu_2)\n",
    "\n",
    "max_lag = 20\n",
    "acf_resid_1 = sample_acf(resid_1, max_lag)\n",
    "acf_resid_2 = sample_acf(resid_2, max_lag)\n",
    "\n",
    "conf = 1.96 / np.sqrt(x.size)\n",
    "lags = np.arange(1, max_lag + 1)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    column_titles=[\"Residual ACF after fitting MA(1)\", \"Residual ACF after fitting MA(2)\"],\n",
    ")\n",
    "\n",
    "for col, acf_resid in enumerate([acf_resid_1, acf_resid_2], start=1):\n",
    "    fig.add_trace(go.Bar(x=lags, y=acf_resid[1:], showlegend=False), row=1, col=col)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[lags[0], lags[-1]],\n",
    "            y=[conf, conf],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"crimson\", dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[lags[0], lags[-1]],\n",
    "            y=[-conf, -conf],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"crimson\", dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"lag\", row=1, col=col)\n",
    "    fig.update_yaxes(title_text=\"ACF\", row=1, col=col, range=[-1, 1])\n",
    "\n",
    "fig.update_layout(\n",
    "    height=350,\n",
    "    width=950,\n",
    "    title_text=\"Residual autocorrelation diagnostic (white-noise residuals ≈ 0)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "(mu_1, theta_1, sigma_1), (mu_2, theta_2, sigma_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8db500",
   "metadata": {},
   "source": [
    "## Pitfalls and diagnostics\n",
    "- Don’t confuse MA($q$) with the rolling mean used for smoothing.\n",
    "- MA($q$) is always stationary, but **invertibility** matters for identification and stable residual computation.\n",
    "- ACF cutoff is a heuristic; always check **residual autocorrelation** (and, in practice, tests like Ljung–Box).\n",
    "- MA parameters can be hard to estimate in small samples; likelihood surfaces can be flat and multi-modal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}