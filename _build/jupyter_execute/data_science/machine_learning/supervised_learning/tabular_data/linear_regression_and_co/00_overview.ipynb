{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1deb2785",
   "metadata": {},
   "source": [
    "# Linear Regression & Friends (OLS, Ridge, Lasso, Elastic Net)\n",
    "\n",
    "Linear regression is the **hello world** of predictive modeling: it’s simple enough to understand end-to-end, but rich enough to teach you the habits you’ll reuse everywhere:\n",
    "\n",
    "- how to translate a question into a loss function\n",
    "- how to solve an optimization problem (closed-form vs iterative)\n",
    "- how to diagnose overfitting\n",
    "- how regularization changes a model’s behavior\n",
    "\n",
    "This notebook builds intuition first (with analogies), then stays honest to the math.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- fit **simple** and **multiple** linear regression\n",
    "- compute coefficients (“betas”) via:\n",
    "  - **closed-form** normal equation\n",
    "  - **Cholesky** factorization (linear algebra)\n",
    "  - **gradient descent** (optimization)\n",
    "- understand and fit **Ridge (L2)**, **Lasso (L1)** and **Elastic Net (L1+L2)**\n",
    "- use `scikit-learn` versions and interpret the key parameters\n",
    "\n",
    "## Notation (quick)\n",
    "\n",
    "- Targets: $y \\in \\mathbb{R}^n$\n",
    "- Features: $X \\in \\mathbb{R}^{n\\times d}$ (each row is a sample)\n",
    "- Coefficients: $\\beta \\in \\mathbb{R}^d$\n",
    "- Intercept: $b \\in \\mathbb{R}$ (sometimes written $\\beta_0$)\n",
    "- Prediction: $\\hat{y} = b + X\\beta$\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. The story: one dial vs many dials\n",
    "2. Simple linear regression (one feature)\n",
    "3. Multiple linear regression (many features)\n",
    "4. Getting betas\n",
    "   - closed-form / normal equation\n",
    "   - Cholesky solve\n",
    "   - gradient descent\n",
    "5. Regularization\n",
    "   - Ridge (L2)\n",
    "   - Lasso (L1)\n",
    "   - Elastic Net (L1 + L2)\n",
    "6. `scikit-learn` equivalents + parameter intuition\n",
    "7. Practical checklist + pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6743864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115860d",
   "metadata": {},
   "source": [
    "## 1) The story: one dial vs many dials\n",
    "\n",
    "Imagine you’re predicting someone’s **commute time**.\n",
    "\n",
    "- With one feature (say, distance), linear regression is like a **single dial**: turn the slope up/down until the line fits.\n",
    "- With many features (distance, traffic, weather, time of day), it becomes a **mixing board** with many sliders.\n",
    "\n",
    "OLS (ordinary least squares) says:\n",
    "\n",
    "> “Pick the dial settings (betas) that make the squared mistakes as small as possible.”\n",
    "\n",
    "That’s it. Everything else in this notebook is different ways to do that—and to prevent the model from getting *too clever*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5916b7",
   "metadata": {},
   "source": [
    "## 2) Simple linear regression (one feature)\n",
    "\n",
    "### Model\n",
    "\n",
    "With one feature $x$, we predict:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "- $\\beta_0$ is the intercept (where the line crosses the y-axis)\n",
    "- $\\beta_1$ is the slope (how much $y$ changes when $x$ increases by 1)\n",
    "\n",
    "### Loss (OLS)\n",
    "\n",
    "We choose betas to minimize the mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "J(\\beta_0, \\beta_1) = \\frac{1}{n} \\sum_{i=1}^n (\\beta_0 + \\beta_1 x_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19900813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic \"one-feature\" dataset\n",
    "n_samples_simple = 120\n",
    "x_simple = rng.uniform(0, 10, size=n_samples_simple)\n",
    "\n",
    "beta0_true = 3.0\n",
    "beta1_true = 2.0\n",
    "noise = rng.normal(0, 2.0, size=n_samples_simple)\n",
    "\n",
    "y_simple = beta0_true + beta1_true * x_simple + noise\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=x_simple,\n",
    "    y=y_simple,\n",
    "    title=\"Synthetic data: one feature\",\n",
    "    labels={\"x\": \"x\", \"y\": \"y\"},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dacce6",
   "metadata": {},
   "source": [
    "### Closed-form for the one-feature case\n",
    "\n",
    "When there’s only one feature, OLS has a compact solution.\n",
    "\n",
    "Let $\\bar{x}$ and $\\bar{y}$ be the means. Then:\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i (x_i - \\bar{x})^2}\n",
    "\\qquad\n",
    "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- numerator = how $x$ and $y$ “move together” (covariance)\n",
    "- denominator = how much $x$ varies\n",
    "\n",
    "If $x$ doesn’t vary at all, you can’t learn a slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed-form simple regression (one feature)\n",
    "x_mean = x_simple.mean()\n",
    "y_mean = y_simple.mean()\n",
    "\n",
    "beta1_hat = np.sum((x_simple - x_mean) * (y_simple - y_mean)) / np.sum((x_simple - x_mean) ** 2)\n",
    "beta0_hat = y_mean - beta1_hat * x_mean\n",
    "\n",
    "beta0_hat, beta1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4312a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fitted line\n",
    "x_line = np.linspace(x_simple.min(), x_simple.max(), 200)\n",
    "y_line = beta0_hat + beta1_hat * x_line\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_simple, y=y_simple, mode=\"markers\", name=\"data\"))\n",
    "fig.add_trace(go.Scatter(x=x_line, y=y_line, mode=\"lines\", name=\"OLS fit\"))\n",
    "fig.update_layout(\n",
    "    title=\"Simple linear regression: OLS line\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3796c",
   "metadata": {},
   "source": [
    "### Residuals: “what the story doesn’t explain”\n",
    "\n",
    "Residuals are $r_i = y_i - \\hat{y}_i$.\n",
    "\n",
    "A friendly mental model: if the model is a **story** about the data, residuals are the parts the story can’t explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94803fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_simple = beta0_hat + beta1_hat * x_simple\n",
    "residuals = y_simple - y_pred_simple\n",
    "\n",
    "fig = px.histogram(residuals, nbins=30, title=\"Residual distribution (simple regression)\")\n",
    "fig.update_layout(xaxis_title=\"residual\", yaxis_title=\"count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b6fc7",
   "metadata": {},
   "source": [
    "## 3) Multiple linear regression (many features)\n",
    "\n",
    "With multiple features, we write predictions compactly.\n",
    "\n",
    "Create a design matrix with an intercept column:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\dots & x_{1d} \\\\\n",
    "1 & x_{21} & x_{22} & \\dots & x_{2d} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2} & \\dots & x_{nd}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\beta =\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X\\beta\n",
    "$$\n",
    "\n",
    "Same idea, more dials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4368a",
   "metadata": {},
   "source": [
    "### Why multiple regression gets tricky (multicollinearity)\n",
    "\n",
    "If two features carry almost the same information (e.g., `x2 ≈ x1`), OLS can still fit well, but the **individual coefficients** can become unstable.\n",
    "\n",
    "Analogy: two coworkers both trying to take credit for the same project.\n",
    "\n",
    "We’ll build a dataset with correlated features to make this visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic multi-feature dataset with correlated features\n",
    "n_samples_multi = 300\n",
    "x1 = rng.normal(0, 1, size=n_samples_multi)\n",
    "\n",
    "# x2 is highly correlated with x1\n",
    "x2 = 0.85 * x1 + rng.normal(0, 0.25, size=n_samples_multi)\n",
    "\n",
    "# x3 is mostly independent\n",
    "x3 = rng.normal(0, 1, size=n_samples_multi)\n",
    "\n",
    "X_raw = np.column_stack([x1, x2, x3])\n",
    "feature_names = [\"x1 (signal)\", \"x2 (correlated)\", \"x3 (signal)\"]\n",
    "\n",
    "true_intercept = 1.5\n",
    "true_beta = np.array([2.0, -1.5, 0.7])\n",
    "\n",
    "y_multi = true_intercept + X_raw @ true_beta + rng.normal(0, 1.0, size=n_samples_multi)\n",
    "\n",
    "corr = np.corrcoef(X_raw, rowvar=False)\n",
    "fig = px.imshow(corr, x=feature_names, y=feature_names, text_auto=True, title=\"Feature correlation\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_multi, test_size=0.25, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d5e2b",
   "metadata": {},
   "source": [
    "## 4) Getting betas (three ways)\n",
    "\n",
    "### 4.1 Closed-form (normal equation)\n",
    "\n",
    "OLS minimizes:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\|X\\beta - y\\|_2^2\n",
    "$$\n",
    "\n",
    "Take derivatives, set to zero → the **normal equation**:\n",
    "\n",
    "$$\n",
    "X^\\top X\\,\\beta = X^\\top y\n",
    "$$\n",
    "\n",
    "If $X^\\top X$ is invertible:\n",
    "\n",
    "$$\n",
    "\\beta = (X^\\top X)^{-1} X^\\top y\n",
    "$$\n",
    "\n",
    "In practice: avoid explicit inverses. Use `solve`, `lstsq`, or factorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2718020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept_column(X: np.ndarray) -> np.ndarray:\n",
    "    return np.column_stack([np.ones(X.shape[0]), X])\n",
    "\n",
    "\n",
    "def ols_via_lstsq(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    # Solves min ||Xb - y||_2 using a stable method (SVD under the hood)\n",
    "    beta, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    return beta\n",
    "\n",
    "\n",
    "def ols_via_normal_equation_solve(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    # Solves (X^T X) b = X^T y without forming an explicit inverse\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    return np.linalg.solve(XtX, Xty)\n",
    "\n",
    "\n",
    "def ols_via_cholesky(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    # If XtX is SPD, we can solve using a Cholesky factorization: XtX = L L^T\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "\n",
    "    L = np.linalg.cholesky(XtX)\n",
    "    z = np.linalg.solve(L, Xty)\n",
    "    beta = np.linalg.solve(L.T, z)\n",
    "    return beta\n",
    "\n",
    "\n",
    "X_design = add_intercept_column(X_train)\n",
    "\n",
    "beta_lstsq = ols_via_lstsq(X_design, y_train)\n",
    "beta_solve = ols_via_normal_equation_solve(X_design, y_train)\n",
    "beta_chol = ols_via_cholesky(X_design, y_train)\n",
    "\n",
    "beta_lstsq, beta_solve, beta_chol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87926110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to the true parameters\n",
    "beta_true = np.concatenate([[true_intercept], true_beta])\n",
    "\n",
    "labels = [\"intercept\", \"x1\", \"x2\", \"x3\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=\"true\", x=labels, y=beta_true))\n",
    "fig.add_trace(go.Bar(name=\"OLS (lstsq)\", x=labels, y=beta_lstsq))\n",
    "fig.add_trace(go.Bar(name=\"OLS (solve)\", x=labels, y=beta_solve))\n",
    "fig.add_trace(go.Bar(name=\"OLS (cholesky)\", x=labels, y=beta_chol))\n",
    "fig.update_layout(barmode=\"group\", title=\"Coefficients: true vs OLS solutions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dedaa4",
   "metadata": {},
   "source": [
    "### 4.2 Gradient descent (optimization)\n",
    "\n",
    "Closed-form is great, but:\n",
    "\n",
    "- it can be expensive when $d$ is large\n",
    "- it doesn’t exist for Lasso\n",
    "- it’s not how many modern models are trained\n",
    "\n",
    "Gradient descent treats the loss like a landscape and repeatedly takes steps “downhill”.\n",
    "\n",
    "For MSE with an intercept column included, a common gradient form is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta\\, J(\\beta) = \\frac{2}{n} X^\\top (X\\beta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ee2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate on the simple 1D dataset\n",
    "X_simple = add_intercept_column(x_simple.reshape(-1, 1))\n",
    "\n",
    "beta_gd, gd_steps, gd_losses, gd_betas = gradient_descent_linear_regression(\n",
    "    X_simple,\n",
    "    y_simple,\n",
    "    learning_rate=0.01,\n",
    "    n_steps=3000,\n",
    ")\n",
    "\n",
    "beta_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=gd_steps, y=gd_losses, title=\"Gradient descent: MSE over iterations\")\n",
    "fig.update_layout(xaxis_title=\"iteration\", yaxis_title=\"MSE\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ee767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss surface J(beta0, beta1) and the GD path\n",
    "beta0_grid = np.linspace(beta0_hat - 6, beta0_hat + 6, 80)\n",
    "beta1_grid = np.linspace(beta1_hat - 2.5, beta1_hat + 2.5, 80)\n",
    "\n",
    "B0, B1 = np.meshgrid(beta0_grid, beta1_grid)\n",
    "\n",
    "# Compute MSE on the grid\n",
    "Y_pred_grid = B0[..., None] + B1[..., None] * x_simple[None, None, :]\n",
    "MSE_grid = np.mean((Y_pred_grid - y_simple[None, None, :]) ** 2, axis=-1)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=beta0_grid,\n",
    "        y=beta1_grid,\n",
    "        z=MSE_grid,\n",
    "        contours_coloring=\"heatmap\",\n",
    "        showscale=True,\n",
    "        name=\"MSE\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=gd_betas[:, 0],\n",
    "        y=gd_betas[:, 1],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"GD path\",\n",
    "        line=dict(color=\"black\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Loss landscape (contours) + gradient descent path\",\n",
    "    xaxis_title=\"beta0\",\n",
    "    yaxis_title=\"beta1\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfcea5",
   "metadata": {},
   "source": [
    "## 5) Regularization: Ridge, Lasso, Elastic Net\n",
    "\n",
    "When features are correlated or numerous, OLS can “spread credit” in unstable ways.\n",
    "\n",
    "Regularization adds a preference:\n",
    "\n",
    "- Ridge (L2): “Prefer smaller coefficients.” (shrinks smoothly)\n",
    "- Lasso (L1): “Prefer fewer non-zero coefficients.” (can set some to zero)\n",
    "- Elastic Net: “A mix of both.”\n",
    "\n",
    "Analogy:\n",
    "\n",
    "- Ridge is like a safety belt: you can still move, but extreme swings are damped.\n",
    "- Lasso is like a budget: if you pay for one coefficient, you have less to spend on others.\n",
    "\n",
    "Important: regularization depends on feature scale → standardize your features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79316359",
   "metadata": {},
   "source": [
    "### Standardization helpers (from scratch)\n",
    "\n",
    "We’ll standardize using train statistics:\n",
    "\n",
    "$$\n",
    "X_{scaled} = \\frac{X - \\mu_{train}}{\\sigma_{train}}\n",
    "$$\n",
    "\n",
    "Then we fit on scaled features and convert back to original units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_fit(X_train: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0, ddof=0)\n",
    "    std = np.where(std == 0, 1.0, std)\n",
    "    return mean, std, (X_train - mean) / std\n",
    "\n",
    "\n",
    "def standardize_apply(X: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def unscale_coefficients(\n",
    "    beta_scaled: np.ndarray,\n",
    "    feature_mean: np.ndarray,\n",
    "    feature_std: np.ndarray,\n",
    "    target_mean: float,\n",
    ") -> tuple[float, np.ndarray]:\n",
    "    # If y_centered = y - target_mean and X_scaled = (X - mean)/std\n",
    "    # then y = target_mean + X_scaled @ beta_scaled\n",
    "    # and beta_original = beta_scaled / std\n",
    "    beta_original = beta_scaled / feature_std\n",
    "    intercept_original = target_mean - feature_mean @ beta_original\n",
    "    return float(intercept_original), beta_original\n",
    "\n",
    "\n",
    "def predict_linear(X: np.ndarray, intercept: float, beta: np.ndarray) -> np.ndarray:\n",
    "    return intercept + X @ beta\n",
    "\n",
    "\n",
    "feature_mean, feature_std, X_train_scaled = standardize_fit(X_train)\n",
    "y_train_mean = y_train.mean()\n",
    "y_train_centered = y_train - y_train_mean\n",
    "\n",
    "X_test_scaled = standardize_apply(X_test, feature_mean, feature_std)\n",
    "\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762355e",
   "metadata": {},
   "source": [
    "### 5.1 Ridge regression (L2)\n",
    "\n",
    "Ridge regression (as implemented in `sklearn.linear_model.Ridge`) minimizes:\n",
    "\n",
    "$$\n",
    "\\|y - X\\beta\\|_2^2 + \\alpha\\|\\beta\\|_2^2\n",
    "$$\n",
    "\n",
    "- $\\alpha \\ge 0$ controls the strength (bigger → more shrinkage)\n",
    "- $\\alpha = 0$ reduces to OLS\n",
    "\n",
    "The solution is still linear algebra—just with a “stabilized” matrix:\n",
    "\n",
    "$$\n",
    "(X^\\top X + \\alpha I)\\,\\beta = X^\\top y\n",
    "$$\n",
    "\n",
    "Practical note:\n",
    "\n",
    "- We typically do **not** penalize the intercept.\n",
    "- In this notebook we center $y$ and standardize $X$, then recover the intercept from the training mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b103c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_closed_form(X: np.ndarray, y: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    n_features = X.shape[1]\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    return np.linalg.solve(XtX + alpha * np.eye(n_features), Xty)\n",
    "\n",
    "\n",
    "alphas = np.logspace(-3, 2, 40)\n",
    "ridge_coefs_scaled = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_coefs_scaled.append(ridge_closed_form(X_train_scaled, y_train_centered, alpha))\n",
    "\n",
    "ridge_coefs_scaled = np.vstack(ridge_coefs_scaled)\n",
    "\n",
    "fig = go.Figure()\n",
    "for j, name in enumerate(feature_names):\n",
    "    fig.add_trace(go.Scatter(x=alphas, y=ridge_coefs_scaled[:, j], mode=\"lines\", name=name))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Ridge coefficient paths (scaled features)\",\n",
    "    xaxis_title=\"alpha (log scale)\",\n",
    "    yaxis_title=\"coefficient (scaled)\",\n",
    "    xaxis_type=\"log\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef175d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ridge for a few alpha values (convert back to original units)\n",
    "\n",
    "def fit_ridge_and_score(alpha: float) -> dict:\n",
    "    beta_scaled = ridge_closed_form(X_train_scaled, y_train_centered, alpha)\n",
    "    intercept, beta = unscale_coefficients(beta_scaled, feature_mean, feature_std, y_train_mean)\n",
    "    y_pred = predict_linear(X_test, intercept, beta)\n",
    "    return {\n",
    "        \"alpha\": alpha,\n",
    "        \"mse\": mean_squared_error(y_test, y_pred),\n",
    "        \"r2\": r2_score(y_test, y_pred),\n",
    "        \"intercept\": intercept,\n",
    "        \"beta\": beta,\n",
    "    }\n",
    "\n",
    "[fit_ridge_and_score(a) for a in [0.0, 0.1, 1.0, 10.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2752030",
   "metadata": {},
   "source": [
    "#### Ridge via Cholesky (linear algebra)\n",
    "\n",
    "Ridge requires solving a symmetric positive definite system (for $\\alpha > 0$):\n",
    "\n",
    "$$\n",
    "(X^\\top X + \\alpha I)\\,\\beta = X^\\top y\n",
    "$$\n",
    "\n",
    "A classic approach is **Cholesky factorization**:\n",
    "\n",
    "- factor $A = X^\\top X + \\alpha I$ as $A = LL^\\top$\n",
    "- solve $Lz = X^\\top y$\n",
    "- solve $L^\\top\\beta = z$\n",
    "\n",
    "This avoids computing any inverse explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a260205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_via_cholesky(X: np.ndarray, y: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    n_features = X.shape[1]\n",
    "    A = (X.T @ X) + alpha * np.eye(n_features)\n",
    "    b = X.T @ y\n",
    "\n",
    "    L = np.linalg.cholesky(A)\n",
    "    z = np.linalg.solve(L, b)\n",
    "    return np.linalg.solve(L.T, z)\n",
    "\n",
    "\n",
    "alpha_demo = 1.0\n",
    "beta_solve = ridge_closed_form(X_train_scaled, y_train_centered, alpha=alpha_demo)\n",
    "beta_chol = ridge_via_cholesky(X_train_scaled, y_train_centered, alpha=alpha_demo)\n",
    "\n",
    "np.max(np.abs(beta_solve - beta_chol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27b884",
   "metadata": {},
   "source": [
    "#### Choosing $\\alpha$: a quick validation curve\n",
    "\n",
    "In real projects you tune $\\alpha$ using a validation set or cross-validation.\n",
    "\n",
    "Below is a simple validation curve for Ridge (using an `sklearn` pipeline so scaling is fit only on the training split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2475d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=7)\n",
    "\n",
    "alphas_ridge = np.logspace(-3, 3, 60)\n",
    "val_mse = []\n",
    "\n",
    "for a in alphas_ridge:\n",
    "    model = make_pipeline(StandardScaler(), Ridge(alpha=a))\n",
    "    model.fit(X_tr, y_tr)\n",
    "    val_mse.append(mean_squared_error(y_val, model.predict(X_val)))\n",
    "\n",
    "val_mse = np.array(val_mse)\n",
    "best_alpha = float(alphas_ridge[np.argmin(val_mse)])\n",
    "\n",
    "fig = px.line(x=alphas_ridge, y=val_mse, title=f\"Ridge validation curve (best alpha ≈ {best_alpha:.4g})\")\n",
    "fig.update_layout(xaxis_title=\"alpha (log scale)\", yaxis_title=\"validation MSE\", xaxis_type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322fb47",
   "metadata": {},
   "source": [
    "### 5.2 Lasso regression (L1)\n",
    "\n",
    "Lasso replaces the L2 penalty with L1:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2n}\\|X\\beta - y\\|_2^2 + \\alpha\\|\\beta\\|_1\n",
    "$$\n",
    "\n",
    "Key effect: L1 tends to create sparsity (some coefficients become exactly 0).\n",
    "\n",
    "There is no closed-form solution in general.\n",
    "\n",
    "A common solver is coordinate descent:\n",
    "\n",
    "- hold all coefficients fixed\n",
    "- update one coefficient at a time\n",
    "- repeat until convergence\n",
    "\n",
    "The core operation becomes soft-thresholding:\n",
    "\n",
    "$$\n",
    "S(z, \\gamma) = \\mathrm{sign}(z)\\,\\max(|z| - \\gamma, 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb215ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(z: float, gamma: float) -> float:\n",
    "    if z > gamma:\n",
    "        return z - gamma\n",
    "    if z < -gamma:\n",
    "        return z + gamma\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def lasso_coordinate_descent(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    alpha: float,\n",
    "    n_iter: int = 5000,\n",
    "    tol: float = 1e-7,\n",
    "    warm_start: np.ndarray | None = None,\n",
    ") -> np.ndarray:\n",
    "    # Objective used here:\n",
    "    #   (1/(2n)) ||y - Xb||^2 + alpha ||b||_1\n",
    "    # Assumes y is centered and columns of X are standardized.\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    beta = np.zeros(n_features) if warm_start is None else warm_start.copy()\n",
    "\n",
    "    residuals = y - X @ beta\n",
    "    feature_norms = np.mean(X ** 2, axis=0)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        beta_prev = beta.copy()\n",
    "\n",
    "        for j in range(n_features):\n",
    "            residuals = residuals + X[:, j] * beta[j]\n",
    "\n",
    "            rho = np.mean(X[:, j] * residuals)\n",
    "            beta[j] = soft_threshold(rho, alpha) / feature_norms[j]\n",
    "\n",
    "            residuals = residuals - X[:, j] * beta[j]\n",
    "\n",
    "        if np.max(np.abs(beta - beta_prev)) < tol:\n",
    "            break\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "alphas = np.logspace(-3, 0.8, 45)\n",
    "lasso_coefs_scaled = []\n",
    "\n",
    "beta_ws = None\n",
    "for alpha in alphas:\n",
    "    beta_ws = lasso_coordinate_descent(X_train_scaled, y_train_centered, alpha=alpha, warm_start=beta_ws)\n",
    "    lasso_coefs_scaled.append(beta_ws.copy())\n",
    "\n",
    "lasso_coefs_scaled = np.vstack(lasso_coefs_scaled)\n",
    "\n",
    "fig = go.Figure()\n",
    "for j, name in enumerate(feature_names):\n",
    "    fig.add_trace(go.Scatter(x=alphas, y=lasso_coefs_scaled[:, j], mode=\"lines\", name=name))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Lasso coefficient paths (scaled features)\",\n",
    "    xaxis_title=\"alpha (log scale)\",\n",
    "    yaxis_title=\"coefficient (scaled)\",\n",
    "    xaxis_type=\"log\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbdb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate lasso for a few alpha values\n",
    "\n",
    "def fit_lasso_and_score(alpha: float) -> dict:\n",
    "    beta_scaled = lasso_coordinate_descent(X_train_scaled, y_train_centered, alpha=alpha)\n",
    "    intercept, beta = unscale_coefficients(beta_scaled, feature_mean, feature_std, y_train_mean)\n",
    "    y_pred = predict_linear(X_test, intercept, beta)\n",
    "    return {\n",
    "        \"alpha\": alpha,\n",
    "        \"mse\": mean_squared_error(y_test, y_pred),\n",
    "        \"r2\": r2_score(y_test, y_pred),\n",
    "        \"beta\": beta,\n",
    "        \"num_nonzero\": int(np.sum(np.abs(beta_scaled) > 1e-10)),\n",
    "    }\n",
    "\n",
    "[fit_lasso_and_score(a) for a in [0.01, 0.05, 0.1, 0.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fb322",
   "metadata": {},
   "source": [
    "### 5.3 Elastic Net (L1 + L2)\n",
    "\n",
    "Elastic Net combines both penalties:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2n}\\|X\\beta - y\\|_2^2\n",
    "+ \\alpha\\left(\\rho\\|\\beta\\|_1 + \\frac{1-\\rho}{2}\\|\\beta\\|_2^2\\right)\n",
    "$$\n",
    "\n",
    "- $\\alpha$ controls overall regularization\n",
    "- $\\rho \\in [0,1]$ (often called `l1_ratio`) controls the mix\n",
    "  - $\\rho=1$ → Lasso\n",
    "  - $\\rho=0$ → Ridge\n",
    "\n",
    "Elastic Net is especially useful when features are correlated: it can select groups of features instead of arbitrarily picking one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e08122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_coordinate_descent(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    alpha: float,\n",
    "    l1_ratio: float,\n",
    "    n_iter: int = 5000,\n",
    "    tol: float = 1e-7,\n",
    "    warm_start: np.ndarray | None = None,\n",
    ") -> np.ndarray:\n",
    "    # Objective used here:\n",
    "    #   (1/(2n)) ||y - Xb||^2 + alpha * (l1_ratio ||b||_1 + (1-l1_ratio)/2 ||b||_2^2)\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    beta = np.zeros(n_features) if warm_start is None else warm_start.copy()\n",
    "\n",
    "    residuals = y - X @ beta\n",
    "    feature_norms = np.mean(X ** 2, axis=0)\n",
    "\n",
    "    l1 = alpha * l1_ratio\n",
    "    l2 = alpha * (1.0 - l1_ratio)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        beta_prev = beta.copy()\n",
    "\n",
    "        for j in range(n_features):\n",
    "            residuals = residuals + X[:, j] * beta[j]\n",
    "\n",
    "            rho = np.mean(X[:, j] * residuals)\n",
    "            beta[j] = soft_threshold(rho, l1) / (feature_norms[j] + l2)\n",
    "\n",
    "            residuals = residuals - X[:, j] * beta[j]\n",
    "\n",
    "        if np.max(np.abs(beta - beta_prev)) < tol:\n",
    "            break\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "alphas = np.logspace(-3, 0.8, 45)\n",
    "l1_ratio = 0.5\n",
    "\n",
    "enet_coefs_scaled = []\n",
    "beta_ws = None\n",
    "for alpha in alphas:\n",
    "    beta_ws = elastic_net_coordinate_descent(\n",
    "        X_train_scaled, y_train_centered, alpha=alpha, l1_ratio=l1_ratio, warm_start=beta_ws\n",
    "    )\n",
    "    enet_coefs_scaled.append(beta_ws.copy())\n",
    "\n",
    "enet_coefs_scaled = np.vstack(enet_coefs_scaled)\n",
    "\n",
    "fig = go.Figure()\n",
    "for j, name in enumerate(feature_names):\n",
    "    fig.add_trace(go.Scatter(x=alphas, y=enet_coefs_scaled[:, j], mode=\"lines\", name=name))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Elastic Net coefficient paths (l1_ratio={l1_ratio})\",\n",
    "    xaxis_title=\"alpha (log scale)\",\n",
    "    yaxis_title=\"coefficient (scaled)\",\n",
    "    xaxis_type=\"log\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef892a",
   "metadata": {},
   "source": [
    "## 6) `scikit-learn` equivalents + parameter intuition\n",
    "\n",
    "The `sklearn` versions are production-grade implementations with good defaults.\n",
    "\n",
    "A few important notes:\n",
    "\n",
    "- Most `sklearn` linear models expect raw features and handle intercept internally (`fit_intercept=True` by default).\n",
    "- For Ridge/Lasso/ElasticNet, feature scaling is critical → use `StandardScaler()`.\n",
    "- `LinearRegression` solves OLS using a stable decomposition (no gradient descent).\n",
    "\n",
    "Below we compare OLS/Ridge/Lasso/ElasticNet with a consistent pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c244af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"OLS\": make_pipeline(StandardScaler(), LinearRegression()),\n",
    "    \"Ridge(alpha=1.0)\": make_pipeline(StandardScaler(), Ridge(alpha=1.0)),\n",
    "    \"Lasso(alpha=0.1)\": make_pipeline(StandardScaler(), Lasso(alpha=0.1, max_iter=50_000)),\n",
    "    \"ElasticNet(alpha=0.1,l1_ratio=0.5)\": make_pipeline(\n",
    "        StandardScaler(), ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=50_000)\n",
    "    ),\n",
    "    \"SGDRegressor (GD)\": make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SGDRegressor(\n",
    "            loss=\"squared_error\",\n",
    "            penalty=\"l2\",\n",
    "            alpha=1e-4,\n",
    "            learning_rate=\"invscaling\",\n",
    "            eta0=0.01,\n",
    "            max_iter=5000,\n",
    "            tol=1e-6,\n",
    "            random_state=42,\n",
    "        ),\n",
    "    ),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"model\": name,\n",
    "            \"mse\": mean_squared_error(y_test, y_pred),\n",
    "            \"r2\": r2_score(y_test, y_pred),\n",
    "        }\n",
    "    )\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ce023",
   "metadata": {},
   "source": [
    "### Parameter cheat sheet (the ones you’ll actually touch)\n",
    "\n",
    "#### `LinearRegression`\n",
    "- `fit_intercept`: include intercept term\n",
    "- `positive`: force coefficients to be non-negative\n",
    "\n",
    "#### `Ridge`\n",
    "- `alpha`: L2 strength (bigger → more shrinkage)\n",
    "- `solver`: numerical method (usually leave as `auto`)\n",
    "- `fit_intercept`: intercept handling\n",
    "\n",
    "#### `Lasso`\n",
    "- `alpha`: L1 strength (bigger → more sparsity)\n",
    "- `max_iter`, `tol`: convergence controls\n",
    "- `selection`: `cyclic` vs `random` coordinate updates\n",
    "\n",
    "#### `ElasticNet`\n",
    "- `alpha`: overall regularization\n",
    "- `l1_ratio`: mix between L1 and L2\n",
    "- `max_iter`, `tol`: convergence controls\n",
    "\n",
    "#### `SGDRegressor`\n",
    "- `loss=\"squared_error\"`: linear regression loss\n",
    "- `penalty`: `\"l2\"`, `\"l1\"`, `\"elasticnet\"`\n",
    "- `alpha`: regularization strength (note: different meaning/scale vs Ridge/Lasso)\n",
    "- `learning_rate`, `eta0`: step size schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155abeb2",
   "metadata": {},
   "source": [
    "## 7) Practical checklist + pitfalls\n",
    "\n",
    "- Always split train/test (or use CV) before deciding on `alpha`.\n",
    "- For Ridge/Lasso/ElasticNet:\n",
    "  - standardize features (`StandardScaler`)\n",
    "  - don’t leak test information into scaling\n",
    "- Don’t interpret Lasso sparsity as “the truth” without domain checks.\n",
    "- If features are strongly correlated:\n",
    "  - Ridge often stabilizes coefficients\n",
    "  - Elastic Net can select groups\n",
    "  - Lasso may pick one arbitrarily\n",
    "\n",
    "### When to use what (rough intuition)\n",
    "\n",
    "- OLS: small/clean problems, inference-focused, or as a baseline\n",
    "- Ridge: lots of correlated features, prediction-focused stability\n",
    "- Lasso: you want a sparse model (feature selection)\n",
    "- Elastic Net: you want sparsity and correlated-feature friendliness\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Increase correlation between `x1` and `x2` and see how OLS coefficients behave.\n",
    "2. Plot test MSE vs `alpha` for Ridge/Lasso and pick the best.\n",
    "3. Add irrelevant noisy features and compare OLS vs Ridge.\n",
    "\n",
    "## References\n",
    "\n",
    "- ESL (Hastie, Tibshirani, Friedman): *The Elements of Statistical Learning*\n",
    "- ISLR (James, Witten, Hastie, Tibshirani): *An Introduction to Statistical Learning*\n",
    "- `sklearn` docs for `Ridge`, `Lasso`, `ElasticNet`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}