{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10cd3a1c",
   "metadata": {},
   "source": [
    "# Linear and Quadratic Discriminant Analysis (LDA / QDA)\n",
    "\n",
    "Discriminant Analysis is a classic family of **generative** classifiers.\n",
    "\n",
    "- **LDA** assumes each class is Gaussian with a **shared covariance** → linear decision boundaries\n",
    "- **QDA** allows each class to have its **own covariance** → quadratic decision boundaries\n",
    "\n",
    "## Learning goals\n",
    "- understand the generative story: $p(x\\mid y)$ + Bayes → $p(y\\mid x)$\n",
    "- derive the discriminant score for LDA/QDA\n",
    "- build intuition for when the boundary becomes linear vs quadratic\n",
    "- implement LDA/QDA from scratch (NumPy)\n",
    "- use scikit-learn’s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis`\n",
    "\n",
    "## Table of contents\n",
    "1. Generative vs discriminative intuition\n",
    "2. Gaussian class-conditional model\n",
    "3. QDA discriminant function (quadratic)\n",
    "4. LDA as a constrained QDA (linear)\n",
    "5. From-scratch LDA/QDA\n",
    "6. Visual decision boundaries (Plotly)\n",
    "7. When does QDA overfit?\n",
    "8. LDA as supervised dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46294b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48492d66",
   "metadata": {},
   "source": [
    "## 1) Generative vs discriminative intuition\n",
    "\n",
    "Two worldviews:\n",
    "\n",
    "### Discriminative\n",
    "Learn $p(y\\mid x)$ directly (or a decision boundary).\n",
    "\n",
    "Examples:\n",
    "- logistic regression\n",
    "- SVM\n",
    "- neural networks\n",
    "\n",
    "### Generative\n",
    "Model how data is *generated*:\n",
    "\n",
    "$$p(x\\mid y)\\ \\text{and}\\ p(y)$$\n",
    "\n",
    "Then use Bayes:\n",
    "\n",
    "$$p(y\\mid x) \\propto p(x\\mid y)\\,p(y)$$\n",
    "\n",
    "Examples:\n",
    "- Naive Bayes\n",
    "- LDA / QDA\n",
    "\n",
    "Anecdote:\n",
    "> Discriminative models are like a judge who learns *where to draw the line*. Generative models are like a novelist who learns *how each class produces data*, then uses that story to decide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbbe05",
   "metadata": {},
   "source": [
    "## 2) Gaussian class-conditional model\n",
    "\n",
    "Assume that for each class $k$:\n",
    "\n",
    "$$\n",
    "(x \\mid y=k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\log p(x\\mid y=k) = -\\tfrac12\\log|\\Sigma_k| - \\tfrac12 (x-\\mu_k)^\\top \\Sigma_k^{-1}(x-\\mu_k) + \\text{const}\n",
    "$$\n",
    "\n",
    "Add the log prior $\\log \\pi_k$ and you get a **discriminant score**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4535f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 2D dataset where classes have different covariance shapes (QDA-friendly)\n",
    "\n",
    "n = 900\n",
    "mu0 = np.array([-2.0, -1.0])\n",
    "mu1 = np.array([+2.0, +1.0])\n",
    "\n",
    "Sigma0 = np.array([[1.8, 0.8], [0.8, 1.0]])\n",
    "Sigma1 = np.array([[1.0, -0.6], [-0.6, 1.6]])\n",
    "\n",
    "X0 = rng.multivariate_normal(mu0, Sigma0, size=n // 2)\n",
    "X1 = rng.multivariate_normal(mu1, Sigma1, size=n // 2)\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * (n // 2) + [1] * (n // 2))\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_tr[:, 0],\n",
    "    y=X_tr[:, 1],\n",
    "    color=y_tr.astype(str),\n",
    "    title=\"Training set (different class covariances)\",\n",
    "    labels={\"x\": \"x1\", \"y\": \"x2\", \"color\": \"class\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.65))\n",
    "fig.update_layout(width=760, height=500)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_ellipse_points(mean: np.ndarray, cov: np.ndarray, n_points: int = 200, n_std: float = 2.0):\n",
    "    \"\"\"Return x,y points of an ellipse representing the covariance.\"\"\"\n",
    "\n",
    "    mean = np.asarray(mean, dtype=float)\n",
    "    cov = np.asarray(cov, dtype=float)\n",
    "\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = np.argsort(vals)[::-1]\n",
    "    vals = vals[order]\n",
    "    vecs = vecs[:, order]\n",
    "\n",
    "    # ellipse axes lengths\n",
    "    radii = n_std * np.sqrt(np.maximum(vals, 0.0))\n",
    "\n",
    "    t = np.linspace(0, 2 * np.pi, n_points)\n",
    "    circle = np.vstack([np.cos(t), np.sin(t)])\n",
    "    ellipse = (vecs @ (radii[:, None] * circle)).T\n",
    "    ellipse = ellipse + mean[None, :]\n",
    "    return ellipse[:, 0], ellipse[:, 1]\n",
    "\n",
    "\n",
    "# Estimate means/covariances from training data\n",
    "mu0_hat = X_tr[y_tr == 0].mean(axis=0)\n",
    "mu1_hat = X_tr[y_tr == 1].mean(axis=0)\n",
    "S0_hat = np.cov(X_tr[y_tr == 0].T, bias=False)\n",
    "S1_hat = np.cov(X_tr[y_tr == 1].T, bias=False)\n",
    "\n",
    "x0e, y0e = covariance_ellipse_points(mu0_hat, S0_hat, n_std=2.0)\n",
    "x1e, y1e = covariance_ellipse_points(mu1_hat, S1_hat, n_std=2.0)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_tr[:, 0], y=X_tr[:, 1], mode=\"markers\",\n",
    "                         marker=dict(color=y_tr, colorscale=\"Viridis\", size=6, opacity=0.6),\n",
    "                         name=\"train\"))\n",
    "fig.add_trace(go.Scatter(x=x0e, y=y0e, mode=\"lines\", name=\"class 0 (2σ ellipse)\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=x1e, y=y1e, mode=\"lines\", name=\"class 1 (2σ ellipse)\", line=dict(width=3)))\n",
    "fig.update_layout(title=\"Empirical covariance ellipses\", width=760, height=520)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dd097",
   "metadata": {},
   "source": [
    "## 3) QDA discriminant function (quadratic)\n",
    "\n",
    "For class $k$, the log posterior (up to a shared constant) is:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = \\log \\pi_k\\; -\\; \\tfrac12\\log|\\Sigma_k|\\; -\\; \\tfrac12 (x-\\mu_k)^\\top \\Sigma_k^{-1}(x-\\mu_k)\n",
    "$$\n",
    "\n",
    "Pick the class with maximum score:\n",
    "\n",
    "$$\\hat{y}(x) = \\arg\\max_k \\delta_k(x)$$\n",
    "\n",
    "Because of the quadratic form $(x-\\mu_k)^\\top\\Sigma_k^{-1}(x-\\mu_k)$, the boundary is generally **quadratic**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358e796",
   "metadata": {},
   "source": [
    "## 4) LDA as a constrained QDA (linear)\n",
    "\n",
    "LDA makes one simplifying assumption:\n",
    "\n",
    "$$\\Sigma_k = \\Sigma \\quad \\text{(shared across classes)}$$\n",
    "\n",
    "When covariances are shared, the quadratic term in $x$ cancels when comparing classes.\n",
    "\n",
    "The discriminant becomes linear:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = x^\\top \\Sigma^{-1}\\mu_k - \\tfrac12 \\mu_k^\\top\\Sigma^{-1}\\mu_k + \\log \\pi_k\n",
    "$$\n",
    "\n",
    "So the decision boundary is a **hyperplane**.\n",
    "\n",
    "Intuition:\n",
    "- LDA is “one common ellipse shape for everyone, but different centers”\n",
    "- QDA is “each class gets its own ellipse shape and orientation”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a51b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLDA:\n",
    "    def __init__(self, reg: float = 1e-6):\n",
    "        self.reg = float(reg)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        self.classes_, y_enc = np.unique(y, return_inverse=True)\n",
    "        K = self.classes_.shape[0]\n",
    "        n, d = X.shape\n",
    "\n",
    "        self.priors_ = np.bincount(y_enc, minlength=K).astype(float)\n",
    "        self.priors_ = self.priors_ / self.priors_.sum()\n",
    "\n",
    "        self.means_ = np.zeros((K, d), dtype=float)\n",
    "        for k in range(K):\n",
    "            self.means_[k] = X[y_enc == k].mean(axis=0)\n",
    "\n",
    "        # pooled covariance (unbiased)\n",
    "        S = np.zeros((d, d), dtype=float)\n",
    "        for k in range(K):\n",
    "            Xk = X[y_enc == k]\n",
    "            Xc = Xk - self.means_[k]\n",
    "            S += Xc.T @ Xc\n",
    "        S /= (n - K)\n",
    "\n",
    "        S = S + self.reg * np.eye(d)\n",
    "        self.cov_ = S\n",
    "        self.precision_ = np.linalg.inv(S)\n",
    "\n",
    "        # precompute terms for linear discriminant\n",
    "        self._w_ = (self.precision_ @ self.means_.T).T  # (K,d)\n",
    "        self._b_ = -0.5 * np.sum(self.means_ * self._w_, axis=1) + np.log(self.priors_ + 1e-300)\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return X @ self._w_.T + self._b_[None, :]\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        scores = scores - scores.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(scores)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        return self.classes_[np.argmax(scores, axis=1)]\n",
    "\n",
    "\n",
    "class ScratchQDA:\n",
    "    def __init__(self, reg: float = 1e-6):\n",
    "        self.reg = float(reg)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        self.classes_, y_enc = np.unique(y, return_inverse=True)\n",
    "        K = self.classes_.shape[0]\n",
    "        n, d = X.shape\n",
    "\n",
    "        self.priors_ = np.bincount(y_enc, minlength=K).astype(float)\n",
    "        self.priors_ = self.priors_ / self.priors_.sum()\n",
    "\n",
    "        self.means_ = np.zeros((K, d), dtype=float)\n",
    "        self.covs_ = np.zeros((K, d, d), dtype=float)\n",
    "        self.precisions_ = np.zeros((K, d, d), dtype=float)\n",
    "        self.logdets_ = np.zeros(K, dtype=float)\n",
    "\n",
    "        for k in range(K):\n",
    "            Xk = X[y_enc == k]\n",
    "            self.means_[k] = Xk.mean(axis=0)\n",
    "            Sk = np.cov(Xk.T, bias=False) + self.reg * np.eye(d)\n",
    "            self.covs_[k] = Sk\n",
    "            self.precisions_[k] = np.linalg.inv(Sk)\n",
    "            self.logdets_[k] = float(np.linalg.slogdet(Sk)[1])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        n = X.shape[0]\n",
    "        K = self.classes_.shape[0]\n",
    "\n",
    "        scores = np.zeros((n, K), dtype=float)\n",
    "        for k in range(K):\n",
    "            diff = X - self.means_[k]\n",
    "            maha = np.sum((diff @ self.precisions_[k]) * diff, axis=1)\n",
    "            scores[:, k] = (\n",
    "                np.log(self.priors_[k] + 1e-300)\n",
    "                - 0.5 * self.logdets_[k]\n",
    "                - 0.5 * maha\n",
    "            )\n",
    "        return scores\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        scores = scores - scores.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(scores)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        return self.classes_[np.argmax(scores, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scratch + sklearn models\n",
    "scratch_lda = ScratchLDA(reg=1e-6).fit(X_tr, y_tr)\n",
    "scratch_qda = ScratchQDA(reg=1e-6).fit(X_tr, y_tr)\n",
    "\n",
    "sk_lda = LinearDiscriminantAnalysis().fit(X_tr, y_tr)\n",
    "sk_qda = QuadraticDiscriminantAnalysis(reg_param=1e-6).fit(X_tr, y_tr)\n",
    "\n",
    "pred_lda_s = scratch_lda.predict(X_te)\n",
    "pred_qda_s = scratch_qda.predict(X_te)\n",
    "\n",
    "pred_lda = sk_lda.predict(X_te)\n",
    "pred_qda = sk_qda.predict(X_te)\n",
    "\n",
    "print(\"Scratch LDA accuracy:\", accuracy_score(y_te, pred_lda_s))\n",
    "print(\"Scratch QDA accuracy:\", accuracy_score(y_te, pred_qda_s))\n",
    "print(\"sklearn LDA accuracy:\", accuracy_score(y_te, pred_lda))\n",
    "print(\"sklearn QDA accuracy:\", accuracy_score(y_te, pred_qda))\n",
    "\n",
    "print(\"\\nClassification report (sklearn QDA):\")\n",
    "print(classification_report(y_te, pred_qda, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194efacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(model, X, y, title: str, grid_steps: int = 260):\n",
    "    x_min, x_max = X[:, 0].min() - 1.2, X[:, 0].max() + 1.2\n",
    "    y_min, y_max = X[:, 1].min() - 1.2, X[:, 1].max() + 1.2\n",
    "\n",
    "    xs = np.linspace(x_min, x_max, grid_steps)\n",
    "    ys = np.linspace(y_min, y_max, grid_steps)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    proba = model.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Contour(\n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        z=proba,\n",
    "        colorscale=\"RdBu\",\n",
    "        opacity=0.75,\n",
    "        contours=dict(showlines=False),\n",
    "        colorbar=dict(title=\"P(class=1)\"),\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=y, colorscale=\"Viridis\", size=6, line=dict(width=0.5, color=\"white\")),\n",
    "        name=\"data\",\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title=title, width=780, height=540)\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig1 = plot_boundary(scratch_lda, X_te, y_te, \"Scratch LDA decision surface\")\n",
    "fig1.show()\n",
    "\n",
    "fig2 = plot_boundary(scratch_qda, X_te, y_te, \"Scratch QDA decision surface\")\n",
    "fig2.show()\n",
    "\n",
    "fig3 = plot_boundary(sk_lda, X_te, y_te, \"sklearn LDA decision surface\")\n",
    "fig3.show()\n",
    "\n",
    "fig4 = plot_boundary(sk_qda, X_te, y_te, \"sklearn QDA decision surface\")\n",
    "fig4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172d61e",
   "metadata": {},
   "source": [
    "## 6) When does QDA overfit?\n",
    "\n",
    "QDA estimates a full covariance matrix *per class*.\n",
    "\n",
    "For $d$ features, a full covariance has $d(d+1)/2$ parameters.\n",
    "\n",
    "So QDA can be data-hungry:\n",
    "- in low dimension with enough data → QDA can be great\n",
    "- in high dimension or small datasets → QDA can overfit\n",
    "\n",
    "LDA shares one covariance across classes, so it has fewer parameters and is often more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gaussian_dataset(n_samples: int, seed: int = 0):\n",
    "    r = np.random.default_rng(seed)\n",
    "\n",
    "    mu0 = np.array([-2.0, -1.0])\n",
    "    mu1 = np.array([+2.0, +1.0])\n",
    "\n",
    "    Sigma0 = np.array([[1.8, 0.8], [0.8, 1.0]])\n",
    "    Sigma1 = np.array([[1.0, -0.6], [-0.6, 1.6]])\n",
    "\n",
    "    X0 = r.multivariate_normal(mu0, Sigma0, size=n_samples // 2)\n",
    "    X1 = r.multivariate_normal(mu1, Sigma1, size=n_samples // 2)\n",
    "\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "train_sizes = [40, 80, 140, 220, 350, 500, 800]\n",
    "acc_lda = []\n",
    "acc_qda = []\n",
    "\n",
    "for n_train in train_sizes:\n",
    "    Xs, ys = sample_gaussian_dataset(n_train + 400, seed=7 + n_train)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(Xs, ys, test_size=400, random_state=7, stratify=ys)\n",
    "\n",
    "    m_lda = LinearDiscriminantAnalysis().fit(X_tr, y_tr)\n",
    "    m_qda = QuadraticDiscriminantAnalysis(reg_param=1e-6).fit(X_tr, y_tr)\n",
    "\n",
    "    acc_lda.append(accuracy_score(y_te, m_lda.predict(X_te)))\n",
    "    acc_qda.append(accuracy_score(y_te, m_qda.predict(X_te)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train_sizes, y=acc_lda, mode=\"lines+markers\", name=\"LDA\"))\n",
    "fig.add_trace(go.Scatter(x=train_sizes, y=acc_qda, mode=\"lines+markers\", name=\"QDA\"))\n",
    "fig.update_layout(title=\"Test accuracy vs training size\", xaxis_title=\"# training samples\", yaxis_title=\"accuracy\", width=780, height=460)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d30031",
   "metadata": {},
   "source": [
    "## 7) LDA as supervised dimensionality reduction\n",
    "\n",
    "LDA is also used as a **supervised projection** technique.\n",
    "\n",
    "It finds directions that separate classes by maximizing the ratio:\n",
    "\n",
    "$$\\frac{\\text{between-class variance}}{\\text{within-class variance}}$$\n",
    "\n",
    "In scikit-learn, `LinearDiscriminantAnalysis(n_components=...)` can transform data into a lower-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa26d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-class dataset → LDA projection to 2D\n",
    "X3, y3 = make_blobs(\n",
    "    n_samples=1200,\n",
    "    centers=[(-2, 0, 0), (2, 0, 0), (0, 2, 2)],\n",
    "    cluster_std=[1.4, 1.4, 1.4],\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "X3_tr, X3_te, y3_tr, y3_te = train_test_split(X3, y3, test_size=0.3, random_state=7, stratify=y3)\n",
    "\n",
    "lda_proj = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda_proj.fit(X3_tr, y3_tr)\n",
    "\n",
    "Z = lda_proj.transform(X3_te)\n",
    "\n",
    "fig = px.scatter(x=Z[:, 0], y=Z[:, 1], color=y3_te.astype(str), title=\"LDA projection (2 components)\", labels={\"x\": \"LD1\", \"y\": \"LD2\", \"color\": \"class\"})\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.7))\n",
    "fig.update_layout(width=760, height=500)\n",
    "fig.show()\n",
    "\n",
    "print(\"LDA projection explained variance ratio (approx):\", getattr(lda_proj, 'explained_variance_ratio_', None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac6fbe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- LDA/QDA are generative classifiers built from Gaussian class-conditional models.\n",
    "- QDA uses class-specific covariances → more flexible, but more parameters.\n",
    "- LDA shares covariance → linear boundaries and better stability with limited data.\n",
    "- LDA can also be used for supervised dimensionality reduction.\n",
    "\n",
    "## Exercises\n",
    "1. Generate a dataset where covariances are truly equal and compare LDA vs QDA.\n",
    "2. Increase dimensionality (e.g. $d=20$) and watch QDA become unstable unless you regularize.\n",
    "3. Compare Gaussian Naive Bayes (diagonal covariance) vs LDA (full covariance).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}