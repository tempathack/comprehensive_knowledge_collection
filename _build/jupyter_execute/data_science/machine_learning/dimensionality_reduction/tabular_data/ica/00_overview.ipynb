{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d4b799",
   "metadata": {},
   "source": [
    "# Independent Component Analysis (ICA)\n",
    "\n",
    "ICA is a method for **blind source separation**: when you only observe *mixtures*, can you recover the original *sources*?\n",
    "\n",
    "Think: **“unmixing voices at a cocktail party.”** Multiple speakers talk at once, each microphone records a different linear mixture, and ICA tries to recover the individual voices.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- explain ICA’s core assumptions (independence + non-Gaussianity)\n",
    "- distinguish **independence** vs **uncorrelatedness** with a counterexample\n",
    "- understand why **non-Gaussianity** is the signal ICA uses (CLT intuition)\n",
    "- implement **whitening** and see its effect on covariance\n",
    "- run ICA (`FastICA`) and compare with PCA visually (Plotly)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- covariance / correlation\n",
    "- eigenvalues & eigenvectors (PCA-style intuition)\n",
    "- basic probability (Gaussian vs non-Gaussian)\n",
    "\n",
    "---\n",
    "\n",
    "## Notation (linear mixing model)\n",
    "\n",
    "- Sources (unknown): $S \\in \\mathbb{R}^{n\\times k}$\n",
    "- Mixing matrix (unknown): $A \\in \\mathbb{R}^{m\\times k}$\n",
    "- Observations (what we measure): $X \\in \\mathbb{R}^{n\\times m}$\n",
    "\n",
    "We assume **linear instantaneous mixing**:\n",
    "\n",
    "$$\n",
    "X = SA^T\n",
    "$$\n",
    "\n",
    "ICA tries to estimate an **unmixing** matrix $W$ so that:\n",
    "\n",
    "$$\n",
    "\\hat{S} = XW^T.\n",
    "$$\n",
    "\n",
    "In practice, $\\hat{S}$ is only identifiable up to **permutation** (ordering) and **scaling/sign**.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Intuition: cocktail party unmixing\n",
    "2. Statistical foundation\n",
    "   - independence vs uncorrelated\n",
    "   - non-Gaussianity (CLT intuition)\n",
    "   - kurtosis / (approx) negentropy\n",
    "3. Algorithm mechanics\n",
    "   - whitening\n",
    "   - maximizing independence (FastICA idea)\n",
    "4. Plotly visualizations\n",
    "   - mixed vs unmixed signals\n",
    "   - PCA vs ICA comparison\n",
    "5. Use cases (signal processing, finance, neuroscience)\n",
    "6. Exercises + references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ec549",
   "metadata": {},
   "source": [
    "## 1) Intuition — “unmixing voices at a cocktail party”\n",
    "\n",
    "Imagine 3 people talking at the same time and 3 microphones in the room.\n",
    "\n",
    "- Each microphone records a different **mixture** of the 3 voices.\n",
    "- You only get the microphone recordings $X$.\n",
    "- You want to recover the original voices $S$ without knowing the room acoustics $A$.\n",
    "\n",
    "In a toy setting we can simulate this by:\n",
    "\n",
    "1) choosing three independent-looking source signals\n",
    "2) mixing them with a random matrix\n",
    "3) attempting to recover the sources from the mixtures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0298a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic sources (toy \"voices\")\n",
    "n_samples = 2500\n",
    "t = np.linspace(0.0, 8.0, n_samples)\n",
    "\n",
    "s1 = np.sin(2 * np.pi * 1.0 * t)\n",
    "s2 = np.where(np.sin(2 * np.pi * 3.0 * t) >= 0.0, 1.0, -1.0)  # square-ish\n",
    "s3 = 2.0 * ((t * 0.7) % 1.0) - 1.0  # sawtooth-ish ramp in [-1, 1)\n",
    "s3 = s3 + 0.10 * rng.normal(size=n_samples)\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "S = StandardScaler().fit_transform(S)  # each source: zero mean, unit variance\n",
    "\n",
    "# Random mixing matrix (\"room\")\n",
    "A = rng.normal(size=(3, 3))\n",
    "while abs(np.linalg.det(A)) < 1e-3:\n",
    "    A = rng.normal(size=(3, 3))\n",
    "\n",
    "X = S @ A.T\n",
    "X = StandardScaler().fit_transform(X)  # each observed channel: zero mean, unit variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=2,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.06,\n",
    "    column_titles=(\"Sources (unknown)\", \"Observed mixtures (microphones)\"),\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    fig.add_trace(go.Scatter(x=t, y=S[:, i], mode=\"lines\", line=dict(width=1)), row=i + 1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=t, y=X[:, i], mode=\"lines\", line=dict(width=1)), row=i + 1, col=2)\n",
    "\n",
    "for i in range(3):\n",
    "    fig.update_yaxes(title_text=f\"s{i + 1}\", row=i + 1, col=1)\n",
    "    fig.update_yaxes(title_text=f\"x{i + 1}\", row=i + 1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"time\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"time\", row=3, col=2)\n",
    "fig.update_layout(title=\"Toy cocktail party: sources vs mixtures\", showlegend=False, height=650)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d148f",
   "metadata": {},
   "source": [
    "Two important “gotchas” that are *features*, not bugs:\n",
    "\n",
    "- **Permutation ambiguity**: ICA doesn’t know which source is “voice 1” vs “voice 2”.\n",
    "- **Scaling/sign ambiguity**: multiplying a source by 2 and dividing the corresponding column of $A$ by 2 leaves $X$ unchanged. Signs can flip too.\n",
    "\n",
    "So success looks like *recovering the shapes*, not recovering a canonical ordering or amplitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc695a",
   "metadata": {},
   "source": [
    "## 2) Statistical foundation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114e324",
   "metadata": {},
   "source": [
    "### 2.1 Independence vs uncorrelated\n",
    "\n",
    "- **Uncorrelated** means the linear relationship is absent: $\\mathrm{Cov}(X, Y) = 0$.\n",
    "- **Independent** means the entire joint distribution factorizes: $p(x, y) = p(x)p(y)$.\n",
    "\n",
    "Independence is strictly stronger. Here’s a classic counterexample: $Y = X^2$ can be uncorrelated with $X$, but it’s obviously not independent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4000\n",
    "x = rng.uniform(-1.0, 1.0, size=n)\n",
    "y = x**2\n",
    "\n",
    "corr = float(np.corrcoef(x, y)[0, 1])\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    opacity=0.4,\n",
    "    title=f\"Uncorrelated ≠ independent (corr ≈ {corr:.3f})\",\n",
    "    labels={\"x\": \"X\", \"y\": \"Y = X²\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf39bfa",
   "metadata": {},
   "source": [
    "ICA wants **independent** components, not merely uncorrelated ones.\n",
    "\n",
    "PCA only guarantees *uncorrelated* components (a second-order property). ICA tries to match a stronger, higher-order notion: independence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48d810",
   "metadata": {},
   "source": [
    "### 2.2 Non-Gaussianity (why ICA has signal to grab)\n",
    "\n",
    "A key intuition comes from the **Central Limit Theorem (CLT)**:\n",
    "\n",
    "> A sum of (many) independent random variables tends to look *more Gaussian*.\n",
    "\n",
    "Mixing independent sources produces observations that are sums of sources, so the mixtures often look *more Gaussian* than the original signals.\n",
    "\n",
    "ICA works by finding directions (linear combinations) that look **maximally non-Gaussian**, because those directions are good candidates for the original sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dbe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(x=S[:, 1], name=\"source s2 (square-ish)\", nbinsx=60, opacity=0.65))\n",
    "fig.add_trace(go.Histogram(x=X[:, 0], name=\"mixture x1\", nbinsx=60, opacity=0.65))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"overlay\",\n",
    "    title=\"Mixing tends to look 'more Gaussian' (CLT intuition)\",\n",
    "    xaxis_title=\"value\",\n",
    "    yaxis_title=\"count\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0eb825",
   "metadata": {},
   "source": [
    "### 2.3 Kurtosis / (approx) negentropy\n",
    "\n",
    "**Non-Gaussianity** can be quantified in many ways. Two common ideas:\n",
    "\n",
    "- **Excess kurtosis**: for a standardized variable, $\\kappa = \\mathbb{E}[Z^4] - 3$. A Gaussian has $\\kappa = 0$.\n",
    "- **Negentropy**: how far a distribution’s entropy is from a Gaussian with the same variance. (Hard to compute exactly; ICA often uses proxies.)\n",
    "\n",
    "FastICA (and many ICA variants) use a contrast function that increases when a projected signal deviates from Gaussian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_1d(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    x = x - x.mean()\n",
    "    return x / (x.std() + 1e-12)\n",
    "\n",
    "\n",
    "def excess_kurtosis(x: np.ndarray) -> float:\n",
    "    z = standardize_1d(x)\n",
    "    return float(np.mean(z**4) - 3.0)\n",
    "\n",
    "\n",
    "def negentropy_proxy(x: np.ndarray, n_ref: int = 20000, seed: int = 0) -> float:\n",
    "    \"\"\"A simple negentropy proxy using the log-cosh contrast.\n",
    "\n",
    "    This is *not* an exact negentropy computation; it’s an intuitive scalar that\n",
    "    increases when a distribution deviates from Gaussian.\n",
    "    \"\"\"\n",
    "    z = standardize_1d(x)\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    v = rng_local.standard_normal(size=n_ref)\n",
    "\n",
    "    def G(u):\n",
    "        u = np.clip(u, -10.0, 10.0)\n",
    "        return np.log(np.cosh(u))\n",
    "\n",
    "    return float((np.mean(G(z)) - np.mean(G(v))) ** 2)\n",
    "\n",
    "\n",
    "labels = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "kurt_S = [excess_kurtosis(S[:, i]) for i in range(3)]\n",
    "kurt_X = [excess_kurtosis(X[:, i]) for i in range(3)]\n",
    "\n",
    "neg_S = [negentropy_proxy(S[:, i], seed=10 + i) for i in range(3)]\n",
    "neg_X = [negentropy_proxy(X[:, i], seed=20 + i) for i in range(3)]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Excess kurtosis\", \"Negentropy proxy (log-cosh)\"))\n",
    "\n",
    "fig.add_trace(go.Bar(x=labels, y=kurt_S, name=\"sources\"), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=labels, y=kurt_X, name=\"mixtures\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=labels, y=neg_S, name=\"sources\"), row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=labels, y=neg_X, name=\"mixtures\"), row=1, col=2)\n",
    "\n",
    "fig.update_layout(barmode=\"group\", title=\"Non-Gaussianity proxies (toy example)\")\n",
    "fig.update_xaxes(title_text=\"component\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"component\", row=1, col=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c46d4",
   "metadata": {},
   "source": [
    "## 3) Algorithm mechanics\n",
    "\n",
    "Most ICA algorithms have the same shape:\n",
    "\n",
    "1) **center**: subtract the mean\n",
    "2) **whiten**: remove second-order correlations (Cov becomes identity)\n",
    "3) **rotate**: find directions that maximize independence (usually via non-Gaussianity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3c529",
   "metadata": {},
   "source": [
    "### 3.1 Whitening\n",
    "\n",
    "Whitening transforms the observations so their covariance matrix becomes approximately the identity.\n",
    "\n",
    "Why it helps:\n",
    "\n",
    "- It removes all **second-order** structure (correlations).\n",
    "- After whitening, the remaining mixing is (roughly) a **rotation**.\n",
    "- ICA can then focus on higher-order structure to find independent components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df177d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(X: np.ndarray, eps: float = 1e-12):\n",
    "    \"\"\"Center + whiten so that Cov(X_white) ≈ I.\"\"\"\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    cov = (Xc.T @ Xc) / Xc.shape[0]\n",
    "\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    order = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[order]\n",
    "    eigvecs = eigvecs[:, order]\n",
    "\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals + eps))\n",
    "    W = eigvecs @ D_inv_sqrt @ eigvecs.T\n",
    "    X_white = Xc @ W.T\n",
    "    return X_white, W\n",
    "\n",
    "\n",
    "X_white, W_white = whiten(X)\n",
    "\n",
    "cov_X = (X.T @ X) / X.shape[0]\n",
    "cov_Xw = (X_white.T @ X_white) / X_white.shape[0]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Cov(X) (mixtures)\", \"Cov(whitened X) (≈ I)\"))\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=cov_X, zmin=-1, zmax=1, colorscale=\"RdBu\"), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=cov_Xw, zmin=-1, zmax=1, colorscale=\"RdBu\"), row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Whitening removes second-order correlations\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778f7ae",
   "metadata": {},
   "source": [
    "### 3.2 Maximizing independence (FastICA idea)\n",
    "\n",
    "After whitening, ICA searches for weight vectors $w$ so that the projection $w^T x$ is **maximally non-Gaussian**.\n",
    "\n",
    "A common fixed-point update (one component) looks like:\n",
    "\n",
    "$$\n",
    "w \\leftarrow \\mathbb{E}[x\\,g(w^T x)] - \\mathbb{E}[g'(w^T x)]\\,w\n",
    "$$\n",
    "\n",
    "followed by normalization and orthogonalization against previously found components.\n",
    "\n",
    "The nonlinearity $g$ (e.g. `tanh`) is chosen so the update increases a non-Gaussianity contrast (a proxy for independence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba914b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "\n",
    "def corr_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Signed correlation between columns of A and B.\"\"\"\n",
    "    A0 = A - A.mean(axis=0, keepdims=True)\n",
    "    B0 = B - B.mean(axis=0, keepdims=True)\n",
    "    A0 = A0 / (A0.std(axis=0, keepdims=True) + 1e-12)\n",
    "    B0 = B0 / (B0.std(axis=0, keepdims=True) + 1e-12)\n",
    "    return (A0.T @ B0) / A0.shape[0]\n",
    "\n",
    "\n",
    "def match_by_correlation(S_true: np.ndarray, S_est: np.ndarray):\n",
    "    \"\"\"Match estimated components to true sources (perm + sign).\n",
    "\n",
    "    This is only for visualization in toy data where we *know* the sources.\n",
    "    \"\"\"\n",
    "    C = corr_matrix(S_true, S_est)\n",
    "    k = C.shape[0]\n",
    "    best_perm = None\n",
    "    best_score = -np.inf\n",
    "    for perm in permutations(range(k)):\n",
    "        score = sum(abs(C[i, perm[i]]) for i in range(k))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_perm = perm\n",
    "\n",
    "    perm = np.array(best_perm)\n",
    "    signs = np.sign([C[i, perm[i]] for i in range(k)]).astype(float)\n",
    "    signs = np.where(signs == 0.0, 1.0, signs)\n",
    "\n",
    "    S_matched = S_est[:, perm] * signs\n",
    "    return S_matched, perm, signs, C\n",
    "\n",
    "\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "S_pca = pca.fit_transform(X)\n",
    "S_pca = StandardScaler().fit_transform(S_pca)\n",
    "\n",
    "ica = FastICA(n_components=3, random_state=42, whiten=\"unit-variance\", max_iter=2000)\n",
    "S_ica = ica.fit_transform(X)\n",
    "S_ica = StandardScaler().fit_transform(S_ica)\n",
    "\n",
    "S_pca_matched, perm_pca, signs_pca, C_pca_raw = match_by_correlation(S, S_pca)\n",
    "S_ica_matched, perm_ica, signs_ica, C_ica_raw = match_by_correlation(S, S_ica)\n",
    "\n",
    "abs_corr_pca = np.abs(corr_matrix(S, S_pca_matched))\n",
    "abs_corr_ica = np.abs(corr_matrix(S, S_ica_matched))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916d24c",
   "metadata": {},
   "source": [
    "### 3.3 Practical ICA with `scikit-learn`\n",
    "\n",
    "`sklearn.decomposition.FastICA` is a solid default implementation of the FastICA family.\n",
    "\n",
    "**Data layout (common gotcha)**\n",
    "- rows = samples (here: time points)\n",
    "- columns = observed mixtures (here: microphone/sensor channels)\n",
    "\n",
    "**Key parameters**\n",
    "- `n_components`: how many independent components to extract\n",
    "- `whiten`: `'unit-variance'` (components scaled to unit variance) vs `'arbitrary-variance'` vs `False`\n",
    "- `algorithm`: `'parallel'` (estimate all components together) vs `'deflation'` (one-by-one)\n",
    "- `fun`: contrast / nonlinearity (`'logcosh'` default, `'exp'`, `'cube'`)\n",
    "- `max_iter`, `tol`: convergence control\n",
    "\n",
    "**What you get back**\n",
    "- `fit_transform(X)` returns $\\hat{S}$ (up to permutation/sign/scale)\n",
    "- `mixing_` and `components_` are estimated mixing/unmixing matrices (same ambiguities)\n",
    "\n",
    "In the toy problem we can *match* components to sources using correlation only because we know the ground truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90af57a",
   "metadata": {},
   "source": [
    "## 4) Plotly visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a448877",
   "metadata": {},
   "source": [
    "### 4.1 Mixed vs unmixed signals\n",
    "\n",
    "In real life you don’t have access to the true sources — but in a toy dataset we can show the idea clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ed26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=3,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.06,\n",
    "    column_titles=(\"Sources (truth)\", \"Mixtures (observed)\", \"ICA (unmixed)\"),\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    fig.add_trace(go.Scatter(x=t, y=S[:, i], mode=\"lines\", line=dict(width=1)), row=i + 1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=t, y=X[:, i], mode=\"lines\", line=dict(width=1)), row=i + 1, col=2)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=t, y=S_ica_matched[:, i], mode=\"lines\", line=dict(width=1)),\n",
    "        row=i + 1,\n",
    "        col=3,\n",
    "    )\n",
    "\n",
    "for i in range(3):\n",
    "    fig.update_yaxes(title_text=f\"{i + 1}\", row=i + 1, col=1)\n",
    "    fig.update_yaxes(title_text=f\"{i + 1}\", row=i + 1, col=2)\n",
    "    fig.update_yaxes(title_text=f\"{i + 1}\", row=i + 1, col=3)\n",
    "\n",
    "fig.update_xaxes(title_text=\"time\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"time\", row=3, col=2)\n",
    "fig.update_xaxes(title_text=\"time\", row=3, col=3)\n",
    "fig.update_layout(title=\"Mixed vs unmixed signals (toy example)\", showlegend=False, height=650)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e71b15",
   "metadata": {},
   "source": [
    "### 4.2 PCA vs ICA comparison\n",
    "\n",
    "PCA and ICA both produce linear components, but they optimize different objectives:\n",
    "\n",
    "- **PCA**: directions of maximum variance (components are orthogonal and uncorrelated).\n",
    "- **ICA**: directions of maximum independence (using non-Gaussianity).\n",
    "\n",
    "In this toy problem, ICA aligns components with sources much better than PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sources = [\"s1\", \"s2\", \"s3\"]\n",
    "labels_components = [\"c1\", \"c2\", \"c3\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"PCA: |corr(component, source)|\", \"ICA: |corr(component, source)|\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=abs_corr_pca.T,\n",
    "        x=labels_sources,\n",
    "        y=labels_components,\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        colorscale=\"Blues\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=abs_corr_ica.T,\n",
    "        x=labels_sources,\n",
    "        y=labels_components,\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        colorscale=\"Blues\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(title=\"PCA vs ICA on the same mixtures\", height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e79e8",
   "metadata": {},
   "source": [
    "## 5) Use cases\n",
    "\n",
    "ICA is most useful when “mixtures of independent-ish things” is a good model.\n",
    "\n",
    "### Signal processing\n",
    "\n",
    "- **Audio source separation** (cocktail party) when mixing is approximately linear.\n",
    "- **Denoising / artifact removal**: separate structured signal from independent noise-like components.\n",
    "\n",
    "### Finance\n",
    "\n",
    "- Discover **latent factors** behind correlated asset returns (market/sector/style components).\n",
    "- Separate independent drivers for **risk modeling** or **portfolio construction**.\n",
    "\n",
    "### Neuroscience\n",
    "\n",
    "- **EEG/MEG**: remove artifacts (eye blinks, muscle activity) by identifying independent sources.\n",
    "- **fMRI**: identify spatially independent activation patterns (often with additional constraints).\n",
    "\n",
    "### Practical pitfalls\n",
    "\n",
    "- ICA is sensitive to preprocessing (centering, whitening, scaling).\n",
    "- Components are only defined up to permutation/sign/scale.\n",
    "- If the independence/non-Gaussian assumptions are wrong, ICA can return unstable or uninterpretable components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13cee6",
   "metadata": {},
   "source": [
    "## 6) Exercises + references\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Make one source Gaussian and see when ICA becomes ambiguous.\n",
    "2. Increase the number of sources/microphones and check when recovery degrades.\n",
    "3. Compare kurtosis/negentropy of PCA vs ICA components on the same mixtures.\n",
    "\n",
    "### References\n",
    "\n",
    "- Hyvärinen & Oja (2000): *Independent Component Analysis: Algorithms and Applications*\n",
    "- scikit-learn docs: `sklearn.decomposition.FastICA`\n",
    "- Cardoso (1998): *Blind signal separation: statistical principles*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}