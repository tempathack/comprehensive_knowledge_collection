{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d79f25b",
   "metadata": {},
   "source": [
    "# D'Agostino's $K^2$ normality test (`dagostino_k2`)\n",
    "\n",
    "D'Agostino–Pearson's $K^2$ test is an **omnibus normality test**: it looks for departures from normality due to **skewness** and/or **kurtosis**.\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- explain what $K^2$ tests (and what it does *not* test)\n",
    "- compute skewness and kurtosis and connect them to distribution shape\n",
    "- implement the full test in **NumPy only** (low-level)\n",
    "- use Plotly visuals (histogram, Q–Q) to understand *why* normality fails\n",
    "- interpret the statistic and p-value responsibly\n",
    "\n",
    "## Table of contents\n",
    "1. What the test is for\n",
    "2. Hypotheses + outputs\n",
    "3. Intuition: skewness + kurtosis\n",
    "4. NumPy-only implementation\n",
    "5. Visual diagnostics\n",
    "6. Geometry + chi-square interpretation\n",
    "7. Interpreting the result\n",
    "8. Pitfalls + guidance\n",
    "9. Exercises + references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cad80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe425be",
   "metadata": {},
   "source": [
    "## 1) What the test is for\n",
    "\n",
    "You typically use D'Agostino's $K^2$ test when you want a **quick, automated check** of whether a sample is plausibly normal.\n",
    "\n",
    "Common workflows:\n",
    "- checking normality of **residuals** (linear regression / ANOVA diagnostics)\n",
    "- choosing between **parametric** vs **non-parametric** tests\n",
    "- deciding whether a **normal noise model** is a reasonable approximation\n",
    "- validating simulation outputs that are supposed to be normal\n",
    "\n",
    "What it does *not* do:\n",
    "- it does not tell you *which* non-normal distribution you have\n",
    "- it does not “prove normality” when the p-value is large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7953a0c",
   "metadata": {},
   "source": [
    "## 2) Hypotheses + outputs\n",
    "\n",
    "- **$H_0$ (null)**: the data are a random sample from a normal distribution (some $\\mu, \\sigma$).\n",
    "- **$H_1$ (alternative)**: the data are not from a normal distribution.\n",
    "\n",
    "The test returns:\n",
    "- `K2`: an omnibus statistic (larger → more evidence against normality)\n",
    "- `p_value`: the upper-tail probability $P(K2_{H0} \\ge K2_{obs})$\n",
    "\n",
    "Rules of thumb:\n",
    "- if `p_value < α` (e.g. 0.05), you **reject** normality at level α\n",
    "- if `p_value ≥ α`, you **fail to reject** (not the same as “accept normality”)\n",
    "\n",
    "Sample size notes:\n",
    "- most references (and SciPy) require **n ≥ 8**\n",
    "- the kurtosis transform is asymptotic and becomes more reliable as n grows (you’ll often see guidance like “n > 20”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3303dff",
   "metadata": {},
   "source": [
    "## 3) Intuition: skewness + kurtosis\n",
    "\n",
    "$K^2$ combines two shape diagnostics:\n",
    "\n",
    "- **Skewness** (3rd standardized moment): asymmetry\n",
    "  - positive skew: long right tail\n",
    "  - negative skew: long left tail\n",
    "- **Kurtosis** (4th standardized moment): tail heaviness / peakedness\n",
    "  - normal has **Pearson kurtosis = 3**\n",
    "  - heavier tails → larger kurtosis; lighter tails → smaller kurtosis\n",
    "\n",
    "$K^2$ is “omnibus” because it can reject normality if **either** skewness or kurtosis (or both) look unusual under $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88489c",
   "metadata": {},
   "source": [
    "## 4) NumPy-only implementation (low-level)\n",
    "\n",
    "We implement the same basic steps as `scipy.stats.normaltest`:\n",
    "1. compute sample skewness and kurtosis\n",
    "2. transform each into an approximately standard-normal z-score\n",
    "3. combine them: $K^2 = z_{skew}^2 + z_{kurt}^2$\n",
    "4. compute the p-value from $\\chi^2(2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49902dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_1d_finite(x):\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    return x[np.isfinite(x)]\n",
    "\n",
    "\n",
    "def _central_moments_1d(x):\n",
    "    x = _as_1d_finite(x)\n",
    "    mean = x.mean()\n",
    "    d = x - mean\n",
    "    m2 = np.mean(d**2)\n",
    "    m3 = np.mean(d**3)\n",
    "    m4 = np.mean(d**4)\n",
    "    return mean, m2, m3, m4\n",
    "\n",
    "\n",
    "def sample_skewness(x):\n",
    "    _, m2, m3, _ = _central_moments_1d(x)\n",
    "    if m2 == 0:\n",
    "        return np.nan\n",
    "    return m3 / (m2 ** 1.5)\n",
    "\n",
    "\n",
    "def sample_kurtosis_pearson(x):\n",
    "    _, m2, _, m4 = _central_moments_1d(x)\n",
    "    if m2 == 0:\n",
    "        return np.nan\n",
    "    return m4 / (m2 ** 2)  # normal → 3\n",
    "\n",
    "\n",
    "def dagostino_skew_z(x):\n",
    "    x = _as_1d_finite(x)\n",
    "    n = x.size\n",
    "    if n < 8:\n",
    "        raise ValueError(f'dagostino_skew_z requires n >= 8; got n={n}.')\n",
    "\n",
    "    g1 = sample_skewness(x)\n",
    "    if not np.isfinite(g1):\n",
    "        return np.nan\n",
    "\n",
    "    y = g1 * np.sqrt(((n + 1) * (n + 3)) / (6.0 * (n - 2)))\n",
    "    beta2 = (\n",
    "        3.0\n",
    "        * (n**2 + 27 * n - 70)\n",
    "        * (n + 1)\n",
    "        * (n + 3)\n",
    "        / ((n - 2.0) * (n + 5) * (n + 7) * (n + 9))\n",
    "    )\n",
    "    W2 = -1.0 + np.sqrt(2.0 * (beta2 - 1.0))\n",
    "    delta = 1.0 / np.sqrt(0.5 * np.log(W2))\n",
    "    alpha = np.sqrt(2.0 / (W2 - 1.0))\n",
    "\n",
    "    # Exactly-zero y can happen for perfectly symmetric toy data; match common implementations.\n",
    "    if y == 0.0:\n",
    "        y = 1.0\n",
    "\n",
    "    z = delta * np.log(y / alpha + np.sqrt((y / alpha) ** 2 + 1.0))\n",
    "    return z\n",
    "\n",
    "\n",
    "def anscombe_glynn_kurt_z(x):\n",
    "    x = _as_1d_finite(x)\n",
    "    n = x.size\n",
    "    if n < 5:\n",
    "        raise ValueError(f'anscombe_glynn_kurt_z requires n >= 5; got n={n}.')\n",
    "\n",
    "    b2 = sample_kurtosis_pearson(x)\n",
    "    if not np.isfinite(b2):\n",
    "        return np.nan\n",
    "\n",
    "    E = 3.0 * (n - 1) / (n + 1)\n",
    "    varb2 = 24.0 * n * (n - 2) * (n - 3) / ((n + 1) ** 2 * (n + 3) * (n + 5))\n",
    "    x_stat = (b2 - E) / np.sqrt(varb2)\n",
    "\n",
    "    sqrtbeta1 = (\n",
    "        6.0\n",
    "        * (n * n - 5 * n + 2)\n",
    "        / ((n + 7) * (n + 9))\n",
    "        * np.sqrt((6.0 * (n + 3) * (n + 5)) / (n * (n - 2) * (n - 3)))\n",
    "    )\n",
    "    A = 6.0 + 8.0 / sqrtbeta1 * (2.0 / sqrtbeta1 + np.sqrt(1.0 + 4.0 / (sqrtbeta1**2)))\n",
    "\n",
    "    term1 = 1.0 - 2.0 / (9.0 * A)\n",
    "    denom = 1.0 + x_stat * np.sqrt(2.0 / (A - 4.0))\n",
    "    if denom == 0.0:\n",
    "        return np.nan\n",
    "\n",
    "    term2 = np.sign(denom) * (((1.0 - 2.0 / A) / np.abs(denom)) ** (1.0 / 3.0))\n",
    "    z = (term1 - term2) / np.sqrt(2.0 / (9.0 * A))\n",
    "    return z\n",
    "\n",
    "\n",
    "def dagostino_k2_test(x):\n",
    "    x = _as_1d_finite(x)\n",
    "    n = x.size\n",
    "    if n < 8:\n",
    "        raise ValueError(f'dagostino_k2_test requires n >= 8; got n={n}.')\n",
    "\n",
    "    g1 = sample_skewness(x)\n",
    "    b2 = sample_kurtosis_pearson(x)\n",
    "    z_skew = dagostino_skew_z(x)\n",
    "    z_kurt = anscombe_glynn_kurt_z(x)\n",
    "\n",
    "    K2 = z_skew**2 + z_kurt**2\n",
    "    p_value = float(np.exp(-0.5 * K2))  # chi-square(df=2) survival function\n",
    "\n",
    "    return {\n",
    "        'n': int(n),\n",
    "        'skewness': float(g1),\n",
    "        'kurtosis_pearson': float(b2),\n",
    "        'z_skew': float(z_skew),\n",
    "        'z_kurt': float(z_kurt),\n",
    "        'K2': float(K2),\n",
    "        'p_value': p_value,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf6db4e",
   "metadata": {},
   "source": [
    "### Quick example: inspect the components\n",
    "\n",
    "The two component z-scores tell you *why* the omnibus statistic is large:\n",
    "- large `|z_skew|` → asymmetry (skewed distribution)\n",
    "- large `|z_kurt|` → tails/peakedness differ from normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc195d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rng.normal(0, 1, size=200)\n",
    "ours = dagostino_k2_test(x)\n",
    "ours\n",
    "\n",
    "# Optional SciPy check (if installed)\n",
    "try:\n",
    "    from scipy.stats import normaltest\n",
    "\n",
    "    sp = normaltest(x)\n",
    "    {\"scipy_K2\": float(sp.statistic), \"scipy_p_value\": float(sp.pvalue)}\n",
    "except Exception as e:\n",
    "    f\"SciPy not available: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90072d63",
   "metadata": {},
   "source": [
    "## 5) Visual diagnostics\n",
    "\n",
    "Normality tests reduce a full distribution to a single number, so you should almost always pair $K^2$ with:\n",
    "- a **histogram + fitted normal curve**\n",
    "- a **Q–Q plot** (straight line suggests normality; curvature suggests tails/skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98de74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu=0.0, sigma=1.0):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return (1.0 / (sigma * np.sqrt(2.0 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "\n",
    "def norm_ppf(p):\n",
    "    \"\"\"Acklam's rational approximation of the standard normal quantile (NumPy-only).\"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if np.any((p <= 0) | (p >= 1)):\n",
    "        raise ValueError('p must be in (0, 1)')\n",
    "\n",
    "    a = np.array([\n",
    "        -3.969683028665376e01,\n",
    "         2.209460984245205e02,\n",
    "        -2.759285104469687e02,\n",
    "         1.383577518672690e02,\n",
    "        -3.066479806614716e01,\n",
    "         2.506628277459239e00,\n",
    "    ])\n",
    "    b = np.array([\n",
    "        -5.447609879822406e01,\n",
    "         1.615858368580409e02,\n",
    "        -1.556989798598866e02,\n",
    "         6.680131188771972e01,\n",
    "        -1.328068155288572e01,\n",
    "    ])\n",
    "    c = np.array([\n",
    "        -7.784894002430293e-03,\n",
    "        -3.223964580411365e-01,\n",
    "        -2.400758277161838e00,\n",
    "        -2.549732539343734e00,\n",
    "         4.374664141464968e00,\n",
    "         2.938163982698783e00,\n",
    "    ])\n",
    "    d = np.array([\n",
    "         7.784695709041462e-03,\n",
    "         3.224671290700398e-01,\n",
    "         2.445134137142996e00,\n",
    "         3.754408661907416e00,\n",
    "    ])\n",
    "\n",
    "    plow = 0.02425\n",
    "    phigh = 1.0 - plow\n",
    "\n",
    "    x = np.empty_like(p)\n",
    "\n",
    "    m1 = p < plow\n",
    "    if np.any(m1):\n",
    "        q = np.sqrt(-2.0 * np.log(p[m1]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1.0)\n",
    "        x[m1] = num / den\n",
    "\n",
    "    m2 = (p >= plow) & (p <= phigh)\n",
    "    if np.any(m2):\n",
    "        q = p[m2] - 0.5\n",
    "        r = q * q\n",
    "        num = (((((a[0] * r + a[1]) * r + a[2]) * r + a[3]) * r + a[4]) * r + a[5]) * q\n",
    "        den = (((((b[0] * r + b[1]) * r + b[2]) * r + b[3]) * r + b[4]) * r + 1.0)\n",
    "        x[m2] = num / den\n",
    "\n",
    "    m3 = p > phigh\n",
    "    if np.any(m3):\n",
    "        q = np.sqrt(-2.0 * np.log(1.0 - p[m3]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1.0)\n",
    "        x[m3] = -(num / den)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def qq_points_standardized(x):\n",
    "    x = _as_1d_finite(x)\n",
    "    x = (x - x.mean()) / x.std(ddof=0)\n",
    "    xs = np.sort(x)\n",
    "    n = xs.size\n",
    "    p = (np.arange(1, n + 1) - 0.5) / n\n",
    "    z = norm_ppf(p)\n",
    "    return z, xs\n",
    "\n",
    "\n",
    "def add_hist_with_normal_fit(fig, x, row, col, title):\n",
    "    x = _as_1d_finite(x)\n",
    "    mu = x.mean()\n",
    "    sigma = x.std(ddof=0)\n",
    "    grid = np.linspace(x.min(), x.max(), 400)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=x,\n",
    "            nbinsx=45,\n",
    "            histnorm='probability density',\n",
    "            opacity=0.65,\n",
    "            name=title,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=grid,\n",
    "            y=normal_pdf(grid, mu=mu, sigma=sigma),\n",
    "            mode='lines',\n",
    "            name='Normal fit',\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.update_xaxes(title_text='x', row=row, col=col)\n",
    "    fig.update_yaxes(title_text='density', row=row, col=col)\n",
    "\n",
    "\n",
    "def add_qq(fig, x, row, col):\n",
    "    z, xs = qq_points_standardized(x)\n",
    "    lo = float(min(z.min(), xs.min()))\n",
    "    hi = float(max(z.max(), xs.max()))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=z, y=xs, mode='markers', marker=dict(size=5), name='Q–Q points'),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[lo, hi], y=[lo, hi], mode='lines', name='45° line'),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "    fig.update_xaxes(title_text='Theoretical quantiles (N(0,1))', row=row, col=col)\n",
    "    fig.update_yaxes(title_text='Sample quantiles (standardized)', row=row, col=col)\n",
    "\n",
    "\n",
    "n = 400\n",
    "x_normal = rng.normal(0, 1, size=n)\n",
    "x_logn = rng.lognormal(mean=0.0, sigma=0.6, size=n)\n",
    "x_mix = np.concatenate([rng.normal(-2, 1, size=n // 2), rng.normal(2, 1, size=n - n // 2)])\n",
    "\n",
    "datasets = {'normal': x_normal, 'lognormal': x_logn, 'mixture': x_mix}\n",
    "titles = []\n",
    "for name, x in datasets.items():\n",
    "    res = dagostino_k2_test(x)\n",
    "    titles.extend([f\"{name}: hist (p={res['p_value']:.3g})\", f\"{name}: Q–Q\"])\n",
    "\n",
    "fig = make_subplots(rows=3, cols=2, subplot_titles=titles, horizontal_spacing=0.12, vertical_spacing=0.12)\n",
    "\n",
    "for i, (name, x) in enumerate(datasets.items(), start=1):\n",
    "    add_hist_with_normal_fit(fig, x, row=i, col=1, title=f'{name} sample')\n",
    "    add_qq(fig, x, row=i, col=2)\n",
    "\n",
    "fig.update_layout(height=950, showlegend=False, title_text='Histogram + Q–Q (with K² p-values)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8e640",
   "metadata": {},
   "source": [
    "## 6) Geometry + chi-square interpretation\n",
    "\n",
    "The test first turns skewness and kurtosis into two approximately standard-normal z-scores:\n",
    "- $z_{skew}$\n",
    "- $z_{kurt}$\n",
    "\n",
    "and then combines them:\n",
    "$$K^2 = z_{skew}^2 + z_{kurt}^2$$\n",
    "\n",
    "So you can think of $K^2$ as a **squared distance from the origin** in the plane $(z_{skew}, z_{kurt})$.\n",
    "\n",
    "Under $H_0$, $K^2$ is approximately $\\chi^2$ with 2 degrees of freedom, and the p-value is:\n",
    "$$p = P(\\chi^2_2 \\ge K^2) = \\exp(-K^2/2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dagostino_components_batch(samples):\n",
    "    \"\"\"Return (z_skew, z_kurt) for many samples at once (samples shape: [sims, n]).\"\"\"\n",
    "    samples = np.asarray(samples, dtype=float)\n",
    "    n = samples.shape[1]\n",
    "    if n < 8:\n",
    "        raise ValueError('need n >= 8')\n",
    "\n",
    "    mean = samples.mean(axis=1, keepdims=True)\n",
    "    d = samples - mean\n",
    "    m2 = np.mean(d**2, axis=1)\n",
    "    m3 = np.mean(d**3, axis=1)\n",
    "    m4 = np.mean(d**4, axis=1)\n",
    "\n",
    "    g1 = m3 / (m2 ** 1.5)\n",
    "    b2 = m4 / (m2 ** 2)\n",
    "\n",
    "    # skew z\n",
    "    y = g1 * np.sqrt(((n + 1) * (n + 3)) / (6.0 * (n - 2)))\n",
    "    beta2 = (\n",
    "        3.0\n",
    "        * (n**2 + 27 * n - 70)\n",
    "        * (n + 1)\n",
    "        * (n + 3)\n",
    "        / ((n - 2.0) * (n + 5) * (n + 7) * (n + 9))\n",
    "    )\n",
    "    W2 = -1.0 + np.sqrt(2.0 * (beta2 - 1.0))\n",
    "    delta = 1.0 / np.sqrt(0.5 * np.log(W2))\n",
    "    alpha = np.sqrt(2.0 / (W2 - 1.0))\n",
    "    y = np.where(y == 0.0, 1.0, y)\n",
    "    z_skew = delta * np.log(y / alpha + np.sqrt((y / alpha) ** 2 + 1.0))\n",
    "\n",
    "    # kurt z\n",
    "    E = 3.0 * (n - 1) / (n + 1)\n",
    "    varb2 = 24.0 * n * (n - 2) * (n - 3) / ((n + 1) ** 2 * (n + 3) * (n + 5))\n",
    "    x_stat = (b2 - E) / np.sqrt(varb2)\n",
    "\n",
    "    sqrtbeta1 = (\n",
    "        6.0\n",
    "        * (n * n - 5 * n + 2)\n",
    "        / ((n + 7) * (n + 9))\n",
    "        * np.sqrt((6.0 * (n + 3) * (n + 5)) / (n * (n - 2) * (n - 3)))\n",
    "    )\n",
    "    A = 6.0 + 8.0 / sqrtbeta1 * (2.0 / sqrtbeta1 + np.sqrt(1.0 + 4.0 / (sqrtbeta1**2)))\n",
    "    term1 = 1.0 - 2.0 / (9.0 * A)\n",
    "    denom = 1.0 + x_stat * np.sqrt(2.0 / (A - 4.0))\n",
    "    term2 = np.sign(denom) * np.where(\n",
    "        denom == 0.0,\n",
    "        np.nan,\n",
    "        ((1.0 - 2.0 / A) / np.abs(denom)) ** (1.0 / 3.0),\n",
    "    )\n",
    "    z_kurt = (term1 - term2) / np.sqrt(2.0 / (9.0 * A))\n",
    "\n",
    "    return z_skew, z_kurt\n",
    "\n",
    "\n",
    "n = 60\n",
    "sims = 2500\n",
    "\n",
    "samples_normal = rng.normal(0, 1, size=(sims, n))\n",
    "samples_logn = rng.lognormal(mean=0.0, sigma=0.6, size=(sims, n))\n",
    "samples_t3 = rng.standard_t(df=3, size=(sims, n))\n",
    "\n",
    "# symmetric but non-normal: 0.5*N(-2,1) + 0.5*N(2,1)\n",
    "sign = rng.choice([-1.0, 1.0], size=(sims, n))\n",
    "samples_mix = rng.normal(loc=2.0, scale=1.0, size=(sims, n)) * sign\n",
    "\n",
    "z1_n, z2_n = dagostino_components_batch(samples_normal)\n",
    "z1_l, z2_l = dagostino_components_batch(samples_logn)\n",
    "z1_t, z2_t = dagostino_components_batch(samples_t3)\n",
    "z1_m, z2_m = dagostino_components_batch(samples_mix)\n",
    "\n",
    "alpha = 0.05\n",
    "k2_thresh = -2.0 * np.log(alpha)\n",
    "r = float(np.sqrt(k2_thresh))\n",
    "theta = np.linspace(0, 2 * np.pi, 400)\n",
    "circle_x = r * np.cos(theta)\n",
    "circle_y = r * np.sin(theta)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=z1_n, y=z2_n, mode='markers', name='normal', opacity=0.35, marker=dict(size=4)))\n",
    "fig.add_trace(go.Scatter(x=z1_l, y=z2_l, mode='markers', name='lognormal', opacity=0.35, marker=dict(size=4)))\n",
    "fig.add_trace(go.Scatter(x=z1_t, y=z2_t, mode='markers', name='t(df=3)', opacity=0.35, marker=dict(size=4)))\n",
    "fig.add_trace(go.Scatter(x=z1_m, y=z2_m, mode='markers', name='mixture', opacity=0.35, marker=dict(size=4)))\n",
    "fig.add_trace(go.Scatter(x=circle_x, y=circle_y, mode='lines', name=f'reject @ α={alpha}', line=dict(color='black', dash='dash')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Z-space view: K² = z_skew² + z_kurt² (n={n}, sims={sims} per distribution)',\n",
    "    xaxis_title='z_skew',\n",
    "    yaxis_title='z_kurt',\n",
    "    width=900,\n",
    "    height=650,\n",
    ")\n",
    "fig.update_yaxes(scaleanchor='x', scaleratio=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f54d9",
   "metadata": {},
   "source": [
    "## 7) Under the null: $K^2 \\sim \\chi^2(2)$ and p-values are uniform\n",
    "\n",
    "A good self-check is to simulate normal samples and confirm:\n",
    "- $K^2$ matches the $\\chi^2(2)$ shape\n",
    "- p-values look approximately Uniform(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 80\n",
    "sims = 12000\n",
    "samples = rng.normal(0, 1, size=(sims, n))\n",
    "z_skew, z_kurt = dagostino_components_batch(samples)\n",
    "K2 = z_skew**2 + z_kurt**2\n",
    "pvals = np.exp(-0.5 * K2)\n",
    "\n",
    "grid = np.linspace(0.0, float(np.quantile(K2, 0.99)), 400)\n",
    "chi2_pdf = 0.5 * np.exp(-0.5 * grid)  # df=2\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Histogram(x=K2, nbinsx=60, histnorm='probability density', opacity=0.65, name='simulated K²'))\n",
    "fig1.add_trace(go.Scatter(x=grid, y=chi2_pdf, mode='lines', name='chi-square(df=2) pdf'))\n",
    "fig1.update_layout(title='K² under H0 ≈ chi-square(df=2)', xaxis_title='K²', yaxis_title='density', barmode='overlay')\n",
    "fig1.show()\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Histogram(x=pvals, nbinsx=50, histnorm='probability density', opacity=0.65, name='p-values'))\n",
    "fig2.add_trace(go.Scatter(x=[0, 1], y=[1, 1], mode='lines', name='Uniform(0,1) density', line=dict(color='black', dash='dash')))\n",
    "fig2.update_layout(title='p-values under H0 should be ~Uniform(0,1)', xaxis_title='p-value', yaxis_title='density', barmode='overlay')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8ea27",
   "metadata": {},
   "source": [
    "## 8) Interpreting the result (what it means)\n",
    "\n",
    "- `p_value` is **not** “the probability the data are normal”.\n",
    "- It is: assuming normality, how surprising is a $K^2$ at least this large?\n",
    "- Small `p_value` → evidence that skewness and/or kurtosis are inconsistent with a normal sample.\n",
    "- Large `p_value` → you did not find strong evidence against normality (but normality can still be false).\n",
    "\n",
    "Use the components:\n",
    "- large `|z_skew|` → asymmetry\n",
    "- large `|z_kurt|` → tails/peakedness differ from normal\n",
    "\n",
    "Reporting example:\n",
    "> D'Agostino $K^2$ normality test, n=200: $K^2$=2.40, p=0.30.\n",
    "\n",
    "With large n, rejection can be “statistically significant” but practically irrelevant — always inspect the Q–Q plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34704d0",
   "metadata": {},
   "source": [
    "## 9) Pitfalls + guidance\n",
    "\n",
    "- **Large n**: tiny, practically irrelevant deviations can produce tiny p-values (high power).\n",
    "- **Outliers**: a few extreme points can dominate skewness/kurtosis and trigger rejection.\n",
    "- **Dependence**: the test assumes i.i.d. samples; autocorrelation (time series) can invalidate p-values.\n",
    "- **Discrete/rounded data**: many ties can cause deviations even when a normal model is a decent approximation.\n",
    "- **Small n**: the approximations are rough; consider Shapiro–Wilk for very small samples and always inspect Q–Q plots.\n",
    "\n",
    "Diagnostics checklist:\n",
    "- for regression diagnostics, test **residuals**\n",
    "- inspect `z_skew` vs `z_kurt` to see *why* it rejects\n",
    "- if normality matters for inference, consider robust or resampling-based alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c450b92",
   "metadata": {},
   "source": [
    "## 10) Exercises\n",
    "\n",
    "1. Estimate the empirical Type I error at α=0.05 by simulating many normal samples.\n",
    "2. Compare power against lognormal, t(3), and a Gaussian mixture as you vary n.\n",
    "3. Fit a linear regression, compute residuals, and run $K^2$ on the residuals. Does it match your Q–Q plot?\n",
    "4. Apply a transform (log, Box–Cox) to a skewed sample and compare `z_skew`, `z_kurt`, and `p_value`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b86f68",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- D'Agostino, R. B. (1971). *An omnibus test of normality for moderate and large sample size*. Biometrika.\n",
    "- D'Agostino, R. B. and Pearson, E. S. (1973). *Tests for departure from normality*. Biometrika.\n",
    "- Anscombe, F. J. and Glynn, W. J. (1983). *Distribution of the kurtosis statistic b2 for normal samples*. Biometrika.\n",
    "- SciPy documentation: `scipy.stats.normaltest`, `skewtest`, `kurtosistest`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}