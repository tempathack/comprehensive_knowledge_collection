{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fb470b",
   "metadata": {},
   "source": [
    "# Paired t-test (`ttest_rel`) — comparing means with paired data\n",
    "\n",
    "The **paired t-test** (also called the **dependent-samples t-test**) answers a very specific question:\n",
    "\n",
    "> Do two measurements taken on the **same units** differ **on average**?\n",
    "\n",
    "Examples:\n",
    "- *Before vs after* an intervention on the same people\n",
    "- Two measurement methods applied to the same items\n",
    "- Matched pairs (e.g., twins, matched customers, matched locations)\n",
    "\n",
    "The key idea is that pairing lets you remove unit-to-unit variability by working with **within-pair differences**.\n",
    "\n",
    "## What you’ll learn\n",
    "- when a paired t-test is appropriate (and when it isn’t)\n",
    "- how it reduces to a **one-sample t-test on differences**\n",
    "- what the **t statistic** measures and how to interpret the **p-value**\n",
    "- a **NumPy-only implementation** (including Monte Carlo p-values / critical values)\n",
    "- Plotly visuals to build intuition: paired lines, difference distribution, null distribution, t distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    import scipy\n",
    "    from scipy import stats\n",
    "except Exception:  # pragma: no cover\n",
    "    scipy = None\n",
    "    stats = None\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(7)\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "if scipy is not None:\n",
    "    print(\"scipy:\", scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d508bd",
   "metadata": {},
   "source": [
    "## 1) What the paired t-test tests\n",
    "\n",
    "Suppose you observe **paired** measurements $(x_i, y_i)$ for $i=1,\\dots,n$.\n",
    "Define the within-pair differences\n",
    "\n",
    "$$d_i = y_i - x_i.$$\n",
    "\n",
    "The paired t-test is simply a **one-sample t-test** applied to the $d_i$’s.\n",
    "\n",
    "- **Parameter of interest**: the mean difference $\\mu_d = \\mathbb{E}[d]$\n",
    "- **Null hypothesis**: $H_0: \\mu_d = \\mu_0$ (usually $\\mu_0=0$)\n",
    "- **Alternative**:\n",
    "  - two-sided: $H_1: \\mu_d \\neq \\mu_0$\n",
    "  - one-sided: $H_1: \\mu_d > \\mu_0$ or $H_1: \\mu_d < \\mu_0$\n",
    "\n",
    "Intuition: if there is *no systematic change*, the differences should fluctuate around 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1bf88",
   "metadata": {},
   "source": [
    "## 2) When to use it (and when not)\n",
    "\n",
    "Use a paired t-test when:\n",
    "- You have **two measurements per unit** (same person/item/place twice), or **matched pairs**.\n",
    "- You care about the **average within-unit change**.\n",
    "- Different pairs are **independent** of each other.\n",
    "\n",
    "Do **not** use a paired t-test when:\n",
    "- The samples are **independent** (different people in each group) → use an *independent* t-test instead.\n",
    "- You have **more than two time points** per unit → consider repeated-measures ANOVA / mixed models.\n",
    "- Pairing is unclear or mismatched (if you pair the wrong units, you create noise and bias).\n",
    "\n",
    "### Why pairing can help (variance reduction)\n",
    "\n",
    "A paired design often has **positive correlation** between $x_i$ and $y_i$ (the same unit tends to be “high” or “low” both times).\n",
    "Pairing exploits that:\n",
    "\n",
    "$$\\mathrm{Var}(Y-X) = \\mathrm{Var}(Y) + \\mathrm{Var}(X) - 2\\,\\mathrm{Cov}(X,Y).$$\n",
    "\n",
    "If $\\mathrm{Cov}(X,Y) > 0$, the variance of the difference is smaller → the test can be **more powerful**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad964c0",
   "metadata": {},
   "source": [
    "## 3) Assumptions (what must be true for the classic p-value)\n",
    "\n",
    "Let $d_i = y_i - x_i$.\n",
    "\n",
    "1. **Independence of pairs**: $(d_1,\\dots,d_n)$ are independent across units.\n",
    "2. **Approximate normality of differences**: $d_i$ are approximately normal (or $n$ is large enough for the CLT to kick in).\n",
    "3. **No extreme outliers in differences**: outliers can dominate $\\bar d$ and $s_d$.\n",
    "\n",
    "Notes:\n",
    "- The test is fairly robust to mild non-normality, especially as $n$ grows, but it is *not* robust to strong outliers.\n",
    "- If the difference distribution is very non-normal / heavy-tailed, consider a **Wilcoxon signed-rank** test or a **randomization/sign-flip** test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f621f8",
   "metadata": {},
   "source": [
    "## 4) The test statistic (what you actually compute)\n",
    "\n",
    "Compute:\n",
    "\n",
    "- Sample mean difference\n",
    "  $$\\bar d = \\frac{1}{n}\\sum_{i=1}^n d_i$$\n",
    "\n",
    "- Sample standard deviation of differences\n",
    "  $$s_d = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (d_i-\\bar d)^2}$$\n",
    "\n",
    "- Standard error of the mean difference\n",
    "  $$\\mathrm{SE}(\\bar d) = \\frac{s_d}{\\sqrt{n}}$$\n",
    "\n",
    "The **paired t-statistic** is\n",
    "\n",
    "$$t = \\frac{\\bar d - \\mu_0}{s_d/\\sqrt{n}}, \\qquad \\text{df}=n-1.$$\n",
    "\n",
    "Under $H_0$ and the assumptions, $t$ follows a **Student t distribution** with $n-1$ degrees of freedom.\n",
    "\n",
    "Interpretation of $t$:\n",
    "- $t$ is “**how many standard errors** away from $\\mu_0$ your sample mean difference is”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ba745",
   "metadata": {},
   "source": [
    "## 5) A small worked example (with visuals)\n",
    "\n",
    "We’ll simulate a *before/after* dataset with a modest average increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "\n",
    "before = rng.normal(loc=50, scale=10, size=n)\n",
    "true_effect = 2.5\n",
    "after = before + true_effect + rng.normal(loc=0, scale=4, size=n)\n",
    "\n",
    "diff = after - before\n",
    "\n",
    "print(\"n:\", n)\n",
    "print(\"mean(before):\", before.mean())\n",
    "print(\"mean(after): \", after.mean())\n",
    "print(\"mean(diff): \", diff.mean())\n",
    "print(\"std(diff):  \", diff.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: paired lines (each line = one unit)\n",
    "xs = np.tile([0, 1, np.nan], n)\n",
    "ys = np.empty(3 * n)\n",
    "ys[0::3] = before\n",
    "ys[1::3] = after\n",
    "ys[2::3] = np.nan\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(color=\"rgba(0,0,0,0.25)\", width=1),\n",
    "        marker=dict(size=6, color=\"rgba(0,0,0,0.35)\"),\n",
    "        showlegend=False,\n",
    "        hoverinfo=\"skip\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[before.mean(), after.mean()],\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(color=\"#1f77b4\", width=4),\n",
    "        marker=dict(size=11, color=\"#1f77b4\"),\n",
    "        name=\"Mean\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Paired measurements (each line is one paired unit)\",\n",
    "    xaxis=dict(tickmode=\"array\", tickvals=[0, 1], ticktext=[\"Before\", \"After\"]),\n",
    "    yaxis_title=\"Measurement\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.5,\n",
    "    y=max(before.max(), after.max()),\n",
    "    text=f\"Mean change = {diff.mean():.2f}\",\n",
    "    showarrow=False,\n",
    "    yshift=15,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74247020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: before vs after (diagonal = 'no change')\n",
    "lo = float(min(before.min(), after.min()))\n",
    "hi = float(max(before.max(), after.max()))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=before,\n",
    "        y=after,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10, opacity=0.75, color=\"#636EFA\"),\n",
    "        name=\"Pairs\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[lo, hi],\n",
    "        y=[lo, hi],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", dash=\"dash\"),\n",
    "        name=\"No change (y=x)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Before vs After (points above diagonal are increases)\",\n",
    "    xaxis_title=\"Before\",\n",
    "    yaxis_title=\"After\",\n",
    "    width=650,\n",
    "    height=550,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: distribution of differences\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=diff,\n",
    "        nbinsx=18,\n",
    "        marker_color=\"#00A6D6\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=0,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=\"0 (no change)\",\n",
    "    annotation_position=\"top left\",\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=float(diff.mean()),\n",
    "    line_color=\"#1f77b4\",\n",
    "    line_width=3,\n",
    "    annotation_text=f\"mean = {diff.mean():.2f}\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of within-pair differences (after − before)\",\n",
    "    xaxis_title=\"difference\",\n",
    "    yaxis_title=\"count\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e8b93",
   "metadata": {},
   "source": [
    "## 6) Interpreting the paired t-test\n",
    "\n",
    "The paired t-test produces a **t statistic** and a **p-value**.\n",
    "\n",
    "- The **t statistic** measures how far $\\bar d$ is from $\\mu_0$ in units of its estimated standard error.\n",
    "- The **p-value** is:\n",
    "\n",
    "  > Assuming $H_0$ is true, the probability of seeing a t-statistic at least as extreme as the one we observed.\n",
    "\n",
    "What the p-value **does not** mean:\n",
    "- It is *not* the probability that $H_0$ is true.\n",
    "- It does *not* tell you the size of the effect.\n",
    "\n",
    "A good paired t-test report typically includes:\n",
    "- $\\bar d$ (mean change)\n",
    "- a confidence interval for $\\mu_d$\n",
    "- the t statistic and degrees of freedom\n",
    "- the p-value\n",
    "- an effect size (e.g., Cohen’s $d_z = \\bar d/s_d$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a342f",
   "metadata": {},
   "source": [
    "## 7) NumPy-only implementation (from scratch)\n",
    "\n",
    "Below is a clean “low-level” implementation:\n",
    "\n",
    "- compute $\\bar d$, $s_d$, $\\mathrm{SE}$, $t$ directly\n",
    "- approximate p-values and critical values using **Monte Carlo** samples from the $t$ distribution\n",
    "\n",
    "This is intentionally explicit so you can see the mechanics.\n",
    "\n",
    "In practice you would use SciPy for an exact CDF/PPF-based p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634149f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _paired_clean(x: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    if x.shape != y.shape:\n",
    "        raise ValueError(f\"x and y must have the same shape; got {x.shape} vs {y.shape}\")\n",
    "\n",
    "    mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    if x.size < 2:\n",
    "        raise ValueError(\"Need at least 2 non-NaN pairs\")\n",
    "\n",
    "    d = y - x\n",
    "    return x, y, d\n",
    "\n",
    "\n",
    "def paired_t_statistic(x: np.ndarray, y: np.ndarray, mu0: float = 0.0) -> dict:\n",
    "    '''Compute paired t-statistic ingredients (no CDF/PPF).'''\n",
    "    _, _, d = _paired_clean(x, y)\n",
    "\n",
    "    n = int(d.size)\n",
    "    df = n - 1\n",
    "\n",
    "    dbar = float(d.mean())\n",
    "    sd = float(d.std(ddof=1))\n",
    "    se = sd / math.sqrt(n)\n",
    "\n",
    "    if se == 0.0:\n",
    "        t = math.inf if dbar > mu0 else (-math.inf if dbar < mu0 else 0.0)\n",
    "    else:\n",
    "        t = (dbar - float(mu0)) / se\n",
    "\n",
    "    dz = (dbar - float(mu0)) / sd if sd > 0 else math.inf\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"df\": df,\n",
    "        \"mean_diff\": dbar,\n",
    "        \"sd_diff\": sd,\n",
    "        \"se_mean_diff\": se,\n",
    "        \"t\": float(t),\n",
    "        \"cohens_dz\": float(dz),\n",
    "    }\n",
    "\n",
    "\n",
    "def simulate_student_t(df: int, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''Sample from Student's t using only NumPy RNG primitives.\n",
    "\n",
    "    If Z ~ N(0,1) and V ~ ChiSquare(df) independent, then\n",
    "        T = Z / sqrt(V/df) ~ t_df\n",
    "    '''\n",
    "    if df <= 0:\n",
    "        raise ValueError(\"df must be >= 1\")\n",
    "    z = rng.standard_normal(size=size)\n",
    "    v = rng.chisquare(df=df, size=size)\n",
    "    return z / np.sqrt(v / df)\n",
    "\n",
    "\n",
    "def paired_t_test_numpy(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    mu0: float = 0.0,\n",
    "    alternative: str = \"two-sided\",\n",
    "    alpha: float = 0.05,\n",
    "    n_mc: int = 300_000,\n",
    "    seed: int = 7,\n",
    ") -> dict:\n",
    "    '''Paired t-test with Monte Carlo p-value + CI (NumPy only).\n",
    "\n",
    "    alternative: 'two-sided' | 'greater' | 'less'\n",
    "    '''\n",
    "    alt = alternative.lower().strip()\n",
    "    if alt not in {\"two-sided\", \"greater\", \"less\"}:\n",
    "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "\n",
    "    if not (0 < alpha < 1):\n",
    "        raise ValueError(\"alpha must be in (0,1)\")\n",
    "\n",
    "    base = paired_t_statistic(x, y, mu0=mu0)\n",
    "    t_obs = float(base[\"t\"])\n",
    "    df = int(base[\"df\"])\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    t_null = simulate_student_t(df=df, size=int(n_mc), rng=rng_local)\n",
    "\n",
    "    if alt == \"two-sided\":\n",
    "        p = float(np.mean(np.abs(t_null) >= abs(t_obs)))\n",
    "        tcrit = float(np.quantile(t_null, 1 - alpha / 2))\n",
    "        ci = (\n",
    "            base[\"mean_diff\"] - tcrit * base[\"se_mean_diff\"],\n",
    "            base[\"mean_diff\"] + tcrit * base[\"se_mean_diff\"],\n",
    "        )\n",
    "    elif alt == \"greater\":\n",
    "        p = float(np.mean(t_null >= t_obs))\n",
    "        tcrit = float(np.quantile(t_null, 1 - alpha))\n",
    "        ci = (base[\"mean_diff\"] - tcrit * base[\"se_mean_diff\"], math.inf)\n",
    "    else:  # alt == 'less'\n",
    "        p = float(np.mean(t_null <= t_obs))\n",
    "        tcrit = float(np.quantile(t_null, alpha))  # negative\n",
    "        ci = (-math.inf, base[\"mean_diff\"] - tcrit * base[\"se_mean_diff\"])\n",
    "\n",
    "    return {\n",
    "        **base,\n",
    "        \"mu0\": float(mu0),\n",
    "        \"alternative\": alt,\n",
    "        \"alpha\": float(alpha),\n",
    "        \"p_value_mc\": p,\n",
    "        \"ci_mc\": (float(ci[0]), float(ci[1])),\n",
    "        \"n_mc\": int(n_mc),\n",
    "        \"seed\": int(seed),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603687aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = paired_t_test_numpy(before, after, alternative=\"two-sided\", alpha=0.05, n_mc=400_000, seed=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a8aab",
   "metadata": {},
   "source": [
    "### Reading the output\n",
    "\n",
    "- `mean_diff` is $\\bar d$ (your estimated average change).\n",
    "- `t` is the t-statistic.\n",
    "- `p_value_mc` is a Monte Carlo approximation to the classical p-value.\n",
    "- `ci_mc` is a Monte Carlo-based confidence interval for the mean difference.\n",
    "- `cohens_dz` is an effect size in SD units of the differences.\n",
    "\n",
    "If the CI for $\\mu_d$ **does not include 0**, the two-sided test will have p-value < $\\alpha$ (up to Monte Carlo noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: t distribution with the observed t statistic\n",
    "\n",
    "def student_t_pdf(t: np.ndarray, df: int) -> np.ndarray:\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    v = float(df)\n",
    "    c = math.gamma((v + 1) / 2) / (math.sqrt(v * math.pi) * math.gamma(v / 2))\n",
    "    return c * (1 + (t**2) / v) ** (-(v + 1) / 2)\n",
    "\n",
    "\n",
    "df = int(res[\"df\"])\n",
    "t_obs = float(res[\"t\"])\n",
    "\n",
    "grid = np.linspace(-6, 6, 1200)\n",
    "pdf = student_t_pdf(grid, df=df)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=grid, y=pdf, mode=\"lines\", line=dict(color=\"#636EFA\"), name=f\"t(df={df})\"))\n",
    "\n",
    "thr = abs(t_obs)\n",
    "left = grid <= -thr\n",
    "right = grid >= thr\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=grid[left],\n",
    "        y=pdf[left],\n",
    "        mode=\"lines\",\n",
    "        fill=\"tozeroy\",\n",
    "        line=dict(color=\"rgba(214,39,40,0.1)\"),\n",
    "        fillcolor=\"rgba(214,39,40,0.25)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=grid[right],\n",
    "        y=pdf[right],\n",
    "        mode=\"lines\",\n",
    "        fill=\"tozeroy\",\n",
    "        line=dict(color=\"rgba(214,39,40,0.1)\"),\n",
    "        fillcolor=\"rgba(214,39,40,0.25)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(x=t_obs, line_width=3, line_color=\"black\")\n",
    "fig.add_vline(x=-t_obs, line_width=3, line_color=\"black\", line_dash=\"dot\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"t distribution under H0 (df={df}) with observed t = {t_obs:.3f} (p≈{res['p_value_mc']:.4f})\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f589d",
   "metadata": {},
   "source": [
    "## 8) A null distribution intuition (sign-flip / randomization)\n",
    "\n",
    "Under the paired null $H_0: \\mu_d = 0$, a common non-parametric idea is:\n",
    "\n",
    "- if there is truly no systematic direction of change, then each difference $d_i$ is just as likely to be positive as negative\n",
    "\n",
    "A **sign-flip test** builds a null distribution by randomly multiplying each $d_i$ by $+1$ or $-1$ and recomputing the mean.\n",
    "\n",
    "This is not the same as the t-test, but it is a great intuition-builder:\n",
    "- it shows what “extreme” means\n",
    "- it highlights how the p-value is a tail probability under a null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 50_000\n",
    "signs = rng.choice([-1.0, 1.0], size=(B, n))\n",
    "mean_null = (signs * diff).mean(axis=1)\n",
    "\n",
    "obs = float(diff.mean())\n",
    "p_signflip = float(np.mean(np.abs(mean_null) >= abs(obs)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=mean_null,\n",
    "        nbinsx=80,\n",
    "        marker_color=\"#AB63FA\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=0,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=\"0\",\n",
    "    annotation_position=\"top left\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=obs,\n",
    "    line_color=\"black\",\n",
    "    line_width=3,\n",
    "    annotation_text=f\"observed mean diff = {obs:.2f}\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Sign-flip null distribution of the mean difference (p≈{p_signflip:.4f})\",\n",
    "    xaxis_title=\"mean difference under H0\",\n",
    "    yaxis_title=\"count\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf18fdf",
   "metadata": {},
   "source": [
    "## 9) Practical usage (SciPy) + sanity check\n",
    "\n",
    "The canonical implementation is `scipy.stats.ttest_rel`.\n",
    "\n",
    "If SciPy is available, we can compare results to ensure our t-statistic matches and the p-value is close\n",
    "(Monte Carlo p-values will differ slightly because they are approximations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats is None:\n",
    "    print(\"SciPy not available; skipping this section.\")\n",
    "else:\n",
    "    # SciPy's ttest_rel uses y and x as paired samples.\n",
    "    try:\n",
    "        scipy_res = stats.ttest_rel(after, before, alternative=\"two-sided\")\n",
    "    except TypeError:\n",
    "        # Older SciPy may not support `alternative=`\n",
    "        scipy_res = stats.ttest_rel(after, before)\n",
    "\n",
    "    dbar = diff.mean()\n",
    "    sd = diff.std(ddof=1)\n",
    "    se = sd / math.sqrt(n)\n",
    "    df = n - 1\n",
    "\n",
    "    ci_scipy = stats.t.interval(0.95, df=df, loc=dbar, scale=se)\n",
    "\n",
    "    print(\"SciPy t:\", float(scipy_res.statistic))\n",
    "    print(\"SciPy p:\", float(scipy_res.pvalue))\n",
    "    print(\"SciPy 95% CI for mean diff:\", tuple(map(float, ci_scipy)))\n",
    "\n",
    "    print()\n",
    "    print(\"Our Monte Carlo estimate:\")\n",
    "    print(\"t:\", res[\"t\"])\n",
    "    print(\"p_value_mc:\", res[\"p_value_mc\"])\n",
    "    print(\"ci_mc:\", res[\"ci_mc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045fcb9",
   "metadata": {},
   "source": [
    "## 10) Pitfalls & diagnostics\n",
    "\n",
    "1. **Forgetting the pairing**\n",
    "   - Don’t reorder one vector and not the other.\n",
    "   - Pairing mistakes can completely change results.\n",
    "\n",
    "2. **Interpreting “non-significant” as “no effect”**\n",
    "   - A large p-value means the data are **not very incompatible** with $H_0$.\n",
    "   - It does not prove $H_0$.\n",
    "\n",
    "3. **Ignoring effect size**\n",
    "   - Always report $\\bar d$ and a CI.\n",
    "   - Small effects can be statistically significant with large $n$.\n",
    "\n",
    "4. **Outliers in differences**\n",
    "   - Check the difference distribution.\n",
    "   - Consider robust alternatives if needed.\n",
    "\n",
    "5. **Multiple testing**\n",
    "   - If you run many tests, adjust for multiplicity (or use hierarchical modeling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76f41b",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Change `true_effect` and the noise level in the simulation. How do $t$, the p-value, and the CI respond?\n",
    "2. Increase `n`. When do the Monte Carlo and SciPy p-values become almost identical?\n",
    "3. Replace the Gaussian noise with a heavy-tailed distribution (e.g., Student t with small df). What breaks first?\n",
    "4. Implement a *bootstrap* CI for $\\mu_d$ using only NumPy, then compare it to the t-based CI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}