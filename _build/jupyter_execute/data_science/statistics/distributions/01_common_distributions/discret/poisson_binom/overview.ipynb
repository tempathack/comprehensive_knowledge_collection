{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f511fab8",
   "metadata": {},
   "source": [
    "# Poisson binomial distribution (`poisson_binom`)\n",
    "\n",
    "The **Poisson binomial distribution** is the distribution of the **sum of independent Bernoulli random variables with *different* success probabilities**.\n",
    "\n",
    "If $X_i \\sim \\mathrm{Bernoulli}(p_i)$ independently for $i=1,\\dots,n$, then\n",
    "\n",
    "$$X = \\sum_{i=1}^n X_i \\sim \\mathrm{PoissonBinomial}(p_1,\\dots,p_n).$$\n",
    "\n",
    "## Learning goals\n",
    "- Recognize Poisson binomial data and typical modeling patterns.\n",
    "- Write the PMF and CDF, and understand the generating-function view.\n",
    "- Compute mean/variance/skewness/kurtosis via additivity of cumulants.\n",
    "- Implement PMF/CDF and sampling (NumPy-only) and visualize behavior.\n",
    "- Use `scipy.stats.poisson_binom` for probability calculations and hypothesis tests.\n",
    "\n",
    "## Prerequisites\n",
    "- Expectation and variance (linearity and independence)\n",
    "- Comfort with logs, products, and basic numerical computing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f77a6",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations (Expectation, Variance, Likelihood)\n",
    "7. Sampling & Simulation (NumPy-only)\n",
    "8. Visualization (PMF, CDF, Monte Carlo)\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit, xlogy\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ed2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import plotly\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"SciPy:\", scipy.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"Seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17c5b6",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `poisson_binom` (Poisson binomial distribution)\n",
    "- **Type**: **Discrete**\n",
    "- **Support**: $k \\in \\{0,1,\\dots,n\\}$ where $n = \\lvert\\mathbf p\\rvert$ is the number of Bernoulli trials\n",
    "- **Parameter space**:\n",
    "  - $\\mathbf p = (p_1,\\dots,p_n)$ with $0 \\le p_i \\le 1$ for all $i$.\n",
    "  - (SciPy) an integer shift `loc` is also available; it shifts the support to $\\{\\mathrm{loc}, \\dots, \\mathrm{loc}+n\\}$.\n",
    "\n",
    "Notation:\n",
    "- $X \\sim \\mathrm{PoissonBinomial}(p_1,\\dots,p_n)$.\n",
    "- Sometimes written as $X \\sim \\mathrm{PB}(\\mathbf p)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4516572",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "You observe **a count of successes** across $n$ Bernoulli trials, but the trials are *heterogeneous*:\n",
    "\n",
    "- Trial $i$ succeeds with probability $p_i$.\n",
    "- Trials are **independent**, but the $p_i$ are not necessarily equal.\n",
    "\n",
    "So $X$ is a *heterogeneous* version of a binomial count.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Reliability engineering**: number of component failures when components have different failure probabilities.\n",
    "- **Marketing / product analytics**: number of conversions when each user/session has a different conversion probability.\n",
    "- **Elections / forecasting**: number of wins across districts with different win probabilities.\n",
    "- **Multiple testing**: number of discoveries when each test has a different Type-I error probability (or power).\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Binomial**: if all $p_i$ are equal to a common $p$, then $X \\sim \\mathrm{Binomial}(n,p)$.\n",
    "- **Poisson approximation**: if all $p_i$ are small and $\\lambda = \\sum_i p_i$ is moderate, then $X \\approx \\mathrm{Poisson}(\\lambda)$.\n",
    "- **Normal approximation (CLT)**: for large $n$ with non-degenerate variance, $X$ is approximately normal with mean $\\sum_i p_i$ and variance $\\sum_i p_i(1-p_i)$.\n",
    "- **Sum of independent Bernoulli**: conceptually, Poisson binomial *is* the distribution of that sum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a1cd2",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $X_1,\\dots,X_n$ be independent with\n",
    "\n",
    "$$\\mathbb P(X_i = 1) = p_i, \\qquad \\mathbb P(X_i = 0) = 1-p_i.$$\n",
    "\n",
    "Define\n",
    "\n",
    "$$X = \\sum_{i=1}^n X_i.$$\n",
    "\n",
    "### PMF\n",
    "For $k \\in \\{0,1,\\dots,n\\}$,\n",
    "\n",
    "$$\\mathbb P(X = k) = \\sum_{A \\subseteq \\{1,\\dots,n\\}:\\, |A|=k} \\ \\prod_{i\\in A} p_i \\prod_{j\\notin A} (1-p_j).$$\n",
    "\n",
    "A very useful equivalent view is via the **probability generating function** (PGF):\n",
    "\n",
    "$$G_X(z) = \\mathbb E[z^X] = \\prod_{i=1}^n \\big((1-p_i) + p_i z\\big).$$\n",
    "\n",
    "Then $\\mathbb P(X=k)$ is exactly the coefficient of $z^k$ in $G_X(z)$:\n",
    "\n",
    "$$\\mathbb P(X=k) = [z^k] \\prod_{i=1}^n \\big((1-p_i) + p_i z\\big).$$\n",
    "\n",
    "### CDF\n",
    "The CDF is the partial sum of the PMF:\n",
    "\n",
    "$$F(k) = \\mathbb P(X \\le k) = \\sum_{j=0}^{\\lfloor k\\rfloor} \\mathbb P(X=j).$$\n",
    "\n",
    "There is no single closed form for general $(p_1,\\dots,p_n)$, but it is efficiently computable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_p_vector(p) -> np.ndarray:\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if p.ndim != 1:\n",
    "        raise ValueError(\"p must be a 1D array-like of probabilities\")\n",
    "    if p.size == 0:\n",
    "        raise ValueError(\"p must have at least one element\")\n",
    "    if not np.all(np.isfinite(p)):\n",
    "        raise ValueError(\"p must be finite\")\n",
    "    if np.any((p < 0.0) | (p > 1.0)):\n",
    "        raise ValueError(\"each p_i must be in [0, 1]\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def poisson_binom_support(p: np.ndarray) -> np.ndarray:\n",
    "    p = validate_p_vector(p)\n",
    "    return np.arange(p.size + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_binom_pmf_array(p: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (k, pmf[k]) for k=0..n using a stable O(n^2) recursion.\n",
    "\n",
    "    Recurrence: after processing probabilities p_1..p_i,\n",
    "        P_i(k) = P_{i-1}(k) (1-p_i) + P_{i-1}(k-1) p_i.\n",
    "    \"\"\"\n",
    "    p = validate_p_vector(p)\n",
    "    n = p.size\n",
    "\n",
    "    pmf = np.zeros(n + 1, dtype=float)\n",
    "    pmf[0] = 1.0\n",
    "\n",
    "    for pi in p:\n",
    "        pmf_next = np.zeros_like(pmf)\n",
    "        pmf_next[0] = pmf[0] * (1.0 - pi)\n",
    "        pmf_next[1:] = pmf[1:] * (1.0 - pi) + pmf[:-1] * pi\n",
    "        pmf = pmf_next\n",
    "\n",
    "    pmf = np.clip(pmf, 0.0, 1.0)\n",
    "    pmf = pmf / pmf.sum()  # guard against tiny rounding drift\n",
    "\n",
    "    k = np.arange(n + 1)\n",
    "    return k, pmf\n",
    "\n",
    "\n",
    "def poisson_binom_logpmf(k, p: np.ndarray) -> np.ndarray:\n",
    "    p = validate_p_vector(p)\n",
    "    n = p.size\n",
    "\n",
    "    k_arr = np.asarray(k)\n",
    "    out = np.full(k_arr.shape, -np.inf, dtype=float)\n",
    "\n",
    "    k_int = k_arr.astype(int)\n",
    "    valid = (k_int == k_arr) & (k_int >= 0) & (k_int <= n)\n",
    "    if not np.any(valid):\n",
    "        return out\n",
    "\n",
    "    _, pmf = poisson_binom_pmf_array(p)\n",
    "    out[valid] = np.log(pmf[k_int[valid]])\n",
    "    return out\n",
    "\n",
    "\n",
    "def poisson_binom_pmf(k, p: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(poisson_binom_logpmf(k, p))\n",
    "\n",
    "\n",
    "def poisson_binom_cdf(x, p: np.ndarray) -> np.ndarray:\n",
    "    p = validate_p_vector(p)\n",
    "    n = p.size\n",
    "\n",
    "    x_arr = np.asarray(x)\n",
    "    out = np.zeros_like(x_arr, dtype=float)\n",
    "\n",
    "    out[x_arr >= n] = 1.0\n",
    "    inside = (x_arr >= 0) & (x_arr < n)\n",
    "    if np.any(inside):\n",
    "        k = np.floor(x_arr[inside]).astype(int)\n",
    "        _, pmf = poisson_binom_pmf_array(p)\n",
    "        cdf = np.cumsum(pmf)\n",
    "        out[inside] = cdf[k]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2b402",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Because $X$ is a **sum of independent Bernoulli variables**, many properties follow from additivity.\n",
    "\n",
    "### Mean and variance\n",
    "Using linearity of expectation and independence:\n",
    "\n",
    "$$\\mathbb E[X] = \\sum_{i=1}^n p_i,\\qquad \\mathrm{Var}(X) = \\sum_{i=1}^n p_i(1-p_i).$$\n",
    "\n",
    "### Higher standardized moments\n",
    "A convenient way to get skewness/kurtosis is via **cumulants**. For a Bernoulli$(p)$ variable, the first four cumulants are:\n",
    "\n",
    "\\begin{align}\n",
    "\\kappa_1 &= p \\\\\n",
    "\\kappa_2 &= p(1-p) \\\\\n",
    "\\kappa_3 &= p(1-p)(1-2p) \\\\\n",
    "\\kappa_4 &= p(1-p)(1 - 6p + 6p^2).\n",
    "\\end{align}\n",
    "\n",
    "For independent sums, cumulants add: $\\kappa_r(X) = \\sum_i \\kappa_r(X_i)$.\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align}\n",
    "\\text{skewness} \\;\\gamma_1 &= \\frac{\\kappa_3}{\\kappa_2^{3/2}}, \\\\\n",
    "\\text{excess kurtosis} \\;\\gamma_2 &= \\frac{\\kappa_4}{\\kappa_2^2}.\n",
    "\\end{align}\n",
    "\n",
    "### MGF and characteristic function\n",
    "The moment generating function (MGF) and characteristic function factorize:\n",
    "\n",
    "\\begin{align}\n",
    "M_X(t) &= \\mathbb E[e^{tX}] = \\prod_{i=1}^n \\big((1-p_i) + p_i e^t\\big), \\\\\n",
    "\\varphi_X(\\omega) &= \\mathbb E[e^{i\\omega X}] = \\prod_{i=1}^n \\big((1-p_i) + p_i e^{i\\omega}\\big).\n",
    "\\end{align}\n",
    "\n",
    "### Entropy\n",
    "There is no simple closed form in general. Given the PMF, entropy (in nats) is\n",
    "\n",
    "$$H(X) = -\\sum_{k=0}^n \\mathbb P(X=k) \\log \\mathbb P(X=k).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_binom_cumulants(p: np.ndarray) -> dict:\n",
    "    p = validate_p_vector(p)\n",
    "    k1 = float(np.sum(p))\n",
    "    k2 = float(np.sum(p * (1.0 - p)))\n",
    "    k3 = float(np.sum(p * (1.0 - p) * (1.0 - 2.0 * p)))\n",
    "    k4 = float(np.sum(p * (1.0 - p) * (1.0 - 6.0 * p + 6.0 * p**2)))\n",
    "    return {\"k1\": k1, \"k2\": k2, \"k3\": k3, \"k4\": k4}\n",
    "\n",
    "\n",
    "def poisson_binom_moments(p: np.ndarray) -> dict:\n",
    "    ks = poisson_binom_cumulants(p)\n",
    "    mean = ks[\"k1\"]\n",
    "    var = ks[\"k2\"]\n",
    "\n",
    "    if var == 0.0:\n",
    "        skew = float(\"nan\")\n",
    "        excess_kurt = float(\"nan\")\n",
    "    else:\n",
    "        skew = ks[\"k3\"] / (var ** 1.5)\n",
    "        excess_kurt = ks[\"k4\"] / (var ** 2)\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"var\": var,\n",
    "        \"skew\": skew,\n",
    "        \"excess_kurt\": excess_kurt,\n",
    "    }\n",
    "\n",
    "\n",
    "def poisson_binom_log_mgf(t, p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"log M_X(t) = sum log((1-p_i) + p_i e^t).\"\"\"\n",
    "    p = validate_p_vector(p)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "\n",
    "    # logaddexp(log(1-p), log(p)+t) is stable at p=0 or p=1\n",
    "    log_terms = np.logaddexp(np.log1p(-p), np.log(p) + t[..., None])\n",
    "    return np.sum(log_terms, axis=-1)\n",
    "\n",
    "\n",
    "def poisson_binom_mgf(t, p: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(poisson_binom_log_mgf(t, p))\n",
    "\n",
    "\n",
    "def poisson_binom_cf(omega, p: np.ndarray) -> np.ndarray:\n",
    "    p = validate_p_vector(p)\n",
    "    omega = np.asarray(omega)\n",
    "    return np.prod(1.0 - p + p * np.exp(1j * omega[..., None]), axis=-1)\n",
    "\n",
    "\n",
    "def poisson_binom_entropy(p: np.ndarray, *, base=np.e) -> float:\n",
    "    p = validate_p_vector(p)\n",
    "    _, pmf = poisson_binom_pmf_array(p)\n",
    "\n",
    "    H_nats = -float(np.sum(xlogy(pmf, pmf)))\n",
    "    if base == np.e:\n",
    "        return H_nats\n",
    "    return H_nats / float(np.log(base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([0.10, 0.25, 0.60, 0.80])\n",
    "\n",
    "k, pmf = poisson_binom_pmf_array(p)\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "mom = poisson_binom_moments(p)\n",
    "print(\"p:\", p)\n",
    "print(\"support:\", k)\n",
    "print(\"pmf sum:\", pmf.sum())\n",
    "print(\"mean/var/skew/excess_kurt (from cumulants):\", mom)\n",
    "print(\"entropy (nats):\", poisson_binom_entropy(p))\n",
    "\n",
    "# Monte Carlo sanity check\n",
    "samples = (rng.random((200_000, p.size)) < p).sum(axis=1)\n",
    "print(\"MC mean:\", samples.mean(), \" | formula:\", mom[\"mean\"])\n",
    "print(\"MC var :\", samples.var(ddof=0), \" | formula:\", mom[\"var\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e28c7",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The parameter vector $\\mathbf p=(p_1,\\dots,p_n)$ is the list of **per-trial success probabilities**.\n",
    "\n",
    "- Each $p_i$ controls how likely trial $i$ is to contribute a 1 to the sum.\n",
    "- The **mean** depends only on the sum $\\sum_i p_i$.\n",
    "- The **variance** depends on $\\sum_i p_i(1-p_i)$.\n",
    "  - For a fixed mean, variance is *maximized* when the $p_i$ are all equal (a consequence of concavity of $p(1-p)$).\n",
    "  - Making probabilities more extreme (closer to 0 or 1) typically *reduces* variance and makes the distribution more concentrated.\n",
    "\n",
    "Because the parameters are a vector, many different shapes are possible even with the same mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pmf_comparison(ps: dict[str, np.ndarray], *, title: str):\n",
    "    fig = go.Figure()\n",
    "    for name, pvec in ps.items():\n",
    "        k, pmf = poisson_binom_pmf_array(pvec)\n",
    "        fig.add_trace(go.Scatter(x=k, y=pmf, mode=\"lines+markers\", name=name))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"k (number of successes)\",\n",
    "        yaxis_title=\"P(X = k)\",\n",
    "        legend_title=\"parameter set\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "n = 30\n",
    "mean_target = 0.30\n",
    "\n",
    "p_equal = np.full(n, mean_target)\n",
    "\n",
    "# Same mean, more heterogeneous probabilities\n",
    "p_mixture = np.r_[np.full(n // 2, 0.10), np.full(n - n // 2, 0.50)]  # mean = 0.30\n",
    "p_extreme = np.r_[np.zeros(n // 2), np.full(n - n // 2, 0.60)]        # mean = 0.30\n",
    "\n",
    "ps = {\n",
    "    \"all p_i = 0.30\": p_equal,\n",
    "    \"half 0.10, half 0.50\": p_mixture,\n",
    "    \"half 0.00, half 0.60\": p_extreme,\n",
    "}\n",
    "\n",
    "for name, pvec in ps.items():\n",
    "    m = poisson_binom_moments(pvec)\n",
    "    print(f\"{name:>22} | mean={m['mean']:.2f}, var={m['var']:.2f}\")\n",
    "\n",
    "show_pmf_comparison(ps, title=\"Same mean, different heterogeneity → different variance/shape\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9a6b9",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### A) Expectation\n",
    "Because $X = \\sum_i X_i$,\n",
    "\n",
    "$$\\mathbb E[X] = \\sum_{i=1}^n \\mathbb E[X_i] = \\sum_{i=1}^n p_i.$$\n",
    "\n",
    "### B) Variance\n",
    "For independent $X_i$,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{Var}(X) &= \\mathrm{Var}\\left(\\sum_i X_i\\right)\n",
    "= \\sum_i \\mathrm{Var}(X_i)\n",
    "= \\sum_i p_i(1-p_i).\n",
    "\\end{align}\n",
    "\n",
    "### C) Likelihood\n",
    "Suppose you observe i.i.d. samples $x_1,\\dots,x_m$ of the *sum* $X$ (each sample is a sum of $n$ Bernoulli trials with the same probability vector $\\mathbf p$).\n",
    "\n",
    "The likelihood is\n",
    "\n",
    "$$L(\\mathbf p; x_{1:m}) = \\prod_{j=1}^m \\mathbb P_{\\mathbf p}(X=x_j).$$\n",
    "\n",
    "Equivalently, the log-likelihood is\n",
    "\n",
    "$$\\ell(\\mathbf p; x_{1:m}) = \\sum_{j=1}^m \\log \\mathbb P_{\\mathbf p}(X=x_j).$$\n",
    "\n",
    "Two important practical notes:\n",
    "- The likelihood is invariant to permuting the entries of $\\mathbf p$ (only the multiset of probabilities matters).\n",
    "- Estimating a full length-$n$ vector $\\mathbf p$ from only the sums is often ill-posed unless $n$ is small or you add structure/priors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_binom_log_likelihood(p: np.ndarray, data: np.ndarray) -> float:\n",
    "    p = validate_p_vector(p)\n",
    "    data = np.asarray(data)\n",
    "\n",
    "    n = p.size\n",
    "    if np.any((data < 0) | (data > n) | (data != data.astype(int))):\n",
    "        return -np.inf\n",
    "\n",
    "    _, pmf = poisson_binom_pmf_array(p)\n",
    "    logpmf = np.log(pmf)\n",
    "    return float(np.sum(logpmf[data.astype(int)]))\n",
    "\n",
    "\n",
    "# A single-observation likelihood surface (n=2)\n",
    "# This is small enough that we can visualize log L(p1,p2) on a grid.\n",
    "\n",
    "x_obs = 1\n",
    "p1_grid = np.linspace(0.01, 0.99, 80)\n",
    "p2_grid = np.linspace(0.01, 0.99, 80)\n",
    "\n",
    "LL = np.empty((p1_grid.size, p2_grid.size))\n",
    "for i, p1 in enumerate(p1_grid):\n",
    "    for j, p2 in enumerate(p2_grid):\n",
    "        LL[i, j] = poisson_binom_log_likelihood(np.array([p1, p2]), np.array([x_obs]))\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        x=p2_grid,\n",
    "        y=p1_grid,\n",
    "        z=LL,\n",
    "        colorscale=\"Viridis\",\n",
    "        colorbar_title=\"log L\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"Log-likelihood surface for a single observation x={x_obs} (n=2)\",\n",
    "    xaxis_title=\"p2\",\n",
    "    yaxis_title=\"p1\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea33b96",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "The most direct sampler matches the definition:\n",
    "\n",
    "1. For each trial $i$, draw $X_i \\sim \\mathrm{Bernoulli}(p_i)$.\n",
    "2. Return $X = \\sum_i X_i$.\n",
    "\n",
    "This is $\\mathcal O(n)$ work per sample and is typically fast in NumPy due to vectorization.\n",
    "\n",
    "Below is a NumPy-only implementation using uniforms: $X_i = \\mathbb 1\\{U_i < p_i\\}$ with $U_i \\sim \\mathrm{Uniform}(0,1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cc4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_poisson_binom_numpy(p: np.ndarray, size=1, *, rng: np.random.Generator) -> np.ndarray:\n",
    "    p = validate_p_vector(p)\n",
    "    n = p.size\n",
    "\n",
    "    if isinstance(size, (int, np.integer)):\n",
    "        size_tuple = (int(size),)\n",
    "    else:\n",
    "        size_tuple = tuple(size)\n",
    "\n",
    "    u = rng.random((*size_tuple, n))\n",
    "    return (u < p).sum(axis=-1)\n",
    "\n",
    "\n",
    "p = np.array([0.10, 0.25, 0.60, 0.80])\n",
    "\n",
    "s = sample_poisson_binom_numpy(p, size=10, rng=rng)\n",
    "print(\"samples:\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969546af",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We'll visualize:\n",
    "- the exact PMF (via dynamic programming),\n",
    "- the exact CDF,\n",
    "- and Monte Carlo samples from the NumPy-only sampler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([0.05, 0.10, 0.20, 0.35, 0.55, 0.70, 0.85])\n",
    "\n",
    "k, pmf = poisson_binom_pmf_array(p)\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "mc = sample_poisson_binom_numpy(p, size=100_000, rng=rng)\n",
    "emp = np.bincount(mc, minlength=p.size + 1) / mc.size\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"PMF\", \"CDF\"))\n",
    "\n",
    "fig.add_trace(go.Bar(x=k, y=pmf, name=\"Exact PMF\", opacity=0.65), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=k, y=emp, mode=\"markers\", name=\"MC empirical PMF\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=k, y=cdf, mode=\"lines+markers\", name=\"Exact CDF\"), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Poisson binomial with n={p.size} (heterogeneous p_i)\",\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"P(X=k)\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"k\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"P(X≤k)\", row=1, col=2, range=[0, 1.02])\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba8f3c",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy (v1.15+) includes `scipy.stats.poisson_binom`.\n",
    "\n",
    "Key methods:\n",
    "- `pmf`, `logpmf`\n",
    "- `cdf`, `sf` (survival function; often numerically better for upper tails)\n",
    "- `rvs` (sampling)\n",
    "- `stats(..., moments='mvsk')` (mean/var/skew/**excess** kurtosis)\n",
    "- `entropy`\n",
    "\n",
    "We'll compare SciPy's PMF to our NumPy recursion and show typical usage.\n",
    "\n",
    "**About fitting**: SciPy's generic `scipy.stats.fit` does not currently support `poisson_binom` because the shape parameter `p` is vector-valued.\n",
    "\n",
    "**About fitting**: `poisson_binom` has a *vector-valued* parameter `p`, and generic MLE fitting utilities are not currently designed for that case. In practice, you usually:\n",
    "- treat $\\mathbf p$ as known/estimated from other data, and then use the distribution for inference on the sum, or\n",
    "- impose structure on $\\mathbf p$ (e.g., a low-dimensional model) and fit that model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76250b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([0.05, 0.10, 0.20, 0.35, 0.55, 0.70, 0.85])\n",
    "\n",
    "rv = stats.poisson_binom(p)\n",
    "\n",
    "k = np.arange(p.size + 1)\n",
    "\n",
    "pmf_scipy = rv.pmf(k)\n",
    "_, pmf_numpy = poisson_binom_pmf_array(p)\n",
    "\n",
    "print(\"max |pmf_scipy - pmf_numpy| =\", float(np.max(np.abs(pmf_scipy - pmf_numpy))))\n",
    "\n",
    "\n",
    "cdf_scipy = rv.cdf(k)\n",
    "cdf_numpy = poisson_binom_cdf(k, p)\n",
    "print(\"max |cdf_scipy - cdf_numpy| =\", float(np.max(np.abs(cdf_scipy - cdf_numpy))))\n",
    "\n",
    "mean, var, skew, excess_kurt = rv.stats(moments=\"mvsk\")\n",
    "print(\"SciPy mean/var/skew/excess_kurt:\", float(mean), float(var), float(skew), float(excess_kurt))\n",
    "print(\"NumPy mean/var/skew/excess_kurt:\", poisson_binom_moments(p))\n",
    "\n",
    "print(\"SciPy entropy (nats):\", float(rv.entropy()))\n",
    "\n",
    "# Sampling\n",
    "r = rv.rvs(size=10_000, random_state=rng)\n",
    "print(\"sample mean (SciPy rvs):\", r.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to use scipy.stats.fit (as of SciPy 1.15)\n",
    "#\n",
    "# poisson_binom has a vector-valued shape parameter p, and SciPy's generic fitter\n",
    "# isn't currently set up to optimize vector-valued shape parameters.\n",
    "\n",
    "import warnings\n",
    "\n",
    "p_true = np.array([0.2, 0.5, 0.7])\n",
    "data = stats.poisson_binom(p_true).rvs(size=500, random_state=rng)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    try:\n",
    "        fit_res = stats.fit(stats.poisson_binom, data)\n",
    "        print(fit_res)\n",
    "    except Exception as e:\n",
    "        print(\"scipy.stats.fit is not currently supported for poisson_binom (vector-valued p).\")\n",
    "        print(\"Error:\", type(e).__name__, \"-\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: demonstrate a simple MLE when n is small (educational)\n",
    "#\n",
    "# Warning: estimating a full vector p from only sum data is often ill-posed.\n",
    "# Here we fit a small n and show that recovery is up to permutation.\n",
    "\n",
    "p_true = np.array([0.15, 0.35, 0.60, 0.85])\n",
    "rv_true = stats.poisson_binom(p_true)\n",
    "\n",
    "data = rv_true.rvs(size=2_000, random_state=rng)\n",
    "\n",
    "\n",
    "def neg_log_likelihood_unconstrained(q: np.ndarray) -> float:\n",
    "    # unconstrained q in R^n -> p in (0,1) via logistic transform\n",
    "    p = expit(q)\n",
    "    ll = poisson_binom_log_likelihood(p, data)\n",
    "    return -ll if np.isfinite(ll) else 1e30\n",
    "\n",
    "\n",
    "# initialize near a binomial approximation: all p_i equal to sample_mean / n\n",
    "p0 = np.full_like(p_true, data.mean() / p_true.size)\n",
    "q0 = np.log(p0) - np.log1p(-p0)\n",
    "\n",
    "res = minimize(neg_log_likelihood_unconstrained, q0, method=\"BFGS\")\n",
    "p_hat = expit(res.x)\n",
    "\n",
    "print(\"converged:\", res.success)\n",
    "print(\"true p (sorted):\", np.sort(p_true))\n",
    "print(\"mle  p (sorted):\", np.sort(p_hat))\n",
    "print(\"note: order is not identifiable; only the multiset matters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5adcb",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (tail probabilities)\n",
    "If you know/assume the probabilities $\\mathbf p$, you can test whether an observed count $x$ is unusually large/small.\n",
    "\n",
    "Example: under the null model $X \\sim \\mathrm{PB}(\\mathbf p)$,\n",
    "\n",
    "- upper-tail p-value: $\\mathbb P(X \\ge x) = \\mathrm{sf}(x-1)$\n",
    "- lower-tail p-value: $\\mathbb P(X \\le x) = \\mathrm{cdf}(x)$\n",
    "\n",
    "### B) Bayesian modeling (uncertain probabilities)\n",
    "If each $p_i$ is uncertain (e.g., you have a posterior over $p_i$), the posterior predictive distribution of the sum is a **mixture of Poisson binomials**.\n",
    "\n",
    "A simple approach is Monte Carlo:\n",
    "1. sample $\\mathbf p^{(s)}$ from the posterior,\n",
    "2. compute the Poisson binomial PMF for each draw,\n",
    "3. average the PMFs.\n",
    "\n",
    "### C) Generative modeling\n",
    "Poisson binomial is a natural building block for generative models that produce *counts* from heterogeneous binary events (e.g., conversions, failures, wins).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b24ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Hypothesis test example: is an observed count unusually high?\n",
    "\n",
    "p = np.array([0.02, 0.05, 0.08, 0.10, 0.15, 0.20, 0.25, 0.30])\n",
    "rv = stats.poisson_binom(p)\n",
    "\n",
    "x_obs = 5\n",
    "p_value_upper = rv.sf(x_obs - 1)\n",
    "\n",
    "print(\"Observed x =\", x_obs)\n",
    "print(\"Upper-tail p-value P(X >= x) =\", float(p_value_upper))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Bayesian predictive example (Monte Carlo mixture)\n",
    "#\n",
    "# Suppose each p_i has a Beta prior and we observed (s_i successes out of m_i trials)\n",
    "# in some historical data. We want the predictive distribution for the *next* round\n",
    "# of n heterogeneous Bernoulli events.\n",
    "\n",
    "from scipy.stats import beta\n",
    "\n",
    "rng2 = np.random.default_rng(SEED)\n",
    "\n",
    "n = 12\n",
    "\n",
    "a0, b0 = 1.0, 1.0  # uniform Beta prior\n",
    "m_i = rng2.integers(20, 80, size=n)\n",
    "\n",
    "# synthetic historical success counts\n",
    "p_latent = rng2.uniform(0.05, 0.8, size=n)\n",
    "s_i = rng2.binomial(m_i, p_latent)\n",
    "\n",
    "# posterior is Beta(a0+s_i, b0+m_i-s_i)\n",
    "a_post = a0 + s_i\n",
    "b_post = b0 + m_i - s_i\n",
    "\n",
    "S = 5_000  # posterior draws\n",
    "pmf_accum = np.zeros(n + 1)\n",
    "\n",
    "for _ in range(S):\n",
    "    p_draw = beta.rvs(a_post, b_post, random_state=rng2)\n",
    "    _, pmf_draw = poisson_binom_pmf_array(p_draw)\n",
    "    pmf_accum += pmf_draw\n",
    "\n",
    "pmf_pred = pmf_accum / S\n",
    "k = np.arange(n + 1)\n",
    "\n",
    "fig = go.Figure(go.Bar(x=k, y=pmf_pred))\n",
    "fig.update_layout(\n",
    "    title=\"Posterior predictive distribution of the sum (mixture of Poisson binomials)\",\n",
    "    xaxis_title=\"k (successes)\",\n",
    "    yaxis_title=\"Predictive probability\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6114c",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: each $p_i$ must lie in $[0,1]$. Values very close to 0/1 can make the distribution nearly degenerate.\n",
    "- **Dependence**: Poisson binomial assumes independent trials. Positive correlation typically inflates variance relative to the model.\n",
    "- **Computing the PMF**:\n",
    "  - Naively summing over subsets is $\\mathcal O(2^n)$ and infeasible.\n",
    "  - The dynamic-programming recursion is $\\mathcal O(n^2)$ and is fine for moderate $n$.\n",
    "  - For large $n$, FFT-based methods can be much faster (SciPy uses fast algorithms internally).\n",
    "- **Tail probabilities**: prefer `sf` over `1-cdf` for upper tails to reduce catastrophic cancellation.\n",
    "- **Fitting**: estimating a full vector $\\mathbf p$ from only sum observations is typically ill-posed; consider adding structure (GLM, hierarchical priors) or using additional data.\n",
    "- **Serialization**: SciPy's `poisson_binom` instances currently do not support pickling/unpickling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7969c",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `poisson_binom` models the number of successes in **independent, non-identical Bernoulli trials**.\n",
    "- The PMF is the coefficient of a polynomial / PGF; dynamic programming computes it in $\\mathcal O(n^2)$ time.\n",
    "- Mean and variance are simple sums: $\\sum p_i$ and $\\sum p_i(1-p_i)$; higher cumulants add as well.\n",
    "- NumPy-only simulation is straightforward: sample each Bernoulli and sum.\n",
    "- SciPy’s `scipy.stats.poisson_binom` provides PMF/CDF/SF/RVS/stats/entropy and is the go-to tool for inference on the sum.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}