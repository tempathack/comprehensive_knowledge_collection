{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051ea21c",
   "metadata": {},
   "source": [
    "# `nchypergeom_wallenius` (Wallenius' noncentral hypergeometric distribution)\n",
    "\n",
    "Wallenius' noncentral hypergeometric distribution models **biased sampling without replacement** from a finite population with **two types** of objects.\n",
    "\n",
    "This notebook uses the same parameterization as `scipy.stats.nchypergeom_wallenius`:\n",
    "\n",
    "- `M` = total population size (integer)\n",
    "- `n` = number of Type I objects in the population (integer)\n",
    "- `N` = number of draws (integer)\n",
    "- `odds` = odds ratio \\(\\omega>0\\) (real)\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "\n",
    "- recognize when Wallenius' noncentral hypergeometric model is appropriate\n",
    "- write down the PMF/CDF and understand the sampling mechanism\n",
    "- compute mean/variance/skewness/kurtosis and entropy **numerically**\n",
    "- implement **NumPy-only** sampling (sequential biased draws)\n",
    "- visualize PMF/CDF and validate with Monte Carlo simulation\n",
    "- use `scipy.stats.nchypergeom_wallenius` for computation and inference workflows\n",
    "\n",
    "## Table of contents\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fa8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34486a0",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name**: `nchypergeom_wallenius` (Wallenius' noncentral hypergeometric distribution)  \n",
    "**Type**: **Discrete**\n",
    "\n",
    "We follow the SciPy naming/notation:\n",
    "\n",
    "- `M` = total population size\n",
    "- `n` = number of Type I objects (so Type II count is `M-n`)\n",
    "- `N` = number of draws (without replacement)\n",
    "- `odds` = \\(\\omega\\), the odds ratio favoring Type I\n",
    "\n",
    "Let \\(X\\) be the number of Type I objects drawn after `N` biased draws.\n",
    "\n",
    "**Support**:\n",
    "\\[\n",
    "x \\in \\{x_\\ell, x_\\ell+1, \\dots, x_u\\}\n",
    "\\]\n",
    "where\n",
    "\\[\n",
    "x_\\ell = \\max\\bigl(0,\\; N-(M-n)\\bigr),\\qquad x_u = \\min(N, n).\n",
    "\\]\n",
    "\n",
    "**Parameter space**:\n",
    "\\[\n",
    "M\\in\\mathbb{N},\\qquad 0\\le n\\le M,\\qquad 0\\le N\\le M,\\qquad \\omega>0.\n",
    "\\]\n",
    "\n",
    "Interpretation:\n",
    "- `M, n` describe the **composition** of the urn/population\n",
    "- `N` is how many draws you take\n",
    "- `odds` controls how strongly Type I is preferred (\\(\\omega>1\\)) or discouraged (\\(\\omega<1\\))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9e18d",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### The sampling story (biased urn)\n",
    "Imagine an urn with:\n",
    "\n",
    "- `n` red balls (Type I)\n",
    "- `M-n` blue balls (Type II)\n",
    "\n",
    "You draw exactly `N` balls **one-by-one without replacement**, but the draw is **biased**:\n",
    "\n",
    "- if there are \\(r\\) red and \\(b\\) blue remaining, then\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(\\text{next draw is Type I}\\mid r,b) = \\frac{\\omega r}{\\omega r + b}\n",
    "\\]\n",
    "\n",
    "So \\(\\omega\\) multiplies the *attractiveness* of Type I relative to Type II.\n",
    "\n",
    "A key sanity check: if there is **one ball of each color** left, then\n",
    "\\[\n",
    "\\mathbb{P}(\\text{Type I}) = \\frac{\\omega}{\\omega+1}.\n",
    "\\]\n",
    "\n",
    "### What this distribution models\n",
    "- **Preferential selection** without replacement (finite population correction matters)\n",
    "- **Biased sampling** from two categories when the bias changes dynamically as items are removed\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Selection bias / preferential sampling**: a subgroup is more likely to be selected, but you cannot select the same individual twice\n",
    "- **Ecology / capture–recapture**: heterogeneous catchability (Type I vs Type II) in a finite pool\n",
    "- **Auditing / inspections**: inspectors target one class of items more aggressively while sampling without replacement\n",
    "- **Recommendation / ranking pipelines**: drawing items sequentially with different propensities until a quota is met\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Hypergeometric**: when \\(\\omega=1\\), the bias disappears and we recover the ordinary hypergeometric distribution.\n",
    "- **Fisher's noncentral hypergeometric** (`nchypergeom_fisher`): another “noncentral hypergeometric” distribution with the same parameters but a *different generative story*.\n",
    "  - Wallenius: biased **sequential** draws (odds applied at each draw)\n",
    "  - Fisher: models a different conditioning story (“a handful of objects taken at once”)\n",
    "- **Binomial approximation**: if `M` is large and `N` is small (sampling fraction \\(N/M\\) is tiny), the changing composition is negligible and\n",
    "\n",
    "\\[\n",
    "X \\approx \\mathrm{Binomial}\\Bigl(N,\\; p\\Bigr),\\qquad\n",
    "p = \\frac{\\omega\\,(n/M)}{\\omega\\,(n/M) + (1-n/M)} = \\frac{\\omega n}{\\omega n + (M-n)}.\n",
    "\\]\n",
    "\n",
    "That is: for small sampling fractions, “biased without replacement” is close to “biased with replacement”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb973877",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### Generative definition (sequential without replacement)\n",
    "Let \\(R_0=n\\) and \\(B_0=M-n\\) be the initial counts (Type I and II remaining).\n",
    "For draws \\(t=1,2,\\dots,N\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(\\text{draw Type I at step }t\\mid R_{t-1},B_{t-1})\n",
    "= \\frac{\\omega R_{t-1}}{\\omega R_{t-1}+B_{t-1}}.\n",
    "\\]\n",
    "\n",
    "After you draw a ball, you decrement the corresponding remaining count.\n",
    "Let \\(X\\) be the total number of Type I objects drawn in the `N` draws.\n",
    "\n",
    "### PMF (integral representation)\n",
    "There is no simple closed-form PMF in elementary functions. A standard integral form is\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X=x)\n",
    "= \\binom{n}{x}\\,\\binom{M-n}{N-x}\n",
    "\\int_0^1 \\left(1-t^{\\omega/D}\\right)^x\\left(1-t^{1/D}\\right)^{N-x}\\,dt,\n",
    "\\qquad x\\in\\{x_\\ell,\\dots,x_u\\}\n",
    "\\]\n",
    "\n",
    "where\n",
    "\n",
    "\\[\n",
    "D = \\omega(n-x) + \\bigl((M-n) - (N-x)\\bigr)\n",
    "= \\omega(n-x) + (M-n-N+x).\n",
    "\\]\n",
    "\n",
    "### CDF\n",
    "The CDF is defined by summing the PMF over the integer support:\n",
    "\n",
    "\\[\n",
    "F(k) = \\mathbb{P}(X\\le k) = \\sum_{x=x_\\ell}^{\\lfloor k \\rfloor} \\mathbb{P}(X=x).\n",
    "\\]\n",
    "\n",
    "In practice (and in libraries), the PMF/CDF are computed using specialized numerical methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec8c9e",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Because the support is finite, **all moments exist**.\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "Given the PMF \\(p(x)=\\mathbb{P}(X=x)\\), the standard definitions are:\n",
    "\n",
    "\\[\n",
    "\\mu = \\mathbb{E}[X] = \\sum_x x\\,p(x),\n",
    "\\qquad\n",
    "\\sigma^2 = \\mathrm{Var}(X) = \\sum_x (x-\\mu)^2\\,p(x).\n",
    "\\]\n",
    "\n",
    "Define central moments \\(\\mu_r = \\mathbb{E}[(X-\\mu)^r]\\). Then\n",
    "\n",
    "\\[\n",
    "\\text{skewness }\\gamma_1 = \\frac{\\mu_3}{\\sigma^3},\n",
    "\\qquad\n",
    "\\text{excess kurtosis }\\gamma_2 = \\frac{\\mu_4}{\\sigma^4}-3.\n",
    "\\]\n",
    "\n",
    "For Wallenius' distribution, these quantities generally have **no simple closed form**, but they are easy to compute **numerically** from the PMF.\n",
    "\n",
    "### MGF / characteristic function\n",
    "With the PMF on integer support,\n",
    "\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = \\sum_x e^{tx}\\,p(x),\n",
    "\\qquad\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\sum_x e^{itx}\\,p(x).\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "The Shannon entropy (in nats) is\n",
    "\n",
    "\\[\n",
    "H(X) = -\\sum_x p(x)\\,\\log p(x).\n",
    "\\]\n",
    "\n",
    "### Symmetry by swapping types\n",
    "If you swap the meaning of Type I and Type II, the count transforms as\n",
    "\\(X \\mapsto N-X\\), and the odds invert:\n",
    "\n",
    "\\[\n",
    "X\\sim\\text{Wallenius}(M,n,N,\\omega)\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "N-X\\sim\\text{Wallenius}(M,M-n,N,1/\\omega).\n",
    "\\]\n",
    "\n",
    "(You can view this as “same biased sequential sampling, just relabeled”.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_M_n_N_odds(M, n, N, odds):\n",
    "    if isinstance(M, bool) or not isinstance(M, (int, np.integer)):\n",
    "        raise TypeError(\"M must be an integer\")\n",
    "    if isinstance(n, bool) or not isinstance(n, (int, np.integer)):\n",
    "        raise TypeError(\"n must be an integer\")\n",
    "    if isinstance(N, bool) or not isinstance(N, (int, np.integer)):\n",
    "        raise TypeError(\"N must be an integer\")\n",
    "\n",
    "    M = int(M)\n",
    "    n = int(n)\n",
    "    N = int(N)\n",
    "    odds = float(odds)\n",
    "\n",
    "    if M < 0:\n",
    "        raise ValueError(\"M must be >= 0\")\n",
    "    if not (0 <= n <= M):\n",
    "        raise ValueError(\"n must satisfy 0 <= n <= M\")\n",
    "    if not (0 <= N <= M):\n",
    "        raise ValueError(\"N must satisfy 0 <= N <= M\")\n",
    "    if not (odds > 0):\n",
    "        raise ValueError(\"odds must be > 0\")\n",
    "\n",
    "    return M, n, N, odds\n",
    "\n",
    "\n",
    "def wallenius_support(M, n, N, odds=None):\n",
    "    M, n, N, _ = _validate_M_n_N_odds(M, n, N, odds if odds is not None else 1.0)\n",
    "    x_min = max(0, N - (M - n))\n",
    "    x_max = min(N, n)\n",
    "    return x_min, x_max\n",
    "\n",
    "\n",
    "def sample_wallenius_numpy(M, n, N, odds, *, size=1, rng: np.random.Generator):\n",
    "    # NumPy-only sampler via sequential biased draws.\n",
    "    # Returns an integer array with shape `size` (like NumPy/SciPy rvs).\n",
    "    M, n, N, odds = _validate_M_n_N_odds(M, n, N, odds)\n",
    "\n",
    "    size = np.atleast_1d(size).astype(int)\n",
    "    if np.any(size < 0):\n",
    "        raise ValueError(\"size must be non-negative\")\n",
    "\n",
    "    if N == 0 or n == 0:\n",
    "        return np.zeros(size, dtype=int)\n",
    "    if n == M:\n",
    "        return np.full(size, N, dtype=int)\n",
    "\n",
    "    # Remaining counts per Monte Carlo replicate.\n",
    "    r = np.full(size, n, dtype=int)\n",
    "    b = np.full(size, M - n, dtype=int)\n",
    "\n",
    "    # Sequentially draw N times.\n",
    "    for _ in range(N):\n",
    "        # Handle edge cases explicitly to avoid division warnings.\n",
    "        p = np.where(\n",
    "            r == 0,\n",
    "            0.0,\n",
    "            np.where(b == 0, 1.0, (odds * r) / (odds * r + b)),\n",
    "        )\n",
    "        u = rng.random(size)\n",
    "        draw_r = u < p\n",
    "        r = r - draw_r.astype(int)\n",
    "        b = b - (~draw_r).astype(int)\n",
    "\n",
    "    return n - r\n",
    "\n",
    "\n",
    "def discrete_moments_from_pmf(xs, pmf):\n",
    "    # Compute mean/var/skew/excess-kurtosis/entropy from a discrete PMF.\n",
    "    xs = np.asarray(xs, dtype=float)\n",
    "    pmf = np.asarray(pmf, dtype=float)\n",
    "\n",
    "    total = pmf.sum()\n",
    "    if not np.isfinite(total) or total <= 0:\n",
    "        raise ValueError(\"PMF must sum to a positive finite value\")\n",
    "\n",
    "    pmf = pmf / total\n",
    "\n",
    "    mean = float(np.sum(xs * pmf))\n",
    "    centered = xs - mean\n",
    "    var = float(np.sum(centered**2 * pmf))\n",
    "\n",
    "    if var == 0.0:\n",
    "        skew = 0.0\n",
    "        exkurt = -3.0\n",
    "    else:\n",
    "        mu3 = float(np.sum(centered**3 * pmf))\n",
    "        mu4 = float(np.sum(centered**4 * pmf))\n",
    "        skew = mu3 / (var ** 1.5)\n",
    "        exkurt = mu4 / (var**2) - 3.0\n",
    "\n",
    "    p = pmf[pmf > 0]\n",
    "    entropy_nats = float(-np.sum(p * np.log(p)))\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"var\": var,\n",
    "        \"skew\": float(skew),\n",
    "        \"exkurt\": float(exkurt),\n",
    "        \"entropy_nats\": entropy_nats,\n",
    "    }\n",
    "\n",
    "\n",
    "def mgf_from_pmf(t, xs, pmf):\n",
    "    xs = np.asarray(xs, dtype=float)\n",
    "    pmf = np.asarray(pmf, dtype=float)\n",
    "    pmf = pmf / pmf.sum()\n",
    "    return float(np.sum(np.exp(t * xs) * pmf))\n",
    "\n",
    "\n",
    "def cf_from_pmf(t, xs, pmf):\n",
    "    xs = np.asarray(xs, dtype=float)\n",
    "    pmf = np.asarray(pmf, dtype=float)\n",
    "    pmf = pmf / pmf.sum()\n",
    "    return complex(np.sum(np.exp(1j * t * xs) * pmf))\n",
    "\n",
    "\n",
    "def wallenius_mean_field_mean(M, n, N, odds, *, tol=1e-12, max_iter=200):\n",
    "    # Approximate E[X] by a mean-field ODE argument.\n",
    "    #\n",
    "    # This solves for mu in:\n",
    "    #     (1 - mu/n)^(1/odds) = 1 - (N - mu)/(M - n)\n",
    "    # within the feasible interval [x_min, x_max].\n",
    "    M, n, N, odds = _validate_M_n_N_odds(M, n, N, odds)\n",
    "\n",
    "    x_min, x_max = wallenius_support(M, n, N, odds)\n",
    "\n",
    "    if N == 0 or n == 0:\n",
    "        return 0.0\n",
    "    if n == M:\n",
    "        return float(N)\n",
    "\n",
    "    m2 = M - n\n",
    "\n",
    "    def f(mu):\n",
    "        mu = float(mu)\n",
    "        left_base = max(0.0, 1.0 - mu / n)\n",
    "        right = 1.0 - (N - mu) / m2\n",
    "\n",
    "        if left_base == 0.0:\n",
    "            left = 0.0\n",
    "        else:\n",
    "            left = math.exp((1.0 / odds) * math.log(left_base))\n",
    "\n",
    "        return left - right\n",
    "\n",
    "    lo, hi = float(x_min), float(x_max)\n",
    "    if lo == hi:\n",
    "        return lo\n",
    "\n",
    "    f_lo, f_hi = f(lo), f(hi)\n",
    "\n",
    "    if f_lo == 0.0:\n",
    "        return lo\n",
    "    if f_hi == 0.0:\n",
    "        return hi\n",
    "\n",
    "    # In extreme cases, numerical bracket might not contain a sign change;\n",
    "    # fall back to an endpoint that matches the direction of bias.\n",
    "    if f_lo * f_hi > 0:\n",
    "        return hi if odds > 1 else lo\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        f_mid = f(mid)\n",
    "        if abs(f_mid) < tol or (hi - lo) < tol:\n",
    "            return mid\n",
    "        if f_lo * f_mid <= 0:\n",
    "            hi, f_hi = mid, f_mid\n",
    "        else:\n",
    "            lo, f_lo = mid, f_mid\n",
    "\n",
    "    return 0.5 * (lo + hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b11f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom, nchypergeom_wallenius\n",
    "\n",
    "M, n, N, odds = 140, 80, 60, 0.5\n",
    "x_min, x_max = wallenius_support(M, n, N, odds)\n",
    "xs = np.arange(x_min, x_max + 1)\n",
    "\n",
    "pmf = nchypergeom_wallenius.pmf(xs, M, n, N, odds)\n",
    "\n",
    "# Sanity check: odds=1 reduces to the ordinary hypergeometric distribution.\n",
    "pmf_unbiased = nchypergeom_wallenius.pmf(xs, M, n, N, 1.0)\n",
    "pmf_h = hypergeom.pmf(xs, M, n, N)\n",
    "max_abs_diff_odds1 = float(np.max(np.abs(pmf_unbiased - pmf_h)))\n",
    "\n",
    "mom_from_pmf = discrete_moments_from_pmf(xs, pmf)\n",
    "mean_scipy, var_scipy, skew_scipy, exkurt_scipy = nchypergeom_wallenius.stats(\n",
    "    M, n, N, odds, moments=\"mvsk\"\n",
    ")\n",
    "\n",
    "# Monte Carlo moments using the NumPy-only sampler\n",
    "mc = sample_wallenius_numpy(M, n, N, odds, size=80_000, rng=rng)\n",
    "\n",
    "{\n",
    "    \"support\": [int(x_min), int(x_max)],\n",
    "    \"pmf_sum\": float(pmf.sum()),\n",
    "    \"odds1_matches_hypergeom_max_abs_diff\": max_abs_diff_odds1,\n",
    "    \"mean_pmf\": mom_from_pmf[\"mean\"],\n",
    "    \"mean_scipy\": float(mean_scipy),\n",
    "    \"mean_mean_field\": wallenius_mean_field_mean(M, n, N, odds),\n",
    "    \"mean_mc\": float(mc.mean()),\n",
    "    \"var_pmf\": mom_from_pmf[\"var\"],\n",
    "    \"var_scipy\": float(var_scipy),\n",
    "    \"var_mc\": float(mc.var(ddof=0)),\n",
    "    \"skew_scipy\": float(skew_scipy),\n",
    "    \"exkurt_scipy\": float(exkurt_scipy),\n",
    "    \"entropy_scipy\": float(nchypergeom_wallenius.entropy(M, n, N, odds)),\n",
    "    \"mgf_t=0.1\": mgf_from_pmf(0.1, xs, pmf),\n",
    "    \"cf_t=1.0\": cf_from_pmf(1.0, xs, pmf),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8d82d",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### `M` and `n` (population composition)\n",
    "- `M` sets the overall population size.\n",
    "- `n/M` is the baseline fraction of Type I objects.\n",
    "\n",
    "When `N` is not tiny relative to `M`, the fact that we sample **without replacement** matters: after you draw some Type I objects, there are fewer Type I left, so Type I becomes harder to draw later.\n",
    "\n",
    "### `N` (number of draws)\n",
    "- Increasing `N` expands the support and typically increases both mean and variance.\n",
    "\n",
    "### `odds` (bias strength)\n",
    "- `odds = 1` means **unbiased** sampling (hypergeometric).\n",
    "- `odds > 1` favors Type I, shifting mass to larger `x`.\n",
    "- `odds < 1` discourages Type I, shifting mass to smaller `x`.\n",
    "\n",
    "In the extreme limits:\n",
    "- \\(\\omega\\to\\infty\\): you draw as many Type I objects as possible, so \\(X\\to\\min(n,N)\\).\n",
    "- \\(\\omega\\to 0\\): you avoid Type I until forced, so \\(X\\to \\max(0, N-(M-n))\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae16dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "M, n, N = 80, 30, 25\n",
    "odds_values = [0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        f\"PMF vs odds (M={M}, n={n}, N={N})\",\n",
    "        f\"PMF vs N (M={M}, n={n}, odds=2.0)\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "x_min, x_max = wallenius_support(M, n, N, odds_values[0])\n",
    "xs = np.arange(x_min, x_max + 1)\n",
    "\n",
    "for w in odds_values:\n",
    "    pmf = nchypergeom_wallenius.pmf(xs, M, n, N, w)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=xs, y=pmf, mode=\"lines+markers\", name=f\"odds={w}\"),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "odds_fixed = 2.0\n",
    "N_values = [5, 15, 25, 40]\n",
    "for N_ in N_values:\n",
    "    x_min, x_max = wallenius_support(M, n, N_, odds_fixed)\n",
    "    xs_ = np.arange(x_min, x_max + 1)\n",
    "    pmf_ = nchypergeom_wallenius.pmf(xs_, M, n, N_, odds_fixed)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=xs_, y=pmf_, mode=\"lines+markers\", name=f\"N={N_}\"),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=420,\n",
    "    width=980,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.25, xanchor=\"left\", x=0),\n",
    ")\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"P(X=x)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"P(X=x)\", row=1, col=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3bb39",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "Wallenius' distribution is defined by a **sequential, state-dependent** sampling process, so closed-form algebraic expressions are rare.\n",
    "\n",
    "We’ll do three things:\n",
    "\n",
    "1. derive a useful **mean-field approximation** for \\(\\mathbb{E}[X]\\)\n",
    "2. show how variance follows from the PMF (numerically)\n",
    "3. write down the likelihood for \\(\\omega\\) and fit it numerically\n",
    "\n",
    "### Expectation (mean-field / ODE derivation)\n",
    "Let \\(a(t)\\) and \\(b(t)\\) be the expected numbers of Type I and II remaining after \\(t\\) draws.\n",
    "Using the sequential sampling rule,\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[\\Delta a] \\approx -\\frac{\\omega a}{\\omega a + b},\n",
    "\\qquad\n",
    "\\mathbb{E}[\\Delta b] \\approx -\\frac{b}{\\omega a + b}.\n",
    "\\]\n",
    "\n",
    "Treating \\(t\\) as continuous gives the ODE approximation\n",
    "\n",
    "\\[\n",
    "\\frac{da}{dt} = -\\frac{\\omega a}{\\omega a + b},\\qquad\n",
    "\\frac{db}{dt} = -\\frac{b}{\\omega a + b}.\n",
    "\\]\n",
    "\n",
    "Take the ratio:\n",
    "\n",
    "\\[\n",
    "\\frac{db}{da} = \\frac{db/dt}{da/dt} = \\frac{b}{\\omega a}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\frac{db}{b} = \\frac{1}{\\omega}\\,\\frac{da}{a}.\n",
    "\\]\n",
    "\n",
    "Integrating:\n",
    "\n",
    "\\[\n",
    "\\log b = \\frac{1}{\\omega}\\log a + C\n",
    "\\quad\\Rightarrow\\quad\n",
    "b = C\\,a^{1/\\omega}.\n",
    "\\]\n",
    "\n",
    "Use the initial condition \\(a(0)=n\\), \\(b(0)=M-n\\) to get\n",
    "\n",
    "\\[\n",
    "\\frac{b}{M-n} = \\left(\\frac{a}{n}\\right)^{1/\\omega}.\n",
    "\\]\n",
    "\n",
    "After \\(N\\) draws, \\(a(N)=n-\\mu\\) and \\(b(N)=(M-n)-(N-\\mu)\\) where \\(\\mu=\\mathbb{E}[X]\\). Plugging in yields\n",
    "\n",
    "\\[\n",
    "1-\\frac{N-\\mu}{M-n} = \\left(1-\\frac{\\mu}{n}\\right)^{1/\\omega}.\n",
    "\\]\n",
    "\n",
    "This scalar equation has a unique solution in the feasible interval \\([x_\\ell, x_u]\\) and can be solved with **bisection**.\n",
    "\n",
    "### Variance\n",
    "Once you can compute the PMF numerically, variance is\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n",
    "= \\sum_x x^2 p(x) - \\left(\\sum_x x p(x)\\right)^2.\n",
    "\\]\n",
    "\n",
    "### Likelihood for \\(\\omega\\)\n",
    "If you observe a single count \\(x\\), the likelihood for \\(\\omega\\) (with `M,n,N` known) is\n",
    "\n",
    "\\[\n",
    "L(\\omega\\mid x) = p(x\\mid M,n,N,\\omega),\\qquad \\ell(\\omega)=\\log L(\\omega\\mid x).\n",
    "\\]\n",
    "\n",
    "For independent observations \\(x_1,\\dots,x_m\\) with the same parameters, the log-likelihood sums:\n",
    "\n",
    "\\[\n",
    "\\ell(\\omega) = \\sum_{i=1}^m \\log p(x_i\\mid M,n,N,\\omega).\n",
    "\\]\n",
    "\n",
    "Because the PMF is evaluated numerically, the MLE \\(\\hat\\omega\\) is typically found with 1D numerical optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eaef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "M, n, N, odds_true = 120, 50, 40, 2.0\n",
    "x_obs = int(nchypergeom_wallenius.rvs(M, n, N, odds_true, random_state=rng))\n",
    "\n",
    "odds_grid = np.logspace(-2, 2, 600)\n",
    "logL = nchypergeom_wallenius.logpmf(x_obs, M, n, N, odds_grid)\n",
    "\n",
    "# MLE for odds (bounded search on a wide log-scale interval)\n",
    "\n",
    "def nll(odds):\n",
    "    return -float(nchypergeom_wallenius.logpmf(x_obs, M, n, N, odds))\n",
    "\n",
    "res = minimize_scalar(nll, bounds=(1e-3, 1e3), method=\"bounded\")\n",
    "odds_hat = float(res.x)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=odds_grid, y=logL, mode=\"lines\", name=\"log-likelihood\"))\n",
    "fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"odds=1\")\n",
    "fig.add_vline(x=odds_hat, line_dash=\"dash\", line_color=\"black\", annotation_text=\"MLE\")\n",
    "fig.update_layout(\n",
    "    title=f\"Log-likelihood for odds (x_obs={x_obs}, M={M}, n={n}, N={N})\",\n",
    "    xaxis_title=\"odds\",\n",
    "    yaxis_title=\"log L(odds)\",\n",
    "    xaxis_type=\"log\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"x_obs\": x_obs,\n",
    "    \"odds_true\": odds_true,\n",
    "    \"odds_mle\": odds_hat,\n",
    "    \"opt_success\": bool(res.success),\n",
    "    \"mean_field_mean_at_true\": wallenius_mean_field_mean(M, n, N, odds_true),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e521c",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "A direct **NumPy-only** sampling algorithm follows the generative story:\n",
    "\n",
    "1. Start with `r = n` Type I and `b = M-n` Type II remaining.\n",
    "2. Repeat `N` times:\n",
    "   - compute \\(p = \\omega r/(\\omega r + b)\\)\n",
    "   - draw \\(U\\sim\\text{Uniform}(0,1)\\)\n",
    "   - if \\(U<p\\), record a Type I draw and decrement `r`; else decrement `b`\n",
    "3. Return the total Type I draws.\n",
    "\n",
    "This is exact but costs \\(O(N\\cdot\\text{size})\\). For many settings (moderate `N`), it’s fast and simple.\n",
    "\n",
    "SciPy’s `rvs` uses a specialized, optimized implementation (BiasedUrn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e942576",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, n, N, odds = 200, 60, 40, 1.7\n",
    "size = 50_000\n",
    "\n",
    "x_np = sample_wallenius_numpy(M, n, N, odds, size=size, rng=rng)\n",
    "x_sp = nchypergeom_wallenius.rvs(M, n, N, odds, size=size, random_state=rng)\n",
    "\n",
    "{\n",
    "    \"np_mean\": float(x_np.mean()),\n",
    "    \"scipy_mean\": float(x_sp.mean()),\n",
    "    \"scipy_theory_mean\": float(nchypergeom_wallenius.mean(M, n, N, odds)),\n",
    "    \"np_var\": float(x_np.var(ddof=0)),\n",
    "    \"scipy_var\": float(x_sp.var(ddof=0)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d1d14",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "\n",
    "- the **PMF** on the integer support\n",
    "- the **CDF** (as a step function)\n",
    "- Monte Carlo samples vs the exact PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, n, N, odds = 140, 80, 60, 0.5\n",
    "x_min, x_max = wallenius_support(M, n, N, odds)\n",
    "xs = np.arange(x_min, x_max + 1)\n",
    "\n",
    "pmf = nchypergeom_wallenius.pmf(xs, M, n, N, odds)\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=xs, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(\n",
    "    title=f\"Wallenius PMF (M={M}, n={n}, N={N}, odds={odds})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X=x)\",\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=xs, y=cdf, mode=\"lines\", line_shape=\"hv\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(\n",
    "    title=f\"Wallenius CDF (M={M}, n={n}, N={N}, odds={odds})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X≤x)\",\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo comparison\n",
    "mc = sample_wallenius_numpy(M, n, N, odds, size=120_000, rng=rng)\n",
    "counts = np.bincount(mc - x_min, minlength=(x_max - x_min + 1))\n",
    "pmf_hat = counts / counts.sum()\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=xs, y=pmf_hat, name=\"Monte Carlo\", opacity=0.6))\n",
    "fig_mc.add_trace(go.Scatter(x=xs, y=pmf, mode=\"lines+markers\", name=\"Exact (SciPy)\"))\n",
    "fig_mc.update_layout(\n",
    "    title=\"Monte Carlo vs exact PMF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c0efd",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.nchypergeom_wallenius`:\n",
    "\n",
    "- `pmf`, `logpmf`\n",
    "- `cdf`, `sf` (survival function), `ppf`\n",
    "- `rvs`\n",
    "- `stats`, `mean`, `var`, `entropy`\n",
    "\n",
    "### About `.fit`\n",
    "Many discrete SciPy distributions (including `nchypergeom_wallenius`) do **not** implement a generic `.fit()` method.\n",
    "\n",
    "In practice, you usually know `M,n,N` from the study design and only want to estimate the bias parameter `odds`.\n",
    "We’ll show a simple **custom MLE** for `odds` using `logpmf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, n, N, odds = 140, 80, 60, 0.5\n",
    "rv = nchypergeom_wallenius(M, n, N, odds)\n",
    "\n",
    "x_min, x_max = wallenius_support(M, n, N, odds)\n",
    "xs = np.arange(x_min, x_max + 1)\n",
    "\n",
    "pmf = rv.pmf(xs)\n",
    "cdf = rv.cdf(xs)\n",
    "samples = rv.rvs(size=20_000, random_state=rng)\n",
    "\n",
    "{\n",
    "    \"pmf_sum\": float(pmf.sum()),\n",
    "    \"cdf_last\": float(cdf[-1]),\n",
    "    \"sample_mean\": float(samples.mean()),\n",
    "    \"theory_mean\": float(rv.mean()),\n",
    "    \"theory_var\": float(rv.var()),\n",
    "    \"entropy\": float(rv.entropy()),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7169a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Custom 1D MLE for odds given data and known (M,n,N)\n",
    "M, n, N, odds_true = 120, 50, 40, 1.8\n",
    "x = nchypergeom_wallenius.rvs(M, n, N, odds_true, size=2_000, random_state=rng)\n",
    "\n",
    "# Negative log-likelihood for odds (vectorized over data)\n",
    "\n",
    "def nll_odds(odds):\n",
    "    if not (odds > 0):\n",
    "        return float(\"inf\")\n",
    "    return -float(nchypergeom_wallenius.logpmf(x, M, n, N, odds).sum())\n",
    "\n",
    "res = minimize_scalar(nll_odds, bounds=(1e-3, 1e3), method=\"bounded\")\n",
    "\n",
    "{\n",
    "    \"odds_true\": odds_true,\n",
    "    \"odds_mle\": float(res.x),\n",
    "    \"opt_success\": bool(res.success),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f395ef6",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (bias vs no bias)\n",
    "A common question: *is selection unbiased?* In this setting, “unbiased” corresponds to \\(\\omega=1\\) and the distribution reduces to the ordinary hypergeometric.\n",
    "\n",
    "Given an observed \\(x_\\text{obs}\\), one-sided p-values are computed with the survival function:\n",
    "\n",
    "\\[\n",
    "\\text{p-value} = \\mathbb{P}(X\\ge x_\\text{obs}\\mid \\omega=1) = \\mathrm{sf}(x_\\text{obs}-1).\n",
    "\\]\n",
    "\n",
    "You can also compare \\(\\omega=1\\) vs \\(\\omega\\ne 1\\) using a likelihood ratio test (asymptotic \\(\\chi^2_1\\) calibration).\n",
    "\n",
    "### B) Bayesian modeling (posterior over odds)\n",
    "Put a prior on \\(\\log\\omega\\) (e.g., Normal) and compute the posterior on a grid:\n",
    "\n",
    "\\[\n",
    "\\log p(\\omega\\mid x) = \\log p(x\\mid \\omega) + \\log p(\\omega) + \\text{const}.\n",
    "\\]\n",
    "\n",
    "Because the parameter is 1D, grid Bayes is often perfectly adequate.\n",
    "\n",
    "### C) Generative modeling\n",
    "In synthetic-data pipelines, Wallenius' distribution is a handy component when you need to simulate **quota-based biased selection without replacement**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46169a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Example: test enrichment under the unbiased model (odds=1)\n",
    "M, n, N = 200, 60, 40\n",
    "x_obs = 18\n",
    "\n",
    "p_ge = nchypergeom_wallenius.sf(x_obs - 1, M, n, N, 1.0)\n",
    "\n",
    "# Likelihood ratio test: H0 odds=1 vs H1 odds free\n",
    "\n",
    "def nll1(odds):\n",
    "    return -float(nchypergeom_wallenius.logpmf(x_obs, M, n, N, odds))\n",
    "\n",
    "res = minimize_scalar(nll1, bounds=(1e-3, 1e3), method=\"bounded\")\n",
    "ll_hat = -float(res.fun)\n",
    "ll_null = float(nchypergeom_wallenius.logpmf(x_obs, M, n, N, 1.0))\n",
    "LR = 2 * (ll_hat - ll_null)\n",
    "p_lr = float(chi2.sf(LR, df=1))\n",
    "\n",
    "{\n",
    "    \"x_obs\": x_obs,\n",
    "    \"p_value_one_sided_ge\": float(p_ge),\n",
    "    \"odds_mle\": float(res.x),\n",
    "    \"LR_stat\": float(LR),\n",
    "    \"p_value_LR_chi2_approx\": p_lr,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a07add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid Bayes for odds (prior on log-odds)\n",
    "\n",
    "M, n, N = 200, 60, 40\n",
    "x_obs = 18\n",
    "\n",
    "log_odds_grid = np.linspace(-3.0, 3.0, 600)\n",
    "odds_grid = np.exp(log_odds_grid)\n",
    "\n",
    "# Prior: log(odds) ~ Normal(0, 1.0) (up to an additive constant)\n",
    "log_prior = -0.5 * (log_odds_grid / 1.0) ** 2\n",
    "\n",
    "log_like = nchypergeom_wallenius.logpmf(x_obs, M, n, N, odds_grid)\n",
    "log_post_unnorm = log_like + log_prior\n",
    "log_post_unnorm -= np.max(log_post_unnorm)\n",
    "\n",
    "post_log = np.exp(log_post_unnorm)\n",
    "post_log /= np.trapz(post_log, log_odds_grid)  # density w.r.t log-odds\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=odds_grid, y=post_log, mode=\"lines\", name=\"posterior\"))\n",
    "fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_layout(\n",
    "    title=f\"Posterior over odds (x_obs={x_obs}, M={M}, n={n}, N={N})\",\n",
    "    xaxis_title=\"odds\",\n",
    "    yaxis_title=\"density (w.r.t log-odds)\",\n",
    "    xaxis_type=\"log\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Posterior mean and a central 90% credible interval (in log-odds space)\n",
    "dlog = log_odds_grid[1] - log_odds_grid[0]\n",
    "cdf = np.cumsum(post_log) * dlog\n",
    "cdf = cdf / cdf[-1]\n",
    "\n",
    "lo_log = float(np.interp(0.05, cdf, log_odds_grid))\n",
    "hi_log = float(np.interp(0.95, cdf, log_odds_grid))\n",
    "\n",
    "{\n",
    "    \"posterior_mean_odds\": float(np.trapz(np.exp(log_odds_grid) * post_log, log_odds_grid)),\n",
    "    \"credible_interval_90_odds\": [float(np.exp(lo_log)), float(np.exp(hi_log))],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling: simulate biased selection outcomes for different odds\n",
    "\n",
    "M, n, N = 150, 50, 35\n",
    "odds_list = [0.5, 1.0, 2.0]\n",
    "size = 30_000\n",
    "\n",
    "rows = []\n",
    "for w in odds_list:\n",
    "    x = sample_wallenius_numpy(M, n, N, w, size=size, rng=rng)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"odds\": w,\n",
    "            \"mean\": float(x.mean()),\n",
    "            \"var\": float(x.var(ddof=0)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357131c",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter constraints**: `M,n,N` must be integers with `0 ≤ n ≤ M` and `0 ≤ N ≤ M`; `odds > 0`.\n",
    "- **Notation clashes**: different sources swap the meaning of `N`, `n`, `M`. Always check the API.\n",
    "- **Degenerate limits**:\n",
    "  - very large `odds` concentrates mass near \\(\\min(n,N)\\)\n",
    "  - very small `odds` concentrates mass near \\(\\max(0, N-(M-n))\\)\n",
    "- **Numerical stability**: PMF values can be tiny for large populations; use `logpmf` when optimizing likelihoods.\n",
    "- **Performance**:\n",
    "  - sequential NumPy sampling is \\(O(N\\cdot\\text{size})\\)\n",
    "  - PMF evaluation is nontrivial; SciPy relies on the BiasedUrn library for efficiency/accuracy.\n",
    "- **Identifiability**: from counts alone you typically cannot estimate `M,n,N,odds` simultaneously; in real studies `M,n,N` are usually known.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681da802",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `nchypergeom_wallenius` is a **discrete** model for **biased sequential sampling without replacement** from two types.\n",
    "- Support is integer and bounded: \\(x\\in\\{\\max(0,N-(M-n)),\\dots,\\min(N,n)\\}\\).\n",
    "- \\(\\omega=1\\) recovers the ordinary hypergeometric; \\(\\omega>1\\) favors Type I.\n",
    "- PMF/CDF and moments are usually handled numerically (SciPy uses BiasedUrn).\n",
    "- Exact sampling is simple to implement via sequential biased draws (NumPy-only), and Monte Carlo checks are straightforward.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}