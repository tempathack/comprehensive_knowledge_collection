{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37309822",
   "metadata": {},
   "source": [
    "# Beta-Binomial (`betabinom`) Distribution\n",
    "\n",
    "The **beta-binomial** distribution models a *count of successes* in a fixed number of trials when the success probability is **unknown and varies** across repeated experiments.\n",
    "\n",
    "A clean way to say it:\n",
    "\n",
    "> Draw a random probability $p \\sim \\mathrm{Beta}(\\alpha, \\beta)$, then draw successes $X\\mid p \\sim \\mathrm{Binomial}(n, p)$.  \n",
    "> The marginal distribution of $X$ is **beta-binomial**.\n",
    "\n",
    "It is a standard choice when **binomial data are overdispersed** (more variable than a binomial model allows).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Understand the *hierarchical story* behind `betabinom` and when to use it\n",
    "- Write the PMF/CDF, compute moments, and interpret parameters $(n,\\alpha,\\beta)$\n",
    "- Derive mean/variance from first principles\n",
    "- Sample and visualize the distribution (NumPy-only sampler)\n",
    "- Use `scipy.stats.betabinom` for PMF/CDF/RVS and implement a practical MLE \"fit\" routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09247bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy import optimize, special, stats\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b568a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import scipy\n",
    "\n",
    "print(\"numpy :\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"scipy :\", scipy.__version__)\n",
    "print(\"plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f826b8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Binomial distribution basics: counts of successes in $n$ trials\n",
    "- Beta distribution basics: a distribution on probabilities $p \\in (0,1)$\n",
    "- Law of total expectation/variance\n",
    "- Comfort reading math in LaTeX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea531dc3",
   "metadata": {},
   "source": [
    "## 1) Title & classification\n",
    "\n",
    "- **Name**: `betabinom` (beta-binomial)\n",
    "- **Type**: **Discrete** distribution\n",
    "- **Support**: \n",
    "  $$\n",
    "  \\mathcal{X} = \\{0, 1, 2, \\dots, n\\}\n",
    "  $$\n",
    "- **Parameter space**:\n",
    "  $$\n",
    "  n \\in \\{0,1,2,\\dots\\},\\qquad \\alpha > 0,\\qquad \\beta > 0\n",
    "  $$\n",
    "\n",
    "SciPy uses the names `(n, a, b)` for `(n, \\alpha, \\beta)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2eeec",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "A binomial model assumes a *single fixed* success probability $p$.\n",
    "\n",
    "In many real settings, $p$ is not constant:\n",
    "\n",
    "- users vary in propensity to click\n",
    "- batches vary in defect rate\n",
    "- clinics vary in response rate\n",
    "\n",
    "A simple way to model this heterogeneity is to treat $p$ as random:\n",
    "\n",
    "$$\n",
    "p \\sim \\mathrm{Beta}(\\alpha, \\beta),\n",
    "\\qquad\n",
    "X \\mid p \\sim \\mathrm{Binomial}(n, p)\n",
    "$$\n",
    "\n",
    "The marginal distribution of $X$ is **beta-binomial**.\n",
    "\n",
    "### Real-world use cases\n",
    "\n",
    "- **Overdispersed binomial counts**: when sample variance exceeds $n\\hat p(1-\\hat p)$\n",
    "- **Empirical Bayes** for rates: estimate a prior over $p$ across many groups\n",
    "- **Posterior predictive** for binomial + beta prior (conjugate Bayesian modeling)\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Mixture**: beta-binomial is a *Beta–Binomial compound distribution* (a mixture of binomials)\n",
    "- **Conjugacy**: Beta is conjugate prior for Binomial; beta-binomial is the predictive distribution\n",
    "- **Pólya’s urn**: beta-binomial counts appear as draws in an urn scheme with reinforcement\n",
    "- **Dirichlet-multinomial**: multivariate generalization (counts across multiple categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09aaf0",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "### Hierarchical definition\n",
    "\n",
    "$$\n",
    "p \\sim \\mathrm{Beta}(\\alpha, \\beta),\n",
    "\\qquad\n",
    "X \\mid p \\sim \\mathrm{Binomial}(n, p)\n",
    "$$\n",
    "\n",
    "### PMF\n",
    "\n",
    "For $k \\in \\{0,1,\\dots,n\\}$:\n",
    "\n",
    "$$\n",
    "\\Pr[X = k]\n",
    "= \\binom{n}{k}\\,\\frac{B(k+\\alpha,\\; n-k+\\beta)}{B(\\alpha,\\beta)}\n",
    "$$\n",
    "\n",
    "where $B(\\cdot,\\cdot)$ is the beta function:\n",
    "\n",
    "$$\n",
    "B(x,y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}.\n",
    "$$\n",
    "\n",
    "A numerically stable computation uses the log form:\n",
    "\n",
    "$$\n",
    "\\log \\Pr[X=k]\n",
    "= \\log\\binom{n}{k}\n",
    "+ \\log B(k+\\alpha, n-k+\\beta)\n",
    "- \\log B(\\alpha,\\beta).\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "The CDF is the cumulative sum of the PMF:\n",
    "\n",
    "$$\n",
    "F(k) = \\Pr[X \\le k] = \\sum_{i=0}^{\\lfloor k \\rfloor} \\Pr[X=i].\n",
    "$$\n",
    "\n",
    "There is no simple elementary closed form; libraries compute it efficiently and stably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d614a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betabinom_logpmf(k: np.ndarray, n: int, a: float, b: float) -> np.ndarray:\n",
    "    \"\"\"Log PMF using stable special functions (vectorized).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : array-like\n",
    "        Counts in {0, ..., n}.\n",
    "    n : int\n",
    "        Number of trials.\n",
    "    a, b : float\n",
    "        Beta shape parameters (alpha, beta) > 0.\n",
    "    \"\"\"\n",
    "    k = np.asarray(k)\n",
    "    log_choose = (\n",
    "        special.gammaln(n + 1)\n",
    "        - special.gammaln(k + 1)\n",
    "        - special.gammaln(n - k + 1)\n",
    "    )\n",
    "    return log_choose + special.betaln(k + a, n - k + b) - special.betaln(a, b)\n",
    "\n",
    "\n",
    "def betabinom_pmf(k: np.ndarray, n: int, a: float, b: float) -> np.ndarray:\n",
    "    return np.exp(betabinom_logpmf(k, n=n, a=a, b=b))\n",
    "\n",
    "\n",
    "def betabinom_cdf(k: np.ndarray, n: int, a: float, b: float) -> np.ndarray:\n",
    "    \"\"\"CDF computed by cumulative sum of PMF on support.\"\"\"\n",
    "    ks = np.arange(0, n + 1)\n",
    "    pmf = betabinom_pmf(ks, n=n, a=a, b=b)\n",
    "    cdf_full = np.cumsum(pmf)\n",
    "    k = np.asarray(k)\n",
    "    k_clip = np.clip(np.floor(k).astype(int), 0, n)\n",
    "    return cdf_full[k_clip]\n",
    "\n",
    "\n",
    "def betabinom_entropy(n: int, a: float, b: float) -> float:\n",
    "    \"\"\"Shannon entropy in nats: H = -sum p log p.\"\"\"\n",
    "    ks = np.arange(0, n + 1)\n",
    "    logp = betabinom_logpmf(ks, n=n, a=a, b=b)\n",
    "    p = np.exp(logp)\n",
    "    return float(-np.sum(p * logp))\n",
    "\n",
    "\n",
    "def betabinom_stats_closed_form(n: int, a: float, b: float):\n",
    "    \"\"\"Mean, variance, skewness, excess kurtosis (Fisher).\"\"\"\n",
    "    s = a + b\n",
    "    p = a / s\n",
    "    q = b / s\n",
    "    mean = n * p\n",
    "    var = n * p * q * (n + s) / (s + 1)\n",
    "\n",
    "    # For n=0 the distribution is degenerate at 0; higher standardized moments are undefined.\n",
    "    if n == 0 or var == 0:\n",
    "        return float(mean), float(var), np.nan, np.nan\n",
    "\n",
    "    skew = (s + 2 * n) * (b - a) * np.sqrt(s + 1) / ((s + 2) * np.sqrt(n * a * b * (s + n)))\n",
    "\n",
    "    t = a * b\n",
    "    kurt_excess = (\n",
    "        (s + 1)\n",
    "        * (\n",
    "            s**4\n",
    "            + (6 * n - 1) * s**3\n",
    "            + (6 * n**2 + 3 * t * (n - 2)) * s**2\n",
    "            - 3 * t * n * (6 - n) * s\n",
    "            - 18 * t * n**2\n",
    "        )\n",
    "        / (n * t * (s + n) * (s + 2) * (s + 3))\n",
    "        - 3\n",
    "    )\n",
    "    return float(mean), float(var), float(skew), float(kurt_excess)\n",
    "\n",
    "\n",
    "def rvs_betabinom_numpy(n: int, a: float, b: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"NumPy-only sampler via Gamma->Beta + Binomial.\n",
    "\n",
    "    - If G1 ~ Gamma(a, 1) and G2 ~ Gamma(b, 1), then P = G1 / (G1 + G2) ~ Beta(a, b).\n",
    "    - Then sample X ~ Binomial(n, P).\n",
    "    \"\"\"\n",
    "    g1 = rng.gamma(shape=a, scale=1.0, size=size)\n",
    "    g2 = rng.gamma(shape=b, scale=1.0, size=size)\n",
    "    p = g1 / (g1 + g2)\n",
    "    return rng.binomial(n, p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450d332",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "Let $X \\sim \\mathrm{BetaBinomial}(n,\\alpha,\\beta)$ and define $s = \\alpha+\\beta$.\n",
    "\n",
    "### Mean\n",
    "$$\n",
    "\\mathbb{E}[X] = n\\,\\frac{\\alpha}{\\alpha+\\beta}.\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "$$\n",
    "\\mathrm{Var}(X) = n\\,\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}\\,\\frac{\\alpha+\\beta+n}{\\alpha+\\beta+1}.\n",
    "$$\n",
    "\n",
    "This is often written as:\n",
    "$$\n",
    "\\mathrm{Var}(X) = n\\,p(1-p)\\,\\frac{n+s}{s+1},\n",
    "\\qquad p=\\frac{\\alpha}{s}.\n",
    "$$\n",
    "\n",
    "Compared to binomial variance $n p(1-p)$, the factor $(n+s)/(s+1) \\ge 1$ produces **overdispersion**.\n",
    "\n",
    "### Skewness\n",
    "$$\n",
    "\\gamma_1\n",
    "= \\frac{(s+2n)(\\beta-\\alpha)\\sqrt{s+1}}{(s+2)\\sqrt{n\\alpha\\beta(s+n)}}.\n",
    "$$\n",
    "\n",
    "### Excess kurtosis (Fisher)\n",
    "\n",
    "A symmetric closed form is (let $t=\\alpha\\beta$):\n",
    "\n",
    "$$\n",
    "\\gamma_2\n",
    "= \\frac{(s+1)}{n\\,t\\,(s+n)(s+2)(s+3)}\\Big[\n",
    "s^4 + (6n-1)s^3 + (6n^2 + 3t(n-2))s^2 - 3tn(6-n)s - 18tn^2\n",
    "\\Big] - 3.\n",
    "$$\n",
    "\n",
    "### PGF / MGF / characteristic function\n",
    "\n",
    "The **probability generating function** (PGF) can be written using the Gauss hypergeometric function:\n",
    "\n",
    "$$\n",
    "G(s) = \\mathbb{E}[s^X] = {}_2F_1\\left(-n,\\;\\alpha;\\;\\alpha+\\beta;\\;1-s\\right).\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "- MGF: $M(t)=\\mathbb{E}[e^{tX}] = G(e^t)$\n",
    "- CF: $\\varphi(t)=\\mathbb{E}[e^{itX}] = G(e^{it})$\n",
    "\n",
    "Because $X \\in \\{0,\\dots,n\\}$ is bounded, all moments exist.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "There is no simple closed form in elementary functions; it is typically computed numerically:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{k=0}^n \\Pr[X=k]\\,\\log \\Pr[X=k].\n",
    "$$\n",
    "\n",
    "### Intraclass correlation (trial-to-trial dependence)\n",
    "\n",
    "In the hierarchical view, the Bernoulli trials are conditionally independent given $p$, but *marginally* correlated.\n",
    "\n",
    "The correlation between two distinct trials in the same group is:\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{1}{\\alpha+\\beta+1} = \\frac{1}{s+1}.\n",
    "$$\n",
    "\n",
    "Binomial data correspond to the limiting case $\\rho \\to 0$ (equivalently $s \\to \\infty$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f58ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick numerical check: closed-form moments vs SciPy\n",
    "n, a, b = 10, 2.0, 3.0\n",
    "mean_cf, var_cf, skew_cf, kurt_cf = betabinom_stats_closed_form(n, a, b)\n",
    "mean_sp, var_sp, skew_sp, kurt_sp = stats.betabinom.stats(n, a, b, moments=\"mvsk\")\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"closed_form\": [mean_cf, var_cf, skew_cf, kurt_cf],\n",
    "        \"scipy\": [float(mean_sp), float(var_sp), float(skew_sp), float(kurt_sp)],\n",
    "    },\n",
    "    index=[\"mean\", \"variance\", \"skewness\", \"excess_kurtosis\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dafcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy (numerical) + PGF/MGF evaluation via hypergeometric function\n",
    "H = betabinom_entropy(n=10, a=2.0, b=3.0)\n",
    "\n",
    "def betabinom_pgf(s: complex, n: int, a: float, b: float) -> complex:\n",
    "    return complex(special.hyp2f1(-n, a, a + b, 1 - s))\n",
    "\n",
    "M0 = betabinom_pgf(1.0, n=10, a=2.0, b=3.0)  # should be 1\n",
    "M_small = betabinom_pgf(np.exp(0.2), n=10, a=2.0, b=3.0)\n",
    "\n",
    "H, M0, M_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b4c3e",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "### $n$ (trials)\n",
    "\n",
    "- Controls the **maximum** number of successes.\n",
    "- Scaling $n$ changes the range and (often) the granularity of the PMF.\n",
    "\n",
    "### $(\\alpha,\\beta)$ (Beta prior over $p$)\n",
    "\n",
    "Interpretations:\n",
    "\n",
    "- Prior mean of the success probability:\n",
    "  $$\n",
    "  \\mathbb{E}[p] = \\frac{\\alpha}{\\alpha+\\beta}\n",
    "  $$\n",
    "- Prior \"concentration\" (how strongly $p$ is concentrated around its mean):\n",
    "  $$\n",
    "  s = \\alpha+\\beta\n",
    "  $$\n",
    "  Larger $s$ means **less heterogeneity** in $p$ and a distribution closer to Binomial$(n,\\mathbb{E}[p])$.\n",
    "\n",
    "Heuristic pseudo-count view: $\\alpha-1$ prior successes and $\\beta-1$ prior failures.\n",
    "\n",
    "### Shape changes\n",
    "\n",
    "- Fix $p=\\alpha/(\\alpha+\\beta)$ and increase $s$: PMF tightens around $np$ (approaches binomial).\n",
    "- Fix $s$ and vary $p$: the mass shifts left/right.\n",
    "- If $\\alpha,\\beta<1$ the Beta prior is U-shaped, so $p$ often lands near 0 or 1 → PMF puts more mass near 0 and $n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of concentration s = a+b at fixed mean p\n",
    "n = 20\n",
    "p = 0.30\n",
    "concentrations = [2, 5, 20, 200]\n",
    "\n",
    "ks = np.arange(n + 1)\n",
    "frames = []\n",
    "for s in concentrations:\n",
    "    a = p * s\n",
    "    b = (1 - p) * s\n",
    "    pmf = stats.betabinom.pmf(ks, n, a, b)\n",
    "    frames.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"k\": ks,\n",
    "                \"pmf\": pmf,\n",
    "                \"concentration (a+b)\": f\"{s:g}\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"k\",\n",
    "    y=\"pmf\",\n",
    "    color=\"concentration (a+b)\",\n",
    "    markers=True,\n",
    "    title=\"Beta-binomial PMF: increasing concentration approaches a Binomial\",\n",
    "    labels={\"k\": \"successes k\", \"pmf\": \"P(X=k)\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of changing mean p at fixed concentration s\n",
    "n = 20\n",
    "s = 10\n",
    "ps = [0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "ks = np.arange(n + 1)\n",
    "frames = []\n",
    "for p in ps:\n",
    "    a = p * s\n",
    "    b = (1 - p) * s\n",
    "    pmf = stats.betabinom.pmf(ks, n, a, b)\n",
    "    frames.append(pd.DataFrame({\"k\": ks, \"pmf\": pmf, \"p\": f\"{p:.1f}\"}))\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"k\",\n",
    "    y=\"pmf\",\n",
    "    color=\"p\",\n",
    "    markers=True,\n",
    "    title=\"Beta-binomial PMF: varying mean p shifts the mass\",\n",
    "    labels={\"k\": \"successes k\", \"pmf\": \"P(X=k)\", \"p\": \"mean p\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f450b0",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation\n",
    "\n",
    "Use the law of total expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X]\n",
    "= \\mathbb{E}\\big[\\,\\mathbb{E}[X\\mid p]\\,\\big]\n",
    "= \\mathbb{E}[n p]\n",
    "= n\\,\\mathbb{E}[p]\n",
    "= n\\,\\frac{\\alpha}{\\alpha+\\beta}.\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "\n",
    "Use the law of total variance:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X)\n",
    "= \\mathbb{E}[\\mathrm{Var}(X\\mid p)] + \\mathrm{Var}(\\mathbb{E}[X\\mid p]).\n",
    "$$\n",
    "\n",
    "For a binomial conditional on $p$:\n",
    "\n",
    "- $\\mathbb{E}[X\\mid p]=np$\n",
    "- $\\mathrm{Var}(X\\mid p)=np(1-p)$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathrm{Var}(X\\mid p)] = n\\,\\mathbb{E}[p(1-p)]\n",
    "\\qquad\n",
    "\\mathrm{Var}(\\mathbb{E}[X\\mid p]) = n^2\\,\\mathrm{Var}(p).\n",
    "$$\n",
    "\n",
    "Using Beta moments:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[p] = \\frac{\\alpha}{s},\n",
    "\\qquad\n",
    "\\mathrm{Var}(p) = \\frac{\\alpha\\beta}{s^2(s+1)}.\n",
    "$$\n",
    "\n",
    "After algebra, this yields:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X) = n\\,\\frac{\\alpha\\beta}{s^2}\\,\\frac{n+s}{s+1}.\n",
    "$$\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "For a single observation $k$ (with fixed $n$), the likelihood is just the PMF viewed as a function of $(\\alpha,\\beta)$:\n",
    "\n",
    "$$\n",
    "L(\\alpha,\\beta\\mid k)\n",
    "= \\binom{n}{k}\\,\\frac{B(k+\\alpha, n-k+\\beta)}{B(\\alpha,\\beta)}.\n",
    "$$\n",
    "\n",
    "For i.i.d. observations $k_1,\\dots,k_m$ (same $n$), the log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ell(\\alpha,\\beta)\n",
    "= \\sum_{i=1}^m\\Big[\n",
    "\\log\\binom{n}{k_i}\n",
    "+ \\log B(k_i+\\alpha, n-k_i+\\beta)\n",
    "- \\log B(\\alpha,\\beta)\n",
    "\\Big].\n",
    "$$\n",
    "\n",
    "In practice we compute this in log-space (e.g., using `scipy.special.betaln`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check of mean/variance\n",
    "n, a, b = 30, 2.0, 5.0\n",
    "x = rvs_betabinom_numpy(n=n, a=a, b=b, size=200_000, rng=rng)\n",
    "\n",
    "mean_mc = float(x.mean())\n",
    "var_mc = float(x.var(ddof=0))\n",
    "\n",
    "mean_cf, var_cf, *_ = betabinom_stats_closed_form(n, a, b)\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"monte_carlo\": [mean_mc, var_mc],\n",
    "        \"closed_form\": [mean_cf, var_cf],\n",
    "    },\n",
    "    index=[\"mean\", \"variance\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8345f",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation\n",
    "\n",
    "The beta-binomial sampler is a direct implementation of the hierarchical model:\n",
    "\n",
    "1. Sample a probability $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$\n",
    "2. Sample a count $X \\sim \\mathrm{Binomial}(n,p)$\n",
    "\n",
    "To keep this **NumPy-only**, we sample $p$ via the Gamma ratio construction:\n",
    "\n",
    "$$\n",
    "G_1 \\sim \\mathrm{Gamma}(\\alpha,1),\\quad G_2 \\sim \\mathrm{Gamma}(\\beta,1),\\quad p = \\frac{G_1}{G_1+G_2} \\sim \\mathrm{Beta}(\\alpha,\\beta).\n",
    "$$\n",
    "\n",
    "Then we sample $X\\sim\\mathrm{Binomial}(n,p)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05880a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, a, b = 20, 1.5, 3.0\n",
    "samples = rvs_betabinom_numpy(n=n, a=a, b=b, size=20_000, rng=rng)\n",
    "samples[:10], samples.mean(), samples.var(ddof=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31b404",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll plot:\n",
    "\n",
    "- the **PMF** on $\\{0,\\dots,n\\}$\n",
    "- the **CDF** as a cumulative sum\n",
    "- a **Monte Carlo** approximation (histogram of samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a521ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, a, b = 20, 1.5, 3.0\n",
    "ks = np.arange(n + 1)\n",
    "\n",
    "pmf = stats.betabinom.pmf(ks, n, a, b)\n",
    "cdf = stats.betabinom.cdf(ks, n, a, b)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"PMF\", \"CDF\"])\n",
    "\n",
    "fig.add_trace(go.Bar(x=ks, y=pmf, name=\"PMF\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=ks, y=cdf, mode=\"lines+markers\", name=\"CDF\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"k (successes)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"k (successes)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"P(X=k)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"P(X\\u2264k)\", row=1, col=2)\n",
    "fig.update_layout(title=f\"Beta-binomial(n={n}, a={a}, b={b})\", showlegend=False)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo histogram vs PMF\n",
    "samples = rvs_betabinom_numpy(n=n, a=a, b=b, size=50_000, rng=rng)\n",
    "counts = np.bincount(samples, minlength=n + 1)\n",
    "pmf_mc = counts / counts.sum()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ks, y=pmf_mc, name=\"Monte Carlo\", opacity=0.6))\n",
    "fig.add_trace(go.Scatter(x=ks, y=pmf, mode=\"lines+markers\", name=\"True PMF\"))\n",
    "fig.update_layout(\n",
    "    title=\"Beta-binomial: empirical frequency vs true PMF\",\n",
    "    xaxis_title=\"k (successes)\",\n",
    "    yaxis_title=\"probability\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfb8cb",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.betabinom`)\n",
    "\n",
    "SciPy provides:\n",
    "\n",
    "- `pmf`, `logpmf`\n",
    "- `cdf`\n",
    "- `rvs`\n",
    "- `stats` (mean/var/skew/kurt)\n",
    "\n",
    "### About `.fit`\n",
    "\n",
    "As of SciPy 1.15, `scipy.stats.betabinom` (a discrete distribution) **does not expose a built-in `.fit`** method.\n",
    "\n",
    "In practice you can fit $(\\alpha,\\beta)$ by maximizing the log-likelihood with `scipy.optimize`.\n",
    "\n",
    "Below is a minimal but usable MLE routine that assumes **known $n$**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96598e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, a, b = 20, 1.5, 3.0\n",
    "ks = np.arange(n + 1)\n",
    "\n",
    "pmf_sp = stats.betabinom.pmf(ks, n, a, b)\n",
    "cdf_sp = stats.betabinom.cdf(ks, n, a, b)\n",
    "x_sp = stats.betabinom.rvs(n, a, b, size=10, random_state=rng)\n",
    "\n",
    "pmf_sp[:5], cdf_sp[:5], x_sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b60620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betabinom_moments_init(data: np.ndarray, n: int):\n",
    "    \"\"\"Method-of-moments style initializer for (a, b).\n",
    "\n",
    "    If empirical variance <= binomial variance, we return a large concentration\n",
    "    (close to a binomial model).\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    mu = float(data.mean())\n",
    "    v = float(data.var(ddof=0))\n",
    "\n",
    "    p = np.clip(mu / n, 1e-6, 1 - 1e-6)\n",
    "    v_bin = n * p * (1 - p)\n",
    "\n",
    "    if v <= v_bin * (1 + 1e-6):\n",
    "        s = 1e6\n",
    "    else:\n",
    "        r = v / v_bin\n",
    "        s = (n - r) / (r - 1)\n",
    "        s = max(s, 1e-3)\n",
    "\n",
    "    a0 = p * s\n",
    "    b0 = (1 - p) * s\n",
    "    return float(a0), float(b0)\n",
    "\n",
    "\n",
    "def fit_betabinom_mle(data: np.ndarray, n: int):\n",
    "    \"\"\"Fit (a,b) by MLE with fixed n.\n",
    "\n",
    "    Uses an unconstrained parameterization a=exp(theta0), b=exp(theta1).\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim != 1:\n",
    "        raise ValueError(\"data must be 1D\")\n",
    "    if np.any((data < 0) | (data > n)):\n",
    "        raise ValueError(\"data must be in {0,...,n}\")\n",
    "    if not np.issubdtype(data.dtype, np.integer):\n",
    "        # allow float arrays with integer values\n",
    "        if not np.all(np.isclose(data, np.round(data))):\n",
    "            raise ValueError(\"data must be integer-valued\")\n",
    "        data = np.round(data).astype(int)\n",
    "\n",
    "    a0, b0 = betabinom_moments_init(data, n=n)\n",
    "    x0 = np.log([a0, b0])\n",
    "\n",
    "    def nll(theta):\n",
    "        a = float(np.exp(theta[0]))\n",
    "        b = float(np.exp(theta[1]))\n",
    "        return -float(np.sum(stats.betabinom.logpmf(data, n, a, b)))\n",
    "\n",
    "    res = optimize.minimize(nll, x0=x0, method=\"L-BFGS-B\")\n",
    "    a_hat, b_hat = np.exp(res.x)\n",
    "    return {\n",
    "        \"a_hat\": float(a_hat),\n",
    "        \"b_hat\": float(b_hat),\n",
    "        \"success\": bool(res.success),\n",
    "        \"nll\": float(res.fun),\n",
    "        \"message\": res.message,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: recover parameters from synthetic data\n",
    "rng_fit = np.random.default_rng(123)\n",
    "\n",
    "n_true, a_true, b_true = 25, 2.0, 5.0\n",
    "data = stats.betabinom.rvs(n_true, a_true, b_true, size=2_000, random_state=rng_fit)\n",
    "\n",
    "fit = fit_betabinom_mle(data, n=n_true)\n",
    "fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d6ce2",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### Hypothesis testing (binomial vs overdispersed alternative)\n",
    "\n",
    "If you have repeated groups with the same $n$ (e.g., 100 clinics each with $n$ patients), a binomial model assumes all groups share a single $p$.\n",
    "\n",
    "Beta-binomial is a common alternative when the observed variability across groups is too large.\n",
    "\n",
    "Practical approaches:\n",
    "\n",
    "- **Model comparison**: compare binomial vs beta-binomial by log-likelihood, AIC/BIC, or held-out predictive performance.\n",
    "- **Overdispersion testing**: parameterize by mean $p$ and intraclass correlation $\\rho=1/(\\alpha+\\beta+1)$.\n",
    "  The null is $\\rho=0$ (binomial), which is a *boundary case*, so classical LRT chi-square theory needs care; a parametric bootstrap is often used.\n",
    "\n",
    "### Bayesian modeling\n",
    "\n",
    "With prior $p\\sim\\mathrm{Beta}(\\alpha_0,\\beta_0)$ and data $x$ successes out of $n$ trials:\n",
    "\n",
    "- Posterior: $p\\mid x \\sim \\mathrm{Beta}(\\alpha_0+x,\\;\\beta_0+n-x)$\n",
    "- Posterior predictive for $m$ future trials:\n",
    "  $$\n",
    "  Y\\mid x \\sim \\mathrm{BetaBinomial}(m,\\;\\alpha_0+x,\\;\\beta_0+n-x)\n",
    "  $$\n",
    "\n",
    "### Generative modeling\n",
    "\n",
    "Beta-binomial is a compact generative model for *count data with bounded support* when you expect latent heterogeneity in probabilities.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- modeling per-user conversions out of impressions\n",
    "- modeling per-batch defects out of inspected items\n",
    "- modeling per-class accuracy counts out of attempts (with varying difficulty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148cf1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison example: Binomial vs Beta-binomial on synthetic overdispersed data\n",
    "rng_use = np.random.default_rng(999)\n",
    "\n",
    "n = 30\n",
    "a_true, b_true = 1.2, 3.5\n",
    "data = stats.betabinom.rvs(n, a_true, b_true, size=500, random_state=rng_use)\n",
    "\n",
    "# Binomial MLE: p_hat = mean/n\n",
    "p_hat = float(data.mean() / n)\n",
    "ll_binom = float(np.sum(stats.binom.logpmf(data, n, p_hat)))\n",
    "aic_binom = 2 * 1 - 2 * ll_binom\n",
    "\n",
    "# Beta-binomial MLE (a,b), fixed n\n",
    "fit = fit_betabinom_mle(data, n=n)\n",
    "ll_bb = -fit[\"nll\"]\n",
    "aic_bb = 2 * 2 - 2 * ll_bb\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"binomial\", \"beta-binomial\"],\n",
    "        \"log_likelihood\": [ll_binom, ll_bb],\n",
    "        \"AIC\": [aic_binom, aic_bb],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016470a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian posterior predictive: Beta prior + Binomial likelihood -> Beta posterior -> Beta-binomial predictive\n",
    "rng_bayes = np.random.default_rng(2024)\n",
    "\n",
    "# Prior on p\n",
    "a0, b0 = 2.0, 2.0\n",
    "\n",
    "# Observed data: x successes out of n_obs\n",
    "n_obs, x_obs = 20, 8\n",
    "a_post, b_post = a0 + x_obs, b0 + (n_obs - x_obs)\n",
    "\n",
    "# Predict m future trials\n",
    "m = 15\n",
    "ks = np.arange(m + 1)\n",
    "pmf_pred = stats.betabinom.pmf(ks, m, a_post, b_post)\n",
    "\n",
    "# Monte Carlo posterior predictive simulation for comparison\n",
    "p_samp = rng_bayes.beta(a_post, b_post, size=100_000)\n",
    "y_samp = rng_bayes.binomial(m, p_samp)\n",
    "pmf_mc = np.bincount(y_samp, minlength=m + 1) / len(y_samp)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ks, y=pmf_mc, name=\"posterior predictive (MC)\", opacity=0.6))\n",
    "fig.add_trace(go.Scatter(x=ks, y=pmf_pred, mode=\"lines+markers\", name=\"beta-binomial closed form\"))\n",
    "fig.update_layout(\n",
    "    title=\"Posterior predictive: Beta-Binomial matches Monte Carlo\",\n",
    "    xaxis_title=\"future successes k\",\n",
    "    yaxis_title=\"probability\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c2dba",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "### Invalid parameters\n",
    "\n",
    "- `n` must be a **nonnegative integer**.\n",
    "- `a` and `b` (i.e., $\\alpha,\\beta$) must be **strictly positive**.\n",
    "\n",
    "SciPy will typically return `nan` for invalid parameters.\n",
    "\n",
    "### Numerical issues\n",
    "\n",
    "- Naively computing $\\binom{n}{k}$ and $B(\\cdot,\\cdot)$ can overflow/underflow for moderate/large $n$.\n",
    "- Prefer `logpmf` / `betaln` / `gammaln` and only exponentiate at the end.\n",
    "- When comparing very small probabilities, compare **log** probabilities.\n",
    "\n",
    "### Estimation issues\n",
    "\n",
    "- If data are close to binomial (little overdispersion), the MLE tends to push $\\alpha+\\beta \\to \\infty$.\n",
    "  Numerically, this can lead to large parameter estimates; using a mean/dispersion parameterization $(p,\\rho)$ can be more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invalid parameters in SciPy\n",
    "ks = np.arange(6)\n",
    "stats.betabinom.pmf(ks, n=5.5, a=1.0, b=2.0)  # invalid n (not integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74635f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underflow demonstration: compare pmf vs logpmf for larger n\n",
    "n, a, b = 500, 2.0, 5.0\n",
    "k = 250\n",
    "\n",
    "pmf_direct = stats.betabinom.pmf(k, n, a, b)\n",
    "logpmf = stats.betabinom.logpmf(k, n, a, b)\n",
    "pmf_from_log = float(np.exp(logpmf))\n",
    "\n",
    "pmf_direct, logpmf, pmf_from_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39086daf",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `betabinom` is a **discrete** distribution on $\\{0,\\dots,n\\}$.\n",
    "- It arises as a **Beta–Binomial mixture**: $p\\sim\\mathrm{Beta}(\\alpha,\\beta)$ then $X\\mid p\\sim\\mathrm{Binomial}(n,p)$.\n",
    "- Mean and variance:\n",
    "  $$\\mathbb{E}[X]=n\\frac{\\alpha}{\\alpha+\\beta},\\qquad \\mathrm{Var}(X)=n\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}\\frac{n+\\alpha+\\beta}{\\alpha+\\beta+1}.$$\n",
    "- Compared to a binomial with the same mean, beta-binomial allows **overdispersion** (extra variability) driven by heterogeneity in $p$.\n",
    "- SciPy provides PMF/CDF/RVS; for fitting $(\\alpha,\\beta)$ you can maximize the log-likelihood with `scipy.optimize`.\n",
    "\n",
    "### Suggested exercises\n",
    "\n",
    "1. Fix $n$ and $p$ and vary $s=\\alpha+\\beta$: quantify how quickly the beta-binomial approaches a Binomial.\n",
    "2. Derive factorial moments $\\mathbb{E}[(X)_r]$ using the hierarchical model.\n",
    "3. Reparameterize by $(p,\\rho)$ with $\\rho=1/(\\alpha+\\beta+1)$ and fit in that space.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}