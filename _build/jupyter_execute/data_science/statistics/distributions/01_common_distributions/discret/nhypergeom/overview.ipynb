{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7a0a8e",
   "metadata": {},
   "source": [
    "# Negative hypergeometric distribution (`nhypergeom`)\n",
    "\n",
    "The negative hypergeometric distribution models **sequential sampling without replacement** from a finite population.\n",
    "\n",
    "**SciPy's parameterization (used in this notebook):**\n",
    "\n",
    "- A population has **`M`** total items: **`n` red** and **`M-n` blue**.\n",
    "- Draw items one-by-one **without replacement** until you have drawn **`r` blue** items.\n",
    "- The random variable is\n",
    "  $$\n",
    "  X = \\text{# of red items drawn when the } r\\text{-th blue item appears}.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Classify the distribution and understand the support/constraints.\n",
    "- Write the PMF and CDF (and connect them to the hypergeometric distribution).\n",
    "- Compute moments (mean/variance/skewness/kurtosis) and interpret parameters.\n",
    "- Derive expectation/variance and write a practical log-likelihood.\n",
    "- Implement **NumPy-only** simulation algorithms.\n",
    "- Visualize PMF/CDF and validate with Monte Carlo.\n",
    "- Use `scipy.stats.nhypergeom` and `scipy.stats.fit`.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic probability (PMF, CDF, expectation)\n",
    "- Combinatorics (binomial coefficients)\n",
    "- Familiarity with sampling without replacement (hypergeometric distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc6cdd",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations (Expectation, Variance, Likelihood)\n",
    "7. Sampling & Simulation (NumPy-only)\n",
    "8. Visualization (PMF, CDF, Monte Carlo)\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import special, stats\n",
    "from scipy.special import logsumexp, xlogy\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbe1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import plotly\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"SciPy:\", scipy.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"Seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99c0c5",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `nhypergeom` (negative hypergeometric distribution)\n",
    "- **Type**: **Discrete**\n",
    "- **Support**: $k \\in \\{0,1,\\dots,n\\}$\n",
    "- **Parameter space** (SciPy):\n",
    "  - $M \\in \\{1,2,\\dots\\}$ (population size)\n",
    "  - $n \\in \\{0,1,\\dots,M\\}$ (number of red items)\n",
    "  - $r \\in \\{0,1,\\dots,M-n\\}$ (number of blue items to draw before stopping)\n",
    "\n",
    "Notation:\n",
    "- $X \\sim \\mathrm{NHG}(M,n,r)$ is common in texts.\n",
    "- In SciPy: `stats.nhypergeom(M, n, r)`.\n",
    "\n",
    "**Non-degenerate case:** typically $r\\ge 1$ and $0<n<M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85553a",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "\n",
    "You have a *finite* population with two types (red/blue). You sample **without replacement** until you have observed **`r` blue** items. The distribution describes how many **red** items you saw by that stopping time.\n",
    "\n",
    "This is an \"inverse\" version of the hypergeometric setting:\n",
    "- **Hypergeometric**: fix the number of draws, count how many reds you see.\n",
    "- **Negative hypergeometric (`nhypergeom`)**: fix the number of blues you must see, count how many reds you see before that happens.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Quality control / audit sampling**: inspect items from a finite lot until you find `r` defects (blue); how many non-defects (red) did you see?\n",
    "- **Reliability / screening**: test units without replacement until `r` failures are found; count the passes.\n",
    "- **Games / cards**: draw from a deck until you see `r` cards of a certain category; count another category.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Hypergeometric connection**: the PMF can be written using a hypergeometric PMF on the first $k+r-1$ draws and the probability the $(k+r)$-th draw is blue.\n",
    "- **Negative binomial limit**: as $M\\to\\infty$ with $n/M\\to p$ fixed (sampling becomes \"with replacement\"), `nhypergeom` approaches a **negative binomial** distribution for the number of red successes before `r` blue failures.\n",
    "- **Beta-binomial identity**: `nhypergeom(M,n,r)` is equivalent to a **beta-binomial** distribution with a particular parameter mapping (used below for moments).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5e18b",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### PMF\n",
    "\n",
    "Let $M$ be the population size, with $n$ red and $M-n$ blue items. Sample without replacement until $r$ blue items have been drawn.\n",
    "\n",
    "For $k \\in \\{0,1,\\dots,n\\}$, the PMF is\n",
    "\n",
    "$$\n",
    "\\Pr(X=k) = \\frac{\\binom{k+r-1}{k}\\,\\binom{M-r-k}{n-k}}{\\binom{M}{n}}.\n",
    "$$\n",
    "\n",
    "A useful interpretation:\n",
    "- In the first $k+r-1$ draws, you must have seen exactly $k$ reds (and $r-1$ blues).\n",
    "- The next draw (the $(k+r)$-th) must be blue.\n",
    "\n",
    "SciPy states the relationship to the hypergeometric PMF as:\n",
    "$$\n",
    "\\mathrm{NHG}(k;M,n,r)\n",
    "= \\mathrm{HG}(k;M,n,k+r-1)\\,\\frac{M-n-(r-1)}{M-(k+r-1)}.\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "For integer $k$,\n",
    "$$\n",
    "F(k) = \\Pr(X \\le k) = \\sum_{j=0}^{k} \\Pr(X=j).\n",
    "$$\n",
    "\n",
    "There is also a helpful hypergeometric view: by time $k+r$ you have drawn $k+r$ items; having stopped by then is equivalent to having seen at least $r$ blues, i.e. at most $k$ reds.\n",
    "If $Y \\sim \\mathrm{Hypergeom}(M, n, k+r)$ counts reds in the first $k+r$ draws, then\n",
    "$$\n",
    "F(k) = \\Pr(Y \\le k).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_nhypergeom_params(M: int, n: int, r: int):\n",
    "    M = int(M)\n",
    "    n = int(n)\n",
    "    r = int(r)\n",
    "    if M <= 0:\n",
    "        raise ValueError(f\"M must be a positive integer, got {M!r}\")\n",
    "    if not (0 <= n <= M):\n",
    "        raise ValueError(f\"n must satisfy 0 <= n <= M, got n={n!r}, M={M!r}\")\n",
    "    if not (0 <= r <= M - n):\n",
    "        raise ValueError(\n",
    "            f\"r must satisfy 0 <= r <= M-n, got r={r!r}, M={M!r}, n={n!r}\"\n",
    "        )\n",
    "    return M, n, r\n",
    "\n",
    "\n",
    "def _is_integral(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    return np.isfinite(x) & (x == np.floor(x))\n",
    "\n",
    "\n",
    "def log_choose(N, K):\n",
    "    \"\"\"log binomial coefficient log(C(N,K)) with broadcasting.\n",
    "\n",
    "    Returns -inf for invalid K.\n",
    "    \"\"\"\n",
    "    N = np.asarray(N)\n",
    "    K = np.asarray(K)\n",
    "    N, K = np.broadcast_arrays(N, K)\n",
    "    out = np.full(N.shape, -np.inf, dtype=float)\n",
    "    valid = (K >= 0) & (K <= N)\n",
    "    out[valid] = (\n",
    "        special.gammaln(N[valid] + 1)\n",
    "        - special.gammaln(K[valid] + 1)\n",
    "        - special.gammaln(N[valid] - K[valid] + 1)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def nhypergeom_logpmf(k, M: int, n: int, r: int):\n",
    "    \"\"\"Log PMF of nhypergeom in SciPy's parameterization (vectorized in k).\"\"\"\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    k = np.asarray(k)\n",
    "\n",
    "    # Handle degenerate r=0: stop immediately, so P(X=0)=1.\n",
    "    if r == 0:\n",
    "        return np.where(k == 0, 0.0, -np.inf).astype(float)\n",
    "\n",
    "    integral = _is_integral(k)\n",
    "    k_int = np.floor(k).astype(int)\n",
    "\n",
    "    valid_k = integral & (0 <= k_int) & (k_int <= n)\n",
    "\n",
    "    logp = np.full(k.shape, -np.inf, dtype=float)\n",
    "    if np.any(valid_k):\n",
    "        kv = k_int[valid_k]\n",
    "        log_num1 = log_choose(kv + r - 1, kv)\n",
    "        log_num2 = log_choose(M - r - kv, n - kv)\n",
    "        log_den = log_choose(M, n)\n",
    "        logp[valid_k] = log_num1 + log_num2 - log_den\n",
    "    return logp\n",
    "\n",
    "\n",
    "def nhypergeom_pmf(k, M: int, n: int, r: int):\n",
    "    return np.exp(nhypergeom_logpmf(k, M=M, n=n, r=r))\n",
    "\n",
    "\n",
    "def nhypergeom_pmf_array(M: int, n: int, r: int):\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    ks = np.arange(0, n + 1)\n",
    "    pmf = nhypergeom_pmf(ks, M=M, n=n, r=r)\n",
    "    return ks, pmf\n",
    "\n",
    "\n",
    "def nhypergeom_cdf_array(M: int, n: int, r: int):\n",
    "    ks, pmf = nhypergeom_pmf_array(M, n, r)\n",
    "    return ks, np.cumsum(pmf)\n",
    "\n",
    "\n",
    "def nhypergeom_cdf(k, M: int, n: int, r: int):\n",
    "    \"\"\"CDF computed by summing the finite PMF (vectorized in k).\"\"\"\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    k = np.asarray(k)\n",
    "\n",
    "    if r == 0:\n",
    "        return np.where(k < 0, 0.0, 1.0).astype(float)\n",
    "\n",
    "    ks, cdf = nhypergeom_cdf_array(M, n, r)\n",
    "\n",
    "    # For non-integer k, use floor(k).\n",
    "    k_floor = np.floor(k).astype(int)\n",
    "    out = np.zeros_like(k_floor, dtype=float)\n",
    "    out[k_floor >= n] = 1.0\n",
    "\n",
    "    mid = (k_floor >= 0) & (k_floor < n)\n",
    "    out[mid] = cdf[k_floor[mid]]\n",
    "    return out\n",
    "\n",
    "\n",
    "# Demo\n",
    "M_demo, n_demo, r_demo = 40, 12, 5\n",
    "ks_demo, pmf_demo = nhypergeom_pmf_array(M_demo, n_demo, r_demo)\n",
    "cdf_demo = np.cumsum(pmf_demo)\n",
    "\n",
    "print(\"support k:\", (ks_demo[0], ks_demo[-1]))\n",
    "print(\"sum pmf:\", pmf_demo.sum())\n",
    "print(\"mean (pmf sum):\", (ks_demo * pmf_demo).sum())\n",
    "print(\"scipy mean:\", stats.nhypergeom.mean(M_demo, n_demo, r_demo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6bdbb",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "A convenient way to get moments is via an identity with the **beta-binomial** distribution.\n",
    "\n",
    "Let $m = M-n$ be the number of blue items. If you view blue as “successes” and red as “failures”, then drawing until $r$ blues means you are counting red failures before $r$ blue successes. One can show:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathrm{BetaBinomial}\\big(N=n,\\;\\alpha=r,\\;\\beta=m-r+1\\big)\n",
    "= \\mathrm{BetaBinomial}\\big(n,\\;r,\\;M-n-r+1\\big).\n",
    "$$\n",
    "\n",
    "This implies clean closed forms for the first four standardized moments.\n",
    "\n",
    "Let\n",
    "$$\n",
    "N=n,\\quad \\alpha=r,\\quad \\beta=M-n-r+1,\\quad s=\\alpha+\\beta=M-n+1.\n",
    "$$\n",
    "\n",
    "### Mean\n",
    "$$\n",
    "\\mathbb{E}[X] = N\\,\\frac{\\alpha}{\\alpha+\\beta}\n",
    "= \\frac{rn}{M-n+1}.\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "$$\n",
    "\\mathrm{Var}(X)\n",
    "= N\\,\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}\\,\\frac{\\alpha+\\beta+N}{\\alpha+\\beta+1}\n",
    "= \\frac{r\\,(M+1)\\,n\\,(M-n-r+1)}{(M-n+1)^2\\,(M-n+2)}.\n",
    "$$\n",
    "\n",
    "### Skewness and excess kurtosis\n",
    "Using the beta-binomial formulas:\n",
    "\n",
    "$$\n",
    "\\gamma_1\n",
    "= \\frac{(s+2N)(\\beta-\\alpha)\\sqrt{s+1}}\n",
    "{(s+2)\\sqrt{N\\alpha\\beta(s+N)}}.\n",
    "$$\n",
    "\n",
    "A closed form for **excess kurtosis** (kurtosis minus 3) is:\n",
    "let $t=\\alpha\\beta$, then\n",
    "\n",
    "$$\n",
    "\\gamma_2\n",
    "= \\frac{(s+1)}{N\\,t\\,(s+N)(s+2)(s+3)}\\Big[\n",
    "s^4 + (6N-1)s^3 + (6N^2 + 3t(N-2))s^2 - 3tN(6-N)s - 18tN^2\n",
    "\\Big] - 3.\n",
    "$$\n",
    "\n",
    "### PGF / MGF / characteristic function\n",
    "Because the support is finite ($0\\le X\\le n$), all moments exist and the MGF exists for all real $t$.\n",
    "\n",
    "The **probability generating function** (PGF) can be written via the Gauss hypergeometric function:\n",
    "$$\n",
    "G(z)=\\mathbb{E}[z^X] = {}_2F_1\\left(-N,\\;\\alpha;\\;\\alpha+\\beta;\\;1-z\\right).\n",
    "$$\n",
    "Then\n",
    "- $M(t)=\\mathbb{E}[e^{tX}] = G(e^t)$\n",
    "- $\\varphi(t)=\\mathbb{E}[e^{itX}] = G(e^{it})$\n",
    "\n",
    "### Entropy\n",
    "There is no simple closed form in general; numerically:\n",
    "$$\n",
    "H(X) = -\\sum_{k=0}^n p(k)\\,\\log p(k).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4444bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nhypergeom_mean(M: int, n: int, r: int) -> float:\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    if r == 0 or n == 0:\n",
    "        return 0.0\n",
    "    return (r * n) / (M - n + 1)\n",
    "\n",
    "\n",
    "def nhypergeom_var(M: int, n: int, r: int) -> float:\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    if r == 0 or n == 0:\n",
    "        return 0.0\n",
    "    denom = (M - n + 1) ** 2 * (M - n + 2)\n",
    "    return (r * (M + 1) * n * (M - n - r + 1)) / denom\n",
    "\n",
    "\n",
    "def nhypergeom_skewness(M: int, n: int, r: int) -> float:\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    v = nhypergeom_var(M, n, r)\n",
    "    if v == 0.0:\n",
    "        return np.nan\n",
    "\n",
    "    N = n\n",
    "    alpha = r\n",
    "    beta = M - n - r + 1\n",
    "    s = alpha + beta\n",
    "\n",
    "    num = (s + 2 * N) * (beta - alpha) * np.sqrt(s + 1)\n",
    "    den = (s + 2) * np.sqrt(N * alpha * beta * (s + N))\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "def nhypergeom_excess_kurtosis(M: int, n: int, r: int) -> float:\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    v = nhypergeom_var(M, n, r)\n",
    "    if v == 0.0:\n",
    "        return np.nan\n",
    "\n",
    "    N = n\n",
    "    alpha = r\n",
    "    beta = M - n - r + 1\n",
    "    s = alpha + beta\n",
    "    t = alpha * beta\n",
    "\n",
    "    poly = (\n",
    "        s**4\n",
    "        + (6 * N - 1) * s**3\n",
    "        + (6 * N**2 + 3 * t * (N - 2)) * s**2\n",
    "        - 3 * t * N * (6 - N) * s\n",
    "        - 18 * t * N**2\n",
    "    )\n",
    "\n",
    "    return float(((s + 1) / (N * t * (s + N) * (s + 2) * (s + 3))) * poly - 3)\n",
    "\n",
    "\n",
    "def nhypergeom_pgf(z: complex, M: int, n: int, r: int) -> complex:\n",
    "    \"\"\"Probability generating function via the beta-binomial identity.\"\"\"\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    if r == 0:\n",
    "        return 1.0 + 0j\n",
    "    N = n\n",
    "    alpha = r\n",
    "    beta = M - n - r + 1\n",
    "    return complex(special.hyp2f1(-N, alpha, alpha + beta, 1 - z))\n",
    "\n",
    "\n",
    "def nhypergeom_mgf(t: float, M: int, n: int, r: int) -> float:\n",
    "    return float(np.real(nhypergeom_pgf(np.exp(t), M=M, n=n, r=r)))\n",
    "\n",
    "\n",
    "def nhypergeom_cf(t: float, M: int, n: int, r: int) -> complex:\n",
    "    return nhypergeom_pgf(np.exp(1j * t), M=M, n=n, r=r)\n",
    "\n",
    "\n",
    "def nhypergeom_entropy(M: int, n: int, r: int, base=np.e) -> float:\n",
    "    \"\"\"Entropy computed from the finite PMF (base=e -> nats, base=2 -> bits).\"\"\"\n",
    "    ks, pmf = nhypergeom_pmf_array(M, n, r)\n",
    "    H_nats = -np.sum(xlogy(pmf, pmf))\n",
    "    return float(H_nats / np.log(base))\n",
    "\n",
    "\n",
    "# Quick sanity check vs SciPy\n",
    "M, n, r = 40, 12, 5\n",
    "print(\"mean (formula):\", nhypergeom_mean(M, n, r))\n",
    "print(\"var  (formula):\", nhypergeom_var(M, n, r))\n",
    "print(\"skew (formula):\", nhypergeom_skewness(M, n, r))\n",
    "print(\"kurt_excess (formula):\", nhypergeom_excess_kurtosis(M, n, r))\n",
    "print(\"entropy (nats, numeric):\", nhypergeom_entropy(M, n, r))\n",
    "print(\"SciPy (mvsk):\", stats.nhypergeom.stats(M, n, r, moments=\"mvsk\"))\n",
    "print(\"MGF(0) should be 1:\", nhypergeom_mgf(0.0, M, n, r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5172b7c",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "Parameters in the **sampling story**:\n",
    "\n",
    "- **`M`**: total population size.\n",
    "- **`n`**: number of red items (often thought of as “good”, “success”, or a target category).\n",
    "- **`r`**: how many blue items you require before stopping (a design/stop rule).\n",
    "\n",
    "The mean\n",
    "$$\n",
    "\\mathbb{E}[X] = \\frac{rn}{M-n+1}\n",
    "$$\n",
    "makes some qualitative behavior obvious:\n",
    "\n",
    "- Increasing **`r`** increases the expected number of reds roughly linearly.\n",
    "- Increasing **`n`** (more reds, fewer blues) increases $\\mathbb{E}[X]$ sharply because blues become rarer.\n",
    "- Increasing **`M`** while holding **`n`** fixed decreases the red fraction and typically decreases $\\mathbb{E}[X]$.\n",
    "\n",
    "Edge behavior:\n",
    "- If $r=0$, you stop immediately and $X=0$.\n",
    "- If $n=0$, there are no reds and $X=0$.\n",
    "- If $r=M-n$ (you require *all* blues), $X$ tends to be large and often close to $n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e5b82",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation via symmetry of “gaps”\n",
    "\n",
    "Think of a random permutation of the $M$ items. Let $m=M-n$ be the number of blue items.\n",
    "\n",
    "Place the $m$ blues in the permutation. This creates $m+1$ **gaps** of red items:\n",
    "- $G_0$: # reds before the 1st blue\n",
    "- $G_1$: # reds between the 1st and 2nd blue\n",
    "- ...\n",
    "- $G_m$: # reds after the last blue\n",
    "\n",
    "These gaps satisfy $G_0+\\cdots+G_m = n$.\n",
    "\n",
    "By symmetry (all placements of the $m$ blue positions are equally likely), the joint distribution of $(G_0,\\dots,G_m)$ is **exchangeable**, hence\n",
    "$$\n",
    "\\mathbb{E}[G_0]=\\cdots=\\mathbb{E}[G_m] = \\frac{n}{m+1}.\n",
    "$$\n",
    "\n",
    "If we stop at the $r$-th blue, then the number of reds observed is the sum of the first $r$ gaps:\n",
    "$$\n",
    "X = G_0+\\cdots+G_{r-1}.\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "\\mathbb{E}[X] = r\\,\\frac{n}{m+1} = \\frac{rn}{M-n+1}.\n",
    "$$\n",
    "\n",
    "### 6.2 Variance via Dirichlet-multinomial / beta-binomial\n",
    "\n",
    "The gap vector $(G_0,\\dots,G_m)$ is **uniform over compositions** of $n$ into $m+1$ nonnegative integers. This is exactly a Dirichlet-multinomial distribution with concentration parameters all equal to 1.\n",
    "\n",
    "A key closure property: sums of Dirichlet-multinomial components are **beta-binomial**. Since $X$ is the sum of $r$ gaps, it follows that\n",
    "$$\n",
    "X \\sim \\mathrm{BetaBinomial}(N=n,\\alpha=r,\\beta=m-r+1).\n",
    "$$\n",
    "Plugging into the beta-binomial variance formula yields\n",
    "$$\n",
    "\\mathrm{Var}(X) = \\frac{r\\,(M+1)\\,n\\,(M-n-r+1)}{(M-n+1)^2\\,(M-n+2)}.\n",
    "$$\n",
    "\n",
    "### 6.3 Likelihood (i.i.d. replicated experiments)\n",
    "\n",
    "If we observe i.i.d. samples $k_1,\\dots,k_T$ from $\\mathrm{NHG}(M,n,r)$ (e.g., repeated experiments on comparable lots), the likelihood is\n",
    "$$\n",
    "L(M,n,r; k_{1:T}) = \\prod_{t=1}^T \\Pr(X=k_t\\mid M,n,r).\n",
    "$$\n",
    "\n",
    "Because parameters are **integers with constraints**, maximum likelihood is typically done via **grid search** or specialized integer optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b847de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nhypergeom_loglikelihood(data: np.ndarray, M: int, n: int, r: int) -> float:\n",
    "    data = np.asarray(data)\n",
    "    return float(np.sum(nhypergeom_logpmf(data, M=M, n=n, r=r)))\n",
    "\n",
    "\n",
    "# Example: infer n (reds) with M,r known via grid search\n",
    "M_true, n_true, r_true = 60, 18, 6\n",
    "dist_true = stats.nhypergeom(M_true, n_true, r_true)\n",
    "data = dist_true.rvs(size=500, random_state=rng)\n",
    "\n",
    "n_min = int(data.max())\n",
    "n_max = M_true - r_true  # because r <= M-n\n",
    "n_grid = np.arange(n_min, n_max + 1)\n",
    "\n",
    "ll = np.array([nhypergeom_loglikelihood(data, M_true, n0, r_true) for n0 in n_grid])\n",
    "n_mle = int(n_grid[np.argmax(ll)])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=n_grid, y=ll, mode=\"lines\", name=\"log-likelihood\"))\n",
    "fig.add_vline(x=n_true, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"n_true\")\n",
    "fig.add_vline(x=n_mle, line_dash=\"dot\", line_color=\"black\", annotation_text=\"n_MLE\")\n",
    "fig.update_layout(\n",
    "    title=f\"NHG log-likelihood for n (M={M_true}, r={r_true})\",\n",
    "    xaxis_title=\"n (number of reds)\",\n",
    "    yaxis_title=\"log L\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"n_true:\", n_true)\n",
    "print(\"n_MLE:\", n_mle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71af96b",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "We want to sample $X$ without calling SciPy.\n",
    "\n",
    "### Algorithm A (order statistic of blue positions)\n",
    "\n",
    "A random permutation of the $M$ items is equivalent to choosing the positions of the **blue** items uniformly.\n",
    "\n",
    "Let $m=M-n$ be the number of blue items and let $T_r$ be the position (1-indexed) of the $r$-th blue in the permutation.\n",
    "Then the number of reds before the $r$-th blue is\n",
    "$$\n",
    "X = T_r - r.\n",
    "$$\n",
    "\n",
    "So a simple sampler is:\n",
    "1. sample the $m$ blue positions without replacement,\n",
    "2. sort them,\n",
    "3. take the $r$-th smallest position.\n",
    "\n",
    "### Algorithm B (sequential urn simulation)\n",
    "\n",
    "Maintain remaining counts of red and blue. At each draw, draw red with probability\n",
    "$$\n",
    "\\Pr(\\text{red next}) = \\frac{n_{\\text{rem}}}{n_{\\text{rem}} + m_{\\text{rem}}},\n",
    "$$\n",
    "update counts, and stop when $r$ blues have been drawn.\n",
    "\n",
    "Algorithm A is usually faster for moderate $M$ because it avoids simulating each draw.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nhypergeom_positions(M: int, n: int, r: int, size: int, rng: np.random.Generator):\n",
    "    \"\"\"NumPy-only sampler via blue position order statistics.\"\"\"\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    size = int(size)\n",
    "    if size < 0:\n",
    "        raise ValueError(\"size must be non-negative\")\n",
    "\n",
    "    if r == 0 or n == 0:\n",
    "        return np.zeros(size, dtype=int)\n",
    "\n",
    "    m = M - n  # # blues\n",
    "    out = np.empty(size, dtype=int)\n",
    "\n",
    "    for i in range(size):\n",
    "        blue_pos = rng.choice(M, size=m, replace=False)\n",
    "        blue_pos.sort()\n",
    "        t0 = blue_pos[r - 1]  # 0-indexed position of r-th blue\n",
    "        out[i] = t0 - (r - 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_nhypergeom_sequential(M: int, n: int, r: int, size: int, rng: np.random.Generator):\n",
    "    \"\"\"NumPy-only sampler via sequential draws without replacement.\"\"\"\n",
    "    M, n, r = validate_nhypergeom_params(M, n, r)\n",
    "    size = int(size)\n",
    "    if size < 0:\n",
    "        raise ValueError(\"size must be non-negative\")\n",
    "\n",
    "    if r == 0 or n == 0:\n",
    "        return np.zeros(size, dtype=int)\n",
    "\n",
    "    out = np.empty(size, dtype=int)\n",
    "    for i in range(size):\n",
    "        red_rem = n\n",
    "        blue_rem = M - n\n",
    "        red_seen = 0\n",
    "        blue_seen = 0\n",
    "\n",
    "        while blue_seen < r:\n",
    "            p_red = red_rem / (red_rem + blue_rem)\n",
    "            if rng.random() < p_red:\n",
    "                red_rem -= 1\n",
    "                red_seen += 1\n",
    "            else:\n",
    "                blue_rem -= 1\n",
    "                blue_seen += 1\n",
    "\n",
    "        out[i] = red_seen\n",
    "    return out\n",
    "\n",
    "\n",
    "# Compare samplers + SciPy moments\n",
    "M, n, r = 60, 18, 6\n",
    "size = 50_000\n",
    "\n",
    "x_pos = sample_nhypergeom_positions(M, n, r, size=size, rng=rng)\n",
    "x_seq = sample_nhypergeom_sequential(M, n, r, size=size, rng=rng)\n",
    "\n",
    "print(\"pos sampler mean/var:\", x_pos.mean(), x_pos.var())\n",
    "print(\"seq sampler mean/var:\", x_seq.mean(), x_seq.var())\n",
    "print(\"theory mean/var:\", nhypergeom_mean(M, n, r), nhypergeom_var(M, n, r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88badd3a",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** on $\\{0,\\dots,n\\}$\n",
    "- the **CDF** (a step function)\n",
    "- Monte Carlo samples vs the exact PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d73e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, n, r = 60, 18, 6\n",
    "ks, pmf = nhypergeom_pmf_array(M, n, r)\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=ks, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(\n",
    "    title=f\"Negative hypergeometric PMF (M={M}, n={n}, r={r})\",\n",
    "    xaxis_title=\"k (reds drawn)\",\n",
    "    yaxis_title=\"P(X=k)\",\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=ks, y=cdf, mode=\"lines\", line_shape=\"hv\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(\n",
    "    title=f\"Negative hypergeometric CDF (M={M}, n={n}, r={r})\",\n",
    "    xaxis_title=\"k (reds drawn)\",\n",
    "    yaxis_title=\"P(X≤k)\",\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "mc = sample_nhypergeom_positions(M, n, r, size=150_000, rng=rng)\n",
    "counts = np.bincount(mc, minlength=n + 1)\n",
    "pmf_hat = counts / counts.sum()\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=ks, y=pmf_hat, name=\"Monte Carlo\", opacity=0.6))\n",
    "fig_mc.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers+lines\", name=\"Exact PMF\"))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo vs exact PMF (size={mc.size:,})\",\n",
    "    xaxis_title=\"k (reds drawn)\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c9b9a",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.nhypergeom` as an `rv_discrete` distribution.\n",
    "\n",
    "Common methods:\n",
    "- `pmf(k, M, n, r)` / `logpmf(...)`\n",
    "- `cdf(k, M, n, r)` / `sf(...)`\n",
    "- `rvs(M, n, r, size=..., random_state=...)`\n",
    "- `stats(M, n, r, moments='mvsk')`, `entropy(...)`\n",
    "\n",
    "### About `fit`\n",
    "\n",
    "`rv_discrete` objects like `nhypergeom` do not expose a `.fit(...)` method.\n",
    "In SciPy 1.15+, use the generic `scipy.stats.fit` function, which supports constraints and integer parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.nhypergeom\n",
    "\n",
    "M, n, r = 60, 18, 6\n",
    "print(\"pmf(0..5):\", dist.pmf(np.arange(6), M, n, r))\n",
    "print(\"cdf(0..5):\", dist.cdf(np.arange(6), M, n, r))\n",
    "print(\"mean/var:\", dist.stats(M, n, r, moments=\"mv\"))\n",
    "\n",
    "samples = dist.rvs(M, n, r, size=10, random_state=rng)\n",
    "print(\"rvs:\", samples)\n",
    "\n",
    "# Fitting example: estimate n with M and r fixed (bounds enforce integrality/constraints)\n",
    "data = dist.rvs(M, n, r, size=2_000, random_state=rng)\n",
    "fit_res = stats.fit(\n",
    "    dist,\n",
    "    data,\n",
    "    bounds={\n",
    "        \"M\": (M, M),\n",
    "        \"r\": (r, r),\n",
    "        \"n\": (0, M - r),\n",
    "        \"loc\": (0, 0),\n",
    "    },\n",
    ")\n",
    "print(fit_res)\n",
    "print(\"n_hat:\", int(fit_res.params.n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64087e1",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing\n",
    "\n",
    "If $M$ and the stopping rule $r$ are fixed by the design, you can test hypotheses about $n$ (how many red items are in the population).\n",
    "\n",
    "Example (one-sided):\n",
    "- $H_0$: $n=n_0$ (a claimed red count)\n",
    "- $H_1$: $n<n_0$ (fewer reds than claimed)\n",
    "\n",
    "An observed $k_\\text{obs}$ that is **too small** is evidence against $H_0$.\n",
    "A p-value is\n",
    "$$\n",
    "p = \\Pr(X\\le k_\\text{obs}\\mid M,n_0,r) = F(k_\\text{obs}).\n",
    "$$\n",
    "\n",
    "### 10.2 Bayesian modeling (posterior on $n$)\n",
    "\n",
    "Because $n$ is an integer, a practical approach is a **discrete prior** on $n$ and an exact posterior by enumeration:\n",
    "$$\n",
    "\\Pr(n\\mid k) \\propto \\Pr(k\\mid n)\\,\\Pr(n).\n",
    "$$\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "\n",
    "`nhypergeom` is a useful component when your data arise from **finite-population** sequential sampling.\n",
    "In a generative model you might:\n",
    "- set/learn $M$ (population size)\n",
    "- model $n$ (red count) as latent\n",
    "- generate $X$ from `nhypergeom(M,n,r)` under a sampling protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea20017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Hypothesis test: too few reds?\n",
    "M, r = 60, 6\n",
    "n0 = 18  # claimed number of reds\n",
    "k_obs = 6\n",
    "\n",
    "p_value = stats.nhypergeom.cdf(k_obs, M, n0, r)\n",
    "print(\"p-value (P[X<=k_obs] under H0):\", p_value)\n",
    "\n",
    "\n",
    "# 10.2 Bayesian posterior on n (uniform prior over feasible n)\n",
    "n_grid = np.arange(k_obs, M - r + 1)  # must satisfy n >= k_obs and r <= M-n\n",
    "log_prior = -np.log(n_grid.size) * np.ones_like(n_grid, dtype=float)\n",
    "log_like = np.array([nhypergeom_logpmf(k_obs, M=M, n=int(n), r=r) for n in n_grid])\n",
    "\n",
    "log_post_unnorm = log_prior + log_like\n",
    "logZ = logsumexp(log_post_unnorm)\n",
    "post = np.exp(log_post_unnorm - logZ)\n",
    "\n",
    "post_mean_n = float(np.sum(n_grid * post))\n",
    "cdf_post = np.cumsum(post)\n",
    "ci_low = int(n_grid[np.searchsorted(cdf_post, 0.05)])\n",
    "ci_high = int(n_grid[np.searchsorted(cdf_post, 0.95)])\n",
    "\n",
    "print(\"posterior mean of n:\", post_mean_n)\n",
    "print(\"90% credible interval for n:\", (ci_low, ci_high))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=n_grid, y=post, name=\"posterior\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Posterior over n given k_obs={k_obs} (M={M}, r={r})\",\n",
    "    xaxis_title=\"n (reds in population)\",\n",
    "    yaxis_title=\"Posterior probability\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# 10.2 Posterior predictive for a future experiment with the same (M,r)\n",
    "ks = np.arange(0, n_grid.max() + 1)\n",
    "pmf_mix = np.zeros_like(ks, dtype=float)\n",
    "for weight, n_val in zip(post, n_grid):\n",
    "    ks_n, pmf_n = nhypergeom_pmf_array(M, int(n_val), r)\n",
    "    pmf_mix[ks_n] += weight * pmf_n\n",
    "\n",
    "fig_pp = go.Figure()\n",
    "fig_pp.add_trace(go.Bar(x=ks, y=pmf_mix, name=\"posterior predictive\"))\n",
    "fig_pp.update_layout(\n",
    "    title=\"Posterior predictive PMF for X\",\n",
    "    xaxis_title=\"k (reds before r blues)\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_pp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189133f",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameterization confusion**: some references define the RV as “# failures before $r$ successes” (a swapped story). Always confirm what $r$ and $k$ mean.\n",
    "- **Invalid parameters**: in SciPy’s definition you must have integers with $0\\le n\\le M$ and $0\\le r\\le M-n$.\n",
    "- **Degenerate cases**: $r=0$ or $n=0$ collapse the distribution to a point mass at 0.\n",
    "- **Numerical stability**: factorials / binomial coefficients overflow quickly; prefer `logpmf` with `gammaln` and only exponentiate at the end.\n",
    "- **Runtime**: naive sequential simulation can be slow when expected stopping time is large; the “blue position” sampler is often faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f96a8c",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `nhypergeom` is a **discrete** distribution for **sequential sampling without replacement** from a finite population.\n",
    "- In SciPy’s parameterization, it models the **number of red items** observed by the time the **$r$-th blue** item appears.\n",
    "- The PMF has a clean combinatorial form and is closely related to the **hypergeometric** distribution.\n",
    "- Mean/variance (and skew/kurtosis) have closed forms via an identity with the **beta-binomial** distribution.\n",
    "- Sampling can be done NumPy-only via **order statistics of blue positions** or a **sequential urn** simulation.\n",
    "- SciPy provides `pmf/cdf/rvs/stats/entropy`; fitting is done via `scipy.stats.fit`.\n",
    "\n",
    "### References\n",
    "\n",
    "- SciPy docstring: `scipy.stats.nhypergeom`\n",
    "- SciPy docstring: `scipy.stats.fit`\n",
    "- Wikipedia: “Negative hypergeometric distribution”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}