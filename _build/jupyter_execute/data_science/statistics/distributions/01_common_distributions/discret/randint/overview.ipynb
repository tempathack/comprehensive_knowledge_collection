{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c677434d",
   "metadata": {},
   "source": [
    "# Random integer (discrete uniform) distribution (`randint`)\n",
    "\n",
    "The `randint` distribution models an integer drawn **uniformly at random** from a finite range of consecutive integers.\n",
    "\n",
    "In SciPy, it corresponds to `scipy.stats.randint(low, high)` with support:\n",
    "$$\n",
    "\\{\\texttt{low},\\ \\texttt{low}+1,\\ \\ldots,\\ \\texttt{high}-1\\}.\n",
    "$$\n",
    "\n",
    "## Learning goals\n",
    "- Recognize when a **bounded integer uniform** model is appropriate.\n",
    "- Write the PMF and CDF carefully (including the half-open interval convention).\n",
    "- Compute mean/variance/skewness/kurtosis and entropy.\n",
    "- Derive the likelihood and the MLE for the bounds.\n",
    "- Implement sampling **from scratch (NumPy-only)** and validate it by simulation.\n",
    "- Use `scipy.stats.randint` for PMF/CDF/sampling and understand how to fit with `scipy.stats.fit`.\n",
    "\n",
    "## Prerequisites\n",
    "- Basic probability (PMF/CDF), expectation, and variance\n",
    "- Familiarity with sums like $\\sum_{k=0}^{n-1} k$ and $\\sum_{k=0}^{n-1} k^2$\n",
    "- Comfort with logs and basic optimization intuition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69da086",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations (Expectation, Variance, Likelihood)\n",
    "7. Sampling & Simulation (NumPy-only)\n",
    "8. Visualization (PMF, CDF, Monte Carlo)\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import plotly\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"SciPy:\", scipy.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"Seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc09e9",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `randint` (random integer / discrete uniform on a contiguous integer interval)\n",
    "- **Type**: **Discrete**\n",
    "- **Support** (SciPy convention):\n",
    "  $$x \\in \\{\\ell,\\ \\ell+1,\\ \\ldots,\\ h-1\\}$$\n",
    "  where $\\ell = \\texttt{low}$ and $h = \\texttt{high}$.\n",
    "- **Parameter space**:\n",
    "  $$\\ell \\in \\mathbb{Z},\\quad h \\in \\mathbb{Z},\\quad h > \\ell.$$\n",
    "\n",
    "It is often convenient to define the **number of possible outcomes**:\n",
    "$$\n",
    "n = h - \\ell.\n",
    "$$\n",
    "Then the support contains exactly $n$ integers.\n",
    "\n",
    "**Note on SciPy's `loc`:** many SciPy distributions also accept a `loc` shift. For `randint`, `loc` shifts the support by addition:\n",
    "$$\n",
    "X \\sim \\texttt{randint}(\\ell, h, \\texttt{loc})\\quad\\Rightarrow\\quad \\text{support } \\{\\ell+\\texttt{loc}, \\ldots, h-1+\\texttt{loc}\\}.\n",
    "$$\n",
    "In this notebook we focus on the common `loc=0` case unless stated otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6be1c2",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "This is the discrete analogue of the continuous uniform distribution: **all allowed integer values are equally likely**.\n",
    "\n",
    "- **What it models**: an integer outcome with no reason to prefer one value over another *within known bounds*.\n",
    "  - Examples: a fair die roll (with an appropriate mapping), choosing a random index into an array, random day-of-week encoding, random A/B group assignment when the groups are equally sized by design.\n",
    "- **Typical real-world use cases**:\n",
    "  - **Random indexing / shuffling**: picking a random element from a list.\n",
    "  - **Simulation**: drawing random IDs, discrete time steps, or random augmentation choices.\n",
    "  - **Bounded non-informative priors** in Bayesian modeling for integer-valued parameters (e.g., a model index among $\\{1,\\dots,M\\}$).\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Continuous uniform**: if $U \\sim \\mathrm{Uniform}(0,1)$ and you set $X = \\ell + \\lfloor nU \\rfloor$, then $X$ is `randint` on $\\{\\ell,\\dots,h-1\\}$.\n",
    "- **Categorical**: `randint` is a categorical distribution over consecutive integers with equal probabilities.\n",
    "- **Bernoulli**: when $n=2$ (two consecutive integers), this is a Bernoulli distribution after a simple re-labeling.\n",
    "- **Discrete uniform on $\\{1,\\dots,N\\}$**: setting `low=1`, `high=N+1` gives the familiar “uniform from 1 to N” model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116641ec",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $X$ be uniformly distributed on the integers $\\{\\ell, \\ell+1, \\ldots, h-1\\}$ where $\\ell,h\\in\\mathbb{Z}$ and $h>\\ell$.\n",
    "Let $n = h-\\ell$.\n",
    "\n",
    "### PMF\n",
    "For integer $k$,\n",
    "$$\n",
    "\\mathbb{P}(X=k) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{n} & k \\in \\{\\ell,\\ell+1,\\ldots,h-1\\}\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "Because this is a discrete distribution, the CDF is a **step function**. For real $x$,\n",
    "$$\n",
    "F(x)=\\mathbb{P}(X\\le x)=\n",
    "\\begin{cases}\n",
    "0 & x < \\ell \\\\\n",
    "\\frac{\\lfloor x\\rfloor-\\ell+1}{n} & \\ell \\le x < h-1 \\\\\n",
    "1 & x \\ge h-1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "An equivalent “clipped” form (useful for implementation) is:\n",
    "$$\n",
    "F(x)=\\mathrm{clip}\\left(\\frac{\\lfloor x\\rfloor-\\ell+1}{n},\\ 0,\\ 1\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_int_like(name: str, value) -> int:\n",
    "    if isinstance(value, (int, np.integer)):\n",
    "        return int(value)\n",
    "    if isinstance(value, (float, np.floating)) and float(value).is_integer():\n",
    "        return int(value)\n",
    "    raise TypeError(f\"{name} must be an integer (or integer-valued float), got {value!r}\")\n",
    "\n",
    "\n",
    "def validate_low_high(low, high) -> tuple[int, int]:\n",
    "    low = _as_int_like(\"low\", low)\n",
    "    high = _as_int_like(\"high\", high)\n",
    "    if high <= low:\n",
    "        raise ValueError(f\"Require high > low, got low={low}, high={high}\")\n",
    "    return low, high\n",
    "\n",
    "\n",
    "def randint_support(low, high) -> np.ndarray:\n",
    "    low, high = validate_low_high(low, high)\n",
    "    return np.arange(low, high)\n",
    "\n",
    "\n",
    "def randint_pmf(x, low, high):\n",
    "    \"\"\"PMF of randint(low, high) evaluated at x.\"\"\"\n",
    "    low, high = validate_low_high(low, high)\n",
    "    n = high - low\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    is_int = x == np.floor(x)\n",
    "    in_support = is_int & (x >= low) & (x < high)\n",
    "\n",
    "    return np.where(in_support, 1.0 / n, 0.0).astype(float)\n",
    "\n",
    "\n",
    "def randint_cdf(x, low, high):\n",
    "    \"\"\"CDF of randint(low, high) evaluated at x (step function).\"\"\"\n",
    "    low, high = validate_low_high(low, high)\n",
    "    n = high - low\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    m = np.floor(x)\n",
    "    cdf = (m - low + 1.0) / n\n",
    "\n",
    "    return np.clip(cdf, 0.0, 1.0)\n",
    "\n",
    "\n",
    "low_demo, high_demo = 2, 7\n",
    "x_demo = np.array([1, 2, 3, 6, 7, 8], dtype=float)\n",
    "print(\"support:\", randint_support(low_demo, high_demo))\n",
    "print(\"x:\", x_demo)\n",
    "print(\"pmf:\", randint_pmf(x_demo, low_demo, high_demo))\n",
    "print(\"cdf:\", randint_cdf(x_demo, low_demo, high_demo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191108a",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let $n=h-\\ell$.\n",
    "\n",
    "### Mean and variance\n",
    "Because the distribution is uniform on a *finite* set, all moments exist.\n",
    "\n",
    "- **Mean**:\n",
    "  $$\\mathbb{E}[X] = \\frac{\\ell + (h-1)}{2} = \\frac{\\ell + h - 1}{2}.$$\n",
    "- **Variance**:\n",
    "  $$\\mathrm{Var}(X) = \\frac{n^2-1}{12}.$$\n",
    "\n",
    "### Skewness and kurtosis\n",
    "- **Skewness**: $0$ (the distribution is symmetric about its mean).\n",
    "- **Excess kurtosis** (Fisher kurtosis):\n",
    "  $$\\gamma_2 = -\\frac{6(n^2+1)}{5(n^2-1)}.$$\n",
    "\n",
    "### MGF and characteristic function\n",
    "For $t\\ne 0$, the moment generating function is a finite geometric series:\n",
    "$$\n",
    "M_X(t)=\\mathbb{E}[e^{tX}] = \\frac{1}{n}\\sum_{k=\\ell}^{h-1} e^{tk}\n",
    "= \\frac{e^{t\\ell}\\,(1-e^{tn})}{n(1-e^{t})}.\n",
    "$$\n",
    "(And $M_X(0)=1$ by continuity.)\n",
    "\n",
    "The characteristic function is the same expression with $t=i\\omega$:\n",
    "$$\n",
    "\\varphi_X(\\omega) = \\mathbb{E}[e^{i\\omega X}] = \\frac{e^{i\\omega\\ell}(1-e^{i\\omega n})}{n(1-e^{i\\omega})}\n",
    "= e^{i\\omega(\\ell + (n-1)/2)}\\,\\frac{\\sin(n\\omega/2)}{n\\sin(\\omega/2)}.\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "Because the distribution is uniform over $n$ outcomes,\n",
    "$$\n",
    "H(X) = -\\sum_k \\mathbb{P}(X=k)\\log \\mathbb{P}(X=k) = \\log n\n",
    "$$\n",
    "(in **nats**; use $\\log_2$ for bits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randint_moments(low, high):\n",
    "    low, high = validate_low_high(low, high)\n",
    "    n = high - low\n",
    "\n",
    "    mean = 0.5 * (low + high - 1)\n",
    "    var = (n**2 - 1) / 12\n",
    "    skew = 0.0 if n > 1 else np.nan\n",
    "    excess_kurtosis = (-6 * (n**2 + 1) / (5 * (n**2 - 1))) if n > 1 else np.nan\n",
    "    entropy = float(np.log(n))\n",
    "\n",
    "    return mean, var, skew, excess_kurtosis, entropy\n",
    "\n",
    "\n",
    "def randint_mgf(t, low, high):\n",
    "    \"\"\"MGF M_X(t) for randint(low, high) using a stable expm1 ratio.\"\"\"\n",
    "    low, high = validate_low_high(low, high)\n",
    "    n = high - low\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    out = np.empty_like(t, dtype=float)\n",
    "\n",
    "    mask0 = t == 0\n",
    "    out[mask0] = 1.0\n",
    "\n",
    "    tt = t[~mask0]\n",
    "    out[~mask0] = np.exp(tt * low) * (np.expm1(tt * n) / np.expm1(tt)) / n\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "low, high = 2, 7\n",
    "mean, var, skew, ex_kurt, ent = randint_moments(low, high)\n",
    "rv = stats.randint(low, high)\n",
    "\n",
    "print(\"Formulas:\")\n",
    "print(\"  mean:\", mean)\n",
    "print(\"  var :\", var)\n",
    "print(\"  skew:\", skew)\n",
    "print(\"  excess kurtosis:\", ex_kurt)\n",
    "print(\"  entropy (nats):\", ent)\n",
    "\n",
    "print()\n",
    "print(\"SciPy:\")\n",
    "print(\"  stats(mvsk):\", rv.stats(moments=\"mvsk\"))\n",
    "print(\"  entropy     :\", rv.entropy())\n",
    "\n",
    "# Numerical check of the MGF against Monte Carlo\n",
    "n_mc = 200_000\n",
    "t = 0.3\n",
    "x_mc = rv.rvs(size=n_mc, random_state=rng)\n",
    "mgf_emp = np.mean(np.exp(t * x_mc))\n",
    "mgf_theo = float(randint_mgf(t, low, high))\n",
    "\n",
    "print()\n",
    "print(\"MGF check at t=0.3:\")\n",
    "print(\"  empirical E[exp(tX)]:\", mgf_emp)\n",
    "print(\"  theoretical M_X(t)  :\", mgf_theo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33d7ee",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "`randint` has two boundary parameters:\n",
    "\n",
    "- $\\ell=\\texttt{low}$ sets the **left endpoint** (included).\n",
    "- $h=\\texttt{high}$ sets the **right endpoint** (excluded).\n",
    "- $n=h-\\ell$ is the **support size** (how many integers are possible).\n",
    "\n",
    "How parameters change the distribution:\n",
    "\n",
    "- Changing $\\ell$ with fixed $n$ **shifts** the distribution left/right but does not change its shape.\n",
    "- Increasing $n$ (moving $h$ farther from $\\ell$) spreads mass over more integers, decreasing each point probability from $1/n$.\n",
    "\n",
    "A useful mental model is:\n",
    "$$\n",
    "X = \\ell + Y,\\qquad Y \\sim \\text{Uniform on } \\{0,1,\\ldots,n-1\\}.\n",
    "$$\n",
    "Shifts affect the mean; the variance depends only on $n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b0d8a",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "Let $X$ be uniform on $\\{\\ell,\\ell+1,\\ldots,h-1\\}$ and let $n=h-\\ell$.\n",
    "\n",
    "### Expectation\n",
    "Write $X=\\ell+Y$ where $Y$ is uniform on $\\{0,1,\\ldots,n-1\\}$.\n",
    "Then\n",
    "$$\n",
    "\\mathbb{E}[X]=\\ell+\\mathbb{E}[Y].\n",
    "$$\n",
    "Compute\n",
    "$$\n",
    "\\mathbb{E}[Y]=\\frac{1}{n}\\sum_{k=0}^{n-1} k = \\frac{1}{n}\\cdot\\frac{(n-1)n}{2}=\\frac{n-1}{2},\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\mathbb{E}[X]=\\ell+\\frac{n-1}{2}=\\frac{\\ell+h-1}{2}.\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "Because variance is shift-invariant, $\\mathrm{Var}(X)=\\mathrm{Var}(Y)$.\n",
    "Use $\\mathrm{Var}(Y)=\\mathbb{E}[Y^2]-(\\mathbb{E}[Y])^2$.\n",
    "\n",
    "First,\n",
    "$$\n",
    "\\mathbb{E}[Y^2]=\\frac{1}{n}\\sum_{k=0}^{n-1} k^2\n",
    "=\\frac{1}{n}\\cdot\\frac{(n-1)n(2n-1)}{6} = \\frac{(n-1)(2n-1)}{6}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\mathrm{Var}(Y)=\\frac{(n-1)(2n-1)}{6}-\\left(\\frac{n-1}{2}\\right)^2\n",
    "=\\frac{n^2-1}{12}.\n",
    "$$\n",
    "\n",
    "### Likelihood (i.i.d. sample)\n",
    "Let $x_1,\\ldots,x_m$ be i.i.d. draws from `randint(\\ell,h)`.\n",
    "The likelihood is\n",
    "$$\n",
    "L(\\ell,h; x_{1:m})=\\prod_{i=1}^m \\mathbb{P}(X=x_i)\n",
    "=\\begin{cases}\n",
    "\\left(\\frac{1}{h-\\ell}\\right)^m & \\text{if all } x_i \\in \\{\\ell,\\ldots,h-1\\} \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So the log-likelihood (when feasible) is\n",
    "$$\n",
    "\\ell(\\ell,h) = -m\\log(h-\\ell).\n",
    "$$\n",
    "To maximize it, we want the **smallest interval** that still contains the data.\n",
    "Let $x_{\\min}=\\min_i x_i$ and $x_{\\max}=\\max_i x_i$.\n",
    "The unique MLE under the half-open support convention is:\n",
    "$$\n",
    "\\hat{\\ell}=x_{\\min},\\qquad \\hat{h}=x_{\\max}+1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8e88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randint_log_likelihood(low, high, x) -> float:\n",
    "    \"\"\"Log-likelihood for i.i.d. data x under randint(low, high).\"\"\"\n",
    "    low, high = validate_low_high(low, high)\n",
    "    x = np.asarray(x)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"x must be non-empty\")\n",
    "\n",
    "    if not np.all(x == np.floor(x)):\n",
    "        raise ValueError(\"x must contain integer-valued observations\")\n",
    "\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    if (x_min < low) or (x_max >= high):\n",
    "        return -np.inf\n",
    "\n",
    "    n = high - low\n",
    "    return -x.size * float(np.log(n))\n",
    "\n",
    "\n",
    "def randint_mle(x) -> tuple[int, int]:\n",
    "    x = np.asarray(x)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"x must be non-empty\")\n",
    "    if not np.all(x == np.floor(x)):\n",
    "        raise ValueError(\"x must contain integer-valued observations\")\n",
    "\n",
    "    low_hat = int(x.min())\n",
    "    high_hat = int(x.max()) + 1\n",
    "    return low_hat, high_hat\n",
    "\n",
    "\n",
    "# Simulate data and visualize how the likelihood prefers the tightest interval.\n",
    "low_true, high_true = 3, 11\n",
    "m = 60\n",
    "x = stats.randint.rvs(low_true, high_true, size=m, random_state=rng)\n",
    "\n",
    "low_hat, high_hat = randint_mle(x)\n",
    "print(\"true (low, high):\", (low_true, high_true))\n",
    "print(\"MLE  (low, high):\", (low_hat, high_hat))\n",
    "\n",
    "low_grid = np.arange(low_hat - 6, low_hat + 1)\n",
    "high_grid = np.arange(high_hat, high_hat + 7)\n",
    "\n",
    "ll = np.empty((low_grid.size, high_grid.size), dtype=float)\n",
    "for i, lo in enumerate(low_grid):\n",
    "    for j, hi in enumerate(high_grid):\n",
    "        ll[i, j] = randint_log_likelihood(lo, hi, x)\n",
    "\n",
    "ll_plot = ll.copy()\n",
    "ll_plot[~np.isfinite(ll_plot)] = np.nan\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=ll_plot,\n",
    "        x=high_grid,\n",
    "        y=low_grid,\n",
    "        colorbar_title=\"log L\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[high_hat],\n",
    "        y=[low_hat],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"red\", size=10),\n",
    "        name=\"MLE\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Log-likelihood over candidate (low, high) intervals\",\n",
    "    xaxis_title=\"high (exclusive)\",\n",
    "    yaxis_title=\"low\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a081abd",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "A simple sampler uses a uniform random variable and a **floor** operation.\n",
    "\n",
    "1. Draw $U \\sim \\mathrm{Uniform}(0,1)$.\n",
    "2. Return\n",
    "   $$X = \\ell + \\lfloor nU \\rfloor,\\quad n=h-\\ell.$$\n",
    "\n",
    "Why this works: $\\lfloor nU \\rfloor$ takes values in $\\{0,\\ldots,n-1\\}$, and each integer interval $[k/n,(k+1)/n)$ has probability $1/n$.\n",
    "\n",
    "This is also the logic behind `rng.integers(low, high)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d972d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_randint_numpy(low, high, size, rng: np.random.Generator | None = None):\n",
    "    \"\"\"Sample from randint(low, high) using only NumPy primitives.\"\"\"\n",
    "    low, high = validate_low_high(low, high)\n",
    "    n = high - low\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    u = rng.random(size=size)  # Uniform on [0,1)\n",
    "    return (low + np.floor(n * u)).astype(int)\n",
    "\n",
    "\n",
    "low, high = 2, 7\n",
    "n = 20_000\n",
    "x = sample_randint_numpy(low, high, size=n, rng=rng)\n",
    "\n",
    "values = np.arange(low, high)\n",
    "emp_pmf = np.array([(x == v).mean() for v in values])\n",
    "theo_pmf = np.full_like(emp_pmf, 1.0 / (high - low), dtype=float)\n",
    "\n",
    "print(\"values:\", values)\n",
    "print(\"empirical pmf:\", emp_pmf)\n",
    "print(\"theoretical pmf:\", theo_pmf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5101d12",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** for several `(low, high)` choices\n",
    "- the **CDF** (step function)\n",
    "- Monte Carlo samples: the **empirical PMF** compared to the theoretical PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PMF for several parameter choices\n",
    "params_list = [(0, 6), (2, 8), (0, 11)]  # (low, high)\n",
    "\n",
    "x_grid = np.arange(-1, 12)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "for low, high in params_list:\n",
    "    fig_pmf.add_trace(\n",
    "        go.Bar(\n",
    "            name=f\"low={low}, high={high}\",\n",
    "            x=x_grid,\n",
    "            y=randint_pmf(x_grid, low, high),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_pmf.update_layout(\n",
    "    title=\"randint PMF for different (low, high)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X = x)\",\n",
    "    barmode=\"group\",\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "\n",
    "# CDF for the same parameters\n",
    "x_cont = np.linspace(-1.0, 12.0, 700)\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "for low, high in params_list:\n",
    "    fig_cdf.add_trace(\n",
    "        go.Scatter(\n",
    "            name=f\"low={low}, high={high}\",\n",
    "            x=x_cont,\n",
    "            y=randint_cdf(x_cont, low, high),\n",
    "            mode=\"lines\",\n",
    "            line_shape=\"hv\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_cdf.update_layout(\n",
    "    title=\"randint CDF (step function)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x)\",\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "\n",
    "# Monte Carlo: empirical PMF vs theoretical PMF\n",
    "low, high = 2, 7\n",
    "n = 50_000\n",
    "x = sample_randint_numpy(low, high, size=n, rng=rng)\n",
    "\n",
    "values = np.arange(low, high)\n",
    "emp = np.array([(x == v).mean() for v in values])\n",
    "theo = np.full_like(emp, 1.0 / (high - low), dtype=float)\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(name=\"empirical\", x=values, y=emp))\n",
    "fig_mc.add_trace(go.Bar(name=\"theoretical\", x=values, y=theo))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo check (n={n:,}) for randint(low={low}, high={high})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"probability\",\n",
    "    barmode=\"group\",\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7205af4",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides this distribution as `scipy.stats.randint`.\n",
    "\n",
    "- PMF / CDF: `stats.randint.pmf`, `stats.randint.cdf`\n",
    "- Sampling: `stats.randint.rvs` or frozen `rv = stats.randint(low, high)` then `rv.rvs(...)`\n",
    "- Entropy and moments: `rv.entropy()`, `rv.stats(moments=\"mvsk\")`\n",
    "\n",
    "### Fitting\n",
    "`randint` has integer shape parameters with **unbounded domains**, so `scipy.stats.fit` requires **finite bounds**.\n",
    "For this distribution, you often have an analytic MLE:\n",
    "$$\n",
    "\\hat{\\ell}=\\min_i x_i,\\qquad \\hat{h}=\\max_i x_i+1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3682d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "randint = stats.randint\n",
    "\n",
    "low, high = 2, 7\n",
    "print(\"pmf(low..high):\", randint.pmf(np.arange(low, high), low, high))\n",
    "print(\"cdf(low-1..high):\", randint.cdf([low - 1, low, high - 1, high], low, high))\n",
    "\n",
    "rv = randint(low, high)\n",
    "samples = rv.rvs(size=10, random_state=rng)\n",
    "print(\"rvs:\", samples)\n",
    "\n",
    "# Analytic MLE from a sample\n",
    "data = randint.rvs(5, 13, size=2_000, random_state=rng)\n",
    "low_hat, high_hat = randint_mle(data)\n",
    "print()\n",
    "print(\"Analytic MLE:\")\n",
    "print(\"  low_hat =\", low_hat)\n",
    "print(\"  high_hat=\", high_hat)\n",
    "\n",
    "# scipy.stats.fit requires finite bounds (and returns float-valued parameters)\n",
    "fit_res = stats.fit(\n",
    "    randint,\n",
    "    data,\n",
    "    bounds={\n",
    "        \"low\": (low_hat - 5, low_hat),\n",
    "        \"high\": (high_hat, high_hat + 5),\n",
    "        \"loc\": (0, 0),\n",
    "    },\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"scipy.stats.fit result:\")\n",
    "print(fit_res)\n",
    "print(\"  (low, high, loc) =\", (fit_res.params.low, fit_res.params.high, fit_res.params.loc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bdf87",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing (goodness-of-fit to uniformity)\n",
    "If you believe outcomes should be equally likely across a fixed integer set, you can test whether observed counts match a uniform distribution using a **chi-square goodness-of-fit** test.\n",
    "\n",
    "### Bayesian modeling (bounded discrete prior)\n",
    "A discrete uniform distribution is a natural prior over a bounded set of integer hypotheses.\n",
    "A classic example is the *German tank problem*: serial numbers are modeled as i.i.d. uniform draws from $\\{1,\\ldots,N\\}$ with unknown $N$.\n",
    "\n",
    "### Generative modeling (uniform mixture weights / random indices)\n",
    "In generative models, you often sample an index uniformly:\n",
    "- choose a mixture component uniformly\n",
    "- choose a data augmentation option uniformly\n",
    "- choose a random class label for synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e483648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hypothesis testing: chi-square goodness-of-fit to a known uniform support ---\n",
    "low, high = 0, 10\n",
    "n = 2_000\n",
    "x = sample_randint_numpy(low, high, size=n, rng=rng)\n",
    "\n",
    "values = np.arange(low, high)\n",
    "counts = np.array([(x == v).sum() for v in values])\n",
    "expected = np.full_like(counts, n / (high - low), dtype=float)\n",
    "\n",
    "chi2, p_value = stats.chisquare(f_obs=counts, f_exp=expected)\n",
    "print(\"Chi-square test for uniformity\")\n",
    "print(\"  chi2 statistic:\", chi2)\n",
    "print(\"  p-value       :\", p_value)\n",
    "\n",
    "\n",
    "# --- Bayesian modeling: German tank problem (posterior over N) ---\n",
    "# Model: serials ~ Uniform{1,2,...,N} i.i.d. (inclusive upper bound)\n",
    "# Map to SciPy's randint with low=1, high=N+1.\n",
    "N_true = 200\n",
    "n_obs = 15\n",
    "serials = stats.randint.rvs(1, N_true + 1, size=n_obs, random_state=rng)\n",
    "max_serial = int(serials.max())\n",
    "\n",
    "N_max = 600  # prior upper limit\n",
    "N_grid = np.arange(max_serial, N_max + 1)\n",
    "\n",
    "# Uniform prior over N_grid: p(N) = constant.\n",
    "# Likelihood: p(data | N) = 1/N^n_obs for N >= max_serial, else 0.\n",
    "log_post_unnorm = -n_obs * np.log(N_grid)\n",
    "log_post = log_post_unnorm - logsumexp(log_post_unnorm)\n",
    "post = np.exp(log_post)\n",
    "\n",
    "post_mean = float(np.sum(N_grid * post))\n",
    "post_map = int(N_grid[np.argmax(post)])\n",
    "\n",
    "cdf = np.cumsum(post)\n",
    "ci_low = int(N_grid[np.searchsorted(cdf, 0.025)])\n",
    "ci_high = int(N_grid[np.searchsorted(cdf, 0.975)])\n",
    "\n",
    "print()\n",
    "print(\"German tank example\")\n",
    "print(\"  true N        :\", N_true)\n",
    "print(\"  max observed  :\", max_serial)\n",
    "print(\"  posterior mean:\", round(post_mean, 2))\n",
    "print(\"  posterior MAP :\", post_map)\n",
    "print(\"  95% credible interval:\", (ci_low, ci_high))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=N_grid, y=post, mode=\"lines\", name=\"posterior\"))\n",
    "fig.add_vline(x=N_true, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"N_true\")\n",
    "fig.add_vline(x=max_serial, line_dash=\"dash\", line_color=\"red\", annotation_text=\"max serial\")\n",
    "fig.update_layout(\n",
    "    title=\"Posterior over N in the German tank problem (uniform prior)\",\n",
    "    xaxis_title=\"N\",\n",
    "    yaxis_title=\"posterior probability\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# --- Generative modeling: uniform mixture component index ---\n",
    "K = 3\n",
    "means = np.array([-2.0, 0.0, 3.0])\n",
    "sigma = 0.6\n",
    "n = 5_000\n",
    "\n",
    "z = sample_randint_numpy(0, K, size=n, rng=rng)  # component index in {0,1,...,K-1}\n",
    "y = rng.normal(loc=means[z], scale=sigma)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=y, nbinsx=60, name=\"samples\"))\n",
    "fig.update_layout(\n",
    "    title=\"Samples from a 3-component Gaussian mixture with uniform weights\",\n",
    "    xaxis_title=\"y\",\n",
    "    yaxis_title=\"count\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c3415",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Inclusive vs exclusive upper bound**:\n",
    "  - SciPy/NumPy use a **half-open** interval: `low` is included and `high` is excluded.\n",
    "  - Python's `random.randint(a, b)` is **inclusive** on both ends.\n",
    "- **Invalid parameters**: you must have `high > low`. If `high == low + 1`, the distribution is degenerate (always returns `low`).\n",
    "- **Fitting requires bounds**: `scipy.stats.fit(stats.randint, data)` fails without finite bounds because `low` and `high` are unbounded a priori.\n",
    "- **Non-integer parameters/observations**: SciPy returns `nan` if `low`/`high` are not integer-valued.\n",
    "- **MGF overflow**: $M_X(t)$ can overflow for large positive $t$; use it for modest $t$ and prefer characteristic functions for numerical stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the 'fit requires bounds' pitfall\n",
    "x = stats.randint.rvs(2, 7, size=500, random_state=rng)\n",
    "\n",
    "try:\n",
    "    stats.fit(stats.randint, x)\n",
    "except Exception as e:\n",
    "    print(\"stats.fit(stats.randint, x) failed as expected:\")\n",
    "    print(\" \", type(e).__name__ + \":\", e)\n",
    "\n",
    "# Provide finite bounds to make it work\n",
    "low_hat, high_hat = randint_mle(x)\n",
    "fit_res = stats.fit(\n",
    "    stats.randint,\n",
    "    x,\n",
    "    bounds={\"low\": (low_hat - 3, low_hat), \"high\": (high_hat, high_hat + 3), \"loc\": (0, 0)},\n",
    ")\n",
    "print()\n",
    "print(\"fit with bounds:\")\n",
    "print(fit_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c13b4",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `randint(low, high)` is a **discrete uniform** distribution on $\\{\\texttt{low},\\ldots,\\texttt{high}-1\\}$.\n",
    "- PMF: $\\mathbb{P}(X=k)=1/(h-\\ell)$ for integers $k$ in the support.\n",
    "- Mean: $(\\ell+h-1)/2$; Variance: $((h-\\ell)^2-1)/12$; Entropy: $\\log(h-\\ell)$.\n",
    "- A simple NumPy sampler is $X=\\ell+\\lfloor (h-\\ell)U\\rfloor$ with $U\\sim\\mathrm{Uniform}(0,1)$.\n",
    "- The likelihood for i.i.d. data is proportional to $(h-\\ell)^{-m}$ when the interval contains all observations, so the MLE is $(\\min x_i,\\ \\max x_i+1)$.\n",
    "- In SciPy, `scipy.stats.fit` needs **finite bounds** for `randint` because parameters are unbounded.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}