{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1c2e3f",
   "metadata": {},
   "source": [
    "# `skellam` (Skellam distribution)\n",
    "\n",
    "The **Skellam** distribution models the difference of two independent Poisson counts:\n",
    "\\(X = Y_1 - Y_2\\), where \\(Y_1 \\sim \\text{Pois}(\\mu_1)\\) and \\(Y_2 \\sim \\text{Pois}(\\mu_2)\\).\n",
    "\n",
    "It is a natural model for **net counts**: wins minus losses, arrivals minus departures, goals for minus goals against, etc.\n",
    "\n",
    "This notebook uses the same parameterization as `scipy.stats.skellam`:\n",
    "- `mu1` = mean/rate of the first Poisson process (\\(\\mu_1 \\ge 0\\))\n",
    "- `mu2` = mean/rate of the second Poisson process (\\(\\mu_2 \\ge 0\\))\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- recognize problems where a Skellam model makes sense (and when it doesn’t)\n",
    "- write down the PMF/CDF and key properties\n",
    "- derive the mean, variance, and log-likelihood\n",
    "- sample from the distribution using **NumPy only**\n",
    "- visualize PMF/CDF and validate formulas with Monte Carlo simulation\n",
    "- use `scipy.stats.skellam` for computation and fitting\n",
    "\n",
    "## Table of contents\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy import optimize, special, stats\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e4a5b",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name**: `skellam` (Skellam distribution)  \n",
    "**Type**: Discrete  \n",
    "**Support**: \\(k \\in \\mathbb{Z}\\)  \n",
    "**Parameters**: \\(\\mu_1, \\mu_2\\)  \n",
    "**Parameter space**: \\(\\mu_1 \\ge 0,\\ \\mu_2 \\ge 0\\) (not both zero for a non-degenerate distribution)\n",
    "\n",
    "In SciPy, `skellam` also accepts a `loc` shift. In this notebook we focus on the standard, unshifted case (`loc = 0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f5a6c",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "Think of two independent counting processes over the same time window:\n",
    "- \\(Y_1\\): “positive” events (arrivals, goals for, wins, upward jumps)\n",
    "- \\(Y_2\\): “negative” events (departures, goals against, losses, downward jumps)\n",
    "\n",
    "The Skellam variable \\(X = Y_1 - Y_2\\) is the **net balance**.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Sports score differential**: if each team’s goals are modeled as independent Poisson counts, the goal difference is Skellam.\n",
    "- **A/B event streams**: net count of two independent event types in logs (e.g., adds − deletes).\n",
    "- **Queueing / inventory**: arrivals minus services (over short windows where rates are approximately constant).\n",
    "- **Change scores**: differences of counts in two conditions or two periods (when each is Poisson-like).\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Poisson**: if \\(\\mu_2 = 0\\), then \\(X \\sim \\text{Pois}(\\mu_1)\\); if \\(\\mu_1 = 0\\), then \\(-X \\sim \\text{Pois}(\\mu_2)\\).\n",
    "- **Normal approximation**: for large \\(\\mu_1 + \\mu_2\\), \\(X\\) is approximately \\(\\mathcal{N}(\\mu_1-\\mu_2,\\ \\mu_1+\\mu_2)\\).\n",
    "- **Poisson–Binomial connection**: if \\(N = Y_1 + Y_2\\), then \\(N \\sim \\text{Pois}(\\mu_1+\\mu_2)\\) and\n",
    "  \\(Y_1 \\mid N \\sim \\text{Bin}(N,\\ \\mu_1/(\\mu_1+\\mu_2))\\). Since \\(X = 2Y_1 - N\\), this gives an alternative view of Skellam.\n",
    "- **Additivity**: differences of independent Poissons add: if \\(X_i = Y_{1,i}-Y_{2,i}\\) with independent Poisson components, then \\(\\sum_i X_i\\) is Skellam with summed rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a6c7d",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(Y_1 \\sim \\text{Pois}(\\mu_1)\\) and \\(Y_2 \\sim \\text{Pois}(\\mu_2)\\) be independent. Define\n",
    "\\[\n",
    "X = Y_1 - Y_2.\n",
    "\\]\n",
    "\n",
    "### PMF\n",
    "For \\(k \\in \\mathbb{Z}\\), the probability mass function is\n",
    "\\[\n",
    "\\mathbb{P}(X = k)\n",
    "= e^{-(\\mu_1+\\mu_2)}\\left(\\frac{\\mu_1}{\\mu_2}\\right)^{k/2}\n",
    "I_{|k|}\\!\\left(2\\sqrt{\\mu_1\\mu_2}\\right),\n",
    "\\]\n",
    "where \\(I_{\\nu}(\\cdot)\\) is the **modified Bessel function of the first kind**.\n",
    "\n",
    "Boundary cases are intuitive:\n",
    "- if \\(\\mu_2=0\\), then \\(X\\) is Poisson on \\(\\{0,1,2,\\dots\\}\\)\n",
    "- if \\(\\mu_1=0\\), then \\(X\\) is the negative of a Poisson on \\(\\{0,-1,-2,\\dots\\}\\)\n",
    "- if \\(\\mu_1=\\mu_2=0\\), then \\(X=0\\) almost surely\n",
    "\n",
    "### CDF\n",
    "The cumulative distribution function is\n",
    "\\[\n",
    "F(k) = \\mathbb{P}(X \\le k) = \\sum_{j=-\\infty}^{k} \\mathbb{P}(X=j).\n",
    "\\]\n",
    "There is no simple closed form in general; in practice it is computed numerically (e.g., by SciPy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b7d8e",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let \\(X \\sim \\text{Skellam}(\\mu_1, \\mu_2)\\).\n",
    "\n",
    "### Mean and variance\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\mu_1 - \\mu_2,\\qquad \\mathrm{Var}(X) = \\mu_1 + \\mu_2.\n",
    "\\]\n",
    "\n",
    "### Skewness and kurtosis\n",
    "Let \\(\\sigma^2 = \\mu_1 + \\mu_2\\) (assuming \\(\\sigma^2>0\\)). Then\n",
    "\\[\n",
    "\\text{skew}(X) = \\frac{\\mu_1 - \\mu_2}{(\\mu_1+\\mu_2)^{3/2}}.\n",
    "\\]\n",
    "The **excess** kurtosis is\n",
    "\\[\n",
    "\\text{excess kurt}(X) = \\frac{1}{\\mu_1+\\mu_2}.\n",
    "\\]\n",
    "(so the full kurtosis is \\(3 + 1/(\\mu_1+\\mu_2)\\)).\n",
    "\n",
    "### MGF, characteristic function, and cumulants\n",
    "The moment generating function exists for all real \\(t\\) and is\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = \\exp\\big(\\mu_1(e^t-1) + \\mu_2(e^{-t}-1)\\big).\n",
    "\\]\n",
    "The characteristic function is\n",
    "\\[\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\exp\\big(\\mu_1(e^{it}-1) + \\mu_2(e^{-it}-1)\\big).\n",
    "\\]\n",
    "The cumulant generating function \\(K(t)=\\log M_X(t)\\) is\n",
    "\\[\n",
    "K(t) = \\mu_1(e^t-1) + \\mu_2(e^{-t}-1).\n",
    "\\]\n",
    "Differentiating shows a neat pattern: odd-order cumulants equal \\(\\mu_1-\\mu_2\\), even-order cumulants equal \\(\\mu_1+\\mu_2\\).\n",
    "\n",
    "### Entropy\n",
    "The (Shannon) entropy is\n",
    "\\[\n",
    "H(X) = -\\sum_{k\\in\\mathbb{Z}} \\mathbb{P}(X=k)\\,\\log \\mathbb{P}(X=k).\n",
    "\\]\n",
    "There is no simple closed form in general, but it can be approximated accurately by truncating the tails.\n",
    "\n",
    "### Other useful properties\n",
    "- **Symmetry**: if \\(\\mu_1=\\mu_2\\), the distribution is symmetric around 0.\n",
    "- **Additivity**: if \\(X_1\\sim\\text{Skellam}(\\mu_{1a},\\mu_{2a})\\) and \\(X_2\\sim\\text{Skellam}(\\mu_{1b},\\mu_{2b})\\) independent,\n",
    "  then \\(X_1+X_2\\sim\\text{Skellam}(\\mu_{1a}+\\mu_{1b},\\ \\mu_{2a}+\\mu_{2b})\\).\n",
    "- **Normal approximation**: when \\(\\mu_1+\\mu_2\\) is large, a Normal approximation is often excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_mu(mu, name):\n",
    "    mu_float = float(mu)\n",
    "    if mu_float < 0.0:\n",
    "        raise ValueError(f\"{name} must be >= 0\")\n",
    "    return mu_float\n",
    "\n",
    "\n",
    "def _validate_mu1_mu2(mu1, mu2):\n",
    "    mu1 = _validate_mu(mu1, \"mu1\")\n",
    "    mu2 = _validate_mu(mu2, \"mu2\")\n",
    "    return mu1, mu2\n",
    "\n",
    "\n",
    "def skellam_logpmf(k, mu1, mu2):\n",
    "    \"\"\"Log PMF for Skellam(mu1, mu2) for integer k. Returns -inf outside support.\"\"\"\n",
    "    mu1, mu2 = _validate_mu1_mu2(mu1, mu2)\n",
    "    k_arr = np.asarray(k)\n",
    "\n",
    "    out = np.full(k_arr.shape, -np.inf, dtype=float)\n",
    "\n",
    "    k_int = k_arr.astype(int)\n",
    "    is_int = k_int == k_arr\n",
    "    if not np.any(is_int):\n",
    "        return out\n",
    "\n",
    "    # Degenerate and boundary cases\n",
    "    if mu1 == 0.0 and mu2 == 0.0:\n",
    "        out[is_int & (k_int == 0)] = 0.0\n",
    "        return out\n",
    "\n",
    "    if mu1 == 0.0:\n",
    "        mask = is_int & (k_int <= 0)\n",
    "        out[mask] = stats.poisson.logpmf(-k_int[mask], mu2)\n",
    "        return out\n",
    "\n",
    "    if mu2 == 0.0:\n",
    "        mask = is_int & (k_int >= 0)\n",
    "        out[mask] = stats.poisson.logpmf(k_int[mask], mu1)\n",
    "        return out\n",
    "\n",
    "    kv = k_int[is_int]\n",
    "    v = np.abs(kv)\n",
    "    z = 2.0 * math.sqrt(mu1 * mu2)\n",
    "\n",
    "    # Use the exponentially scaled Bessel function for numerical stability:\n",
    "    # ive(v, z) = exp(-|z|) * I_v(z)  => log I_v(z) = log ive(v, z) + |z|\n",
    "    log_iv = np.log(special.ive(v, z)) + abs(z)\n",
    "\n",
    "    log_ratio = math.log(mu1) - math.log(mu2)\n",
    "    out[is_int] = -(mu1 + mu2) + 0.5 * kv * log_ratio + log_iv\n",
    "    return out\n",
    "\n",
    "\n",
    "def skellam_pmf(k, mu1, mu2):\n",
    "    return np.exp(skellam_logpmf(k, mu1, mu2))\n",
    "\n",
    "\n",
    "def skellam_moments(mu1, mu2):\n",
    "    mu1, mu2 = _validate_mu1_mu2(mu1, mu2)\n",
    "    mean = mu1 - mu2\n",
    "    var = mu1 + mu2\n",
    "\n",
    "    if var == 0.0:\n",
    "        return {\n",
    "            \"mean\": 0.0,\n",
    "            \"var\": 0.0,\n",
    "            \"skew\": float(\"nan\"),\n",
    "            \"excess_kurt\": float(\"nan\"),\n",
    "            \"kurt\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    skew = (mu1 - mu2) / (var**1.5)\n",
    "    excess_kurt = 1.0 / var\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"var\": var,\n",
    "        \"skew\": skew,\n",
    "        \"excess_kurt\": excess_kurt,\n",
    "        \"kurt\": 3.0 + excess_kurt,\n",
    "    }\n",
    "\n",
    "\n",
    "def skellam_mgf(t, mu1, mu2):\n",
    "    mu1, mu2 = _validate_mu1_mu2(mu1, mu2)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    return np.exp(mu1 * (np.exp(t) - 1.0) + mu2 * (np.exp(-t) - 1.0))\n",
    "\n",
    "\n",
    "def skellam_cf(t, mu1, mu2):\n",
    "    mu1, mu2 = _validate_mu1_mu2(mu1, mu2)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    return np.exp(mu1 * (np.exp(1j * t) - 1.0) + mu2 * (np.exp(-1j * t) - 1.0))\n",
    "\n",
    "\n",
    "def sample_skewness(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    m = x.mean()\n",
    "    c = x - m\n",
    "    m2 = np.mean(c**2)\n",
    "    if m2 == 0.0:\n",
    "        return float(\"nan\")\n",
    "    m3 = np.mean(c**3)\n",
    "    return float(m3 / (m2**1.5))\n",
    "\n",
    "\n",
    "def sample_excess_kurtosis(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    m = x.mean()\n",
    "    c = x - m\n",
    "    m2 = np.mean(c**2)\n",
    "    if m2 == 0.0:\n",
    "        return float(\"nan\")\n",
    "    m4 = np.mean(c**4)\n",
    "    return float(m4 / (m2**2) - 3.0)\n",
    "\n",
    "\n",
    "def skellam_entropy(mu1, mu2, *, tail_prob=1e-12, base=math.e):\n",
    "    \"\"\"Approximate entropy by truncating tails with total probability `tail_prob`.\"\"\"\n",
    "    mu1, mu2 = _validate_mu1_mu2(mu1, mu2)\n",
    "    if mu1 == 0.0 and mu2 == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    tail_prob = float(tail_prob)\n",
    "    if not (0.0 < tail_prob < 1.0):\n",
    "        raise ValueError(\"tail_prob must be in (0, 1)\")\n",
    "\n",
    "    lo = stats.skellam.ppf(tail_prob / 2.0, mu1, mu2)\n",
    "    hi = stats.skellam.ppf(1.0 - tail_prob / 2.0, mu1, mu2)\n",
    "\n",
    "    if not (np.isfinite(lo) and np.isfinite(hi)):\n",
    "        mean = mu1 - mu2\n",
    "        sd = math.sqrt(mu1 + mu2)\n",
    "        lo = math.floor(mean - 20.0 * sd)\n",
    "        hi = math.ceil(mean + 20.0 * sd)\n",
    "\n",
    "    lo = int(math.floor(lo))\n",
    "    hi = int(math.ceil(hi))\n",
    "\n",
    "    ks = np.arange(lo, hi + 1)\n",
    "    logp = stats.skellam.logpmf(ks, mu1, mu2)\n",
    "    p = np.exp(logp)\n",
    "\n",
    "    h_nats = -np.sum(p * logp)\n",
    "    return float(h_nats / math.log(base))\n",
    "\n",
    "\n",
    "def skellam_mom_estimates(x):\n",
    "    \"\"\"Method-of-moments estimates from sample mean and variance.\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    m = float(x.mean())\n",
    "    v = float(x.var(ddof=0))\n",
    "\n",
    "    mu1_hat = 0.5 * (v + m)\n",
    "    mu2_hat = 0.5 * (v - m)\n",
    "\n",
    "    return max(mu1_hat, 0.0), max(mu2_hat, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = 8.0, 5.0\n",
    "mom = skellam_moments(mu1, mu2)\n",
    "\n",
    "samples = rng.poisson(mu1, size=300_000) - rng.poisson(mu2, size=300_000)\n",
    "\n",
    "{\n",
    "    \"formula_mean\": mom[\"mean\"],\n",
    "    \"mc_mean\": float(samples.mean()),\n",
    "    \"formula_var\": mom[\"var\"],\n",
    "    \"mc_var\": float(samples.var(ddof=0)),\n",
    "    \"formula_skew\": mom[\"skew\"],\n",
    "    \"mc_skew\": sample_skewness(samples),\n",
    "    \"formula_excess_kurt\": mom[\"excess_kurt\"],\n",
    "    \"mc_excess_kurt\": sample_excess_kurtosis(samples),\n",
    "    \"entropy_bits (approx)\": skellam_entropy(mu1, mu2, base=2),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9eab1c",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "You can think of \\(\\mu_1\\) as the expected number of **positive** events and \\(\\mu_2\\) as the expected number of **negative** events.\n",
    "\n",
    "- **Location / drift**: \\(\\mathbb{E}[X] = \\mu_1 - \\mu_2\\). Increasing \\(\\mu_1\\) shifts mass to the right; increasing \\(\\mu_2\\) shifts mass to the left.\n",
    "- **Dispersion**: \\(\\mathrm{Var}(X) = \\mu_1 + \\mu_2\\). Increasing both parameters makes the distribution wider.\n",
    "- **Symmetry**: if \\(\\mu_1=\\mu_2\\), the distribution is symmetric around 0.\n",
    "- **Asymptotics**: for large \\(\\mu_1+\\mu_2\\), the PMF looks increasingly bell-shaped (Normal approximation).\n",
    "\n",
    "The plot below compares several parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sets = [\n",
    "    (5.0, 5.0),\n",
    "    (10.0, 5.0),\n",
    "    (5.0, 10.0),\n",
    "    (20.0, 20.0),\n",
    "]\n",
    "\n",
    "alpha = 0.001\n",
    "lo = int(min(stats.skellam.ppf(alpha, m1, m2) for m1, m2 in param_sets))\n",
    "hi = int(max(stats.skellam.ppf(1 - alpha, m1, m2) for m1, m2 in param_sets))\n",
    "ks = np.arange(lo, hi + 1)\n",
    "\n",
    "fig = go.Figure()\n",
    "for m1, m2 in param_sets:\n",
    "    pmf = stats.skellam.pmf(ks, m1, m2)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=ks,\n",
    "            y=pmf,\n",
    "            mode=\"lines+markers\",\n",
    "            name=f\"mu1={m1:g}, mu2={m2:g}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Skellam PMF for different (mu1, mu2)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(X=k)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c2d3e",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### A) Expectation\n",
    "Using linearity of expectation and the fact that a Poisson(\\(\\mu\\)) variable has mean \\(\\mu\\):\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\mathbb{E}[Y_1 - Y_2] = \\mathbb{E}[Y_1] - \\mathbb{E}[Y_2] = \\mu_1 - \\mu_2.\n",
    "\\]\n",
    "\n",
    "### B) Variance\n",
    "Independence gives \\(\\mathrm{Cov}(Y_1, Y_2)=0\\). Poisson variables have variance equal to their mean, so\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\mathrm{Var}(Y_1 - Y_2) = \\mathrm{Var}(Y_1) + \\mathrm{Var}(Y_2) = \\mu_1 + \\mu_2.\n",
    "\\]\n",
    "\n",
    "(You can also get both results by differentiating the cumulant generating function \\(K(t)=\\mu_1(e^t-1)+\\mu_2(e^{-t}-1)\\) at \\(t=0\\).)\n",
    "\n",
    "### C) Likelihood\n",
    "For observations \\(x_1,\\dots,x_n\\in\\mathbb{Z}\\), the likelihood is\n",
    "\\[\n",
    "L(\\mu_1,\\mu_2) = \\prod_{i=1}^{n} \\mathbb{P}(X=x_i\\mid \\mu_1,\\mu_2),\n",
    "\\]\n",
    "and the log-likelihood (for \\(\\mu_1,\\mu_2>0\\)) is\n",
    "\\[\n",
    "\\ell(\\mu_1,\\mu_2)\n",
    "= \\sum_{i=1}^{n}\\Big[-(\\mu_1+\\mu_2) + \\tfrac{x_i}{2}\\log(\\mu_1/\\mu_2)\n",
    "+ \\log I_{|x_i|}(2\\sqrt{\\mu_1\\mu_2})\\Big].\n",
    "\\]\n",
    "There is no general closed-form MLE; we typically use numerical optimization, often initialized with method-of-moments estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1_true, mu2_true = 7.0, 4.0\n",
    "data = rng.poisson(mu1_true, size=3_000) - rng.poisson(mu2_true, size=3_000)\n",
    "\n",
    "mu1_mom, mu2_mom = skellam_mom_estimates(data)\n",
    "\n",
    "\n",
    "def nll(log_params):\n",
    "    mu1 = math.exp(log_params[0])\n",
    "    mu2 = math.exp(log_params[1])\n",
    "    return -stats.skellam.logpmf(data, mu1, mu2).sum()\n",
    "\n",
    "\n",
    "x0 = np.log([max(mu1_mom, 1e-6), max(mu2_mom, 1e-6)])\n",
    "res = optimize.minimize(nll, x0=x0, method=\"L-BFGS-B\")\n",
    "\n",
    "mu1_mle, mu2_mle = np.exp(res.x)\n",
    "\n",
    "{\n",
    "    \"mu1_true\": mu1_true,\n",
    "    \"mu2_true\": mu2_true,\n",
    "    \"mu1_mom\": mu1_mom,\n",
    "    \"mu2_mom\": mu2_mom,\n",
    "    \"mu1_mle\": float(mu1_mle),\n",
    "    \"mu2_mle\": float(mu2_mle),\n",
    "    \"success\": bool(res.success),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e4f5a",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "The defining construction immediately gives a simple sampler:\n",
    "1. draw \\(Y_1 \\sim \\text{Pois}(\\mu_1)\\)\n",
    "2. draw \\(Y_2 \\sim \\text{Pois}(\\mu_2)\\) independently\n",
    "3. return \\(X = Y_1 - Y_2\\)\n",
    "\n",
    "This is **NumPy-only** because NumPy’s random generator includes a Poisson sampler (`rng.poisson`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_skellam_numpy(mu1, mu2, size, *, rng):\n",
    "    mu1, mu2 = _validate_mu1_mu2(mu1, mu2)\n",
    "    y1 = rng.poisson(lam=mu1, size=size)\n",
    "    y2 = rng.poisson(lam=mu2, size=size)\n",
    "    return y1 - y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = 3.5, 2.0\n",
    "x = sample_skellam_numpy(mu1, mu2, size=200_000, rng=rng)\n",
    "\n",
    "{\n",
    "    \"sample_mean\": float(x.mean()),\n",
    "    \"theory_mean\": mu1 - mu2,\n",
    "    \"sample_var\": float(x.var(ddof=0)),\n",
    "    \"theory_var\": mu1 + mu2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b7c8d",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** over a high-probability range (a finite window covering most mass)\n",
    "- the **CDF** as a step function\n",
    "- Monte Carlo samples vs the exact PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = 9.0, 6.0\n",
    "\n",
    "alpha = 0.001\n",
    "k_min = int(stats.skellam.ppf(alpha, mu1, mu2))\n",
    "k_max = int(stats.skellam.ppf(1 - alpha, mu1, mu2))\n",
    "ks = np.arange(k_min, k_max + 1)\n",
    "\n",
    "pmf = stats.skellam.pmf(ks, mu1, mu2)\n",
    "cdf = stats.skellam.cdf(ks, mu1, mu2)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=ks, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(\n",
    "    title=f\"Skellam PMF (mu1={mu1}, mu2={mu2})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(X=k)\",\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=ks, y=cdf, mode=\"lines\", line_shape=\"hv\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(\n",
    "    title=f\"Skellam CDF (mu1={mu1}, mu2={mu2})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(X≤k)\",\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "mc = sample_skellam_numpy(mu1, mu2, size=200_000, rng=rng)\n",
    "mask = (mc >= k_min) & (mc <= k_max)\n",
    "counts = np.bincount(mc[mask] - k_min, minlength=len(ks))\n",
    "pmf_hat = counts / mc.size\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=ks, y=pmf_hat, name=\"Monte Carlo\", opacity=0.6))\n",
    "fig_mc.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers+lines\", name=\"Exact PMF\"))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo vs exact PMF (mu1={mu1}, mu2={mu2})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_mc.show()\n",
    "\n",
    "{\n",
    "    \"prob_mass_in_plot_window\": float(pmf.sum()),\n",
    "    \"mc_mass_in_plot_window\": float(((mc >= k_min) & (mc <= k_max)).mean()),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d9eaf",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides a numerically robust implementation via `scipy.stats.skellam`.\n",
    "\n",
    "- Use `pmf`, `cdf`, `sf`, `rvs`, `logpmf`, `logcdf`, …\n",
    "- Many `rv_discrete` objects (including `skellam`) do **not** expose a `.fit()` method.\n",
    "  In SciPy 1.15+, you can use the generic `scipy.stats.fit` for MLE with bounds.\n",
    "- `skellam` also has a `loc` parameter (integer shift). In most modeling contexts you want `loc=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39eaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "skellam = stats.skellam\n",
    "\n",
    "mu1, mu2 = 5.0, 3.0\n",
    "alpha = 0.001\n",
    "k_min = int(skellam.ppf(alpha, mu1, mu2))\n",
    "k_max = int(skellam.ppf(1 - alpha, mu1, mu2))\n",
    "ks = np.arange(k_min, k_max + 1)\n",
    "\n",
    "pmf = skellam.pmf(ks, mu1, mu2)\n",
    "cdf = skellam.cdf(ks, mu1, mu2)\n",
    "samples = skellam.rvs(mu1, mu2, size=10_000, random_state=rng)\n",
    "\n",
    "mom = skellam_moments(mu1, mu2)\n",
    "\n",
    "# Fit with scipy.stats.fit (MLE)\n",
    "data = skellam.rvs(7.0, 4.0, size=2_000, random_state=rng)\n",
    "fit_res = stats.fit(\n",
    "    skellam,\n",
    "    data,\n",
    "    bounds={\"mu1\": (0.0, 50.0), \"mu2\": (0.0, 50.0), \"loc\": (0.0, 0.0)},\n",
    ")\n",
    "\n",
    "{\n",
    "    \"pmf_sum_on_grid\": float(pmf.sum()),\n",
    "    \"cdf_last_on_grid\": float(cdf[-1]),\n",
    "    \"sample_mean\": float(samples.mean()),\n",
    "    \"theory_mean\": mom[\"mean\"],\n",
    "    \"sample_var\": float(samples.var(ddof=0)),\n",
    "    \"theory_var\": mom[\"var\"],\n",
    "    \"fit_success\": bool(fit_res.success),\n",
    "    \"fit_mu1\": float(fit_res.params.mu1),\n",
    "    \"fit_mu2\": float(fit_res.params.mu2),\n",
    "    \"fit_loc\": float(fit_res.params.loc),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af1021",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (difference of Poisson means)\n",
    "If you have a model for two independent Poisson counts, Skellam gives the exact distribution of the difference.\n",
    "This can be used for exact p-values on a score differential or “net count”.\n",
    "\n",
    "### B) Bayesian modeling (two Poisson rates)\n",
    "In many applications you model \\(Y_1\\) and \\(Y_2\\) directly with Poisson likelihoods and put Gamma priors on \\(\\mu_1, \\mu_2\\).\n",
    "The induced predictive distribution for \\(X=Y_1-Y_2\\) is then a (mixture of) Skellam.\n",
    "\n",
    "### C) Generative modeling (integer-valued innovations)\n",
    "Skellam is a convenient choice when you need **integer noise that can be negative**, e.g. in state-space models or random walks with drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5102132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: test whether there is an advantage (H0: mu1 = mu2)\n",
    "mu0 = 1.4\n",
    "d_obs = 3\n",
    "\n",
    "# Two-sided p-value under a symmetric Skellam(mu0, mu0)\n",
    "p_upper = stats.skellam.sf(d_obs - 1, mu0, mu0)  # P(X >= d_obs)\n",
    "p_lower = stats.skellam.cdf(-d_obs, mu0, mu0)  # P(X <= -d_obs)\n",
    "p_two = p_upper + p_lower\n",
    "\n",
    "# Conditional exact test given N = Y1 + Y2 (does not depend on mu0 under H0)\n",
    "y1, y2 = 4, 1\n",
    "n = y1 + y2\n",
    "binom_res = stats.binomtest(y1, n=n, p=0.5, alternative=\"two-sided\")\n",
    "\n",
    "{\n",
    "    \"skellam_two_sided_p\": float(p_two),\n",
    "    \"conditional_binomtest_p\": float(binom_res.pvalue),\n",
    "    \"note\": \"Under H0 mu1=mu2, conditioning on N=Y1+Y2 gives a Binomial test.\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6213243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Bayesian example (Gamma–Poisson) + posterior predictive for the score difference\n",
    "y1_obs = np.array([2, 1, 0, 3, 1, 2])\n",
    "y2_obs = np.array([1, 0, 1, 1, 2, 1])\n",
    "n_games = len(y1_obs)\n",
    "\n",
    "# Prior: mu ~ Gamma(alpha, beta) with beta = rate (so scale = 1/beta)\n",
    "alpha1, beta1 = 1.0, 1.0\n",
    "alpha2, beta2 = 1.0, 1.0\n",
    "\n",
    "post_alpha1 = alpha1 + y1_obs.sum()\n",
    "post_beta1 = beta1 + n_games\n",
    "post_alpha2 = alpha2 + y2_obs.sum()\n",
    "post_beta2 = beta2 + n_games\n",
    "\n",
    "m = 50_000\n",
    "mu1_s = rng.gamma(shape=post_alpha1, scale=1.0 / post_beta1, size=m)\n",
    "mu2_s = rng.gamma(shape=post_alpha2, scale=1.0 / post_beta2, size=m)\n",
    "\n",
    "# Posterior predictive for next-game difference (mixture of Skellam)\n",
    "y1_pred = rng.poisson(mu1_s)\n",
    "y2_pred = rng.poisson(mu2_s)\n",
    "d_pred = y1_pred - y2_pred\n",
    "\n",
    "k_lo = int(np.percentile(d_pred, 0.5))\n",
    "k_hi = int(np.percentile(d_pred, 99.5))\n",
    "ks = np.arange(k_lo, k_hi + 1)\n",
    "mask = (d_pred >= k_lo) & (d_pred <= k_hi)\n",
    "counts = np.bincount(d_pred[mask] - k_lo, minlength=len(ks))\n",
    "pmf_hat = counts / d_pred.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ks, y=pmf_hat, name=\"posterior predictive\"))\n",
    "fig.update_layout(\n",
    "    title=\"Posterior predictive distribution of difference (Gamma–Poisson model)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"posterior_mean_mu1\": float(mu1_s.mean()),\n",
    "    \"posterior_mean_mu2\": float(mu2_s.mean()),\n",
    "    \"predictive_mean_diff\": float(d_pred.mean()),\n",
    "    \"predictive_var_diff\": float(d_pred.var(ddof=0)),\n",
    "    \"p(diff>0)\": float((d_pred > 0).mean()),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7324354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling: Skellam innovations in an integer-valued random walk\n",
    "T = 200\n",
    "mu_plus, mu_minus = 2.0, 1.5\n",
    "\n",
    "increments = sample_skellam_numpy(mu_plus, mu_minus, size=T, rng=rng)\n",
    "path = np.cumsum(increments)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=path, mode=\"lines\", name=\"x_t\"))\n",
    "fig.update_layout(\n",
    "    title=\"Random walk with Skellam innovations\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"x_t\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"drift_theory\": mu_plus - mu_minus,\n",
    "    \"drift_empirical\": float(increments.mean()),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8435465",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: \\(\\mu_1, \\mu_2\\) must be \\(\\ge 0\\). The boundary case \\(\\mu_1=\\mu_2=0\\) is degenerate (all mass at 0).\n",
    "- **Model misspecification**: Skellam assumes *independent* Poisson counts. Correlation or over/under-dispersion can invalidate p-values and CIs.\n",
    "  If overdispersion is present, consider modeling \\(Y_1\\) and \\(Y_2\\) with Negative Binomial (Gamma–Poisson mixture) or adding random effects.\n",
    "- **Unequal exposure**: if the two counts come from different time windows or populations, interpret \\(\\mu_1\\) and \\(\\mu_2\\) as expected counts for those exposures.\n",
    "- **Numerical issues**: the PMF involves a modified Bessel function \\(I_{|k|}(\\cdot)\\), which grows roughly like \\(e^{2\\sqrt{\\mu_1\\mu_2}}\\).\n",
    "  Naive evaluation can overflow; prefer `logpmf` or scaled Bessel forms (as in `special.ive`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9546576",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = 200.0, 190.0\n",
    "k = 0\n",
    "z = 2.0 * math.sqrt(mu1 * mu2)\n",
    "\n",
    "iv_val = special.iv(abs(k), z)\n",
    "ive_val = special.ive(abs(k), z)\n",
    "\n",
    "# Naive PMF computation can produce inf/0 -> nan for large parameters\n",
    "pmf_naive = math.exp(-(mu1 + mu2)) * (mu1 / mu2) ** (k / 2) * iv_val\n",
    "\n",
    "logpmf_stable = skellam_logpmf(k, mu1, mu2)\n",
    "logpmf_scipy = stats.skellam.logpmf(k, mu1, mu2)\n",
    "\n",
    "{\n",
    "    \"z\": z,\n",
    "    \"iv(abs(k), z)\": float(iv_val),\n",
    "    \"ive(abs(k), z)\": float(ive_val),\n",
    "    \"pmf_naive\": float(pmf_naive) if np.isfinite(pmf_naive) else str(pmf_naive),\n",
    "    \"logpmf_stable\": float(logpmf_stable),\n",
    "    \"logpmf_scipy\": float(logpmf_scipy),\n",
    "}\n",
    "\n",
    "k_far = 600\n",
    "{\n",
    "    \"k_far\": k_far,\n",
    "    \"pmf\": float(stats.skellam.pmf(k_far, mu1, mu2)),\n",
    "    \"logpmf\": float(stats.skellam.logpmf(k_far, mu1, mu2)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da657687",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- Skellam models the difference of two independent Poisson counts: \\(X=Y_1-Y_2\\).\n",
    "- Support is all integers; parameters \\(\\mu_1,\\mu_2\\ge 0\\).\n",
    "- Key moments: \\(\\mathbb{E}[X]=\\mu_1-\\mu_2\\), \\(\\mathrm{Var}(X)=\\mu_1+\\mu_2\\), skewness \\((\\mu_1-\\mu_2)/(\\mu_1+\\mu_2)^{3/2}\\).\n",
    "- PMF involves a modified Bessel function; use `logpmf` / scaled Bessel forms for numerical stability.\n",
    "- Sampling is easy: draw two Poisson counts and subtract.\n",
    "- Common uses include score differentials, net event counts, and integer-valued innovations in generative models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}