{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b01bdc1",
   "metadata": {},
   "source": [
    "# Dirichlet–Multinomial Distribution (`dirichlet_multinomial`)\n",
    "\n",
    "The **Dirichlet–multinomial** (a.k.a. *Dirichlet compound multinomial*) is a **discrete multivariate** distribution over count vectors.\n",
    "It appears when you model category probabilities as *uncertain*: draw probabilities $p$ from a Dirichlet distribution, then draw counts $X$ from a multinomial given $p$.\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- explain the Dirichlet–multinomial as a “multinomial with random probabilities” and why it captures **overdispersion**\n",
    "- write down the PMF and understand its constraints (support + parameter space)\n",
    "- derive the mean and covariance from the hierarchical model\n",
    "- sample from it in NumPy and visualize it (1D and simplex plots)\n",
    "- use SciPy’s `scipy.stats.dirichlet_multinomial` for PMF/moments, and implement missing pieces (CDF/sampling/fit) yourself\n",
    "\n",
    "## Prerequisites\n",
    "- basic probability (expectation, variance, conditional expectation)\n",
    "- familiarity with the multinomial and Dirichlet distributions\n",
    "- comfort reading Gamma/Beta functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30562587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import digamma, gammaln, logsumexp\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511b5f2",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 1) Title & Classification\n",
    "\n",
    "            - **Name**: `dirichlet_multinomial` (Dirichlet–multinomial, Dirichlet compound multinomial)\n",
    "            - **Type**: **Discrete** (multivariate counts)\n",
    "            - **Support** (for $K$ categories and total count $n$):\n",
    "\n",
    "              $$\n",
    "              \\mathcal{S}_{n,K} = \\left\\{x \\in \\{0,1,2,\\dots\\}^K : \\sum_{i=1}^K x_i = n\\right\\}\n",
    "              $$\n",
    "\n",
    "            - **Parameter space**:\n",
    "              - $n \\in \\{0,1,2,\\dots\\}$ (integer total count)\n",
    "              - $\\alpha = (\\alpha_1,\\dots,\\alpha_K)$ with $\\alpha_i > 0$\n",
    "              - define $\\alpha_0 = \\sum_{i=1}^K \\alpha_i$ (total concentration)\n",
    "\n",
    "            A draw $X \\sim \\text{DirichletMultinomial}(n,\\alpha)$ is a **count vector** with a fixed total: $\\sum_i X_i = n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80fabc",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 2) Intuition & Motivation\n",
    "\n",
    "            ### What it models\n",
    "            The Dirichlet–multinomial models **counts across categories** when the category probabilities themselves vary across trials/replicates.\n",
    "\n",
    "            A common hierarchical story is:\n",
    "\n",
    "            $$\n",
    "            p \\sim \\mathrm{Dirichlet}(\\alpha),\n",
    "            \\qquad\n",
    "            X \\mid p \\sim \\mathrm{Multinomial}(n, p).\n",
    "            $$\n",
    "\n",
    "            If $p$ were fixed, you’d just have a multinomial. But if $p$ changes (e.g., each document has a different word distribution), the multinomial is often **too confident**.\n",
    "            The Dirichlet–multinomial captures this extra variability (“**overdispersion**”) and induces **negative correlations** between counts because they must sum to $n$.\n",
    "\n",
    "            ### Typical real-world use cases\n",
    "            - **Text / NLP**: bag-of-words counts (posterior predictive of a Dirichlet–multinomial model)\n",
    "            - **Ecology**: species counts across sites with heterogeneous composition\n",
    "            - **Genomics**: overdispersed categorical counts (e.g., allelic counts)\n",
    "            - **A/B testing on categories**: uncertainty in category probabilities across cohorts\n",
    "\n",
    "            ### Relations to other distributions\n",
    "            - **Dirichlet + Multinomial**: it is the **Dirichlet mixture** of multinomials.\n",
    "            - **Beta–binomial**: when $K=2$, the first component $X_1$ is Beta–binomial.\n",
    "            - **Multinomial limit**: as $\\alpha_0 \\to \\infty$ with $\\alpha/\\alpha_0$ fixed, the Dirichlet–multinomial approaches a multinomial with fixed probabilities.\n",
    "            - **Pólya urn**: an equivalent sampling scheme is “reinforcement” sampling where each draw increases the chance of drawing that category again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff0ab9",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 3) Formal Definition\n",
    "\n",
    "            Let $X=(X_1,\\dots,X_K)$ be a count vector with $\\sum_i X_i = n$.\n",
    "\n",
    "            ### PMF (discrete analogue of a PDF)\n",
    "            For $x \\in \\mathcal{S}_{n,K}$:\n",
    "\n",
    "            $$\n",
    "            \\Pr(X=x \\mid n,\\alpha)\n",
    "            = \\frac{n!}{\\prod_{i=1}^K x_i!}\n",
    "              \\frac{\\Gamma(\\alpha_0)}{\\Gamma(\\alpha_0+n)}\n",
    "              \\prod_{i=1}^K \\frac{\\Gamma(\\alpha_i + x_i)}{\\Gamma(\\alpha_i)}.\n",
    "            $$\n",
    "\n",
    "            Using the rising factorial (Pochhammer symbol) $(a)_m = \\Gamma(a+m)/\\Gamma(a)$, this can be written:\n",
    "\n",
    "            $$\n",
    "            \\Pr(X=x \\mid n,\\alpha)\n",
    "            = \\frac{n!}{\\prod_i x_i!}\n",
    "              \\frac{\\prod_i (\\alpha_i)_{x_i}}{(\\alpha_0)_n}.\n",
    "            $$\n",
    "\n",
    "            ### CDF\n",
    "            A common multivariate “CDF” is the **lower-orthant CDF**:\n",
    "\n",
    "            $$\n",
    "            F(x) = \\Pr(X_1 \\le x_1,\\dots,X_K \\le x_K)\n",
    "            = \\sum_{y \\in \\mathcal{S}_{n,K}:\\; y_i \\le x_i\\;\\forall i} \\Pr(X=y).\n",
    "            $$\n",
    "\n",
    "            There is no simple closed form in general. For $K=2$, this reduces to the usual **univariate CDF** of the Beta–binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c89bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_alpha(alpha) -> np.ndarray:\n",
    "    alpha = np.asarray(alpha, dtype=float)\n",
    "    if alpha.ndim != 1:\n",
    "        raise ValueError(\"alpha must be a 1D array of positive values\")\n",
    "    if alpha.size < 2:\n",
    "        raise ValueError(\"alpha must have length K>=2\")\n",
    "    if not np.all(np.isfinite(alpha)):\n",
    "        raise ValueError(\"alpha must be finite\")\n",
    "    if np.any(alpha <= 0):\n",
    "        raise ValueError(\"alpha must be strictly positive\")\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def _validate_counts(x, k: int) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    if x.ndim != 2 or x.shape[1] != k:\n",
    "        raise ValueError(f\"x must have shape (k,) or (m,k) with k={k}\")\n",
    "\n",
    "    if not np.issubdtype(x.dtype, np.integer):\n",
    "        if np.any(np.abs(x - np.round(x)) > 0):\n",
    "            raise ValueError(\"x must contain integers\")\n",
    "        x = np.round(x).astype(int)\n",
    "    else:\n",
    "        x = x.astype(int)\n",
    "\n",
    "    if np.any(x < 0):\n",
    "        raise ValueError(\"x must be nonnegative\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dirichlet_multinomial_logpmf(x, alpha, n: int | None = None) -> np.ndarray:\n",
    "    '''Vectorized log PMF for the Dirichlet–multinomial.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Count vector(s), shape (k,) or (m,k). Each row must sum to n.\n",
    "    alpha:\n",
    "        Concentration parameters (k,), alpha_i > 0.\n",
    "    n:\n",
    "        Total count. If None, inferred from x row sums.\n",
    "    '''\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    x = _validate_counts(x, k=alpha.size)\n",
    "\n",
    "    row_sums = x.sum(axis=1)\n",
    "    if n is None:\n",
    "        n_vec = row_sums\n",
    "    else:\n",
    "        if np.any(row_sums != n):\n",
    "            raise ValueError(\"Each row of x must sum to n\")\n",
    "        n_vec = np.full_like(row_sums, fill_value=n)\n",
    "\n",
    "    alpha0 = alpha.sum()\n",
    "\n",
    "    log_multinomial_coeff = gammaln(n_vec + 1) - np.sum(gammaln(x + 1), axis=1)\n",
    "    log_norm = gammaln(alpha0) - gammaln(alpha0 + n_vec)\n",
    "    log_ratio = np.sum(gammaln(alpha + x) - gammaln(alpha), axis=1)\n",
    "\n",
    "    out = log_multinomial_coeff + log_norm + log_ratio\n",
    "    return out[0] if out.size == 1 else out\n",
    "\n",
    "\n",
    "def dirichlet_multinomial_pmf(x, alpha, n: int | None = None) -> np.ndarray:\n",
    "    return np.exp(dirichlet_multinomial_logpmf(x, alpha=alpha, n=n))\n",
    "\n",
    "\n",
    "def compositions(n: int, k: int):\n",
    "    '''Generate all k-tuples of nonnegative integers summing to n (stars and bars).'''\n",
    "    if k == 1:\n",
    "        yield (n,)\n",
    "        return\n",
    "    for i in range(n + 1):\n",
    "        for tail in compositions(n - i, k - 1):\n",
    "            yield (i,) + tail\n",
    "\n",
    "\n",
    "def enumerate_support(n: int, k: int) -> np.ndarray:\n",
    "    '''Enumerate the support S_{n,k}. Size is comb(n+k-1, k-1).'''\n",
    "    return np.array(list(compositions(n, k)), dtype=int)\n",
    "\n",
    "\n",
    "def dm_cdf_small_n(x, alpha, n: int) -> float:\n",
    "    '''Lower-orthant CDF by brute-force summation (only feasible for small n,k).'''\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    x = _validate_counts(x, k=alpha.size)[0]\n",
    "    if x.sum() != n:\n",
    "        raise ValueError(\"x must sum to n\")\n",
    "\n",
    "    ys = enumerate_support(n=n, k=alpha.size)\n",
    "    mask = np.all(ys <= x[None, :], axis=1)\n",
    "    return float(np.sum(dirichlet_multinomial_pmf(ys[mask], alpha=alpha, n=n)))\n",
    "\n",
    "\n",
    "def simplex_xy_3(counts: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    '''Map 3-category compositions to 2D barycentric coordinates for plotting.'''\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    counts = np.atleast_2d(counts)\n",
    "    if counts.shape[1] != 3:\n",
    "        raise ValueError(\"simplex_xy_3 expects shape (m,3)\")\n",
    "\n",
    "    n = counts.sum(axis=1)\n",
    "    if np.any(n <= 0):\n",
    "        raise ValueError(\"All rows must sum to a positive n\")\n",
    "\n",
    "    p = counts / n[:, None]\n",
    "    x = p[:, 1] + 0.5 * p[:, 2]\n",
    "    y = (np.sqrt(3) / 2.0) * p[:, 2]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def dirichlet_rvs_numpy(alpha, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''Sample Dirichlet(alpha) via Gamma normalization (NumPy-only).'''\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    g = rng.gamma(shape=alpha, scale=1.0, size=(size, alpha.size))\n",
    "    return g / g.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def dirichlet_multinomial_rvs_numpy(alpha, n: int, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''Sample Dirichlet–multinomial(n, alpha) (NumPy-only).\n",
    "\n",
    "    Algorithm:\n",
    "    1) p ~ Dirichlet(alpha)\n",
    "    2) X | p ~ Multinomial(n, p)\n",
    "    '''\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    ps = dirichlet_rvs_numpy(alpha, size=size, rng=rng)\n",
    "    out = np.empty((size, alpha.size), dtype=int)\n",
    "    for i, p in enumerate(ps):\n",
    "        out[i] = rng.multinomial(n, p)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check against SciPy's PMF\n",
    "alpha = np.array([1.0, 2.0, 3.0])\n",
    "n = 10\n",
    "x = np.array([2, 3, 5])\n",
    "\n",
    "pmf_numpy = dirichlet_multinomial_pmf(x, alpha=alpha, n=n)\n",
    "pmf_scipy = stats.dirichlet_multinomial.pmf(x, alpha=alpha, n=n)\n",
    "\n",
    "pmf_numpy, pmf_scipy, float(pmf_numpy - pmf_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d9f15",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 4) Moments & Properties\n",
    "\n",
    "            ### Mean\n",
    "            Using the hierarchical model $p \\sim \\mathrm{Dirichlet}(\\alpha)$ and $X\\mid p \\sim \\mathrm{Multinomial}(n,p)$,\n",
    "\n",
    "            $$\n",
    "            \\mathbb{E}[X_i] = n\\,\\frac{\\alpha_i}{\\alpha_0}.\n",
    "            $$\n",
    "\n",
    "            ### Covariance\n",
    "\n",
    "            For $i \\ne j$:\n",
    "\n",
    "            $$\n",
    "            \\mathrm{Cov}(X_i, X_j)\n",
    "            = -\\,n\\,\\frac{\\alpha_i\\alpha_j}{\\alpha_0^2}\\,\\frac{n+\\alpha_0}{\\alpha_0+1}.\n",
    "            $$\n",
    "\n",
    "            For the variance (the $i=j$ case):\n",
    "\n",
    "            $$\n",
    "            \\mathrm{Var}(X_i)\n",
    "            = n\\,\\frac{\\alpha_i}{\\alpha_0}\\left(1-\\frac{\\alpha_i}{\\alpha_0}\\right)\\,\\frac{n+\\alpha_0}{\\alpha_0+1}.\n",
    "            $$\n",
    "\n",
    "            ### Marginals (Beta–binomial)\n",
    "            Each component $X_i$ marginally follows a Beta–binomial distribution:\n",
    "\n",
    "            $$\n",
    "            X_i \\sim \\mathrm{BetaBinomial}\\big(n,\\; \\alpha_i,\\; \\alpha_0-\\alpha_i\\big).\n",
    "            $$\n",
    "\n",
    "            This is useful because it gives you **univariate** quantities like skewness and kurtosis for each component.\n",
    "\n",
    "            A clean way to get higher moments is via **factorial moments**. For $r\\in\\{1,2,3,\\dots\\}$:\n",
    "\n",
    "            $$\n",
    "            \\mathbb{E}[(X_i)_{r}] = (n)_{r}\\,\\frac{(\\alpha_i)_{r}}{(\\alpha_0)_{r}},\n",
    "            $$\n",
    "\n",
    "            where $(a)_r$ is the rising factorial and $(X_i)_r$ on the left denotes the *falling* factorial.\n",
    "            From these you can reconstruct raw/central moments (and thus skewness/kurtosis).\n",
    "\n",
    "            ### MGF / characteristic function\n",
    "            For a vector $t\\in\\mathbb{R}^K$, the MGF can be written as a (finite) sum over the support:\n",
    "\n",
    "            $$\n",
    "            M_X(t) = \\mathbb{E}[e^{t^\\top X}] = \\sum_{x\\in\\mathcal{S}_{n,K}} e^{t^\\top x}\\,\\Pr(X=x).\n",
    "            $$\n",
    "\n",
    "            Equivalently, via the mixture:\n",
    "\n",
    "            $$\n",
    "            M_X(t) = \\mathbb{E}_{p\\sim\\mathrm{Dir}(\\alpha)}\\left[\\left(\\sum_{i=1}^K p_i e^{t_i}\\right)^n\\right].\n",
    "            $$\n",
    "\n",
    "            Closed forms involve special functions (multivariate hypergeometric functions). For small $n$ you can compute it by enumeration.\n",
    "            The characteristic function is $\\varphi(\\omega)=M_X(i\\omega)$.\n",
    "\n",
    "            ### Entropy\n",
    "            The Shannon entropy is\n",
    "\n",
    "            $$\n",
    "            H(X) = -\\sum_{x\\in\\mathcal{S}_{n,K}} \\Pr(X=x)\\,\\log \\Pr(X=x).\n",
    "            $$\n",
    "\n",
    "            There is no simple closed form in general; you can compute it exactly by enumeration for small $n$, or estimate it by Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm_mean(alpha, n: int) -> np.ndarray:\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    return n * alpha / alpha.sum()\n",
    "\n",
    "\n",
    "def dm_cov(alpha, n: int) -> np.ndarray:\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    k = alpha.size\n",
    "    alpha0 = alpha.sum()\n",
    "\n",
    "    # Cov(X_i, X_j) = -n * alpha_i*alpha_j / alpha0^2 * (n+alpha0)/(alpha0+1)\n",
    "    factor = n * (n + alpha0) / (alpha0 + 1.0) / (alpha0**2)\n",
    "    cov = -factor * np.outer(alpha, alpha)\n",
    "\n",
    "    # Fix diagonal to variance formula\n",
    "    p = alpha / alpha0\n",
    "    var = n * p * (1.0 - p) * (n + alpha0) / (alpha0 + 1.0)\n",
    "    np.fill_diagonal(cov, var)\n",
    "    return cov\n",
    "\n",
    "\n",
    "def beta_binomial_moments_via_factorials(n: int, a: float, b: float):\n",
    "    '''Return (mean, variance, skewness, excess_kurtosis) for BetaBinomial(n,a,b).\n",
    "\n",
    "    Uses factorial moments (stable + avoids a giant closed-form expression).\n",
    "    '''\n",
    "    a0 = a + b\n",
    "\n",
    "    # Falling factorial moments of X: E[(X)_r] = (n)_r E[p^r]\n",
    "    # with p~Beta(a,b), E[p^r] = (a)_r/(a0)_r (rising factorial).\n",
    "    f1 = n * a / a0\n",
    "    f2 = n * (n - 1) * a * (a + 1) / (a0 * (a0 + 1)) if n >= 2 else 0.0\n",
    "    f3 = (\n",
    "        n * (n - 1) * (n - 2) * a * (a + 1) * (a + 2) / (a0 * (a0 + 1) * (a0 + 2))\n",
    "        if n >= 3\n",
    "        else 0.0\n",
    "    )\n",
    "    f4 = (\n",
    "        n\n",
    "        * (n - 1)\n",
    "        * (n - 2)\n",
    "        * (n - 3)\n",
    "        * a\n",
    "        * (a + 1)\n",
    "        * (a + 2)\n",
    "        * (a + 3)\n",
    "        / (a0 * (a0 + 1) * (a0 + 2) * (a0 + 3))\n",
    "        if n >= 4\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    # Stirling-number conversion (X^r = sum_k S(r,k) (X)_k)\n",
    "    m1 = f1\n",
    "    m2 = f1 + f2\n",
    "    m3 = f1 + 3 * f2 + f3\n",
    "    m4 = f1 + 7 * f2 + 6 * f3 + f4\n",
    "\n",
    "    mu = m1\n",
    "    mu2 = m2 - mu**2\n",
    "    mu3 = m3 - 3 * m2 * mu + 2 * mu**3\n",
    "    mu4 = m4 - 4 * m3 * mu + 6 * m2 * mu**2 - 3 * mu**4\n",
    "\n",
    "    skew = mu3 / (mu2 ** 1.5) if mu2 > 0 else np.nan\n",
    "    kurt_excess = mu4 / (mu2**2) - 3.0 if mu2 > 0 else np.nan\n",
    "    return mu, mu2, skew, kurt_excess\n",
    "\n",
    "\n",
    "def dm_entropy_small_n(alpha, n: int) -> float:\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    xs = enumerate_support(n=n, k=alpha.size)\n",
    "    logp = dirichlet_multinomial_logpmf(xs, alpha=alpha, n=n)\n",
    "    p = np.exp(logp)\n",
    "    return float(-np.sum(p * logp))\n",
    "\n",
    "\n",
    "def dm_mgf_small_n(t, alpha, n: int) -> float:\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    if t.shape != alpha.shape:\n",
    "        raise ValueError(f\"t must have shape {alpha.shape}\")\n",
    "\n",
    "    xs = enumerate_support(n=n, k=alpha.size)\n",
    "    logp = dirichlet_multinomial_logpmf(xs, alpha=alpha, n=n)\n",
    "    return float(np.exp(logsumexp(logp + xs @ t)))\n",
    "\n",
    "\n",
    "# Example: moments for a 3-category model\n",
    "alpha = np.array([1.5, 2.0, 4.5])\n",
    "n = 20\n",
    "\n",
    "mean = dm_mean(alpha, n=n)\n",
    "cov = dm_cov(alpha, n=n)\n",
    "ent = dm_entropy_small_n(alpha, n=n)\n",
    "\n",
    "mean, cov, ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed74288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal skewness/kurtosis via the Beta–binomial identity\n",
    "alpha0 = alpha.sum()\n",
    "for i in range(alpha.size):\n",
    "    a = alpha[i]\n",
    "    b = alpha0 - alpha[i]\n",
    "    m, v, s, kex = beta_binomial_moments_via_factorials(n=n, a=float(a), b=float(b))\n",
    "    print(\n",
    "        f\"X_{i+1}: mean={m:.3f}, var={v:.3f}, skew={s:.3f}, excess_kurtosis={kex:.3f} \"\n",
    "        f\"(BetaBinomial(n={n}, a={a:.2f}, b={b:.2f}))\"\n",
    "    )\n",
    "\n",
    "# Cross-check against SciPy's betabinom for one component\n",
    "i = 0\n",
    "a = float(alpha[i])\n",
    "b = float(alpha0 - alpha[i])\n",
    "scipy_mean, scipy_var, scipy_skew, scipy_kex = stats.betabinom.stats(n=n, a=a, b=b, moments=\"mvsk\")\n",
    "(scipy_mean, scipy_var, scipy_skew, scipy_kex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae62739",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 5) Parameter Interpretation\n",
    "\n",
    "            Think of $\\alpha$ as **prior pseudo-counts** for category probabilities.\n",
    "\n",
    "            - The **mean proportions** are\n",
    "\n",
    "              $$\\mathbb{E}[p_i] = \\frac{\\alpha_i}{\\alpha_0},\\quad \\alpha_0=\\sum_i\\alpha_i.$$\n",
    "\n",
    "            - The **total concentration** $\\alpha_0$ controls how much $p$ varies across replicates:\n",
    "\n",
    "              - small $\\alpha_0$  → $p$ is highly variable → counts are **more dispersed** (more mass near simplex corners)\n",
    "              - large $\\alpha_0$  → $p$ concentrates near its mean → counts look more like a plain multinomial\n",
    "\n",
    "            - Holding $\\alpha_0$ fixed, changing the **ratios** $\\alpha_i/\\alpha_0$ shifts mass toward categories with larger ratios.\n",
    "\n",
    "            Below we visualize samples for the same mean proportions but different total concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c83dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same mean proportions, different concentration alpha0\n",
    "n = 25\n",
    "base = np.array([1.0, 2.0, 3.0])\n",
    "base = base / base.sum()  # mean proportions\n",
    "\n",
    "scales = [0.3, 1.0, 5.0]\n",
    "size = 2500\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# draw simplex triangle\n",
    "tri_x = [0.0, 1.0, 0.5, 0.0]\n",
    "tri_y = [0.0, 0.0, np.sqrt(3) / 2.0, 0.0]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=tri_x, y=tri_y, mode=\"lines\", line=dict(color=\"black\"), showlegend=False)\n",
    ")\n",
    "\n",
    "for s in scales:\n",
    "    alpha_s = s * base * 30.0  # scale into a reasonable pseudo-count regime\n",
    "    samples = dirichlet_multinomial_rvs_numpy(alpha=alpha_s, n=n, size=size, rng=rng)\n",
    "    x, y = simplex_xy_3(samples)\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode=\"markers\",\n",
    "            name=f\"alpha0≈{alpha_s.sum():.1f}\",\n",
    "            marker=dict(size=4, opacity=0.25),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Dirichlet–multinomial samples on the 3-simplex (same mean, different concentration)\",\n",
    "    xaxis_title=\"barycentric x\",\n",
    "    yaxis_title=\"barycentric y\",\n",
    "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
    "    width=850,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddba680",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 6) Derivations\n",
    "\n",
    "            We derive mean and covariance using the mixture representation:\n",
    "\n",
    "            $$\n",
    "            p \\sim \\mathrm{Dirichlet}(\\alpha),\n",
    "            \\qquad\n",
    "            X \\mid p \\sim \\mathrm{Multinomial}(n, p).\n",
    "            $$\n",
    "\n",
    "            ### Expectation\n",
    "            By the law of total expectation:\n",
    "\n",
    "            $$\n",
    "            \\mathbb{E}[X_i] = \\mathbb{E}\\big[\\,\\mathbb{E}[X_i\\mid p]\\,\\big]\n",
    "            = \\mathbb{E}[n p_i]\n",
    "            = n\\,\\mathbb{E}[p_i]\n",
    "            = n\\,\\frac{\\alpha_i}{\\alpha_0}.\n",
    "            $$\n",
    "\n",
    "            ### Variance\n",
    "            By the law of total variance:\n",
    "\n",
    "            $$\n",
    "            \\mathrm{Var}(X_i) = \\mathbb{E}[\\mathrm{Var}(X_i\\mid p)] + \\mathrm{Var}(\\mathbb{E}[X_i\\mid p]).\n",
    "            $$\n",
    "\n",
    "            For a multinomial:\n",
    "\n",
    "            $$\n",
    "            \\mathbb{E}[X_i\\mid p] = n p_i,\n",
    "            \\qquad\n",
    "            \\mathrm{Var}(X_i\\mid p) = n p_i(1-p_i).\n",
    "            $$\n",
    "\n",
    "            So:\n",
    "\n",
    "            $$\n",
    "            \\mathrm{Var}(X_i)\n",
    "            = \\mathbb{E}[n p_i(1-p_i)] + \\mathrm{Var}(n p_i)\n",
    "            = n\\,\\mathbb{E}[p_i - p_i^2] + n^2\\,\\mathrm{Var}(p_i).\n",
    "            $$\n",
    "\n",
    "            Using Dirichlet moments\n",
    "            $\\mathbb{E}[p_i]=\\alpha_i/\\alpha_0$ and $\\mathrm{Var}(p_i)=\\alpha_i(\\alpha_0-\\alpha_i)/(\\alpha_0^2(\\alpha_0+1))$\n",
    "            yields the variance formula in Section 4.\n",
    "\n",
    "            ### Covariance\n",
    "            Similarly, for $i\\ne j$:\n",
    "\n",
    "            $$\n",
    "            \\mathrm{Cov}(X_i, X_j)\n",
    "            = \\mathbb{E}[\\mathrm{Cov}(X_i,X_j\\mid p)] + \\mathrm{Cov}(\\mathbb{E}[X_i\\mid p],\\mathbb{E}[X_j\\mid p]).\n",
    "            $$\n",
    "\n",
    "            For a multinomial, $\\mathrm{Cov}(X_i,X_j\\mid p) = -n p_i p_j$ for $i\\ne j$.\n",
    "            With Dirichlet moments for $\\mathbb{E}[p_i p_j]$, you arrive at the negative covariance formula.\n",
    "\n",
    "            ### Likelihood (for fitting $\\alpha$)\n",
    "            Given an observed count vector $x$ with total $n$, the likelihood as a function of $\\alpha$ is:\n",
    "\n",
    "            $$\n",
    "            L(\\alpha; x)\n",
    "            \\propto\n",
    "            \\frac{\\Gamma(\\alpha_0)}{\\Gamma(\\alpha_0+n)}\n",
    "            \\prod_{i=1}^K \\frac{\\Gamma(\\alpha_i + x_i)}{\\Gamma(\\alpha_i)}.\n",
    "            $$\n",
    "\n",
    "            Taking logs gives:\n",
    "\n",
    "            $$\n",
    "            \\ell(\\alpha; x)\n",
    "            = \\log\\Gamma(\\alpha_0) - \\log\\Gamma(\\alpha_0+n)\n",
    "              + \\sum_i \\big(\\log\\Gamma(\\alpha_i + x_i) - \\log\\Gamma(\\alpha_i)\\big)\n",
    "              + \\text{const}(x).\n",
    "            $$\n",
    "\n",
    "            There is no closed-form MLE for $\\alpha$ in general; you typically optimize $\\ell(\\alpha)$ numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ec3dc",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 7) Sampling & Simulation\n",
    "\n",
    "            A simple **NumPy-only** sampling algorithm follows directly from the hierarchical story:\n",
    "\n",
    "            1. Sample $p \\sim \\mathrm{Dirichlet}(\\alpha)$.\n",
    "               A standard implementation uses Gamma variables: draw $g_i \\sim \\mathrm{Gamma}(\\alpha_i, 1)$ and set $p_i = g_i / \\sum_j g_j$.\n",
    "            2. Sample counts $X \\mid p \\sim \\mathrm{Multinomial}(n, p)$.\n",
    "\n",
    "            This is exactly what `dirichlet_multinomial_rvs_numpy` implements.\n",
    "\n",
    "            Below we verify mean/covariance by Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([1.5, 2.0, 4.5])\n",
    "            n = 25\n",
    "\n",
    "            theory_mean = dm_mean(alpha, n=n)\n",
    "            theory_cov = dm_cov(alpha, n=n)\n",
    "\n",
    "            samples = dirichlet_multinomial_rvs_numpy(alpha=alpha, n=n, size=50_000, rng=rng)\n",
    "            sample_mean = samples.mean(axis=0)\n",
    "            sample_cov = np.cov(samples.T, ddof=0)\n",
    "\n",
    "            print('theory mean:', theory_mean)\n",
    "            print('sample mean:', sample_mean)\n",
    "            print('\n",
    "max abs mean error:', np.max(np.abs(sample_mean - theory_mean)))\n",
    "\n",
    "            print('\n",
    "max abs cov error:', np.max(np.abs(sample_cov - theory_cov)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff68fe",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 8) Visualization\n",
    "\n",
    "            Because the Dirichlet–multinomial is multivariate, visuals depend on $K$:\n",
    "\n",
    "            - For $K=2$ it reduces to a **Beta–binomial** and you can plot a standard PMF/CDF over $\\{0,1,\\dots,n\\}$.\n",
    "            - For $K=3$ you can plot probabilities/samples on a 2D simplex (triangle).\n",
    "\n",
    "            We do both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd45c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=2: PMF and CDF (Beta–binomial view)\n",
    "n = 30\n",
    "alpha2 = np.array([2.0, 5.0])\n",
    "\n",
    "xs = np.arange(n + 1)\n",
    "pmf = np.array([dirichlet_multinomial_pmf([x, n - x], alpha=alpha2, n=n) for x in xs])\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=xs, y=pmf, name=\"PMF\"))\n",
    "fig.update_layout(\n",
    "    title=\"Dirichlet–multinomial with K=2 (PMF of X1)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X1=x)\",\n",
    "    bargap=0.05,\n",
    "    width=850,\n",
    "    height=380,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=cdf, mode=\"lines+markers\", name=\"CDF\"))\n",
    "fig.update_layout(\n",
    "    title=\"Dirichlet–multinomial with K=2 (CDF of X1)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X1≤x)\",\n",
    "    width=850,\n",
    "    height=380,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo samples vs PMF (K=2)\n",
    "size = 20_000\n",
    "s = dirichlet_multinomial_rvs_numpy(alpha=alpha2, n=n, size=size, rng=rng)\n",
    "x1 = s[:, 0]\n",
    "\n",
    "fig = px.histogram(x1, nbins=n + 1, histnorm=\"probability\", title=\"Monte Carlo histogram vs PMF\")\n",
    "fig.add_trace(go.Scatter(x=xs, y=pmf, mode=\"lines\", name=\"PMF\", line=dict(color=\"black\")))\n",
    "fig.update_layout(xaxis_title=\"x1\", yaxis_title=\"probability\", width=850, height=420)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72670f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=3: PMF on the simplex (small n, exact enumeration)\n",
    "n = 20\n",
    "alpha3 = np.array([1.2, 2.5, 4.0])\n",
    "\n",
    "support = enumerate_support(n=n, k=3)\n",
    "logp = dirichlet_multinomial_logpmf(support, alpha=alpha3, n=n)\n",
    "p = np.exp(logp)\n",
    "\n",
    "sx, sy = simplex_xy_3(support)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sx,\n",
    "        y=sy,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=np.log10(p),\n",
    "            colorscale=\"Viridis\",\n",
    "            colorbar=dict(title=\"log10 PMF\"),\n",
    "        ),\n",
    "        text=[str(tuple(row)) for row in support],\n",
    "        hovertemplate=\"x=%{text}<br>log10 p=%{marker.color:.3f}<extra></extra>\",\n",
    "        name=\"support\",\n",
    "    )\n",
    ")\n",
    "\n",
    "tri_x = [0.0, 1.0, 0.5, 0.0]\n",
    "tri_y = [0.0, 0.0, np.sqrt(3) / 2.0, 0.0]\n",
    "fig.add_trace(go.Scatter(x=tri_x, y=tri_y, mode=\"lines\", line=dict(color=\"black\"), showlegend=False))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Dirichlet–multinomial PMF on the 3-simplex (exact, enumerated support)\",\n",
    "    xaxis_title=\"barycentric x\",\n",
    "    yaxis_title=\"barycentric y\",\n",
    "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
    "    width=850,\n",
    "    height=520,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=3: Monte Carlo samples on the simplex\n",
    "n = 20\n",
    "alpha3 = np.array([1.2, 2.5, 4.0])\n",
    "\n",
    "samples = dirichlet_multinomial_rvs_numpy(alpha=alpha3, n=n, size=6000, rng=rng)\n",
    "x, y = simplex_xy_3(samples)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=4, opacity=0.25),\n",
    "        text=[str(tuple(row)) for row in samples],\n",
    "        hovertemplate=\"x=%{text}<extra></extra>\",\n",
    "        name=\"samples\",\n",
    "    )\n",
    ")\n",
    "\n",
    "tri_x = [0.0, 1.0, 0.5, 0.0]\n",
    "tri_y = [0.0, 0.0, np.sqrt(3) / 2.0, 0.0]\n",
    "fig.add_trace(go.Scatter(x=tri_x, y=tri_y, mode=\"lines\", line=dict(color=\"black\"), showlegend=False))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Dirichlet–multinomial Monte Carlo samples on the 3-simplex\",\n",
    "    xaxis_title=\"barycentric x\",\n",
    "    yaxis_title=\"barycentric y\",\n",
    "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
    "    width=850,\n",
    "    height=520,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde976e",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 9) SciPy Integration\n",
    "\n",
    "            SciPy exposes the Dirichlet–multinomial as `scipy.stats.dirichlet_multinomial`.\n",
    "\n",
    "            In this environment (SciPy version may vary), the object provides:\n",
    "            - `pmf` / `logpmf`\n",
    "            - moment methods like `mean`, `var`, and `cov`\n",
    "\n",
    "            But it may **not** provide `cdf`, `rvs`, or `fit` for this multivariate distribution.\n",
    "            We’ll show:\n",
    "            - how to use SciPy where available\n",
    "            - how to implement missing pieces (CDF by summation for small $n$, sampling via the hierarchical model, and MLE via `scipy.optimize`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "            print('SciPy version:', scipy.__version__)\n",
    "\n",
    "            n = 10\n",
    "            alpha = np.array([1.0, 2.0, 3.0])\n",
    "            x = np.array([2, 3, 5])\n",
    "\n",
    "            dm = stats.dirichlet_multinomial(n=n, alpha=alpha)\n",
    "\n",
    "            print('pmf:', dm.pmf(x))\n",
    "            print('logpmf:', dm.logpmf(x))\n",
    "            print('mean:', dm.mean())\n",
    "            print('cov:\n",
    "', dm.cov())\n",
    "\n",
    "            # Feature check\n",
    "            for name in ['cdf', 'rvs', 'fit']:\n",
    "                print(name, 'available?', hasattr(dm, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF: SciPy doesn't implement a multivariate cdf here, but we can compute it by brute force for small n.\n",
    "n = 12\n",
    "alpha = np.array([1.0, 2.0, 3.0])\n",
    "x = np.array([3, 4, 5])\n",
    "\n",
    "dm_cdf_small_n(x, alpha=alpha, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccaa3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For K=2, the CDF reduces to the usual univariate Beta–binomial CDF\n",
    "n = 30\n",
    "alpha2 = np.array([2.0, 5.0])\n",
    "xs = np.arange(n + 1)\n",
    "\n",
    "# X1 ~ BetaBinomial(n, a=alpha1, b=alpha2)\n",
    "cdf_scipy = stats.betabinom.cdf(xs, n=n, a=alpha2[0], b=alpha2[1])\n",
    "cdf_numpy = np.cumsum([dirichlet_multinomial_pmf([x, n - x], alpha=alpha2, n=n) for x in xs])\n",
    "\n",
    "float(np.max(np.abs(cdf_scipy - cdf_numpy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2319cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling: SciPy's dirichlet_multinomial may not expose rvs, but sampling is easy via the hierarchical model.\n",
    "# Here is a SciPy-flavored sampler (Dirichlet from SciPy + Multinomial from NumPy):\n",
    "\n",
    "def dirichlet_multinomial_rvs_scipy(alpha, n: int, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    ps = stats.dirichlet.rvs(alpha, size=size, random_state=rng)\n",
    "    out = np.empty((size, alpha.size), dtype=int)\n",
    "    for i, p in enumerate(ps):\n",
    "        out[i] = rng.multinomial(n, p)\n",
    "    return out\n",
    "\n",
    "\n",
    "alpha = np.array([1.0, 2.0, 3.0])\n",
    "n = 10\n",
    "samples = dirichlet_multinomial_rvs_scipy(alpha=alpha, n=n, size=5, rng=rng)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fce65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (MLE): optimize the Dirichlet–multinomial log-likelihood for alpha\n",
    "\n",
    "def dm_loglik(alpha, X: np.ndarray) -> float:\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    X = _validate_counts(X, k=alpha.size)\n",
    "    n_vec = X.sum(axis=1)\n",
    "    alpha0 = alpha.sum()\n",
    "\n",
    "    # omit multinomial coefficient terms (constants wrt alpha)\n",
    "    ll = (\n",
    "        X.shape[0] * gammaln(alpha0)\n",
    "        - np.sum(gammaln(alpha0 + n_vec))\n",
    "        + np.sum(gammaln(alpha + X) - gammaln(alpha), axis=1).sum()\n",
    "    )\n",
    "    return float(ll)\n",
    "\n",
    "\n",
    "def dm_loglik_grad(alpha, X: np.ndarray) -> np.ndarray:\n",
    "    alpha = _validate_alpha(alpha)\n",
    "    X = _validate_counts(X, k=alpha.size)\n",
    "    n_vec = X.sum(axis=1)\n",
    "    alpha0 = alpha.sum()\n",
    "\n",
    "    m = X.shape[0]\n",
    "    common = m * digamma(alpha0) - np.sum(digamma(alpha0 + n_vec))\n",
    "    grad = common + np.sum(digamma(alpha + X), axis=0) - m * digamma(alpha)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def dm_mom_alpha_init(X: np.ndarray) -> np.ndarray:\n",
    "    '''Method-of-moments-ish initializer for alpha (works best when n is constant).'''\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.atleast_2d(X)\n",
    "\n",
    "    n_vec = X.sum(axis=1)\n",
    "    if not np.allclose(n_vec, n_vec[0]):\n",
    "        # fall back: mean proportions with moderate concentration\n",
    "        p_hat = X.sum(axis=0) / X.sum()\n",
    "        return 20.0 * p_hat\n",
    "\n",
    "    n = float(n_vec[0])\n",
    "    p_hat = X.mean(axis=0) / n\n",
    "    s2 = X.var(axis=0, ddof=0)\n",
    "\n",
    "    # v_i ≈ Var / (n p(1-p)) = (n+alpha0)/(alpha0+1)\n",
    "    denom = n * p_hat * (1.0 - p_hat)\n",
    "    usable = denom > 1e-12\n",
    "    v = np.median((s2[usable] / denom[usable]).clip(min=1.0)) if np.any(usable) else 1.0\n",
    "\n",
    "    if v <= 1.0 + 1e-8:\n",
    "        alpha0 = 1e3\n",
    "    else:\n",
    "        alpha0 = (n - v) / (v - 1.0)\n",
    "        alpha0 = float(np.clip(alpha0, 1e-3, 1e4))\n",
    "\n",
    "    return alpha0 * p_hat\n",
    "\n",
    "\n",
    "def fit_dirichlet_multinomial_mle(X: np.ndarray, alpha_init: np.ndarray | None = None) -> np.ndarray:\n",
    "    X = _validate_counts(X, k=np.asarray(X).shape[-1])\n",
    "    k = X.shape[1]\n",
    "\n",
    "    if alpha_init is None:\n",
    "        alpha_init = dm_mom_alpha_init(X)\n",
    "    alpha_init = np.asarray(alpha_init, dtype=float)\n",
    "    if alpha_init.shape != (k,):\n",
    "        raise ValueError(f\"alpha_init must have shape ({k},)\")\n",
    "\n",
    "    # optimize over log(alpha) to enforce positivity\n",
    "    x0 = np.log(alpha_init)\n",
    "\n",
    "    bounds = [(-10.0, 10.0)] * k  # keeps alpha in a safe numeric range\n",
    "\n",
    "    def obj(log_alpha):\n",
    "        a = np.exp(log_alpha)\n",
    "        return -dm_loglik(a, X)\n",
    "\n",
    "    def grad(log_alpha):\n",
    "        a = np.exp(log_alpha)\n",
    "        return -(dm_loglik_grad(a, X) * a)  # chain rule\n",
    "\n",
    "    res = minimize(obj, x0=x0, jac=grad, method='L-BFGS-B', bounds=bounds)\n",
    "    if not res.success:\n",
    "        raise RuntimeError(f\"MLE optimization failed: {res.message}\")\n",
    "    return np.exp(res.x)\n",
    "\n",
    "\n",
    "# Demo: simulate + fit\n",
    "rng_fit = np.random.default_rng(0)\n",
    "\n",
    "alpha_true = np.array([1.2, 2.5, 4.0])\n",
    "n = 20\n",
    "m = 300\n",
    "\n",
    "X = dirichlet_multinomial_rvs_numpy(alpha=alpha_true, n=n, size=m, rng=rng_fit)\n",
    "alpha_hat = fit_dirichlet_multinomial_mle(X)\n",
    "\n",
    "alpha_true, alpha_hat, alpha_hat / alpha_hat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec84518",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 10) Statistical Use Cases\n",
    "\n",
    "            ### 1) Hypothesis testing: multinomial vs overdispersed counts\n",
    "            A common question is whether a plain multinomial is *too restrictive*.\n",
    "            You can compare:\n",
    "\n",
    "            - **$H_0$**: $X \\sim \\mathrm{Multinomial}(n, p)$ (fixed $p$)\n",
    "            - **$H_1$**: $X \\sim \\mathrm{DirichletMultinomial}(n, \\alpha)$ (random $p$)\n",
    "\n",
    "            A likelihood ratio statistic can be used, but the usual $\\chi^2$ reference is unreliable because the multinomial is a boundary case (roughly $\\alpha_0\\to\\infty$).\n",
    "            A practical approach is a **parametric bootstrap** under $H_0$.\n",
    "\n",
    "            ### 2) Bayesian modeling: posterior predictive\n",
    "            If you place a Dirichlet prior on multinomial probabilities, the posterior is Dirichlet and the **posterior predictive** for new counts is Dirichlet–multinomial.\n",
    "\n",
    "            ### 3) Generative modeling\n",
    "            Dirichlet–multinomial is a natural “bag-of-words” generator: it samples a document-level word distribution and then generates word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18216510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian modeling: Dirichlet posterior + Dirichlet–multinomial posterior predictive\n",
    "\n",
    "alpha_prior = np.array([1.0, 1.0, 1.0])\n",
    "x_obs = np.array([4, 1, 5])\n",
    "\n",
    "alpha_post = alpha_prior + x_obs\n",
    "\n",
    "print('prior mean p:', alpha_prior / alpha_prior.sum())\n",
    "print('posterior mean p:', alpha_post / alpha_post.sum())\n",
    "\n",
    "# Posterior predictive for future n_new counts\n",
    "n_new = 12\n",
    "x_future = np.array([3, 5, 4])\n",
    "p_pred = dirichlet_multinomial_pmf(x_future, alpha=alpha_post, n=n_new)\n",
    "p_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dab65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing demo: parametric bootstrap LRT (small example)\n",
    "\n",
    "def multinomial_loglik(X: np.ndarray, p: np.ndarray) -> float:\n",
    "    X = _validate_counts(X, k=p.size)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if p.ndim != 1 or p.size != X.shape[1]:\n",
    "        raise ValueError('p must be shape (k,)')\n",
    "    if np.any(p <= 0):\n",
    "        raise ValueError('p must be strictly positive (use smoothing if needed)')\n",
    "    p = p / p.sum()\n",
    "\n",
    "    n_vec = X.sum(axis=1)\n",
    "    ll = (\n",
    "        gammaln(n_vec + 1)\n",
    "        - np.sum(gammaln(X + 1), axis=1)\n",
    "        + (X * np.log(p)).sum(axis=1)\n",
    "    ).sum()\n",
    "    return float(ll)\n",
    "\n",
    "\n",
    "def lrt_statistic(X: np.ndarray) -> tuple[float, np.ndarray, np.ndarray]:\n",
    "    X = _validate_counts(X, k=np.asarray(X).shape[-1])\n",
    "    n_vec = X.sum(axis=1)\n",
    "    if not np.all(n_vec == n_vec[0]):\n",
    "        raise ValueError('This demo assumes constant n across rows')\n",
    "\n",
    "    # H0: multinomial MLE for p\n",
    "    p_hat = X.sum(axis=0) / X.sum()\n",
    "    p_hat = (p_hat + 1e-12) / (p_hat.sum() + 1e-12 * p_hat.size)\n",
    "\n",
    "    ll0 = multinomial_loglik(X, p_hat)\n",
    "\n",
    "    # H1: Dirichlet–multinomial MLE for alpha\n",
    "    alpha_hat = fit_dirichlet_multinomial_mle(X)\n",
    "    ll1 = dm_loglik(alpha_hat, X)\n",
    "\n",
    "    return 2.0 * (ll1 - ll0), p_hat, alpha_hat\n",
    "\n",
    "\n",
    "rng_test = np.random.default_rng(123)\n",
    "\n",
    "# Simulate an overdispersed dataset under H1\n",
    "alpha_true = np.array([1.2, 2.5, 4.0])\n",
    "n = 20\n",
    "m = 80\n",
    "X = dirichlet_multinomial_rvs_numpy(alpha=alpha_true, n=n, size=m, rng=rng_test)\n",
    "\n",
    "lrt_obs, p_hat_obs, alpha_hat_obs = lrt_statistic(X)\n",
    "print('Observed LRT:', lrt_obs)\n",
    "print('alpha_hat:', alpha_hat_obs)\n",
    "\n",
    "# Bootstrap under H0 (multinomial)\n",
    "B = 30\n",
    "lrt_boot = []\n",
    "for _ in range(B):\n",
    "    Xb = rng_test.multinomial(n, p_hat_obs, size=m)\n",
    "    stat, _, _ = lrt_statistic(Xb)\n",
    "    lrt_boot.append(stat)\n",
    "\n",
    "lrt_boot = np.array(lrt_boot)\n",
    "p_value = float(np.mean(lrt_boot >= lrt_obs))\n",
    "\n",
    "print('bootstrap LRT mean:', lrt_boot.mean())\n",
    "print('bootstrap p-value (rough, small B):', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17607712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling example: \"documents\" as category-count vectors\n",
    "\n",
    "alpha_topic = np.array([0.4, 0.4, 0.4])  # sparse-ish p for each document\n",
    "n_words = 60\n",
    "n_docs = 200\n",
    "\n",
    "docs = dirichlet_multinomial_rvs_numpy(alpha=alpha_topic, n=n_words, size=n_docs, rng=rng)\n",
    "\n",
    "# Visualize document-level proportions\n",
    "props = docs / docs.sum(axis=1, keepdims=True)\n",
    "fig = px.scatter_3d(\n",
    "    x=props[:, 0], y=props[:, 1], z=props[:, 2],\n",
    "    title=\"Document-level proportions (Dirichlet–multinomial generator)\",\n",
    "    labels={'x': 'p1', 'y': 'p2', 'z': 'p3'}\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3, opacity=0.6))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a83b85",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 11) Pitfalls\n",
    "\n",
    "            - **Invalid parameters**:\n",
    "              - $\\alpha_i$ must be strictly positive.\n",
    "              - $x_i$ must be nonnegative integers and must satisfy $\\sum_i x_i = n$.\n",
    "\n",
    "            - **Numerical issues**:\n",
    "              - PMFs can underflow quickly when $n$ is large. Prefer `logpmf` and compute in log-space.\n",
    "              - Use `gammaln` / `digamma` rather than `gamma` / factorials.\n",
    "\n",
    "            - **Combinatorial explosion**:\n",
    "              - The support size is $\\binom{n+K-1}{K-1}$.\n",
    "              - Exact enumeration (for entropy, CDF, full PMF plots) is only feasible for small $n$ and moderate $K$.\n",
    "\n",
    "            - **Fitting**:\n",
    "              - The multinomial is a limiting case ($\\alpha_0\\to\\infty$). In near-multinomial data, MLE may push $\\alpha$ very large.\n",
    "              - Use good initialization and consider bounds / regularization if optimization is unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104c473",
   "metadata": {},
   "source": [
    "\\\n",
    "            ## 12) Summary\n",
    "\n",
    "            - The Dirichlet–multinomial is the **posterior predictive** distribution for multinomial counts with a Dirichlet prior.\n",
    "            - It models **overdispersed** multinomial counts by letting the category probabilities vary across replicates.\n",
    "            - Mean proportions are $\\alpha/\\alpha_0$; total concentration $\\alpha_0$ controls dispersion.\n",
    "            - PMF evaluation is stable in log-space via Gamma functions.\n",
    "            - Exact CDF/entropy require summation over a combinatorial support; for larger problems use Monte Carlo or approximations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}