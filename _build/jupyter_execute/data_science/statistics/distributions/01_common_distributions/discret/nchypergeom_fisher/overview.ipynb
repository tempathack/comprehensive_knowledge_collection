{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad29f26f",
   "metadata": {},
   "source": [
    "# `nchypergeom_fisher` (Fisher’s noncentral hypergeometric distribution)\n",
    "\n",
    "The Fisher noncentral hypergeometric distribution models **biased sampling without replacement** from a population containing **two types** of items.\n",
    "\n",
    "This notebook uses the same parameterization as `scipy.stats.nchypergeom_fisher`:\n",
    "- `M` = total population size\n",
    "- `n` = number of **Type I** items in the population\n",
    "- `N` = number of draws (sample size, without replacement)\n",
    "- `odds` = odds ratio (>0) favoring Type I over Type II\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- state the support and write the PMF/CDF\n",
    "- understand how the odds ratio **tilts** a hypergeometric distribution\n",
    "- compute moments via the log-partition function (and numerically via the PMF)\n",
    "- sample from the distribution with a **NumPy-only inverse-CDF** method\n",
    "- connect `nchypergeom_fisher` to **2×2 contingency tables** and exact inference on odds ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5d348",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name**: `nchypergeom_fisher` (Fisher’s noncentral hypergeometric distribution)  \n",
    "**Type**: **Discrete**  \n",
    "\n",
    "**Support**:\n",
    "Let:\n",
    "- `M` be the total population size\n",
    "- `n` be the number of Type I items (so `M-n` are Type II)\n",
    "- `N` be the number of draws (without replacement)\n",
    "\n",
    "If \\(X\\) is the number of Type I items drawn, then:\n",
    "\n",
    "\\[\n",
    "X \\in \\{x_{\\min}, x_{\\min}+1, \\dots, x_{\\max}\\},\\qquad\n",
    "x_{\\min} = \\max\\bigl(0,\\, N-(M-n)\\bigr),\\quad\n",
    "x_{\\max} = \\min(n, N).\n",
    "\\]\n",
    "\n",
    "**Parameter space**:\n",
    "\\[\n",
    "M \\in \\{0,1,2,\\dots\\},\\quad\n",
    "n\\in\\{0,\\dots,M\\},\\quad\n",
    "N\\in\\{0,\\dots,M\\},\\quad\n",
    "\\text{odds} > 0.\n",
    "\\]\n",
    "\n",
    "Interpretation:\n",
    "- `M` sets the urn size.\n",
    "- `n` is how many Type I (“success”) items exist in the population.\n",
    "- `N` is how many items you draw.\n",
    "- `odds` controls the bias toward Type I (\\(\\text{odds}=1\\) recovers the usual hypergeometric distribution).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a38a3",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "You have an urn with two types of objects:\n",
    "- Type I: \\(n\\) objects\n",
    "- Type II: \\(M-n\\) objects\n",
    "\n",
    "You draw \\(N\\) objects **without replacement**, but the draw is **biased**.\n",
    "A convenient way to define Fisher’s version is:\n",
    "\n",
    "> each possible sample composition is weighted by an **odds ratio** \\(\\omega\\) (“odds”) raised to the number of Type I objects in that sample.\n",
    "\n",
    "So if a sample contains \\(x\\) Type I objects (and \\(N-x\\) Type II objects), it gets a weight proportional to \\(\\omega^x\\), on top of the usual combinatorial counts.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **2×2 contingency tables / case–control studies**: conditional inference about an odds ratio given fixed margins\n",
    "- **Biased audits / inspections**: a fixed-size sample is collected, but certain types are more likely to appear\n",
    "- **Ecology / genetics**: sampling a fixed number of individuals where types have different sampling propensities\n",
    "\n",
    "### Relations to other distributions\n",
    "- If \\(\\omega = 1\\), this reduces to the **hypergeometric** distribution (unbiased sampling without replacement).\n",
    "- If the sampling fraction is small (\\(N \\ll M\\)), it is often well-approximated by a **binomial** with a bias-adjusted success probability.\n",
    "- Compare with `scipy.stats.nchypergeom_wallenius` (Wallenius’ noncentral hypergeometric):\n",
    "  - Fisher: the bias is applied to the *final sample composition* (“draw a handful at once”).\n",
    "  - Wallenius: the bias is applied *sequentially* as you draw items.\n",
    "  - They coincide at \\(\\omega=1\\) but differ in general.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79177c16",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(X\\) be the number of Type I objects in a biased sample of size \\(N\\).\n",
    "Define the combinatorial term\n",
    "\\[\n",
    "a_x = \\binom{n}{x}\\binom{M-n}{N-x}\n",
    "\\]\n",
    "for integers \\(x\\) in the feasible range.\n",
    "\n",
    "### PMF\n",
    "For \\(x \\in \\{x_{\\min},\\dots,x_{\\max}\\}\\), the PMF is\n",
    "\\[\n",
    "\\Pr(X=x\\mid M,n,N,\\omega)\n",
    "= \\frac{a_x\\,\\omega^x}{Z(\\omega)},\n",
    "\\qquad\n",
    "Z(\\omega)=\\sum_{k=x_{\\min}}^{x_{\\max}} a_k\\,\\omega^k,\n",
    "\\qquad \\omega=\\text{odds}>0.\n",
    "\\]\n",
    "Outside the support, \\(\\Pr(X=x)=0\\).\n",
    "\n",
    "### CDF\n",
    "For a real number \\(t\\), the CDF is the finite sum\n",
    "\\[\n",
    "F(t) = \\Pr(X\\le t) = \\sum_{k=x_{\\min}}^{\\lfloor t\\rfloor} \\Pr(X=k).\n",
    "\\]\n",
    "\n",
    "There is no simple closed form for the CDF in general; it is typically evaluated by summation in a numerically stable way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d32f4",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "A key observation is that Fisher’s noncentral hypergeometric distribution is an **exponential family** in the natural parameter\n",
    "\\(\\theta = \\log \\omega\\).\n",
    "\n",
    "Define the log-partition function\n",
    "\\[\n",
    "A(\\theta) = \\log Z(\\theta),\\qquad\n",
    "Z(\\theta) = \\sum_{k=x_{\\min}}^{x_{\\max}} a_k\\,e^{\\theta k}.\n",
    "\\]\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "Derivatives of \\(A(\\theta)\\) give **cumulants**:\n",
    "\\[\n",
    "\\mathbb{E}[X] = A'(\\theta),\\qquad\n",
    "\\mathrm{Var}(X) = A''(\\theta).\n",
    "\\]\n",
    "More generally, the \\(r\\)-th cumulant is \\(\\kappa_r = A^{(r)}(\\theta)\\). Then\n",
    "\\[\n",
    "\\text{skew}(X) = \\frac{\\kappa_3}{\\kappa_2^{3/2}},\n",
    "\\qquad\n",
    "\\text{excess kurt}(X) = \\frac{\\kappa_4}{\\kappa_2^2}.\n",
    "\\]\n",
    "In practice, these are often computed numerically from the PMF.\n",
    "\n",
    "### MGF and characteristic function\n",
    "Using the partition function, the MGF has a clean ratio form:\n",
    "\\[\n",
    "M_X(t)=\\mathbb{E}[e^{tX}] = \\frac{Z(\\theta+t)}{Z(\\theta)} = \\frac{Z(\\omega e^{t})}{Z(\\omega)}.\n",
    "\\]\n",
    "Similarly, the characteristic function is\n",
    "\\[\n",
    "\\varphi_X(t)=\\mathbb{E}[e^{itX}] = \\frac{Z(\\theta+it)}{Z(\\theta)}.\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "The (Shannon) entropy is\n",
    "\\[\n",
    "H(X) = -\\sum_x p(x)\\log p(x).\n",
    "\\]\n",
    "For this family, you can also write\n",
    "\\[\n",
    "H(X) = A(\\theta) - \\theta\\,\\mathbb{E}[X] - \\mathbb{E}[\\log a_X],\n",
    "\\]\n",
    "which is useful conceptually, but still requires numerical evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb01b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_M_n_N(M, n, N):\n",
    "    for name, v in [(\"M\", M), (\"n\", n), (\"N\", N)]:\n",
    "        if isinstance(v, bool) or not isinstance(v, (int, np.integer)):\n",
    "            raise TypeError(f\"{name} must be an integer\")\n",
    "\n",
    "    M = int(M)\n",
    "    n = int(n)\n",
    "    N = int(N)\n",
    "\n",
    "    if M < 0:\n",
    "        raise ValueError(\"M must be >= 0\")\n",
    "    if not (0 <= n <= M):\n",
    "        raise ValueError(\"n must satisfy 0 <= n <= M\")\n",
    "    if not (0 <= N <= M):\n",
    "        raise ValueError(\"N must satisfy 0 <= N <= M\")\n",
    "\n",
    "    return M, n, N\n",
    "\n",
    "\n",
    "def _validate_params(M, n, N, odds):\n",
    "    M, n, N = _validate_M_n_N(M, n, N)\n",
    "\n",
    "    odds = float(odds)\n",
    "    if not np.isfinite(odds) or odds <= 0:\n",
    "        raise ValueError(\"odds must be a positive finite number\")\n",
    "\n",
    "    return M, n, N, odds\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_support(M, n, N) -> np.ndarray:\n",
    "    M, n, N = _validate_M_n_N(M, n, N)\n",
    "    lo = max(0, N - (M - n))\n",
    "    hi = min(n, N)\n",
    "    return np.arange(lo, hi + 1, dtype=int)\n",
    "\n",
    "\n",
    "def _log_factorials(max_n: int) -> np.ndarray:\n",
    "    max_n = int(max_n)\n",
    "    if max_n < 0:\n",
    "        raise ValueError(\"max_n must be >= 0\")\n",
    "\n",
    "    logfact = np.zeros(max_n + 1, dtype=float)\n",
    "    if max_n >= 2:\n",
    "        logfact[2:] = np.cumsum(np.log(np.arange(2, max_n + 1)))\n",
    "    return logfact\n",
    "\n",
    "\n",
    "def _log_choose(n: int, k: np.ndarray, logfact: np.ndarray) -> np.ndarray:\n",
    "    k = np.asarray(k, dtype=int)\n",
    "    return logfact[n] - logfact[k] - logfact[n - k]\n",
    "\n",
    "\n",
    "def _logsumexp(a: np.ndarray):\n",
    "    \"\"\"Stable log(sum(exp(a))) for real or complex a.\"\"\"\n",
    "    a = np.asarray(a)\n",
    "    m = np.max(a.real)\n",
    "    return m + np.log(np.sum(np.exp(a - m)))\n",
    "\n",
    "\n",
    "def _nchgf_loga(M, n, N):\n",
    "    \"\"\"Return support x and log a_x = log[C(n,x) C(M-n, N-x)].\"\"\"\n",
    "    x = nchypergeom_fisher_support(M, n, N)\n",
    "    logfact = _log_factorials(M)\n",
    "    loga = _log_choose(n, x, logfact) + _log_choose(M - n, N - x, logfact)\n",
    "    return x, loga\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_logZ_theta(M, n, N, theta):\n",
    "    M, n, N = _validate_M_n_N(M, n, N)\n",
    "    x, loga = _nchgf_loga(M, n, N)\n",
    "    return _logsumexp(loga + theta * x)\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_logZ(M, n, N, odds):\n",
    "    M, n, N, odds = _validate_params(M, n, N, odds)\n",
    "    return nchypergeom_fisher_logZ_theta(M, n, N, np.log(odds))\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_logpmf_array(M, n, N, odds):\n",
    "    M, n, N, odds = _validate_params(M, n, N, odds)\n",
    "\n",
    "    x, loga = _nchgf_loga(M, n, N)\n",
    "\n",
    "    theta = np.log(odds)\n",
    "    logw = loga + theta * x\n",
    "    logZ = _logsumexp(logw)\n",
    "\n",
    "    return x, logw - logZ\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_logpmf(k, M, n, N, odds):\n",
    "    \"\"\"Log-PMF evaluated at integer k; non-integers return -inf.\"\"\"\n",
    "    M, n, N, odds = _validate_params(M, n, N, odds)\n",
    "    k_arr = np.asarray(k)\n",
    "    k_int = k_arr.astype(int)\n",
    "\n",
    "    xs, logp = nchypergeom_fisher_logpmf_array(M, n, N, odds)\n",
    "    lo, hi = int(xs[0]), int(xs[-1])\n",
    "\n",
    "    out = np.full_like(k_arr, -np.inf, dtype=float)\n",
    "    inside = (k_arr == k_int) & (k_int >= lo) & (k_int <= hi)\n",
    "    if np.any(inside):\n",
    "        out[inside] = logp[k_int[inside] - lo]\n",
    "    return out\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_pmf_array(M, n, N, odds):\n",
    "    xs, logp = nchypergeom_fisher_logpmf_array(M, n, N, odds)\n",
    "    p = np.exp(logp)\n",
    "    p = p / p.sum()\n",
    "    return xs, p\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_pmf(k, M, n, N, odds):\n",
    "    return np.exp(nchypergeom_fisher_logpmf(k, M, n, N, odds))\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_cdf_array(M, n, N, odds):\n",
    "    xs, p = nchypergeom_fisher_pmf_array(M, n, N, odds)\n",
    "    cdf = np.cumsum(p)\n",
    "    cdf[-1] = 1.0\n",
    "    return xs, cdf\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_cdf(x, M, n, N, odds):\n",
    "    M, n, N, odds = _validate_params(M, n, N, odds)\n",
    "    x_arr = np.asarray(x)\n",
    "\n",
    "    xs = nchypergeom_fisher_support(M, n, N)\n",
    "    lo, hi = int(xs[0]), int(xs[-1])\n",
    "\n",
    "    _, cdf = nchypergeom_fisher_cdf_array(M, n, N, odds)\n",
    "\n",
    "    k = np.floor(x_arr).astype(int)\n",
    "    out = np.zeros_like(x_arr, dtype=float)\n",
    "    out[k >= hi] = 1.0\n",
    "\n",
    "    inside = (k >= lo) & (k < hi)\n",
    "    if np.any(inside):\n",
    "        out[inside] = cdf[k[inside] - lo]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_stats(M, n, N, odds):\n",
    "    xs, p = nchypergeom_fisher_pmf_array(M, n, N, odds)\n",
    "\n",
    "    mean = float(np.sum(xs * p))\n",
    "    var = float(np.sum((xs - mean) ** 2 * p))\n",
    "\n",
    "    if var == 0.0:\n",
    "        skew = 0.0\n",
    "        kurt_excess = 0.0\n",
    "    else:\n",
    "        std = np.sqrt(var)\n",
    "        skew = float(np.sum((xs - mean) ** 3 * p) / std**3)\n",
    "        kurt_excess = float(np.sum((xs - mean) ** 4 * p) / std**4 - 3.0)\n",
    "\n",
    "    entropy_nats = float(-np.sum(p * np.log(p)))\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"var\": var,\n",
    "        \"skew\": skew,\n",
    "        \"kurt_excess\": kurt_excess,\n",
    "        \"entropy_nats\": entropy_nats,\n",
    "    }\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_log_mgf(t, M, n, N, odds):\n",
    "    M, n, N, odds = _validate_params(M, n, N, odds)\n",
    "    theta = np.log(odds)\n",
    "    return nchypergeom_fisher_logZ_theta(M, n, N, theta + t) - nchypergeom_fisher_logZ_theta(M, n, N, theta)\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_mgf(t, M, n, N, odds):\n",
    "    return np.exp(nchypergeom_fisher_log_mgf(t, M, n, N, odds))\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_log_cf(t, M, n, N, odds):\n",
    "    M, n, N, odds = _validate_params(M, n, N, odds)\n",
    "    theta = np.log(odds)\n",
    "    return nchypergeom_fisher_logZ_theta(M, n, N, theta + 1j * t) - nchypergeom_fisher_logZ_theta(M, n, N, theta)\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_cf(t, M, n, N, odds):\n",
    "    return np.exp(nchypergeom_fisher_log_cf(t, M, n, N, odds))\n",
    "\n",
    "\n",
    "def nchypergeom_fisher_mean_from_logodds(M, n, N, logodds: float) -> float:\n",
    "    M, n, N = _validate_M_n_N(M, n, N)\n",
    "    xs, loga = _nchgf_loga(M, n, N)\n",
    "\n",
    "    logw = loga + logodds * xs\n",
    "    logp = logw - _logsumexp(logw)\n",
    "    p = np.exp(logp)\n",
    "\n",
    "    return float(np.sum(xs * p))\n",
    "\n",
    "\n",
    "def fit_nchypergeom_fisher_odds_mle(data, M, n, N, *, max_iter=80, tol=1e-10) -> float:\n",
    "    # Conditional MLE of odds with (M,n,N) fixed.\n",
    "    # For Fisher's NCHG, the MLE solves E_theta[X] = sample_mean.\n",
    "\n",
    "    M, n, N = _validate_M_n_N(M, n, N)\n",
    "\n",
    "    data = np.asarray(data)\n",
    "    if not np.issubdtype(data.dtype, np.integer):\n",
    "        data_int = data.astype(int)\n",
    "        if np.any(data_int != data):\n",
    "            raise ValueError(\"data must be integer-valued\")\n",
    "        data = data_int\n",
    "\n",
    "    xs = nchypergeom_fisher_support(M, n, N)\n",
    "    lo, hi = int(xs[0]), int(xs[-1])\n",
    "\n",
    "    if np.any(data < lo) or np.any(data > hi):\n",
    "        raise ValueError(\"data outside the support\")\n",
    "\n",
    "    target = float(np.mean(data))\n",
    "\n",
    "    # If the sample mean hits the boundary, the MLE is at odds -> 0 or odds -> +inf.\n",
    "    if target <= lo + 1e-12:\n",
    "        return 1e-12\n",
    "    if target >= hi - 1e-12:\n",
    "        return 1e12\n",
    "\n",
    "    theta_lo, theta_hi = -10.0, 10.0\n",
    "    m_lo = nchypergeom_fisher_mean_from_logodds(M, n, N, theta_lo)\n",
    "    m_hi = nchypergeom_fisher_mean_from_logodds(M, n, N, theta_hi)\n",
    "\n",
    "    # Expand the bracket if needed.\n",
    "    while m_lo > target:\n",
    "        theta_hi = theta_lo\n",
    "        theta_lo -= 10.0\n",
    "        m_lo = nchypergeom_fisher_mean_from_logodds(M, n, N, theta_lo)\n",
    "\n",
    "    while m_hi < target:\n",
    "        theta_lo = theta_hi\n",
    "        theta_hi += 10.0\n",
    "        m_hi = nchypergeom_fisher_mean_from_logodds(M, n, N, theta_hi)\n",
    "\n",
    "    # Bisection on theta = log odds.\n",
    "    for _ in range(max_iter):\n",
    "        theta_mid = 0.5 * (theta_lo + theta_hi)\n",
    "        m_mid = nchypergeom_fisher_mean_from_logodds(M, n, N, theta_mid)\n",
    "\n",
    "        if abs(m_mid - target) < tol:\n",
    "            break\n",
    "\n",
    "        if m_mid < target:\n",
    "            theta_lo = theta_mid\n",
    "        else:\n",
    "            theta_hi = theta_mid\n",
    "\n",
    "    return float(np.exp(theta_mid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check + moments\n",
    "\n",
    "M, n, N, odds = 50, 20, 15, 3.0\n",
    "\n",
    "xs, pmf = nchypergeom_fisher_pmf_array(M, n, N, odds)\n",
    "\n",
    "moments = nchypergeom_fisher_stats(M, n, N, odds)\n",
    "\n",
    "# MGF spot-check: ratio-of-partition vs direct sum\n",
    "\n",
    "t = 0.2\n",
    "mgf_ratio = float(nchypergeom_fisher_mgf(t, M, n, N, odds))\n",
    "mgf_direct = float(np.sum(np.exp(t * xs) * pmf))\n",
    "\n",
    "{\n",
    "    \"support\": (int(xs[0]), int(xs[-1])),\n",
    "    \"pmf_sum\": float(pmf.sum()),\n",
    "    \"moments\": moments,\n",
    "    \"mgf_ratio\": mgf_ratio,\n",
    "    \"mgf_direct\": mgf_direct,\n",
    "    \"mgf_abs_diff\": float(abs(mgf_ratio - mgf_direct)),\n",
    "    \"cf_at_1.0\": complex(nchypergeom_fisher_cf(1.0, M, n, N, odds)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4635f",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### Meaning of the parameters\n",
    "- **`M` (population size)**: total number of items.\n",
    "- **`n` (Type I count)**: how many Type I items exist in the population (baseline prevalence \\(n/M\\)).\n",
    "- **`N` (draws)**: sample size drawn without replacement.\n",
    "- **`odds` (odds ratio \\(\\omega\\))**: the *relative* preference for Type I.\n",
    "  - \\(\\omega>1\\) shifts mass toward larger \\(X\\)\n",
    "  - \\(\\omega<1\\) shifts mass toward smaller \\(X\\)\n",
    "  - \\(\\omega=1\\) is unbiased (hypergeometric)\n",
    "\n",
    "### Shape changes\n",
    "- As \\(\\omega\\to 0\\), the distribution concentrates at \\(x_{\\min}\\) (as few Type I draws as possible).\n",
    "- As \\(\\omega\\to \\infty\\), it concentrates at \\(x_{\\max}\\) (as many Type I draws as possible).\n",
    "- The map \\(\\omega \\mapsto \\mathbb{E}[X]\\) is monotone increasing (useful for estimation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d43f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "M, n, N = 60, 20, 15\n",
    "odds_values = [0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        f\"PMF for different odds (M={M}, n={n}, N={N})\",\n",
    "        \"Mean as a function of odds\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Left: PMF curves\n",
    "for o in odds_values:\n",
    "    xs, pmf = nchypergeom_fisher_pmf_array(M, n, N, o)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=xs,\n",
    "            y=pmf,\n",
    "            mode=\"lines+markers\",\n",
    "            name=f\"odds={o}\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"P(X=x)\", row=1, col=1)\n",
    "\n",
    "# Right: mean curve\n",
    "odds_grid = np.logspace(-2, 2, 220)\n",
    "means = np.array([nchypergeom_fisher_stats(M, n, N, o)[\"mean\"] for o in odds_grid])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=odds_grid,\n",
    "        y=means,\n",
    "        mode=\"lines\",\n",
    "        name=\"E[X]\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"odds (log scale)\", type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"E[X]\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"How the odds ratio changes the distribution\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0563522",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "Write the PMF in exponential-family form. Let \\(\\theta = \\log \\omega\\) and\n",
    "\\(\n",
    "Z(\\theta)=\\sum_x a_x e^{\\theta x},\\; A(\\theta)=\\log Z(\\theta).\n",
    "\\)\n",
    "Then\n",
    "\\[\n",
    "\\Pr(X=x) = \\exp\\bigl(\\log a_x + \\theta x - A(\\theta)\\bigr).\n",
    "\\]\n",
    "\n",
    "### Expectation\n",
    "Differentiate \\(A(\\theta)\\):\n",
    "\\[\n",
    "A'(\\theta)\n",
    "= \\frac{Z'(\\theta)}{Z(\\theta)}\n",
    "= \\frac{\\sum_x a_x x e^{\\theta x}}{\\sum_x a_x e^{\\theta x}}\n",
    "= \\sum_x x\\,\\Pr(X=x)\n",
    "= \\mathbb{E}[X].\n",
    "\\]\n",
    "\n",
    "### Variance\n",
    "Differentiate again:\n",
    "\\[\n",
    "A''(\\theta) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathrm{Var}(X).\n",
    "\\]\n",
    "\n",
    "### Likelihood (odds ratio)\n",
    "With fixed \\((M,n,N)\\), treat \\(\\omega\\) (or \\(\\theta=\\log\\omega\\)) as the parameter.\n",
    "\n",
    "For a single observation \\(X=x\\), the likelihood is\n",
    "\\[\n",
    "L(\\omega\\mid x) \\propto \\omega^x\\, /\\, Z(\\omega),\n",
    "\\]\n",
    "and the log-likelihood (in \\(\\theta\\)) is\n",
    "\\[\n",
    "\\ell(\\theta) = \\theta x - A(\\theta) + \\text{const}.\n",
    "\\]\n",
    "\n",
    "For i.i.d. data \\(x_1,\\dots,x_m\\):\n",
    "\\[\n",
    "\\ell(\\theta) = \\theta\\sum_{i=1}^m x_i - m A(\\theta) + \\text{const}.\n",
    "\\]\n",
    "The score equation is\n",
    "\\[\n",
    "\\frac{\\partial\\ell}{\\partial\\theta} = \\sum_{i=1}^m x_i - m\\,\\mathbb{E}_\\theta[X]=0\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\mathbb{E}_\\theta[X] = \\bar{x}.\n",
    "\\]\n",
    "So the conditional MLE for \\(\\omega\\) solves “model mean = sample mean”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe0ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the likelihood for odds (single observation)\n",
    "\n",
    "M, n, N = 50, 20, 15\n",
    "x_obs = 10\n",
    "\n",
    "odds_grid = np.logspace(-2, 2, 500)\n",
    "logL = np.array([float(nchypergeom_fisher_logpmf(x_obs, M, n, N, o)) for o in odds_grid])\n",
    "\n",
    "odds_hat = fit_nchypergeom_fisher_odds_mle(np.array([x_obs]), M, n, N)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=odds_grid, y=logL, mode=\"lines\", name=\"log-likelihood\"))\n",
    "fig.add_vline(\n",
    "    x=odds_hat,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=f\"MLE oddŝ={odds_hat:.3f}\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Fisher NCHG log-likelihood for odds (M,n,N fixed)\",\n",
    "    xaxis_title=\"odds\",\n",
    "    yaxis_title=\"log L(odds)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"x_obs\": x_obs,\n",
    "    \"odds_hat\": odds_hat,\n",
    "    \"E[X] at odds_hat\": nchypergeom_fisher_stats(M, n, N, odds_hat)[\"mean\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d367d5b",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "Because the support is finite, a clean **NumPy-only** approach is inverse transform sampling.\n",
    "\n",
    "### Inverse CDF algorithm\n",
    "1. Compute log-weights\n",
    "   \\(\\log w_x = \\log\\binom{n}{x} + \\log\\binom{M-n}{N-x} + x\\log\\omega\\).\n",
    "2. Normalize to get the PMF \\(p(x)\\) (use log-sum-exp for stability).\n",
    "3. Compute the discrete CDF \\(F(x)=\\sum_{k\\le x} p(k)\\).\n",
    "4. Draw \\(U\\sim\\mathrm{Uniform}(0,1)\\) and return the smallest \\(x\\) with \\(F(x)\\ge U\\).\n",
    "\n",
    "Cost: \\(O(|\\text{support}| + \\text{size}\\cdot\\log|\\text{support}|)\\).\n",
    "For large parameters, production implementations use more specialized methods (SciPy provides one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c93bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nchypergeom_fisher_inverse_cdf(M, n, N, odds, size=1, *, rng: np.random.Generator):\n",
    "    M, n, N, odds = _validate_params(M, n, N, odds)\n",
    "\n",
    "    xs, pmf = nchypergeom_fisher_pmf_array(M, n, N, odds)\n",
    "    if len(xs) == 1:\n",
    "        return np.full(size, xs[0], dtype=int)\n",
    "\n",
    "    cdf = np.cumsum(pmf)\n",
    "    cdf[-1] = 1.0\n",
    "\n",
    "    u = rng.random(size=size)\n",
    "    idx = np.searchsorted(cdf, u, side=\"right\")\n",
    "    return xs[idx]\n",
    "\n",
    "\n",
    "# Monte Carlo check\n",
    "M, n, N, odds = 60, 20, 15, 2.5\n",
    "mom = nchypergeom_fisher_stats(M, n, N, odds)\n",
    "\n",
    "samples = sample_nchypergeom_fisher_inverse_cdf(M, n, N, odds, size=200_000, rng=rng)\n",
    "\n",
    "{\n",
    "    \"theory_mean\": mom[\"mean\"],\n",
    "    \"mc_mean\": float(samples.mean()),\n",
    "    \"theory_var\": mom[\"var\"],\n",
    "    \"mc_var\": float(samples.var(ddof=0)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322ddd3",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** on its integer support\n",
    "- the **CDF** (a step function)\n",
    "- Monte Carlo samples vs the exact PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, n, N, odds = 50, 20, 15, 3.0\n",
    "\n",
    "xs, pmf = nchypergeom_fisher_pmf_array(M, n, N, odds)\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=xs, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(\n",
    "    title=f\"Fisher NCHG PMF (M={M}, n={n}, N={N}, odds={odds})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X=x)\",\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=xs, y=cdf, mode=\"lines\", line_shape=\"hv\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(\n",
    "    title=f\"Fisher NCHG CDF (M={M}, n={n}, N={N}, odds={odds})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X≤x)\",\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "mc = sample_nchypergeom_fisher_inverse_cdf(M, n, N, odds, size=250_000, rng=rng)\n",
    "lo = int(xs[0])\n",
    "counts = np.bincount(mc - lo, minlength=len(xs))\n",
    "pmf_hat = counts / counts.sum()\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=xs, y=pmf_hat, name=\"Monte Carlo\", opacity=0.6))\n",
    "fig_mc.add_trace(go.Scatter(x=xs, y=pmf, mode=\"lines+markers\", name=\"Exact PMF\"))\n",
    "fig_mc.update_layout(\n",
    "    title=\"Monte Carlo samples vs exact PMF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability\",\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241e9f1",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides a fast, numerically robust implementation via `scipy.stats.nchypergeom_fisher`.\n",
    "\n",
    "- Use `pmf`, `cdf`, `sf`, `rvs`, `logpmf`, …\n",
    "- `rv_discrete` distributions do **not** expose a generic `.fit()` method (SciPy 1.15).\n",
    "  For Fisher’s NCHG, you typically fit `odds` with custom likelihood-based code while treating `(M,n,N)` as fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d11393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Compare NumPy implementation to SciPy\n",
    "M, n, N, odds = 50, 20, 15, 3.0\n",
    "xs = nchypergeom_fisher_support(M, n, N)\n",
    "\n",
    "pmf_np = nchypergeom_fisher_pmf(xs, M, n, N, odds)\n",
    "cdf_np = nchypergeom_fisher_cdf(xs, M, n, N, odds)\n",
    "\n",
    "pmf_sp = stats.nchypergeom_fisher.pmf(xs, M, n, N, odds)\n",
    "cdf_sp = stats.nchypergeom_fisher.cdf(xs, M, n, N, odds)\n",
    "\n",
    "mean_sp, var_sp, skew_sp, kurt_sp = stats.nchypergeom_fisher.stats(M, n, N, odds, moments=\"mvsk\")\n",
    "ent_sp = stats.nchypergeom_fisher.entropy(M, n, N, odds)\n",
    "\n",
    "{\n",
    "    \"max_abs_pmf_diff\": float(np.max(np.abs(pmf_np - pmf_sp))),\n",
    "    \"max_abs_cdf_diff\": float(np.max(np.abs(cdf_np - cdf_sp))),\n",
    "    \"numpy_moments\": nchypergeom_fisher_stats(M, n, N, odds),\n",
    "    \"scipy_mean\": float(mean_sp),\n",
    "    \"scipy_var\": float(var_sp),\n",
    "    \"scipy_skew\": float(skew_sp),\n",
    "    \"scipy_kurt_excess\": float(kurt_sp),\n",
    "    \"scipy_entropy_nats\": float(ent_sp),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Fit\" odds with (M,n,N) fixed: conditional MLE\n",
    "\n",
    "M, n, N, odds_true = 80, 25, 20, 4.0\n",
    "\n",
    "data = stats.nchypergeom_fisher.rvs(M, n, N, odds_true, size=5_000, random_state=rng)\n",
    "\n",
    "odds_hat_bisect = fit_nchypergeom_fisher_odds_mle(data, M, n, N)\n",
    "\n",
    "# Optimization check on log-odds (should agree with the MLE condition E[X]=mean)\n",
    "\n",
    "def nll(theta):\n",
    "    odds = float(np.exp(theta))\n",
    "    return -float(stats.nchypergeom_fisher.logpmf(data, M, n, N, odds).sum())\n",
    "\n",
    "res = minimize_scalar(nll, bounds=(-6, 6), method=\"bounded\")\n",
    "odds_hat_opt = float(np.exp(res.x))\n",
    "\n",
    "{\n",
    "    \"odds_true\": odds_true,\n",
    "    \"odds_hat_bisect\": odds_hat_bisect,\n",
    "    \"odds_hat_opt\": odds_hat_opt,\n",
    "    \"opt_success\": bool(res.success),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1fa86",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (conditional test for an odds ratio)\n",
    "For a 2×2 table\n",
    "\n",
    "|            | Exposed | Unexposed |\n",
    "|------------|---------|-----------|\n",
    "| Cases      | \\(a\\)   | \\(b\\)     |\n",
    "| Controls   | \\(c\\)   | \\(d\\)     |\n",
    "\n",
    "conditioning on the margins fixes:\n",
    "- total exposed \\(a+c\\)\n",
    "- total cases \\(a+b\\)\n",
    "- total sample size \\(M=a+b+c+d\\)\n",
    "\n",
    "Then the conditional distribution of \\(A=a\\) given the margins is Fisher’s NCHG with\n",
    "\\[\n",
    "M=a+b+c+d,\\quad n=a+c,\\quad N=a+b,\\quad \\omega=\\text{odds ratio}.\n",
    "\\]\n",
    "The null hypothesis of independence corresponds to \\(\\omega=1\\) (hypergeometric).\n",
    "\n",
    "### B) Bayesian modeling (posterior over odds)\n",
    "With fixed margins, Fisher’s NCHG provides a likelihood for \\(\\omega\\). Combine it with a prior over \\(\\theta=\\log\\omega\\) (e.g., Normal) to get a posterior.\n",
    "\n",
    "### C) Generative modeling (simulate tables with fixed margins)\n",
    "To generate random 2×2 tables with fixed margins and a chosen odds ratio, sample \\(A\\sim\\text{nchypergeom\\_fisher}(M,n,N,\\omega)\\) and fill in the remaining cells deterministically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce846f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Example 2x2 table\n",
    "#            Exposed  Unexposed\n",
    "# Cases         a         b\n",
    "# Controls      c         d\n",
    "\n",
    "a, b, c, d = 12, 5, 8, 15\n",
    "\n",
    "M = a + b + c + d\n",
    "n = a + c       # total exposed\n",
    "N = a + b       # total cases\n",
    "\n",
    "# A) Conditional one-sided p-value under H0: odds=1 (independence)\n",
    "# For \"greater\" association: P(A >= a_obs | odds=1)\n",
    "\n",
    "p_cond_greater = float(stats.nchypergeom_fisher.sf(a - 1, M, n, N, 1.0))\n",
    "\n",
    "oddsratio_hat, p_fisher_greater = fisher_exact([[a, b], [c, d]], alternative=\"greater\")\n",
    "\n",
    "# B) Simple Bayesian posterior on odds via grid on theta = log(odds)\n",
    "\n",
    "a_obs = a\n",
    "sigma = 1.0  # prior std on log-odds\n",
    "\n",
    "theta_grid = np.linspace(-4, 4, 801)\n",
    "odds_grid = np.exp(theta_grid)\n",
    "\n",
    "loglik = np.array([float(nchypergeom_fisher_logpmf(a_obs, M, n, N, o)) for o in odds_grid])\n",
    "logprior = -0.5 * (theta_grid / sigma) ** 2 - np.log(sigma * np.sqrt(2 * np.pi))\n",
    "\n",
    "logpost = loglik + logprior\n",
    "logpost = logpost - np.max(logpost)\n",
    "post = np.exp(logpost)\n",
    "post = post / np.trapz(post, odds_grid)  # density over odds\n",
    "\n",
    "cdf_post = np.cumsum(post) * (odds_grid[1] - odds_grid[0])\n",
    "\n",
    "post_mean = float(np.trapz(odds_grid * post, odds_grid))\n",
    "ci_low = float(odds_grid[np.searchsorted(cdf_post, 0.025)])\n",
    "ci_high = float(odds_grid[np.searchsorted(cdf_post, 0.975)])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=odds_grid, y=post, mode=\"lines\", name=\"posterior density\"))\n",
    "fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"black\", annotation_text=\"odds=1\")\n",
    "fig.update_layout(\n",
    "    title=\"Posterior over odds (log-odds Normal prior; fixed margins)\",\n",
    "    xaxis_title=\"odds\",\n",
    "    yaxis_title=\"posterior density\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# C) Generative: sample a new table with the same margins at a chosen odds\n",
    "\n",
    "def sample_table_with_fixed_margins(M, n, N, odds, *, rng: np.random.Generator):\n",
    "    a_new = int(sample_nchypergeom_fisher_inverse_cdf(M, n, N, odds, size=1, rng=rng)[0])\n",
    "    b_new = N - a_new\n",
    "    c_new = n - a_new\n",
    "    d_new = (M - n) - b_new\n",
    "    return np.array([[a_new, b_new], [c_new, d_new]], dtype=int)\n",
    "\n",
    "odds_sim = 2.5\n",
    "sample_table = sample_table_with_fixed_margins(M, n, N, odds_sim, rng=rng)\n",
    "\n",
    "{\n",
    "    \"margins\": {\"M\": M, \"n_exposed\": n, \"N_cases\": N},\n",
    "    \"p_cond_greater_odds1\": p_cond_greater,\n",
    "    \"fisher_exact_greater\": float(p_fisher_greater),\n",
    "    \"posterior_mean_odds\": post_mean,\n",
    "    \"posterior_95%_CI\": (ci_low, ci_high),\n",
    "    \"sample_table_at_odds_sim\": sample_table.tolist(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43010fac",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter confusion**: SciPy uses `M` (total), `n` (Type I in population), `N` (draws), `odds` (bias). This matches `scipy.stats.hypergeom`’s `(M,n,N)` convention.\n",
    "- **Fisher vs Wallenius**: they correspond to different biased sampling mechanisms. Don’t simulate Fisher’s distribution by sequentially drawing with changing probabilities (that produces Wallenius).\n",
    "- **Invalid parameters**:\n",
    "  - `M`, `n`, `N` must be integers with `0 ≤ n ≤ M` and `0 ≤ N ≤ M`.\n",
    "  - `odds` must be strictly positive.\n",
    "- **Numerical issues**:\n",
    "  - Directly computing \\(\\binom{n}{x}\\) can overflow; use log-space + log-sum-exp.\n",
    "  - Very large `M` makes the simple NumPy log-factorial approach slower; prefer SciPy’s implementation.\n",
    "- **Degenerate cases**: if the support has length 1 (e.g., `N=0` or `n=0`), variance is 0 and standardized skew/kurtosis are not informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc56ab8",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `nchypergeom_fisher` is a **finite-support discrete** distribution for biased sampling without replacement.\n",
    "- The PMF is a **tilted hypergeometric**: \\(p(x) \\propto \\binom{n}{x}\\binom{M-n}{N-x}\\,\\omega^x\\).\n",
    "- Moments and the MGF follow naturally from the **log-partition function** \\(A(\\theta)\\).\n",
    "- A simple NumPy-only sampler uses **inverse CDF** on the finite support.\n",
    "- The distribution is central in **exact conditional inference** for odds ratios in 2×2 tables.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}