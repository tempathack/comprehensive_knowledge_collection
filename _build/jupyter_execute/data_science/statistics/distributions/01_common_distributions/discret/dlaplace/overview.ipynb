{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b6f1c7",
   "metadata": {},
   "source": [
    "# Discrete Laplace: `dlaplace` (two-sided geometric)\n",
    "\n",
    "The **discrete Laplace** distribution is an integer-valued analogue of the continuous Laplace (\\u201cdouble exponential\\u201d): it is symmetric, sharply peaked at its location, and has exponentially decaying tails.\n",
    "\n",
    "In SciPy it appears as `scipy.stats.dlaplace`.\n",
    "\n",
    "## Learning goals\n",
    "- understand what `dlaplace` models and when it is useful\n",
    "- write down the PMF and CDF and connect common parameterizations\n",
    "- derive mean/variance and the maximum-likelihood estimator (MLE)\n",
    "- sample from the distribution with **NumPy only**\n",
    "- visualize PMF/CDF and validate formulas by Monte Carlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81df9a8",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & classification\n",
    "2) Intuition & motivation\n",
    "3) Formal definition (PMF/CDF)\n",
    "4) Moments & properties\n",
    "5) Parameter interpretation\n",
    "6) Derivations (expectation, variance, likelihood)\n",
    "7) Sampling & simulation (NumPy-only)\n",
    "8) Visualization (PMF/CDF + Monte Carlo)\n",
    "9) SciPy integration (`scipy.stats.dlaplace`)\n",
    "10) Statistical use cases\n",
    "11) Pitfalls\n",
    "12) Summary\n",
    "\n",
    "## Prerequisites\n",
    "- basic probability (PMF/CDF, expectation)\n",
    "- comfort with geometric series\n",
    "- familiarity with `numpy` arrays and plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy.stats import chi2, dlaplace, fit\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3a7ea",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Distribution name:** `dlaplace` (Discrete Laplace / two-sided geometric)\n",
    "\n",
    "**Type:** Discrete\n",
    "\n",
    "**Support (standard form):** \\(k \\in \\mathbb{Z}\\)\n",
    "\n",
    "SciPy includes a shift parameter `loc`, so the support becomes a shifted integer lattice:\n",
    "\n",
    "\\[\n",
    "k \\in \\mathrm{loc} + \\mathbb{Z}.\n",
    "\\]\n",
    "\n",
    "**Parameter space (SciPy):**\n",
    "- shape `a > 0`\n",
    "- location `loc \\u2208 \\u211d` (practically: integer shifts keep the support integer-valued)\n",
    "\n",
    "We\\u2019ll also use the reparameterization\n",
    "\n",
    "\\[\n",
    "q = e^{-a} \\in (0, 1),\n",
    "\\]\n",
    "\n",
    "which acts like a *tail ratio*: \\(P(|K|=m+1)/P(|K|=m)=q\\) for \\(m\\ge 0\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b6b2c",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "`dlaplace` models **symmetric integer-valued noise**: deviations of size 1 are common, size 10 are possible but much rarer, and probabilities drop **exponentially** with distance from the center.\n",
    "\n",
    "### What it models\n",
    "- integer measurement error (quantization/rounding + heavy tails)\n",
    "- net count changes (difference of two nonnegative counts)\n",
    "- robust integer residuals (\\u201cmostly small, occasionally big\\u201d)\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Differential privacy** for integer queries (the \\u201cgeometric mechanism\\u201d): add noise with PMF \\(\\propto e^{-\\varepsilon |k|}\\).\n",
    "- **Robust modeling** of rounded residuals: negative log-likelihood is proportional to \\(|k-\\mathrm{loc}|\\), mirroring the \\(\\ell_1\\) loss.\n",
    "- **Signal processing on integers**: priors/penalties on integer differences that encourage sparsity in changes.\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Continuous Laplace** is the difference of two i.i.d. exponentials; **discrete Laplace** is the difference of two i.i.d. geometric random variables.\n",
    "- It is also called the **two-sided geometric** distribution.\n",
    "- With small `a` (so \\(q=e^{-a}\\approx 1\\)) tails are heavy; with large `a` tails die off quickly and mass concentrates at `loc`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6ff69",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(a>0\\) and define \\(q=e^{-a}\\in(0,1)\\). Write\n",
    "\n",
    "\\[\n",
    "c = \\tanh(a/2) = \\frac{1-q}{1+q}.\n",
    "\\]\n",
    "\n",
    "### PMF\n",
    "For \\(K\\in \\mathbb{Z}\\) (standard form, `loc=0`), the probability mass function is\n",
    "\n",
    "\\[\n",
    "P(K=k) = f(k) = \\tanh(a/2)\\,\\exp\\bigl(-a|k|\\bigr) = \\frac{1-q}{1+q}\\,q^{|k|},\\qquad k\\in\\mathbb{Z}.\n",
    "\\]\n",
    "\n",
    "With a location shift `loc`, SciPy uses\n",
    "\n",
    "\\[\n",
    "P(K=k\\mid a,\\mathrm{loc}) = \\tanh(a/2)\\,\\exp\\bigl(-a|k-\\mathrm{loc}|\\bigr),\\qquad k\\in \\mathrm{loc}+\\mathbb{Z}.\n",
    "\\]\n",
    "\n",
    "### CDF (standard form, `loc=0`)\n",
    "Using geometric-series sums, the CDF has a simple closed form. For integer \\(k\\):\n",
    "\n",
    "\\[\n",
    "F(k)=P(K\\le k)=\n",
    "\\begin{cases}\n",
    "\\dfrac{q^{-k}}{1+q}, & k\\le -1,\\\\\n",
    "1-\\\\dfrac{q^{k+1}}{1+q}, & k\\ge 0.\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "For a shifted distribution, replace \\(k\\) by \\(k-\\mathrm{loc}\\) (and remember a discrete CDF is stepwise constant between lattice points).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_a(a: float) -> float:\n",
    "    a = float(a)\n",
    "    if not np.isfinite(a) or a <= 0:\n",
    "        raise ValueError(\"`a` must be a positive, finite number.\")\n",
    "    return a\n",
    "\n",
    "\n",
    "def dlaplace_pmf(k, a: float, loc=0):\n",
    "    \"\"\"PMF of the discrete Laplace distribution in SciPy's parameterization.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The PMF is defined on the shifted integer lattice: k in loc + Z.\n",
    "    - If k-loc is not (approximately) an integer, the PMF is 0.\n",
    "    \"\"\"\n",
    "    a = _check_a(a)\n",
    "    k = np.asarray(k)\n",
    "    x = k - loc\n",
    "    x_round = np.round(x)\n",
    "    is_int = np.isclose(x, x_round)\n",
    "\n",
    "    c = np.tanh(a / 2.0)\n",
    "    out = np.where(is_int, c * np.exp(-a * np.abs(x_round)), 0.0)\n",
    "    return float(out) if out.ndim == 0 else out\n",
    "\n",
    "\n",
    "def dlaplace_logpmf(k, a: float, loc=0):\n",
    "    a = _check_a(a)\n",
    "    k = np.asarray(k)\n",
    "    x = k - loc\n",
    "    x_round = np.round(x)\n",
    "    is_int = np.isclose(x, x_round)\n",
    "\n",
    "    logc = np.log(np.tanh(a / 2.0))\n",
    "    out = np.where(is_int, logc - a * np.abs(x_round), -np.inf)\n",
    "    return float(out) if out.ndim == 0 else out\n",
    "\n",
    "\n",
    "def dlaplace_cdf(k, a: float, loc=0):\n",
    "    \"\"\"CDF in closed form (stepwise constant between lattice points).\"\"\"\n",
    "    a = _check_a(a)\n",
    "    q = np.exp(-a)\n",
    "\n",
    "    k = np.asarray(k)\n",
    "    x = np.floor(k - loc)  # makes the function well-defined for non-integer inputs\n",
    "\n",
    "    out = np.where(x >= 0, 1.0 - (q ** (x + 1)) / (1.0 + q), (q ** (-x)) / (1.0 + q))\n",
    "    return float(out) if out.ndim == 0 else out\n",
    "\n",
    "\n",
    "def dlaplace_moments(a: float, loc=0):\n",
    "    \"\"\"Return mean, variance, skewness, excess kurtosis, entropy (nats).\"\"\"\n",
    "    a = _check_a(a)\n",
    "    q = np.exp(-a)\n",
    "\n",
    "    mean = float(loc)\n",
    "    var = 2.0 * q / (1.0 - q) ** 2\n",
    "    skew = 0.0\n",
    "    kurt_excess = np.cosh(a) + 2.0\n",
    "    entropy = -np.log(np.tanh(a / 2.0)) + a / np.sinh(a)\n",
    "    return mean, var, skew, kurt_excess, entropy\n",
    "\n",
    "\n",
    "def dlaplace_mgf(t, a: float, loc=0):\n",
    "    \"\"\"MGF M(t) = E[e^{tK}] for |t| < a.\"\"\"\n",
    "    a = _check_a(a)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    q = np.exp(-a)\n",
    "\n",
    "    denom = (1.0 - q * np.exp(t)) * (1.0 - q * np.exp(-t))\n",
    "    core = (1.0 - q) ** 2 / denom\n",
    "    out = np.exp(t * loc) * core\n",
    "    return float(out) if out.ndim == 0 else out\n",
    "\n",
    "\n",
    "def dlaplace_cf(w, a: float, loc=0):\n",
    "    \"\"\"Characteristic function phi(w) = E[e^{i w K}] (exists for all real w).\"\"\"\n",
    "    a = _check_a(a)\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    q = np.exp(-a)\n",
    "\n",
    "    denom = 1.0 - 2.0 * q * np.cos(w) + q**2\n",
    "    core = (1.0 - q) ** 2 / denom\n",
    "    out = np.exp(1j * w * loc) * core\n",
    "    return out\n",
    "\n",
    "\n",
    "def dlaplace_rvs_numpy(a: float, loc=0, size=1, rng: np.random.Generator | None = None):\n",
    "    \"\"\"Sample from dlaplace using NumPy only (difference of two geometrics).\"\"\"\n",
    "    a = _check_a(a)\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    q = np.exp(-a)\n",
    "    p = 1.0 - q\n",
    "    g1 = rng.geometric(p, size=size) - 1  # 0-based geometric: P(G=k)=(1-p)^k p\n",
    "    g2 = rng.geometric(p, size=size) - 1\n",
    "    return loc + (g1 - g2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks\n",
    "a = 0.8\n",
    "loc = 0\n",
    "\n",
    "k = np.arange(-200, 201)\n",
    "pmf = dlaplace_pmf(k, a, loc=loc)\n",
    "mass_in_range = pmf.sum()\n",
    "\n",
    "# Tail mass outside [-N, N] has a closed form: P(|K| > N) = 2 q^{N+1} / (1+q)\n",
    "q = np.exp(-a)\n",
    "N = 200\n",
    "tail_mass = 2 * (q ** (N + 1)) / (1 + q)\n",
    "\n",
    "print(f\"Mass in [-{N}, {N}]       = {mass_in_range:.12f}\")\n",
    "print(f\"Tail mass outside range   = {tail_mass:.12e}\")\n",
    "print(f\"Mass + tail (should be 1) = {mass_in_range + tail_mass:.12f}\")\n",
    "\n",
    "# CDF consistency: F(k) - F(k-1) = P(K=k)\n",
    "Fk = dlaplace_cdf(k, a, loc=loc)\n",
    "diff = Fk[1:] - Fk[:-1]\n",
    "max_err = np.max(np.abs(diff - pmf[1:]))\n",
    "print(f\"Max |(F(k)-F(k-1)) - pmf(k)| over grid: {max_err:.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f41c56",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let \\(q=e^{-a}\\). In standard form (`loc=0`):\n",
    "\n",
    "- **Mean:** \\(\\mathbb{E}[K]=0\\) (symmetry). With location: \\(\\mathbb{E}[K]=\\mathrm{loc}\\).\n",
    "- **Variance:**\n",
    "\\[\n",
    "\\mathrm{Var}(K)=\\mathbb{E}[K^2]=\\frac{2q}{(1-q)^2}=\\frac{2e^{-a}}{(1-e^{-a})^2}.\n",
    "\\]\n",
    "- **Skewness:** 0\n",
    "- **(Excess) kurtosis:**\n",
    "\\[\n",
    "\\gamma_2=\\frac{\\mathbb{E}[(K-\\mu)^4]}{\\sigma^4}-3 = \\cosh(a)+2.\n",
    "\\]\n",
    "\n",
    "### MGF and characteristic function\n",
    "- **MGF** \\(M(t)=\\mathbb{E}[e^{tK}]\\) exists for \\(|t|<a\\) and equals\n",
    "\\[\n",
    "M(t)=\\frac{(1-q)^2}{(1-qe^{t})(1-qe^{-t})}.\n",
    "\\]\n",
    "- **Characteristic function** \\(\\varphi(\\omega)=\\mathbb{E}[e^{i\\omega K}]\\) exists for all real \\(\\omega\\):\n",
    "\\[\n",
    "\\varphi(\\omega)=\\frac{(1-q)^2}{1-2q\\cos\\omega+q^2}.\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "The (Shannon) entropy in **nats** is\n",
    "\\[\n",
    "H(K) = -\\sum_k p(k)\\log p(k) = -\\log\\tanh(a/2) + \\frac{a}{\\sinh(a)}.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_moments(x: np.ndarray):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    m = x.mean()\n",
    "    c = x - m\n",
    "    v = np.mean(c**2)\n",
    "    skew = np.mean(c**3) / (v ** 1.5)\n",
    "    kurt_excess = np.mean(c**4) / (v**2) - 3.0\n",
    "    return m, v, skew, kurt_excess\n",
    "\n",
    "\n",
    "a = 0.8\n",
    "loc = 2\n",
    "\n",
    "mean_f, var_f, skew_f, kurt_f, ent_f = dlaplace_moments(a, loc=loc)\n",
    "mean_s, var_s, skew_s, kurt_s = dlaplace.stats(a, loc=loc, moments=\"mvsk\")\n",
    "ent_s = dlaplace.entropy(a, loc=loc)\n",
    "\n",
    "x = dlaplace_rvs_numpy(a, loc=loc, size=200_000, rng=rng)\n",
    "mean_mc, var_mc, skew_mc, kurt_mc = sample_moments(x)\n",
    "\n",
    "print(\"Formula moments:\")\n",
    "print(\"  mean, var, skew, kurt_excess =\", (mean_f, var_f, skew_f, kurt_f))\n",
    "print(f\"  entropy (nats)              = {ent_f:.6f}\")\n",
    "\n",
    "print(\"\\nSciPy moments:\")\n",
    "print(\"  mean, var, skew, kurt_excess =\", (float(mean_s), float(var_s), float(skew_s), float(kurt_s)))\n",
    "print(f\"  entropy (nats)               = {float(ent_s):.6f}\")\n",
    "\n",
    "print(\"\\nMonte Carlo (n=200k):\")\n",
    "print(\"  mean, var, skew, kurt_excess =\", (mean_mc, var_mc, skew_mc, kurt_mc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6a2a8",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### Shape parameter `a` (tail decay)\n",
    "In the PMF \\(p(k)\\propto e^{-a|k-\\mathrm{loc}|}\\), the parameter `a` is a **decay rate**:\n",
    "- larger `a` \\u2192 faster decay \\u2192 distribution becomes sharply concentrated at `loc`\n",
    "- smaller `a` \\u2192 slower decay \\u2192 heavier tails and larger variance\n",
    "\n",
    "Using \\(q=e^{-a}\\), the tail ratio is\n",
    "\\[\n",
    "\\frac{P(|K-\\mathrm{loc}|=m+1)}{P(|K-\\mathrm{loc}|=m)} = q.\n",
    "\\]\n",
    "\n",
    "### Location parameter `loc`\n",
    "`loc` shifts the distribution left/right without changing its shape. For integer-valued modeling, `loc` is typically chosen as an **integer** (a shifted lattice).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a78f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(-20, 21)\n",
    "a_values = [0.4, 0.8, 1.6]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in a_values:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=k,\n",
    "            y=dlaplace_pmf(k, a),\n",
    "            mode=\"lines+markers\",\n",
    "            name=f\"a={a}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PMF of dlaplace for different a (loc=0)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(K=k)\",\n",
    "    width=800,\n",
    "    height=430,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(-20, 21)\n",
    "a_values = [0.4, 0.8, 1.6]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in a_values:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=k,\n",
    "            y=dlaplace_cdf(k, a),\n",
    "            mode=\"lines\",\n",
    "            line_shape=\"hv\",\n",
    "            name=f\"a={a}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"CDF of dlaplace for different a (loc=0)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"F(k)=P(K\\u2264k)\",\n",
    "    width=800,\n",
    "    height=430,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b09d3a",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation\n",
    "In standard form (`loc=0`), the PMF is symmetric: \\(p(k)=p(-k)\\). Therefore\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[K] = \\sum_{k\\in\\mathbb{Z}} k\\,p(k) = 0\n",
    "\\]\n",
    "\n",
    "because each \\(k\\) term cancels with the \\(-k\\) term. With a location shift, \\(\\mathbb{E}[K]=\\mathrm{loc}\\).\n",
    "\n",
    "### Variance\n",
    "Let \\(q=e^{-a}\\) and \\(c=(1-q)/(1+q)\\). Then\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(K)=\\mathbb{E}[K^2]=2c\\sum_{k=1}^{\\infty} k^2 q^k.\n",
    "\\]\n",
    "\n",
    "Using the standard geometric-series identity\n",
    "\n",
    "\\[\n",
    "\\sum_{k=1}^{\\infty} k^2 q^k = \\frac{q(1+q)}{(1-q)^3},\\qquad |q|<1,\n",
    "\\]\n",
    "\n",
    "we get\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[K^2] = 2\\,\\frac{1-q}{1+q}\\,\\frac{q(1+q)}{(1-q)^3} = \\frac{2q}{(1-q)^2}.\n",
    "\\]\n",
    "\n",
    "### Likelihood and MLE\n",
    "For observations \\(x_1,\\dots,x_n\\in\\mathrm{loc}+\\mathbb{Z}\\), the log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell(a,\\mathrm{loc}) = \\sum_{i=1}^n \\log p(x_i\\mid a,\\mathrm{loc})\n",
    "= n\\,\\log\\tanh(a/2) - a\\sum_{i=1}^n |x_i-\\mathrm{loc}|.\n",
    "\\]\n",
    "\n",
    "- For fixed `a`, maximizing \\(\\ell\\) over `loc` is equivalent to minimizing \\(\\sum_i |x_i-\\mathrm{loc}|\\), so **any median** is an MLE of `loc`.\n",
    "- For fixed `loc`, differentiate with respect to `a`:\n",
    "\\[\n",
    "\\frac{\\partial}{\\partial a} \\log\\tanh(a/2) = \\frac{1}{\\sinh(a)}.\n",
    "\\]\n",
    "Setting the score to zero yields the closed-form MLE\n",
    "\\[\n",
    "\\hat a = \\operatorname{asinh}\\!\\left(\\frac{n}{\\sum_i |x_i-\\mathrm{loc}|}\\right).\n",
    "\\]\n",
    "If all observations equal `loc` then the sum in the denominator is zero and the likelihood increases as \\(a\\to\\infty\\) (degenerate spike at `loc`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfe2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlaplace_loglik(data: np.ndarray, a: float, loc=0) -> float:\n",
    "    a = _check_a(a)\n",
    "    data = np.asarray(data)\n",
    "    return data.size * np.log(np.tanh(a / 2.0)) - a * np.sum(np.abs(data - loc))\n",
    "\n",
    "\n",
    "def dlaplace_mle_closed_form(data: np.ndarray):\n",
    "    \"\"\"Closed-form MLE under an integer-location constraint.\"\"\"\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim != 1:\n",
    "        raise ValueError(\"data must be 1D\")\n",
    "\n",
    "    x = np.sort(data)\n",
    "    n = x.size\n",
    "\n",
    "    # Any median minimizes sum |x_i - loc|. For even n, choose the lower median.\n",
    "    loc_hat = x[(n - 1) // 2]\n",
    "    S = np.sum(np.abs(x - loc_hat))\n",
    "    if S == 0:\n",
    "        a_hat = np.inf\n",
    "    else:\n",
    "        a_hat = np.arcsinh(n / S)\n",
    "    return float(a_hat), float(loc_hat)\n",
    "\n",
    "\n",
    "# Demo: estimate parameters from simulated data\n",
    "a_true = 0.9\n",
    "loc_true = -3\n",
    "data = dlaplace_rvs_numpy(a_true, loc=loc_true, size=3_000, rng=rng)\n",
    "\n",
    "a_hat, loc_hat = dlaplace_mle_closed_form(data)\n",
    "print(\"True (a, loc):\", (a_true, loc_true))\n",
    "print(\"MLE  (a, loc):\", (a_hat, loc_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3f8c2",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "A very convenient representation is:\n",
    "\n",
    "**If** \\(G_1,G_2\\) are i.i.d. geometric random variables on \\(\\{0,1,2,\\dots\\}\\) with\n",
    "\\[\n",
    "P(G=g)=(1-q)q^g,\\qquad q=e^{-a},\n",
    "\\]\n",
    "then\n",
    "\\[\n",
    "K = G_1 - G_2\n",
    "\\]\n",
    "has\n",
    "\\[\n",
    "P(K=k)=\\frac{1-q}{1+q} q^{|k|},\n",
    "\\]\n",
    "which is exactly the `dlaplace` PMF (standard form).\n",
    "\n",
    "So we can sample by drawing two geometrics and subtracting them. The function `dlaplace_rvs_numpy` above implements this in a vectorized way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd3d2b",
   "metadata": {},
   "source": [
    "## 8) Visualization (PMF/CDF + Monte Carlo)\n",
    "\n",
    "Below we compare the theoretical PMF to a Monte Carlo estimate from NumPy-only sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.8\n",
    "loc = 0\n",
    "n = 80_000\n",
    "\n",
    "x = dlaplace_rvs_numpy(a, loc=loc, size=n, rng=rng)\n",
    "\n",
    "k = np.arange(-25, 26)\n",
    "pmf_theory = dlaplace_pmf(k, a, loc=loc)\n",
    "\n",
    "# Empirical PMF on the grid (probability outside grid is ignored for plotting)\n",
    "counts = np.array([(x == ki).mean() for ki in k])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=k, y=counts, name=\"Monte Carlo\", opacity=0.65))\n",
    "fig.add_trace(go.Scatter(x=k, y=pmf_theory, mode=\"lines+markers\", name=\"Theory\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"PMF: Monte Carlo vs theory (a={a}, loc={loc}, n={n:,})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(K=k)\",\n",
    "    width=850,\n",
    "    height=440,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e9e7b",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.dlaplace` with the usual discrete-distribution API:\n",
    "- `dlaplace.pmf(k, a, loc=0)`\n",
    "- `dlaplace.cdf(k, a, loc=0)`\n",
    "- `dlaplace.rvs(a, loc=0, size=..., random_state=...)`\n",
    "\n",
    "For fitting parameters, `dlaplace` itself does **not** expose a `.fit(...)` method (unlike many continuous distributions). In SciPy\\u00a01.15+, you can use the generic function `scipy.stats.fit` to perform MLE subject to bounds (and integer constraints where applicable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.8\n",
    "loc = 2\n",
    "k = np.arange(-5, 6)\n",
    "\n",
    "pmf_np = dlaplace_pmf(k, a, loc=loc)\n",
    "pmf_sp = dlaplace.pmf(k, a, loc=loc)\n",
    "cdf_np = dlaplace_cdf(k, a, loc=loc)\n",
    "cdf_sp = dlaplace.cdf(k, a, loc=loc)\n",
    "\n",
    "print(\"Max |pmf_numpy - pmf_scipy|:\", float(np.max(np.abs(pmf_np - pmf_sp))))\n",
    "print(\"Max |cdf_numpy - cdf_scipy|:\", float(np.max(np.abs(cdf_np - cdf_sp))))\n",
    "\n",
    "# SciPy sampling\n",
    "data = dlaplace.rvs(a, loc=loc, size=2_000, random_state=rng)\n",
    "\n",
    "# Closed-form MLE\n",
    "a_hat_cf, loc_hat_cf = dlaplace_mle_closed_form(data)\n",
    "\n",
    "# SciPy's generic fitter: must provide bounds to let loc vary (otherwise it's fixed at 0 by default).\n",
    "bounds = {\n",
    "    \"a\": (1e-6, 10.0),\n",
    "    \"loc\": (int(data.min()) - 10, int(data.max()) + 10),\n",
    "}\n",
    "res = fit(dlaplace, data, bounds=bounds, method=\"mle\")\n",
    "\n",
    "print(\"\\nTrue (a, loc):\", (a, loc))\n",
    "print(\"Closed-form MLE:\", (a_hat_cf, loc_hat_cf))\n",
    "print(\"SciPy fit params:\", (float(res.params.a), float(res.params.loc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7d2b5",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (example: tail parameter)\n",
    "Because the likelihood is available in closed form, you can build likelihood-ratio tests for `a` (often with `loc` fixed/known).\n",
    "\n",
    "### B) Bayesian modeling (robust integer noise)\n",
    "If\n",
    "\\[\n",
    "Y_i = \\theta + \\varepsilon_i,\\qquad \\varepsilon_i\\sim\\text{dlaplace}(a,\\,\\mathrm{loc}=0),\n",
    "\\]\n",
    "and you put a flat prior over integer \\(\\theta\\), then the posterior satisfies\n",
    "\\[\n",
    "\\log p(\\theta\\mid y) = \\text{const} - a\\sum_i |y_i-\\theta|,\n",
    "\\]\n",
    "so the MAP estimate is (any) **median** of the observations. This is the discrete counterpart of the well-known \\(\\ell_1\\) connection for the continuous Laplace.\n",
    "\n",
    "### C) Generative modeling (integer-valued heavy-tailed noise)\n",
    "Use `dlaplace` as an emission/noise model when your observations are integers and you expect occasional large deviations (e.g., perturbed counts, rounded sensor data, integer residuals).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12df6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood-ratio test example: H0: a = a0 vs H1: a free (loc known)\n",
    "a0 = 0.8\n",
    "a_true = 1.1\n",
    "loc = 0\n",
    "n = 800\n",
    "\n",
    "data = dlaplace_rvs_numpy(a_true, loc=loc, size=n, rng=rng)\n",
    "S = np.sum(np.abs(data - loc))\n",
    "a_hat = np.inf if S == 0 else np.arcsinh(n / S)\n",
    "\n",
    "ll_hat = dlaplace_loglik(data, a_hat, loc=loc)\n",
    "ll_0 = dlaplace_loglik(data, a0, loc=loc)\n",
    "\n",
    "LR = 2.0 * (ll_hat - ll_0)\n",
    "p_value = chi2.sf(LR, df=1)\n",
    "\n",
    "print(f\"a_true={a_true}, a0={a0}, a_hat={a_hat:.4f}\")\n",
    "print(f\"LR statistic={LR:.3f}, approx p-value={p_value:.4f}\")\n",
    "print(\"(Asymptotic chi-square calibration is approximate; check via simulation for small n.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian modeling demo: posterior over an integer location parameter theta\n",
    "a = 0.9\n",
    "theta_true = 4\n",
    "n = 50\n",
    "\n",
    "y = dlaplace_rvs_numpy(a, loc=theta_true, size=n, rng=rng)\n",
    "\n",
    "theta_grid = np.arange(int(y.min()) - 10, int(y.max()) + 11)\n",
    "log_post = np.array([-a * np.sum(np.abs(y - th)) for th in theta_grid], dtype=float)\n",
    "log_post -= log_post.max()\n",
    "post = np.exp(log_post)\n",
    "post /= post.sum()\n",
    "\n",
    "theta_map = theta_grid[np.argmax(post)]\n",
    "theta_median = np.sort(y)[(n - 1) // 2]\n",
    "\n",
    "print(\"theta_true:\", theta_true)\n",
    "print(\"MAP theta:\", int(theta_map))\n",
    "print(\"sample median:\", int(theta_median))\n",
    "\n",
    "fig = px.line(\n",
    "    x=theta_grid,\n",
    "    y=post,\n",
    "    title=\"Posterior over integer location (flat prior; a known)\",\n",
    "    labels={\"x\": \"theta\", \"y\": \"posterior probability\"},\n",
    ")\n",
    "fig.add_vline(x=theta_true, line_dash=\"dash\", line_color=\"black\", annotation_text=\"true\")\n",
    "fig.add_vline(x=theta_map, line_dash=\"dot\", line_color=\"red\", annotation_text=\"MAP\")\n",
    "fig.update_layout(width=850, height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling / differential privacy-style demo: noisy release of a count\n",
    "true_count = 250\n",
    "a_values = [0.5, 1.0, 2.0]  # larger a => less noise\n",
    "n = 40_000\n",
    "\n",
    "fig = go.Figure()\n",
    "xs = np.arange(true_count - 40, true_count + 41)\n",
    "\n",
    "for a in a_values:\n",
    "    noise = dlaplace_rvs_numpy(a, loc=0, size=n, rng=rng)\n",
    "    released = true_count + noise\n",
    "    emp = np.array([(released == x).mean() for x in xs])\n",
    "    fig.add_trace(go.Scatter(x=xs, y=emp, mode=\"lines\", name=f\"a={a}\"))\n",
    "\n",
    "fig.add_vline(x=true_count, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(\n",
    "    title=\"Noisy count release: empirical PMF for different a\",\n",
    "    xaxis_title=\"released count\",\n",
    "    yaxis_title=\"probability\",\n",
    "    width=900,\n",
    "    height=430,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d293a20",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters:** `a` must be strictly positive. As `a \\u2192 0+`, tails become extremely heavy and moments blow up.\n",
    "- **Location lattice:** the distribution lives on `loc + Z`. For most integer-data use cases, choose `loc` as an integer.\n",
    "- **Underflow in tails:** for large \\(|k|\\), `pmf(k, ...)` can underflow to 0. Use `logpmf` (or work in log-space) for likelihood computations.\n",
    "- **MGF domain:** the MGF exists only for \\(|t|<a\\). Near \\(\\pm a\\) the denominator becomes ill-conditioned.\n",
    "- **Parameterization confusion:** some sources use `q \\in (0,1)` directly (two-sided geometric). SciPy uses `a>0` with `q=e^{-a}`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f3f46",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `dlaplace` is a **symmetric discrete** distribution on \\(\\mathbb{Z}\\) (or `loc + Z`) with exponentially decaying tails.\n",
    "- PMF: \\(p(k)=\\tanh(a/2)\\,e^{-a|k-\\mathrm{loc}|}\\) with `a>0`.\n",
    "- A convenient reparameterization is \\(q=e^{-a}\\), giving \\(p(k)=\\frac{1-q}{1+q}q^{|k-\\mathrm{loc}|}\\).\n",
    "- Mean = `loc`, variance \\(2q/(1-q)^2\\), skewness 0, excess kurtosis \\(\\cosh(a)+2\\), entropy \\(-\\log\\tanh(a/2)+a/\\sinh(a)\\).\n",
    "- Sampling (NumPy-only): draw two 0-based geometrics with \\(q=e^{-a}\\) and subtract: \\(K=G_1-G_2+\\mathrm{loc}\\).\n",
    "- For inference: `loc` MLE is a median; `a` MLE is \\(\\operatorname{asinh}(n/\\sum|x_i-\\mathrm{loc}|)\\). SciPy provides `dlaplace` plus the generic `scipy.stats.fit` utility for bounded MLE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}