{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b005a75",
   "metadata": {},
   "source": [
    "# Power log-normal distribution (`powerlognorm`)\n",
    "\n",
    "The **power log-normal** distribution (SciPy: `scipy.stats.powerlognorm`) is a *continuous* distribution on $(0,\\infty)$. A useful way to think about it is as a **lognormal distribution whose survival function is raised to a power**.\n",
    "\n",
    "Equivalently (when the power parameter is an integer), it describes the **minimum of several independent lognormal variables** — a natural model for **“time to first failure”** among multiple components.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- classify `powerlognorm` (support, parameters, SciPy mapping)\n",
    "- write down the PDF/CDF/SF and derive the quantile function\n",
    "- understand how the *power* parameter changes the distribution (order-statistics intuition)\n",
    "- compute moments numerically and understand which transforms exist / don’t exist\n",
    "- sample from `powerlognorm` using a **NumPy-only** inverse-transform sampler\n",
    "- use `scipy.stats.powerlognorm` for `pdf`, `cdf`, `rvs`, and `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9eac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import integrate, optimize\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import powerlognorm as powerlognorm_dist\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Record versions for reproducibility (useful when numerical details matter).\n",
    "VERSIONS = {\n",
    "    \"numpy\": np.__version__,\n",
    "    \"scipy\": scipy.__version__,\n",
    "    \"plotly\": plotly.__version__,\n",
    "}\n",
    "VERSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19cae0",
   "metadata": {},
   "source": [
    "## 1) Title & classification\n",
    "\n",
    "| Item | Value |\n",
    "|---|---|\n",
    "| Name | `powerlognorm` (power log-normal; SciPy: `scipy.stats.powerlognorm`) |\n",
    "| Type | Continuous |\n",
    "| Support (standard form) | $x \\in (0,\\infty)$ |\n",
    "| Shape parameters | $c>0$ (power), $s>0$ (log-scale) |\n",
    "| Location/scale | `loc \\in \\mathbb{R}`, `scale>0` |\n",
    "\n",
    "SciPy uses the standard location/scale transform\n",
    "\n",
    "$$\n",
    "X = \\text{loc} + \\text{scale}\\,Y,\\qquad Y \\sim \\texttt{powerlognorm}(c,s).\n",
    "$$\n",
    "\n",
    "So the full support is $x>\\text{loc}$.\n",
    "\n",
    "Unless stated otherwise, we work with the **standard form** `loc=0`, `scale=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51c48b",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "A clean way to see the “power” is through the **survival function** (tail probability):\n",
    "\n",
    "- Let $Y$ be a lognormal variable.\n",
    "- Define a new variable $X$ whose survival is a *powered* lognormal survival:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X>x) = \\big(\\mathbb{P}(Y>x)\\big)^c.\n",
    "$$\n",
    "\n",
    "If $c$ is a positive integer, this has an immediate interpretation:\n",
    "\n",
    "- If $Y_1,\\dots,Y_c$ are i.i.d. copies of $Y$, then\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\min_i Y_i > x) = \\prod_{i=1}^c \\mathbb{P}(Y_i>x) = \\big(\\mathbb{P}(Y>x)\\big)^c.\n",
    "$$\n",
    "\n",
    "So for integer $c$, `powerlognorm` is the distribution of the **minimum of $c$ i.i.d. lognormals**.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Reliability / survival**: time to **first failure** among $c$ components with (approximately) lognormal lifetimes.\n",
    "- **Competing risks**: multiple independent mechanisms can “trigger” an event; the observed time is the minimum.\n",
    "- **Extreme-value flavored modeling**: modeling the *best-case* / *smallest* outcome when each candidate outcome is lognormal-ish.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Lognormal**: when $c=1$, `powerlognorm` reduces exactly to a lognormal with $\\log X \\sim \\mathcal{N}(0, s^2)$ (in standard form).\n",
    "- **Order statistics**: for integer $c$, it is the distribution of a **minimum**.\n",
    "- **Lehmann alternatives / exponentiated families**: raising a baseline CDF or survival function to a power creates a flexible one-parameter extension.\n",
    "- **Power-normal on the log scale**: if $Z = \\log X / s$, then $Z$ has PDF $c\\,\\phi(z)\\,\\Phi(-z)^{c-1}$ — the “power” version of a standard normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b30c7",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "Let $\\phi$ and $\\Phi$ be the standard normal PDF and CDF:\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\tfrac{1}{2}z^2\\right),\n",
    "\\qquad\n",
    "\\Phi(z) = \\int_{-\\infty}^z \\phi(t)\\,dt.\n",
    "$$\n",
    "\n",
    "### PDF\n",
    "\n",
    "For $x>0$ and $c,s>0$, the `powerlognorm` PDF is\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\;f(x\\mid c,s)\n",
    "= \\frac{c}{x\\,s}\\,\\phi\\!\\left(\\frac{\\log x}{s}\\right)\\,\\Big(\\Phi\\!\\left(-\\frac{\\log x}{s}\\right)\\Big)^{c-1}\\;}\n",
    "$$\n",
    "\n",
    "(and $f(x)=0$ for $x\\le 0$).\n",
    "\n",
    "### CDF / survival\n",
    "\n",
    "A convenient closed form is in terms of the survival function:\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\\mathbb{P}(X>x) = \\Big(\\Phi\\!\\left(-\\frac{\\log x}{s}\\right)\\Big)^c\\;},\\qquad x>0.\n",
    "$$\n",
    "\n",
    "Therefore the CDF is\n",
    "\n",
    "$$\n",
    "\\boxed{\\;F(x\\mid c,s) = 1 - \\Big(\\Phi\\!\\left(-\\frac{\\log x}{s}\\right)\\Big)^c\\;},\\qquad x>0.\n",
    "$$\n",
    "\n",
    "### Quantile function (PPF)\n",
    "\n",
    "Set $u=F(x)$. Then $1-u = \\Phi(-\\log x/s)^c$, so\n",
    "\n",
    "$$\n",
    "\\Phi^{-1}\\big((1-u)^{1/c}\\big) = -\\frac{\\log x}{s}\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boxed{\\;F^{-1}(u) = \\exp\\!\\Big(-s\\,\\Phi^{-1}((1-u)^{1/c})\\Big).\\;}\n",
    "$$\n",
    "\n",
    "This is the basis of inverse-transform sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae57588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlognorm_logpdf(x: np.ndarray, c: float, s: float) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of the standard powerlognorm(c, s) (loc=0, scale=1).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    c = float(c)\n",
    "    s = float(s)\n",
    "    if c <= 0 or s <= 0:\n",
    "        raise ValueError(\"c and s must be > 0\")\n",
    "\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "    if np.any(mask):\n",
    "        z = np.log(x[mask]) / s\n",
    "        out[mask] = (\n",
    "            np.log(c)\n",
    "            - np.log(x[mask])\n",
    "            - np.log(s)\n",
    "            + norm.logpdf(z)\n",
    "            + (c - 1.0) * norm.logcdf(-z)\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "def powerlognorm_pdf(x: np.ndarray, c: float, s: float) -> np.ndarray:\n",
    "    return np.exp(powerlognorm_logpdf(x, c, s))\n",
    "\n",
    "\n",
    "def powerlognorm_logsf(x: np.ndarray, c: float, s: float) -> np.ndarray:\n",
    "    \"\"\"Log survival function log P(X > x) (standard form).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    c = float(c)\n",
    "    s = float(s)\n",
    "    if c <= 0 or s <= 0:\n",
    "        raise ValueError(\"c and s must be > 0\")\n",
    "\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "    if np.any(mask):\n",
    "        z = np.log(x[mask]) / s\n",
    "        out[mask] = c * norm.logcdf(-z)\n",
    "\n",
    "    # For x <= 0, P(X > x) = 1, so logsf = 0.\n",
    "    out[~mask] = 0.0\n",
    "    return out\n",
    "\n",
    "\n",
    "def powerlognorm_sf(x: np.ndarray, c: float, s: float) -> np.ndarray:\n",
    "    return np.exp(powerlognorm_logsf(x, c, s))\n",
    "\n",
    "\n",
    "def powerlognorm_cdf(x: np.ndarray, c: float, s: float) -> np.ndarray:\n",
    "    \"\"\"CDF computed stably from logsf: F = 1 - exp(logsf).\"\"\"\n",
    "    return -np.expm1(powerlognorm_logsf(x, c, s))\n",
    "\n",
    "\n",
    "def powerlognorm_ppf(u: np.ndarray, c: float, s: float) -> np.ndarray:\n",
    "    \"\"\"Quantile function F^{-1}(u) for u in (0,1).\"\"\"\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    c = float(c)\n",
    "    s = float(s)\n",
    "    if c <= 0 or s <= 0:\n",
    "        raise ValueError(\"c and s must be > 0\")\n",
    "\n",
    "    if np.any((u <= 0) | (u >= 1)):\n",
    "        raise ValueError(\"u must be in (0,1)\")\n",
    "\n",
    "    q = (1.0 - u) ** (1.0 / c)\n",
    "    return np.exp(-s * norm.ppf(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa10b6a",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "Because `powerlognorm` is lognormal-like in its right tail, it has **all positive moments** (mean/variance/etc.), but—like the lognormal—it does **not** have an MGF for $t>0$.\n",
    "\n",
    "### Log-scale representation\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "Z = \\frac{\\log X}{s} \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "A change of variables shows that $Z$ has PDF\n",
    "\n",
    "$$\n",
    "\\boxed{\\;g(z\\mid c) = c\\,\\phi(z)\\,\\Phi(-z)^{c-1},\\qquad z\\in\\mathbb{R}.\\;}\n",
    "$$\n",
    "\n",
    "For integer $c$, this is exactly the PDF of the **minimum of $c$ i.i.d. standard normals**.\n",
    "\n",
    "### Raw moments\n",
    "\n",
    "For $k>0$, using $X=\\exp(sZ)$,\n",
    "\n",
    "$$\n",
    "\\mu_k' := \\mathbb{E}[X^k] = \\mathbb{E}[e^{ksZ}]\n",
    "= c\\int_{-\\infty}^{\\infty} e^{ksz}\\,\\phi(z)\\,\\Phi(-z)^{c-1}\\,dz.\n",
    "$$\n",
    "\n",
    "From raw moments:\n",
    "\n",
    "- Mean: $\\mathbb{E}[X]=\\mu_1'$\n",
    "- Variance: $\\mathrm{Var}(X)=\\mu_2' - (\\mu_1')^2$\n",
    "- Skewness:\n",
    "  $$\\gamma_1 = \\frac{\\mu_3' - 3\\mu_2'\\mu_1' + 2(\\mu_1')^3}{\\mathrm{Var}(X)^{3/2}}$$\n",
    "- Kurtosis (excess):\n",
    "  $$\\gamma_2 = \\frac{\\mu_4' - 4\\mu_3'\\mu_1' + 6\\mu_2'(\\mu_1')^2 - 3(\\mu_1')^4}{\\mathrm{Var}(X)^{2}} - 3$$\n",
    "\n",
    "Special case $c=1$ (lognormal, standard form):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = e^{s^2/2},\n",
    "\\qquad\n",
    "\\mathrm{Var}(X) = (e^{s^2}-1)e^{s^2}.\n",
    "$$\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "- **MGF** $M_X(t)=\\mathbb{E}[e^{tX}]$ is **infinite for every $t>0$** (the tail is too heavy).\n",
    "- The **characteristic function** $\\varphi_X(t)=\\mathbb{E}[e^{itX}]$ exists for all real $t$, but has no simple closed form; compute numerically (quadrature) or via Monte Carlo.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy is\n",
    "\n",
    "$$\n",
    "H(X) = -\\mathbb{E}[\\log f(X)],\n",
    "$$\n",
    "\n",
    "which can be evaluated numerically (SciPy implements `entropy`).\n",
    "\n",
    "### Hazard function\n",
    "\n",
    "Because the survival is a power, the hazard scales simply:\n",
    "\n",
    "$$\n",
    "h(x) = \\frac{f(x)}{\\mathbb{P}(X>x)} = \\frac{c}{x s}\\,\\frac{\\phi(\\log x/s)}{\\Phi(-\\log x/s)}.\n",
    "$$\n",
    "\n",
    "For integer $c$ this matches the “minimum of $c$ i.i.d.” interpretation: the minimum’s hazard is $c$ times the baseline hazard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlognorm_raw_moment_quad(k: int, c: float, s: float) -> float:\n",
    "    \"\"\"Raw moment E[X^k] via 1D quadrature over z = log(x)/s.\"\"\"\n",
    "    k = int(k)\n",
    "    c = float(c)\n",
    "    s = float(s)\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be a positive integer\")\n",
    "    if c <= 0 or s <= 0:\n",
    "        raise ValueError(\"c and s must be > 0\")\n",
    "\n",
    "    def integrand(z):\n",
    "        # log integrand for stability\n",
    "        log_val = (\n",
    "            np.log(c)\n",
    "            + (k * s) * z\n",
    "            + norm.logpdf(z)\n",
    "            + (c - 1.0) * norm.logcdf(-z)\n",
    "        )\n",
    "        return np.exp(log_val)\n",
    "\n",
    "    val, err = integrate.quad(integrand, -np.inf, np.inf, limit=200)\n",
    "    return float(val)\n",
    "\n",
    "\n",
    "c_demo, s_demo = 2.2, 0.6\n",
    "\n",
    "m1 = powerlognorm_raw_moment_quad(1, c_demo, s_demo)\n",
    "m2 = powerlognorm_raw_moment_quad(2, c_demo, s_demo)\n",
    "mean_quad = m1\n",
    "var_quad = m2 - m1**2\n",
    "\n",
    "mean_scipy, var_scipy, skew_scipy, kurt_scipy = powerlognorm_dist.stats(\n",
    "    c_demo, s_demo, moments=\"mvsk\"\n",
    ")\n",
    "\n",
    "mean_quad, var_quad, float(mean_scipy), float(var_scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ba827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skew/kurtosis from raw moments (quadrature)\n",
    "m3 = powerlognorm_raw_moment_quad(3, c_demo, s_demo)\n",
    "m4 = powerlognorm_raw_moment_quad(4, c_demo, s_demo)\n",
    "\n",
    "mu = mean_quad\n",
    "var = var_quad\n",
    "\n",
    "skew_quad = (m3 - 3 * m2 * mu + 2 * mu**3) / (var ** 1.5)\n",
    "kurt_excess_quad = (m4 - 4 * m3 * mu + 6 * m2 * mu**2 - 3 * mu**4) / (var**2) - 3\n",
    "\n",
    "float(skew_quad), float(kurt_excess_quad), float(skew_scipy), float(kurt_scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy: SciPy (numerical) vs Monte Carlo estimate\n",
    "n_mc = 200_000\n",
    "samples_mc = powerlognorm_dist.rvs(c_demo, s_demo, size=n_mc, random_state=rng)\n",
    "\n",
    "entropy_scipy = float(powerlognorm_dist.entropy(c_demo, s_demo))\n",
    "entropy_mc = -float(np.mean(powerlognorm_logpdf(samples_mc, c_demo, s_demo)))\n",
    "\n",
    "entropy_scipy, entropy_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4143fe",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "### $s>0$ (log-scale)\n",
    "\n",
    "- When $c=1$ (lognormal), $\\log X \\sim \\mathcal{N}(0, s^2)$, so $s$ is literally the **standard deviation on the log scale**.\n",
    "- More generally, $s$ still controls how spread-out $\\log X$ is.\n",
    "\n",
    "Increasing $s$ typically:\n",
    "\n",
    "- increases right-skew and the heaviness of the right tail\n",
    "- pushes more mass toward very small values (because the log scale is wider)\n",
    "\n",
    "### $c>0$ (power / “minimum-of-$c$”)\n",
    "\n",
    "From the survival function,\n",
    "\n",
    "$$\\mathbb{P}(X>x) = \\Big(\\Phi\\!\\left(-\\frac{\\log x}{s}\\right)\\Big)^c.$$\n",
    "\n",
    "- $c=1$ gives the **baseline lognormal**.\n",
    "- $c>1$ makes the survival smaller, so $X$ tends to be **smaller** (interpretable as the minimum of more components).\n",
    "- $0<c<1$ makes the survival larger, yielding a **heavier tail** than the lognormal baseline.\n",
    "\n",
    "A practical interpretation (integer $c$): you’re observing the **earliest** among $c$ independent lognormal times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30375f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape changes: vary c (power) and s (log-scale)\n",
    "\n",
    "def _x_grid_for_params(param_list, lo=1e-4, hi=0.999):\n",
    "    # Use quantiles to choose a reasonable plotting domain.\n",
    "    xs = []\n",
    "    for c, s in param_list:\n",
    "        xs.append(float(powerlognorm_dist.ppf(lo, c, s)))\n",
    "        xs.append(float(powerlognorm_dist.ppf(hi, c, s)))\n",
    "    x_min = max(min(xs), 1e-12)\n",
    "    x_max = max(xs)\n",
    "    return np.geomspace(x_min, x_max, 500)\n",
    "\n",
    "\n",
    "# 1) Fix s, vary c\n",
    "s_fixed = 0.6\n",
    "cs = [0.5, 1.0, 2.0, 5.0]\n",
    "params_c = [(c, s_fixed) for c in cs]\n",
    "x_grid_c = _x_grid_for_params(params_c)\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in cs:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_grid_c,\n",
    "            y=powerlognorm_dist.pdf(x_grid_c, c, s_fixed),\n",
    "            mode=\"lines\",\n",
    "            name=f\"c={c:g}\",\n",
    "        )\n",
    "    )\n",
    "fig.update_layout(\n",
    "    title=f\"powerlognorm PDF (vary c, fixed s={s_fixed:g})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"pdf(x)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# 2) Fix c, vary s\n",
    "c_fixed = 2.0\n",
    "ss = [0.25, 0.5, 1.0]\n",
    "params_s = [(c_fixed, s) for s in ss]\n",
    "x_grid_s = _x_grid_for_params(params_s)\n",
    "\n",
    "fig = go.Figure()\n",
    "for s in ss:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_grid_s,\n",
    "            y=powerlognorm_dist.pdf(x_grid_s, c_fixed, s),\n",
    "            mode=\"lines\",\n",
    "            name=f\"s={s:g}\",\n",
    "        )\n",
    "    )\n",
    "fig.update_layout(\n",
    "    title=f\"powerlognorm PDF (vary s, fixed c={c_fixed:g})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"pdf(x)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41400da8",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation (raw moments)\n",
    "\n",
    "Start from\n",
    "\n",
    "$$\n",
    "\\mu_k' = \\mathbb{E}[X^k] = \\int_0^\\infty x^k\\,f(x\\mid c,s)\\,dx.\n",
    "$$\n",
    "\n",
    "Substitute $z = \\log x / s$ so $x = e^{sz}$ and $dx = s e^{sz}\\,dz = s x\\,dz$.\n",
    "\n",
    "Using the PDF,\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k'\n",
    "&= \\int_0^\\infty x^k\\,\\frac{c}{xs}\\,\\phi(\\log x/s)\\,\\Phi(-\\log x/s)^{c-1}\\,dx\\\\\n",
    "&= \\int_{-\\infty}^{\\infty} e^{ksz}\\,c\\,\\phi(z)\\,\\Phi(-z)^{c-1}\\,dz.\n",
    "\\end{align}\n",
    "\n",
    "This gives a clean 1D integral for numerical evaluation.\n",
    "\n",
    "### Variance\n",
    "\n",
    "Compute $\\mu_1'$ and $\\mu_2'$ and use\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X) = \\mu_2' - (\\mu_1')^2.\n",
    "$$\n",
    "\n",
    "### Likelihood / log-likelihood\n",
    "\n",
    "Given i.i.d. data $x_1,\\dots,x_n>0$, the log-likelihood for $(c,s)$ in the standard form is\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(c,s; x_{1:n})\n",
    "&= \\sum_{i=1}^n \\log f(x_i\\mid c,s)\\\\\n",
    "&= n\\log c - n\\log s - \\sum_{i=1}^n \\log x_i\n",
    " + \\sum_{i=1}^n \\log\\phi\\!\\left(\\frac{\\log x_i}{s}\\right)\n",
    " + (c-1)\\sum_{i=1}^n \\log\\Phi\\!\\left(-\\frac{\\log x_i}{s}\\right).\n",
    "\\end{align}\n",
    "\n",
    "There’s no closed-form MLE in general; you typically optimize this numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlognorm_nll(params_unconstrained, x):\n",
    "    \"\"\"Negative log-likelihood in terms of unconstrained params (log c, log s).\"\"\"\n",
    "    log_c, log_s = params_unconstrained\n",
    "    c = np.exp(log_c)\n",
    "    s = np.exp(log_s)\n",
    "    return -np.sum(powerlognorm_logpdf(x, c, s))\n",
    "\n",
    "\n",
    "# Demo: recover parameters from synthetic data (standard form)\n",
    "c_true, s_true = 2.5, 0.7\n",
    "x_obs = powerlognorm_dist.rvs(c_true, s_true, size=4000, random_state=rng)\n",
    "\n",
    "x0 = np.log([1.0, 0.5])\n",
    "res = optimize.minimize(powerlognorm_nll, x0=x0, args=(x_obs,), method=\"Nelder-Mead\")\n",
    "\n",
    "c_hat, s_hat = np.exp(res.x)\n",
    "(c_true, s_true), (float(c_hat), float(s_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0677efa7",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation (NumPy-only)\n",
    "\n",
    "The quantile expression makes inverse-transform sampling straightforward.\n",
    "\n",
    "### Inverse-transform sampling via the survival function\n",
    "\n",
    "We have, for $x>0$,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X>x) = \\Big(\\Phi\\!\\left(-\\frac{\\log x}{s}\\right)\\Big)^c.\n",
    "$$\n",
    "\n",
    "Let $U\\sim\\text{Uniform}(0,1)$ and set it equal to the survival probability:\n",
    "\n",
    "$$\n",
    "U = \\mathbb{P}(X>x).\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "U^{1/c} = \\Phi\\!\\left(-\\frac{\\log x}{s}\\right)\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\Phi^{-1}(U^{1/c}) = -\\frac{\\log x}{s}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\boxed{\\;X = \\exp\\big(-s\\,\\Phi^{-1}(U^{1/c})\\big).\\;}\n",
    "$$\n",
    "\n",
    "So sampling reduces to:\n",
    "\n",
    "1) draw $U\\sim\\text{Unif}(0,1)$\n",
    "2) compute $Z=\\Phi^{-1}(U^{1/c})$\n",
    "3) return $X=\\exp(-sZ)$\n",
    "\n",
    "### Integer $c$ shortcut (order-statistics)\n",
    "\n",
    "If $c\\in\\mathbb{N}$, another NumPy-only method is:\n",
    "\n",
    "- sample $c$ i.i.d. lognormals and take the minimum.\n",
    "\n",
    "Both methods appear below. The only “special function” we need is an approximation of the standard normal quantile $\\Phi^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fdba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_ppf_acklam(p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Approximate standard normal quantile (Acklam's rational approximation).\n",
    "\n",
    "    Accuracy is typically ~1e-9 in the central region for float64.\n",
    "\n",
    "    Reference: Peter J. Acklam (2003), \"An algorithm for computing the inverse\n",
    "    normal cumulative distribution function\".\n",
    "    \"\"\"\n",
    "\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if np.any((p <= 0) | (p >= 1)):\n",
    "        raise ValueError(\"p must be strictly between 0 and 1\")\n",
    "\n",
    "    # Coefficients in rational approximations.\n",
    "    a = np.array(\n",
    "        [\n",
    "            -3.969683028665376e01,\n",
    "            2.209460984245205e02,\n",
    "            -2.759285104469687e02,\n",
    "            1.383577518672690e02,\n",
    "            -3.066479806614716e01,\n",
    "            2.506628277459239e00,\n",
    "        ]\n",
    "    )\n",
    "    b = np.array(\n",
    "        [\n",
    "            -5.447609879822406e01,\n",
    "            1.615858368580409e02,\n",
    "            -1.556989798598866e02,\n",
    "            6.680131188771972e01,\n",
    "            -1.328068155288572e01,\n",
    "        ]\n",
    "    )\n",
    "    c = np.array(\n",
    "        [\n",
    "            -7.784894002430293e-03,\n",
    "            -3.223964580411365e-01,\n",
    "            -2.400758277161838e00,\n",
    "            -2.549732539343734e00,\n",
    "            4.374664141464968e00,\n",
    "            2.938163982698783e00,\n",
    "        ]\n",
    "    )\n",
    "    d = np.array(\n",
    "        [\n",
    "            7.784695709041462e-03,\n",
    "            3.224671290700398e-01,\n",
    "            2.445134137142996e00,\n",
    "            3.754408661907416e00,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plow = 0.02425\n",
    "    phigh = 1.0 - plow\n",
    "\n",
    "    x = np.empty_like(p, dtype=float)\n",
    "\n",
    "    # Lower region\n",
    "    mask = p < plow\n",
    "    if np.any(mask):\n",
    "        q = np.sqrt(-2.0 * np.log(p[mask]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1.0)\n",
    "        x[mask] = num / den\n",
    "\n",
    "    # Central region\n",
    "    mask = (p >= plow) & (p <= phigh)\n",
    "    if np.any(mask):\n",
    "        q = p[mask] - 0.5\n",
    "        r = q * q\n",
    "        num = (((((a[0] * r + a[1]) * r + a[2]) * r + a[3]) * r + a[4]) * r + a[5]) * q\n",
    "        den = (((((b[0] * r + b[1]) * r + b[2]) * r + b[3]) * r + b[4]) * r + 1.0)\n",
    "        x[mask] = num / den\n",
    "\n",
    "    # Upper region\n",
    "    mask = p > phigh\n",
    "    if np.any(mask):\n",
    "        q = np.sqrt(-2.0 * np.log(1.0 - p[mask]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1.0)\n",
    "        x[mask] = -(num / den)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def powerlognorm_rvs_numpy(c: float, s: float, size: int | tuple[int, ...], rng) -> np.ndarray:\n",
    "    \"\"\"NumPy-only sampler for the standard powerlognorm(c, s) distribution.\"\"\"\n",
    "    c = float(c)\n",
    "    s = float(s)\n",
    "    if c <= 0 or s <= 0:\n",
    "        raise ValueError(\"c and s must be > 0\")\n",
    "\n",
    "    u = rng.random(size=size)\n",
    "    # Avoid exact 0 which would produce -inf in log/ppf.\n",
    "    u = np.maximum(u, np.nextafter(0.0, 1.0))\n",
    "\n",
    "    p = np.exp(np.log(u) / c)  # u**(1/c), but stable for extreme c\n",
    "    z = norm_ppf_acklam(p)\n",
    "    return np.exp(-s * z)\n",
    "\n",
    "\n",
    "def powerlognorm_rvs_numpy_integer_c(c_int: int, s: float, size: int, rng) -> np.ndarray:\n",
    "    \"\"\"Integer-c shortcut: min of c lognormals (standard form).\"\"\"\n",
    "    c_int = int(c_int)\n",
    "    s = float(s)\n",
    "    if c_int <= 0 or s <= 0:\n",
    "        raise ValueError(\"c_int must be >= 1 and s must be > 0\")\n",
    "\n",
    "    z = rng.normal(size=(c_int, size)).min(axis=0)\n",
    "    return np.exp(s * z)\n",
    "\n",
    "\n",
    "# Quick correctness check (compare against SciPy moments)\n",
    "c_chk, s_chk = 2.0, 0.5\n",
    "n_chk = 200_000\n",
    "samples_numpy = powerlognorm_rvs_numpy(c_chk, s_chk, size=n_chk, rng=rng)\n",
    "\n",
    "m_numpy = samples_numpy.mean()\n",
    "v_numpy = samples_numpy.var(ddof=0)\n",
    "\n",
    "m_scipy, v_scipy = powerlognorm_dist.stats(c_chk, s_chk, moments=\"mv\")\n",
    "float(m_numpy), float(v_numpy), float(m_scipy), float(v_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad4bf0",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "\n",
    "- the **PDF** (and compare to a Monte Carlo histogram)\n",
    "- the **CDF** (and compare to an empirical CDF)\n",
    "- sample behavior under different parameters\n",
    "\n",
    "We’ll generate samples using the NumPy-only sampler from Section 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(samples: np.ndarray):\n",
    "    x = np.sort(np.asarray(samples))\n",
    "    y = np.arange(1, x.size + 1) / x.size\n",
    "    return x, y\n",
    "\n",
    "\n",
    "c_viz, s_viz = 2.5, 0.7\n",
    "n_viz = 120_000\n",
    "\n",
    "samples = powerlognorm_rvs_numpy(c_viz, s_viz, size=n_viz, rng=rng)\n",
    "\n",
    "x_lo = float(powerlognorm_dist.ppf(0.001, c_viz, s_viz))\n",
    "x_hi = float(powerlognorm_dist.ppf(0.995, c_viz, s_viz))\n",
    "x_grid = np.geomspace(max(x_lo, 1e-12), x_hi, 600)\n",
    "\n",
    "# PDF + histogram\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples,\n",
    "        nbinsx=80,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"Monte Carlo samples\",\n",
    "        opacity=0.55,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_grid,\n",
    "        y=powerlognorm_dist.pdf(x_grid, c_viz, s_viz),\n",
    "        mode=\"lines\",\n",
    "        name=\"Theoretical PDF (SciPy)\",\n",
    "        line=dict(width=3),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"powerlognorm(c={c_viz:g}, s={s_viz:g}): PDF vs histogram\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# CDF + empirical CDF\n",
    "x_ecdf, y_ecdf = ecdf(samples)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_ecdf, y=y_ecdf, mode=\"lines\", name=\"Empirical CDF\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_grid,\n",
    "        y=powerlognorm_dist.cdf(x_grid, c_viz, s_viz),\n",
    "        mode=\"lines\",\n",
    "        name=\"Theoretical CDF (SciPy)\",\n",
    "        line=dict(width=3),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f\"powerlognorm(c={c_viz:g}, s={s_viz:g}): CDF vs empirical CDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0af0b",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.powerlognorm`)\n",
    "\n",
    "SciPy’s implementation is `scipy.stats.powerlognorm`.\n",
    "\n",
    "- Shape parameters are passed as `(c, s)`.\n",
    "- `loc` shifts and `scale` rescales as usual.\n",
    "\n",
    "Common operations:\n",
    "\n",
    "- `powerlognorm.pdf(x, c, s, loc=..., scale=...)`\n",
    "- `powerlognorm.cdf(x, c, s, loc=..., scale=...)`\n",
    "- `powerlognorm.rvs(c, s, size=..., random_state=...)`\n",
    "- `powerlognorm.fit(data, ...)` for MLE fitting\n",
    "\n",
    "Also consider the numerically stable variants `logpdf`, `logcdf`, `sf`, and `logsf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd352d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen distribution\n",
    "rv = powerlognorm_dist(c_viz, s_viz)\n",
    "\n",
    "# Evaluate pdf/cdf at a point\n",
    "x0 = 1.0\n",
    "rv.pdf(x0), rv.cdf(x0), rv.sf(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit parameters to data (here: samples from the standard form).\n",
    "# To recover (c, s) cleanly, we fix loc=0 and scale=1.\n",
    "\n",
    "c_hat, s_hat, loc_hat, scale_hat = powerlognorm_dist.fit(samples, floc=0.0, fscale=1.0)\n",
    "(c_viz, s_viz), (float(c_hat), float(s_hat)), (float(loc_hat), float(scale_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58332f91",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### Hypothesis testing (lognormal vs powerlognorm)\n",
    "\n",
    "Because $c=1$ reduces to the lognormal, you can test\n",
    "\n",
    "- $H_0: c=1$ (lognormal)\n",
    "- $H_1: c\\ne 1$ (powerlognorm)\n",
    "\n",
    "via a **likelihood ratio test** (LRT). Under regularity conditions, the LRT statistic is asymptotically $\\chi^2$ with 1 degree of freedom.\n",
    "\n",
    "### Bayesian modeling\n",
    "\n",
    "A common Bayesian use is as a likelihood for strictly positive measurements where a lognormal baseline is plausible but you want extra flexibility:\n",
    "\n",
    "\\begin{align}\n",
    "X_i \\mid c,s &\\sim \\text{powerlognorm}(c,s)\\\\\n",
    "\\log c &\\sim \\mathcal{N}(\\mu_c, \\sigma_c^2),\\qquad \\log s \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2).\n",
    "\\end{align}\n",
    "\n",
    "The integer-$c$ story provides a structural prior: $c$ can represent the number of competing risks/components.\n",
    "\n",
    "### Generative modeling\n",
    "\n",
    "When you want synthetic **first-hit / earliest-event** times with lognormal-ish noise:\n",
    "\n",
    "- sample $c$ latent lognormal times and take the minimum (integer $c$)\n",
    "- or sample directly from `powerlognorm` (real-valued $c$)\n",
    "\n",
    "You get a tunable family that interpolates between heavier tails ($c<1$) and smaller, earlier minima ($c>1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e115f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood ratio test demo: H0 c=1 (lognormal) vs H1 free c\n",
    "\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Simulate from a non-lognormal powerlognorm\n",
    "c_alt, s_alt = 2.0, 0.6\n",
    "x_test = powerlognorm_dist.rvs(c_alt, s_alt, size=6000, random_state=rng)\n",
    "\n",
    "# Fit H1: estimate (c, s) with loc=0, scale=1\n",
    "c1, s1, _, _ = powerlognorm_dist.fit(x_test, floc=0.0, fscale=1.0)\n",
    "ll1 = np.sum(powerlognorm_dist.logpdf(x_test, c1, s1, loc=0.0, scale=1.0))\n",
    "\n",
    "# Fit H0: fix c=1, estimate s with loc=0, scale=1\n",
    "c0, s0, _, _ = powerlognorm_dist.fit(x_test, f0=1.0, floc=0.0, fscale=1.0)\n",
    "ll0 = np.sum(powerlognorm_dist.logpdf(x_test, c0, s0, loc=0.0, scale=1.0))\n",
    "\n",
    "lrt = 2.0 * (ll1 - ll0)\n",
    "p_value = float(chi2.sf(lrt, df=1))\n",
    "\n",
    "{\n",
    "    \"true\": {\"c\": c_alt, \"s\": s_alt},\n",
    "    \"H1_fit\": {\"c\": float(c1), \"s\": float(s1)},\n",
    "    \"H0_fit\": {\"c\": float(c0), \"s\": float(s0)},\n",
    "    \"LRT\": float(lrt),\n",
    "    \"p_value\": p_value,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a42d6",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: $c\\le 0$ or $s\\le 0$ is invalid; the standard-form support is $x>0$.\n",
    "- **Log at / below zero**: formulas use $\\log x$; handle $x\\le 0$ explicitly (PDF=0, logpdf=$-\\infty$, CDF=0).\n",
    "- **Numerical stability in the tail**:\n",
    "  - $\\Phi(-\\log x/s)$ can underflow for large $x$.\n",
    "  - prefer `logcdf`/`logsf` and compute powers in log space: $\\log\\,\\text{sf} = c\\,\\log\\Phi(-\\log x/s)$.\n",
    "- **Fitting can be fragile**:\n",
    "  - with `loc` and `scale` free, the optimization can be poorly identified; if your data are strictly positive, it often helps to fix `loc=0`.\n",
    "  - MLE for heavy-tailed families can be sensitive to outliers; inspect diagnostics (QQ plots, tail fit).\n",
    "- **MGF misconceptions**: even though mean/variance exist, the MGF diverges for $t>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf0e61",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `powerlognorm(c, s)` is a continuous distribution on $(0,\\infty)$ with $c>0$ and $s>0$.\n",
    "- It generalizes the lognormal: **$c=1$ gives a lognormal** ($\\log X\\sim\\mathcal{N}(0,s^2)$ in standard form).\n",
    "- Its survival function is a powered normal tail: $\\mathbb{P}(X>x)=\\Phi(-\\log x/s)^c$.\n",
    "- For integer $c$, it is the **minimum of $c$ i.i.d. lognormal** variables (time-to-first-failure interpretation).\n",
    "- Mean/variance/etc. exist but typically require **numerical evaluation**; the **MGF does not exist for $t>0$**.\n",
    "- Sampling is easy via inverse transform once you can compute the normal quantile; SciPy provides robust `pdf/cdf/rvs/fit` implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}