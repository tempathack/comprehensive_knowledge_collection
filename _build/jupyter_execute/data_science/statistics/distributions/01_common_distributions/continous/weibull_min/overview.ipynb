{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d062344c",
   "metadata": {},
   "source": [
    "# Weibull minimum distribution (`weibull_min`)\n",
    "\n",
    "The **Weibull distribution** (SciPy: `scipy.stats.weibull_min`) is a flexible *continuous* distribution on $[0,\\infty)$ that is widely used to model **lifetimes / time-to-failure**.\n",
    "\n",
    "Its most important practical feature is that its **hazard rate** can be *decreasing*, *constant*, or *increasing* depending on the shape parameter — making it a go-to model in **reliability engineering** and **survival analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Write down the PDF/CDF (and survival/hazard) and connect them to intuition.\n",
    "- Interpret the **shape** and **scale** parameters via the hazard rate.\n",
    "- Derive moments (mean/variance) using Gamma-function integrals.\n",
    "- Sample from the distribution using a NumPy-only inverse-CDF method.\n",
    "- Use `scipy.stats.weibull_min` for evaluation, sampling, and fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize, special\n",
    "from scipy.stats import weibull_min as weibull_min_dist\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Record versions for reproducibility (useful when numerical details matter).\n",
    "VERSIONS = {\"numpy\": np.__version__, \"scipy\": scipy.__version__, \"plotly\": plotly.__version__}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c628c",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `weibull_min` (Weibull *minimum* distribution; SciPy: `scipy.stats.weibull_min`)\n",
    "- **Type**: Continuous\n",
    "- **Support (standard form)**: $x \\in [0,\\infty)$\n",
    "- **Parameter space (standard form)**: shape $c>0$\n",
    "- **SciPy location/scale**: `loc \\in \\mathbb{R}`, `scale > 0` with\n",
    "  $$X = \\text{loc} + \\text{scale}\\,Y, \\qquad Y \\sim \\mathrm{WeibullMin}(c).$$\n",
    "\n",
    "Unless stated otherwise, this notebook uses the **standard form** (`loc=0`, `scale=1`).\n",
    "\n",
    "> Note on naming: this is the *usual* Weibull distribution used for positive lifetimes. SciPy also provides `weibull_max`, which is a reflected version used in some extreme-value contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce2634",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "The Weibull distribution is a workhorse model for **positive durations** and **lifetimes**. A central concept in reliability is the **hazard rate** (instantaneous failure rate)\n",
    "\n",
    "$$\n",
    " h(x) = \\lim_{\\Delta\\downarrow 0}\\frac{\\mathbb{P}(x\\le X < x+\\Delta \\mid X\\ge x)}{\\Delta}.\n",
    "$$\n",
    "\n",
    "For a Weibull with shape $c$ and scale $\\lambda$ (and `loc=0`), the hazard is a simple power law:\n",
    "\n",
    "$$\n",
    " h(x) = \\frac{c}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{c-1}.\n",
    "$$\n",
    "\n",
    "This gives a clean interpretation:\n",
    "\n",
    "- $c<1$: **decreasing hazard** (“infant mortality” / early failures)\n",
    "- $c=1$: **constant hazard** (memoryless **exponential** case)\n",
    "- $c>1$: **increasing hazard** (“wear-out” / aging)\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Reliability / life testing**: time-to-failure of components, fatigue life.\n",
    "- **Survival analysis**: parametric survival model when hazards are monotone.\n",
    "- **Wind speed and hydrology**: positive-valued environmental measurements.\n",
    "- **Material strength**: weakest-link arguments often motivate Weibull-like models.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Exponential**: if $c=1$, then $X\\sim \\mathrm{Exp}(\\text{scale}=\\lambda)$.\n",
    "- **Rayleigh**: if $c=2$ and $\\lambda=\\sqrt{2}\\,\\sigma$, then $X$ is Rayleigh($\\sigma$).\n",
    "- **Gumbel (minimum) via log transform**: if $X\\sim\\mathrm{Weibull}(c,\\lambda)$ and $Y=\\log X$, then\n",
    "  $$F_Y(y)=\\mathbb{P}(Y\\le y)=1-\\exp\\{-\\exp(c(y-\\log\\lambda))\\},$$\n",
    "  which is a **Gumbel-min** (left-skewed extreme value) distribution.\n",
    "- **Generative story from an exponential**: if $T\\sim\\mathrm{Exp}(1)$ then\n",
    "  $$X = \\lambda\\,T^{1/c} \\sim \\mathrm{Weibull}(c,\\lambda).$$\n",
    "  This directly yields an efficient sampler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689497b",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "We use the common reliability parameterization with **shape** $c>0$ and **scale** $\\lambda>0$ (SciPy’s `scale`). In the standard form, $\\lambda=1$.\n",
    "\n",
    "Let $z = \\frac{x-\\text{loc}}{\\lambda}$.\n",
    "\n",
    "### PDF\n",
    "\n",
    "For $x\\ge \\text{loc}$,\n",
    "\n",
    "$$\n",
    " f(x; c,\\lambda,\\text{loc})\n",
    " = \\frac{c}{\\lambda}\\,z^{c-1}\\,\\exp\\{-z^c\\},\n",
    " \\qquad z=\\frac{x-\\text{loc}}{\\lambda}.\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "For $x\\ge \\text{loc}$,\n",
    "\n",
    "$$\n",
    " F(x; c,\\lambda,\\text{loc}) = 1 - \\exp\\{-z^c\\}.\n",
    "$$\n",
    "\n",
    "Equivalently, the survival function is\n",
    "\n",
    "$$\n",
    " S(x) = 1-F(x)=\\exp\\{-z^c\\}.\n",
    "$$\n",
    "\n",
    "### Hazard rate\n",
    "\n",
    "Whenever $S(x)>0$ (i.e., for finite $x$),\n",
    "\n",
    "$$\n",
    " h(x) = \\frac{f(x)}{S(x)} = \\frac{c}{\\lambda} z^{c-1}.\n",
    "$$\n",
    "\n",
    "### Quantile function (PPF)\n",
    "\n",
    "For $0<q<1$,\n",
    "\n",
    "$$\n",
    " F^{-1}(q) = \\text{loc} + \\lambda\\,\\bigl(-\\log(1-q)\\bigr)^{1/c}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_min_pdf(x: np.ndarray, c: float) -> np.ndarray:\n",
    "    \"\"\"PDF of the standard WeibullMin(c) distribution (loc=0, scale=1).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0:\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x >= 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\", over=\"ignore\"):\n",
    "        out[mask] = c * np.power(xm, c - 1.0) * np.exp(-np.power(xm, c))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_logpdf(x: np.ndarray, c: float) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of the standard WeibullMin(c) distribution (stable for tiny densities).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0:\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "\n",
    "    # Strictly positive region.\n",
    "    mask_pos = x > 0\n",
    "    xp = x[mask_pos]\n",
    "    out[mask_pos] = np.log(c) + (c - 1.0) * np.log(xp) - np.power(xp, c)\n",
    "\n",
    "    # Boundary at 0.\n",
    "    mask_zero = x == 0\n",
    "    if np.any(mask_zero):\n",
    "        if np.isclose(c, 1.0):\n",
    "            out[mask_zero] = 0.0  # pdf(0)=1\n",
    "        elif c < 1.0:\n",
    "            out[mask_zero] = np.inf  # pdf(0)=+inf\n",
    "        else:\n",
    "            out[mask_zero] = -np.inf  # pdf(0)=0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_cdf(x: np.ndarray, c: float) -> np.ndarray:\n",
    "    \"\"\"CDF of the standard WeibullMin(c) distribution.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0:\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x >= 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    # Use expm1 for accuracy near x=0: 1 - exp(-t) = -expm1(-t).\n",
    "    t = np.power(xm, c)\n",
    "    out[mask] = -np.expm1(-t)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_sf(x: np.ndarray, c: float) -> np.ndarray:\n",
    "    \"\"\"Survival function S(x)=P(X>x) for the standard WeibullMin(c).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0:\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "    out = np.ones_like(x, dtype=float)\n",
    "    mask = x >= 0\n",
    "    xm = x[mask]\n",
    "    out[mask] = np.exp(-np.power(xm, c))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_hazard(x: np.ndarray, c: float) -> np.ndarray:\n",
    "    \"\"\"Hazard rate h(x)=f(x)/S(x) for the standard WeibullMin(c).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0:\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x >= 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\", over=\"ignore\"):\n",
    "        out[mask] = c * np.power(xm, c - 1.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_ppf(q: np.ndarray, c: float) -> np.ndarray:\n",
    "    \"\"\"Quantile function (inverse CDF) of the standard WeibullMin(c).\"\"\"\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    if c <= 0:\n",
    "        return np.full_like(q, np.nan, dtype=float)\n",
    "\n",
    "    out = np.full_like(q, np.nan, dtype=float)\n",
    "\n",
    "    mask = (q >= 0) & (q <= 1)\n",
    "    qm = q[mask]\n",
    "\n",
    "    out[mask] = np.power(-np.log1p(-qm), 1.0 / c)\n",
    "    out[q == 0] = 0.0\n",
    "    out[q == 1] = np.inf\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_pdf_loc_scale(x: np.ndarray, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"PDF with SciPy-style loc/scale using the standard-form implementation.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0:\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "    z = (x - loc) / scale\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = z >= 0\n",
    "    out[mask] = weibull_min_pdf(z[mask], c) / scale\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_cdf_loc_scale(x: np.ndarray, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"CDF with SciPy-style loc/scale using the standard-form implementation.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0:\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "    z = (x - loc) / scale\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = z >= 0\n",
    "    out[mask] = weibull_min_cdf(z[mask], c)\n",
    "    return out\n",
    "\n",
    "\n",
    "def weibull_min_ppf_loc_scale(q: np.ndarray, c: float, *, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"PPF with SciPy-style loc/scale using the standard-form implementation.\"\"\"\n",
    "    if scale <= 0:\n",
    "        q = np.asarray(q, dtype=float)\n",
    "        return np.full_like(q, np.nan, dtype=float)\n",
    "\n",
    "    return loc + scale * weibull_min_ppf(q, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: our formulas match SciPy (standard form and loc/scale).\n",
    "\n",
    "c = 1.7\n",
    "x = np.logspace(-4, 2, 25)\n",
    "q = np.linspace(0.01, 0.99, 9)\n",
    "\n",
    "rv_std = weibull_min_dist(c)\n",
    "\n",
    "assert np.allclose(weibull_min_pdf(x, c), rv_std.pdf(x))\n",
    "assert np.allclose(weibull_min_cdf(x, c), rv_std.cdf(x))\n",
    "assert np.allclose(weibull_min_ppf(q, c), rv_std.ppf(q))\n",
    "\n",
    "# loc/scale\n",
    "loc, scale = -0.3, 2.5\n",
    "rv_ls = weibull_min_dist(c, loc=loc, scale=scale)\n",
    "\n",
    "assert np.allclose(weibull_min_pdf_loc_scale(x, c, loc=loc, scale=scale), rv_ls.pdf(x))\n",
    "assert np.allclose(weibull_min_cdf_loc_scale(x, c, loc=loc, scale=scale), rv_ls.cdf(x))\n",
    "assert np.allclose(weibull_min_ppf_loc_scale(q, c, loc=loc, scale=scale), rv_ls.ppf(q))\n",
    "\n",
    "# hazard matches f/S in standard form (avoid survival underflow in the far tail)\n",
    "x_haz = np.logspace(-4, 1, 30)\n",
    "haz = weibull_min_pdf(x_haz, c) / weibull_min_sf(x_haz, c)\n",
    "assert np.allclose(haz, weibull_min_hazard(x_haz, c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fed56",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "A convenient property of the Weibull distribution is that **all positive moments exist** and have a clean Gamma-function form.\n",
    "\n",
    "### Raw moments\n",
    "\n",
    "If $X\\sim\\mathrm{Weibull}(c,\\lambda)$ with `loc=0`, then for any $r>-c$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^r] = \\lambda^r\\,\\Gamma\\!\\left(1+\\frac{r}{c}\\right),\n",
    "$$\n",
    "\n",
    "where $\\Gamma(\\cdot)$ is the Gamma function.\n",
    "\n",
    "### Mean and variance\n",
    "\n",
    "Let $g_k = \\Gamma(1+k/c)$. Then\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\lambda\\,g_1,\n",
    "\\qquad\n",
    "\\mathrm{Var}(X) = \\lambda^2\\,(g_2 - g_1^2).\n",
    "$$\n",
    "\n",
    "### Skewness and kurtosis\n",
    "\n",
    "Using raw moments and central-moment identities, the third and fourth central moments are\n",
    "\n",
    "$$\n",
    "\\mu_3 = \\lambda^3\\,(g_3 - 3 g_1 g_2 + 2 g_1^3),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_4 = \\lambda^4\\,(g_4 - 4 g_1 g_3 + 6 g_1^2 g_2 - 3 g_1^4).\n",
    "$$\n",
    "\n",
    "Skewness and **excess** kurtosis are\n",
    "\n",
    "$$\n",
    "\\gamma_1 = \\frac{\\mu_3}{\\sigma^3},\n",
    "\\qquad\n",
    "\\gamma_2 = \\frac{\\mu_4}{\\sigma^4} - 3.\n",
    "$$\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "There is no simple elementary closed form for the **moment generating function**\n",
    "\n",
    "$$M_X(t)=\\mathbb{E}[e^{tX}],$$\n",
    "\n",
    "but it can be written as a power series using the moments:\n",
    "\n",
    "$$\n",
    "M_X(t) = \\sum_{n=0}^{\\infty} \\frac{t^n}{n!}\\,\\mathbb{E}[X^n] = \\sum_{n=0}^{\\infty}\\frac{(t\\lambda)^n}{n!}\\,\\Gamma\\!\\left(1+\\frac{n}{c}\\right),\n",
    "$$\n",
    "\n",
    "with a radius of convergence that depends on $c$:\n",
    "\n",
    "- $c>1$: $M_X(t)$ exists for all real $t$ (the tail is lighter than exponential).\n",
    "- $c=1$: $M_X(t)$ exists for $t < 1/\\lambda$ (exponential case).\n",
    "- $0<c<1$: $M_X(t)$ diverges for every $t>0$ (tail is heavier than exponential).\n",
    "\n",
    "The **characteristic function** $\\varphi_X(t)=\\mathbb{E}[e^{itX}]$ exists for all real $t$ (bounded integrand).\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy (for `loc=0`) has a simple closed form:\n",
    "\n",
    "$$\n",
    " h(X) = 1 + \\log\\left(\\frac{\\lambda}{c}\\right) + \\gamma\\,\\left(1-\\frac{1}{c}\\right),\n",
    "$$\n",
    "\n",
    "where $\\gamma\\approx 0.57721$ is the Euler–Mascheroni constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5046c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EULER_GAMMA = float(-special.digamma(1.0))  # Euler–Mascheroni constant γ\n",
    "\n",
    "def weibull_min_raw_moment(r: float, c: float, *, scale: float = 1.0) -> float:\n",
    "    \"\"\"E[X^r] for Weibull(c, scale) with loc=0.\"\"\"\n",
    "    if c <= 0 or scale <= 0:\n",
    "        return float(\"nan\")\n",
    "    return float((scale**r) * special.gamma(1.0 + r / c))\n",
    "\n",
    "\n",
    "def weibull_min_mean(c: float, *, scale: float = 1.0, loc: float = 0.0) -> float:\n",
    "    if c <= 0 or scale <= 0:\n",
    "        return float(\"nan\")\n",
    "    return float(loc + scale * special.gamma(1.0 + 1.0 / c))\n",
    "\n",
    "\n",
    "def weibull_min_variance(c: float, *, scale: float = 1.0) -> float:\n",
    "    if c <= 0 or scale <= 0:\n",
    "        return float(\"nan\")\n",
    "    g1 = special.gamma(1.0 + 1.0 / c)\n",
    "    g2 = special.gamma(1.0 + 2.0 / c)\n",
    "    return float((scale**2) * (g2 - g1**2))\n",
    "\n",
    "\n",
    "def weibull_min_skewness(c: float, *, scale: float = 1.0) -> float:\n",
    "    if c <= 0 or scale <= 0:\n",
    "        return float(\"nan\")\n",
    "    g1 = special.gamma(1.0 + 1.0 / c)\n",
    "    g2 = special.gamma(1.0 + 2.0 / c)\n",
    "    g3 = special.gamma(1.0 + 3.0 / c)\n",
    "\n",
    "    mu3 = (scale**3) * (g3 - 3.0 * g1 * g2 + 2.0 * g1**3)\n",
    "    var = (scale**2) * (g2 - g1**2)\n",
    "    return float(mu3 / (var ** 1.5))\n",
    "\n",
    "\n",
    "def weibull_min_excess_kurtosis(c: float, *, scale: float = 1.0) -> float:\n",
    "    if c <= 0 or scale <= 0:\n",
    "        return float(\"nan\")\n",
    "    g1 = special.gamma(1.0 + 1.0 / c)\n",
    "    g2 = special.gamma(1.0 + 2.0 / c)\n",
    "    g3 = special.gamma(1.0 + 3.0 / c)\n",
    "    g4 = special.gamma(1.0 + 4.0 / c)\n",
    "\n",
    "    mu4 = (scale**4) * (g4 - 4.0 * g1 * g3 + 6.0 * (g1**2) * g2 - 3.0 * g1**4)\n",
    "    var = (scale**2) * (g2 - g1**2)\n",
    "    return float(mu4 / (var**2) - 3.0)\n",
    "\n",
    "\n",
    "def weibull_min_entropy(c: float, *, scale: float = 1.0) -> float:\n",
    "    if c <= 0 or scale <= 0:\n",
    "        return float(\"nan\")\n",
    "    return float(1.0 + np.log(scale / c) + EULER_GAMMA * (1.0 - 1.0 / c))\n",
    "\n",
    "\n",
    "# Compare to SciPy.\n",
    "c = 1.5\n",
    "scale = 2.0\n",
    "rv = weibull_min_dist(c, scale=scale)\n",
    "\n",
    "mean_sp, var_sp, skew_sp, kurt_sp = rv.stats(moments=\"mvsk\")\n",
    "entropy_sp = rv.entropy()\n",
    "\n",
    "mean_f = weibull_min_mean(c, scale=scale)\n",
    "var_f = weibull_min_variance(c, scale=scale)\n",
    "skew_f = weibull_min_skewness(c, scale=scale)\n",
    "kurt_f = weibull_min_excess_kurtosis(c, scale=scale)\n",
    "entropy_f = weibull_min_entropy(c, scale=scale)\n",
    "\n",
    "print(\"SciPy vs formulas (c=1.5, scale=2.0):\")\n",
    "print(\"  mean   \", float(mean_sp), \"|\", mean_f)\n",
    "print(\"  var    \", float(var_sp), \"|\", var_f)\n",
    "print(\"  skew   \", float(skew_sp), \"|\", skew_f)\n",
    "print(\"  kurt   \", float(kurt_sp), \"|\", kurt_f)\n",
    "print(\"  entropy\", float(entropy_sp), \"|\", entropy_f)\n",
    "\n",
    "assert np.allclose([mean_sp, var_sp, skew_sp, kurt_sp, entropy_sp], [mean_f, var_f, skew_f, kurt_f, entropy_f])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04445831",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "SciPy’s `weibull_min` uses:\n",
    "\n",
    "- `c` as the **shape** parameter (often written $k$ or $\\beta$)\n",
    "- `scale` as the **scale** parameter (often written $\\lambda$ or $\\eta$)\n",
    "- `loc` as a **location shift**\n",
    "\n",
    "### Shape `c`\n",
    "\n",
    "- Controls the **hazard rate** behavior:\n",
    "  $$h(x)=\\frac{c}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{c-1}.$$\n",
    "- Also controls the **shape near zero**:\n",
    "  - if $c<1$, the density blows up at $x=0$ (many very small values)\n",
    "  - if $c=1$, the density is finite at $x=0$ (exponential)\n",
    "  - if $c>1$, the density is 0 at $x=0$ and has a mode at $x>0$\n",
    "\n",
    "### Scale `scale = \\lambda`\n",
    "\n",
    "- Stretches the distribution horizontally: if $Y\\sim\\mathrm{Weibull}(c,1)$ then $X=\\lambda Y\\sim\\mathrm{Weibull}(c,\\lambda)$.\n",
    "- For lifetimes, $\\lambda$ is a *characteristic life*: $F(\\lambda)=1-e^{-1}\\approx 0.632$.\n",
    "\n",
    "### Location `loc`\n",
    "\n",
    "- Shifts support: support becomes $x\\ge \\text{loc}$.\n",
    "- Useful for modeling a **minimum lifetime** (or a measurement offset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2012bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape effects: PDF and hazard for different c (standard form).\n",
    "\n",
    "x_pdf = np.linspace(1e-4, 4.0, 800)\n",
    "x_haz = np.logspace(-3, 1, 600)\n",
    "\n",
    "c_values = [0.5, 1.0, 1.5, 3.0]\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in c_values:\n",
    "    y = weibull_min_pdf(x_pdf, c)\n",
    "    fig.add_trace(go.Scatter(x=x_pdf, y=y, mode=\"lines\", name=f\"c={c}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Weibull_min PDF for different shapes (scale=1)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x; c)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in c_values:\n",
    "    y = weibull_min_hazard(x_haz, c)\n",
    "    fig.add_trace(go.Scatter(x=x_haz, y=y, mode=\"lines\", name=f\"c={c}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Hazard rate h(x)=c x^{c-1} for different shapes (scale=1)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"h(x)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# Scale effects at fixed c.\n",
    "c = 2.0\n",
    "scales = [0.5, 1.0, 2.0]\n",
    "\n",
    "x = np.linspace(0, 6, 900)\n",
    "fig = go.Figure()\n",
    "for scale in scales:\n",
    "    y = weibull_min_pdf_loc_scale(x, c, loc=0.0, scale=scale)\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=f\"scale={scale}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Scale stretches the distribution (fixed c=2)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x; c, scale)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF shapes (standard form).\n",
    "\n",
    "x = np.linspace(0, 6.0, 800)\n",
    "c_values = [0.5, 1.0, 1.5, 3.0]\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in c_values:\n",
    "    fig.add_trace(go.Scatter(x=x, y=weibull_min_cdf(x, c), mode=\"lines\", name=f\"c={c}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Weibull_min CDF for different shapes (scale=1)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x; c)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10362ad7",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "We derive moments and the likelihood in the `loc=0` case. (Location just shifts $X$ and does not change variance or shape.)\n",
    "\n",
    "### 6.1 Expectation and general moments\n",
    "\n",
    "Start from the PDF with `loc=0`:\n",
    "\n",
    "$$\n",
    " f(x)=\\frac{c}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{c-1}\\exp\\{-(x/\\lambda)^c\\},\\qquad x\\ge 0.\n",
    "$$\n",
    "\n",
    "For $r>-c$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^r]\n",
    "=\\int_0^\\infty x^r f(x)\\,dx.\n",
    "$$\n",
    "\n",
    "Use the substitution $u=(x/\\lambda)^c \\Rightarrow x=\\lambda u^{1/c}$ and\n",
    "$dx = \\lambda\\,\\frac{1}{c}\\,u^{1/c - 1}\\,du$.\n",
    "\n",
    "After cancellation, the integral becomes\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^r]\n",
    "= \\lambda^r \\int_0^\\infty u^{r/c} e^{-u}\\,du\n",
    "= \\lambda^r\\,\\Gamma\\!\\left(1+\\frac{r}{c}\\right).\n",
    "$$\n",
    "\n",
    "Setting $r=1$ and $r=2$ yields mean and variance.\n",
    "\n",
    "### 6.2 Variance\n",
    "\n",
    "Using $\\mathrm{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2$ and the moment formula:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X) = \\lambda^2\\Bigl(\\Gamma(1+2/c) - \\Gamma(1+1/c)^2\\Bigr).\n",
    "$$\n",
    "\n",
    "### 6.3 Likelihood (i.i.d. sample)\n",
    "\n",
    "Let $x_1,\\dots,x_n$ be i.i.d. from a Weibull with parameters $(c,\\lambda)$ and `loc=0`.\n",
    "\n",
    "The likelihood is\n",
    "\n",
    "$$\n",
    "L(c,\\lambda; x_{1:n})\n",
    "= \\prod_{i=1}^n \\frac{c}{\\lambda}\\left(\\frac{x_i}{\\lambda}\\right)^{c-1}\\exp\\{-(x_i/\\lambda)^c\\}.\n",
    "$$\n",
    "\n",
    "The log-likelihood simplifies to\n",
    "\n",
    "$$\n",
    "\\ell(c,\\lambda)\n",
    "= n\\log c + (c-1)\\sum_{i=1}^n \\log x_i - nc\\log\\lambda - \\sum_{i=1}^n (x_i/\\lambda)^c.\n",
    "$$\n",
    "\n",
    "A useful fact: for fixed $c$, the MLE of $\\lambda$ has a closed form:\n",
    "\n",
    "$$\n",
    "\\hat\\lambda(c)=\\left(\\frac{1}{n}\\sum_{i=1}^n x_i^c\\right)^{1/c}.\n",
    "$$\n",
    "\n",
    "Plugging $\\hat\\lambda(c)$ back into $\\ell$ yields a **profile likelihood** in $c$; the resulting score equation has to be solved numerically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa560569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_min_loglik(c: float, scale: float, x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0 or scale <= 0 or np.any(x <= 0):\n",
    "        return -np.inf\n",
    "\n",
    "    n = x.size\n",
    "    return float(n * np.log(c) + (c - 1.0) * np.sum(np.log(x)) - n * c * np.log(scale) - np.sum((x / scale) ** c))\n",
    "\n",
    "\n",
    "def weibull_min_scale_hat(c: float, x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0 or np.any(x <= 0):\n",
    "        return float(\"nan\")\n",
    "    return float(np.mean(x**c) ** (1.0 / c))\n",
    "\n",
    "\n",
    "def weibull_min_shape_score_profile(c: float, x: np.ndarray) -> float:\n",
    "    \"\"\"Score equation in c after profiling out scale (loc=0).\n",
    "\n",
    "    Root of this function gives the MLE for c.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if c <= 0 or np.any(x <= 0):\n",
    "        return float(\"nan\")\n",
    "\n",
    "    logx = np.log(x)\n",
    "    x_c = x**c\n",
    "\n",
    "    return float(1.0 / c + np.mean(logx) - np.sum(x_c * logx) / np.sum(x_c))\n",
    "\n",
    "\n",
    "# MLE demo in the standard loc=0 form.\n",
    "c_true = 1.7\n",
    "scale_true = 2.5\n",
    "x = weibull_min_dist(c_true, scale=scale_true).rvs(size=4_000, random_state=rng)\n",
    "\n",
    "# Find a bracket where the profile score changes sign.\n",
    "c_grid = np.linspace(0.15, 8.0, 400)\n",
    "score_vals = np.array([weibull_min_shape_score_profile(c, x) for c in c_grid])\n",
    "\n",
    "idx = np.where(np.sign(score_vals[:-1]) * np.sign(score_vals[1:]) < 0)[0]\n",
    "if idx.size == 0:\n",
    "    raise RuntimeError(\"Could not bracket the MLE root for c; try a wider grid.\")\n",
    "\n",
    "c_lo, c_hi = float(c_grid[idx[0]]), float(c_grid[idx[0] + 1])\n",
    "sol = optimize.root_scalar(weibull_min_shape_score_profile, bracket=(c_lo, c_hi), args=(x,), method=\"brentq\")\n",
    "\n",
    "c_hat = float(sol.root)\n",
    "scale_hat = weibull_min_scale_hat(c_hat, x)\n",
    "\n",
    "print(\"True (c, scale):\", (c_true, scale_true))\n",
    "print(\"MLE  (c, scale):\", (c_hat, scale_hat))\n",
    "\n",
    "# Compare to SciPy's fit (fix loc=0).\n",
    "c_hat_sp, loc_hat_sp, scale_hat_sp = weibull_min_dist.fit(x, floc=0)\n",
    "print(\"SciPy fit (floc=0):\", (float(c_hat_sp), float(scale_hat_sp)))\n",
    "\n",
    "# Profile log-likelihood over c (scale profiled out).\n",
    "c_grid = np.linspace(0.3, 5.0, 250)\n",
    "ll_prof = np.array([weibull_min_loglik(c, weibull_min_scale_hat(c, x), x) for c in c_grid])\n",
    "\n",
    "fig = go.Figure(go.Scatter(x=c_grid, y=ll_prof, mode=\"lines\", name=\"profile loglik\"))\n",
    "fig.add_vline(x=c_true, line_dash=\"dash\", line_color=\"green\", annotation_text=\"true c\")\n",
    "fig.add_vline(x=c_hat, line_dash=\"dash\", line_color=\"red\", annotation_text=\"MLE c\")\n",
    "fig.update_layout(title=\"Profile log-likelihood for c (loc=0)\", xaxis_title=\"c\", yaxis_title=\"log-likelihood\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280cc49",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only algorithm (inverse transform)\n",
    "\n",
    "From the CDF in the `loc=0` case:\n",
    "\n",
    "$$\n",
    "F(x)=1-\\exp\\{-(x/\\lambda)^c\\}.\n",
    "$$\n",
    "\n",
    "Let $U\\sim\\mathrm{Uniform}(0,1)$. Setting $U=F(X)$ and solving for $X$ gives\n",
    "\n",
    "$$\n",
    "X = \\lambda\\,\\bigl(-\\log(1-U)\\bigr)^{1/c}.\n",
    "$$\n",
    "\n",
    "This is an **exact** sampler (no rejection needed) and is typically the fastest way to generate Weibull samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_min_rvs_numpy(\n",
    "    c: float,\n",
    "    *,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    "    size: int | tuple[int, ...] = 1,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Sample from weibull_min using NumPy only (inverse-CDF sampler).\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    if c <= 0 or scale <= 0:\n",
    "        raise ValueError(\"Require c>0 and scale>0\")\n",
    "\n",
    "    u = rng.random(size=size)\n",
    "    # -log1p(-u) is stable when u is very close to 1.\n",
    "    x = scale * np.power(-np.log1p(-u), 1.0 / c)\n",
    "    return loc + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462834d",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll compare:\n",
    "\n",
    "- the theoretical PDF and CDF\n",
    "- Monte Carlo samples (NumPy-only sampler)\n",
    "- SciPy’s implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72810c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1.3\n",
    "scale = 2.0\n",
    "n = 80_000\n",
    "\n",
    "x_np = weibull_min_rvs_numpy(c, scale=scale, size=n, rng=rng)\n",
    "x_sp = weibull_min_dist(c, scale=scale).rvs(size=n, random_state=rng)\n",
    "\n",
    "# Histogram vs theoretical PDF\n",
    "x_grid = np.linspace(0, np.quantile(x_np, 0.995), 500)\n",
    "pdf_grid = weibull_min_pdf_loc_scale(x_grid, c, loc=0.0, scale=scale)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=x_np,\n",
    "    nbins=140,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Monte Carlo histogram (NumPy-only) vs theoretical PDF\",\n",
    "    labels={\"x\": \"x\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=pdf_grid, mode=\"lines\", name=\"theoretical PDF\"))\n",
    "fig.show()\n",
    "\n",
    "# Empirical CDF vs theoretical CDF\n",
    "x_sorted = np.sort(x_np)\n",
    "ecdf = np.arange(1, n + 1) / n\n",
    "cdf_grid = weibull_min_cdf_loc_scale(x_grid, c, loc=0.0, scale=scale)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_sorted, y=ecdf, mode=\"lines\", name=\"empirical CDF (NumPy-only)\"))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=cdf_grid, mode=\"lines\", name=\"theoretical CDF\"))\n",
    "fig.update_layout(title=\"CDF: empirical vs theoretical\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "fig.show()\n",
    "\n",
    "# Quick check: NumPy-only samples and SciPy samples should look similar.\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "ks = ks_2samp(x_np, x_sp)\n",
    "print(\"KS two-sample test (NumPy vs SciPy samples):\")\n",
    "print(ks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cc7c8",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "`scipy.stats.weibull_min` provides the standard distribution API:\n",
    "\n",
    "- `weibull_min.pdf(x, c, loc=0, scale=1)`\n",
    "- `weibull_min.cdf(x, c, loc=0, scale=1)`\n",
    "- `weibull_min.rvs(c, loc=0, scale=1, size=..., random_state=...)`\n",
    "- `weibull_min.fit(data, ...)` (MLE)\n",
    "\n",
    "A common workflow is to **freeze** the distribution: `rv = weibull_min(c, loc=..., scale=...)`, then call `rv.pdf`, `rv.cdf`, `rv.rvs`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1.8\n",
    "loc = 0.0\n",
    "scale = 3.0\n",
    "\n",
    "rv = weibull_min_dist(c, loc=loc, scale=scale)\n",
    "\n",
    "x = np.array([0.2, 1.0, 3.0, 8.0])\n",
    "print(\"pdf:\", rv.pdf(x))\n",
    "print(\"cdf:\", rv.cdf(x))\n",
    "print(\"sf :\", rv.sf(x))\n",
    "\n",
    "samples = rv.rvs(size=5, random_state=rng)\n",
    "print(\"rvs:\", samples)\n",
    "\n",
    "# Fitting: estimate (c, scale) with loc fixed to 0.\n",
    "true_c, true_scale = 1.4, 2.2\n",
    "data = weibull_min_dist(true_c, scale=true_scale).rvs(size=5_000, random_state=rng)\n",
    "\n",
    "c_hat, loc_hat, scale_hat = weibull_min_dist.fit(data, floc=0)\n",
    "print(\"\\nFit (fixed loc=0):\")\n",
    "print(\"  true (c, scale):\", (true_c, true_scale))\n",
    "print(\"  est  (c, scale):\", (float(c_hat), float(scale_hat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165663b",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing (goodness-of-fit)\n",
    "\n",
    "If the parameters are **specified in advance** (not fit from the same sample), you can test whether data plausibly comes from a Weibull distribution using a goodness-of-fit test such as Kolmogorov–Smirnov (KS).\n",
    "\n",
    "Caveat: if you estimate parameters from the data and then run KS on the same data, the usual KS p-values are no longer exact (use a parametric bootstrap or a corrected procedure).\n",
    "\n",
    "### Bayesian modeling\n",
    "\n",
    "There is no simple conjugate prior for $(c,\\lambda)$ jointly, but there *is* a convenient conjugate update when **shape $c$ is known**.\n",
    "\n",
    "If $X\\sim\\mathrm{Weibull}(c,\\lambda)$ with `loc=0`, then $Y=X^c$ has\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y\\le y) = 1-\\exp\\{-y/\\lambda^c\\},\n",
    "$$\n",
    "\n",
    "so $Y\\sim\\mathrm{Exp}(\\text{rate}=\\beta)$ with $\\beta = 1/\\lambda^c$.\n",
    "\n",
    "A Gamma prior on the rate $\\beta$ is conjugate.\n",
    "\n",
    "### Generative modeling\n",
    "\n",
    "Weibull distributions are commonly used as **generative models for survival times** and as components of mixture models (e.g., to model early-failure and wear-out subpopulations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing example: KS test when parameters are known.\n",
    "\n",
    "from scipy.stats import kstest\n",
    "\n",
    "c = 1.2\n",
    "scale = 2.0\n",
    "x = weibull_min_dist(c, scale=scale).rvs(size=2_000, random_state=rng)\n",
    "\n",
    "D, p_value = kstest(x, weibull_min_dist(c, scale=scale).cdf)\n",
    "print(\"KS test against Weibull(c=1.2, scale=2.0):\")\n",
    "print(\"  D      =\", D)\n",
    "print(\"  p-value=\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a63ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian modeling (conjugate update) when shape c is known.\n",
    "# Model: X ~ Weibull(c, scale=lambda), loc=0.\n",
    "# Transform: Y = X^c ~ Exp(rate=beta) with beta = 1 / lambda^c.\n",
    "# Prior: beta ~ Gamma(alpha0, rate=r0).\n",
    "\n",
    "from scipy.stats import gamma as gamma_dist\n",
    "\n",
    "rng_local = np.random.default_rng(123)\n",
    "\n",
    "c_known = 1.6\n",
    "lambda_true = 2.5\n",
    "x = weibull_min_dist(c_known, scale=lambda_true).rvs(size=800, random_state=rng_local)\n",
    "\n",
    "y = x**c_known\n",
    "\n",
    "# Conjugate Gamma prior on beta (rate parameterization).\n",
    "alpha0 = 2.0\n",
    "r0 = 1.0\n",
    "\n",
    "alpha_post = alpha0 + y.size\n",
    "r_post = r0 + np.sum(y)\n",
    "\n",
    "# Draw posterior samples for beta, then transform to lambda.\n",
    "beta_samps = gamma_dist(a=alpha_post, scale=1.0 / r_post).rvs(size=50_000, random_state=rng_local)\n",
    "lambda_samps = (1.0 / beta_samps) ** (1.0 / c_known)\n",
    "\n",
    "ci = np.quantile(lambda_samps, [0.05, 0.5, 0.95])\n",
    "print(\"True lambda:\", lambda_true)\n",
    "print(\"Posterior lambda 90% CI + median:\", ci)\n",
    "\n",
    "fig = px.histogram(\n",
    "    lambda_samps,\n",
    "    nbins=120,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Posterior over scale (lambda) with known shape c\",\n",
    "    labels={\"value\": \"lambda\"},\n",
    ")\n",
    "fig.add_vline(x=lambda_true, line_dash=\"dash\", line_color=\"green\", annotation_text=\"true\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d042eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling example: early-failure vs wear-out mixture.\n",
    "\n",
    "n = 60_000\n",
    "\n",
    "# Early failures: c<1 (decreasing hazard), shorter characteristic life.\n",
    "x_early = weibull_min_rvs_numpy(0.7, scale=0.8, size=n // 2, rng=rng)\n",
    "\n",
    "# Wear-out: c>1 (increasing hazard), longer characteristic life.\n",
    "x_wear = weibull_min_rvs_numpy(3.0, scale=2.0, size=n // 2, rng=rng)\n",
    "\n",
    "x_mix = np.concatenate([x_early, x_wear])\n",
    "\n",
    "fig = px.histogram(\n",
    "    x_mix,\n",
    "    nbins=160,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Mixture of Weibulls (early-failure + wear-out)\",\n",
    "    labels={\"value\": \"time\"},\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"Mixture summaries:\")\n",
    "print(\"  mean   =\", float(x_mix.mean()))\n",
    "print(\"  median =\", float(np.median(x_mix)))\n",
    "print(\"  90%    =\", float(np.quantile(x_mix, 0.9)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ebcd3",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: require `c>0` and `scale>0`. With `loc`, support is $x\\ge \\text{loc}$.\n",
    "- **Boundary behavior at 0**:\n",
    "  - if $c<1$, the PDF and hazard blow up at 0 (this is a *feature* of the model, not a bug).\n",
    "  - plots that include exactly $x=0$ can show infinities; start from a small $\\varepsilon>0$.\n",
    "- **Numerical stability**:\n",
    "  - use `logpdf` when multiplying many densities or when probabilities are tiny;\n",
    "  - for $x$ near 0, compute CDF via `-expm1(-t)` (as done above);\n",
    "  - for $q$ near 1, use `-log1p(-q)` in the PPF (as done above).\n",
    "- **Fitting with `loc`**:\n",
    "  - allowing `loc` to vary can lead to unstable fits or unintuitive parameter estimates;\n",
    "  - in reliability, it’s common to fix `loc=0` unless a physical minimum lifetime is justified.\n",
    "- **Model misspecification**:\n",
    "  - Weibull enforces a *monotone* hazard; if the true hazard is bathtub-shaped (decrease then increase), consider mixtures or more flexible survival models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c565b2",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `weibull_min` is a continuous distribution on $[0,\\infty)$ parameterized by shape $c>0$ and (optionally) `loc` and `scale`.\n",
    "- Its CDF has the simple form $F(x)=1-\\exp\\{-(x/\\lambda)^c\\}$, leading to an exact inverse-CDF sampler.\n",
    "- The hazard rate is $h(x)=(c/\\lambda)(x/\\lambda)^{c-1}$: decreasing for $c<1$, constant for $c=1$, increasing for $c>1$.\n",
    "- Moments are expressed with the Gamma function: $\\mathbb{E}[X^r]=\\lambda^r\\Gamma(1+r/c)$.\n",
    "- `scipy.stats.weibull_min` provides robust numerics for PDF/CDF/SF/PPF, sampling, and MLE fitting.\n",
    "\n",
    "### References\n",
    "\n",
    "- Johnson, Kotz, and Balakrishnan. *Continuous Univariate Distributions, Volume 1* (2nd ed.), Wiley, 1994.\n",
    "- Nelson. *Applied Life Data Analysis*, Wiley, 1982.\n",
    "- SciPy documentation: `scipy.stats.weibull_min`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}