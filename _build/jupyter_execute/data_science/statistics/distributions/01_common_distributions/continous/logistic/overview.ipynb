{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9353cc0a",
   "metadata": {},
   "source": [
    "# Logistic distribution — the “sigmoid” law on ℝ\n",
    "\n",
    "The **logistic** distribution is a symmetric, bell-shaped continuous distribution on the real line whose CDF is the **logistic (sigmoid) function**.\n",
    "It is closely tied to **log-odds (logit) transformations**, to **logistic regression** (as an error model), and it provides a simple, heavier-tailed alternative to the normal distribution.\n",
    "\n",
    "## What you’ll learn\n",
    "- how the PDF/CDF/quantile relate to the sigmoid and logit\n",
    "- closed-form moments (mean/variance/skewness/kurtosis), MGF/CF, and entropy\n",
    "- parameter interpretation (location $\\mu$, scale $s$) and how shape changes\n",
    "- **NumPy-only** sampling via inverse transform + Monte Carlo validation\n",
    "- practical usage via `scipy.stats.logistic` (`pdf`, `cdf`, `rvs`, `fit`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd134a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize, stats\n",
    "from scipy.stats import chi2, logistic, norm\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(7)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"Python\", platform.python_version())\n",
    "print(\"NumPy\", np.__version__)\n",
    "print(\"SciPy\", scipy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bf2cb",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `logistic`\n",
    "- **Type**: **continuous** distribution\n",
    "- **Support**: $x \\in (-\\infty, \\infty)$\n",
    "- **Parameter space**: location $\\mu \\in \\mathbb{R}$ and scale $s > 0$\n",
    "\n",
    "We write:\n",
    "\n",
    "$$X \\sim \\mathrm{Logistic}(\\mu, s).$$\n",
    "\n",
    "The **standard logistic** is $\\mathrm{Logistic}(0,1)$.\n",
    "\n",
    "> SciPy uses the same location/scale form: `stats.logistic(loc=mu, scale=s)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377d6dd",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### 2.1 What it models\n",
    "The logistic distribution is a good model for **real-valued noise** that is:\n",
    "\n",
    "- **symmetric** (centered around $\\mu$)\n",
    "- **unimodal** (single peak at $\\mu$)\n",
    "- **heavier-tailed than a normal** (but still exponentially decaying)\n",
    "\n",
    "A practical intuition: compared to a normal distribution with the same variance, logistic puts **more probability mass in the tails**.\n",
    "\n",
    "### 2.2 Typical real-world use cases\n",
    "- **Latent-variable view of logistic regression**: if a latent score is perturbed by *logistic* noise and thresholded, the resulting class probability is a sigmoid.\n",
    "- **Log-odds modeling**: if $P \\in (0,1)$ is a random probability, then $\\log\\!\\left(\\frac{P}{1-P}\\right)$ lives on $\\mathbb{R}$; logistic is a natural simple choice for such log-odds.\n",
    "- **Convenient alternative to a normal**: similar bell shape, simple CDF/quantile.\n",
    "- **Mixture models / generative models**: mixtures of logistics are used to model complex continuous densities (notably in some neural image models).\n",
    "\n",
    "### 2.3 Relations to other distributions\n",
    "- **Uniform ↔ logistic (logit link)**: if $U\\sim\\mathrm{Unif}(0,1)$, then\n",
    "  $$\\log\\!\\left(\\frac{U}{1-U}\\right) \\sim \\mathrm{Logistic}(0,1).$$\n",
    "  Conversely, if $X\\sim\\mathrm{Logistic}(\\mu,s)$ then $F(X)$ is Uniform$(0,1)$.\n",
    "\n",
    "- **Gumbel difference**: if $G_1, G_2$ are i.i.d. Gumbel with the same scale, then $G_1 - G_2$ is logistic.\n",
    "\n",
    "- **Normal approximation**: matching variances gives\n",
    "  $$\\mathrm{Logistic}(0, s) \\approx \\mathcal{N}(0,1) \\quad\\text{when}\\quad s=\\sqrt{3}/\\pi\\approx 0.5513.$$\n",
    "\n",
    "- **Log-logistic**: if $X\\sim\\mathrm{Logistic}(\\mu,s)$, then $\\exp(X)$ is log-logistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b27cf3",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let\n",
    "\n",
    "$$z = \\frac{x-\\mu}{s}.$$\n",
    "\n",
    "### 3.1 PDF\n",
    "Different equivalent forms are useful:\n",
    "\n",
    "$$f(x\\mid\\mu,s) = \\frac{e^{-z}}{s\\,(1+e^{-z})^2}\n",
    "= \\frac{1}{s}\\,\\sigma(z)\\bigl(1-\\sigma(z)\\bigr)\n",
    "= \\frac{1}{4s}\\,\\operatorname{sech}^2\\!\\left(\\frac{z}{2}\\right),$$\n",
    "\n",
    "where $\\sigma(z)=\\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "### 3.2 CDF\n",
    "\n",
    "$$F(x\\mid\\mu,s) = \\sigma\\!\\left(\\frac{x-\\mu}{s}\\right)=\\frac{1}{1+e^{-z}}.$$\n",
    "\n",
    "### 3.3 Quantile function (inverse CDF)\n",
    "For $p\\in(0,1)$:\n",
    "\n",
    "$$F^{-1}(p) = \\mu + s\\,\\log\\!\\left(\\frac{p}{1-p}\\right).$$\n",
    "\n",
    "This closed-form inverse CDF makes **inverse transform sampling** especially simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e56a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    # Stable logistic function σ(z) = 1 / (1 + exp(-z)).\n",
    "\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    out = np.empty_like(z)\n",
    "\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "\n",
    "    ez = np.exp(z[~pos])\n",
    "    out[~pos] = ez / (1.0 + ez)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def logistic_cdf(x: np.ndarray, mu: float = 0.0, s: float = 1.0) -> np.ndarray:\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "    z = (np.asarray(x, dtype=float) - mu) / s\n",
    "    return sigmoid(z)\n",
    "\n",
    "\n",
    "def logistic_pdf(x: np.ndarray, mu: float = 0.0, s: float = 1.0) -> np.ndarray:\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "    z = (np.asarray(x, dtype=float) - mu) / s\n",
    "    p = sigmoid(z)\n",
    "    return (p * (1.0 - p)) / s\n",
    "\n",
    "\n",
    "def logistic_logpdf(x: np.ndarray, mu: float = 0.0, s: float = 1.0) -> np.ndarray:\n",
    "    # Stable log-PDF using logaddexp:\n",
    "    # log f(x) = -log s - z - 2 log(1 + exp(-z)), where z=(x-mu)/s.\n",
    "\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "    z = (np.asarray(x, dtype=float) - mu) / s\n",
    "    return -np.log(s) - z - 2.0 * np.logaddexp(0.0, -z)\n",
    "\n",
    "\n",
    "def logistic_ppf(p: np.ndarray, mu: float = 0.0, s: float = 1.0, eps: float = 1e-12) -> np.ndarray:\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = np.clip(p, eps, 1.0 - eps)\n",
    "    return mu + s * (np.log(p) - np.log1p(-p))\n",
    "\n",
    "\n",
    "def logistic_rvs(\n",
    "    rng: np.random.Generator,\n",
    "    size: int | tuple[int, ...],\n",
    "    mu: float = 0.0,\n",
    "    s: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    # NumPy-only sampling via inverse CDF.\n",
    "\n",
    "    u = rng.random(size=size)\n",
    "    return logistic_ppf(u, mu=mu, s=s)\n",
    "\n",
    "\n",
    "def logistic_moments(mu: float = 0.0, s: float = 1.0) -> dict:\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "\n",
    "    mean = mu\n",
    "    var = (np.pi * s) ** 2 / 3.0\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"variance\": var,\n",
    "        \"skewness\": 0.0,\n",
    "        \"kurtosis\": 4.2,  # non-excess\n",
    "        \"excess_kurtosis\": 6.0 / 5.0,\n",
    "        \"median\": mu,\n",
    "        \"mode\": mu,\n",
    "    }\n",
    "\n",
    "\n",
    "def logistic_entropy(s: float = 1.0) -> float:\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "    return float(np.log(s) + 2.0)\n",
    "\n",
    "\n",
    "def logistic_mgf(t: np.ndarray, mu: float = 0.0, s: float = 1.0) -> np.ndarray:\n",
    "    # MGF M_X(t) = E[e^{tX}] for |t| < 1/s.\n",
    "\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    x = np.pi * s * t\n",
    "\n",
    "    out = np.full_like(t, np.nan, dtype=float)\n",
    "    ok = np.abs(t) < (1.0 / s)\n",
    "\n",
    "    ratio = np.empty_like(x)\n",
    "    small = np.abs(x) < 1e-4\n",
    "    ratio[small] = 1.0 + (x[small] ** 2) / 6.0 + 7.0 * (x[small] ** 4) / 360.0\n",
    "    ratio[~small] = x[~small] / np.sin(x[~small])\n",
    "\n",
    "    out[ok] = np.exp(mu * t[ok]) * ratio[ok]\n",
    "    return out\n",
    "\n",
    "\n",
    "def logistic_cf(t: np.ndarray, mu: float = 0.0, s: float = 1.0) -> np.ndarray:\n",
    "    # Characteristic function φ_X(t) = E[e^{itX}] for real t.\n",
    "\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"scale s must be > 0\")\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    x = np.pi * s * t\n",
    "\n",
    "    ratio = np.empty_like(x)\n",
    "    small = np.abs(x) < 1e-4\n",
    "    ratio[small] = 1.0 - (x[small] ** 2) / 6.0 + 7.0 * (x[small] ** 4) / 360.0\n",
    "    ratio[~small] = x[~small] / np.sinh(x[~small])\n",
    "\n",
    "    return np.exp(1j * mu * t) * ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdc787",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let $X\\sim\\mathrm{Logistic}(\\mu,s)$.\n",
    "\n",
    "### 4.1 Mean, variance, skewness, kurtosis\n",
    "- **Mean**: $\\mathbb{E}[X] = \\mu$.\n",
    "- **Variance**: $\\mathrm{Var}(X) = \\dfrac{\\pi^2 s^2}{3}$.\n",
    "- **Skewness**: $0$ (symmetry).\n",
    "- **Kurtosis**: $4.2$ (so **excess kurtosis** is $6/5=1.2$).\n",
    "\n",
    "Also:\n",
    "\n",
    "- **Median**: $\\mu$.\n",
    "- **Mode**: $\\mu$.\n",
    "\n",
    "### 4.2 MGF and characteristic function\n",
    "The MGF exists only on a strip around 0 (because tails are exponential):\n",
    "\n",
    "$$M_X(t)=\\mathbb{E}[e^{tX}] = e^{\\mu t}\\,\\frac{\\pi s t}{\\sin(\\pi s t)},\\qquad |t|<\\frac{1}{s}.$$\n",
    "\n",
    "The characteristic function exists for all real $t$:\n",
    "\n",
    "$$\\varphi_X(t)=\\mathbb{E}[e^{itX}] = e^{i\\mu t}\\,\\frac{\\pi s t}{\\sinh(\\pi s t)}.$$\n",
    "\n",
    "### 4.3 Differential entropy\n",
    "The logistic distribution has a simple differential entropy:\n",
    "\n",
    "$$h(X) = \\ln(s) + 2.$$\n",
    "\n",
    "### 4.4 Tail behavior\n",
    "For large $|x|$, the logistic density behaves like\n",
    "\n",
    "$$f(x) \\approx \\frac{1}{s}e^{-|x-\\mu|/s},$$\n",
    "\n",
    "so it has **exponential** tails (heavier than Gaussian, lighter than power-law tails).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46013b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick numerical checks: moments + MGF (Monte Carlo)\n",
    "mu0, s0 = 0.7, 1.3\n",
    "n = 200_000\n",
    "\n",
    "samples = logistic_rvs(rng, size=n, mu=mu0, s=s0)\n",
    "\n",
    "mom = logistic_moments(mu=mu0, s=s0)\n",
    "mean_mc = samples.mean()\n",
    "var_mc = samples.var(ddof=0)\n",
    "\n",
    "skew_mc = stats.skew(samples)\n",
    "kurt_mc = stats.kurtosis(samples, fisher=False)  # non-excess\n",
    "\n",
    "mom, mean_mc, var_mc, skew_mc, kurt_mc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52215187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MGF check for a few t in the valid range |t| < 1/s\n",
    "# (Monte Carlo estimate: mean(exp(tX)))\n",
    "\n",
    "ts = np.array([-0.4, -0.2, 0.2, 0.4]) / s0  # safely within (-1/s, 1/s)\n",
    "\n",
    "mgf_theory = logistic_mgf(ts, mu=mu0, s=s0)\n",
    "mgf_mc = np.array([np.mean(np.exp(t * samples)) for t in ts])\n",
    "\n",
    "np.column_stack([ts, mgf_theory, mgf_mc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88b7cf",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### 5.1 Meaning of the parameters\n",
    "- **Location $\\mu$** shifts the distribution left/right.\n",
    "  - mean = median = mode = $\\mu$\n",
    "\n",
    "- **Scale $s$** stretches/compresses the distribution.\n",
    "  - standard deviation: $\\sigma = \\dfrac{\\pi s}{\\sqrt{3}}$\n",
    "  - interquartile range (IQR):\n",
    "    $$\\mathrm{IQR} = F^{-1}(0.75)-F^{-1}(0.25)=2s\\log 3.$$\n",
    "\n",
    "### 5.2 Shape changes\n",
    "- Increasing $s$ makes the density **wider** and the peak **lower**.\n",
    "- Decreasing $s$ concentrates mass more tightly around $\\mu$.\n",
    "\n",
    "Because this is a location–scale family, changing $(\\mu,s)$ never changes the *fundamental* shape; it only shifts and rescales it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1eab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful scale relationships\n",
    "\n",
    "def logistic_sd(s: float) -> float:\n",
    "    return float(np.pi * s / np.sqrt(3.0))\n",
    "\n",
    "\n",
    "def logistic_iqr(s: float) -> float:\n",
    "    return float(2.0 * s * np.log(3.0))\n",
    "\n",
    "for s in [0.5, 1.0, 2.0]:\n",
    "    print(f\"s={s:>4}: sd={logistic_sd(s):.4f}, IQR={logistic_iqr(s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d708735",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "A very convenient representation comes from inverse-CDF sampling.\n",
    "If $U\\sim\\mathrm{Unif}(0,1)$ then\n",
    "\n",
    "$$X = \\mu + s\\,\\log\\!\\left(\\frac{U}{1-U}\\right).$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\\mathbb{E}[X]=\\mu + s\\,\\mathbb{E}\\left[\\log\\!\\left(\\frac{U}{1-U}\\right)\\right].$$\n",
    "\n",
    "But the integrand is antisymmetric around $1/2$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left[\\log\\!\\left(\\frac{U}{1-U}\\right)\\right]\n",
    "&=\\int_0^1 \\log\\!\\left(\\frac{u}{1-u}\\right)\\,du \\\\\n",
    "&= -\\int_0^1 \\log\\!\\left(\\frac{u}{1-u}\\right)\\,du \\quad (u\\mapsto 1-u),\n",
    "\\end{align}\n",
    "\n",
    "so the integral must be $0$. Therefore $\\mathbb{E}[X]=\\mu$.\n",
    "\n",
    "### 6.2 MGF and variance\n",
    "Let $Z\\sim\\mathrm{Logistic}(0,1)$ with CDF $F(z)=\\sigma(z)$. Use the substitution $u=F(z)$.\n",
    "Because $du=f(z)\\,dz$, we get\n",
    "\n",
    "\\begin{align}\n",
    "M_Z(t)\n",
    "&=\\int_{-\\infty}^{\\infty} e^{tz} f(z)\\,dz \\\\\n",
    "&=\\int_0^1 \\exp\\left(t\\log\\!\\left(\\frac{u}{1-u}\\right)\\right)\\,du \\\\\n",
    "&=\\int_0^1 u^t (1-u)^{-t}\\,du \\\\\n",
    "&= B(1+t,1-t) = \\Gamma(1+t)\\Gamma(1-t).\n",
    "\\end{align}\n",
    "\n",
    "This integral is finite only if $t\\in(-1,1)$.\n",
    "Using the reflection identity $\\Gamma(1+t)\\Gamma(1-t)=\\dfrac{\\pi t}{\\sin(\\pi t)}$, we obtain\n",
    "\n",
    "$$M_Z(t)=\\frac{\\pi t}{\\sin(\\pi t)},\\qquad |t|<1.$$\n",
    "\n",
    "For a general location–scale transform $X=\\mu+sZ$,\n",
    "\n",
    "$$M_X(t)=e^{\\mu t}M_Z(st)=e^{\\mu t}\\,\\frac{\\pi s t}{\\sin(\\pi s t)},\\qquad |t|<\\frac{1}{s}.$$\n",
    "\n",
    "To get the variance, expand around $t=0$. Using\n",
    "\n",
    "$$\\frac{x}{\\sin x} = 1 + \\frac{x^2}{6} + O(x^4),$$\n",
    "\n",
    "we get\n",
    "\n",
    "\\begin{align}\n",
    "M_X(t)\n",
    "&= e^{\\mu t}\\left(1 + \\frac{(\\pi s t)^2}{6} + O(t^4)\\right) \\\\\n",
    "&= 1 + \\mu t + \\left(\\frac{\\mu^2}{2} + \\frac{\\pi^2 s^2}{6}\\right)t^2 + O(t^3).\n",
    "\\end{align}\n",
    "\n",
    "So $\\mathbb{E}[X]=M_X'(0)=\\mu$ and $\\mathbb{E}[X^2]=M_X''(0)=\\mu^2+\\dfrac{\\pi^2 s^2}{3}$.\n",
    "Therefore\n",
    "\n",
    "$$\\mathrm{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2=\\frac{\\pi^2 s^2}{3}.$$\n",
    "\n",
    "### 6.3 Likelihood (iid sample)\n",
    "For data $x_1,\\ldots,x_n$ i.i.d. from $\\mathrm{Logistic}(\\mu,s)$,\n",
    "\n",
    "$$L(\\mu,s)=\\prod_{i=1}^n \\frac{e^{-z_i}}{s(1+e^{-z_i})^2},\\qquad z_i=\\frac{x_i-\\mu}{s}.$$\n",
    "\n",
    "The log-likelihood is\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(\\mu,s)\n",
    "&=\\sum_{i=1}^n \\log f(x_i\\mid\\mu,s)\\\\\n",
    "&= -n\\log s - \\sum_{i=1}^n z_i - 2\\sum_{i=1}^n \\log(1+e^{-z_i}).\n",
    "\\end{align}\n",
    "\n",
    "There is no closed-form MLE in general; it is typically found by **numerical optimization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loglik(x: np.ndarray, mu: float, s: float) -> float:\n",
    "    return float(np.sum(logistic_logpdf(x, mu=mu, s=s)))\n",
    "\n",
    "\n",
    "def fit_logistic_mle(x: np.ndarray, mu_init: float | None = None, s_init: float | None = None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    if mu_init is None:\n",
    "        mu_init = float(np.median(x))\n",
    "    if s_init is None:\n",
    "        s_init = float(np.std(x, ddof=0) * np.sqrt(3.0) / np.pi)\n",
    "        s_init = max(s_init, 1e-3)\n",
    "\n",
    "    def nll(theta: np.ndarray) -> float:\n",
    "        mu, log_s = float(theta[0]), float(theta[1])\n",
    "        s = float(np.exp(log_s))\n",
    "        return -logistic_loglik(x, mu=mu, s=s)\n",
    "\n",
    "    res = optimize.minimize(nll, x0=np.array([mu_init, np.log(s_init)]), method=\"BFGS\")\n",
    "    mu_hat, log_s_hat = res.x\n",
    "    return {\n",
    "        \"mu_hat\": float(mu_hat),\n",
    "        \"s_hat\": float(np.exp(log_s_hat)),\n",
    "        \"success\": bool(res.success),\n",
    "        \"message\": res.message,\n",
    "        \"fun\": float(res.fun),\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare our simple MLE to SciPy's fit on simulated data\n",
    "x_data = logistic_rvs(rng, size=5_000, mu=1.2, s=0.8)\n",
    "\n",
    "ours = fit_logistic_mle(x_data)\n",
    "scipy_loc, scipy_scale = stats.logistic.fit(x_data)\n",
    "\n",
    "ours, (scipy_loc, scipy_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fd446",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### 7.1 Inverse transform sampling\n",
    "Because the logistic CDF is invertible in closed form, we can sample using the inverse CDF.\n",
    "\n",
    "If $U\\sim\\mathrm{Unif}(0,1)$ and $X=F^{-1}(U)$, then $X$ has CDF $F$.\n",
    "For logistic:\n",
    "\n",
    "$$X = \\mu + s\\,\\log\\!\\left(\\frac{U}{1-U}\\right).$$\n",
    "\n",
    "### 7.2 Practical notes\n",
    "- When implementing $\\log\\!\\left(\\frac{U}{1-U}\\right)$ numerically, use\n",
    "  $$\\log U - \\log(1-U)$$\n",
    "  with `log1p` for stability.\n",
    "- Clip $U$ away from exactly 0 and 1 to avoid returning $\\pm\\infty$.\n",
    "\n",
    "**Algorithm (vectorized)**\n",
    "\n",
    "1. Draw $u \\leftarrow \\mathrm{Uniform}(0,1)$\n",
    "2. Set $u \\leftarrow \\mathrm{clip}(u,\\varepsilon, 1-\\varepsilon)$\n",
    "3. Return $x \\leftarrow \\mu + s(\\log u - \\log(1-u))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling sanity checks\n",
    "mu0, s0 = -0.5, 1.7\n",
    "\n",
    "x = logistic_rvs(rng, size=200_000, mu=mu0, s=s0)\n",
    "\n",
    "# 1) Mean/variance\n",
    "print('mean (mc)', x.mean(), 'theory', logistic_moments(mu0, s0)['mean'])\n",
    "print('var  (mc)', x.var(ddof=0), 'theory', logistic_moments(mu0, s0)['variance'])\n",
    "\n",
    "# 2) Probability integral transform: F(X) should look Uniform(0,1)\n",
    "u = logistic_cdf(x, mu=mu0, s=s0)\n",
    "print('u mean', u.mean(), 'u var', u.var(ddof=0))\n",
    "\n",
    "# Compare a few quantiles to Uniform(0,1)\n",
    "qs = np.array([0.01, 0.1, 0.5, 0.9, 0.99])\n",
    "print('empirical u-quantiles:', np.quantile(u, qs))\n",
    "print('target quantiles     :', qs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a73884",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the theoretical **PDF** and **CDF** for several parameter choices\n",
    "- **Monte Carlo** samples from the NumPy-only sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF/CDF for several parameter choices\n",
    "\n",
    "params = [\n",
    "    (0.0, 0.6),\n",
    "    (0.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (2.0, 1.0),\n",
    "]\n",
    "\n",
    "# choose an x-range that covers all cases (0.001 to 0.999 quantiles)\n",
    "lo = min(logistic_ppf(1e-3, mu=mu, s=s) for mu, s in params)\n",
    "hi = max(logistic_ppf(1 - 1e-3, mu=mu, s=s) for mu, s in params)\n",
    "xx = np.linspace(lo, hi, 800)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"PDF\", \"CDF\"))\n",
    "\n",
    "for mu, s in params:\n",
    "    label = f\"μ={mu}, s={s}\"\n",
    "    fig.add_trace(go.Scatter(x=xx, y=logistic_pdf(xx, mu=mu, s=s), mode=\"lines\", name=label), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=xx, y=logistic_cdf(xx, mu=mu, s=s), mode=\"lines\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"density\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"probability\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=\"Logistic distribution: PDF and CDF\", width=950, height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eaf8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo histogram + PDF overlay\n",
    "\n",
    "mu0, s0 = 0.0, 1.0\n",
    "samples_mc = logistic_rvs(rng, size=80_000, mu=mu0, s=s0)\n",
    "\n",
    "x_grid = np.linspace(logistic_ppf(1e-4, mu0, s0), logistic_ppf(1 - 1e-4, mu0, s0), 900)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples_mc,\n",
    "        nbinsx=70,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"Monte Carlo (NumPy-only)\",\n",
    "        opacity=0.55,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_grid,\n",
    "        y=logistic_pdf(x_grid, mu=mu0, s=s0),\n",
    "        mode=\"lines\",\n",
    "        name=\"True PDF\",\n",
    "        line=dict(width=3),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(title=f\"Logistic(μ={mu0}, s={s0}): histogram vs PDF\", width=900, height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF: theoretical vs empirical\n",
    "\n",
    "x_grid = np.linspace(logistic_ppf(1e-4, mu0, s0), logistic_ppf(1 - 1e-4, mu0, s0), 700)\n",
    "\n",
    "emp_x = np.sort(samples_mc)\n",
    "emp_cdf = np.arange(1, emp_x.size + 1) / emp_x.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=logistic_cdf(x_grid, mu=mu0, s=s0), mode=\"lines\", name=\"True CDF\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=emp_x[::200],\n",
    "        y=emp_cdf[::200],\n",
    "        mode=\"markers\",\n",
    "        name=\"Empirical CDF (subsampled)\",\n",
    "        marker=dict(size=4, opacity=0.55),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(title=f\"Logistic(μ={mu0}, s={s0}): CDF vs empirical\", width=900, height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e796a",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.logistic`)\n",
    "\n",
    "SciPy parameterization:\n",
    "\n",
    "```python\n",
    "stats.logistic(loc=mu, scale=s)\n",
    "```\n",
    "\n",
    "- `loc` is the location parameter $\\mu$.\n",
    "- `scale` is the scale parameter $s>0$.\n",
    "\n",
    "SciPy provides:\n",
    "- `pdf`, `logpdf`, `cdf`, `ppf`\n",
    "- `rvs` for sampling\n",
    "- `fit` for MLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.logistic(loc=mu0, scale=s0)\n",
    "\n",
    "x_test = np.linspace(-2, 2, 5)\n",
    "\n",
    "pdf = dist.pdf(x_test)\n",
    "cdf = dist.cdf(x_test)\n",
    "samples_scipy = dist.rvs(size=5, random_state=rng)\n",
    "\n",
    "pdf, cdf, samples_scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438de5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE fit example\n",
    "true_mu, true_s = 1.5, 0.9\n",
    "x_fit = stats.logistic(loc=true_mu, scale=true_s).rvs(size=10_000, random_state=rng)\n",
    "\n",
    "mu_hat, s_hat = stats.logistic.fit(x_fit)  # returns (loc, scale)\n",
    "\n",
    "true_mu, true_s, mu_hat, s_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8eb6d",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing (location)\n",
    "If you assume data are logistic with unknown $(\\mu,s)$, a common hypothesis is\n",
    "\n",
    "$$H_0: \\mu = \\mu_0 \\quad \\text{vs} \\quad H_1: \\mu \\ne \\mu_0.$$\n",
    "\n",
    "You can use a **likelihood-ratio test (LRT)**:\n",
    "\n",
    "$$\\Lambda = 2\\bigl(\\ell(\\hat\\mu,\\hat s) - \\ell(\\mu_0, \\tilde s)\\bigr) \\overset{approx}{\\sim} \\chi^2_1,$$\n",
    "\n",
    "where $(\\hat\\mu,\\hat s)$ are the unrestricted MLEs and $\\tilde s$ is the MLE under $H_0$.\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "- **Error model**: logistic noise is a heavy-tailed alternative to Gaussian noise.\n",
    "- **Latent-variable logistic regression**: if $Y=\\mathbf{1}\\{\\eta+\\varepsilon>0\\}$ with $\\varepsilon\\sim\\mathrm{Logistic}(0,1)$, then\n",
    "  $$\\Pr(Y=1\\mid\\eta)=\\sigma(\\eta).$$\n",
    "  This gives the familiar logistic likelihood used in Bayesian logistic regression.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "- **Inverse-CDF sampling** makes logistic a convenient base distribution.\n",
    "- **Mixtures of logistics** can model multimodal or skewed densities and appear in modern neural generative models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Likelihood-ratio test example: H0: mu = 0\n",
    "\n",
    "rng_test = np.random.default_rng(123)\n",
    "\n",
    "n = 400\n",
    "mu_true, s_true = 0.35, 1.0\n",
    "x = logistic_rvs(rng_test, size=n, mu=mu_true, s=s_true)\n",
    "\n",
    "\n",
    "def mle_unrestricted(x: np.ndarray):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    def nll(theta: np.ndarray) -> float:\n",
    "        mu, log_s = float(theta[0]), float(theta[1])\n",
    "        s = float(np.exp(log_s))\n",
    "        return -logistic_loglik(x, mu=mu, s=s)\n",
    "\n",
    "    mu_init = float(np.median(x))\n",
    "    s_init = float(np.std(x, ddof=0) * np.sqrt(3.0) / np.pi)\n",
    "\n",
    "    res = optimize.minimize(nll, x0=np.array([mu_init, np.log(max(s_init, 1e-3))]), method=\"BFGS\")\n",
    "    mu_hat, log_s_hat = res.x\n",
    "    return float(mu_hat), float(np.exp(log_s_hat)), float(-res.fun)\n",
    "\n",
    "\n",
    "def mle_mu_fixed(x: np.ndarray, mu0: float):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    def nll(log_s: np.ndarray) -> float:\n",
    "        s = float(np.exp(float(log_s)))\n",
    "        return -logistic_loglik(x, mu=mu0, s=s)\n",
    "\n",
    "    s_init = float(np.std(x, ddof=0) * np.sqrt(3.0) / np.pi)\n",
    "    res = optimize.minimize(nll, x0=np.array([np.log(max(s_init, 1e-3))]), method=\"BFGS\")\n",
    "    s_hat = float(np.exp(float(res.x)))\n",
    "    return s_hat, float(-res.fun)\n",
    "\n",
    "\n",
    "mu0 = 0.0\n",
    "mu_hat, s_hat, ll1 = mle_unrestricted(x)\n",
    "s_tilde, ll0 = mle_mu_fixed(x, mu0=mu0)\n",
    "\n",
    "lrt = 2.0 * (ll1 - ll0)\n",
    "p_value = 1.0 - chi2.cdf(lrt, df=1)\n",
    "\n",
    "{\n",
    "    \"n\": n,\n",
    "    \"true\": (mu_true, s_true),\n",
    "    \"mle_unrestricted\": (mu_hat, s_hat),\n",
    "    \"mle_H0\": (mu0, s_tilde),\n",
    "    \"LRT\": lrt,\n",
    "    \"p_value\": p_value,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d89886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian example: posterior over mu with known scale (grid approximation)\n",
    "\n",
    "x = logistic_rvs(rng, size=200, mu=0.6, s=1.0)\n",
    "s_known = 1.0\n",
    "\n",
    "# Prior: mu ~ Normal(0, 2^2)\n",
    "mu_grid = np.linspace(-2.5, 2.5, 1201)\n",
    "log_prior = norm(loc=0.0, scale=2.0).logpdf(mu_grid)\n",
    "\n",
    "# Log-likelihood for each mu on the grid\n",
    "log_like = np.array([logistic_loglik(x, mu=mu, s=s_known) for mu in mu_grid])\n",
    "log_post_unnorm = log_prior + log_like\n",
    "log_post = log_post_unnorm - np.max(log_post_unnorm)\n",
    "post = np.exp(log_post)\n",
    "post /= post.sum()\n",
    "\n",
    "post_mean = float(np.sum(mu_grid * post))\n",
    "post_cdf = np.cumsum(post)\n",
    "ci_low = float(mu_grid[np.searchsorted(post_cdf, 0.025)])\n",
    "ci_high = float(mu_grid[np.searchsorted(post_cdf, 0.975)])\n",
    "\n",
    "(post_mean, (ci_low, ci_high))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7dd230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the posterior\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_grid, y=post, mode=\"lines\", name=\"posterior\"))\n",
    "fig.add_vline(x=post_mean, line_dash=\"dash\", line_color=\"black\", annotation_text=\"posterior mean\")\n",
    "fig.add_vrect(x0=ci_low, x1=ci_high, fillcolor=\"gray\", opacity=0.2, line_width=0)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Posterior over μ (known s): grid approximation\",\n",
    "    xaxis_title=\"μ\",\n",
    "    yaxis_title=\"posterior density (discrete grid)\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Generative modeling: a simple mixture of logistics\n",
    "\n",
    "weights = np.array([0.55, 0.45])\n",
    "components = [(-1.2, 0.6), (1.4, 0.9)]  # (mu, s)\n",
    "\n",
    "\n",
    "def mixture_logistic_pdf(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.zeros_like(x)\n",
    "    for w, (mu, s) in zip(weights, components):\n",
    "        out += w * logistic_pdf(x, mu=mu, s=s)\n",
    "    return out\n",
    "\n",
    "\n",
    "def mixture_logistic_rvs(rng: np.random.Generator, size: int) -> np.ndarray:\n",
    "    k = rng.choice(len(weights), size=size, p=weights)\n",
    "    out = np.empty(size, dtype=float)\n",
    "    for idx in range(len(weights)):\n",
    "        mask = k == idx\n",
    "        mu, s = components[idx]\n",
    "        out[mask] = logistic_rvs(rng, size=int(mask.sum()), mu=mu, s=s)\n",
    "    return out\n",
    "\n",
    "\n",
    "mix_samples = mixture_logistic_rvs(rng, size=60_000)\n",
    "\n",
    "x_grid = np.linspace(np.quantile(mix_samples, 0.001), np.quantile(mix_samples, 0.999), 900)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=mix_samples,\n",
    "        nbinsx=90,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"samples\",\n",
    "        opacity=0.55,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=mixture_logistic_pdf(x_grid), mode=\"lines\", name=\"mixture PDF\", line=dict(width=3)))\n",
    "\n",
    "fig.update_layout(title=\"Mixture of logistics: histogram vs PDF\", width=900, height=420)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5af82",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid scale**: $s\\le 0$ is not a valid logistic distribution.\n",
    "- **Overflow in naive formulas**:\n",
    "  - `np.exp(-z)` overflows if $z$ is very negative.\n",
    "  - use stable forms (piecewise sigmoid, `logaddexp`, `log1p`).\n",
    "- **Sampling at the boundaries**:\n",
    "  - the inverse CDF uses $\\log\\!\\left(\\frac{p}{1-p}\\right)$; if $p$ is exactly 0 or 1, you get $\\pm\\infty$.\n",
    "  - clip $p$ (or the underlying uniform draws) away from {0,1}.\n",
    "- **MGF domain**:\n",
    "  - $M_X(t)$ exists only for $|t|<1/s$.\n",
    "- **Parameterization confusion**:\n",
    "  - some sources parameterize logistic by a “steepness” $k=1/s$.\n",
    "  - SciPy uses `(loc, scale)`.\n",
    "- **Fitting**:\n",
    "  - for small samples, MLE can be noisy; prefer robust starting points (median + variance-based scale).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd3384",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `logistic` is a **continuous** distribution on $\\mathbb{R}$ with CDF equal to the **sigmoid**.\n",
    "- Parameters: location $\\mu\\in\\mathbb{R}$ and scale $s>0$ (a pure shift/scale family).\n",
    "- Key formulas:\n",
    "  - $\\mathbb{E}[X]=\\mu$,\n",
    "  - $\\mathrm{Var}(X)=\\pi^2 s^2/3$,\n",
    "  - $h(X)=\\ln(s)+2$,\n",
    "  - $M_X(t)=e^{\\mu t}\\,\\pi s t/\\sin(\\pi s t)$ for $|t|<1/s$.\n",
    "- Sampling is easy via inverse CDF: $\\mu+s\\log\\!\\left(\\frac{U}{1-U}\\right)$.\n",
    "\n",
    "**References**\n",
    "- SciPy documentation: `scipy.stats.logistic`.\n",
    "- Reflection identity: $\\Gamma(z)\\Gamma(1-z)=\\pi/\\sin(\\pi z)$.\n",
    "- Mixture of logistics in neural generative modeling: *PixelCNN++* (Salimans et al., 2017) uses discretized logistic mixtures.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}