{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf334efa",
   "metadata": {},
   "source": [
    "# Skewed Cauchy distribution (`skewcauchy`)\n",
    "\n",
    "The `skewcauchy` distribution is a **heavy-tailed** continuous distribution that generalizes the Cauchy by introducing a **skewness** parameter `a ∈ (-1, 1)`.\n",
    "\n",
    "- When `a = 0`, it reduces to the standard (symmetric) Cauchy.\n",
    "- For `a > 0`, the distribution has **more probability mass on the right** and a **heavier right tail** (large positive outliers are more common).\n",
    "- For `a < 0`, the left tail is heavier.\n",
    "\n",
    "A key feature (shared with the Cauchy): **the mean and variance do not exist**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824caa1",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "- Understand the **piecewise** nature of the PDF/CDF and how `a` controls **asymmetry**.\n",
    "- Know which quantities are **well-defined** (quantiles, entropy) and which are **not** (mean/variance).\n",
    "- Implement **NumPy-only sampling** via the inverse CDF (and an equivalent mixture view).\n",
    "- Visualize PDF/CDF and verify Monte Carlo simulations.\n",
    "- Use `scipy.stats.skewcauchy` for evaluation, simulation, and MLE fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import scipy\n",
    "from scipy import special\n",
    "from scipy.stats import binomtest, skewcauchy\n",
    "\n",
    "import plotly\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "print(\"Python\", platform.python_version())\n",
    "print(\"NumPy\", np.__version__)\n",
    "print(\"SciPy\", scipy.__version__)\n",
    "print(\"Plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb360f",
   "metadata": {},
   "source": [
    "## 1) Title & classification\n",
    "\n",
    "- **Name**: `skewcauchy`\n",
    "- **Type**: **continuous** distribution\n",
    "- **Support**: $x \\in (-\\infty, \\infty)$\n",
    "- **Parameter space**:\n",
    "  - **shape / skewness**: $a \\in (-1,1)$\n",
    "  - **location**: $\\text{loc} \\in \\mathbb{R}$\n",
    "  - **scale**: $\\text{scale} > 0$\n",
    "\n",
    "We write the standardized form as:\n",
    "\n",
    "$$Y \\sim \\mathrm{SkewCauchy}(a)$$\n",
    "\n",
    "and the location/scale family as:\n",
    "\n",
    "$$X = \\text{loc} + \\text{scale}\\,Y.$$\n",
    "\n",
    "SciPy uses this parameterization: `skewcauchy(a, loc=..., scale=...)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b43da",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "`skewcauchy` models **real-valued data with extremely heavy tails** (Cauchy-like) **and asymmetry**:\n",
    "\n",
    "- you expect **outliers** to be common;\n",
    "- you expect those outliers to be **more common on one side**.\n",
    "\n",
    "This is useful when Gaussian or even Student-$t$ noise is too light-tailed, and when symmetry is not appropriate.\n",
    "\n",
    "### Typical use cases\n",
    "- **Robust residuals with skew**: regression or measurement error where large deviations happen, but predominantly positive or predominantly negative.\n",
    "- **Asymmetric shock models**: systems where positive jumps are more likely than negative jumps (or vice versa).\n",
    "- **A building block** in mixture models when you want components that can be both **heavy-tailed** and **skewed**.\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Cauchy**: `skewcauchy(a=0)` is the usual Cauchy.\n",
    "- **Half-Cauchy**: conditional on the sign, the magnitude is half-Cauchy with a scale that depends on `a`.\n",
    "- **Skewed generalized t**: `skewcauchy` is a special case (degrees of freedom $\\nu=1$) of a broader skewed-$t$ family.\n",
    "\n",
    "A convenient generative picture (standardized case):\n",
    "\n",
    "1. Draw a sign $S\\in\\{-1,+1\\}$ with $\\mathbb{P}(S=+1)=(1+a)/2$.\n",
    "2. Draw a magnitude $R\\ge 0$ from a half-Cauchy whose scale is $(1+a)$ if $S=+1$ and $(1-a)$ if $S=-1$.\n",
    "3. Return $Y = S\\,R$.\n",
    "\n",
    "This makes the meaning of `a` very concrete: it controls both **which side** is more likely and **how heavy** the tail is on that side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d23dc",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "Throughout this section, $-1<a<1$.\n",
    "\n",
    "### PDF\n",
    "The standardized PDF used by SciPy is:\n",
    "\n",
    "$$\n",
    " f(y; a) = \\frac{1}{\\pi\\left(\\frac{y^2}{\\left(1+a\\,\\mathrm{sign}(y)\\right)^2} + 1\\right)}.\n",
    "$$\n",
    "\n",
    "Equivalently, it is piecewise:\n",
    "\n",
    "$$\n",
    " f(y;a) =\n",
    " \\begin{cases}\n",
    " \\dfrac{1}{\\pi\\left(1 + \\left(\\dfrac{y}{1-a}\\right)^2\\right)}, & y<0,\\\\\n",
    " \\dfrac{1}{\\pi\\left(1 + \\left(\\dfrac{y}{1+a}\\right)^2\\right)}, & y\\ge 0.\n",
    " \\end{cases}\n",
    "$$\n",
    "\n",
    "The location/scale form is:\n",
    "\n",
    "$$\n",
    " f_X(x; a,\\text{loc},\\text{scale}) = \\frac{1}{\\text{scale}}\\, f\\!\\left(\\frac{x-\\text{loc}}{\\text{scale}}; a\\right).\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "The standardized CDF has a closed form:\n",
    "\n",
    "$$\n",
    " F(y;a) =\n",
    " \\begin{cases}\n",
    " \\dfrac{1-a}{2} + \\dfrac{1-a}{\\pi}\\arctan\\left(\\dfrac{y}{1-a}\\right), & y\\le 0,\\\\\n",
    " \\dfrac{1-a}{2} + \\dfrac{1+a}{\\pi}\\arctan\\left(\\dfrac{y}{1+a}\\right), & y>0.\n",
    " \\end{cases}\n",
    "$$\n",
    "\n",
    "And for $X = \\text{loc}+\\text{scale}\\,Y$:\n",
    "\n",
    "$$F_X(x)=F\\!\\left(\\frac{x-\\text{loc}}{\\text{scale}};a\\right).$$\n",
    "\n",
    "We'll implement `pdf`, `cdf`, `ppf` (inverse CDF), and sampling from scratch using only NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45674e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_skewcauchy_params(a: float, scale: float) -> None:\n",
    "    if not (-1.0 < float(a) < 1.0):\n",
    "        raise ValueError(\"shape parameter a must satisfy -1 < a < 1\")\n",
    "    if float(scale) <= 0.0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "\n",
    "def skewcauchy_pdf(x: np.ndarray, a: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    _check_skewcauchy_params(a, scale)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = (x - loc) / scale\n",
    "\n",
    "    s = np.sign(y)\n",
    "    denom = np.pi * (y * y / (1.0 + a * s) ** 2 + 1.0)\n",
    "    return 1.0 / (scale * denom)\n",
    "\n",
    "\n",
    "def skewcauchy_logpdf(x: np.ndarray, a: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    _check_skewcauchy_params(a, scale)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = (x - loc) / scale\n",
    "\n",
    "    s = np.sign(y)\n",
    "    return -np.log(scale) - np.log(np.pi) - np.log(y * y / (1.0 + a * s) ** 2 + 1.0)\n",
    "\n",
    "\n",
    "def skewcauchy_cdf(x: np.ndarray, a: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    _check_skewcauchy_params(a, scale)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = (x - loc) / scale\n",
    "\n",
    "    left = (1.0 - a) / 2.0 + (1.0 - a) / np.pi * np.arctan(y / (1.0 - a))\n",
    "    right = (1.0 - a) / 2.0 + (1.0 + a) / np.pi * np.arctan(y / (1.0 + a))\n",
    "    return np.where(y <= 0.0, left, right)\n",
    "\n",
    "\n",
    "def skewcauchy_ppf(q: np.ndarray, a: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    _check_skewcauchy_params(a, scale)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "\n",
    "    q0 = (1.0 - a) / 2.0  # F(0)\n",
    "    left = (1.0 - a) * np.tan(np.pi / (1.0 - a) * (q - q0))\n",
    "    right = (1.0 + a) * np.tan(np.pi / (1.0 + a) * (q - q0))\n",
    "\n",
    "    y = np.where(q < q0, left, right)\n",
    "    return loc + scale * y\n",
    "\n",
    "\n",
    "def skewcauchy_rvs(\n",
    "    a: float,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    "    size=None,\n",
    "    *,\n",
    "    rng=None,\n",
    "    eps: float = 1e-12,\n",
    ") -> np.ndarray:\n",
    "    _check_skewcauchy_params(a, scale)\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    u = rng.random(size)\n",
    "    u = np.clip(u, eps, 1.0 - eps)\n",
    "    return skewcauchy_ppf(u, a, loc=loc, scale=scale)\n",
    "\n",
    "\n",
    "def skewcauchy_rvs_mixture(\n",
    "    a: float,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    "    size=None,\n",
    "    *,\n",
    "    rng=None,\n",
    "    eps: float = 1e-12,\n",
    ") -> np.ndarray:\n",
    "    _check_skewcauchy_params(a, scale)\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    p_right = (1.0 + a) / 2.0\n",
    "    u_sign = rng.random(size)\n",
    "    s = np.where(u_sign < p_right, 1.0, -1.0)\n",
    "\n",
    "    u = rng.random(size)\n",
    "    u = np.clip(u, eps, 1.0 - eps)\n",
    "\n",
    "    # Half-Cauchy inverse CDF: R = b * tan(pi * U / 2)\n",
    "    b = np.where(s > 0, 1.0 + a, 1.0 - a)\n",
    "    r = b * np.tan(0.5 * np.pi * u)\n",
    "\n",
    "    y = s * r\n",
    "    return loc + scale * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick numerical checks against SciPy\n",
    "\n",
    "a = 0.4\n",
    "loc, scale = -1.25, 2.0\n",
    "\n",
    "x = rng.normal(size=15) * 3\n",
    "\n",
    "pdf_err = np.max(np.abs(skewcauchy_pdf(x, a, loc, scale) - skewcauchy.pdf(x, a, loc=loc, scale=scale)))\n",
    "cdf_err = np.max(np.abs(skewcauchy_cdf(x, a, loc, scale) - skewcauchy.cdf(x, a, loc=loc, scale=scale)))\n",
    "\n",
    "qs = np.array([1e-6, 0.1, 0.5, 0.9, 1 - 1e-6])\n",
    "ppf_err = np.max(np.abs(skewcauchy_ppf(qs, a, loc, scale) - skewcauchy.ppf(qs, a, loc=loc, scale=scale)))\n",
    "\n",
    "print(\"max |pdf diff|:\", pdf_err)\n",
    "print(\"max |cdf diff|:\", cdf_err)\n",
    "print(\"max |ppf diff|:\", ppf_err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767e169",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "Like the Cauchy, `skewcauchy` is heavy-tailed enough that the usual moments do not exist:\n",
    "\n",
    "- **Mean**: does not exist (not finite)\n",
    "- **Variance**: does not exist (infinite)\n",
    "- **Skewness**: undefined\n",
    "- **Kurtosis**: undefined\n",
    "\n",
    "SciPy reflects this by returning `nan` for `stats(..., moments='mvsk')`.\n",
    "\n",
    "### MGF and characteristic function\n",
    "- The **MGF** $M(t)=\\mathbb{E}[e^{tX}]$ does **not** exist for any $t\\ne 0$.\n",
    "- The **characteristic function** $\\varphi(t)=\\mathbb{E}[e^{itX}]$ does exist for all real $t$.\n",
    "\n",
    "For the **standardized** $Y\\sim\\mathrm{SkewCauchy}(a)$, the **real part** has a clean form:\n",
    "\n",
    "$$\\Re\\,\\varphi_Y(t) = \\frac{1-a}{2}e^{-(1-a)|t|} + \\frac{1+a}{2}e^{-(1+a)|t|}.$$\n",
    "\n",
    "The **imaginary part** is nonzero when $a\\ne 0$ and can be written using special functions (hyperbolic sine/cosine integrals, `Shi`/`Chi`). We'll compute it in code.\n",
    "\n",
    "### Entropy\n",
    "The **differential entropy** of the standardized distribution is\n",
    "\n",
    "$$h(Y)=\\log(4\\pi),$$\n",
    "\n",
    "and for $X=\\text{loc}+\\text{scale}\\,Y$:\n",
    "\n",
    "$$h(X)=\\log(4\\pi\\,\\text{scale}).$$\n",
    "\n",
    "Notably, it is **independent of** the skewness parameter `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewcauchy_cf_standard(t: np.ndarray, a: float) -> np.ndarray:\n",
    "    if not (-1.0 < float(a) < 1.0):\n",
    "        raise ValueError(\"-1 < a < 1 required\")\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    u = np.abs(t)\n",
    "    sgn = np.sign(t)\n",
    "\n",
    "    b_minus = 1.0 - a\n",
    "    b_plus = 1.0 + a\n",
    "\n",
    "    re = 0.5 * b_minus * np.exp(-b_minus * u) + 0.5 * b_plus * np.exp(-b_plus * u)\n",
    "\n",
    "    im = np.zeros_like(re)\n",
    "    mask = u > 0\n",
    "\n",
    "    if np.any(mask):\n",
    "        um = u[mask]\n",
    "\n",
    "        # I_s(u, b) = ∫_0^∞ sin(u x) / (x^2 + b^2) dx\n",
    "        #           = (cosh(bu) Shi(bu) - sinh(bu) Chi(bu)) / b, for u>0, b>0.\n",
    "        Shi_p, Chi_p = special.shichi(b_plus * um)\n",
    "        Shi_m, Chi_m = special.shichi(b_minus * um)\n",
    "\n",
    "        Isp = (np.cosh(b_plus * um) * Shi_p - np.sinh(b_plus * um) * Chi_p) / b_plus\n",
    "        Ism = (np.cosh(b_minus * um) * Shi_m - np.sinh(b_minus * um) * Chi_m) / b_minus\n",
    "\n",
    "        im[mask] = sgn[mask] * (b_plus**2 / np.pi * Isp - b_minus**2 / np.pi * Ism)\n",
    "\n",
    "    return re + 1j * im\n",
    "\n",
    "\n",
    "ts = np.linspace(-10, 10, 2001)\n",
    "a_vals = [-0.7, 0.0, 0.7]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=(\"Re φ(t)\", \"Im φ(t)\"))\n",
    "\n",
    "for a in a_vals:\n",
    "    phi = skewcauchy_cf_standard(ts, a)\n",
    "    fig.add_trace(go.Scatter(x=ts, y=np.real(phi), mode=\"lines\", name=f\"a={a:+.1f}\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=ts, y=np.imag(phi), mode=\"lines\", name=f\"a={a:+.1f}\"), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=600, title=\"Characteristic function of standardized skewcauchy\")\n",
    "fig.update_xaxes(title_text=\"t\", row=2, col=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f721f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy is constant in a (standardized case)\n",
    "\n",
    "for a in [-0.9, -0.5, 0.0, 0.5, 0.9]:\n",
    "    print(f\"a={a:+.1f} -> entropy {skewcauchy.entropy(a):.12f}\")\n",
    "\n",
    "print(\"log(4π) =\", float(np.log(4 * np.pi)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed46bdb",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "### Shape / skewness parameter `a`\n",
    "A simple, very interpretable identity for the standardized distribution is:\n",
    "\n",
    "$$\\mathbb{P}(Y>0)=\\frac{1+a}{2},\\qquad \\mathbb{P}(Y<0)=\\frac{1-a}{2}.$$\n",
    "\n",
    "So `a` is literally the **difference in sign probabilities**:\n",
    "\n",
    "$$a = \\mathbb{P}(Y>0) - \\mathbb{P}(Y<0).$$\n",
    "\n",
    "It also controls the **tail constants**:\n",
    "\n",
    "- as $y\\to +\\infty$, $f(y)\\sim \\dfrac{(1+a)^2}{\\pi y^2}$\n",
    "- as $y\\to -\\infty$, $f(y)\\sim \\dfrac{(1-a)^2}{\\pi y^2}$\n",
    "\n",
    "So `a>0` means a heavier **right** tail, and `a<0` a heavier **left** tail.\n",
    "\n",
    "### Location `loc` and scale `scale`\n",
    "- `loc` shifts the distribution: $X=\\text{loc}+\\text{scale}\\,Y$.\n",
    "- `scale` stretches it, and increases entropy by $\\log(\\text{scale})$.\n",
    "\n",
    "Important nuance: for `skewcauchy`, `loc` is **not** the median unless `a=0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c809ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the median changes with a\n",
    "\n",
    "a_grid = np.linspace(-0.95, 0.95, 401)\n",
    "med = skewcauchy_ppf(0.5, a_grid)  # standardized median\n",
    "p_right = (1 + a_grid) / 2\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Median of Y\", \"P(Y>0)\"))\n",
    "fig.add_trace(go.Scatter(x=a_grid, y=med, mode=\"lines\", name=\"median\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=a_grid, y=p_right, mode=\"lines\", name=\"P(Y>0)\"), row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"a\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"a\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"median\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"probability\", row=1, col=2)\n",
    "fig.update_layout(height=350, title=\"Parameter interpretation\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a54a5",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "We'll use the standardized PDF and set:\n",
    "\n",
    "$$b_+ = 1+a,\\qquad b_- = 1-a.$$\n",
    "\n",
    "For $y\\ge 0$:\n",
    "\n",
    "$$f(y)=\\frac{b_+^2}{\\pi(y^2+b_+^2)},$$\n",
    "\n",
    "and for $y<0$:\n",
    "\n",
    "$$f(y)=\\frac{b_-^2}{\\pi(y^2+b_-^2)}.$$\n",
    "\n",
    "### Expectation (why it does not exist)\n",
    "The mean exists (as a Lebesgue integral) iff $\\mathbb{E}[|Y|]<\\infty$.\n",
    "\n",
    "Consider the truncated absolute first moment:\n",
    "\n",
    "$$\\mathbb{E}[|Y|\\,\\mathbf{1}\\{|Y|\\le A\\}] = \\int_{-A}^{A} |y| f(y)\\,dy.$$\n",
    "\n",
    "Using the piecewise form and symmetry of $|y|$:\n",
    "\n",
    "$$\n",
    "\\int_0^A \\frac{y\\,b^2}{\\pi(y^2+b^2)}\\,dy\n",
    "= \\frac{b^2}{2\\pi}\\log\\left(\\frac{A^2+b^2}{b^2}\\right).\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[|Y|\\,\\mathbf{1}\\{|Y|\\le A\\}] = \\frac{b_+^2}{2\\pi}\\log\\left(\\frac{A^2+b_+^2}{b_+^2}\\right)\n",
    "+ \\frac{b_-^2}{2\\pi}\\log\\left(\\frac{A^2+b_-^2}{b_-^2}\\right)\n",
    "\\xrightarrow[A\\to\\infty]{} \\infty.\n",
    "$$\n",
    "\n",
    "Therefore $\\mathbb{E}[|Y|]=\\infty$ and the mean is **undefined**.\n",
    "\n",
    "A subtle but important extra point: for $a\\ne 0$, even the *principal value* mean diverges because the positive and negative tails have different constants.\n",
    "\n",
    "### Variance (why it does not exist)\n",
    "The truncated second moment is\n",
    "\n",
    "$$\\mathbb{E}[Y^2\\,\\mathbf{1}\\{|Y|\\le A\\}] = \\int_{-A}^{A} y^2 f(y)\\,dy.$$\n",
    "\n",
    "For $y\\ge 0$:\n",
    "\n",
    "$$\n",
    "\\int_0^A \\frac{y^2 b^2}{\\pi(y^2+b^2)}\\,dy\n",
    "= \\frac{b^2}{\\pi}\\left(A - b\\arctan\\left(\\frac{A}{b}\\right)\\right),\n",
    "$$\n",
    "\n",
    "which grows linearly in $A$. Hence $\\mathbb{E}[Y^2]=\\infty$ and the variance does not exist.\n",
    "\n",
    "### Likelihood\n",
    "Given observations $x_1,\\dots,x_n$, the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(a,\\text{loc},\\text{scale})\n",
    "= \\sum_{i=1}^n \\Bigl[-\\log(\\text{scale}) - \\log\\pi - \\log\\bigl(1 + y_i^2/(1+a\\,\\mathrm{sign}(y_i))^2\\bigr)\\Bigr]\n",
    "$$\n",
    "\n",
    "where $y_i=(x_i-\\text{loc})/\\text{scale}$.\n",
    "\n",
    "We can maximize this numerically (SciPy does this in `skewcauchy.fit`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2336c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_abs_moment(A: np.ndarray, a: float) -> np.ndarray:\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    b_plus, b_minus = 1 + a, 1 - a\n",
    "    term_p = (b_plus**2) / (2 * np.pi) * np.log((A * A + b_plus**2) / (b_plus**2))\n",
    "    term_m = (b_minus**2) / (2 * np.pi) * np.log((A * A + b_minus**2) / (b_minus**2))\n",
    "    return term_p + term_m\n",
    "\n",
    "\n",
    "def trunc_pv_mean(A: np.ndarray, a: float) -> np.ndarray:\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    b_plus, b_minus = 1 + a, 1 - a\n",
    "    term_p = (b_plus**2) / (2 * np.pi) * np.log((A * A + b_plus**2) / (b_plus**2))\n",
    "    term_m = (b_minus**2) / (2 * np.pi) * np.log((A * A + b_minus**2) / (b_minus**2))\n",
    "    return term_p - term_m\n",
    "\n",
    "\n",
    "def trunc_second_moment(A: np.ndarray, a: float) -> np.ndarray:\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    b_plus, b_minus = 1 + a, 1 - a\n",
    "    term_p = (b_plus**2) / np.pi * (A - b_plus * np.arctan(A / b_plus))\n",
    "    term_m = (b_minus**2) / np.pi * (A - b_minus * np.arctan(A / b_minus))\n",
    "    return term_p + term_m\n",
    "\n",
    "\n",
    "A = np.logspace(-1, 3, 300)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\"E[|Y| 1{|Y|≤A}]\", \"E[Y 1{|Y|≤A}]\", \"E[Y^2 1{|Y|≤A}]\"),\n",
    ")\n",
    "\n",
    "for a in [-0.7, 0.0, 0.7]:\n",
    "    fig.add_trace(go.Scatter(x=A, y=trunc_abs_moment(A, a), mode=\"lines\", name=f\"a={a:+.1f}\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=A, y=trunc_pv_mean(A, a), mode=\"lines\", name=f\"a={a:+.1f}\"), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=A, y=trunc_second_moment(A, a), mode=\"lines\", name=f\"a={a:+.1f}\"), row=1, col=3)\n",
    "\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_layout(height=350, title=\"Divergence of truncated moments\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f420c53",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation\n",
    "\n",
    "### NumPy-only inverse CDF sampling\n",
    "Because we have a closed-form CDF and PPF, inverse transform sampling is straightforward:\n",
    "\n",
    "1. Draw $U\\sim\\mathrm{Unif}(0,1)$.\n",
    "2. Return $Y = F^{-1}(U)$.\n",
    "\n",
    "We must clip $U$ away from $0$ and $1$ because the tails are so heavy that the tangent in the PPF can overflow.\n",
    "\n",
    "### Equivalent mixture sampler\n",
    "Using the mixture intuition from Section 2:\n",
    "\n",
    "1. Draw $S\\in\\{-1,+1\\}$ with $\\mathbb{P}(S=+1)=(1+a)/2$.\n",
    "2. Draw $R\\sim \\mathrm{HalfCauchy}(\\text{scale}=1+a)$ if $S=+1$ else $R\\sim \\mathrm{HalfCauchy}(\\text{scale}=1-a)$.\n",
    "3. Return $Y=S\\,R$.\n",
    "\n",
    "We'll implement both and check they agree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed32e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.6\n",
    "\n",
    "s1 = skewcauchy_rvs(a, size=200_000, rng=rng)\n",
    "s2 = skewcauchy_rvs_mixture(a, size=200_000, rng=rng)\n",
    "\n",
    "qs = [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]\n",
    "print(\"Quantiles (inverse-CDF sampler):\", np.quantile(s1, qs))\n",
    "print(\"Quantiles (mixture sampler):    \", np.quantile(s2, qs))\n",
    "\n",
    "print(\"Empirical P(Y>0):\", float(np.mean(s1 > 0)), \"theory:\", (1 + a) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60234e9a",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We'll visualize:\n",
    "\n",
    "- the **PDF** for several `a`\n",
    "- the **CDF** for several `a`\n",
    "- Monte Carlo samples (histogram + empirical CDF)\n",
    "\n",
    "To keep plots readable, we focus on a finite $x$-range; the true distribution has extremely large outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ea92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 3000)\n",
    "a_vals = [-0.8, -0.4, 0.0, 0.4, 0.8]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"PDF\", \"CDF\"))\n",
    "\n",
    "for a in a_vals:\n",
    "    fig.add_trace(go.Scatter(x=x, y=skewcauchy_pdf(x, a), mode=\"lines\", name=f\"a={a:+.1f}\"), row=1, col=1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=skewcauchy_cdf(x, a), mode=\"lines\", name=f\"a={a:+.1f}\", showlegend=False),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"density\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"probability\", row=1, col=2)\n",
    "fig.update_layout(height=450, title=\"skewcauchy PDF/CDF (standardized)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873daf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.6\n",
    "samples = skewcauchy_rvs(a, size=80_000, rng=rng)\n",
    "\n",
    "x_min, x_max = -15, 20\n",
    "\n",
    "x_grid = np.linspace(x_min, x_max, 1200)\n",
    "emp_cdf = np.searchsorted(np.sort(samples), x_grid, side=\"right\") / samples.size\n",
    "th_cdf = skewcauchy_cdf(x_grid, a)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Histogram + PDF\", \"Empirical CDF vs CDF\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=np.clip(samples, x_min, x_max),\n",
    "        nbinsx=120,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"samples (clipped for display)\",\n",
    "        opacity=0.35,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=skewcauchy_pdf(x_grid, a), mode=\"lines\", name=\"PDF\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=emp_cdf, mode=\"lines\", name=\"empirical\"), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=th_cdf, mode=\"lines\", name=\"theoretical\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"density\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"probability\", row=1, col=2)\n",
    "fig.update_layout(height=450, title=f\"Monte Carlo check (a={a})\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711496a",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.skewcauchy`)\n",
    "\n",
    "SciPy exposes the distribution as `scipy.stats.skewcauchy`.\n",
    "\n",
    "Key methods:\n",
    "\n",
    "- `skewcauchy.pdf(x, a, loc, scale)`\n",
    "- `skewcauchy.cdf(x, a, loc, scale)`\n",
    "- `skewcauchy.rvs(a, loc, scale, size, random_state)`\n",
    "- `skewcauchy.fit(data)`  (MLE)\n",
    "\n",
    "Note: `skewcauchy.stats(a, moments='mvsk')` returns `nan`s because moments do not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen distribution and MLE fit demo\n",
    "\n",
    "a_true, loc_true, scale_true = 0.5, -1.0, 2.0\n",
    "rv = skewcauchy(a_true, loc=loc_true, scale=scale_true)\n",
    "\n",
    "data = rv.rvs(size=1500, random_state=7)\n",
    "\n",
    "# Fit returns (a_hat, loc_hat, scale_hat)\n",
    "a_hat, loc_hat, scale_hat = skewcauchy.fit(data)\n",
    "\n",
    "print(\"true:\", (a_true, loc_true, scale_true))\n",
    "print(\"fit: \", (a_hat, loc_hat, scale_hat))\n",
    "\n",
    "ll_true = float(np.sum(skewcauchy.logpdf(data, a_true, loc=loc_true, scale=scale_true)))\n",
    "ll_fit = float(np.sum(skewcauchy.logpdf(data, a_hat, loc=loc_hat, scale=scale_hat)))\n",
    "print(\"loglik(true) =\", ll_true)\n",
    "print(\"loglik(fit)  =\", ll_fit)\n",
    "\n",
    "print(\"stats(mvsk)  =\", skewcauchy.stats(a_true, moments='mvsk'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377f408",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### Hypothesis testing\n",
    "Because the mean/variance do not exist, **mean-based** tests are not appropriate.\n",
    "\n",
    "A simple skewness-related identity for the standardized distribution is:\n",
    "\n",
    "$$\\mathbb{P}(Y>0)=\\frac{1+a}{2}.$$\n",
    "\n",
    "So if the location is known (or you have a robust location estimate), you can test skewness via a **binomial sign test**:\n",
    "\n",
    "- $H_0: a=0$ implies $\\mathbb{P}(Y>0)=1/2$.\n",
    "\n",
    "This is not the most powerful test (it ignores magnitudes), but it is robust and fast.\n",
    "\n",
    "### Bayesian modeling\n",
    "- As a **likelihood** for skewed heavy-tailed noise.\n",
    "- As a robust alternative to Gaussian noise when outliers are frequent and asymmetric.\n",
    "\n",
    "### Generative modeling\n",
    "- As a heavy-tailed component in mixture models (e.g., clustering with outliers).\n",
    "- As a noise model in simulations where extreme events must be common and directionally biased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: sign test for a = 0 (assuming loc=0 is known)\n",
    "\n",
    "n = 200\n",
    "\n",
    "a_true = 0.5\n",
    "samples = skewcauchy_rvs(a_true, size=n, rng=rng)  # standardized\n",
    "k_pos = int(np.sum(samples > 0))\n",
    "\n",
    "res = binomtest(k_pos, n=n, p=0.5, alternative=\"two-sided\")\n",
    "\n",
    "print(\"k positives:\", k_pos, \"out of\", n)\n",
    "print(\"a_hat_from_signs =\", 2 * (k_pos / n) - 1)\n",
    "print(\"p-value for H0: a=0 ->\", res.pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583d30b",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: you must have `-1 < a < 1` and `scale > 0`.\n",
    "- **Moments do not exist**: avoid mean/variance-based estimators, CLT intuition, and moment matching.\n",
    "- **Tails are extreme**: plots and histograms often need clipping or robust axis limits.\n",
    "- **Numerical issues in sampling**: the PPF uses `tan(·)`, which blows up near 0 and 1; clip uniforms away from the endpoints.\n",
    "- **Fitting can be tricky**: MLE is sensitive to extreme points and can be flat in some directions; good initialization helps.\n",
    "- **Use `logpdf` in products**: multiplying many densities underflows; summing log-densities is stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ff8a8",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `skewcauchy` is a **skewed, heavy-tailed** generalization of the Cauchy with shape parameter `a∈(-1,1)`.\n",
    "- It has **no finite mean or variance** (and thus no skewness/kurtosis moments).\n",
    "- The PDF/CDF are **piecewise** and have closed forms; sampling is easy via the **inverse CDF**.\n",
    "- The skewness parameter `a` controls both **sign probability** $\\mathbb{P}(Y>0)=(1+a)/2$ and **tail heaviness**.\n",
    "- SciPy provides a full implementation via `scipy.stats.skewcauchy`, including `fit`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}