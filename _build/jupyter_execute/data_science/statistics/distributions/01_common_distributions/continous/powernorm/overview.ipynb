{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d548ef9",
   "metadata": {},
   "source": [
    "# Power Normal Distribution (`powernorm`) — a proportional-hazards / “minimum of Normals” family\n",
    "\n",
    "The **power normal** distribution is built by taking the **Normal survival function** and raising it to a positive power.\n",
    "It is a simple way to introduce **skewness** and to model **extreme minima** while staying close to the Normal baseline.\n",
    "\n",
    "## What you’ll learn\n",
    "- how `powernorm` is defined (PDF/CDF/survival)\n",
    "- how the shape parameter `c` controls skewness and extremeness\n",
    "- how to compute moments/entropy numerically\n",
    "- how to sample using a NumPy-only inverse-CDF method\n",
    "- how to visualize and fit the model with `scipy.stats.powernorm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a0763",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `powernorm` (power normal distribution)\n",
    "- **Type**: **Continuous**\n",
    "- **Support**: $x \\in \\mathbb{R}$\n",
    "- **Parameters** (SciPy parameterization):\n",
    "  - shape: $c > 0$\n",
    "  - location: $\\text{loc} \\in \\mathbb{R}$\n",
    "  - scale: $\\text{scale} > 0$\n",
    "\n",
    "We write:\n",
    "\n",
    "$$X \\sim \\mathrm{PowerNorm}(c,\\,\\text{loc},\\,\\text{scale}).$$\n",
    "\n",
    "When $\\text{loc}=0$ and $\\text{scale}=1$ we speak about the **standardized** distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27ef0b",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### 2.1 What it models\n",
    "Let $Z$ be a baseline Normal variable with CDF $\\Phi$ and survival function $S_0(x)=1-\\Phi(x)=\\Phi(-x)$.\n",
    "The power normal distribution is defined by the transformed survival function\n",
    "\n",
    "$$S(x) = S_0(x)^c = \\big(\\Phi(-x)\\big)^c.$$\n",
    "\n",
    "This is a **proportional hazards** construction:\n",
    "\n",
    "- baseline hazard: $h_0(x) = \\dfrac{\\phi(x)}{\\Phi(-x)}$ (Normal hazard)\n",
    "- power-normal hazard: $h(x) = c\\,h_0(x)$\n",
    "\n",
    "So $c$ acts like a **hazard multiplier** relative to the Normal baseline.\n",
    "\n",
    "### 2.2 “Minimum of Normals” interpretation (integer $c$)\n",
    "If $c$ is a positive integer and $Z_1,\\dots,Z_c \\overset{iid}{\\sim} \\mathcal{N}(0,1)$, then\n",
    "\n",
    "$$\\min(Z_1,\\dots,Z_c) \\sim \\mathrm{PowerNorm}(c).$$\n",
    "\n",
    "Intuition: the minimum gets more extreme as $c$ increases.\n",
    "\n",
    "### 2.3 Real-world use cases\n",
    "- **Reliability / weakest-link modeling**: the minimum of several latent “strength” variables\n",
    "- **Quality control**: the worst of $c$ subcomponents drives the overall behavior\n",
    "- **Risk modeling**: left-tail emphasis (rare but severe negative events)\n",
    "- **Survival analysis**: a Normal baseline with **proportional hazards** scaling\n",
    "\n",
    "### 2.4 Relations to other distributions\n",
    "- $c=1$ gives the **standard Normal**.\n",
    "- For integer $c$, it is an **order statistic** (minimum) of Normal samples.\n",
    "- It is a member of “power” / **Lehmann-type** transformations (here applied to the survival function).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3faec8",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $\\phi$ and $\\Phi$ be the standard Normal PDF and CDF.\n",
    "\n",
    "### 3.1 PDF (standardized form)\n",
    "For $x \\in \\mathbb{R}$ and $c>0$:\n",
    "\n",
    "$$f(x\\mid c) = c\\,\\phi(x)\\,\\big(\\Phi(-x)\\big)^{c-1}.$$\n",
    "\n",
    "### 3.2 CDF / survival function\n",
    "Because $\\dfrac{d}{dx}\\Phi(-x) = -\\phi(x)$, the CDF has a simple closed form:\n",
    "\n",
    "$$F(x\\mid c) = 1 - \\big(\\Phi(-x)\\big)^c,\\qquad S(x\\mid c)=1-F(x\\mid c)=\\big(\\Phi(-x)\\big)^c.$$\n",
    "\n",
    "### 3.3 Location–scale form\n",
    "For $X \\sim \\mathrm{PowerNorm}(c,\\text{loc},\\text{scale})$ define\n",
    "\n",
    "$$z = \\frac{x-\\text{loc}}{\\text{scale}}.$$\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align}\n",
    " f(x\\mid c,\\text{loc},\\text{scale})\n",
    " &= \\frac{c}{\\text{scale}}\\,\\phi(z)\\,\\big(\\Phi(-z)\\big)^{c-1},\\\\\n",
    " F(x\\mid c,\\text{loc},\\text{scale})\n",
    " &= 1 - \\big(\\Phi(-z)\\big)^c.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powernorm_logpdf(x: np.ndarray, c: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Log-PDF using stable Normal log-CDF evaluation (SciPy).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "\n",
    "    if c <= 0:\n",
    "        raise ValueError(\"c must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    z = (x - loc) / scale\n",
    "    return (\n",
    "        np.log(c)\n",
    "        - np.log(scale)\n",
    "        + stats.norm.logpdf(z)\n",
    "        + (c - 1.0) * stats.norm.logcdf(-z)\n",
    "    )\n",
    "\n",
    "\n",
    "def powernorm_pdf(x: np.ndarray, c: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    return np.exp(powernorm_logpdf(x, c, loc=loc, scale=scale))\n",
    "\n",
    "\n",
    "def powernorm_cdf(x: np.ndarray, c: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"CDF computed as 1 - sf, using expm1 for accuracy when sf is close to 1.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "\n",
    "    if c <= 0:\n",
    "        raise ValueError(\"c must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    z = (x - loc) / scale\n",
    "    log_sf = c * stats.norm.logcdf(-z)\n",
    "    return -np.expm1(log_sf)\n",
    "\n",
    "\n",
    "def powernorm_ppf(q: np.ndarray, c: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "\n",
    "    if c <= 0:\n",
    "        raise ValueError(\"c must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    eps = np.finfo(float).eps\n",
    "    q = np.clip(q, eps, 1.0 - eps)\n",
    "\n",
    "    z = -stats.norm.ppf(np.power(1.0 - q, 1.0 / c))\n",
    "    return loc + scale * z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2cdd0",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Unlike many textbook families, `powernorm` does **not** generally have simple closed-form moments.\n",
    "The key quantities are typically computed via **numerical integration** (or Monte Carlo).\n",
    "\n",
    "### 4.1 Raw moments\n",
    "For the standardized form ($\\text{loc}=0,\\text{scale}=1$), the $k$-th raw moment is\n",
    "\n",
    "$$\\mathbb{E}[X^k] = \\int_{-\\infty}^{\\infty} x^k\\,c\\,\\phi(x)\\,(\\Phi(-x))^{c-1}\\,dx.$$\n",
    "\n",
    "A useful change of variables is $u = \\Phi(-x)$, so $du = -\\phi(x)\\,dx$ and $x = -\\Phi^{-1}(u)$:\n",
    "\n",
    "$$\\mathbb{E}[X^k] = c\\int_0^1 \\big(-\\Phi^{-1}(u)\\big)^k\\,u^{c-1}\\,du.$$\n",
    "\n",
    "This shows an equivalent generative representation:\n",
    "\n",
    "- If $U \\sim \\mathrm{Beta}(c,1)$ then $X = -\\Phi^{-1}(U) \\sim \\mathrm{PowerNorm}(c)$.\n",
    "\n",
    "### 4.2 Mean, variance, skewness, kurtosis\n",
    "You can compute\n",
    "\n",
    "- mean $\\mu = \\mathbb{E}[X]$\n",
    "- variance $\\sigma^2 = \\mathbb{E}[X^2]-\\mu^2$\n",
    "- skewness $\\gamma_1$\n",
    "- excess kurtosis $\\gamma_2$ (kurtosis minus 3)\n",
    "\n",
    "numerically. SciPy’s `stats.powernorm(...).stats(moments='mvsk')` uses robust numerical routines.\n",
    "\n",
    "### 4.3 MGF / characteristic function\n",
    "The MGF and characteristic function can be written as integrals:\n",
    "\n",
    "\\begin{align}\n",
    "M_X(t) &= \\mathbb{E}[e^{tX}] = c\\int_{-\\infty}^{\\infty} e^{tx}\\,\\phi(x)\\,(\\Phi(-x))^{c-1}\\,dx,\\\\\n",
    "\\varphi_X(\\omega) &= \\mathbb{E}[e^{i\\omega X}] = c\\int_{-\\infty}^{\\infty} e^{i\\omega x}\\,\\phi(x)\\,(\\Phi(-x))^{c-1}\\,dx.\n",
    "\\end{align}\n",
    "\n",
    "### 4.4 Entropy\n",
    "The differential entropy is\n",
    "\n",
    "$$h(X) = -\\mathbb{E}[\\log f(X)].$$\n",
    "\n",
    "SciPy provides `dist.entropy()` (numerical).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72978bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical moments via SciPy (entropy via Monte Carlo)\n",
    "cs = np.array([0.5, 0.8, 1.0, 1.5, 3.0, 8.0])\n",
    "\n",
    "# SciPy's dist.entropy() uses numerical integration and may emit warnings for c<1.\n",
    "# A robust alternative is a Monte Carlo estimate:  h(X) = -E[log f(X)].\n",
    "\n",
    "n_entropy = 80_000\n",
    "\n",
    "rows = []\n",
    "for c in cs:\n",
    "    dist = stats.powernorm(c)\n",
    "    mean, var, skew, exkurt = dist.stats(moments=\"mvsk\")\n",
    "\n",
    "    x_ent = dist.rvs(size=n_entropy, random_state=rng)\n",
    "    ent_mc = -dist.logpdf(x_ent).mean()\n",
    "\n",
    "    rows.append([c, float(mean), float(var), float(skew), float(exkurt), float(ent_mc)])\n",
    "\n",
    "rows = np.array(rows)\n",
    "cols = [\"c\", \"mean\", \"var\", \"skew\", \"excess_kurt\", \"entropy_mc\"]\n",
    "rows, cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check (MGF + characteristic function at a few points)\n",
    "c0 = 3.0\n",
    "n = 200_000\n",
    "samples = stats.powernorm(c0).rvs(size=n, random_state=rng)\n",
    "\n",
    "mc_mean = samples.mean()\n",
    "mc_var = samples.var(ddof=0)\n",
    "\n",
    "ts = np.array([-1.0, -0.5, 0.5, 1.0])\n",
    "mgf_mc = np.array([np.mean(np.exp(t * samples)) for t in ts])\n",
    "\n",
    "ws = np.array([0.5, 1.0, 2.0])\n",
    "cf_mc = np.array([np.mean(np.exp(1j * w * samples)) for w in ws])\n",
    "\n",
    "true_mean, true_var, true_skew, true_exkurt = stats.powernorm(c0).stats(moments=\"mvsk\")\n",
    "true_ent = stats.powernorm(c0).entropy()\n",
    "\n",
    "{\n",
    "    \"true_mean\": float(true_mean),\n",
    "    \"mc_mean\": float(mc_mean),\n",
    "    \"true_var\": float(true_var),\n",
    "    \"mc_var\": float(mc_var),\n",
    "    \"true_entropy\": float(true_ent),\n",
    "    \"mgf_mc(t)\": dict(zip(ts, mgf_mc)),\n",
    "    \"cf_mc(w)\": dict(zip(ws, cf_mc)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48d1fc",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### 5.1 Shape parameter $c$\n",
    "The key identity is the survival function:\n",
    "\n",
    "$$S(x\\mid c) = \\big(\\Phi(-x)\\big)^c.$$\n",
    "\n",
    "- **$c=1$**: exactly standard Normal.\n",
    "- **$c>1$**: $S(x)$ shrinks faster than the Normal survival, so the distribution shifts **left** (more extreme minima) and typically has **negative skewness**.\n",
    "- **$0<c<1$**: $S(x)$ shrinks more slowly; the distribution shifts **right** and typically has **positive skewness**.\n",
    "\n",
    "If $c$ is an integer, $c$ is literally the number of Normal draws whose **minimum** you are taking.\n",
    "\n",
    "### 5.2 `loc` and `scale`\n",
    "`loc` shifts the distribution; `scale` stretches it:\n",
    "\n",
    "$$X = \\text{loc} + \\text{scale}\\cdot Z,\\quad Z \\sim \\mathrm{PowerNorm}(c).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape changes: PDFs for different c\n",
    "c_values = [0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "# Choose a common plotting range based on central quantiles\n",
    "qs = np.array([0.001, 0.999])\n",
    "lo = min(stats.powernorm(c).ppf(qs[0]) for c in c_values)\n",
    "hi = max(stats.powernorm(c).ppf(qs[1]) for c in c_values)\n",
    "\n",
    "x = np.linspace(lo, hi, 800)\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in c_values:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=stats.powernorm(c).pdf(x), mode=\"lines\", name=f\"c={c}\")\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PowerNorm PDF for different c (standardized)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=430,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d54776",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation (standardized)\n",
    "Start from the definition:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x\\,c\\,\\phi(x)\\,(\\Phi(-x))^{c-1}\\,dx.$$\n",
    "\n",
    "Substitute $u=\\Phi(-x)$ so $du=-\\phi(x)\\,dx$ and $x=-\\Phi^{-1}(u)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[X]\n",
    "&= c\\int_0^1 \\big(-\\Phi^{-1}(u)\\big)\\,u^{c-1}\\,du \\\\\n",
    "&= \\mathbb{E}\\big[-\\Phi^{-1}(U)\\big],\\qquad U\\sim\\mathrm{Beta}(c,1).\n",
    "\\end{align}\n",
    "\n",
    "No general closed form is known; compute numerically (SciPy `expect`/`stats`) or by Monte Carlo.\n",
    "\n",
    "### 6.2 Variance\n",
    "Similarly,\n",
    "\n",
    "$$\\mathbb{E}[X^2] = c\\int_0^1 \\big(-\\Phi^{-1}(u)\\big)^2\\,u^{c-1}\\,du,$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2.$$\n",
    "\n",
    "### 6.3 Likelihood (with `loc`, `scale`)\n",
    "Let $x_1,\\dots,x_n$ be i.i.d. from $\\mathrm{PowerNorm}(c,\\text{loc},\\text{scale})$ and define $z_i=(x_i-\\text{loc})/\\text{scale}$.\n",
    "The log-likelihood is\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(c,\\text{loc},\\text{scale})\n",
    "&= \\sum_{i=1}^n \\log f(x_i)\\\\\n",
    "&= n\\log c - n\\log \\text{scale} + \\sum_{i=1}^n \\log \\phi(z_i) + (c-1)\\sum_{i=1}^n \\log \\Phi(-z_i).\n",
    "\\end{align}\n",
    "\n",
    "**Conditional MLE for $c$ (given `loc`, `scale`)**\n",
    "\n",
    "Differentiate w.r.t. $c$:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial c} = \\frac{n}{c} + \\sum_{i=1}^n \\log \\Phi(-z_i).$$\n",
    "\n",
    "Setting this to zero yields a closed-form conditional estimator:\n",
    "\n",
    "$$\\hat c = -\\frac{n}{\\sum_{i=1}^n \\log \\Phi(-z_i)}.$$\n",
    "\n",
    "Numerical note: use `logcdf` for stability when $\\Phi(-z_i)$ is tiny.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: conditional MLE for c when loc/scale are known\n",
    "c_true = 2.5\n",
    "x = stats.powernorm(c_true).rvs(size=5_000, random_state=rng)\n",
    "\n",
    "log_u = stats.norm.logcdf(-x)  # log Phi(-x)\n",
    "\n",
    "c_hat = -x.size / log_u.sum()\n",
    "\n",
    "c_true, float(c_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84202fe",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### 7.1 Inverse-CDF sampling\n",
    "From the CDF\n",
    "\n",
    "$$F(x\\mid c)=1-\\big(\\Phi(-x)\\big)^c,$$\n",
    "\n",
    "set $U\\sim\\mathrm{Uniform}(0,1)$ and solve $U=F(X)$:\n",
    "\n",
    "\\begin{align}\n",
    "U &= 1-\\big(\\Phi(-X)\\big)^c \\\\\n",
    "\\Phi(-X) &= (1-U)^{1/c} \\\\\n",
    "X &= -\\Phi^{-1}\\big((1-U)^{1/c}\\big).\n",
    "\\end{align}\n",
    "\n",
    "Equivalently (by renaming $1-U$ as another Uniform random variable):\n",
    "\n",
    "$$X = -\\Phi^{-1}\\big(U^{1/c}\\big).$$\n",
    "\n",
    "### 7.2 NumPy-only implementation\n",
    "NumPy does not ship a vectorized Normal inverse-CDF, so below we implement a high-quality **rational approximation** (Acklam’s approximation) using only NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7535c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_ppf_acklam(p: np.ndarray) -> np.ndarray:\n",
    "    '''Approximate standard Normal quantile function Φ^{-1}(p).\n",
    "\n",
    "    Vectorized rational approximation due to Peter John Acklam.\n",
    "    Accuracy is typically ~1e-9 in the central region.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : array-like\n",
    "        Probabilities in (0, 1).\n",
    "    '''\n",
    "\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if np.any((p <= 0) | (p >= 1)):\n",
    "        raise ValueError('p must be strictly between 0 and 1')\n",
    "\n",
    "    # Coefficients in rational approximations\n",
    "    a = np.array(\n",
    "        [\n",
    "            -3.969683028665376e01,\n",
    "            2.209460984245205e02,\n",
    "            -2.759285104469687e02,\n",
    "            1.383577518672690e02,\n",
    "            -3.066479806614716e01,\n",
    "            2.506628277459239e00,\n",
    "        ]\n",
    "    )\n",
    "    b = np.array(\n",
    "        [\n",
    "            -5.447609879822406e01,\n",
    "            1.615858368580409e02,\n",
    "            -1.556989798598866e02,\n",
    "            6.680131188771972e01,\n",
    "            -1.328068155288572e01,\n",
    "        ]\n",
    "    )\n",
    "    c = np.array(\n",
    "        [\n",
    "            -7.784894002430293e-03,\n",
    "            -3.223964580411365e-01,\n",
    "            -2.400758277161838e00,\n",
    "            -2.549732539343734e00,\n",
    "            4.374664141464968e00,\n",
    "            2.938163982698783e00,\n",
    "        ]\n",
    "    )\n",
    "    d = np.array(\n",
    "        [\n",
    "            7.784695709041462e-03,\n",
    "            3.224671290700398e-01,\n",
    "            2.445134137142996e00,\n",
    "            3.754408661907416e00,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plow = 0.02425\n",
    "    phigh = 1.0 - plow\n",
    "\n",
    "    x = np.empty_like(p)\n",
    "\n",
    "    # Lower region\n",
    "    mask = p < plow\n",
    "    if np.any(mask):\n",
    "        q = np.sqrt(-2.0 * np.log(p[mask]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q) + c[5]\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q) + 1.0\n",
    "        x[mask] = -num / den\n",
    "\n",
    "    # Central region\n",
    "    mask = (p >= plow) & (p <= phigh)\n",
    "    if np.any(mask):\n",
    "        q = p[mask] - 0.5\n",
    "        r = q * q\n",
    "        num = (\n",
    "            (((((a[0] * r + a[1]) * r + a[2]) * r + a[3]) * r + a[4]) * r) + a[5]\n",
    "        ) * q\n",
    "        den = (((((b[0] * r + b[1]) * r + b[2]) * r + b[3]) * r + b[4]) * r) + 1.0\n",
    "        x[mask] = num / den\n",
    "\n",
    "    # Upper region\n",
    "    mask = p > phigh\n",
    "    if np.any(mask):\n",
    "        q = np.sqrt(-2.0 * np.log(1.0 - p[mask]))\n",
    "        num = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q) + c[5]\n",
    "        den = ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q) + 1.0\n",
    "        x[mask] = num / den\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def powernorm_rvs_numpy(\n",
    "    c: float,\n",
    "    size: int | tuple[int, ...] = 1,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    '''NumPy-only sampler for PowerNorm(c, loc, scale) using inverse transform.'''\n",
    "\n",
    "    c = float(c)\n",
    "    loc = float(loc)\n",
    "    scale = float(scale)\n",
    "\n",
    "    if c <= 0:\n",
    "        raise ValueError('c must be > 0')\n",
    "    if scale <= 0:\n",
    "        raise ValueError('scale must be > 0')\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    eps = np.finfo(float).eps\n",
    "    u = rng.random(size=size)\n",
    "    u = np.clip(u, eps, 1.0 - eps)\n",
    "\n",
    "    z = -norm_ppf_acklam(u ** (1.0 / c))\n",
    "    return loc + scale * z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e27470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: NumPy-only sampler vs SciPy moments\n",
    "c0 = 4.0\n",
    "n = 200_000\n",
    "samples_np = powernorm_rvs_numpy(c0, size=n, rng=rng)\n",
    "\n",
    "mean_np = samples_np.mean()\n",
    "var_np = samples_np.var(ddof=0)\n",
    "\n",
    "mean_sp, var_sp = stats.powernorm(c0).stats(moments=\"mv\")\n",
    "\n",
    "float(mean_np), float(mean_sp), float(var_np), float(var_sp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e7460",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PDF** for a fixed parameter choice\n",
    "- the **CDF** vs empirical CDF from Monte Carlo samples\n",
    "- a histogram of **Monte Carlo samples** against the theoretical density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60006e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = 4.0\n",
    "n = 120_000\n",
    "samples = powernorm_rvs_numpy(c0, size=n, rng=rng)\n",
    "\n",
    "dist = stats.powernorm(c0)\n",
    "\n",
    "x = np.linspace(dist.ppf(0.001), dist.ppf(0.999), 900)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples,\n",
    "        nbinsx=120,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"samples (NumPy)\",\n",
    "        opacity=0.35,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=dist.pdf(x), mode=\"lines\", name=\"theoretical pdf\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"PowerNorm(c={c0}): histogram vs PDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=430,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF: theoretical vs empirical\n",
    "x = np.linspace(dist.ppf(0.001), dist.ppf(0.999), 900)\n",
    "\n",
    "emp_x = np.sort(samples)\n",
    "emp_cdf = np.arange(1, emp_x.size + 1) / emp_x.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=dist.cdf(x), mode=\"lines\", name=\"theoretical CDF\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=emp_x[::200],\n",
    "        y=emp_cdf[::200],\n",
    "        mode=\"markers\",\n",
    "        name=\"empirical CDF (subsampled)\",\n",
    "        marker=dict(size=4, opacity=0.6),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"PowerNorm(c={c0}): theoretical CDF vs empirical CDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"CDF\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d43ac8",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.powernorm`)\n",
    "\n",
    "SciPy parameterization:\n",
    "\n",
    "```python\n",
    "stats.powernorm(c, loc=0, scale=1)\n",
    "```\n",
    "\n",
    "- `c` is the shape parameter ($c>0$).\n",
    "- `loc` and `scale` provide the usual location–scale transform.\n",
    "\n",
    "Useful methods: `pdf`, `logpdf`, `cdf`, `sf`, `ppf`, `rvs`, `stats`, `entropy`, `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c234d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.powernorm(2.5, loc=1.0, scale=2.0)\n",
    "\n",
    "x = np.linspace(dist.ppf(0.01), dist.ppf(0.99), 5)\n",
    "\n",
    "pdf = dist.pdf(x)\n",
    "cdf = dist.cdf(x)\n",
    "ppf = dist.ppf(np.array([0.1, 0.5, 0.9]))\n",
    "samples = dist.rvs(size=5, random_state=rng)\n",
    "\n",
    "pdf, cdf, ppf, samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting (MLE) with SciPy\n",
    "# Tip: if you know data are already standardized, fix loc=0 and scale=1.\n",
    "\n",
    "c_true, loc_true, scale_true = 3.0, -0.5, 1.2\n",
    "x = stats.powernorm(c_true, loc=loc_true, scale=scale_true).rvs(size=8_000, random_state=rng)\n",
    "\n",
    "c_hat, loc_hat, scale_hat = stats.powernorm.fit(x)\n",
    "\n",
    "(c_true, loc_true, scale_true), (float(c_hat), float(loc_hat), float(scale_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70279bd",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing\n",
    "A common question is whether data are well-modeled by a Normal distribution.\n",
    "Since $c=1$ recovers the Normal, you can test:\n",
    "\n",
    "- $H_0: c=1$ (Normal)\n",
    "- $H_1: c\\neq 1$ (PowerNorm)\n",
    "\n",
    "using a **likelihood ratio test** (LRT) when `loc` and `scale` are known/fixed (or under large-sample approximations).\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "Treat $c$ as an unknown parameter with a prior (e.g. log-normal), and compute a posterior over $c$.\n",
    "There is no conjugacy, but **grid inference** works well for a single parameter.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "Because for integer $c$ the distribution equals the **minimum of $c$ Normals**, it provides a simple generative story for “worst-case” effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a49455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Likelihood-ratio test (loc=0, scale=1 assumed known)\n",
    "\n",
    "n = 2_000\n",
    "c_true = 2.0\n",
    "x = stats.powernorm(c_true).rvs(size=n, random_state=rng)\n",
    "\n",
    "# Under H1, use the closed-form conditional MLE for c (since loc/scale known)\n",
    "log_u = stats.norm.logcdf(-x)\n",
    "c_hat = -n / log_u.sum()\n",
    "\n",
    "ll0 = powernorm_logpdf(x, c=1.0).sum()\n",
    "ll1 = powernorm_logpdf(x, c=float(c_hat)).sum()\n",
    "\n",
    "lrt = 2 * (ll1 - ll0)\n",
    "p_value = stats.chi2.sf(lrt, df=1)\n",
    "\n",
    "{\n",
    "    \"c_true\": c_true,\n",
    "    \"c_hat\": float(c_hat),\n",
    "    \"LRT\": float(lrt),\n",
    "    \"p_value(chi2, df=1)\": float(p_value),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe22724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian grid inference for c (loc=0, scale=1 fixed)\n",
    "\n",
    "x = stats.powernorm(2.0).rvs(size=500, random_state=rng)\n",
    "\n",
    "n = x.size\n",
    "sum_log_phi = stats.norm.logpdf(x).sum()\n",
    "sum_log_u = stats.norm.logcdf(-x).sum()  # sum log Phi(-x)\n",
    "\n",
    "c_grid = np.linspace(0.2, 6.0, 700)\n",
    "\n",
    "# Prior: log c ~ Normal(0, 0.7)\n",
    "logc = np.log(c_grid)\n",
    "log_prior = stats.norm.logpdf(logc, loc=0.0, scale=0.7) - logc\n",
    "\n",
    "log_lik = n * np.log(c_grid) + sum_log_phi + (c_grid - 1.0) * sum_log_u\n",
    "log_post_unnorm = log_prior + log_lik\n",
    "\n",
    "log_post_unnorm -= log_post_unnorm.max()\n",
    "post = np.exp(log_post_unnorm)\n",
    "post /= np.trapz(post, c_grid)\n",
    "\n",
    "post_mean = np.trapz(c_grid * post, c_grid)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=c_grid, y=post, mode=\"lines\", name=\"posterior density\"))\n",
    "fig.add_vline(x=post_mean, line_dash=\"dash\", line_color=\"black\", annotation_text=\"posterior mean\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Posterior over c (grid); posterior mean ≈ {post_mean:.3f}\",\n",
    "    xaxis_title=\"c\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=420,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8908446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Generative story: min of c Normals (integer c)\n",
    "\n",
    "c_int = 6\n",
    "n_groups = 80_000\n",
    "\n",
    "mins = rng.standard_normal((n_groups, c_int)).min(axis=1)\n",
    "\n",
    "dist = stats.powernorm(c_int)\n",
    "x = np.linspace(dist.ppf(0.001), dist.ppf(0.999), 900)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=mins,\n",
    "        nbinsx=120,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"min of c Normals\",\n",
    "        opacity=0.35,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=dist.pdf(x), mode=\"lines\", name=\"PowerNorm(c) pdf\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"min of {c_int} i.i.d. Normals ≈ PowerNorm(c={c_int})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    width=900,\n",
    "    height=430,\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0c6d7",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: `c <= 0` or `scale <= 0` is not valid.\n",
    "- **Interpreting `c` as a sample size**: the “minimum of $c$ Normals” story is exact only when $c$ is an integer.\n",
    "- **Numerical underflow in tails**:\n",
    "  - $\\Phi(-x)$ can be extremely small for large positive $x$.\n",
    "  - Directly computing $(\\Phi(-x))^c$ can underflow to 0.\n",
    "  - Prefer log-space computations: `stats.norm.logcdf` and `powernorm.logpdf`.\n",
    "- **Fitting can be sensitive**:\n",
    "  - With free `loc` and `scale`, likelihood surfaces can be flat or multi-modal for small datasets.\n",
    "  - If domain knowledge suggests a fixed `loc`/`scale`, constrain them (e.g., `floc=0, fscale=1`) to stabilize MLE.\n",
    "- **Approximate NumPy-only sampler**:\n",
    "  - The Acklam approximation is accurate, but extreme tail quantiles can still accumulate error.\n",
    "  - For production-grade sampling in tails, prefer SciPy’s `powernorm.rvs`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce71dcd",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `powernorm` is a continuous distribution on $\\mathbb{R}$ with shape parameter $c>0$.\n",
    "- It is defined by raising the Normal **survival function** to a power: $S(x)=\\Phi(-x)^c$.\n",
    "- $c=1$ recovers the Normal; larger $c$ corresponds to more extreme **minima** (left shift and negative skew).\n",
    "- Moments, MGF/CF, and entropy are typically evaluated **numerically**; SciPy provides reliable routines.\n",
    "- Sampling is easy via inverse-CDF: $X=-\\Phi^{-1}(U^{1/c})$.\n",
    "\n",
    "**References**\n",
    "- SciPy documentation: `scipy.stats.powernorm`\n",
    "- NIST Engineering Statistics Handbook, “Power Normal Distribution”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}