{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1113e9d",
   "metadata": {},
   "source": [
    "# Burr Distribution (`burr`, Burr Type III / Dagum)\n",
    "\n",
    "The **Burr (Type III)** distribution (called the **Dagum distribution** in economics) is a flexible family on **positive real values** with **polynomial (heavy) tails**.\n",
    "\n",
    "It is a good default when:\n",
    "\n",
    "- your data are **strictly positive** (income, sizes, waiting times, lifetimes)\n",
    "- the right tail can be **much heavier** than Lognormal/Gamma\n",
    "- you want a model with **interpretable tail behavior** and an easy sampler\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- write the PDF/CDF/quantile function and interpret the parameters\n",
    "- compute moments (and know when they do *not* exist)\n",
    "- derive expectation/variance and the likelihood\n",
    "- sample from `burr` using inverse-transform sampling (**NumPy-only**)\n",
    "- fit and use the distribution via `scipy.stats.burr`\n",
    "\n",
    "## Notation\n",
    "\n",
    "- Shape parameters: $c > 0$, $d > 0$\n",
    "- Random variable: $X \\sim \\mathrm{BurrIII}(c, d)$\n",
    "- Standard support: $x > 0$\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c882b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import gammaln, psi, logsumexp\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"scipy:\", scipy.__version__)\n",
    "print(\"plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135edec",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `burr` (Burr Type III; also known as the **Dagum distribution**)\n",
    "- **Type**: **continuous**\n",
    "- **Standard support**: $x > 0$ (SciPy defines the PDF for $x \\ge 0$; depending on parameters, the density can diverge as $x \\to 0^+$)\n",
    "- **Parameter space (standard form)**: $c > 0$, $d > 0$\n",
    "- **Location/scale form (SciPy)**: $X = \\mathrm{loc} + \\mathrm{scale}\\cdot Y$ with $Y \\sim \\mathrm{BurrIII}(c, d)$\n",
    "  - Support becomes $x > \\mathrm{loc}$\n",
    "  - $\\mathrm{scale} > 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afbb82",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "The Burr Type III/Dagum family is a **positive**, **right-skewed**, **heavy-tailed** model. It can represent distributions where:\n",
    "\n",
    "- very small values are possible (depending on $c d$)\n",
    "- extremely large values occasionally occur\n",
    "- the right tail decays like a **power law**\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Income/wealth modeling** (Dagum distribution is common in economics)\n",
    "- **Insurance claim sizes** and other positive heavy-tailed costs\n",
    "- **Reliability** / lifetime modeling when failures can occur very late\n",
    "- **Hydrology** and environmental quantities with occasional extremes\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Log-logistic (Fisk)**: when $d = 1$,\n",
    "  $$F(x) = \\frac{1}{1 + x^{-c}}$$\n",
    "  and $\\log X$ is logistic with scale $1/c$.\n",
    "- **Reciprocal relationship to Burr XII**: if $X \\sim \\mathrm{BurrIII}(c,d)$, then $1/X \\sim \\mathrm{BurrXII}(c,d)$. SciPy provides both `scipy.stats.burr` (Type III/Dagum) and `scipy.stats.burr12` (Type XII).\n",
    "- **Key transformation**: $T = X^{-c}$ follows a Lomax (Pareto II) distribution with shape $d$. This turns many calculations into Beta/Gamma-function identities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407f589",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### CDF\n",
    "\n",
    "In the standard (unshifted, unit-scale) parameterization, for $x>0$:\n",
    "\n",
    "$$\n",
    "F(x; c, d) = \\left(1 + x^{-c}\\right)^{-d}, \\qquad c>0,\\ d>0.\n",
    "$$\n",
    "\n",
    "### PDF\n",
    "\n",
    "Differentiating the CDF gives the density:\n",
    "\n",
    "$$\n",
    "f(x; c, d)\n",
    "= \\frac{\\partial}{\\partial x} \\left(1 + x^{-c}\\right)^{-d}\n",
    "= c d\\, \\frac{x^{-c-1}}{\\left(1 + x^{-c}\\right)^{d+1}}, \\qquad x>0.\n",
    "$$\n",
    "\n",
    "### Quantile function (inverse CDF)\n",
    "\n",
    "Let $p \\in (0,1)$. Solving $p = (1 + x^{-c})^{-d}$ gives:\n",
    "\n",
    "$$\n",
    "Q(p) = \\left(p^{-1/d} - 1\\right)^{-1/c}.\n",
    "$$\n",
    "\n",
    "### Location/scale\n",
    "\n",
    "SciPy uses the standard location/scale convention:\n",
    "\n",
    "$$\n",
    "X = \\mathrm{loc} + \\mathrm{scale}\\cdot Y, \\qquad Y \\sim \\mathrm{BurrIII}(c,d),\\ \\mathrm{scale}>0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67df217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burr_logpdf(x, c, d):\n",
    "    \"\"\"Log-PDF of the standard Burr Type III / Dagum distribution (loc=0, scale=1).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "    logx = np.log(xm)\n",
    "    # log(1 + x^{-c}) = log(1 + exp(-c log x)) computed stably\n",
    "    log1p_xnegc = np.logaddexp(0.0, -c * logx)\n",
    "    out[mask] = (\n",
    "        np.log(c)\n",
    "        + np.log(d)\n",
    "        + (-c - 1.0) * logx\n",
    "        - (d + 1.0) * log1p_xnegc\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def burr_pdf(x, c, d):\n",
    "    return np.exp(burr_logpdf(x, c, d))\n",
    "\n",
    "\n",
    "def burr_logcdf(x, c, d):\n",
    "    \"\"\"Log-CDF of the standard Burr Type III / Dagum distribution.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "    logx = np.log(xm)\n",
    "    log1p_xnegc = np.logaddexp(0.0, -c * logx)\n",
    "    out[mask] = -d * log1p_xnegc\n",
    "    return out\n",
    "\n",
    "\n",
    "def burr_cdf(x, c, d):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x > 0\n",
    "    out[mask] = np.exp(burr_logcdf(x[mask], c, d))\n",
    "    return out\n",
    "\n",
    "\n",
    "def burr_ppf(p, c, d):\n",
    "    \"\"\"Quantile function Q(p) for p in [0,1].\"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    x = np.full_like(p, np.nan, dtype=float)\n",
    "    x[p == 0] = 0.0\n",
    "    x[p == 1] = np.inf\n",
    "    mask = (p > 0) & (p < 1)\n",
    "    # p^{-1/d} - 1 = exp(-log p / d) - 1; expm1 is stable when p ~ 1.\n",
    "    t = np.expm1(-np.log(p[mask]) / d)\n",
    "    x[mask] = np.power(t, -1.0 / c)\n",
    "    return x\n",
    "\n",
    "\n",
    "def burr_rvs_numpy(c, d, size, rng=None):\n",
    "    \"\"\"NumPy-only sampler via inverse-transform sampling.\"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    u = rng.random(size)\n",
    "    return burr_ppf(u, c, d)\n",
    "\n",
    "\n",
    "def burr_raw_moment(k, c, d):\n",
    "    \"\"\"Raw moment E[X^k] for k < c; returns +inf when the moment diverges.\"\"\"\n",
    "    if k >= c:\n",
    "        return np.inf\n",
    "    return np.exp(gammaln(1.0 - k / c) + gammaln(d + k / c) - gammaln(d))\n",
    "\n",
    "\n",
    "def burr_entropy(c, d):\n",
    "    \"\"\"Differential entropy of the standard Burr Type III / Dagum distribution.\"\"\"\n",
    "    return -np.log(c * d) - (1.0 + 1.0 / c) * (psi(1.0) - psi(d)) + 1.0 + 1.0 / d\n",
    "\n",
    "\n",
    "def burr_summary_stats(c, d):\n",
    "    \"\"\"Mean/variance/skewness/excess kurtosis (when finite), else nan/inf.\"\"\"\n",
    "    mean = burr_raw_moment(1.0, c, d) if c > 1 else np.inf\n",
    "    if c <= 2:\n",
    "        return mean, np.inf, np.nan, np.nan\n",
    "\n",
    "    m2 = burr_raw_moment(2.0, c, d)\n",
    "    var = m2 - mean**2\n",
    "\n",
    "    skew = np.nan\n",
    "    exkurt = np.nan\n",
    "\n",
    "    if c > 3:\n",
    "        m3 = burr_raw_moment(3.0, c, d)\n",
    "        mu3 = m3 - 3 * m2 * mean + 2 * mean**3\n",
    "        skew = mu3 / (var ** 1.5)\n",
    "\n",
    "    if c > 4:\n",
    "        m3 = burr_raw_moment(3.0, c, d)  # defined since c>4\n",
    "        m4 = burr_raw_moment(4.0, c, d)\n",
    "        mu4 = m4 - 4 * m3 * mean + 6 * m2 * mean**2 - 3 * mean**4\n",
    "        exkurt = mu4 / (var**2) - 3.0\n",
    "\n",
    "    return mean, var, skew, exkurt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05ec78",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Existence of moments (key takeaway)\n",
    "\n",
    "The right tail behaves like a power law:\n",
    "\n",
    "$$\n",
    "\\Pr(X > x) = 1 - F(x) \\sim d\\,x^{-c}\\quad \\text{as } x\\to\\infty.\n",
    "$$\n",
    "\n",
    "So the $k$-th moment exists **iff** $k < c$ (independent of $d$).\n",
    "\n",
    "### Raw moments\n",
    "\n",
    "For $k < c$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k]\n",
    "= \\frac{\\Gamma\\left(1 - \\tfrac{k}{c}\\right)\\,\\Gamma\\left(d + \\tfrac{k}{c}\\right)}{\\Gamma(d)}.\n",
    "$$\n",
    "\n",
    "### Mean and variance\n",
    "\n",
    "- Mean (exists for $c>1$):\n",
    "  $$\\mathbb{E}[X] = \\frac{\\Gamma\\left(1 - \\tfrac{1}{c}\\right)\\,\\Gamma\\left(d + \\tfrac{1}{c}\\right)}{\\Gamma(d)}$$\n",
    "- Second moment exists for $c>2$:\n",
    "  $$\\mathbb{E}[X^2] = \\frac{\\Gamma\\left(1 - \\tfrac{2}{c}\\right)\\,\\Gamma\\left(d + \\tfrac{2}{c}\\right)}{\\Gamma(d)}$$\n",
    "- Variance (exists for $c>2$):\n",
    "  $$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$$\n",
    "\n",
    "### Skewness and kurtosis\n",
    "\n",
    "- Skewness exists for $c>3$\n",
    "- Excess kurtosis exists for $c>4$\n",
    "\n",
    "You can compute them from raw moments $m_k = \\mathbb{E}[X^k]$ via standard formulas.\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "- The MGF $M(t)=\\mathbb{E}[e^{tX}]$ **diverges for any $t>0$** because the tail is polynomial.\n",
    "- The characteristic function $\\varphi(t)=\\mathbb{E}[e^{itX}]$ exists for all real $t$, but does not have a simple elementary closed form (it can be expressed using special functions and evaluated numerically).\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The (differential) entropy has a closed form:\n",
    "\n",
    "$$\n",
    "h(X) = -\\log(cd) - \\left(1 + \\frac{1}{c}\\right)\\left(\\psi(1) - \\psi(d)\\right) + 1 + \\frac{1}{d},\n",
    "$$\n",
    "\n",
    "where $\\psi$ is the digamma function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54de48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0, d0 = 3.5, 2.0\n",
    "x_test = np.array([0.2, 0.5, 1.0, 2.0, 5.0])\n",
    "\n",
    "pdf_np = burr_pdf(x_test, c0, d0)\n",
    "pdf_sp = stats.burr.pdf(x_test, c0, d0)\n",
    "print(\"max |pdf_numpy - pdf_scipy|:\", np.max(np.abs(pdf_np - pdf_sp)))\n",
    "\n",
    "mean, var, skew, exkurt = burr_summary_stats(c0, d0)\n",
    "mean_sp, var_sp, skew_sp, exkurt_sp = stats.burr.stats(c0, d0, moments=\"mvsk\")\n",
    "print(\"mean:\", mean, \"(scipy:\", float(mean_sp), \")\")\n",
    "print(\"var:\", var, \"(scipy:\", float(var_sp), \")\")\n",
    "print(\"skew:\", skew, \"(scipy:\", float(skew_sp), \")\")\n",
    "print(\"excess kurtosis:\", exkurt, \"(scipy:\", float(exkurt_sp), \")\")\n",
    "\n",
    "h = burr_entropy(c0, d0)\n",
    "print(\"entropy:\", h, \"(scipy:\", float(stats.burr.entropy(c0, d0)), \")\")\n",
    "\n",
    "print(\"\\nMoment existence demo (moment finite iff k < c):\")\n",
    "for k in [0.5, 1.0, 2.0, 3.0, 4.0]:\n",
    "    m = burr_raw_moment(k, c0, d0)\n",
    "    print(f\"E[X^{k}] = {m}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fa71a",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The parameters $c$ and $d$ both affect shape, but in different ways.\n",
    "\n",
    "### $c$ (tail index)\n",
    "\n",
    "- Controls the **right-tail exponent**: $\\Pr(X>x) \\sim d\\,x^{-c}$.\n",
    "- Larger $c$ means a **lighter tail** and **more finite moments**.\n",
    "  - mean exists for $c>1$\n",
    "  - variance exists for $c>2$\n",
    "\n",
    "### $d$ (body / lower-tail behavior)\n",
    "\n",
    "- Controls the **body** and behavior near 0.\n",
    "- As $x\\to 0^+$, $F(x) \\approx x^{c d}$ and $f(x) \\approx c d\\,x^{c d - 1}$.\n",
    "  - If $c d \\ge 1$, the PDF is finite at 0.\n",
    "  - If $c d < 1$, the PDF diverges at 0.\n",
    "\n",
    "A useful mental model comes from the transformation $T = X^{-c}$:\n",
    "\n",
    "- $T$ follows a Lomax distribution with shape $d$.\n",
    "- Then $X = T^{-1/c}$ stretches/compresses that Lomax behavior on a log scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.logspace(-3, 3, 800)\n",
    "\n",
    "# Effect of changing c (tail index)\n",
    "d_fixed = 2.0\n",
    "c_list = [1.2, 2.0, 5.0]\n",
    "\n",
    "fig_pdf_c = go.Figure()\n",
    "for c in c_list:\n",
    "    y = np.maximum(burr_pdf(x, c, d_fixed), 1e-300)\n",
    "    fig_pdf_c.add_trace(\n",
    "        go.Scatter(x=x, y=y, mode=\"lines\", name=f\"c={c}, d={d_fixed}\")\n",
    "    )\n",
    "\n",
    "fig_pdf_c.update_layout(title=\"PDF shape when varying c (d fixed)\")\n",
    "fig_pdf_c.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_pdf_c.update_yaxes(type=\"log\", title=\"pdf(x)\")\n",
    "fig_pdf_c.show()\n",
    "\n",
    "# Effect of changing d (body / lower-tail behavior)\n",
    "c_fixed = 3.0\n",
    "d_list = [0.5, 1.0, 3.0]\n",
    "\n",
    "fig_pdf_d = go.Figure()\n",
    "for d in d_list:\n",
    "    y = np.maximum(burr_pdf(x, c_fixed, d), 1e-300)\n",
    "    fig_pdf_d.add_trace(\n",
    "        go.Scatter(x=x, y=y, mode=\"lines\", name=f\"c={c_fixed}, d={d}\")\n",
    "    )\n",
    "\n",
    "fig_pdf_d.update_layout(title=\"PDF shape when varying d (c fixed)\")\n",
    "fig_pdf_d.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_pdf_d.update_yaxes(type=\"log\", title=\"pdf(x)\")\n",
    "fig_pdf_d.show()\n",
    "\n",
    "# CDF view (often easier to interpret)\n",
    "fig_cdf = go.Figure()\n",
    "for c in c_list:\n",
    "    fig_cdf.add_trace(\n",
    "        go.Scatter(x=x, y=burr_cdf(x, c, d_fixed), mode=\"lines\", name=f\"c={c}, d={d_fixed}\")\n",
    "    )\n",
    "fig_cdf.update_layout(title=\"CDF when varying c (d fixed)\")\n",
    "fig_cdf.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_cdf.update_yaxes(title=\"cdf(x)\")\n",
    "fig_cdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9eecd",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "A clean way to derive many results is to transform $X$ into a Lomax random variable.\n",
    "\n",
    "### Step 1: show $T = X^{-c}$ is Lomax\n",
    "\n",
    "Let $T = X^{-c}$. For $t>0$:\n",
    "\n",
    "$$\n",
    "\\Pr(T \\le t)\n",
    "= \\Pr(X^{-c} \\le t)\n",
    "= \\Pr\\left(X \\ge t^{-1/c}\\right)\n",
    "= 1 - F\\left(t^{-1/c}\\right).\n",
    "$$\n",
    "\n",
    "But\n",
    "\n",
    "$$\n",
    "F\\left(t^{-1/c}\\right)\n",
    "= \\left(1 + (t^{-1/c})^{-c}\\right)^{-d}\n",
    "= (1 + t)^{-d}.\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\Pr(T \\le t) = 1 - (1+t)^{-d},\n",
    "$$\n",
    "\n",
    "which is exactly the Lomax/Pareto-II CDF with shape $d$ and unit scale.\n",
    "\n",
    "### Step 2: derive $\\mathbb{E}[X^k]$\n",
    "\n",
    "Since $X = T^{-1/c}$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k] = \\mathbb{E}[T^{-k/c}].\n",
    "$$\n",
    "\n",
    "With Lomax density $g(t) = d (1+t)^{-d-1}$ for $t>0$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k]\n",
    "= d\\int_0^\\infty t^{-k/c}(1+t)^{-d-1}\\,dt.\n",
    "$$\n",
    "\n",
    "This is a Beta-function integral (valid when $-1 < -k/c$, i.e. $k<c$):\n",
    "\n",
    "$$\n",
    "\\int_0^\\infty t^{a}(1+t)^{-d-1}\\,dt = \\mathrm{B}(a+1, d-a)\n",
    "$$\n",
    "\n",
    "with $a = -k/c$. Substituting and simplifying yields\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k]\n",
    "= \\frac{\\Gamma\\left(1 - \\tfrac{k}{c}\\right)\\,\\Gamma\\left(d + \\tfrac{k}{c}\\right)}{\\Gamma(d)},\\qquad k<c.\n",
    "$$\n",
    "\n",
    "### Variance\n",
    "\n",
    "Plug $k=1$ and $k=2$ into the raw-moment formula and use\n",
    "\n",
    "$$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$$\n",
    "\n",
    "(finite only when $c>2$).\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Given i.i.d. data $x_1,\\dots,x_n$ (all $>0$) in the standard form, the log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ell(c,d) = \\sum_{i=1}^n \\log f(x_i;c,d)\n",
    "= n(\\log c + \\log d) - (c+1)\\sum_{i=1}^n \\log x_i - (d+1)\\sum_{i=1}^n \\log(1 + x_i^{-c}).\n",
    "$$\n",
    "\n",
    "There is no closed-form MLE for $(c,d)$ in general; numerical optimization is used in practice (see `scipy.stats.burr.fit`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burr_loglik(c, d, x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if (c <= 0) or (d <= 0) or np.any(x <= 0):\n",
    "        return -np.inf\n",
    "    return float(np.sum(burr_logpdf(x, c, d)))\n",
    "\n",
    "\n",
    "# quick sanity check: log-likelihood is higher near the true parameters (on average)\n",
    "c_true, d_true = 3.0, 2.0\n",
    "x_data = burr_rvs_numpy(c_true, d_true, size=2000, rng=rng)\n",
    "\n",
    "for (c_try, d_try) in [(2.0, 2.0), (3.0, 2.0), (4.0, 2.0), (3.0, 1.0), (3.0, 3.0)]:\n",
    "    print((c_try, d_try), burr_loglik(c_try, d_try, x_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74413f2",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "The standard Burr III CDF is\n",
    "\n",
    "$$F(x) = (1 + x^{-c})^{-d}.$$\n",
    "\n",
    "Using inverse-transform sampling:\n",
    "\n",
    "1. Draw $U \\sim \\mathrm{Uniform}(0,1)$\n",
    "2. Set $U = (1 + X^{-c})^{-d}$\n",
    "3. Solve:\n",
    "   $$U^{-1/d} = 1 + X^{-c} \\Rightarrow X = (U^{-1/d} - 1)^{-1/c}$$\n",
    "\n",
    "This gives a fast sampler without any rejection steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samp, d_samp = 3.0, 2.0\n",
    "samples = burr_rvs_numpy(c_samp, d_samp, size=50_000, rng=rng)\n",
    "\n",
    "qs = np.array([0.1, 0.5, 0.9, 0.99])\n",
    "q_emp = np.quantile(samples, qs)\n",
    "q_theory = burr_ppf(qs, c_samp, d_samp)\n",
    "\n",
    "print(\"Quantiles p:\", qs)\n",
    "print(\"Empirical:\", q_emp)\n",
    "print(\"Theory:\", q_theory)\n",
    "\n",
    "print(\"\\nSample mean/var (finite here since c=3>2):\")\n",
    "print(\"mean:\", samples.mean(), \"(theory:\", burr_raw_moment(1, c_samp, d_samp), \")\")\n",
    "print(\"var:\", samples.var(), \"(theory:\", burr_raw_moment(2, c_samp, d_samp) - burr_raw_moment(1, c_samp, d_samp) ** 2, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa21285",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "\n",
    "- the theoretical PDF and CDF\n",
    "- a Monte Carlo sample (histogram + empirical CDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec74f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vis, d_vis = 3.0, 2.0\n",
    "x_grid = np.logspace(-3, 3, 800)\n",
    "\n",
    "# PDF\n",
    "fig_pdf = go.Figure()\n",
    "fig_pdf.add_trace(\n",
    "    go.Scatter(x=x_grid, y=np.maximum(burr_pdf(x_grid, c_vis, d_vis), 1e-300), mode=\"lines\", name=\"pdf\")\n",
    ")\n",
    "fig_pdf.update_layout(title=f\"Burr III PDF (c={c_vis}, d={d_vis})\")\n",
    "fig_pdf.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_pdf.update_yaxes(type=\"log\", title=\"pdf(x)\")\n",
    "fig_pdf.show()\n",
    "\n",
    "# CDF\n",
    "fig_cdf2 = go.Figure()\n",
    "fig_cdf2.add_trace(go.Scatter(x=x_grid, y=burr_cdf(x_grid, c_vis, d_vis), mode=\"lines\", name=\"cdf\"))\n",
    "fig_cdf2.update_layout(title=f\"Burr III CDF (c={c_vis}, d={d_vis})\")\n",
    "fig_cdf2.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_cdf2.update_yaxes(title=\"cdf(x)\")\n",
    "fig_cdf2.show()\n",
    "\n",
    "# Monte Carlo samples: histogram + PDF overlay\n",
    "samples_vis = burr_rvs_numpy(c_vis, d_vis, size=30_000, rng=rng)\n",
    "fig_hist = px.histogram(\n",
    "    samples_vis,\n",
    "    nbins=80,\n",
    "    histnorm=\"probability density\",\n",
    "    log_x=True,\n",
    "    opacity=0.55,\n",
    "    title=f\"Monte Carlo histogram vs PDF (c={c_vis}, d={d_vis})\",\n",
    ")\n",
    "fig_hist.add_trace(\n",
    "    go.Scatter(x=x_grid, y=burr_pdf(x_grid, c_vis, d_vis), mode=\"lines\", name=\"pdf\")\n",
    ")\n",
    "fig_hist.update_xaxes(title=\"x\")\n",
    "fig_hist.update_yaxes(title=\"density\")\n",
    "fig_hist.show()\n",
    "\n",
    "# Empirical CDF vs theoretical CDF\n",
    "x_sorted = np.sort(samples_vis)\n",
    "ecdf = np.arange(1, len(x_sorted) + 1) / len(x_sorted)\n",
    "\n",
    "fig_ecdf = go.Figure()\n",
    "fig_ecdf.add_trace(go.Scatter(x=x_sorted, y=ecdf, mode=\"lines\", name=\"empirical CDF\"))\n",
    "fig_ecdf.add_trace(go.Scatter(x=x_grid, y=burr_cdf(x_grid, c_vis, d_vis), mode=\"lines\", name=\"theoretical CDF\"))\n",
    "fig_ecdf.update_layout(title=\"Empirical vs theoretical CDF\")\n",
    "fig_ecdf.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_ecdf.update_yaxes(title=\"CDF\")\n",
    "fig_ecdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e0c7e",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.burr`)\n",
    "\n",
    "SciPy provides a ready-to-use implementation:\n",
    "\n",
    "- `stats.burr.pdf(x, c, d, loc=0, scale=1)`\n",
    "- `stats.burr.cdf(x, c, d, loc=0, scale=1)`\n",
    "- `stats.burr.rvs(c, d, loc=0, scale=1, size=..., random_state=...)`\n",
    "- `stats.burr.fit(data, ...)` (MLE)\n",
    "\n",
    "Reminder: SciPy’s `burr` is **Burr Type III / Dagum**. SciPy’s `burr12` is **Burr Type XII**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_true, d_true = 3.0, 2.0\n",
    "\n",
    "dist = stats.burr(c_true, d_true)  # loc=0, scale=1 by default\n",
    "x_eval = np.array([0.5, 1.0, 2.0, 5.0])\n",
    "print(\"pdf:\", dist.pdf(x_eval))\n",
    "print(\"cdf:\", dist.cdf(x_eval))\n",
    "\n",
    "# rvs\n",
    "data = dist.rvs(size=3000, random_state=rng)\n",
    "print(\"sample min/max:\", data.min(), data.max())\n",
    "\n",
    "# fit (fix loc=0, scale=1 to estimate only c and d)\n",
    "c_hat, d_hat, loc_hat, scale_hat = stats.burr.fit(data, floc=0, fscale=1)\n",
    "print(\"\\nTrue (c,d):\", (c_true, d_true))\n",
    "print(\"Fit  (c,d):\", (c_hat, d_hat))\n",
    "print(\"Returned loc/scale:\", (loc_hat, scale_hat))\n",
    "\n",
    "# Compare numpy vs SciPy implementations numerically\n",
    "x_dense = np.logspace(-3, 3, 1000)\n",
    "max_pdf_diff = np.max(np.abs(burr_pdf(x_dense, c_true, d_true) - dist.pdf(x_dense)))\n",
    "max_cdf_diff = np.max(np.abs(burr_cdf(x_dense, c_true, d_true) - dist.cdf(x_dense)))\n",
    "print(\"\\nmax |pdf_numpy - pdf_scipy|:\", max_pdf_diff)\n",
    "print(\"max |cdf_numpy - cdf_scipy|:\", max_cdf_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c101ae1",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing\n",
    "\n",
    "- **Nested model test**: the case $d=1$ is log-logistic. You can test $H_0: d=1$ vs $H_1: d \\ne 1$ using a likelihood-ratio test (LRT).\n",
    "- **Goodness-of-fit**: QQ-plots or distribution tests (KS/AD) can be used as diagnostics. Be careful: classical p-values assume parameters are known, while in practice they’re often estimated.\n",
    "\n",
    "### Bayesian modeling\n",
    "\n",
    "When modeling positive heavy-tailed data, you can use Burr III as a likelihood and place priors on $c$ and $d$ (e.g., log-normal or log-uniform priors). There is no conjugate prior, but posterior inference is straightforward with MCMC or even a simple grid approximation in low dimensions.\n",
    "\n",
    "### Generative modeling\n",
    "\n",
    "- Useful as a **base distribution** for positive heavy-tailed generative models.\n",
    "- Can be used in **mixtures** (mixture of Burrs) to model multimodal positive data.\n",
    "- Reciprocal relationship to Burr XII can be exploited depending on whether your domain is naturally modeled by $X$ or $1/X$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood-ratio test example: H0: d=1 (log-logistic) vs H1: d free\n",
    "\n",
    "c0, d0 = 2.5, 1.0\n",
    "x = stats.burr.rvs(c0, d0, size=1500, random_state=rng)\n",
    "\n",
    "# Fit under H1 (free c, d) and H0 (d fixed to 1). Fix loc=0, scale=1 for simplicity.\n",
    "c_hat1, d_hat1, _, _ = stats.burr.fit(x, floc=0, fscale=1)\n",
    "c_hat0, d_hat0, _, _ = stats.burr.fit(x, f1=1.0, floc=0, fscale=1)  # d fixed\n",
    "\n",
    "ll1 = np.sum(stats.burr.logpdf(x, c_hat1, d_hat1))\n",
    "ll0 = np.sum(stats.burr.logpdf(x, c_hat0, 1.0))\n",
    "\n",
    "lrt_stat = 2 * (ll1 - ll0)\n",
    "p_value = stats.chi2.sf(lrt_stat, df=1)\n",
    "\n",
    "print(\"True params:\", (c0, d0))\n",
    "print(\"Fit H1 (c,d):\", (c_hat1, d_hat1))\n",
    "print(\"Fit H0 (c,d=1):\", (c_hat0, 1.0))\n",
    "print(\"LRT stat:\", float(lrt_stat))\n",
    "print(\"Approx p-value (chi^2_1):\", float(p_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4629b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Bayesian grid posterior over (c,d) with a log-uniform prior p(c,d) ∝ 1/(c d)\n",
    "# This is an approximation for intuition (not a replacement for MCMC for serious work).\n",
    "\n",
    "c_true, d_true = 3.0, 2.0\n",
    "data = stats.burr.rvs(c_true, d_true, size=400, random_state=rng)\n",
    "logx = np.log(data)\n",
    "sum_logx = logx.sum()\n",
    "n = data.size\n",
    "\n",
    "c_grid = np.linspace(1.1, 6.0, 90)   # avoid c<=1 where mean diverges\n",
    "d_grid = np.linspace(0.2, 6.0, 90)\n",
    "\n",
    "log_post = np.empty((c_grid.size, d_grid.size), dtype=float)\n",
    "for i, c in enumerate(c_grid):\n",
    "    # sum_i log(1 + x_i^{-c}) computed stably\n",
    "    s = np.logaddexp(0.0, -c * logx).sum()\n",
    "\n",
    "    # log-likelihood for each d (vectorized)\n",
    "    loglike = n * np.log(c) + n * np.log(d_grid) + (-c - 1.0) * sum_logx - (d_grid + 1.0) * s\n",
    "\n",
    "    # log-uniform prior on (c,d) over the grid bounds\n",
    "    logprior = -np.log(c) - np.log(d_grid)\n",
    "    log_post[i, :] = loglike + logprior\n",
    "\n",
    "# Normalize on the discrete grid (treating cells as equal-area for visualization)\n",
    "log_post -= logsumexp(log_post)\n",
    "post = np.exp(log_post)\n",
    "\n",
    "i_map, j_map = np.unravel_index(np.argmax(post), post.shape)\n",
    "print(\"True (c,d):\", (c_true, d_true))\n",
    "print(\"MAP  (c,d):\", (float(c_grid[i_map]), float(d_grid[j_map])))\n",
    "\n",
    "fig_post = go.Figure(\n",
    "    data=go.Contour(\n",
    "        x=d_grid,\n",
    "        y=c_grid,\n",
    "        z=post,\n",
    "        contours_coloring=\"heatmap\",\n",
    "        colorbar_title=\"posterior\",\n",
    "    )\n",
    ")\n",
    "fig_post.update_layout(title=\"Grid posterior p(c,d | data) with log-uniform prior\")\n",
    "fig_post.update_xaxes(title=\"d\")\n",
    "fig_post.update_yaxes(title=\"c\")\n",
    "fig_post.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64be61",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: require $c>0$ and $d>0$. With location/scale, also require $\\mathrm{scale}>0$.\n",
    "- **Support mismatch**: data must satisfy $x>0$ in the standard form, and $x>\\mathrm{loc}$ with a location shift.\n",
    "- **Non-existent moments**: the $k$-th moment exists iff $k<c$.\n",
    "  - If $c\\le 1$, the mean diverges.\n",
    "  - If $c\\le 2$, the variance diverges.\n",
    "- **Near-zero behavior**: if $c d < 1$, the PDF diverges at 0 (this can be fine mathematically, but it affects numerics and interpretation).\n",
    "- **Numerical issues**: direct computation of $x^{-c}$ can overflow for tiny $x$. Prefer log-domain computations (as in `burr_logpdf`).\n",
    "- **Fitting**:\n",
    "  - MLE can be sensitive to starting points and to whether `loc`/`scale` are also being fit.\n",
    "  - Use `logpdf` for likelihood work to avoid underflow.\n",
    "- **Name confusion**: Burr has multiple “types”. SciPy `burr` is Type III (Dagum), while SciPy `burr12` is Type XII.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68461e88",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `burr` (Burr Type III / Dagum) is a flexible **positive**, **heavy-tailed** continuous distribution.\n",
    "- The CDF is $F(x)=(1+x^{-c})^{-d}$ and the PDF is $f(x)=cd\\,x^{-c-1}/(1+x^{-c})^{d+1}$.\n",
    "- Tail heaviness is controlled by **$c$**: moments exist iff $k<c$.\n",
    "- The transform $T=X^{-c}$ yields a Lomax distribution, making derivations and sampling straightforward.\n",
    "- Sampling is easy via inverse CDF: $X=(U^{-1/d}-1)^{-1/c}$.\n",
    "- SciPy integration is direct via `scipy.stats.burr` (and `burr12` for Burr Type XII).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}