{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450261",
   "metadata": {},
   "source": [
    "# Double Pareto Lognormal distribution (`dpareto_lognorm`)\n",
    "\n",
    "The **double Pareto lognormal (dPlN)** distribution is a *continuous* distribution on $(0,\\infty)$ that combines:\n",
    "\n",
    "- a **lognormal “body”** (multiplicative variability / proportional growth), and\n",
    "- **Pareto (power-law) tails** (rare but much more frequent extremes than lognormal)\n",
    "\n",
    "It’s a strong candidate for positive *size variables* that look roughly lognormal but show unusually large outliers (e.g., income/wealth, firm size, city size, file sizes, web traffic).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Classify the distribution and interpret its parameters $(\\mu,\\sigma,\\alpha,\\beta)$.\n",
    "- Write down the PDF and CDF (and implement stable `logpdf` / `logcdf`).\n",
    "- Derive moments and understand when they are finite.\n",
    "- Implement a fast **NumPy-only** sampler using a simple generative representation.\n",
    "- Use SciPy’s implementation (`scipy.stats.dpareto_lognorm`) for inference and fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07125a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import special\n",
    "from scipy.stats import dpareto_lognorm as dpln_dist\n",
    "from scipy.stats import lognorm, norm\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Record versions for reproducibility (useful when numerical details matter).\n",
    "VERSIONS = {\"numpy\": np.__version__, \"scipy\": scipy.__version__, \"plotly\": plotly.__version__}\n",
    "VERSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1eb83",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `dpareto_lognorm` (Double Pareto Lognormal; SciPy: `scipy.stats.dpareto_lognorm`)\n",
    "- **Type**: Continuous\n",
    "- **Support (standard form)**: $x \\in (0,\\infty)$\n",
    "- **Parameter space (standard form)**:\n",
    "  - $\\mu \\in \\mathbb{R}$\n",
    "  - $\\sigma > 0$\n",
    "  - $\\alpha > 0$\n",
    "  - $\\beta > 0$\n",
    "- **SciPy shape parameters**: `u=\\mu`, `s=\\sigma`, `a=\\alpha`, `b=\\beta`\n",
    "- **SciPy location/scale**: `loc \\in \\mathbb{R}`, `scale > 0` with\n",
    "  $$X = \\text{loc} + \\text{scale}\\,Y, \\qquad Y \\sim \\mathrm{dPlN}(\\mu,\\sigma,\\alpha,\\beta).$$\n",
    "\n",
    "Unless stated otherwise, this notebook uses the **standard form** (`loc=0`, `scale=1`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef940cae",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "Many real-world positive quantities are well-modeled by a **lognormal** in the middle (because they arise from *multiplicative* effects), but have **heavier tails** than a lognormal:\n",
    "\n",
    "- *too many very large observations* (e.g., “winner-take-most” phenomena)\n",
    "- sometimes *extra mass near 0* (many very small values)\n",
    "\n",
    "The double Pareto lognormal addresses this by keeping a lognormal-like bulk while giving both tails **power-law behavior**.\n",
    "\n",
    "A particularly useful *generative representation* is:\n",
    "\n",
    "$$\n",
    "\\log X = Z + \\frac{E_1}{\\alpha} - \\frac{E_2}{\\beta},\n",
    "\\qquad\n",
    "Z \\sim \\mathcal{N}(\\mu,\\sigma^2),\\; E_1,E_2 \\sim \\mathrm{Exp}(1),\\; \\text{independent}.\n",
    "$$\n",
    "\n",
    "Equivalently (exponentiating),\n",
    "\n",
    "$$\n",
    "X = U\\,\\frac{V_1}{V_2},\n",
    "\\qquad\n",
    "\\log U \\sim \\mathcal{N}(\\mu,\\sigma^2),\\; V_1 \\sim \\mathrm{Pareto}(\\alpha),\\; V_2 \\sim \\mathrm{Pareto}(\\beta),\\; \\text{independent},\n",
    "$$\n",
    "\n",
    "where SciPy’s Pareto here corresponds to support $[1,\\infty)$ with tail $\\mathbb{P}(V>v)=v^{-\\alpha}$.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Income / wealth** distributions (lognormal-ish center with Pareto upper tail)\n",
    "- **Firm sizes**, **city sizes**, **file sizes**\n",
    "- **Web traffic**, **sales volumes**, and other “size” variables with many small values and occasional extremes\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Lognormal**: the “body” looks lognormal when tails are not too heavy.\n",
    "- **Pareto**: the far right tail behaves like a Pareto with index $\\alpha$.\n",
    "- **Laplace on log-scale**: the term $E_1/\\alpha - E_2/\\beta$ is an *asymmetric Laplace* random variable, so $\\log X$ is Normal + (asymmetric) Laplace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e0e49",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $\\phi$ and $\\Phi$ denote the standard normal PDF and CDF:\n",
    "\n",
    "$$\n",
    "\\phi(z) = \f",
    "rac{1}{\\sqrt{2\\pi}}\\exp\\left(-\tfrac{1}{2}z^2\r",
    "ight),\n",
    "\\qquad\n",
    "\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t)\\,dt.\n",
    "$$\n",
    "\n",
    "Define\n",
    "\n",
    "$$\n",
    "z = \f",
    "rac{\\log x - \\mu}{\\sigma},\n",
    "\\qquad\n",
    "y_1 = \u0007lpha\\sigma - z,\n",
    "\\qquad\n",
    "y_2 = \beta\\sigma + z,\n",
    "$$\n",
    "\n",
    "and the (normal) Mills ratio\n",
    "\n",
    "$$\n",
    "R(t) = \f",
    "rac{1-\\Phi(t)}{\\phi(t)} = \f",
    "rac{\\Phi(-t)}{\\phi(t)}.\n",
    "$$\n",
    "\n",
    "### PDF\n",
    "\n",
    "For $x>0$,\n",
    "\n",
    "$$\n",
    "f(x;\\mu,\\sigma,\u0007lpha,\beta)\n",
    "= \f",
    "rac{\u0007lpha\beta}{(\u0007lpha+\beta)\\,x}\n",
    "\\;\\phi(z)\\;\bigl(R(y_1) + R(y_2)\bigr).\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "For $x>0$,\n",
    "\n",
    "$$\n",
    "F(x;\\mu,\\sigma,\u0007lpha,\beta)\n",
    "= \\Phi(z) + \f",
    "rac{\\phi(z)}{\u0007lpha+\beta}\\Bigl(\u0007lpha\\,R(y_2) - \beta\\,R(y_1)\\Bigr),\n",
    "$$\n",
    "\n",
    "and $F(x)=0$ for $x\\le 0$ in the standard form.\n",
    "\n",
    "### Quantile function (PPF)\n",
    "\n",
    "There is no simple closed form for the PPF; SciPy computes it numerically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SQRT_2PI = 0.5 * np.log(2.0 * np.pi)\n",
    "\n",
    "\n",
    "def log_phi(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Log of standard normal PDF.\"\"\"\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return -0.5 * z**2 - LOG_SQRT_2PI\n",
    "\n",
    "\n",
    "def log_Phi(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Log of standard normal CDF (stable).\"\"\"\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return special.log_ndtr(z)\n",
    "\n",
    "\n",
    "def log_Phic(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Log of standard normal survival function 1-Φ(z) = Φ(-z) (stable).\"\"\"\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return special.log_ndtr(-z)\n",
    "\n",
    "\n",
    "def log_R(t: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Log Mills ratio R(t) = (1-Φ(t))/φ(t), computed stably.\"\"\"\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    return log_Phic(t) - log_phi(t)\n",
    "\n",
    "\n",
    "def log1mexp(a: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute log(1 - exp(a)) for a <= 0 in a numerically stable way.\"\"\"\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    if np.any(a > 0):\n",
    "        raise ValueError(\"log1mexp is only defined for a <= 0\")\n",
    "\n",
    "    out = np.empty_like(a)\n",
    "    cutoff = -np.log(2.0)\n",
    "    mask = a < cutoff\n",
    "    out[mask] = np.log1p(-np.exp(a[mask]))\n",
    "    out[~mask] = np.log(-np.expm1(a[~mask]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def dpln_logpdf(x: np.ndarray, mu: float, sigma: float, alpha: float, beta: float) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of the standard dPlN(μ,σ,α,β) distribution (loc=0, scale=1).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.full_like(x, fill_value=-np.inf, dtype=float)\n",
    "\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    z = (np.log(xm) - mu) / sigma\n",
    "    y1 = alpha * sigma - z\n",
    "    y2 = beta * sigma + z\n",
    "\n",
    "    out[mask] = (\n",
    "        np.log(alpha)\n",
    "        + np.log(beta)\n",
    "        - np.log(alpha + beta)\n",
    "        - np.log(xm)\n",
    "        + log_phi(z)\n",
    "        + np.logaddexp(log_R(y1), log_R(y2))\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def dpln_pdf(x: np.ndarray, mu: float, sigma: float, alpha: float, beta: float) -> np.ndarray:\n",
    "    \"\"\"PDF of the standard dPlN(μ,σ,α,β) distribution.\"\"\"\n",
    "    return np.exp(dpln_logpdf(x, mu, sigma, alpha, beta))\n",
    "\n",
    "\n",
    "def dpln_logcdf(x: np.ndarray, mu: float, sigma: float, alpha: float, beta: float) -> np.ndarray:\n",
    "    \"\"\"Log-CDF of the standard dPlN(μ,σ,α,β) distribution (stable).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.full_like(x, fill_value=-np.inf, dtype=float)\n",
    "\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    z = (np.log(xm) - mu) / sigma\n",
    "    y1 = alpha * sigma - z\n",
    "    y2 = beta * sigma + z\n",
    "\n",
    "    t1 = log_Phi(z)\n",
    "    t2 = log_phi(z)\n",
    "    t3 = np.log(beta) + log_R(y1)\n",
    "    t4 = np.log(alpha) + log_R(y2)\n",
    "\n",
    "    # log(beta*R(y1) - alpha*R(y2)) with sign tracking.\n",
    "    t5, sign = special.logsumexp(\n",
    "        np.stack([t3, t4], axis=0),\n",
    "        b=np.stack([np.ones_like(t3), -np.ones_like(t3)], axis=0),\n",
    "        axis=0,\n",
    "        return_sign=True,\n",
    "    )\n",
    "\n",
    "    temp = np.stack([t1, t2 + t5 - np.log(alpha + beta)], axis=0)\n",
    "    out[mask] = special.logsumexp(\n",
    "        temp,\n",
    "        b=np.stack([np.ones_like(t1), -sign], axis=0),\n",
    "        axis=0,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def dpln_cdf(x: np.ndarray, mu: float, sigma: float, alpha: float, beta: float) -> np.ndarray:\n",
    "    \"\"\"CDF of the standard dPlN(μ,σ,α,β) distribution.\"\"\"\n",
    "    return np.exp(dpln_logcdf(x, mu, sigma, alpha, beta))\n",
    "\n",
    "\n",
    "def dpln_logsf(x: np.ndarray, mu: float, sigma: float, alpha: float, beta: float) -> np.ndarray:\n",
    "    \"\"\"Log survival function log(1-F(x)), computed stably via log1mexp.\"\"\"\n",
    "    return log1mexp(dpln_logcdf(x, mu, sigma, alpha, beta))\n",
    "\n",
    "\n",
    "def dpln_sf(x: np.ndarray, mu: float, sigma: float, alpha: float, beta: float) -> np.ndarray:\n",
    "    \"\"\"Survival function 1-F(x).\"\"\"\n",
    "    return np.exp(dpln_logsf(x, mu, sigma, alpha, beta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: our formulas match SciPy.\n",
    "mu, sigma, alpha, beta = 0.2, 0.7, 2.5, 1.2\n",
    "x = np.logspace(-4, 4, 200)\n",
    "\n",
    "assert np.allclose(dpln_pdf(x, mu, sigma, alpha, beta), dpln_dist.pdf(x, mu, sigma, alpha, beta))\n",
    "assert np.allclose(dpln_cdf(x, mu, sigma, alpha, beta), dpln_dist.cdf(x, mu, sigma, alpha, beta))\n",
    "assert np.allclose(dpln_logpdf(x, mu, sigma, alpha, beta), dpln_dist.logpdf(x, mu, sigma, alpha, beta))\n",
    "assert np.allclose(dpln_logcdf(x, mu, sigma, alpha, beta), dpln_dist.logcdf(x, mu, sigma, alpha, beta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07a948",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Tail behavior (why “double Pareto”)\n",
    "\n",
    "The distribution has **power-law tails**:\n",
    "\n",
    "- As $x\\to\\infty$, the right tail behaves like\n",
    "  $$\\mathbb{P}(X>x) \\sim C\\,x^{-\\alpha},$$\n",
    "  so the PDF decays like $x^{-\\alpha-1}$.\n",
    "- As $x\\to 0^+$, the left tail behaves like\n",
    "  $$\\mathbb{P}(X\\le x) \\sim C\\,x^{\\beta},$$\n",
    "  so the PDF behaves like $x^{\\beta-1}$ near 0.\n",
    "\n",
    "This is why $\\alpha$ controls **upper extremes**, while $\\beta$ controls how much mass sits near **very small** values.\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "\n",
    "All positive moments have a clean form. For $k<\\alpha$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k]\n",
    "= \\frac{\\alpha\\beta}{(\\alpha-k)(\\beta+k)}\\;\\exp\\left(k\\mu + \\tfrac{1}{2}k^2\\sigma^2\\right).\n",
    "$$\n",
    "\n",
    "Consequences:\n",
    "\n",
    "- Mean exists iff $\\alpha>1$.\n",
    "- Variance exists iff $\\alpha>2$.\n",
    "- Skewness exists iff $\\alpha>3$.\n",
    "- Kurtosis exists iff $\\alpha>4$.\n",
    "\n",
    "(Similarly, negative moments exist up to order $\\beta$.)\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "- The MGF $M_X(t)=\\mathbb{E}[e^{tX}]$ does **not** exist for any $t>0$ because the right tail is polynomial.\n",
    "- The Laplace transform exists for $t<0$.\n",
    "- The characteristic function $\\varphi_X(t)=\\mathbb{E}[e^{itX}]$ exists for all real $t$ (bounded integrand), but there is no simple elementary closed form.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy $h(X)=-\\mathbb{E}[\\log f(X)]$ is finite for typical parameters and is available via `scipy.stats.dpareto_lognorm.entropy` (computed numerically).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57648dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpln_raw_moment(k: float, mu: float, sigma: float, alpha: float, beta: float) -> float:\n",
    "    \"\"\"Raw moment E[X^k] for k < alpha (standard form).\"\"\"\n",
    "    if alpha <= k:\n",
    "        return np.nan\n",
    "    return (alpha * beta) / ((alpha - k) * (beta + k)) * np.exp(k * mu + 0.5 * (k * sigma) ** 2)\n",
    "\n",
    "\n",
    "def dpln_mvsk_from_raw(mu: float, sigma: float, alpha: float, beta: float):\n",
    "    \"\"\"Return (mean, var, skew, kurt_excess) when moments exist; else NaNs.\"\"\"\n",
    "    m1 = dpln_raw_moment(1, mu, sigma, alpha, beta)\n",
    "    m2 = dpln_raw_moment(2, mu, sigma, alpha, beta)\n",
    "    m3 = dpln_raw_moment(3, mu, sigma, alpha, beta)\n",
    "    m4 = dpln_raw_moment(4, mu, sigma, alpha, beta)\n",
    "\n",
    "    mean = m1\n",
    "    var = m2 - m1**2\n",
    "\n",
    "    # Central moments via raw moments.\n",
    "    mu3 = m3 - 3 * m2 * m1 + 2 * m1**3\n",
    "    mu4 = m4 - 4 * m3 * m1 + 6 * m2 * m1**2 - 3 * m1**4\n",
    "\n",
    "    skew = mu3 / (var ** 1.5)\n",
    "    kurt_excess = mu4 / (var**2) - 3.0\n",
    "\n",
    "    return mean, var, skew, kurt_excess\n",
    "\n",
    "\n",
    "mu, sigma, alpha, beta = 0.2, 0.6, 6.0, 2.0\n",
    "\n",
    "mean_f, var_f, skew_f, kurt_f = dpln_mvsk_from_raw(mu, sigma, alpha, beta)\n",
    "mean_s, var_s, skew_s, kurt_s = dpln_dist.stats(mu, sigma, alpha, beta, moments=\"mvsk\")\n",
    "entropy_s = dpln_dist.entropy(mu, sigma, alpha, beta)\n",
    "\n",
    "print(\"Moments from closed-form raw moments:\")\n",
    "print(\"  mean   =\", mean_f)\n",
    "print(\"  var    =\", var_f)\n",
    "print(\"  skew   =\", skew_f)\n",
    "print(\"  kurt   =\", kurt_f)\n",
    "\n",
    "print()\n",
    "print(\"Moments from SciPy:\")\n",
    "print(\"  mean   =\", mean_s)\n",
    "print(\"  var    =\", var_s)\n",
    "print(\"  skew   =\", skew_s)\n",
    "print(\"  kurt   =\", kurt_s)\n",
    "print(\"  entropy=\", entropy_s)\n",
    "\n",
    "# Monte Carlo check (should be close when moments are finite).\n",
    "N = 80_000\n",
    "x_mc = dpln_dist.rvs(mu, sigma, alpha, beta, size=N, random_state=rng)\n",
    "\n",
    "print()\n",
    "print(\"Monte Carlo (N=80k):\")\n",
    "print(\"  mean ≈\", x_mc.mean())\n",
    "print(\"  var  ≈\", x_mc.var())\n",
    "print(\"  entropy ≈\", -dpln_dist.logpdf(x_mc, mu, sigma, alpha, beta).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c8a12",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "- $\\mu$ (**log-location**) shifts the distribution multiplicatively: increasing $\\mu$ scales typical values by $e^{\\Delta\\mu}$.\n",
    "- $\\sigma$ (**log-scale**) increases spread in the lognormal “body” and also amplifies variability in both tails.\n",
    "- $\\alpha$ (**right-tail index**) controls how fast the upper tail decays:\n",
    "  - smaller $\\alpha$ → heavier right tail (more extremes)\n",
    "  - moments exist only up to order $<\\alpha$\n",
    "- $\\beta$ (**left-tail index**) controls behavior near 0:\n",
    "  - smaller $\\beta$ → more mass near 0 (and stronger divergence of the density as $x\\to 0^+$ if $\\beta<1$)\n",
    "\n",
    "Below we visualize how the PDF changes when varying one parameter at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a44e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sweeps: how the PDF changes.\n",
    "\n",
    "x = np.logspace(-3, 3, 600)\n",
    "\n",
    "base = dict(mu=0.0, sigma=0.6, alpha=3.0, beta=2.0)\n",
    "\n",
    "# 1) Vary alpha (right tail)\n",
    "fig_alpha = go.Figure()\n",
    "for a in [1.5, 3.0, 8.0]:\n",
    "    y = dpln_dist.pdf(x, base[\"mu\"], base[\"sigma\"], a, base[\"beta\"])\n",
    "    fig_alpha.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=f\"alpha={a}\"))\n",
    "fig_alpha.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_alpha.update_yaxes(type=\"log\", title=\"pdf(x)\")\n",
    "fig_alpha.update_layout(title=\"Effect of alpha: right-tail heaviness\")\n",
    "fig_alpha.show()\n",
    "\n",
    "# 2) Vary beta (left tail)\n",
    "fig_beta = go.Figure()\n",
    "for b in [0.5, 1.5, 4.0]:\n",
    "    y = dpln_dist.pdf(x, base[\"mu\"], base[\"sigma\"], base[\"alpha\"], b)\n",
    "    fig_beta.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=f\"beta={b}\"))\n",
    "fig_beta.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_beta.update_yaxes(type=\"log\", title=\"pdf(x)\")\n",
    "fig_beta.update_layout(title=\"Effect of beta: mass near 0\")\n",
    "fig_beta.show()\n",
    "\n",
    "# 3) Vary sigma (lognormal spread)\n",
    "fig_sigma = go.Figure()\n",
    "for s in [0.2, 0.6, 1.0]:\n",
    "    y = dpln_dist.pdf(x, base[\"mu\"], s, base[\"alpha\"], base[\"beta\"])\n",
    "    fig_sigma.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=f\"sigma={s}\"))\n",
    "fig_sigma.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig_sigma.update_yaxes(type=\"log\", title=\"pdf(x)\")\n",
    "fig_sigma.update_layout(title=\"Effect of sigma: lognormal-body spread\")\n",
    "fig_sigma.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833f791",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation (general moment)\n",
    "\n",
    "Using the generative form\n",
    "\n",
    "$$\n",
    "\\log X = Z + \\frac{E_1}{\\alpha} - \\frac{E_2}{\\beta},\n",
    "\\qquad Z\\perp E_1\\perp E_2,\n",
    "$$\n",
    "\n",
    "we get for any real $k$ (when the expectations exist)\n",
    "\n",
    "$$\n",
    "X^k = \\exp(kZ)\\,\\exp\\left(\\frac{kE_1}{\\alpha}\\right)\\,\\exp\\left(-\\frac{kE_2}{\\beta}\\right).\n",
    "$$\n",
    "\n",
    "Independence implies\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k] = \\mathbb{E}[e^{kZ}]\\,\\mathbb{E}\\left[e^{kE_1/\\alpha}\\right]\\,\\mathbb{E}\\left[e^{-kE_2/\\beta}\\right].\n",
    "$$\n",
    "\n",
    "Now:\n",
    "\n",
    "- If $Z\\sim\\mathcal{N}(\\mu,\\sigma^2)$ then $\\mathbb{E}[e^{kZ}] = \\exp\\left(k\\mu + \\tfrac{1}{2}k^2\\sigma^2\\right)$.\n",
    "- If $E\\sim\\mathrm{Exp}(1)$ then $\\mathbb{E}[e^{tE}] = \\frac{1}{1-t}$ for $t<1$.\n",
    "\n",
    "Therefore, for $k<\\alpha$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[e^{kE_1/\\alpha}\\right] = \\frac{1}{1-k/\\alpha} = \\frac{\\alpha}{\\alpha-k},\n",
    "\\qquad\n",
    "\\mathbb{E}\\left[e^{-kE_2/\\beta}\\right] = \\frac{1}{1+k/\\beta} = \\frac{\\beta}{\\beta+k},\n",
    "$$\n",
    "\n",
    "and multiplying gives\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k]\n",
    "= \\frac{\\alpha\\beta}{(\\alpha-k)(\\beta+k)}\\;\\exp\\left(k\\mu + \\tfrac{1}{2}k^2\\sigma^2\\right).\n",
    "$$\n",
    "\n",
    "Setting $k=1$ yields the mean (when $\\alpha>1$).\n",
    "\n",
    "### 6.2 Variance\n",
    "\n",
    "When $\\alpha>2$,\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2,\n",
    "$$\n",
    "\n",
    "using the moment formula with $k=1$ and $k=2$.\n",
    "\n",
    "### 6.3 Likelihood (i.i.d. sample)\n",
    "\n",
    "For i.i.d. data $x_1,\\ldots,x_n$ (all $>0$), the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(\\mu,\\sigma,\\alpha,\\beta\\mid x_{1:n}) = \\sum_{i=1}^n \\log f(x_i;\\mu,\\sigma,\\alpha,\\beta).\n",
    "$$\n",
    "\n",
    "A convenient stable form (using the definitions from Section 3) is\n",
    "\n",
    "$$\n",
    "\\log f(x_i)\n",
    "= \\log\\alpha + \\log\\beta - \\log(\\alpha+\\beta) - \\log x_i\n",
    "+ \\log\\phi(z_i)\n",
    "+ \\log\\bigl( R(y_{1,i}) + R(y_{2,i}) \\bigr),\n",
    "$$\n",
    "\n",
    "where the last term is best computed with a `logaddexp` (log-sum-exp) trick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e745450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpln_loglik(x: np.ndarray, mu: float, sigma: float, alpha: float, beta: float) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return float(dpln_logpdf(x, mu, sigma, alpha, beta).sum())\n",
    "\n",
    "\n",
    "# Quick check: likelihood prefers the true parameters on synthetic data.\n",
    "mu0, sigma0, alpha0, beta0 = 0.1, 0.7, 3.5, 2.0\n",
    "x = dpln_dist.rvs(mu0, sigma0, alpha0, beta0, size=2_000, random_state=rng)\n",
    "\n",
    "ll_true = dpln_loglik(x, mu0, sigma0, alpha0, beta0)\n",
    "ll_perturbed = dpln_loglik(x, mu0 + 0.3, sigma0 * 1.1, alpha0 * 0.8, beta0 * 1.3)\n",
    "\n",
    "print(\"log-likelihood at true params     =\", ll_true)\n",
    "print(\"log-likelihood at perturbed params=\", ll_perturbed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a114f",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only algorithm\n",
    "\n",
    "Using the representation\n",
    "\n",
    "$$\n",
    "\\log X = Z + \\frac{E_1}{\\alpha} - \\frac{E_2}{\\beta}\n",
    "$$\n",
    "\n",
    "with $Z\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and $E_1,E_2\\sim\\mathrm{Exp}(1)$ independent, sampling is straightforward:\n",
    "\n",
    "1. Draw $Z$ from a normal.\n",
    "2. Draw $E_1,E_2$ from independent exponentials.\n",
    "3. Return $X=\\exp\\left(Z + E_1/\\alpha - E_2/\\beta\\right)$.\n",
    "\n",
    "This is exactly the approach used internally by SciPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d54a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpln_rvs_numpy(\n",
    "    mu: float,\n",
    "    sigma: float,\n",
    "    alpha: float,\n",
    "    beta: float,\n",
    "    size: int | tuple[int, ...],\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"NumPy-only sampler for the standard dPlN(μ,σ,α,β) distribution.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    Z = rng.normal(mu, sigma, size=size)\n",
    "    E1 = rng.standard_exponential(size=size)\n",
    "    E2 = rng.standard_exponential(size=size)\n",
    "    return np.exp(Z + E1 / alpha - E2 / beta)\n",
    "\n",
    "\n",
    "mu, sigma, alpha, beta = 0.0, 0.6, 3.0, 2.0\n",
    "\n",
    "x_np = dpln_rvs_numpy(mu, sigma, alpha, beta, size=200_000, rng=rng)\n",
    "x_sp = dpln_dist.rvs(mu, sigma, alpha, beta, size=200_000, random_state=rng)\n",
    "\n",
    "print(\"NumPy sampler vs SciPy sampler (same RNG stream not expected to match):\")\n",
    "print(\"  mean NumPy:\", x_np.mean())\n",
    "print(\"  mean SciPy:\", x_sp.mean())\n",
    "print(\"  median NumPy:\", np.median(x_np))\n",
    "print(\"  median SciPy:\", np.median(x_sp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6508440",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll look at:\n",
    "\n",
    "- the **PDF** (and Monte Carlo histogram)\n",
    "- the **CDF** (and an empirical CDF)\n",
    "- the **tail** on a log–log plot to reveal the Pareto slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, alpha, beta = 0.0, 0.6, 3.0, 2.0\n",
    "\n",
    "samples = dpln_rvs_numpy(mu, sigma, alpha, beta, size=60_000, rng=rng)\n",
    "\n",
    "# Focus on the body up to a high quantile; tails are shown separately.\n",
    "x_max = np.quantile(samples, 0.995)\n",
    "x_grid = np.logspace(-3, np.log10(x_max), 600)\n",
    "\n",
    "pdf_grid = dpln_dist.pdf(x_grid, mu, sigma, alpha, beta)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_histogram(\n",
    "    x=samples,\n",
    "    histnorm=\"probability density\",\n",
    "    nbinsx=140,\n",
    "    name=\"Monte Carlo\",\n",
    "    opacity=0.55,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_grid, y=pdf_grid, mode=\"lines\", name=\"PDF (theory)\", line=dict(width=3))\n",
    ")\n",
    "\n",
    "fig.update_xaxes(type=\"log\", title=\"x\", range=[np.log10(1e-3), np.log10(x_max)])\n",
    "fig.update_yaxes(title=\"density\")\n",
    "fig.update_layout(title=\"dpareto_lognorm: PDF + Monte Carlo histogram\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF + empirical CDF\n",
    "\n",
    "x_grid = np.logspace(-3, 3, 700)\n",
    "cdf_grid = dpln_dist.cdf(x_grid, mu, sigma, alpha, beta)\n",
    "\n",
    "xs = np.sort(samples)\n",
    "ecdf = np.arange(1, xs.size + 1) / xs.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=cdf_grid, mode=\"lines\", name=\"CDF (theory)\", line=dict(width=3)))\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=xs, y=ecdf, mode=\"lines\", name=\"empirical CDF\", line=dict(width=2, dash=\"dot\"))\n",
    ")\n",
    "\n",
    "fig.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig.update_yaxes(title=\"F(x)\")\n",
    "fig.update_layout(title=\"dpareto_lognorm: CDF vs empirical CDF\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail plot: survival function on log-log scale\n",
    "\n",
    "x_tail = np.logspace(0, 4, 200)\n",
    "sf_tail = dpln_dist.sf(x_tail, mu, sigma, alpha, beta)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_tail, y=sf_tail, mode=\"lines\", name=\"SF(x)=P(X>x)\"))\n",
    "\n",
    "# Reference slope ~ x^{-alpha}\n",
    "C = sf_tail[0] * (x_tail[0] ** alpha)\n",
    "ref = C * x_tail ** (-alpha)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x_tail, y=ref, mode=\"lines\", name=f\"reference ~ x^(-{alpha})\", line=dict(dash=\"dash\"))\n",
    ")\n",
    "\n",
    "fig.update_xaxes(type=\"log\", title=\"x\")\n",
    "fig.update_yaxes(type=\"log\", title=\"P(X>x)\")\n",
    "fig.update_layout(title=\"Right tail looks Pareto on log-log axes\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66f66c",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.dpareto_lognorm` with the usual distribution API:\n",
    "\n",
    "- `pdf`, `logpdf`\n",
    "- `cdf`, `logcdf`\n",
    "- `sf`, `logsf`\n",
    "- `rvs`\n",
    "- `fit` (MLE)\n",
    "\n",
    "You can also *freeze* a distribution with specific parameters to get a reusable object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen distribution\n",
    "mu, sigma, alpha, beta = 0.1, 0.7, 3.5, 2.0\n",
    "\n",
    "dist = dpln_dist(mu, sigma, alpha, beta)\n",
    "\n",
    "x = np.logspace(-2, 3, 6)\n",
    "print(\"pdf:\", dist.pdf(x))\n",
    "print(\"cdf:\", dist.cdf(x))\n",
    "print(\"rvs:\", dist.rvs(size=5, random_state=rng))\n",
    "\n",
    "# Fitting example (MLE) on synthetic data.\n",
    "true_params = (0.1, 0.7, 3.5, 2.0)\n",
    "data = dpln_dist.rvs(*true_params, size=5_000, random_state=rng)\n",
    "\n",
    "# Fix loc=0 and scale=1 to estimate only (u,s,a,b) in the standard form.\n",
    "(u_hat, s_hat, a_hat, b_hat, loc_hat, scale_hat) = dpln_dist.fit(data, floc=0, fscale=1)\n",
    "\n",
    "print()\n",
    "print(\"True (u,s,a,b):\", true_params)\n",
    "print(\"Fit  (u,s,a,b):\", (u_hat, s_hat, a_hat, b_hat))\n",
    "print(\"(loc, scale) fixed:\", (loc_hat, scale_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c1db0",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing (goodness-of-fit)\n",
    "\n",
    "If parameters are **specified in advance** (not estimated on the same data), you can test whether data plausibly comes from a dPlN distribution using goodness-of-fit tests like Kolmogorov–Smirnov (KS).\n",
    "\n",
    "Caveat: if you estimate parameters from the data and then run KS on the same data, usual p-values are not exact (you’d want a corrected procedure or a bootstrap).\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "\n",
    "Because we can evaluate the likelihood $p(x\\mid\\mu,\\sigma,\\alpha,\\beta)$, we can place priors on parameters and do Bayesian inference:\n",
    "\n",
    "$$\n",
    "p(\\theta\\mid x_{1:n}) \\propto p(x_{1:n}\\mid \\theta)\\,p(\\theta),\n",
    "\\qquad \\theta=(\\mu,\\sigma,\\alpha,\\beta).\n",
    "$$\n",
    "\n",
    "Below we show a simple 1D example: infer $\\mu$ with $(\\sigma,\\alpha,\\beta)$ held fixed.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "\n",
    "In simulation or synthetic-data generation, dPlN is a convenient way to generate **lognormal-like** samples with **realistic heavy tails**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing example: KS test when parameters are known.\n",
    "from scipy.stats import kstest\n",
    "\n",
    "mu, sigma, alpha, beta = 0.0, 0.6, 3.0, 2.0\n",
    "x = dpln_dist.rvs(mu, sigma, alpha, beta, size=3_000, random_state=rng)\n",
    "\n",
    "D, p_value = kstest(x, dpln_dist(mu, sigma, alpha, beta).cdf)\n",
    "print(\"KS test against dPlN(u=0, s=0.6, a=3, b=2):\")\n",
    "print(\"  D      =\", D)\n",
    "print(\"  p-value=\", p_value)\n",
    "\n",
    "# Compare against a (mis-specified) lognormal with the same fitted parameters.\n",
    "shape_ln, loc_ln, scale_ln = lognorm.fit(x, floc=0)\n",
    "D_ln, p_ln = kstest(x, lognorm(shape_ln, loc_ln, scale_ln).cdf)\n",
    "print()\n",
    "print(\"KS test against fitted lognormal (p-value not exact due to fitting):\")\n",
    "print(\"  D      =\", D_ln)\n",
    "print(\"  p-value=\", p_ln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison via AIC: dPlN vs lognormal on the same data.\n",
    "\n",
    "# Fit dPlN (u,s,a,b) with loc/scale fixed.\n",
    "(u_hat, s_hat, a_hat, b_hat, _, _) = dpln_dist.fit(x, floc=0, fscale=1)\n",
    "ll_dpln = dpln_dist.logpdf(x, u_hat, s_hat, a_hat, b_hat).sum()\n",
    "\n",
    "# Fit lognormal (shape, scale) with loc fixed.\n",
    "(shape_ln, _, scale_ln) = lognorm.fit(x, floc=0)\n",
    "ll_lognorm = lognorm.logpdf(x, shape_ln, loc=0, scale=scale_ln).sum()\n",
    "\n",
    "k_dpln = 4  # u, s, a, b\n",
    "k_lognorm = 2  # shape (sigma), scale (exp(mu))\n",
    "\n",
    "AIC_dpln = 2 * k_dpln - 2 * ll_dpln\n",
    "AIC_lognorm = 2 * k_lognorm - 2 * ll_lognorm\n",
    "\n",
    "print(\"AIC (lower is better):\")\n",
    "print(\"  dPlN     =\", AIC_dpln)\n",
    "print(\"  lognormal=\", AIC_lognorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian example: posterior for mu with (sigma, alpha, beta) fixed.\n",
    "\n",
    "mu_true, sigma, alpha, beta = 0.0, 0.6, 3.0, 2.0\n",
    "x = dpln_dist.rvs(mu_true, sigma, alpha, beta, size=600, random_state=rng)\n",
    "\n",
    "mu_grid = np.linspace(-1.0, 1.0, 600)\n",
    "\n",
    "# Prior: mu ~ Normal(0, 0.7^2)\n",
    "mu0, tau = 0.0, 0.7\n",
    "log_prior = norm.logpdf(mu_grid, loc=mu0, scale=tau)\n",
    "\n",
    "# Vectorized log-likelihood over the grid.\n",
    "log_like = dpln_dist.logpdf(x[:, None], mu_grid[None, :], sigma, alpha, beta).sum(axis=0)\n",
    "\n",
    "log_post_unnorm = log_prior + log_like\n",
    "log_post = log_post_unnorm - special.logsumexp(log_post_unnorm)\n",
    "post = np.exp(log_post)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_grid, y=post, mode=\"lines\", name=\"posterior p(mu|x)\"))\n",
    "fig.add_vline(x=mu_true, line_dash=\"dash\", line_color=\"black\", annotation_text=\"true mu\")\n",
    "fig.update_xaxes(title=\"mu\")\n",
    "fig.update_yaxes(title=\"posterior density\")\n",
    "fig.update_layout(title=\"Bayesian inference for mu (sigma, alpha, beta fixed)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2022c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling: how alpha changes extremes (same mu,sigma,beta).\n",
    "\n",
    "mu, sigma, beta = 0.0, 0.6, 2.0\n",
    "\n",
    "alphas = [1.5, 3.0, 8.0]\n",
    "N = 50_000\n",
    "\n",
    "summary = []\n",
    "for a in alphas:\n",
    "    xs = dpln_dist.rvs(mu, sigma, a, beta, size=N, random_state=rng)\n",
    "    summary.append(\n",
    "        {\n",
    "            \"alpha\": a,\n",
    "            \"median\": float(np.median(xs)),\n",
    "            \"q95\": float(np.quantile(xs, 0.95)),\n",
    "            \"q99\": float(np.quantile(xs, 0.99)),\n",
    "            \"max\": float(xs.max()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b990a",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter constraints**: require $\\sigma>0$, $\\alpha>0$, $\\beta>0$. The support is $x>0$ in the standard form.\n",
    "- **Moment existence**: mean/variance/skewness/kurtosis require $\\alpha>1,2,3,4$ respectively; otherwise these summaries are infinite/undefined.\n",
    "- **Numerical stability**:\n",
    "  - tails can underflow in `pdf` / `cdf`; prefer `logpdf`, `logcdf`, `logsf`.\n",
    "  - computing $R(t)=(1-\\Phi(t))/\\phi(t)$ naively can overflow/underflow; use stable log-space computations.\n",
    "- **Fitting (MLE)**:\n",
    "  - heavy tails can make optimization sensitive; check diagnostics (Q–Q plots, tail plots).\n",
    "  - parameters can trade off (e.g., $\\sigma$ vs tail indices), so estimates can be noisy with limited data.\n",
    "- **KS test after fitting**: standard p-values are not exact when parameters are estimated from the same sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cf25b",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `dpareto_lognorm` is a **continuous** distribution on $(0,\\infty)$ with a **lognormal-like bulk** and **Pareto tails**.\n",
    "- The tail index $\\alpha$ controls upper extremes and determines which positive moments exist.\n",
    "- Moments have a clean closed form via the representation $\\log X = Z + E_1/\\alpha - E_2/\\beta$.\n",
    "- Sampling is easy with NumPy (normal + exponentials), and SciPy provides a full API including `fit`.\n",
    "\n",
    "### References\n",
    "\n",
    "- Reed, W. J., & Jorgensen, M. (2004). *The double Pareto-lognormal distribution — a new parametric model for size distributions.* Communications in Statistics — Theory and Methods.\n",
    "- Hajargasht, G., & Griffiths, W. E. (2013). *Pareto-lognormal distributions: Inequality, poverty, and estimation from grouped income data.* Economic Modelling.\n",
    "- SciPy documentation: `scipy.stats.dpareto_lognorm`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}