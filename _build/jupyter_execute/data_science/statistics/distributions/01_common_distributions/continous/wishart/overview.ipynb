{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c9b9ef",
   "metadata": {},
   "source": [
    "# Wishart distribution (`wishart`) — Random scatter / covariance matrices\n",
    "\n",
    "The **Wishart distribution** is a probability distribution over **symmetric positive-definite (SPD) matrices**.\n",
    "It is the multivariate generalization of the **chi-square distribution**, and it appears whenever you sum outer products of multivariate Gaussian vectors.\n",
    "\n",
    "A canonical generative story (integer degrees of freedom):\n",
    "- Draw \\(x_1,\\dots,x_\\nu \\stackrel{iid}{\\sim} \\mathcal{N}_p(0,\\Sigma)\\)\n",
    "- Form the scatter matrix \\(W = \\sum_{i=1}^\\nu x_i x_i^\\top = X^\\top X\\)\n",
    "\n",
    "Then \\(W \\sim \\mathrm{Wishart}_p(\\nu,\\Sigma)\\).\n",
    "\n",
    "## Learning goals\n",
    "- Classify the distribution and understand its support/parameter space.\n",
    "- Write the PDF and interpret the matrix-valued CDF.\n",
    "- Compute/derive mean and covariance of entries, the (matrix) MGF, and entropy.\n",
    "- Implement **NumPy-only** sampling (Bartlett decomposition).\n",
    "- Visualize scalar marginals and Monte Carlo behavior.\n",
    "- Use `scipy.stats.wishart` (and understand what it does *not* implement).\n",
    "\n",
    "## Prerequisites\n",
    "- Linear algebra: eigenvalues, determinants, trace, Cholesky\n",
    "- Multivariate normal distribution\n",
    "- Basic matrix calculus intuition (for the likelihood section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341391e",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import wishart\n",
    "from scipy.special import multigammaln, psi\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Quick/slow toggle (mirrors patterns used elsewhere in this repo)\n",
    "FAST_RUN = True\n",
    "N_SAMPLES = 25_000 if FAST_RUN else 250_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import plotly\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"SciPy:\", scipy.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"Seed:\", SEED)\n",
    "print(\"FAST_RUN:\", FAST_RUN, \"N_SAMPLES:\", N_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040a25c",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `wishart` (Wishart distribution)\n",
    "- **Type**: **Continuous** (matrix-valued distribution)\n",
    "- **Support**: the cone of \\(p\\times p\\) symmetric positive-definite matrices\n",
    "\n",
    "\\[\n",
    "\\mathcal{S}_p^{++} = \\left\\{W \\in \\mathbb{R}^{p\\times p} : W=W^\\top,\\; x^\\top W x > 0\\;\\;\\forall x\\neq 0\\right\\}.\n",
    "\\]\n",
    "\n",
    "- **Parameter space**:\n",
    "  - degrees of freedom: \\(\\nu > p-1\\)\n",
    "  - scale matrix: \\(\\Sigma \\in \\mathcal{S}_p^{++}\\)\n",
    "\n",
    "Notation:\n",
    "\\[\n",
    "W \\sim \\mathrm{Wishart}_p(\\nu, \\Sigma).\n",
    "\\]\n",
    "\n",
    "SciPy uses `scipy.stats.wishart(df=nu, scale=Sigma)`.\n",
    "\n",
    "> Some texts require \\(\\nu\\in\\mathbb{N}\\) (because of the Gaussian “sum of outer products” construction), but the PDF extends to real \\(\\nu>p-1\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498b37a",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### 2.1 What it models\n",
    "The Wishart distribution models a **random SPD matrix** that behaves like a noisy version of a target matrix \\(\\Sigma\\).\n",
    "The most important example is the **Gaussian scatter / covariance** setting:\n",
    "\n",
    "- If \\(x_1,\\dots,x_\\nu \\stackrel{iid}{\\sim} \\mathcal{N}_p(0,\\Sigma)\\), then\n",
    "  \\[\n",
    "  W = \\sum_{i=1}^{\\nu} x_i x_i^\\top \\sim \\mathrm{Wishart}_p(\\nu,\\Sigma).\n",
    "  \\]\n",
    "- The (unbiased) sample covariance is \\(S = \\frac{1}{\\nu}W\\), so Wishart governs the sampling variability of covariance matrices.\n",
    "\n",
    "### 2.2 Typical real-world use cases\n",
    "- **Multivariate statistics**: sampling distribution of covariance matrices, PCA/FA uncertainty.\n",
    "- **Hypothesis testing**: tests about \\(\\Sigma\\) (sphericity, equality of covariances, etc.).\n",
    "- **Bayesian modeling**: conjugate prior/posterior for the **precision matrix** of a multivariate normal.\n",
    "- **Random matrix theory / simulation**: generating random SPD matrices with controlled mean/scale.\n",
    "\n",
    "### 2.3 Relations to other distributions\n",
    "- **Chi-square**: for \\(p=1\\), Wishart reduces to a scaled chi-square distribution.\n",
    "- **Gamma**: \\(\\chi^2_\\nu\\) is a Gamma special case, so \\(p=1\\) Wishart is also (scaled) Gamma.\n",
    "- **Inverse-Wishart**: distribution of \\(W^{-1}\\); commonly used as a prior on covariance matrices.\n",
    "- **Matrix normal**: if \\(X\\) is matrix-normal, then \\(X^\\top X\\) has a Wishart form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322dce84",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(W\\in\\mathcal{S}_p^{++}\\), \\(\\nu>p-1\\), and \\(\\Sigma\\in\\mathcal{S}_p^{++}\\).\n",
    "\n",
    "### 3.1 Multivariate gamma function\n",
    "The multivariate gamma function is\n",
    "\\[\n",
    "\\Gamma_p(a) = \\pi^{p(p-1)/4}\\prod_{j=1}^{p}\\Gamma\\!\\left(a + \\frac{1-j}{2}\\right).\n",
    "\\]\n",
    "\n",
    "SciPy provides the stable log version as `scipy.special.multigammaln(a, p)`.\n",
    "\n",
    "### 3.2 PDF\n",
    "The Wishart PDF is\n",
    "\\[\n",
    " f(W\\mid \\nu,\\Sigma) =\n",
    " \\frac{ |W|^{(\\nu-p-1)/2}\\,\\exp\\!\\left(-\\tfrac{1}{2}\\,\\mathrm{tr}(\\Sigma^{-1}W)\\right)}\n",
    " {2^{\\nu p/2}\\,|\\Sigma|^{\\nu/2}\\,\\Gamma_p(\\nu/2)}\n",
    "\\quad\\text{for } W\\in\\mathcal{S}_p^{++}.\n",
    "\\]\n",
    "\n",
    "### 3.3 CDF (matrix-valued)\n",
    "A natural CDF uses the **Loewner (PSD) order** \\(\\preceq\\):\n",
    "\\[\n",
    "F(X) = \\mathbb{P}(W \\preceq X) = \\int_{0\\prec W\\preceq X} f(W)\\,dW.\n",
    "\\]\n",
    "\n",
    "- For \\(p>1\\), this CDF generally has **no simple closed form**; expressions involve special functions of a *matrix argument*.\n",
    "- In practice, you often work with **scalar functionals** (e.g., diagonal entries, trace, determinant) whose CDFs are easier.\n",
    "\n",
    "### 3.4 Scalar special case (\\(p=1\\))\n",
    "If \\(p=1\\), then \\(W\\) is a positive scalar and\n",
    "\\[\n",
    "W \\sim \\mathrm{Wishart}_1(\\nu, \\Sigma)\\quad\\Longleftrightarrow\\quad \\frac{W}{\\Sigma} \\sim \\chi^2_{\\nu}.\n",
    "\\]\n",
    "So\n",
    "\\[\n",
    "F(w) = \\mathbb{P}(W\\le w) = F_{\\chi^2_{\\nu}}\\!\\left(\\frac{w}{\\Sigma}\\right).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d809dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_square_matrix(A: np.ndarray) -> bool:\n",
    "    A = np.asarray(A)\n",
    "    return A.ndim == 2 and A.shape[0] == A.shape[1]\n",
    "\n",
    "\n",
    "def is_symmetric(A: np.ndarray, *, atol: float = 1e-10) -> bool:\n",
    "    A = np.asarray(A)\n",
    "    return is_square_matrix(A) and np.allclose(A, A.T, atol=atol)\n",
    "\n",
    "\n",
    "def cholesky_spd(A: np.ndarray) -> np.ndarray:\n",
    "    '''Return the Cholesky factor L of an SPD matrix A = L L^T (raises if not SPD).'''\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    if not is_symmetric(A):\n",
    "        raise ValueError(\"Matrix must be symmetric.\")\n",
    "    return np.linalg.cholesky(A)\n",
    "\n",
    "\n",
    "def validate_scale(scale: np.ndarray) -> np.ndarray:\n",
    "    scale = np.asarray(scale, dtype=float)\n",
    "    if not is_square_matrix(scale):\n",
    "        raise ValueError(f\"scale must be a square 2D matrix, got shape={scale.shape}\")\n",
    "    if not is_symmetric(scale):\n",
    "        raise ValueError(\"scale must be symmetric\")\n",
    "    _ = cholesky_spd(scale)  # raises if not SPD\n",
    "    return scale\n",
    "\n",
    "\n",
    "def validate_df(df: float, p: int) -> float:\n",
    "    df = float(df)\n",
    "    if not (df > p - 1):\n",
    "        raise ValueError(f\"df must be > p-1 (p={p}), got df={df}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def wishart_logpdf_numpy(W: np.ndarray, df: float, scale: np.ndarray) -> float:\n",
    "    '''Log-PDF of Wishart_p(df, scale) at a single SPD matrix W.'''\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "\n",
    "    W = np.asarray(W, dtype=float)\n",
    "    if W.shape != (p, p):\n",
    "        raise ValueError(f\"W must have shape {(p, p)}, got {W.shape}\")\n",
    "    if not is_symmetric(W):\n",
    "        raise ValueError(\"W must be symmetric\")\n",
    "\n",
    "    # W must be SPD for the density to be finite.\n",
    "    _ = cholesky_spd(W)\n",
    "\n",
    "    sign_w, logdet_w = np.linalg.slogdet(W)\n",
    "    sign_s, logdet_s = np.linalg.slogdet(scale)\n",
    "    if sign_w <= 0 or sign_s <= 0:\n",
    "        return -np.inf\n",
    "\n",
    "    trace_term = np.trace(np.linalg.solve(scale, W))\n",
    "\n",
    "    log_norm = (df * p / 2) * np.log(2.0) + (df / 2) * logdet_s + multigammaln(df / 2, p)\n",
    "    return 0.5 * (df - p - 1) * logdet_w - 0.5 * trace_term - log_norm\n",
    "\n",
    "\n",
    "def wishart_pdf_numpy(W: np.ndarray, df: float, scale: np.ndarray) -> float:\n",
    "    return float(np.exp(wishart_logpdf_numpy(W, df, scale)))\n",
    "\n",
    "\n",
    "def wishart_mean(df: float, scale: np.ndarray) -> np.ndarray:\n",
    "    scale = validate_scale(scale)\n",
    "    df = validate_df(df, scale.shape[0])\n",
    "    return df * scale\n",
    "\n",
    "\n",
    "def wishart_cov_entry(df: float, scale: np.ndarray, i: int, j: int, k: int, l: int) -> float:\n",
    "    '''Cov(W_ij, W_kl) for W ~ Wishart_p(df, scale).'''\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "    for idx in (i, j, k, l):\n",
    "        if not (0 <= idx < p):\n",
    "            raise IndexError(\"index out of bounds\")\n",
    "    return df * (scale[i, k] * scale[j, l] + scale[i, l] * scale[j, k])\n",
    "\n",
    "\n",
    "def wishart_var_matrix(df: float, scale: np.ndarray) -> np.ndarray:\n",
    "    '''Elementwise variances Var(W_ij).'''\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "\n",
    "    out = np.empty((p, p), dtype=float)\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            out[i, j] = wishart_cov_entry(df, scale, i, j, i, j)\n",
    "    return out\n",
    "\n",
    "\n",
    "def wishart_expected_logdet(df: float, scale: np.ndarray) -> float:\n",
    "    '''E[log |W|] for W ~ Wishart_p(df, scale).'''\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "\n",
    "    sign_s, logdet_s = np.linalg.slogdet(scale)\n",
    "    if sign_s <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    terms = psi((df + 1 - np.arange(1, p + 1)) / 2)\n",
    "    return float(logdet_s + p * np.log(2.0) + np.sum(terms))\n",
    "\n",
    "\n",
    "def wishart_entropy(df: float, scale: np.ndarray) -> float:\n",
    "    '''Differential entropy H(W) for W ~ Wishart_p(df, scale).'''\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "\n",
    "    _, logdet_s = np.linalg.slogdet(scale)\n",
    "    digamma_sum = float(np.sum(psi((df + 1 - np.arange(1, p + 1)) / 2)))\n",
    "\n",
    "    return float(\n",
    "        (df * p / 2)\n",
    "        + multigammaln(df / 2, p)\n",
    "        + ((p + 1) / 2) * logdet_s\n",
    "        + (p * (p + 1) / 2) * np.log(2.0)\n",
    "        - ((df - p - 1) / 2) * digamma_sum\n",
    "    )\n",
    "\n",
    "\n",
    "def wishart_mgf(T: np.ndarray, df: float, scale: np.ndarray) -> float:\n",
    "    '''Matrix MGF: M(T)=E[exp(tr(TW))].\n",
    "\n",
    "    Domain: requires I - 2 * scale^(1/2) * T * scale^(1/2) to be SPD.\n",
    "    '''\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "\n",
    "    T = np.asarray(T, dtype=float)\n",
    "    if T.shape != (p, p) or not is_symmetric(T):\n",
    "        raise ValueError(f\"T must be a symmetric {(p,p)} matrix\")\n",
    "\n",
    "    # Use a symmetric similarity transform for numerical stability.\n",
    "    L = np.linalg.cholesky(scale)\n",
    "    A = np.eye(p) - 2.0 * (L @ T @ L.T)\n",
    "    A = 0.5 * (A + A.T)\n",
    "\n",
    "    _ = cholesky_spd(A)  # domain check\n",
    "    sign, logdet = np.linalg.slogdet(A)\n",
    "    if sign <= 0:\n",
    "        return np.nan\n",
    "    return float(np.exp(-(df / 2) * logdet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e2ca7",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let \\(W \\sim \\mathrm{Wishart}_p(\\nu,\\Sigma)\\).\n",
    "\n",
    "### 4.1 Mean\n",
    "\\[\n",
    "\\mathbb{E}[W] = \\nu\\,\\Sigma.\n",
    "\\]\n",
    "\n",
    "### 4.2 Covariance structure (entrywise)\n",
    "For indices \\(i,j,k,\\ell\\in\\{1,\\dots,p\\}\\),\n",
    "\\[\n",
    "\\mathrm{Cov}(W_{ij}, W_{k\\ell}) = \\nu\\,\\big(\\Sigma_{ik}\\Sigma_{j\\ell} + \\Sigma_{i\\ell}\\Sigma_{jk}\\big).\n",
    "\\]\n",
    "In particular,\n",
    "\\[\n",
    "\\mathrm{Var}(W_{ij}) = \\nu\\,(\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2).\n",
    "\\]\n",
    "\n",
    "### 4.3 Useful scalar marginals\n",
    "Even though \\(W\\) is matrix-valued, some scalar pieces are simple:\n",
    "\n",
    "- **Diagonal entries**: for every \\(i\\),\n",
    "  \\[\n",
    "  \\frac{W_{ii}}{\\Sigma_{ii}} \\sim \\chi^2_{\\nu}.\n",
    "  \\]\n",
    "  So\n",
    "  - \\(\\mathbb{E}[W_{ii}] = \\nu\\Sigma_{ii}\\)\n",
    "  - \\(\\mathrm{Var}(W_{ii}) = 2\\nu\\Sigma_{ii}^2\\)\n",
    "  - skewness \\(=\\sqrt{8/\\nu}\\)\n",
    "  - excess kurtosis \\(=12/\\nu\\)\n",
    "\n",
    "- **Trace (special case)**: if \\(\\Sigma=I\\), then\n",
    "  \\[\n",
    "  \\mathrm{tr}(W) \\sim \\chi^2_{\\nu p}\n",
    "  \\]\n",
    "  because it is the sum of \\(\\nu p\\) independent standard-normal squares.\n",
    "\n",
    "### 4.4 MGF / characteristic function (matrix argument)\n",
    "For a symmetric matrix \\(T\\) such that \\(I-2T\\Sigma\\) is SPD,\n",
    "\\[\n",
    "M(T) = \\mathbb{E}[\\exp(\\mathrm{tr}(TW))] = |I-2T\\Sigma|^{-\\nu/2}.\n",
    "\\]\n",
    "The characteristic function is the same with \\(T\\mapsto iT\\).\n",
    "\n",
    "### 4.5 Mode\n",
    "If \\(\\nu \\ge p+1\\), the mode is\n",
    "\\[\n",
    "W_{\\text{mode}} = (\\nu-p-1)\\,\\Sigma.\n",
    "\\]\n",
    "\n",
    "### 4.6 Entropy\n",
    "A closed form exists in terms of the multivariate gamma and digamma functions. One convenient form is\n",
    "\\[\n",
    "\\mathbb{E}[\\log|W|] = \\log|\\Sigma| + p\\log 2 + \\sum_{i=1}^p \\psi\\!\\left(\\frac{\\nu+1-i}{2}\\right)\n",
    "\\]\n",
    "and\n",
    "\\[\n",
    "H(W) = \\frac{\\nu p}{2} + \\log\\Gamma_p\\!\\left(\\frac{\\nu}{2}\\right)\n",
    "+ \\frac{p+1}{2}\\log|\\Sigma| + \\frac{p(p+1)}{2}\\log 2\n",
    "-\\frac{\\nu-p-1}{2}\\sum_{i=1}^p \\psi\\!\\left(\\frac{\\nu+1-i}{2}\\right).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d280560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 8\n",
    "scale = np.array([\n",
    "    [1.5, 0.4],\n",
    "    [0.4, 1.0],\n",
    "])\n",
    "\n",
    "rv = wishart(df=df, scale=scale)\n",
    "W0 = rv.rvs(random_state=rng)\n",
    "\n",
    "print(\"Example W:\")\n",
    "print(np.round(W0, 4))\n",
    "print()\n",
    "print(\"Mean (theory):\")\n",
    "print(wishart_mean(df, scale))\n",
    "print(\"Mean (SciPy):\")\n",
    "print(rv.mean())\n",
    "print()\n",
    "print(\"Var elementwise (theory):\")\n",
    "print(np.round(wishart_var_matrix(df, scale), 6))\n",
    "print(\"Var elementwise (SciPy):\")\n",
    "print(np.round(rv.var(), 6))\n",
    "print()\n",
    "print(\"logpdf (ours):\", wishart_logpdf_numpy(W0, df, scale))\n",
    "print(\"logpdf (SciPy):\", rv.logpdf(W0))\n",
    "print(\"entropy (ours):\", wishart_entropy(df, scale))\n",
    "print(\"entropy (SciPy):\", rv.entropy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42ecfa",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### Degrees of freedom \\(\\nu\\)\n",
    "- Controls the **amount of information** in the scatter matrix.\n",
    "- In the Gaussian construction, \\(\\nu\\) is literally the number of vectors being summed (a sample size).\n",
    "- As \\(\\nu\\) increases, \\(W/\\nu\\) concentrates around \\(\\Sigma\\) with relative fluctuations on the order of \\(1/\\sqrt{\\nu}\\).\n",
    "\n",
    "### Scale matrix \\(\\Sigma\\)\n",
    "- Sets the **mean shape**: \\(\\mathbb{E}[W]=\\nu\\Sigma\\).\n",
    "- Encodes directions and scales via eigen-decomposition \\(\\Sigma = Q\\Lambda Q^\\top\\):\n",
    "  - the eigenvectors \\(Q\\) describe preferred directions\n",
    "  - the eigenvalues \\(\\Lambda\\) describe how spread those directions are on average\n",
    "\n",
    "### Shape changes (high level)\n",
    "- Increasing \\(\\nu\\): distribution tightens around \\(\\nu\\Sigma\\) (or around \\(\\Sigma\\) if you look at \\(W/\\nu\\)).\n",
    "- Increasing overall scale (e.g., \\(\\Sigma\\mapsto c\\Sigma\\)): multiplies \\(W\\) by \\(c\\).\n",
    "- Changing correlations in \\(\\Sigma\\): changes how strongly entries/eigenvalues of \\(W\\) move together.\n",
    "\n",
    "In Section 8 we’ll visualize these effects via scalar summaries (diagonal entries, eigenvalues, log-determinant).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2faf1c",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "These derivations use the Gaussian construction with integer \\(\\nu\\):\n",
    "\\(x_1,\\dots,x_\\nu \\stackrel{iid}{\\sim} \\mathcal{N}_p(0,\\Sigma)\\), \\(W=\\sum_i x_i x_i^\\top\\).\n",
    "The resulting formulas extend to real \\(\\nu>p-1\\).\n",
    "\n",
    "### 6.1 Expectation\n",
    "\\[\n",
    "\\mathbb{E}[W] = \\sum_{i=1}^{\\nu} \\mathbb{E}[x_i x_i^\\top] = \\nu\\,\\Sigma.\n",
    "\\]\n",
    "\n",
    "### 6.2 Variance / covariance of entries\n",
    "Write \\(W_{ij}=\\sum_{r=1}^{\\nu} x_{r,i}x_{r,j}\\). Using independence across \\(r\\) and Isserlis’ theorem for Gaussian fourth moments,\n",
    "\\[\n",
    "\\mathrm{Cov}(W_{ij}, W_{k\\ell})\n",
    "= \\nu\\,\\mathrm{Cov}(x_{1,i}x_{1,j}, x_{1,k}x_{1,\\ell})\n",
    "= \\nu\\,(\\Sigma_{ik}\\Sigma_{j\\ell} + \\Sigma_{i\\ell}\\Sigma_{jk}).\n",
    "\\]\n",
    "\n",
    "### 6.3 Likelihood (scale matrix \\(\\Sigma\\))\n",
    "Given one observation \\(W\\), the log-likelihood (up to constants in \\(W\\)) is\n",
    "\\[\n",
    "\\ell(\\Sigma;W) = -\\frac{\\nu}{2}\\log|\\Sigma| - \\frac{1}{2}\\mathrm{tr}(\\Sigma^{-1}W) + \\text{const}.\n",
    "\\]\n",
    "Differentiating w.r.t. \\(\\Sigma^{-1}\\) yields the MLE\n",
    "\\[\n",
    "\\widehat{\\Sigma}_{\\text{MLE}} = \\frac{1}{\\nu}W.\n",
    "\\]\n",
    "\n",
    "For \\(m\\) i.i.d. observations \\(W_1,\\dots,W_m\\) with common \\(\\nu\\),\n",
    "\\[\n",
    "\\widehat{\\Sigma}_{\\text{MLE}} = \\frac{1}{m\\nu}\\sum_{t=1}^m W_t.\n",
    "\\]\n",
    "\n",
    "Estimating \\(\\nu\\) jointly with \\(\\Sigma\\) requires solving a nonlinear equation (involving digamma functions); we’ll use a simple method-of-moments estimator as a practical alternative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wishart_fit_scale_mle(samples: np.ndarray, df: float) -> np.ndarray:\n",
    "    '''MLE of scale given df for i.i.d. Wishart draws.'''\n",
    "    samples = np.asarray(samples, dtype=float)\n",
    "    if samples.ndim != 3 or samples.shape[1] != samples.shape[2]:\n",
    "        raise ValueError(\"samples must have shape (n, p, p)\")\n",
    "    p = samples.shape[1]\n",
    "    df = validate_df(df, p)\n",
    "    return samples.mean(axis=0) / df\n",
    "\n",
    "\n",
    "def wishart_fit_df_mom(samples: np.ndarray) -> tuple[float, np.ndarray]:\n",
    "    '''Method-of-moments df estimate using diagonal entries.\n",
    "\n",
    "    Uses: Var(W_ii) / E[W_ii]^2 = 2/df  (since W_ii / Sigma_ii ~ chi^2_df).\n",
    "    '''\n",
    "    samples = np.asarray(samples, dtype=float)\n",
    "    if samples.ndim != 3 or samples.shape[1] != samples.shape[2]:\n",
    "        raise ValueError(\"samples must have shape (n, p, p)\")\n",
    "\n",
    "    diag = np.diagonal(samples, axis1=1, axis2=2)  # (n, p)\n",
    "    m = diag.mean(axis=0)\n",
    "    v = diag.var(axis=0, ddof=0)\n",
    "\n",
    "    if np.any(m <= 0):\n",
    "        raise ValueError(\"Diagonal means must be positive for MOM df estimator\")\n",
    "\n",
    "    df_hats = 2.0 / (v / (m**2))\n",
    "    df_hat = float(np.median(df_hats))\n",
    "    return df_hat, df_hats\n",
    "\n",
    "\n",
    "def wishart_fit_df_scale_mom(samples: np.ndarray) -> dict:\n",
    "    df_hat, df_hats = wishart_fit_df_mom(samples)\n",
    "    scale_hat = wishart_fit_scale_mle(samples, df_hat)\n",
    "    return {\"df\": df_hat, \"df_by_diag\": df_hats, \"scale\": scale_hat}\n",
    "\n",
    "\n",
    "# Demo: recover df and scale from synthetic samples\n",
    "p = 3\n",
    "scale_true = np.array([\n",
    "    [1.0, 0.4, 0.2],\n",
    "    [0.4, 1.7, 0.1],\n",
    "    [0.2, 0.1, 0.9],\n",
    "])\n",
    "df_true = 12\n",
    "\n",
    "rv_true = wishart(df=df_true, scale=scale_true)\n",
    "samples = rv_true.rvs(size=8_000 if FAST_RUN else 40_000, random_state=rng)\n",
    "\n",
    "fit = wishart_fit_df_scale_mom(samples)\n",
    "\n",
    "print(\"df true:\", df_true)\n",
    "print(\"df hat :\", round(fit[\"df\"], 3))\n",
    "print(\"df hats by diagonal:\", np.round(fit[\"df_by_diag\"], 3))\n",
    "print()\n",
    "print(\"scale true:\")\n",
    "print(np.round(scale_true, 3))\n",
    "print(\"scale hat :\")\n",
    "print(np.round(fit[\"scale\"], 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f2823",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "The most common efficient sampler is the **Bartlett decomposition**.\n",
    "\n",
    "### 7.1 Bartlett decomposition (idea)\n",
    "For \\(W \\sim \\mathrm{Wishart}_p(\\nu, I)\\):\n",
    "\n",
    "1. Build a lower-triangular matrix \\(A\\) such that\n",
    "   - diagonal: \\(A_{ii}^2 \\sim \\chi^2_{\\nu-i+1}\\)\n",
    "   - sub-diagonal: \\(A_{ij}\\sim \\mathcal{N}(0,1)\\) for \\(i>j\\)\n",
    "   - above diagonal: \\(0\\)\n",
    "2. Then\n",
    "   \\[\n",
    "   W = AA^\\top.\n",
    "   \\]\n",
    "\n",
    "For general scale \\(\\Sigma\\): if \\(\\Sigma = LL^\\top\\) (Cholesky), then\n",
    "\\[\n",
    "W = LAA^\\top L^\\top.\n",
    "\\]\n",
    "\n",
    "This works for any real \\(\\nu>p-1\\) because chi-square is defined for non-integer degrees of freedom.\n",
    "\n",
    "### 7.2 Direct Gaussian construction (also NumPy-only)\n",
    "If \\(\\nu\\in\\mathbb{N}\\), you can also sample \\(x_r\\sim \\mathcal{N}_p(0,\\Sigma)\\) and return \\(\\sum_r x_r x_r^\\top\\). It is conceptually simple but can be slower for large \\(\\nu\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wishart_rvs_bartlett(df: float, scale: np.ndarray, *, size: int = 1, rng=None) -> np.ndarray:\n",
    "    '''NumPy-only sampling via Bartlett decomposition.\n",
    "\n",
    "    Returns shape (p, p) if size=1, else (size, p, p).\n",
    "    '''\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "\n",
    "    L = np.linalg.cholesky(scale)\n",
    "\n",
    "    out = np.empty((size, p, p), dtype=float)\n",
    "    for s in range(size):\n",
    "        A = np.zeros((p, p), dtype=float)\n",
    "        for i in range(p):\n",
    "            A[i, i] = np.sqrt(rng.chisquare(df - i))\n",
    "            if i > 0:\n",
    "                A[i, :i] = rng.standard_normal(i)\n",
    "\n",
    "        LA = L @ A\n",
    "        W = LA @ LA.T\n",
    "        out[s] = W\n",
    "\n",
    "    return out[0] if size == 1 else out\n",
    "\n",
    "\n",
    "def wishart_rvs_normals(df: float, scale: np.ndarray, *, size: int = 1, rng=None) -> np.ndarray:\n",
    "    '''NumPy-only sampling via the Gaussian construction (requires integer df).'''\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    df_int = int(df)\n",
    "    if df_int != df:\n",
    "        raise ValueError(\"Gaussian construction requires integer df\")\n",
    "\n",
    "    scale = validate_scale(scale)\n",
    "    p = scale.shape[0]\n",
    "    df = validate_df(df, p)\n",
    "\n",
    "    L = np.linalg.cholesky(scale)\n",
    "\n",
    "    Z = rng.standard_normal(size=(size, df_int, p))\n",
    "    X = Z @ L.T  # each row ~ N(0, scale)\n",
    "\n",
    "    # W = X^T X for each sample\n",
    "    W = np.einsum(\"sni,snj->sij\", X, X)\n",
    "    return W[0] if size == 1 else W\n",
    "\n",
    "\n",
    "# Quick sanity check: sampler returns SPD\n",
    "scale_demo = np.array([\n",
    "    [1.5, 0.4],\n",
    "    [0.4, 1.0],\n",
    "])\n",
    "W_demo = wishart_rvs_bartlett(8, scale_demo, size=1, rng=rng)\n",
    "_ = np.linalg.cholesky(W_demo)  # should not raise\n",
    "W_demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca5239c",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "Because Wishart is **matrix-valued**, direct visualization of its full PDF/CDF is difficult for \\(p>2\\).\n",
    "A practical approach is to visualize:\n",
    "\n",
    "- **Scalar marginals** that have known forms (e.g., diagonal entries)\n",
    "- **Scalar summaries** (trace, determinant, eigenvalues)\n",
    "\n",
    "Below we show:\n",
    "1. A single Monte Carlo draw \\(W\\) (as a heatmap)\n",
    "2. PDF/CDF of a diagonal entry \\(W_{11}\\) (a scaled chi-square)\n",
    "3. Scatter of \\((W_{11}, W_{22})\\) to show dependence\n",
    "4. How \\(\\nu\\) changes concentration of \\(W/\\nu\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 8\n",
    "scale = np.array([\n",
    "    [1.5, 0.4],\n",
    "    [0.4, 1.0],\n",
    "])\n",
    "\n",
    "Ws = wishart_rvs_bartlett(df, scale, size=N_SAMPLES, rng=rng)\n",
    "\n",
    "W_one = Ws[0]\n",
    "fig = px.imshow(W_one, text_auto=\".3f\", title=\"One Wishart draw W (heatmap)\")\n",
    "fig.update_layout(coloraxis_showscale=True)\n",
    "fig.show()\n",
    "\n",
    "w11 = Ws[:, 0, 0]\n",
    "w22 = Ws[:, 1, 1]\n",
    "\n",
    "# Skewness/kurtosis of a diagonal element (should match chi-square)\n",
    "skew_theory = np.sqrt(8.0 / df)\n",
    "excess_kurt_theory = 12.0 / df\n",
    "\n",
    "skew_mc = stats.skew(w11, bias=False)\n",
    "excess_kurt_mc = stats.kurtosis(w11, fisher=True, bias=False)\n",
    "\n",
    "print(\"W_11 skewness theory:\", round(skew_theory, 4), \"MC:\", round(float(skew_mc), 4))\n",
    "print(\"W_11 excess kurtosis theory:\", round(excess_kurt_theory, 4), \"MC:\", round(float(excess_kurt_mc), 4))\n",
    "\n",
    "# --- PDF of W_11 ---\n",
    "x = np.linspace(np.percentile(w11, 0.5), np.percentile(w11, 99.5), 400)\n",
    "pdf_theory = stats.chi2.pdf(x / scale[0, 0], df=df) / scale[0, 0]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=w11, nbinsx=60, histnorm=\"probability density\", name=\"MC (W_11)\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=pdf_theory, mode=\"lines\", name=\"Theory (scaled chi-square)\"))\n",
    "fig.update_layout(\n",
    "    title=\"Wishart diagonal marginal: PDF of W_11\",\n",
    "    xaxis_title=\"w\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# --- CDF of W_11 ---\n",
    "xs = np.sort(w11)\n",
    "ecdf = np.arange(1, xs.size + 1) / xs.size\n",
    "cdf_theory = stats.chi2.cdf(xs / scale[0, 0], df=df)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=ecdf, mode=\"lines\", name=\"Empirical CDF\"))\n",
    "fig.add_trace(go.Scatter(x=xs, y=cdf_theory, mode=\"lines\", name=\"Theory CDF\"))\n",
    "fig.update_layout(\n",
    "    title=\"Wishart diagonal marginal: CDF of W_11\",\n",
    "    xaxis_title=\"w\",\n",
    "    yaxis_title=\"F(w)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# --- Dependence between diagonal entries ---\n",
    "fig = px.scatter(\n",
    "    x=w11,\n",
    "    y=w22,\n",
    "    opacity=0.25,\n",
    "    title=\"Monte Carlo samples: (W_11, W_22)\",\n",
    "    labels={\"x\": \"W_11\", \"y\": \"W_22\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of df on concentration of W/df around scale\n",
    "\n",
    "scale3 = np.array([\n",
    "    [1.0, 0.6, 0.2],\n",
    "    [0.6, 1.8, 0.3],\n",
    "    [0.2, 0.3, 0.7],\n",
    "])\n",
    "p = scale3.shape[0]\n",
    "\n",
    "dfs = [p + 0.5, 10, 40]\n",
    "ns = 5_000 if FAST_RUN else 25_000\n",
    "\n",
    "rows = []\n",
    "for df0 in dfs:\n",
    "    Ws0 = wishart_rvs_bartlett(df0, scale3, size=ns, rng=rng)\n",
    "    # Frobenius distance of W/df to the target scale\n",
    "    dist = np.linalg.norm(Ws0 / df0 - scale3, axis=(1, 2))\n",
    "    rows.append({\"df\": np.full(ns, df0), \"dist\": dist})\n",
    "\n",
    "df_col = np.concatenate([r[\"df\"] for r in rows])\n",
    "dist_col = np.concatenate([r[\"dist\"] for r in rows])\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=dist_col,\n",
    "    color=df_col.astype(str),\n",
    "    nbins=70,\n",
    "    barmode=\"overlay\",\n",
    "    opacity=0.55,\n",
    "    title=\"Concentration increases with df: ||W/df - scale||_F\",\n",
    "    labels={\"x\": \"Frobenius distance\", \"color\": \"df\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81454a4b",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.wishart`)\n",
    "\n",
    "SciPy provides a frozen Wishart distribution via:\n",
    "\n",
    "```python\n",
    "rv = scipy.stats.wishart(df=nu, scale=Sigma)\n",
    "```\n",
    "\n",
    "Available methods (SciPy 1.15):\n",
    "- `pdf`, `logpdf`\n",
    "- `rvs`\n",
    "- `mean`, `var`, `mode`, `entropy`\n",
    "\n",
    "Notably missing (as of SciPy 1.15):\n",
    "- `cdf` (matrix CDF is nontrivial)\n",
    "- `fit` (no built-in MLE)\n",
    "\n",
    "Workarounds:\n",
    "- For CDF-like quantities, use **scalar marginals** (e.g., diagonal entries) or Monte Carlo estimates of \\(\\mathbb{P}(W\\preceq X)\\).\n",
    "- For fitting, use the closed-form MLE for \\(\\Sigma\\) given \\(\\nu\\), or a method-of-moments / numerical MLE for \\(\\nu\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 8\n",
    "scale = np.array([\n",
    "    [1.5, 0.4],\n",
    "    [0.4, 1.0],\n",
    "])\n",
    "\n",
    "rv = wishart(df=df, scale=scale)\n",
    "W = rv.rvs(random_state=rng)\n",
    "\n",
    "print(\"W:\")\n",
    "print(np.round(W, 4))\n",
    "print(\"pdf:\", rv.pdf(W))\n",
    "print(\"logpdf:\", rv.logpdf(W))\n",
    "print(\"mean:\")\n",
    "print(rv.mean())\n",
    "print(\"var (elementwise):\")\n",
    "print(np.round(rv.var(), 6))\n",
    "print(\"mode:\")\n",
    "print(np.round(rv.mode(), 4))\n",
    "print(\"entropy:\", rv.entropy())\n",
    "print()\n",
    "print(\"has cdf?\", hasattr(rv, \"cdf\"))\n",
    "print(\"has fit?\", hasattr(rv, \"fit\"))\n",
    "\n",
    "# A \"CDF\" we *can* compute: diagonal marginal (scaled chi-square)\n",
    "w = float(W[0, 0])\n",
    "print()\n",
    "print(\"P(W_11 <= w) via chi-square:\", stats.chi2.cdf(w / scale[0, 0], df=df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo estimate of the matrix CDF: P(W \\preceq X)\n",
    "\n",
    "scale = np.array([\n",
    "    [1.2, 0.3],\n",
    "    [0.3, 0.9],\n",
    "])\n",
    "df = 9\n",
    "\n",
    "# Pick a threshold X (SPD). For example, a multiple of the mean.\n",
    "X = 1.15 * wishart_mean(df, scale)\n",
    "\n",
    "Ws = wishart_rvs_bartlett(df, scale, size=30_000 if FAST_RUN else 150_000, rng=rng)\n",
    "\n",
    "# W \\preceq X  <=>  X - W is PSD. For 2x2, PSD can be checked by eigenvalues.\n",
    "# (For larger p you would use a Cholesky attempt on X-W, but numerical noise matters.)\n",
    "\n",
    "mats = X[None, :, :] - Ws\n",
    "\n",
    "# numerical tolerance: allow tiny negative eigenvalues from floating-point error\n",
    "lmin = np.linalg.eigvalsh(mats)[:, 0]\n",
    "prob_hat = np.mean(lmin >= -1e-10)\n",
    "\n",
    "print(\"X:\")\n",
    "print(np.round(X, 3))\n",
    "print(r\"Monte Carlo P(W \\preceq X) ≈\", prob_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b675c",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing\n",
    "If \\(x_1,\\dots,x_\\nu\\sim\\mathcal{N}_p(0,\\Sigma)\\), then the scatter matrix \\(W=\\sum_i x_i x_i^\\top\\) is Wishart.\n",
    "This gives exact sampling distributions for statistics built from \\(W\\).\n",
    "\n",
    "A simple example uses whitening under a null \\(\\Sigma_0\\):\n",
    "\\[\n",
    "\\Sigma_0^{-1/2} W\\, \\Sigma_0^{-1/2} \\sim \\mathrm{Wishart}_p(\\nu, I)\n",
    "\\]\n",
    "when \\(\\Sigma=\\Sigma_0\\).\n",
    "Then \\(\\mathrm{tr}(\\Sigma_0^{-1} W)\\sim\\chi^2_{\\nu p}\\) provides an exact test of total variance.\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "Wishart is conjugate to the precision matrix \\(\\Lambda=\\Sigma^{-1}\\) in a multivariate normal model.\n",
    "With known mean (say 0):\n",
    "\n",
    "- Prior: \\(\\Lambda \\sim \\mathrm{Wishart}_p(\\nu_0, S_0)\\)\n",
    "- Data: \\(x_i\\mid\\Lambda \\sim \\mathcal{N}_p(0,\\Lambda^{-1})\\)\n",
    "- Let \\(S=\\sum_i x_i x_i^\\top\\). Posterior:\n",
    "  \\[\n",
    "  \\Lambda\\mid x_{1:n} \\sim \\mathrm{Wishart}_p\\big(\\nu_0+n,\\; (S_0^{-1}+S)^{-1}\\big).\n",
    "  \\]\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "Wishart gives a principled way to generate random SPD matrices that can be used as:\n",
    "- random covariance / precision matrices\n",
    "- random kernels for Gaussian processes\n",
    "- synthetic correlation structures for simulation studies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Hypothesis test example via whitening + trace statistic\n",
    "\n",
    "p = 3\n",
    "nu = 15\n",
    "\n",
    "Sigma0 = np.array([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "])\n",
    "\n",
    "# Create data under an alternative (not equal to Sigma0)\n",
    "Sigma_true = np.array([\n",
    "    [1.0, 0.6, 0.2],\n",
    "    [0.6, 1.5, 0.1],\n",
    "    [0.2, 0.1, 0.7],\n",
    "])\n",
    "\n",
    "X = rng.multivariate_normal(mean=np.zeros(p), cov=Sigma_true, size=nu)\n",
    "W = X.T @ X\n",
    "\n",
    "# Under H0: Sigma=Sigma0, the whitened scatter is Wishart(nu, I)\n",
    "# and tr(Sigma0^{-1} W) = tr(W) ~ chi2_{nu*p}.\n",
    "\n",
    "T = float(np.trace(np.linalg.solve(Sigma0, W)))\n",
    "p_value = 1.0 - stats.chi2.cdf(T, df=nu * p)\n",
    "\n",
    "print(\"Trace test statistic T=tr(Sigma0^{-1} W):\", round(T, 4))\n",
    "print(\"p-value (right tail):\", p_value)\n",
    "\n",
    "# Note: this test is sensitive to overall variance (trace), not all aspects of covariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian update: Wishart prior on precision\n",
    "\n",
    "p = 2\n",
    "n = 30\n",
    "\n",
    "Sigma_true = np.array([\n",
    "    [1.0, 0.7],\n",
    "    [0.7, 1.8],\n",
    "])\n",
    "\n",
    "# Data model: x ~ N(0, Sigma_true)\n",
    "X = rng.multivariate_normal(mean=np.zeros(p), cov=Sigma_true, size=n)\n",
    "S = X.T @ X\n",
    "\n",
    "# Prior on precision Lambda = Sigma^{-1}\n",
    "nu0 = p + 2  # must be > p-1\n",
    "S0 = np.eye(p)  # prior scale (on precision)\n",
    "\n",
    "nu_post = nu0 + n\n",
    "S_post = np.linalg.inv(np.linalg.inv(S0) + S)\n",
    "\n",
    "post = wishart(df=nu_post, scale=S_post)\n",
    "Lambda_samples = post.rvs(size=3_000 if FAST_RUN else 15_000, random_state=rng)\n",
    "\n",
    "# Posterior mean of precision: E[Lambda] = nu_post * S_post\n",
    "Lambda_mean = nu_post * S_post\n",
    "\n",
    "# Convert to covariance by inversion (Monte Carlo)\n",
    "Sigma_samples = np.linalg.inv(Lambda_samples)\n",
    "Sigma_mean_mc = Sigma_samples.mean(axis=0)\n",
    "\n",
    "print(\"Posterior df:\", nu_post)\n",
    "print(\"Posterior scale (precision):\")\n",
    "print(np.round(S_post, 4))\n",
    "print()\n",
    "print(\"E[Lambda|data] (closed form):\")\n",
    "print(np.round(Lambda_mean, 4))\n",
    "print(\"E[Sigma|data] (MC via inversion):\")\n",
    "print(np.round(Sigma_mean_mc, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wishartgen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Generative modeling: random covariance matrices\n",
    "\n",
    "p = 2\n",
    "nu = 25\n",
    "\n",
    "Sigma_target = np.array([\n",
    "    [1.0, 0.5],\n",
    "    [0.5, 1.3],\n",
    "])\n",
    "\n",
    "# Sample a random scatter matrix with mean nu * Sigma_target\n",
    "W = wishart_rvs_bartlett(nu, Sigma_target, size=1, rng=rng)\n",
    "\n",
    "# Turn it into a random covariance estimate with mean Sigma_target\n",
    "Sigma_sample = W / nu\n",
    "\n",
    "Y = rng.multivariate_normal(mean=np.zeros(p), cov=Sigma_sample, size=300)\n",
    "\n",
    "print(\"Sigma target:\")\n",
    "print(np.round(Sigma_target, 3))\n",
    "print(\"Sigma sampled:\")\n",
    "print(np.round(Sigma_sample, 3))\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=Y[:, 0],\n",
    "    y=Y[:, 1],\n",
    "    opacity=0.6,\n",
    "    title=\"Data generated from a random covariance (Sigma_sample)\",\n",
    "    labels={\"x\": \"y1\", \"y\": \"y2\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb37c40",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter constraints**:\n",
    "  - \\(\\nu > p-1\\) is required for a proper density.\n",
    "  - \\(\\Sigma\\) must be symmetric positive definite (not just semidefinite).\n",
    "- **SPD checks are numerical**: floating-point symmetry / PSD checks need tolerances.\n",
    "- **Determinants and inverses can overflow/underflow**:\n",
    "  - prefer `logpdf` over `pdf`\n",
    "  - use `slogdet` and solves (`np.linalg.solve`) instead of explicit inverses\n",
    "- **CDF is hard**: for \\(p>1\\), the matrix CDF is not available in SciPy; use scalar functionals or Monte Carlo.\n",
    "- **Inverse-Wishart is often what you want**: many Bayesian covariance models place a prior on \\(\\Sigma\\), not on \\(\\Sigma^{-1}\\).\n",
    "- **Interpretation**: \\(\\nu\\) scales the mean; looking at \\(W/\\nu\\) is often more interpretable as a noisy estimate of \\(\\Sigma\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f6f54",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- Wishart is a **continuous distribution on SPD matrices**, parameterized by degrees of freedom \\(\\nu\\) and scale \\(\\Sigma\\).\n",
    "- It is the sampling distribution of Gaussian scatter matrices: \\(W=\\sum_i x_i x_i^\\top\\).\n",
    "- Key formulas:\n",
    "  - \\(\\mathbb{E}[W]=\\nu\\Sigma\\)\n",
    "  - \\(\\mathrm{Cov}(W_{ij},W_{k\\ell})=\\nu(\\Sigma_{ik}\\Sigma_{j\\ell}+\\Sigma_{i\\ell}\\Sigma_{jk})\\)\n",
    "  - diagonal entries are scaled chi-square\n",
    "  - matrix MGF: \\(|I-2T\\Sigma|^{-\\nu/2}\\)\n",
    "- For simulation, Bartlett decomposition provides an efficient **NumPy-only** sampler.\n",
    "- In SciPy, `scipy.stats.wishart` supports `pdf/logpdf`, `rvs`, `mean/var/mode/entropy`, but not `cdf` or `fit`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}