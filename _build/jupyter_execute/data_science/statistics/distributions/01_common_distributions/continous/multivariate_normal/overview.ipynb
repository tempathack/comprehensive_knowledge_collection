{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5cd4a6",
   "metadata": {},
   "source": [
    "# Multivariate Normal distribution (`multivariate_normal`)\n",
    "\n",
    "The **multivariate normal** (a.k.a. **multivariate Gaussian**) is the canonical distribution for **continuous, correlated** random vectors.\n",
    "It generalizes the univariate normal by replacing the scalar variance with a **covariance matrix**.\n",
    "\n",
    "It is foundational in statistics and machine learning: linear regression, Kalman filtering, Gaussian processes, LDA/QDA, PCA, and as a building block for Gaussian mixture models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "print(\"NumPy \", np.__version__)\n",
    "print(\"SciPy \", scipy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc47a82e",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Definition and parameter space\n",
    "2. Intuition and relationships\n",
    "3. PDF / CDF\n",
    "4. Moments and key properties\n",
    "5. How parameters change the shape\n",
    "6. Likelihood + MLE sketch\n",
    "7. NumPy-only sampling\n",
    "8. Visualizations (PDF, CDF, samples)\n",
    "9. SciPy integration (`scipy.stats.multivariate_normal`)\n",
    "10. Statistical use cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa20f1",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "| Item | Value |\n",
    "|---|---|\n",
    "| Name | Multivariate Normal (`multivariate_normal`) |\n",
    "| Type | **Continuous** |\n",
    "| Dimension | $d \\in \\mathbb{N}$ (fixed) |\n",
    "| Support | $x \\in \\mathbb{R}^d$ |\n",
    "| Parameters | mean $\\mu \\in \\mathbb{R}^d$, covariance $\\Sigma \\in \\mathbb{S}_{++}^d$ |\n",
    "\n",
    "Here $\\mathbb{S}_{++}^d$ denotes the set of **symmetric positive definite** $d\\times d$ matrices.\n",
    "\n",
    "**SciPy note**: `scipy.stats.multivariate_normal` also supports `allow_singular=True`, which relaxes $\\Sigma$ to be **positive semidefinite**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528d7c0",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "A multivariate normal models a random vector whose uncertainty looks like an **ellipsoid** in $\\mathbb{R}^d$.\n",
    "The key geometric object is the **Mahalanobis distance**\n",
    "\n",
    "$$\n",
    "\\delta^2(x) = (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu),\n",
    "$$\n",
    "\n",
    "which replaces the “$z$-score squared” from the univariate normal.\n",
    "\n",
    "A very useful generative story:\n",
    "\n",
    "1. Draw $Z \\sim \\mathcal{N}(0, I_d)$ (independent standard normals).\n",
    "2. Choose a matrix $L$ such that $\\Sigma = L L^\\top$ (e.g. Cholesky factor).\n",
    "3. Set\n",
    "   $$X = \\mu + L Z.$$\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **Measurement noise** with correlated sensors / features\n",
    "- **Kalman filters** and state-space models (Gaussian noise + linear dynamics)\n",
    "- **Gaussian processes**: any finite set of function values is multivariate normal\n",
    "- **LDA/QDA**: class-conditional feature distributions are modeled as Gaussian\n",
    "- **PCA / factor models**: covariance structure is the main object\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Univariate normal** is the special case $d=1$.\n",
    "- **Linear transformations** preserve normality: if $X\\sim\\mathcal{N}(\\mu,\\Sigma)$, then $AX+b$ is normal.\n",
    "- **Conditionals and marginals** of a multivariate normal are again (multi)variate normals.\n",
    "- The quadratic form $\\delta^2(X)$ has a **chi-square** distribution: $\\delta^2(X) \\sim \\chi^2_d$.\n",
    "- The sample covariance of Gaussian data is **Wishart**-distributed.\n",
    "\n",
    "(A deeper fact: among all continuous distributions with a given mean and covariance, the multivariate normal has **maximum entropy**.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33bf3a",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $d$ be the dimension, $\\mu\\in\\mathbb{R}^d$, and $\\Sigma\\in\\mathbb{S}_{++}^d$.\n",
    "We write\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu, \\Sigma).\n",
    "$$\n",
    "\n",
    "### PDF\n",
    "\n",
    "The probability density function (PDF) is\n",
    "\n",
    "$$\n",
    "f(x\\mid\\mu,\\Sigma)\n",
    "= \\frac{1}{(2\\pi)^{d/2}\\,|\\Sigma|^{1/2}}\n",
    "\\exp\\left(-\\tfrac{1}{2}(x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)\\right),\n",
    "\\qquad x\\in\\mathbb{R}^d.\n",
    "$$\n",
    "\n",
    "A numerically stable way to work is the **log-density**:\n",
    "\n",
    "$$\n",
    "\\log f(x) = -\\tfrac{1}{2}\\Big(d\\log(2\\pi) + \\log|\\Sigma| + (x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)\\Big).\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "The multivariate CDF is defined by integrating over a rectangle (orthant):\n",
    "\n",
    "$$\n",
    "F(x_1,\\ldots,x_d) = \\mathbb{P}(X_1\\le x_1,\\ldots,X_d\\le x_d)\n",
    "= \\int_{-\\infty}^{x_1}\\cdots\\int_{-\\infty}^{x_d} f(u)\\,du.\n",
    "$$\n",
    "\n",
    "For $d>1$, there is **no general closed form**; practical evaluation uses **numerical methods**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ef9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_2d(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim == 1:\n",
    "        return x.reshape(1, -1)\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(\"x must be a 1D or 2D array\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def _check_mean_cov(mean: np.ndarray, cov: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    mean = np.asarray(mean, dtype=float)\n",
    "    cov = np.asarray(cov, dtype=float)\n",
    "\n",
    "    if mean.ndim != 1:\n",
    "        raise ValueError(\"mean must be 1D\")\n",
    "    if cov.ndim != 2 or cov.shape[0] != cov.shape[1]:\n",
    "        raise ValueError(\"cov must be a square 2D array\")\n",
    "\n",
    "    d = mean.shape[0]\n",
    "    if cov.shape != (d, d):\n",
    "        raise ValueError(f\"cov must have shape ({d}, {d})\")\n",
    "\n",
    "    if not np.allclose(cov, cov.T, atol=1e-12, rtol=0):\n",
    "        raise ValueError(\"cov must be symmetric\")\n",
    "\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def mvn_logpdf(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of a multivariate normal using NumPy only (SPD covariance required).\n",
    "\n",
    "    Args:\n",
    "        x: shape (d,) or (n, d)\n",
    "        mean: shape (d,)\n",
    "        cov: shape (d, d), symmetric positive definite\n",
    "\n",
    "    Returns:\n",
    "        logpdf values with shape (n,) (or scalar if x is (d,)).\n",
    "    \"\"\"\n",
    "    mean, cov = _check_mean_cov(mean, cov)\n",
    "    x2 = _as_2d(x)\n",
    "    d = mean.shape[0]\n",
    "    if x2.shape[1] != d:\n",
    "        raise ValueError(f\"x must have dimension d={d}\")\n",
    "\n",
    "    # Cholesky: cov = L L^T (requires SPD)\n",
    "    L = np.linalg.cholesky(cov)\n",
    "\n",
    "    diff = (x2 - mean)  # (n, d)\n",
    "    # Solve L y = diff^T => y = L^{-1} diff^T\n",
    "    y = np.linalg.solve(L, diff.T)  # (d, n)\n",
    "    maha2 = np.sum(y**2, axis=0)  # (n,)\n",
    "\n",
    "    log_det = 2.0 * np.sum(np.log(np.diag(L)))\n",
    "    log_norm = -0.5 * (d * np.log(2.0 * np.pi) + log_det)\n",
    "    out = log_norm - 0.5 * maha2\n",
    "\n",
    "    if np.asarray(x).ndim == 1:\n",
    "        return float(out[0])\n",
    "    return out\n",
    "\n",
    "\n",
    "def mvn_pdf(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(mvn_logpdf(x, mean, cov))\n",
    "\n",
    "\n",
    "def mvn_rvs(\n",
    "    mean: np.ndarray,\n",
    "    cov: np.ndarray,\n",
    "    size: int,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Sample from N(mean, cov) using NumPy only (via Cholesky).\n",
    "\n",
    "    Returns an array of shape (size, d).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    mean, cov = _check_mean_cov(mean, cov)\n",
    "    d = mean.shape[0]\n",
    "    L = np.linalg.cholesky(cov)\n",
    "    z = rng.standard_normal(size=(int(size), d))\n",
    "    return mean + z @ L.T\n",
    "\n",
    "\n",
    "def mvn_cdf_mc(\n",
    "    x: np.ndarray,\n",
    "    mean: np.ndarray,\n",
    "    cov: np.ndarray,\n",
    "    n: int = 50_000,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Monte Carlo estimate of F(x) = P(X_1<=x_1,...,X_d<=x_d).\n",
    "\n",
    "    Returns (estimate, standard_error).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"x must be a 1D point\")\n",
    "\n",
    "    samples = mvn_rvs(mean, cov, size=int(n), rng=rng)\n",
    "    hits = np.all(samples <= x[None, :], axis=1)\n",
    "    p = float(np.mean(hits))\n",
    "    se = float(np.sqrt(p * (1.0 - p) / n))\n",
    "    return p, se\n",
    "\n",
    "\n",
    "# Quick spot-check vs SciPy\n",
    "mu = np.array([0.5, -1.0])\n",
    "Sigma = np.array([[1.5, 0.4], [0.4, 0.8]])\n",
    "x_test = np.array([[0.0, 0.0], [1.0, -1.5], [2.0, -0.5]])\n",
    "\n",
    "ours = mvn_logpdf(x_test, mu, Sigma)\n",
    "theirs = multivariate_normal(mean=mu, cov=Sigma).logpdf(x_test)\n",
    "print(\"max |logpdf_ours - logpdf_scipy| =\", float(np.max(np.abs(ours - theirs))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cb508",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let $X \\sim \\mathcal{N}(\\mu,\\Sigma)$ in dimension $d$.\n",
    "\n",
    "### Moments\n",
    "\n",
    "| Quantity | Value |\n",
    "|---|---|\n",
    "| Mean | $\\mathbb{E}[X]=\\mu$ |\n",
    "| Covariance | $\\mathrm{Cov}(X)=\\Sigma$ |\n",
    "| Variance of component $i$ | $\\mathrm{Var}(X_i)=\\Sigma_{ii}$ |\n",
    "| Variance of linear combo $a^\\top X$ | $\\mathrm{Var}(a^\\top X)=a^\\top\\Sigma a$ |\n",
    "| Skewness | 0 (all odd central moments are 0) |\n",
    "| Kurtosis | any 1D marginal has kurtosis 3; Mardia kurtosis $\\beta_{2,d}=d(d+2)$ |\n",
    "\n",
    "A common multivariate kurtosis measure (Mardia) is\n",
    "\n",
    "$$\n",
    "\\beta_{2,d} = \\mathbb{E}\\big[\\delta^4(X)\\big]\n",
    "= \\mathbb{E}\\big[\\big((X-\\mu)^\\top\\Sigma^{-1}(X-\\mu)\\big)^2\\big] = d(d+2).\n",
    "$$\n",
    "\n",
    "### MGF and characteristic function\n",
    "\n",
    "For $t\\in\\mathbb{R}^d$,\n",
    "\n",
    "$$\n",
    "M_X(t)=\\mathbb{E}[e^{t^\\top X}]=\\exp\\left(t^\\top\\mu + \\tfrac{1}{2} t^\\top\\Sigma t\\right),\n",
    "$$\n",
    "\n",
    "and the characteristic function is\n",
    "\n",
    "$$\n",
    "\\varphi_X(t)=\\mathbb{E}[e^{i t^\\top X}]=\\exp\\left(i t^\\top\\mu - \\tfrac{1}{2} t^\\top\\Sigma t\\right).\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy (in **nats**) is\n",
    "\n",
    "$$\n",
    "H(X)=\\tfrac{1}{2}\\log\\big((2\\pi e)^d |\\Sigma|\\big).\n",
    "$$\n",
    "\n",
    "### Key closure properties\n",
    "\n",
    "- **Affine transform**: if $X\\sim\\mathcal{N}(\\mu,\\Sigma)$ then $Y=AX+b$ is normal with\n",
    "  $\\mathbb{E}[Y]=A\\mu+b$ and $\\mathrm{Cov}(Y)=A\\Sigma A^\\top$.\n",
    "- **Marginals**: any subvector of $X$ is multivariate normal.\n",
    "- **Conditionals**: $X_A\\mid X_B=b$ is multivariate normal (with closed-form mean/cov).\n",
    "- **Uncorrelated = independent** (special to the multivariate normal): if $\\mathrm{Cov}(X_i,X_j)=0$ then $X_i$ and $X_j$ are independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c6910",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "- The mean vector $\\mu$ is a **location** parameter: it shifts the center of mass.\n",
    "- The covariance matrix $\\Sigma$ controls:\n",
    "  - **scale** (overall spread),\n",
    "  - **anisotropy** (different variance along different directions),\n",
    "  - **correlation** (tilt/rotation of the ellipsoids).\n",
    "\n",
    "A helpful view is the eigendecomposition:\n",
    "\n",
    "$$\n",
    "\\Sigma = Q\\Lambda Q^\\top,\n",
    "$$\n",
    "\n",
    "where columns of $Q$ are orthonormal eigenvectors and $\\Lambda=\\mathrm{diag}(\\lambda_1,\\ldots,\\lambda_d)$.\n",
    "Then:\n",
    "\n",
    "- eigenvectors give the **principal axes** of the density,\n",
    "- eigenvalues give the **variances along those axes**.\n",
    "\n",
    "Increasing an eigenvalue stretches the ellipsoid in that direction; changing correlation rotates the ellipsoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How correlation changes the shape (d=2, unit variances)\n",
    "mu = np.zeros(2)\n",
    "rhos = [-0.8, 0.0, 0.8]\n",
    "\n",
    "x1 = np.linspace(-3.5, 3.5, 160)\n",
    "x2 = np.linspace(-3.5, 3.5, 160)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "grid = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=[f\"correlation ρ = {rho}\" for rho in rhos],\n",
    "    horizontal_spacing=0.06,\n",
    ")\n",
    "\n",
    "for j, rho in enumerate(rhos, start=1):\n",
    "    Sigma = np.array([[1.0, rho], [rho, 1.0]])\n",
    "    Z = mvn_pdf(grid, mu, Sigma).reshape(X1.shape)\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=x1,\n",
    "            y=x2,\n",
    "            z=Z,\n",
    "            contours=dict(showlabels=False),\n",
    "            colorscale=\"Viridis\",\n",
    "            line_smoothing=0.8,\n",
    "            showscale=(j == 3),\n",
    "            colorbar=dict(title=\"pdf\") if j == 3 else None,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=j,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"x1\", row=1, col=j)\n",
    "    fig.update_yaxes(title_text=\"x2\", row=1, col=j)\n",
    "\n",
    "fig.update_layout(title=\"Effect of correlation on the PDF (unit variances)\", height=360)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf3565",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "Below are standard derivations/sketches that are useful to remember.\n",
    "\n",
    "### Expectation and covariance\n",
    "\n",
    "Using the generative representation $X=\\mu+LZ$ with $Z\\sim\\mathcal{N}(0,I)$ and $\\Sigma=LL^\\top$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\mathbb{E}[\\mu + LZ] = \\mu + L\\,\\mathbb{E}[Z] = \\mu.\n",
    "$$\n",
    "\n",
    "For the covariance:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X) = \\mathrm{Cov}(LZ) = L\\,\\mathrm{Cov}(Z)\\,L^\\top = L I L^\\top = \\Sigma.\n",
    "$$\n",
    "\n",
    "### Likelihood (i.i.d. data)\n",
    "\n",
    "Let $x_1,\\ldots,x_n$ be i.i.d. from $\\mathcal{N}(\\mu,\\Sigma)$.\n",
    "The log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(\\mu,\\Sigma)\n",
    "= -\\frac{n}{2}\\Big(d\\log(2\\pi) + \\log|\\Sigma|\\Big)\n",
    "  - \\frac{1}{2}\\sum_{i=1}^n (x_i-\\mu)^\\top\\Sigma^{-1}(x_i-\\mu).\n",
    "$$\n",
    "\n",
    "The maximum-likelihood estimators (MLEs) are:\n",
    "\n",
    "$$\n",
    "\\hat\\mu = \\bar x,\\qquad\n",
    "\\hat\\Sigma_{\\mathrm{MLE}} = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar x)(x_i-\\bar x)^\\top.\n",
    "$$\n",
    "\n",
    "(The **unbiased** sample covariance replaces $1/n$ by $1/(n-1)$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84199e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the mean/covariance derivation empirically\n",
    "d = 3\n",
    "mu_true = np.array([1.0, -2.0, 0.5])\n",
    "Sigma_true = np.array(\n",
    "    [\n",
    "        [2.0, 0.3, -0.2],\n",
    "        [0.3, 1.0, 0.4],\n",
    "        [-0.2, 0.4, 1.5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = mvn_rvs(mu_true, Sigma_true, size=120_000, rng=rng)\n",
    "\n",
    "mu_hat = X.mean(axis=0)\n",
    "cov_hat_mle = np.cov(X, rowvar=False, bias=True)  # 1/n\n",
    "\n",
    "print(\"mu_true:\", mu_true)\n",
    "print(\"mu_hat :\", mu_hat)\n",
    "print(\"\\nSigma_true:\\n\", Sigma_true)\n",
    "print(\"\\nSigma_hat (MLE, 1/n):\\n\", cov_hat_mle)\n",
    "\n",
    "# Compare MLE to SciPy's fit\n",
    "mu_fit, cov_fit = multivariate_normal.fit(X)\n",
    "print(\"\\nmax |mu_fit - mu_hat| =\", float(np.max(np.abs(mu_fit - mu_hat))))\n",
    "print(\"max |cov_fit - cov_hat_mle| =\", float(np.max(np.abs(cov_fit - cov_hat_mle))))\n",
    "\n",
    "# Log-likelihood at the true params vs at the fitted params\n",
    "ll_true = float(np.sum(multivariate_normal(mean=mu_true, cov=Sigma_true).logpdf(X)))\n",
    "ll_fit = float(np.sum(multivariate_normal(mean=mu_fit, cov=cov_fit).logpdf(X)))\n",
    "print(\"\\nlog-likelihood (true params):\", ll_true)\n",
    "print(\"log-likelihood (fitted MLE) :\", ll_fit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af6c4f",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only algorithm (Cholesky)\n",
    "\n",
    "To sample $X\\sim\\mathcal{N}(\\mu,\\Sigma)$:\n",
    "\n",
    "1. Compute the Cholesky factor $L$ of the covariance: $\\Sigma = LL^\\top$.\n",
    "2. Draw $Z\\in\\mathbb{R}^d$ with i.i.d. standard normal entries.\n",
    "3. Return $X = \\mu + LZ$.\n",
    "\n",
    "This works because affine transformations of Gaussians are Gaussian.\n",
    "\n",
    "**Cost**:\n",
    "- one Cholesky factorization costs $\\mathcal{O}(d^3)$,\n",
    "- each sample costs $\\mathcal{O}(d^2)$ for the matrix-vector multiply.\n",
    "\n",
    "If $\\Sigma$ is nearly singular, Cholesky can fail; common fixes include adding a small diagonal “jitter” $\\varepsilon I$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fbcb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling demo: squared Mahalanobis distance should look chi-square(d)\n",
    "d = 4\n",
    "mu0 = np.zeros(d)\n",
    "A = rng.normal(size=(d, d))\n",
    "Sigma0 = A @ A.T + 0.5 * np.eye(d)  # SPD\n",
    "\n",
    "X = mvn_rvs(mu0, Sigma0, size=50_000, rng=rng)\n",
    "\n",
    "# Compute delta^2(x) = (x-mu)^T Sigma^{-1} (x-mu) without forming Sigma^{-1}\n",
    "L = np.linalg.cholesky(Sigma0)\n",
    "Y = np.linalg.solve(L, (X - mu0).T)  # (d, n)\n",
    "delta2 = np.sum(Y**2, axis=0)\n",
    "\n",
    "# Compare moments with chi-square(d)\n",
    "print(\"E[delta^2] empirical:\", float(np.mean(delta2)), \"theory:\", d)\n",
    "print(\"Var[delta^2] empirical:\", float(np.var(delta2)), \"theory:\", 2 * d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269b9ad",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll focus on $d=2$ so we can visualize:\n",
    "\n",
    "- **PDF** as contours\n",
    "- **CDF** as a heatmap of $F(x_1,x_2)=\\mathbb{P}(X_1\\le x_1, X_2\\le x_2)$\n",
    "- **Monte Carlo samples** as a scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d63d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu2 = np.array([0.5, -0.5])\n",
    "Sigma2 = np.array([[1.0, 0.75], [0.75, 1.8]])\n",
    "\n",
    "# Grid\n",
    "x1 = np.linspace(-3.5, 4.0, 180)\n",
    "x2 = np.linspace(-4.5, 3.5, 180)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "grid = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "\n",
    "# PDF via our NumPy implementation\n",
    "Z_pdf = mvn_pdf(grid, mu2, Sigma2).reshape(X1.shape)\n",
    "\n",
    "# Samples\n",
    "S = mvn_rvs(mu2, Sigma2, size=2_000, rng=rng)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=x1,\n",
    "        y=x2,\n",
    "        z=Z_pdf,\n",
    "        contours=dict(showlabels=False),\n",
    "        colorscale=\"Viridis\",\n",
    "        line_smoothing=0.8,\n",
    "        showscale=True,\n",
    "        name=\"PDF\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=S[:, 0],\n",
    "        y=S[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=4, color=\"rgba(0,0,0,0.35)\"),\n",
    "        name=\"samples\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Multivariate normal: PDF contours + Monte Carlo samples (d=2)\",\n",
    "    xaxis_title=\"x1\",\n",
    "    yaxis_title=\"x2\",\n",
    "    legend=dict(orientation=\"h\"),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcccd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF heatmap using SciPy's multivariate_normal.cdf (numerical evaluation)\n",
    "rv2 = multivariate_normal(mean=mu2, cov=Sigma2)\n",
    "Z_cdf = rv2.cdf(grid).reshape(X1.shape)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        x=x1,\n",
    "        y=x2,\n",
    "        z=Z_cdf,\n",
    "        colorscale=\"Blues\",\n",
    "        colorbar=dict(title=\"F(x1,x2)\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Multivariate normal: CDF heatmap (d=2)\",\n",
    "    xaxis_title=\"x1\",\n",
    "    yaxis_title=\"x2\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# A quick MC check at one point\n",
    "x_point = np.array([0.0, 0.0])\n",
    "p_mc, se_mc = mvn_cdf_mc(x_point, mu2, Sigma2, n=100_000, rng=rng)\n",
    "p_scipy = float(rv2.cdf(x_point))\n",
    "print(\"F(0,0) SciPy:\", p_scipy)\n",
    "print(\"F(0,0) MC  :\", p_mc, \"+/-\", 2 * se_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfadcb7",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.multivariate_normal`)\n",
    "\n",
    "SciPy provides the multivariate normal as `scipy.stats.multivariate_normal`.\n",
    "\n",
    "```python\n",
    "rv = scipy.stats.multivariate_normal(mean=mu, cov=Sigma)\n",
    "```\n",
    "\n",
    "Common methods:\n",
    "\n",
    "- `pdf`, `logpdf`\n",
    "- `cdf` (numerical)\n",
    "- `rvs`\n",
    "- `entropy`\n",
    "- `fit` (MLE for mean and covariance)\n",
    "\n",
    "Note: `pdf` can underflow in moderate dimension; prefer `logpdf` for likelihood work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([1.0, 2.0])\n",
    "Sigma = np.array([[2.0, -0.3], [-0.3, 1.0]])\n",
    "\n",
    "rv = multivariate_normal(mean=mu, cov=Sigma)\n",
    "\n",
    "pts = np.array([[1.0, 2.0], [0.0, 0.0], [2.0, 3.0]])\n",
    "\n",
    "print(\"pdf   :\", rv.pdf(pts))\n",
    "print(\"logpdf:\", rv.logpdf(pts))\n",
    "print(\"cdf   :\", rv.cdf(pts))\n",
    "print(\"entropy:\", float(rv.entropy()), \"nats\")\n",
    "\n",
    "samples = rv.rvs(size=5_000, random_state=rng)\n",
    "\n",
    "# Fit parameters (MLE)\n",
    "mu_fit, cov_fit = multivariate_normal.fit(samples)\n",
    "print(\"\\nmu_fit:\", mu_fit)\n",
    "print(\"cov_fit:\\n\", cov_fit)\n",
    "\n",
    "# Compare to our NumPy-only logpdf\n",
    "ours = mvn_logpdf(pts, mu, Sigma)\n",
    "theirs = rv.logpdf(pts)\n",
    "print(\"\\nmax |logpdf_ours - logpdf_scipy| =\", float(np.max(np.abs(ours - theirs))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acc61a",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing\n",
    "\n",
    "- **Mean vector testing**: Hotelling’s $T^2$ tests $H_0: \\mu=\\mu_0$ for i.i.d. Gaussian data.\n",
    "- **Covariance testing**: tests about $\\Sigma$ (equality, sphericity) often use Wishart theory.\n",
    "- **Normality checks**: multivariate normality can be assessed with Q–Q plots of Mahalanobis distances (should be $\\chi^2_d$).\n",
    "\n",
    "### Bayesian modeling\n",
    "\n",
    "- A multivariate normal is a natural **prior** for a vector of parameters.\n",
    "- With a Gaussian likelihood and known covariance, the posterior for the mean is again Gaussian.\n",
    "- For unknown mean and covariance, the conjugate prior is **Normal–Inverse-Wishart**.\n",
    "\n",
    "### Generative modeling\n",
    "\n",
    "- **Gaussian mixture models (GMMs)** approximate complex densities by mixing Gaussians.\n",
    "- Many latent-variable models assume Gaussian latent variables (e.g., factor analysis, VAEs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c94481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotelling's T^2 test demo: H0: mu = mu0\n",
    "# Under H0: (n-d)/(d*(n-1)) * T^2 ~ F_{d, n-d}\n",
    "n = 60\n",
    "d = 3\n",
    "\n",
    "mu_true = np.array([0.2, -0.1, 0.3])\n",
    "Sigma_true = np.array(\n",
    "    [\n",
    "        [1.2, 0.2, 0.0],\n",
    "        [0.2, 1.0, 0.4],\n",
    "        [0.0, 0.4, 1.5],\n",
    "    ]\n",
    ")\n",
    "X = mvn_rvs(mu_true, Sigma_true, size=n, rng=rng)\n",
    "\n",
    "mu0 = np.zeros(d)  # null hypothesis\n",
    "\n",
    "xbar = X.mean(axis=0)\n",
    "S_unbiased = np.cov(X, rowvar=False, bias=False)  # 1/(n-1)\n",
    "\n",
    "# Solve S^{-1}(xbar-mu0) without explicit inverse\n",
    "diff = (xbar - mu0).reshape(-1, 1)\n",
    "sol = np.linalg.solve(S_unbiased, diff)\n",
    "T2 = float(n * (diff.T @ sol))\n",
    "\n",
    "F_stat = (n - d) / (d * (n - 1)) * T2\n",
    "p_value = float(stats.f.sf(F_stat, dfn=d, dfd=n - d))\n",
    "\n",
    "print(\"T^2 statistic:\", T2)\n",
    "print(\"F statistic :\", F_stat)\n",
    "print(\"p-value     :\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate normality check: Mahalanobis-distance Q-Q vs chi-square\n",
    "n = 600\n",
    "d = 2\n",
    "\n",
    "mu_true = np.array([0.0, 0.0])\n",
    "Sigma_true = np.array([[1.0, 0.6], [0.6, 1.3]])\n",
    "X = mvn_rvs(mu_true, Sigma_true, size=n, rng=rng)\n",
    "\n",
    "mu_hat = X.mean(axis=0)\n",
    "S_unbiased = np.cov(X, rowvar=False, bias=False)\n",
    "\n",
    "# delta^2_i = (x_i-mu_hat)^T S^{-1} (x_i-mu_hat)\n",
    "L = np.linalg.cholesky(S_unbiased)\n",
    "Y = np.linalg.solve(L, (X - mu_hat).T)\n",
    "delta2 = np.sum(Y**2, axis=0)\n",
    "\n",
    "delta2_sorted = np.sort(delta2)\n",
    "q = (np.arange(1, n + 1) - 0.5) / n\n",
    "chi2_q = stats.chi2.ppf(q, df=d)\n",
    "\n",
    "maxv = float(max(delta2_sorted.max(), chi2_q.max()))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=chi2_q, y=delta2_sorted, mode=\"markers\", name=\"data\"))\n",
    "fig.add_trace(go.Scatter(x=[0, maxv], y=[0, maxv], mode=\"lines\", name=\"y = x\"))\n",
    "fig.update_layout(\n",
    "    title=\"Mahalanobis-distance Q-Q plot (should be ~ linear under MVN)\",\n",
    "    xaxis_title=\"Chi-square quantiles (df=d)\",\n",
    "    yaxis_title=\"Sorted Mahalanobis distances (delta^2)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian update for the mean (known covariance):\n",
    "# Prior:   mu ~ N(m0, V0)\n",
    "# Likelihood: x_i | mu ~ N(mu, Sigma)\n",
    "# Posterior: mu | X ~ N(mn, Vn)\n",
    "\n",
    "d = 2\n",
    "Sigma = np.array([[1.0, 0.4], [0.4, 1.5]])\n",
    "\n",
    "m0 = np.array([0.0, 0.0])\n",
    "V0 = 4.0 * np.eye(d)  # diffuse prior\n",
    "\n",
    "mu_true = np.array([1.0, -1.0])\n",
    "X = mvn_rvs(mu_true, Sigma, size=40, rng=rng)\n",
    "\n",
    "# Work in precision form\n",
    "Prec0 = np.linalg.inv(V0)\n",
    "PrecL = np.linalg.inv(Sigma)\n",
    "\n",
    "Vn = np.linalg.inv(Prec0 + X.shape[0] * PrecL)\n",
    "mn = Vn @ (Prec0 @ m0 + PrecL @ X.sum(axis=0))\n",
    "\n",
    "print(\"prior mean:\", m0)\n",
    "print(\"posterior mean:\", mn)\n",
    "print(\"true mean:\", mu_true)\n",
    "print(\"posterior cov:\\n\", Vn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbebe1",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid covariance**: $\\Sigma$ must be symmetric positive (semi)definite. Small asymmetries from floating-point noise can break factorization.\n",
    "- **Near-singularity**: Cholesky may fail if $\\Sigma$ is ill-conditioned. A common fix is adding jitter: $\\Sigma \\leftarrow \\Sigma + \\varepsilon I$.\n",
    "- **Avoid explicit matrix inverses**: compute quadratic forms with solves (as we did) for stability.\n",
    "- **Underflow in `pdf`**: in moderate/high dimension, densities can be tiny; use `logpdf` for likelihoods.\n",
    "- **Interpreting correlation**: zero correlation implies independence only for jointly Gaussian variables.\n",
    "- **CDF in high dimension**: multivariate CDF evaluation can become expensive or less accurate as $d$ grows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45eb17",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `multivariate_normal` is a **continuous** distribution on $\\mathbb{R}^d$ with parameters mean $\\mu$ and covariance $\\Sigma$.\n",
    "- Its density is controlled by the **Mahalanobis distance** $(x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)$.\n",
    "- Key formulas: $\\mathbb{E}[X]=\\mu$, $\\mathrm{Cov}(X)=\\Sigma$, $M_X(t)=\\exp(t^\\top\\mu + \\tfrac{1}{2}t^\\top\\Sigma t)$, and $H(X)=\\tfrac{1}{2}\\log((2\\pi e)^d|\\Sigma|)$.\n",
    "- Sampling is easy via **Cholesky**: $X=\\mu+LZ$ with $Z\\sim\\mathcal{N}(0,I)$.\n",
    "- SciPy’s `scipy.stats.multivariate_normal` provides `pdf/logpdf/cdf/rvs/fit`.\n",
    "\n",
    "### References\n",
    "\n",
    "- SciPy docs: `scipy.stats.multivariate_normal`\n",
    "- Any standard multivariate statistics text (e.g., Anderson, *An Introduction to Multivariate Statistical Analysis*)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}