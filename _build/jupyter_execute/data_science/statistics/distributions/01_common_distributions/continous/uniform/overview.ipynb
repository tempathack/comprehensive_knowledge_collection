{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a16ecd1",
   "metadata": {},
   "source": [
    "# Uniform Distribution — Bounded Randomness\n",
    "\n",
    "The **continuous uniform distribution** is the simplest model for a quantity that can take *any* value within a known interval and is **equally likely** across that interval.\n",
    "\n",
    "It shows up as:\n",
    "- a *building block* for simulation (inverse-CDF / transforms)\n",
    "- the canonical distribution for **p-values under a true null**: $p \\sim \\mathrm{Uniform}(0,1)$\n",
    "- the **maximum-entropy** distribution on a bounded interval (no other information)\n",
    "\n",
    "## What you’ll learn\n",
    "- definition (PDF/CDF), support, and parameter constraints\n",
    "- closed-form moments, MGF/CF, and entropy\n",
    "- MLE / likelihood geometry (why the MLE hits min/max)\n",
    "- NumPy-only sampling and basic visual diagnostics\n",
    "- how SciPy parameterizes `scipy.stats.uniform`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import plotly\n",
    "from scipy import stats\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")  # CKC convention\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daeca3e",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `uniform` (continuous uniform distribution)\n",
    "- **Type**: **Continuous**\n",
    "- **Support**: $x \\in [a, b]$\n",
    "- **Parameter space**: $a,b \\in \\mathbb{R}$ with $a < b$\n",
    "\n",
    "We write:\n",
    "\n",
    "$$X \\sim \\mathrm{Uniform}(a,b).$$\n",
    "\n",
    "**Library note (SciPy):** `scipy.stats.uniform` uses parameters `(loc, scale)` with support $x \\in [\\mathrm{loc},\\, \\mathrm{loc}+\\mathrm{scale}]$ and constraint $\\mathrm{scale}>0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fd4a7",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### 2.1 What it models\n",
    "Use a uniform distribution when you only know that a quantity lies in a bounded interval $[a,b]$ and you have **no reason to prefer** any sub-interval.\n",
    "\n",
    "Equivalently: among all continuous distributions supported on $[a,b]$, the uniform has **maximum differential entropy**.\n",
    "\n",
    "### 2.2 Typical real-world use cases\n",
    "- **Randomized experiments**: random assignment, random offsets, random jitter\n",
    "- **Simulation / Monte Carlo**: base source of randomness used to generate other distributions\n",
    "- **Quality control**: tolerances where any value in a band is “equally plausible”\n",
    "- **P-values under $H_0$**: if a test is valid and the null is true, $p \\sim \\mathrm{Uniform}(0,1)$\n",
    "\n",
    "### 2.3 Relations to other distributions\n",
    "- **Beta**: $\\mathrm{Uniform}(0,1) = \\mathrm{Beta}(1,1)$\n",
    "- **Order statistics**: the sample min/max have Beta-distributed rescalings\n",
    "- **Transforms**: if $U\\sim\\mathrm{Uniform}(0,1)$ and $X = F^{-1}(U)$, then $X$ has CDF $F$ (inverse transform sampling)\n",
    "- **Sums/averages**: sums of i.i.d. uniforms give the Irwin–Hall distribution; the mean gives the Bates distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531745fa",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "For $a<b$:\n",
    "\n",
    "### 3.1 PDF\n",
    "\\[\n",
    " f(x\\mid a,b) = \begin{cases}\n",
    " \f",
    "\n",
    "rac{1}{b-a}, & a \\le x \\le b\\\n",
    " 0, & \text{otherwise.}\n",
    " \\end{cases}\n",
    "\\]\n",
    "\n",
    "### 3.2 CDF\n",
    "\\[\n",
    " F(x\\mid a,b) = \\mathbb{P}(X\\le x) = \begin{cases}\n",
    " 0, & x < a\\\n",
    " \f",
    "\n",
    "rac{x-a}{b-a}, & a \\le x \\le b\\\n",
    " 1, & x > b.\n",
    " \\end{cases}\n",
    "\\]\n",
    "\n",
    "Because this is a *continuous* distribution, what happens at single points like $x=a$ or $x=b$ does not affect probabilities (those points have probability 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926748c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_pdf(x: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    Uniform(a,b) PDF (vectorized).\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    pdf = np.zeros_like(x, dtype=float)\n",
    "    inside = (a <= x) & (x <= b)\n",
    "    pdf[inside] = 1.0 / (b - a)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def uniform_cdf(x: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    Uniform(a,b) CDF (vectorized).\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return np.where(\n",
    "        x < a,\n",
    "        0.0,\n",
    "        np.where(x > b, 1.0, (x - a) / (b - a)),\n",
    "    )\n",
    "\n",
    "\n",
    "def uniform_logpdf(x: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    Uniform(a,b) log-PDF (vectorized).\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    logpdf = np.full_like(x, -np.inf, dtype=float)\n",
    "    inside = (a <= x) & (x <= b)\n",
    "    logpdf[inside] = -np.log(b - a)\n",
    "    return logpdf\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "xs = np.array([-1.0, 0.0, 0.5, 1.0, 2.0])\n",
    "a, b = 0.0, 1.0\n",
    "print(\"pdf:\", uniform_pdf(xs, a, b))\n",
    "print(\"cdf:\", uniform_cdf(xs, a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92a0c3",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "Let $X \\sim \\mathrm{Uniform}(a,b)$ and define the width $w = b-a > 0$.\n",
    "\n",
    "### Moments\n",
    "- **Mean**:\n",
    "  \\[\\mathbb{E}[X] = \f",
    "\n",
    "rac{a+b}{2}.\\]\n",
    "- **Variance**:\n",
    "  \\[\\mathrm{Var}(X) = \f",
    "\n",
    "rac{(b-a)^2}{12} = \f",
    "\n",
    "rac{w^2}{12}.\\]\n",
    "- **Skewness**: $0$ (symmetric around the midpoint)\n",
    "- **(Excess) kurtosis**: $-\tfrac{6}{5}$ (thinner tails than a normal)\n",
    "\n",
    "### MGF and characteristic function\n",
    "- **MGF** (all real $t$):\n",
    "\\[\n",
    "M_X(t)=\\mathbb{E}[e^{tX}] = \begin{cases}\n",
    "\f",
    "\n",
    "rac{e^{tb}-e^{ta}}{t(b-a)}, & t\n",
    "e 0\\\n",
    "1, & t=0.\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "- **Characteristic function**:\n",
    "\\[\n",
    "\u000b",
    "\n",
    "arphi_X(t)=\\mathbb{E}[e^{itX}] = \f",
    "\n",
    "rac{e^{itb}-e^{ita}}{it(b-a)}\\quad (t\n",
    "e 0),\\qquad \u000b",
    "\n",
    "arphi_X(0)=1.\n",
    "\\]\n",
    "\n",
    "### Entropy (differential, in nats)\n",
    "\\[\n",
    "H(X) = \\ln(b-a) = \\ln w.\n",
    "\\]\n",
    "\n",
    "### Other notable properties\n",
    "- **Maximum entropy** on $[a,b]$\n",
    "- **Affine invariance**: if $Y=cX+d$ with $c>0$, then $Y\\sim\\mathrm{Uniform}(ca+d, cb+d)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c70ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_mean(a: float, b: float) -> float:\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "    return 0.5 * (a + b)\n",
    "\n",
    "\n",
    "def uniform_var(a: float, b: float) -> float:\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "    w = b - a\n",
    "    return (w * w) / 12.0\n",
    "\n",
    "\n",
    "def uniform_mgf(t: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    MGF using a numerically stable expm1 form.\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    w = b - a\n",
    "\n",
    "    out = np.empty_like(t, dtype=float)\n",
    "    near0 = np.isclose(t, 0.0)\n",
    "\n",
    "    out[near0] = 1.0\n",
    "    tt = t[~near0]\n",
    "    out[~near0] = np.exp(tt * a) * np.expm1(tt * w) / (tt * w)\n",
    "    return out\n",
    "\n",
    "\n",
    "def uniform_cf(t: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    Characteristic function.\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    w = b - a\n",
    "\n",
    "    out = np.empty_like(t, dtype=complex)\n",
    "    near0 = np.isclose(t, 0.0)\n",
    "    out[near0] = 1.0 + 0.0j\n",
    "\n",
    "    tt = t[~near0]\n",
    "    out[~near0] = np.exp(1j * tt * a) * np.expm1(1j * tt * w) / (1j * tt * w)\n",
    "    return out\n",
    "\n",
    "\n",
    "def uniform_entropy(a: float, b: float) -> float:\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "    return float(np.log(b - a))\n",
    "\n",
    "\n",
    "a, b = -2.0, 3.0\n",
    "n = 200_000\n",
    "x = a + (b - a) * rng.random(n)  # NumPy-only sampling\n",
    "\n",
    "mu_hat = float(np.mean(x))\n",
    "var_hat = float(np.var(x))\n",
    "\n",
    "centered = x - mu_hat\n",
    "skew_hat = float(np.mean(centered**3) / (var_hat ** 1.5))\n",
    "exkurt_hat = float(np.mean(centered**4) / (var_hat**2) - 3.0)\n",
    "\n",
    "print(\"theory mean:\", uniform_mean(a, b), \" sample:\", mu_hat)\n",
    "print(\"theory var :\", uniform_var(a, b), \" sample:\", var_hat)\n",
    "print(\"theory skew:\", 0.0, \" sample:\", skew_hat)\n",
    "print(\"theory ex-kurt:\", -6/5, \" sample:\", exkurt_hat)\n",
    "print(\"entropy (nats):\", uniform_entropy(a, b))\n",
    "\n",
    "# MGF check at a few t values\n",
    "for t0 in [0.0, 0.2, -0.3]:\n",
    "    mgf_mc = float(np.mean(np.exp(t0 * x)))\n",
    "    mgf_th = float(uniform_mgf(np.array([t0]), a, b)[0])\n",
    "    print(f\"t={t0:+.1f}  mgf theory={mgf_th:.6f}  mc={mgf_mc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ef4df",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The parameters are literal **bounds**:\n",
    "- $a$ is the lower limit; $b$ is the upper limit.\n",
    "- The distribution is **flat** on $[a,b]$ with height $1/(b-a)$.\n",
    "\n",
    "Useful derived quantities:\n",
    "- **Midpoint** $m = (a+b)/2$ sets the location (the mean).\n",
    "- **Width** $w = b-a$ controls dispersion and uncertainty:\n",
    "  - variance grows like $w^2/12$\n",
    "  - entropy grows like $\\ln w$\n",
    "\n",
    "Changing $(a,b)$ only **shifts** and **stretches** the interval; it does not change the “shape” (it always remains a rectangle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [(-1, 1), (0, 1), (0, 3)]\n",
    "xs = np.linspace(-2.5, 3.5, 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "for a, b in intervals:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=xs,\n",
    "            y=uniform_pdf(xs, a, b),\n",
    "            mode=\"lines\",\n",
    "            name=f\"a={a}, b={b}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Uniform PDF for different intervals\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92864f7",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "Using $f(x)=1/(b-a)$ on $[a,b]$:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\int_a^b x\\,\f",
    "\n",
    "rac{1}{b-a}\\,dx\n",
    "= \f",
    "\n",
    "rac{1}{b-a}\\left[\f",
    "\n",
    "rac{x^2}{2}\r\n",
    "ight]_a^b\n",
    "= \f",
    "\n",
    "rac{b^2-a^2}{2(b-a)}\n",
    "= \f",
    "\n",
    "rac{a+b}{2}.\n",
    "\\]\n",
    "\n",
    "### 6.2 Variance\n",
    "First compute $\\mathbb{E}[X^2]$:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X^2] = \\int_a^b x^2\\,\f",
    "\n",
    "rac{1}{b-a}\\,dx\n",
    "= \f",
    "\n",
    "rac{1}{b-a}\\left[\f",
    "\n",
    "rac{x^3}{3}\r\n",
    "ight]_a^b\n",
    "= \f",
    "\n",
    "rac{b^3-a^3}{3(b-a)}\n",
    "= \f",
    "\n",
    "rac{a^2+ab+b^2}{3}.\n",
    "\\]\n",
    "\n",
    "Then\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n",
    "= \f",
    "\n",
    "rac{a^2+ab+b^2}{3} - \\left(\f",
    "\n",
    "rac{a+b}{2}\r\n",
    "ight)^2\n",
    "= \f",
    "\n",
    "rac{(b-a)^2}{12}.\n",
    "\\]\n",
    "\n",
    "### 6.3 Likelihood and MLE\n",
    "For i.i.d. data $x_1,\\dots,x_n$:\n",
    "\n",
    "\\[\n",
    "L(a,b) = \\prod_{i=1}^n f(x_i\\mid a,b)\n",
    "= \\left(\f",
    "\n",
    "rac{1}{b-a}\r\n",
    "ight)^n \\mathbf{1}\\{a \\le x_{(1)},\\; b \\ge x_{(n)}\\},\n",
    "\\]\n",
    "\n",
    "where $x_{(1)}=\\min_i x_i$ and $x_{(n)}=\\max_i x_i$.\n",
    "\n",
    "So the log-likelihood (when the indicator is 1) is\n",
    "\\[\n",
    "\\ell(a,b) = -n\\ln(b-a).\n",
    "\\]\n",
    "\n",
    "To maximize it you want $b-a$ as small as possible while still containing the data, giving the **MLE**:\n",
    "\\[\n",
    "\\hat a = x_{(1)},\\qquad \\hat b = x_{(n)}.\n",
    "\\]\n",
    "\n",
    "This also explains why `fit` for the uniform distribution is **extremely sensitive to outliers**: the likelihood depends only on the min and max.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the MLE geometry\n",
    "true_a, true_b = 2.0, 5.0\n",
    "x = true_a + (true_b - true_a) * rng.random(200)\n",
    "\n",
    "a_hat = float(np.min(x))\n",
    "b_hat = float(np.max(x))\n",
    "\n",
    "print(\"true (a,b):\", (true_a, true_b))\n",
    "print(\"MLE  (a,b):\", (a_hat, b_hat))\n",
    "\n",
    "# Compare to SciPy's fit (maps to loc, scale)\n",
    "loc_hat, scale_hat = stats.uniform.fit(x)\n",
    "print(\"scipy fit loc, scale:\", (float(loc_hat), float(scale_hat)))\n",
    "print(\"scipy fit a,b:\", (float(loc_hat), float(loc_hat + scale_hat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e8efc0",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "### Inverse transform sampling\n",
    "The CDF on $[a,b]$ is $F(x)=(x-a)/(b-a)$. If $U\\sim\\mathrm{Uniform}(0,1)$ and we set $U = F(X)$, we get:\n",
    "\n",
    "\\[\n",
    "U = \f",
    "\n",
    "rac{X-a}{b-a}\\quad\\Rightarrow\\quad X = a + (b-a)U.\n",
    "\\]\n",
    "\n",
    "Algorithm:\n",
    "1) draw $U$ uniformly on $[0,1)$\n",
    "2) return $X=a+(b-a)U$\n",
    "\n",
    "`rng.random(size)` gives samples in $[0,1)$, which is perfect for continuous sampling (endpoint inclusion is a probability-zero event).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1124f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_uniform(a: float, b: float, size: int | tuple[int, ...], rng: np.random.Generator) -> np.ndarray:\n",
    "    NumPy-only sampler for Uniform(a,b).\n",
    "    if not (a < b):\n",
    "        raise ValueError(\"Require a < b\")\n",
    "    return a + (b - a) * rng.random(size)\n",
    "\n",
    "\n",
    "a, b = -1.0, 2.0\n",
    "x = sample_uniform(a, b, size=10_000, rng=rng)\n",
    "print(\"sample mean:\", float(np.mean(x)), \" theory:\", uniform_mean(a, b))\n",
    "print(\"sample var :\", float(np.var(x)), \" theory:\", uniform_var(a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670f82",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PDF** (flat “rectangle”)\n",
    "- the **CDF** (a linear ramp from 0 to 1)\n",
    "- a **Monte Carlo** histogram compared to the analytic PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26920801",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0.0, 1.5\n",
    "xs = np.linspace(-0.5, 2.0, 800)\n",
    "\n",
    "# PDF\n",
    "fig_pdf = go.Figure(\n",
    "    data=[go.Scatter(x=xs, y=uniform_pdf(xs, a, b), mode=\"lines\", name=\"PDF\")]\n",
    ")\n",
    "fig_pdf.update_layout(title=\"Uniform PDF\", xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "fig_pdf.show()\n",
    "\n",
    "# CDF\n",
    "fig_cdf = go.Figure(\n",
    "    data=[go.Scatter(x=xs, y=uniform_cdf(xs, a, b), mode=\"lines\", name=\"CDF\")]\n",
    ")\n",
    "fig_cdf.update_layout(title=\"Uniform CDF\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo samples\n",
    "n = 8_000\n",
    "samples = sample_uniform(a, b, size=n, rng=rng)\n",
    "\n",
    "hist = px.histogram(samples, nbins=40, histnorm=\"probability density\", title=\"Monte Carlo samples\")\n",
    "hist.add_trace(go.Scatter(x=xs, y=uniform_pdf(xs, a, b), mode=\"lines\", name=\"PDF (theory)\"))\n",
    "hist.update_layout(xaxis_title=\"x\", yaxis_title=\"density\")\n",
    "hist.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259406d",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.uniform`)\n",
    "\n",
    "SciPy parameterizes the uniform as:\n",
    "\n",
    "\\[\n",
    "X \\sim \texttt{stats.uniform}(\text{loc}, \text{scale})\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "X \\sim \\mathrm{Uniform}(a,b)\\;\text{with}\\; a=\text{loc},\\; b=\text{loc}+\text{scale}.\n",
    "\\]\n",
    "\n",
    "Common methods:\n",
    "- `pdf(x)`, `cdf(x)`\n",
    "- `rvs(size, random_state=...)`\n",
    "- `fit(data)` (MLE for `loc` and `scale`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503fe340",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = -2.0, 1.0\n",
    "rv = stats.uniform(loc=a, scale=b - a)\n",
    "\n",
    "xs = np.linspace(-3.0, 2.0, 400)\n",
    "\n",
    "print(\"pdf at 0:\", float(rv.pdf(0.0)))\n",
    "print(\"cdf at 0:\", float(rv.cdf(0.0)))\n",
    "\n",
    "# Sampling\n",
    "s = rv.rvs(size=5, random_state=SEED)\n",
    "print(\"rvs:\", s)\n",
    "\n",
    "# Fitting\n",
    "data = rv.rvs(size=300, random_state=123)\n",
    "loc_hat, scale_hat = stats.uniform.fit(data)\n",
    "print(\"fit loc, scale:\", (float(loc_hat), float(scale_hat)))\n",
    "print(\"fit interval :\", (float(loc_hat), float(loc_hat + scale_hat)))\n",
    "\n",
    "# Visual comparison: analytic vs SciPy\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=uniform_pdf(xs, a, b), mode=\"lines\", name=\"PDF (ours)\"))\n",
    "fig.add_trace(go.Scatter(x=xs, y=rv.pdf(xs), mode=\"lines\", name=\"PDF (SciPy)\", line=dict(dash=\"dash\")))\n",
    "fig.update_layout(title=\"PDF: our implementation vs SciPy\", xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80bf85",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing\n",
    "- **P-values under a true null**: if $H_0$ is true and the test is calibrated, then $p \\sim \\mathrm{Uniform}(0,1)$.\n",
    "- **Testing for uniformity**: the Kolmogorov–Smirnov test compares the empirical CDF to $F(x)=x$ on $[0,1]$.\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "- **Bounded priors**: $\theta\\sim\\mathrm{Uniform}(a,b)$ is a simple prior when $\theta$ is known to be in $[a,b]$.\n",
    "- **Caution**: “uniform” is not invariant to reparameterization (uniform in $\theta$ is not uniform in $\\log\theta$). For scale parameters, it’s common to consider log-uniform or Jeffreys-type priors instead.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "- Uniform noise is a common *base distribution*.\n",
    "- With a transform $X = g(U)$ you can generate complex distributions; inverse-CDF sampling is the special case $g=F^{-1}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-values are Uniform(0,1) under a true null (illustration)\n",
    "# We'll repeatedly test whether N(0,1) data has mean 0 (true).\n",
    "\n",
    "m = 10_000\n",
    "n = 25\n",
    "\n",
    "x = rng.normal(loc=0.0, scale=1.0, size=(m, n))\n",
    "res = stats.ttest_1samp(x, popmean=0.0, axis=1)\n",
    "pvals = res.pvalue\n",
    "\n",
    "# Visualize histogram against the Uniform(0,1) PDF (which equals 1 on [0,1])\n",
    "fig = px.histogram(pvals, nbins=40, histnorm=\"probability density\", title=\"Histogram of p-values under H0\")\n",
    "fig.add_hline(y=1.0, line_dash=\"dash\", line_color=\"black\", annotation_text=\"Uniform(0,1) density = 1\")\n",
    "fig.update_layout(xaxis_title=\"p-value\", yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "# KS test for uniformity\n",
    "ks = stats.kstest(pvals, \"uniform\")\n",
    "print(\"KS statistic:\", float(ks.statistic), \" p-value:\", float(ks.pvalue))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbd27f",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Continuous vs discrete**: “uniform distribution” can mean a discrete uniform on $\\{1,\\dots,k\\}$ or a continuous uniform on $[a,b]$.\n",
    "- **Invalid parameters**: must have $a<b$ (SciPy: `scale>0`). The case $a=b$ is a degenerate distribution (a point mass), not a continuous uniform.\n",
    "- **Outliers dominate `fit`**: MLE uses only the sample min and max.\n",
    "- **Not automatically “uninformative”**: a uniform prior depends on the chosen parameterization.\n",
    "- **Numerical issues**: when $b-a$ is extremely small, the density $1/(b-a)$ is huge; log-likelihood can be very large and optimization can be unstable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88a7b1",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- $X\\sim\\mathrm{Uniform}(a,b)$ is the canonical continuous distribution on a bounded interval.\n",
    "- PDF is constant $1/(b-a)$ on $[a,b]$; CDF is a linear ramp.\n",
    "- Mean $(a+b)/2$, variance $(b-a)^2/12$, entropy $\\ln(b-a)$.\n",
    "- Sampling is just **scale + shift** of $U\\sim\\mathrm{Uniform}(0,1)$.\n",
    "- The MLE for $(a,b)$ is $(\\min x_i, \\max x_i)$, which makes fitting sensitive to outliers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}