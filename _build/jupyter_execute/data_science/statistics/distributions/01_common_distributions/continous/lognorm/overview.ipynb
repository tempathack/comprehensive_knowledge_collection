{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc1d60f",
   "metadata": {},
   "source": [
    "# Lognormal Distribution (`lognorm`)\n",
    "\n",
    "The lognormal distribution is the canonical model for **positive, right-skewed** quantities that arise from **multiplicative** effects.\n",
    "\n",
    "If a variable is built as a product of many small random factors, its logarithm often becomes approximately normal (by a CLT-like argument on sums), making the original variable approximately **lognormal**.\n",
    "\n",
    "## What you’ll learn\n",
    "- what `lognorm` models and when it’s a good choice\n",
    "- the PDF/CDF in clean LaTeX form\n",
    "- closed-form moments (mean/variance/skewness/kurtosis) and what *doesn’t* have a closed form (MGF/CF)\n",
    "- how \\(\\mu,\\sigma\\) control location and tail heaviness\n",
    "- core derivations: \\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), likelihood + MLE\n",
    "- NumPy-only sampling and Monte Carlo checks\n",
    "- visual intuition via PDF/CDF/histograms\n",
    "- the SciPy API (`scipy.stats.lognorm`) and its parameterization\n",
    "- practical use cases + common pitfalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bf966",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & classification\n",
    "2) Intuition & motivation\n",
    "3) Formal definition (PDF/CDF)\n",
    "4) Moments & properties\n",
    "5) Parameter interpretation\n",
    "6) Derivations (\\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), likelihood)\n",
    "7) Sampling & simulation (NumPy-only)\n",
    "8) Visualization (PDF, CDF, Monte Carlo)\n",
    "9) SciPy integration (`scipy.stats.lognorm`)\n",
    "10) Statistical use cases\n",
    "11) Pitfalls\n",
    "12) Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b340d1",
   "metadata": {},
   "source": [
    "## Prerequisites & notation\n",
    "\n",
    "**Prerequisites**\n",
    "- comfort with basic calculus (change of variables)\n",
    "- basic probability (PDF/CDF, expectation)\n",
    "\n",
    "**Notation (2-parameter lognormal)**\n",
    "\n",
    "We use the standard two-parameter definition:\n",
    "\n",
    "- \\(X \\sim \\mathrm{LogNormal}(\\mu,\\sigma^2)\\) means \\(\\log X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\n",
    "- \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma>0\\).\n",
    "\n",
    "A helpful identity to remember:\n",
    "\n",
    "\\[\n",
    "X = \\exp(\\mu + \\sigma Z), \\qquad Z\\sim\\mathcal{N}(0,1).\n",
    "\\]\n",
    "\n",
    "**Mapping to SciPy**\n",
    "\n",
    "SciPy parameterizes `lognorm` as:\n",
    "\n",
    "- `scipy.stats.lognorm(s=σ, loc=0, scale=exp(μ))`\n",
    "\n",
    "So:\n",
    "\n",
    "\\[\n",
    "\\texttt{s} = \\sigma, \\qquad \\texttt{scale} = e^{\\mu}, \\qquad \\texttt{loc}=0 \\;\\text{(standard)}.\n",
    "\\]\n",
    "\n",
    "If `loc` is nonzero, the support becomes \\(x>\\texttt{loc}\\) and the distribution is a shifted lognormal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccebbbf",
   "metadata": {},
   "source": [
    "## 1) Title & classification\n",
    "\n",
    "- **Name**: `lognorm` (Lognormal distribution)\n",
    "- **Type**: **continuous**\n",
    "- **Support**: \\(x \\in (0,\\infty)\\) (standard 2-parameter form)\n",
    "- **Parameter space**:\n",
    "  - \\(\\mu \\in \\mathbb{R}\\)\n",
    "  - \\(\\sigma \\in (0,\\infty)\\)\n",
    "\n",
    "A 3-parameter (shifted) lognormal uses an additional location parameter \\(\\mathrm{loc}\\), giving support \\(x>\\mathrm{loc}\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd7842",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "A lognormal random variable is positive and typically **right-skewed**. It is appropriate when variability is best thought of as **multiplicative** rather than additive.\n",
    "\n",
    "If\n",
    "\n",
    "\\[\n",
    "X = X_0 \\prod_{j=1}^m U_j,\n",
    "\\]\n",
    "\n",
    "then\n",
    "\n",
    "\\[\n",
    "\\log X = \\log X_0 + \\sum_{j=1}^m \\log U_j,\n",
    "\\]\n",
    "\n",
    "and sums of many small, weakly dependent contributions often look approximately normal — making \\(X\\) approximately lognormal.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Finance**: asset prices under geometric Brownian motion (log-returns are modeled as normal)\n",
    "- **Reliability / survival**: positive durations with multiplicative heterogeneity (lognormal competes with Weibull/Gamma)\n",
    "- **Environmental / biomedical**: concentrations, exposure levels, positive measurements spanning orders of magnitude\n",
    "- **Measurement error**: multiplicative noise (e.g., \\(Y = X \\times \\varepsilon\\) with \\(\\varepsilon\\) lognormal)\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Normal**: \\(\\log X\\) is normal; many inference tasks reduce to normal theory on \\(\\log X\\).\n",
    "- **Products**: product of independent lognormals is lognormal (logs add).\n",
    "- **Gamma/Weibull**: alternative positive skewed families; lognormal often has a heavier right tail than Gamma/Weibull for comparable variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587427a0",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "Let \\(X \\sim \\mathrm{LogNormal}(\\mu,\\sigma^2)\\) with \\(\\sigma>0\\).\n",
    "\n",
    "Equivalently, let \\(Y=\\log X\\). Then \\(Y\\sim\\mathcal{N}(\\mu,\\sigma^2)\\).\n",
    "\n",
    "### PDF\n",
    "For \\(x>0\\):\n",
    "\n",
    "\\[\n",
    " f(x\\mid\\mu,\\sigma) = \\frac{1}{x\\,\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right).\n",
    "\\]\n",
    "\n",
    "And \\(f(x\\mid\\mu,\\sigma)=0\\) for \\(x\\le 0\\).\n",
    "\n",
    "### CDF\n",
    "For \\(x>0\\):\n",
    "\n",
    "\\[\n",
    " F(x\\mid\\mu,\\sigma) = \\mathbb{P}(X\\le x) = \\Phi\\!\\left(\\frac{\\ln x-\\mu}{\\sigma}\\right),\n",
    "\\]\n",
    "\n",
    "where \\(\\Phi\\) is the standard normal CDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea865bf8",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "A very useful closed form is the raw (power) moment:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X^k] = \\exp\\left(k\\mu + \\tfrac{1}{2}k^2\\sigma^2\\right), \\qquad k\\in\\mathbb{R}.\n",
    "\\]\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "Let \\(X\\sim\\mathrm{LogNormal}(\\mu,\\sigma^2)\\). Then:\n",
    "\n",
    "- **Mean**:\n",
    "\\[\n",
    "\\mathbb{E}[X]=\\exp\\left(\\mu + \\tfrac{1}{2}\\sigma^2\\right)\n",
    "\\]\n",
    "\n",
    "- **Variance**:\n",
    "\\[\n",
    "\\mathrm{Var}(X)=\\bigl(e^{\\sigma^2}-1\\bigr)\\,\\exp\\left(2\\mu+\\sigma^2\\right)\n",
    "\\]\n",
    "\n",
    "- **Skewness**:\n",
    "\\[\n",
    "\\gamma_1 = \\bigl(e^{\\sigma^2}+2\\bigr)\\sqrt{e^{\\sigma^2}-1}\n",
    "\\]\n",
    "\n",
    "- **(Excess) kurtosis**:\n",
    "\\[\n",
    "\\gamma_2 = e^{4\\sigma^2}+2e^{3\\sigma^2}+3e^{2\\sigma^2}-6\n",
    "\\]\n",
    "\n",
    "Other useful summaries:\n",
    "- **Median**: \\(\\mathrm{med}(X)=e^{\\mu}\\)\n",
    "- **Mode**: \\(\\mathrm{mode}(X)=e^{\\mu-\\sigma^2}\\)\n",
    "- **Quantile**: \\(Q(p)=\\exp\\bigl(\\mu+\\sigma\\,\\Phi^{-1}(p)\\bigr)\\)\n",
    "\n",
    "### MGF / characteristic function\n",
    "- The **MGF** \\(M_X(t)=\\mathbb{E}[e^{tX}]\\) does **not** exist (is infinite) for any \\(t>0\\).\n",
    "- For \\(t<0\\), the Laplace transform \\(\\mathbb{E}[e^{tX}]\\) exists, but it has **no simple elementary closed form**.\n",
    "- The **characteristic function** \\(\\varphi_X(\\omega)=\\mathbb{E}[e^{i\\omega X}]\\) exists for all real \\(\\omega\\), but also has **no elementary closed form**.\n",
    "\n",
    "In practice, you typically work with moments, quantiles, or compute transforms numerically.\n",
    "\n",
    "### Entropy (differential, in nats)\n",
    "Using the change-of-variables relation between \\(X\\) and \\(Y=\\log X\\):\n",
    "\n",
    "\\[\n",
    " h(X) = \\mu + \\tfrac{1}{2}\\log(2\\pi e\\,\\sigma^2).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SQRT_2PI = 0.5 * math.log(2 * math.pi)\n",
    "\n",
    "\n",
    "def lognorm_logpdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"Lognormal log-PDF for x>0; returns -inf for x<=0.\n",
    "\n",
    "    Parameterization: log X ~ Normal(mu, sigma^2).\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if sigma <= 0:\n",
    "        return np.full_like(x, -np.inf, dtype=float)\n",
    "\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "    if np.any(mask):\n",
    "        logx = np.log(x[mask])\n",
    "        z = (logx - mu) / sigma\n",
    "        out[mask] = -logx - math.log(sigma) - LOG_SQRT_2PI - 0.5 * (z * z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def lognorm_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "    return np.exp(lognorm_logpdf(x, mu=mu, sigma=sigma))\n",
    "\n",
    "\n",
    "def lognorm_cdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if sigma <= 0:\n",
    "        raise ValueError(\"sigma must be > 0\")\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x > 0\n",
    "    if np.any(mask):\n",
    "        z = (np.log(x[mask]) - mu) / sigma\n",
    "        out[mask] = stats.norm.cdf(z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def lognorm_ppf(p: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    if np.any((p <= 0) | (p >= 1)):\n",
    "        raise ValueError(\"p must be in (0,1)\")\n",
    "    if sigma <= 0:\n",
    "        raise ValueError(\"sigma must be > 0\")\n",
    "    return np.exp(mu + sigma * stats.norm.ppf(p))\n",
    "\n",
    "\n",
    "def lognorm_raw_moment(k: float, mu: float, sigma: float) -> float:\n",
    "    # E[X^k] = exp(k*mu + 0.5*k^2*sigma^2)\n",
    "    return math.exp(k * mu + 0.5 * (k * k) * (sigma * sigma))\n",
    "\n",
    "\n",
    "def lognorm_mean(mu: float, sigma: float) -> float:\n",
    "    return lognorm_raw_moment(1.0, mu=mu, sigma=sigma)\n",
    "\n",
    "\n",
    "def lognorm_var(mu: float, sigma: float) -> float:\n",
    "    m1 = lognorm_raw_moment(1.0, mu=mu, sigma=sigma)\n",
    "    m2 = lognorm_raw_moment(2.0, mu=mu, sigma=sigma)\n",
    "    return m2 - m1 * m1\n",
    "\n",
    "\n",
    "def lognorm_skewness(sigma: float) -> float:\n",
    "    # depends only on sigma\n",
    "    a = math.exp(sigma * sigma)\n",
    "    return (a + 2.0) * math.sqrt(a - 1.0)\n",
    "\n",
    "\n",
    "def lognorm_excess_kurtosis(sigma: float) -> float:\n",
    "    s2 = sigma * sigma\n",
    "    return math.exp(4 * s2) + 2 * math.exp(3 * s2) + 3 * math.exp(2 * s2) - 6\n",
    "\n",
    "\n",
    "def lognorm_entropy(mu: float, sigma: float) -> float:\n",
    "    return mu + 0.5 * math.log(2 * math.pi * math.e * sigma * sigma)\n",
    "\n",
    "\n",
    "def sample_lognorm(n: int, mu: float, sigma: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"NumPy-only sampling via X = exp(mu + sigma Z), Z~N(0,1).\"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be >= 1\")\n",
    "    if sigma <= 0:\n",
    "        raise ValueError(\"sigma must be > 0\")\n",
    "    z = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "    return np.exp(mu + sigma * z)\n",
    "\n",
    "\n",
    "def lognorm_loglik(mu: float, sigma: float, x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if sigma <= 0 or np.any(x <= 0):\n",
    "        return -np.inf\n",
    "    return float(lognorm_logpdf(x, mu=mu, sigma=sigma).sum())\n",
    "\n",
    "\n",
    "# Quick Monte Carlo sanity check of moments\n",
    "mu, sigma = 0.3, 0.7\n",
    "x_mc = sample_lognorm(200_000, mu=mu, sigma=sigma, rng=rng)\n",
    "\n",
    "mean_emp = x_mc.mean()\n",
    "var_emp = x_mc.var()\n",
    "\n",
    "mean_th = lognorm_mean(mu, sigma)\n",
    "var_th = lognorm_var(mu, sigma)\n",
    "\n",
    "{\n",
    "    \"mean_emp\": mean_emp,\n",
    "    \"mean_theory\": mean_th,\n",
    "    \"var_emp\": var_emp,\n",
    "    \"var_theory\": var_th,\n",
    "    \"log_mean_emp\": float(np.log(x_mc).mean()),\n",
    "    \"log_std_emp\": float(np.log(x_mc).std(ddof=0)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1bd12",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "Recall \\(\\log X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\n",
    "\n",
    "### Meaning of parameters\n",
    "- \\(\\mu\\) is the **location in log-space**:\n",
    "  - \\(\\mathrm{median}(X)=e^{\\mu}\\)\n",
    "  - multiplying \\(X\\) by a constant \\(c>0\\) adds \\(\\log c\\) to \\(\\mu\\)\n",
    "- \\(\\sigma\\) is the **spread in log-space**:\n",
    "  - controls right-tail heaviness, skewness, and how far the mean sits above the median\n",
    "  - increasing \\(\\sigma\\) leaves the median fixed but inflates \\(\\mathbb{E}[X]=e^{\\mu+\\sigma^2/2}\\)\n",
    "\n",
    "### Shape changes (qualitative)\n",
    "- Larger \\(\\mu\\): shifts the distribution to the right (multiplicative scaling).\n",
    "- Larger \\(\\sigma\\): increases dispersion and skewness; the mode moves left (\\(e^{\\mu-\\sigma^2}\\)) while the tail gets much heavier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08603492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the PDF changes with mu and sigma\n",
    "\n",
    "mu0 = 0.0\n",
    "sigmas = [0.25, 0.5, 1.0]\n",
    "\n",
    "x_min = float(lognorm_ppf(0.001, mu=mu0, sigma=min(sigmas)))\n",
    "x_max = float(lognorm_ppf(0.995, mu=mu0, sigma=max(sigmas)))\n",
    "x_grid = np.linspace(0.0, x_max, 700)\n",
    "\n",
    "fig = go.Figure()\n",
    "for s in sigmas:\n",
    "    fig.add_trace(go.Scatter(x=x_grid, y=lognorm_pdf(x_grid, mu=mu0, sigma=s), mode=\"lines\", name=f\"σ={s:g}\"))\n",
    "    fig.add_vline(x=math.exp(mu0), line_dash=\"dot\", opacity=0.25)  # median\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Lognormal PDF: increasing σ increases skew and tail weight (μ fixed)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Changing mu mostly rescales x\n",
    "mu_values = [-0.8, 0.0, 0.8]\n",
    "sigma = 0.5\n",
    "\n",
    "x_max = float(lognorm_ppf(0.995, mu=max(mu_values), sigma=sigma))\n",
    "x_grid = np.linspace(0.0, x_max, 700)\n",
    "\n",
    "fig = go.Figure()\n",
    "for m in mu_values:\n",
    "    fig.add_trace(go.Scatter(x=x_grid, y=lognorm_pdf(x_grid, mu=m, sigma=sigma), mode=\"lines\", name=f\"μ={m:g}\"))\n",
    "    fig.add_vline(x=math.exp(m), line_dash=\"dot\", opacity=0.25)  # median\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Lognormal PDF: changing μ shifts the scale (σ fixed)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a97e4a",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### A) Expectation\n",
    "Start from the definition (for \\(x>0\\)):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\int_0^\\infty x\\,\\frac{1}{x\\,\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right)dx.\n",
    "\\]\n",
    "\n",
    "Cancel the \\(x\\) and substitute \\(y=\\ln x\\) (so \\(x=e^y\\), \\(dx=e^y dy\\)):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)e^y\\,dy\n",
    "= \\mathbb{E}[e^Y],\\quad Y\\sim\\mathcal{N}(\\mu,\\sigma^2).\n",
    "\\]\n",
    "\n",
    "Using the normal MGF \\(\\mathbb{E}[e^{tY}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)\\), set \\(t=1\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\exp\\left(\\mu + \\tfrac{1}{2}\\sigma^2\\right).\n",
    "\\]\n",
    "\n",
    "### B) Variance\n",
    "Similarly, compute \\(\\mathbb{E}[X^2]=\\exp(2\\mu+2\\sigma^2)\\) and subtract the square of the mean:\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2\n",
    "=\\exp(2\\mu+2\\sigma^2)-\\exp(2\\mu+\\sigma^2)\n",
    "=\\bigl(e^{\\sigma^2}-1\\bigr)\\exp(2\\mu+\\sigma^2).\n",
    "\\]\n",
    "\n",
    "### C) Likelihood and MLE\n",
    "For iid data \\(x_1,\\dots,x_n\\) with \\(x_i>0\\), the log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell(\\mu,\\sigma) = -n\\log\\sigma - n\\tfrac{1}{2}\\log(2\\pi) - \\sum_{i=1}^n \\log x_i - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (\\log x_i-\\mu)^2.\n",
    "\\]\n",
    "\n",
    "Let \\(y_i=\\log x_i\\). Then this is exactly the normal log-likelihood for \\(y_i\\sim\\mathcal{N}(\\mu,\\sigma^2)\\) plus the Jacobian term \\(-\\sum\\log x_i\\) which does not depend on \\(\\mu\\) or \\(\\sigma\\).\n",
    "\n",
    "Therefore the MLEs are the same as for the normal:\n",
    "\n",
    "\\[\n",
    "\\hat\\mu = \\bar{y},\\qquad \\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i-\\bar{y})^2.\n",
    "\\]\n",
    "\n",
    "(Notice the \\(1/n\\), not \\(1/(n-1)\\): MLE vs unbiased variance estimator.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac661b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE demo: estimate (mu, sigma) from simulated data\n",
    "\n",
    "mu_true, sigma_true = 0.4, 0.8\n",
    "n = 800\n",
    "x = sample_lognorm(n, mu=mu_true, sigma=sigma_true, rng=rng)\n",
    "\n",
    "y = np.log(x)\n",
    "mu_hat = float(y.mean())\n",
    "sigma_hat = float(y.std(ddof=0))\n",
    "\n",
    "loglik_true = lognorm_loglik(mu_true, sigma_true, x)\n",
    "loglik_mle = lognorm_loglik(mu_hat, sigma_hat, x)\n",
    "\n",
    "{\n",
    "    \"mu_true\": mu_true,\n",
    "    \"mu_hat\": mu_hat,\n",
    "    \"sigma_true\": sigma_true,\n",
    "    \"sigma_hat\": sigma_hat,\n",
    "    \"loglik_true\": loglik_true,\n",
    "    \"loglik_mle\": loglik_mle,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debb4ab",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation (NumPy-only)\n",
    "\n",
    "Because \\(\\log X\\) is normal, sampling is extremely simple:\n",
    "\n",
    "1) sample \\(Z\\sim\\mathcal{N}(0,1)\\)\n",
    "2) return \\(X = \\exp(\\mu + \\sigma Z)\\)\n",
    "\n",
    "This is exact (not an approximation), and uses only a normal RNG plus exponentiation.\n",
    "\n",
    "**Why it works:** If \\(Z\\sim\\mathcal{N}(0,1)\\) then \\(\\mu+\\sigma Z\\sim\\mathcal{N}(\\mu,\\sigma^2)\\), and exponentiating turns a normal into a lognormal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation check: log(samples) should look Normal(mu, sigma^2)\n",
    "\n",
    "mu, sigma = 0.3, 0.7\n",
    "n = 50_000\n",
    "samples = sample_lognorm(n, mu=mu, sigma=sigma, rng=rng)\n",
    "log_samples = np.log(samples)\n",
    "\n",
    "z = (log_samples - mu) / sigma\n",
    "\n",
    "# Histogram of log-samples + normal PDF overlay\n",
    "grid = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 500)\n",
    "\n",
    "fig = px.histogram(\n",
    "    log_samples,\n",
    "    nbins=70,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"log(X) is normal: histogram of log-samples\",\n",
    "    labels={\"value\": \"y = log(x)\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=grid, y=stats.norm.pdf(grid, loc=mu, scale=sigma), mode=\"lines\", name=\"Normal PDF\"))\n",
    "fig.update_layout(yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"log_mean_emp\": float(log_samples.mean()),\n",
    "    \"log_std_emp\": float(log_samples.std(ddof=0)),\n",
    "    \"z_mean_emp\": float(z.mean()),\n",
    "    \"z_std_emp\": float(z.std(ddof=0)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca9022",
   "metadata": {},
   "source": [
    "## 8) Visualization (PDF, CDF, Monte Carlo)\n",
    "\n",
    "Because lognormal tails can span orders of magnitude, it’s often useful to look at:\n",
    "\n",
    "- PDF on a **linear** x-axis (to see the mode)\n",
    "- CDF (to see where most mass lies)\n",
    "- Monte Carlo samples (histogram) compared to the theoretical PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo samples vs theoretical PDF\n",
    "\n",
    "mu, sigma = 0.2, 0.9\n",
    "n = 80_000\n",
    "samples = sample_lognorm(n, mu=mu, sigma=sigma, rng=rng)\n",
    "\n",
    "# Plot most of the mass (truncate extreme tail for visualization)\n",
    "x_max = float(np.quantile(samples, 0.995))\n",
    "x_grid = np.linspace(0.0, x_max, 600)\n",
    "\n",
    "fig = px.histogram(\n",
    "    samples[samples <= x_max],\n",
    "    nbins=80,\n",
    "    histnorm=\"probability density\",\n",
    "    title=f\"Lognormal: samples vs PDF (n={n}, μ={mu:g}, σ={sigma:g})\",\n",
    "    labels={\"value\": \"x (tail truncated at 99.5% quantile)\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=lognorm_pdf(x_grid, mu=mu, sigma=sigma), mode=\"lines\", name=\"true PDF\"))\n",
    "fig.update_layout(yaxis_title=\"density\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19635ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF and CDF for multiple sigmas\n",
    "\n",
    "mu = 0.0\n",
    "sigmas = [0.25, 0.5, 1.0]\n",
    "\n",
    "x_max = float(lognorm_ppf(0.995, mu=mu, sigma=max(sigmas)))\n",
    "x_grid = np.linspace(0.0, x_max, 700)\n",
    "\n",
    "fig_pdf = go.Figure()\n",
    "fig_cdf = go.Figure()\n",
    "\n",
    "for s in sigmas:\n",
    "    fig_pdf.add_trace(go.Scatter(x=x_grid, y=lognorm_pdf(x_grid, mu=mu, sigma=s), mode=\"lines\", name=f\"σ={s:g}\"))\n",
    "    fig_cdf.add_trace(go.Scatter(x=x_grid, y=lognorm_cdf(x_grid, mu=mu, sigma=s), mode=\"lines\", name=f\"σ={s:g}\"))\n",
    "\n",
    "fig_pdf.update_layout(title=\"Lognormal PDF (μ fixed)\", xaxis_title=\"x\", yaxis_title=\"f(x)\")\n",
    "fig_cdf.update_layout(title=\"Lognormal CDF (μ fixed)\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "\n",
    "fig_pdf.show()\n",
    "fig_cdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical CDF vs true CDF\n",
    "\n",
    "mu, sigma = 0.2, 0.9\n",
    "n = 30_000\n",
    "samples = sample_lognorm(n, mu=mu, sigma=sigma, rng=rng)\n",
    "\n",
    "xs = np.sort(samples)\n",
    "ys = np.arange(1, n + 1) / n\n",
    "\n",
    "x_grid = np.linspace(0.0, float(np.quantile(xs, 0.995)), 700)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", name=\"empirical CDF\"))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=lognorm_cdf(x_grid, mu=mu, sigma=sigma), mode=\"lines\", name=\"true CDF\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Empirical CDF vs true CDF (n={n}, μ={mu:g}, σ={sigma:g})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fd4e5",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.lognorm`)\n",
    "\n",
    "SciPy’s `lognorm` distribution uses:\n",
    "\n",
    "- `s` = \\(\\sigma\\) (shape)\n",
    "- `scale` = \\(e^{\\mu}\\)\n",
    "- `loc` = location shift (0 for the standard lognormal)\n",
    "\n",
    "So if \\(X\\sim\\mathrm{LogNormal}(\\mu,\\sigma^2)\\), then:\n",
    "\n",
    "```python\n",
    "rv = stats.lognorm(s=sigma, loc=0.0, scale=math.exp(mu))\n",
    "```\n",
    "\n",
    "Methods you’ll commonly use:\n",
    "- `rv.pdf(x)`, `rv.logpdf(x)`\n",
    "- `rv.cdf(x)`\n",
    "- `rv.rvs(size=..., random_state=...)`\n",
    "- `stats.lognorm.fit(data, floc=0.0)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99509848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "\n",
    "mu, sigma = 0.2, 0.9\n",
    "rv = lognorm(s=sigma, loc=0.0, scale=math.exp(mu))\n",
    "\n",
    "x_grid = np.linspace(0.0, float(lognorm_ppf(0.995, mu=mu, sigma=sigma)), 600)\n",
    "\n",
    "# Compare our PDF/CDF with SciPy\n",
    "pdf_max_abs_diff = float(np.max(np.abs(rv.pdf(x_grid) - lognorm_pdf(x_grid, mu=mu, sigma=sigma))))\n",
    "cdf_max_abs_diff = float(np.max(np.abs(rv.cdf(x_grid) - lognorm_cdf(x_grid, mu=mu, sigma=sigma))))\n",
    "\n",
    "# Sampling\n",
    "samples_scipy = rv.rvs(size=20_000, random_state=rng)\n",
    "\n",
    "# Fit (standard lognormal: fix loc=0)\n",
    "shape_hat, loc_hat, scale_hat = lognorm.fit(samples_scipy, floc=0.0)\n",
    "mu_hat = math.log(scale_hat)\n",
    "sigma_hat = shape_hat\n",
    "\n",
    "{\n",
    "    \"pdf_max_abs_diff\": pdf_max_abs_diff,\n",
    "    \"cdf_max_abs_diff\": cdf_max_abs_diff,\n",
    "    \"fit_loc_hat\": float(loc_hat),\n",
    "    \"fit_mu_hat\": mu_hat,\n",
    "    \"fit_sigma_hat\": sigma_hat,\n",
    "    \"true_mu\": mu,\n",
    "    \"true_sigma\": sigma,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672eae10",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### A) Hypothesis testing\n",
    "A common workflow is:\n",
    "\n",
    "1) transform data with \\(y_i=\\log x_i\\)\n",
    "2) check whether \\(y_i\\) is plausibly normal (QQ plot / normality tests)\n",
    "3) if reasonable, use normal-theory inference for \\(\\mu,\\sigma\\) on \\(y\\)\n",
    "\n",
    "Example tests:\n",
    "- **Normality on log-data**: Shapiro-Wilk, Anderson-Darling, etc.\n",
    "- **Testing the median**: since \\(\\mathrm{median}(X)=e^{\\mu}\\), testing a median corresponds to testing \\(\\mu\\) on \\(\\log X\\).\n",
    "\n",
    "### B) Bayesian modeling\n",
    "A lognormal likelihood is often convenient in hierarchical models for positive outcomes.\n",
    "\n",
    "In log-space, \\(y_i=\\log x_i\\) is normal, so you can use conjugate priors like the **Normal-Inverse-Gamma** for \\((\\mu,\\sigma^2)\\).\n",
    "\n",
    "### C) Generative modeling\n",
    "Lognormal noise is a natural choice for **multiplicative perturbations**:\n",
    "\n",
    "\\[\n",
    "X_{\\text{observed}} = X_{\\text{true}} \\times \\varepsilon, \\qquad \\varepsilon\\sim\\mathrm{LogNormal}(0,\\tau^2)\n",
    "\\]\n",
    "\n",
    "It also composes nicely: products of independent lognormals are lognormal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Hypothesis testing ideas via log-transform\n",
    "\n",
    "mu, sigma = 0.0, 0.7\n",
    "n = 500\n",
    "x = sample_lognorm(n, mu=mu, sigma=sigma, rng=rng)\n",
    "y = np.log(x)\n",
    "\n",
    "# 1) Normality test on y = log(x)\n",
    "shapiro_stat, shapiro_p = stats.shapiro(y)\n",
    "\n",
    "# 2) Testing the median m0: median(X)=exp(mu) -> H0: mu = log(m0)\n",
    "m0 = math.exp(mu)  # true median in this simulation\n",
    "mu0 = math.log(m0)\n",
    "t_stat, t_p = stats.ttest_1samp(y, popmean=mu0)\n",
    "\n",
    "# 3) QQ plot for y against Normal\n",
    "(osm, osr), (slope, intercept, r) = stats.probplot(y, dist=\"norm\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=osm, y=osr, mode=\"markers\", name=\"log-data quantiles\"))\n",
    "line_x = np.array([osm.min(), osm.max()])\n",
    "fig.add_trace(go.Scatter(x=line_x, y=intercept + slope * line_x, mode=\"lines\", name=\"fit line\"))\n",
    "fig.update_layout(\n",
    "    title=\"QQ plot: log(x) vs Normal\",\n",
    "    xaxis_title=\"theoretical quantiles\",\n",
    "    yaxis_title=\"sample quantiles of log(x)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"shapiro_stat\": float(shapiro_stat),\n",
    "    \"shapiro_p\": float(shapiro_p),\n",
    "    \"t_test_stat_for_mu\": float(t_stat),\n",
    "    \"t_test_p\": float(t_p),\n",
    "    \"qq_r\": float(r),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28091068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Bayesian modeling in log-space: Normal-Inverse-Gamma prior\n",
    "\n",
    "# Model: y_i = log x_i ~ Normal(mu, sigma^2)\n",
    "\n",
    "# Prior hyperparameters\n",
    "mu0 = 0.0\n",
    "kappa0 = 1.0\n",
    "alpha0 = 2.0\n",
    "beta0 = 1.0\n",
    "\n",
    "# Simulated data\n",
    "mu_true, sigma_true = 0.3, 0.6\n",
    "n = 200\n",
    "x = sample_lognorm(n, mu=mu_true, sigma=sigma_true, rng=rng)\n",
    "y = np.log(x)\n",
    "\n",
    "y_bar = float(y.mean())\n",
    "ssq = float(((y - y_bar) ** 2).sum())\n",
    "\n",
    "# Posterior update (Normal-Inverse-Gamma)\n",
    "kappa_n = kappa0 + n\n",
    "mu_n = (kappa0 * mu0 + n * y_bar) / kappa_n\n",
    "alpha_n = alpha0 + 0.5 * n\n",
    "beta_n = beta0 + 0.5 * ssq + (kappa0 * n * (y_bar - mu0) ** 2) / (2 * kappa_n)\n",
    "\n",
    "# Sample from posterior\n",
    "M = 20_000\n",
    "sigma2_samps = stats.invgamma(a=alpha_n, scale=beta_n).rvs(size=M, random_state=rng)\n",
    "mu_samps = rng.normal(loc=mu_n, scale=np.sqrt(sigma2_samps / kappa_n))\n",
    "\n",
    "# Posterior for the median m = exp(mu)\n",
    "median_samps = np.exp(mu_samps)\n",
    "\n",
    "ci_95 = np.quantile(median_samps, [0.025, 0.975])\n",
    "\n",
    "fig = px.histogram(\n",
    "    median_samps,\n",
    "    nbins=80,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Posterior for median exp(mu) under N-Inv-Gamma prior (log-space)\",\n",
    "    labels={\"value\": \"median = exp(mu)\"},\n",
    ")\n",
    "fig.add_vline(x=math.exp(mu_true), line_dash=\"dot\", opacity=0.5)\n",
    "fig.add_vline(x=float(ci_95[0]), line_dash=\"dash\", opacity=0.35)\n",
    "fig.add_vline(x=float(ci_95[1]), line_dash=\"dash\", opacity=0.35)\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"posterior_mu_mean\": float(mu_samps.mean()),\n",
    "    \"posterior_sigma_mean\": float(np.sqrt(sigma2_samps).mean()),\n",
    "    \"median_true\": math.exp(mu_true),\n",
    "    \"median_95_CI\": [float(ci_95[0]), float(ci_95[1])],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c32713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Generative modeling: products of lognormals are lognormal\n",
    "\n",
    "# If X1 ~ LN(mu1, s1^2) and X2 ~ LN(mu2, s2^2) independently,\n",
    "# then X1*X2 ~ LN(mu1+mu2, s1^2 + s2^2).\n",
    "\n",
    "mu1, s1 = 0.2, 0.4\n",
    "mu2, s2 = -0.1, 0.7\n",
    "\n",
    "n = 120_000\n",
    "x1 = sample_lognorm(n, mu=mu1, sigma=s1, rng=rng)\n",
    "x2 = sample_lognorm(n, mu=mu2, sigma=s2, rng=rng)\n",
    "prod = x1 * x2\n",
    "\n",
    "mu_pred = mu1 + mu2\n",
    "sigma_pred = math.sqrt(s1 * s1 + s2 * s2)\n",
    "\n",
    "# Compare moments in log-space\n",
    "log_prod = np.log(prod)\n",
    "\n",
    "# Visual check: histogram vs predicted PDF (truncate tail)\n",
    "x_max = float(np.quantile(prod, 0.995))\n",
    "x_grid = np.linspace(0.0, x_max, 700)\n",
    "\n",
    "fig = px.histogram(\n",
    "    prod[prod <= x_max],\n",
    "    nbins=90,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Product of independent lognormals is lognormal (empirical vs theory)\",\n",
    "    labels={\"value\": \"x (tail truncated at 99.5% quantile)\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=lognorm_pdf(x_grid, mu=mu_pred, sigma=sigma_pred), mode=\"lines\", name=\"predicted PDF\"))\n",
    "fig.update_layout(yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"mu_pred\": mu_pred,\n",
    "    \"sigma_pred\": sigma_pred,\n",
    "    \"log_mean_emp\": float(log_prod.mean()),\n",
    "    \"log_std_emp\": float(log_prod.std(ddof=0)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3167667",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Nonpositive data**: the standard lognormal requires \\(x>0\\). Zeros/negatives break the \\(\\log\\) transform and make likelihood \\(-\\infty\\).\n",
    "- **Parameterization confusion**:\n",
    "  - many texts use \\((\\mu,\\sigma)\\) for the normal parameters of \\(\\log X\\)\n",
    "  - SciPy uses `lognorm(s=σ, scale=exp(μ), loc=...)`\n",
    "- **Tail truncation in plots**: lognormal tails can be huge; for readability it’s common to truncate at a high quantile (e.g., 99.5%).\n",
    "- **Overflow/underflow**:\n",
    "  - sampling uses `exp(mu + sigma*z)` which can overflow if \\(\\mu+\\sigma z\\) is too large\n",
    "  - evaluating the PDF can underflow for extreme \\(x\\) or large \\(\\sigma\\); prefer `logpdf` for likelihood work\n",
    "- **Fitting with `loc` free**: allowing `loc` to vary can yield a shifted fit; if you expect the standard lognormal, use `floc=0.0`.\n",
    "- **MGF-based methods**: the MGF diverges for \\(t>0\\), so techniques relying on an MGF neighborhood around 0 can fail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc88cce",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `lognorm` is **continuous** on \\((0,\\infty)\\) and is defined by \\(\\log X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\).\n",
    "- PDF: \\(f(x)=\\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\bigl(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\bigr)\\), CDF: \\(F(x)=\\Phi\\bigl((\\ln x-\\mu)/\\sigma\\bigr)\\).\n",
    "- Raw moments: \\(\\mathbb{E}[X^k]=\\exp(k\\mu+\\tfrac{1}{2}k^2\\sigma^2)\\); in particular \\(\\mathbb{E}[X]=e^{\\mu+\\sigma^2/2}\\) and \\(\\mathrm{Var}(X)=(e^{\\sigma^2}-1)e^{2\\mu+\\sigma^2}\\).\n",
    "- Median \\(=e^{\\mu}\\), mode \\(=e^{\\mu-\\sigma^2}\\); increasing \\(\\sigma\\) dramatically increases skewness and tail heaviness.\n",
    "- MLEs are normal-theory MLEs on \\(y=\\log x\\): \\(\\hat\\mu=\\bar y\\), \\(\\hat\\sigma^2=\\frac{1}{n}\\sum(y_i-\\bar y)^2\\).\n",
    "- SciPy mapping: `stats.lognorm(s=sigma, loc=0, scale=exp(mu))` and `lognorm.fit(data, floc=0)` for the standard case.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}