{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9859b1",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (MLPs) for Tabular Data\n",
    "\n",
    "An **artificial neural network** for tabular data is usually a **multi-layer perceptron (MLP)**: stacks of `Linear` layers with nonlinear activations (ReLU, GELU, …).\n",
    "\n",
    "MLPs are a great learning tool because you can understand them end-to-end:\n",
    "\n",
    "- a forward pass is just matrix multiplications + activations\n",
    "- training is “just” gradient descent on a loss (via backprop)\n",
    "\n",
    "On many real-world tabular problems, **tree-based models** (XGBoost/LightGBM/CatBoost) are often the strongest baseline; MLPs tend to shine when you have **lots of data**, **learned embeddings** for categorical features, or you need to combine tabular with other modalities.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end, you should be able to:\n",
    "\n",
    "- explain how an MLP turns features into predictions\n",
    "- implement a 2-layer MLP in **NumPy** (forward + backprop)\n",
    "- train it with mini-batch SGD and visualize learning curves\n",
    "- build the same model in **PyTorch** and compare results\n",
    "- diagnose common tabular-MLP pitfalls (scaling, overfitting, LR)\n",
    "\n",
    "## Notation (quick)\n",
    "\n",
    "- Features: $X \\in \\mathbb{R}^{n\\times d}$ (rows are samples)\n",
    "- Labels (binary): $y \\in \\{0,1\\}^n$\n",
    "- First layer: $z_1 = XW_1 + b_1$, $a_1 = \\mathrm{ReLU}(z_1)$\n",
    "- Output logits: $\\ell = a_1W_2 + b_2$ (probability via sigmoid)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. What makes tabular data special?\n",
    "2. A tiny nonlinear dataset + why scaling matters\n",
    "3. Baseline: logistic regression (linear boundary)\n",
    "4. From scratch: a 2-layer MLP in NumPy\n",
    "5. Practical: the same model in PyTorch\n",
    "6. Compare models + diagnostics\n",
    "7. Practical tips for real tabular data\n",
    "8. Exercises + references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "import warnings\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"CUDA initialization*\", category=UserWarning)\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "if has_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0212521",
   "metadata": {},
   "source": [
    "## 1) What makes tabular data special?\n",
    "\n",
    "Tabular data usually means:\n",
    "\n",
    "- each row is an entity (customer, transaction, patient)\n",
    "- columns are heterogeneous features (numeric + categorical + missing)\n",
    "\n",
    "Compared to images/text, tabular datasets are often smaller and noisier, and the “right” inductive bias is less obvious.\n",
    "\n",
    "For MLPs specifically, two habits matter a lot:\n",
    "\n",
    "- **standardize numeric features** (helps optimization)\n",
    "- treat **categorical features** carefully (often via embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b4444",
   "metadata": {},
   "source": [
    "## 2) A tiny nonlinear dataset + why scaling matters\n",
    "\n",
    "We’ll use a simple 2D dataset so we can visualize the decision boundary.\n",
    "\n",
    "Even though it’s 2D, it’s still “tabular”: each row is a sample, and the two columns are features.\n",
    "\n",
    "To make the scaling issue obvious, we’ll intentionally stretch one feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "n_samples = 2000\n",
    "X_raw, y = make_moons(n_samples=n_samples, noise=0.25, random_state=SEED)\n",
    "\n",
    "# Force a scale mismatch (common in real tabular datasets)\n",
    "X_raw = X_raw.astype(np.float64)\n",
    "X_raw[:, 1] *= 3.0\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# Train/val/test split\n",
    "X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "    X_raw,\n",
    "    y,\n",
    "    test_size=0.30,\n",
    "    random_state=SEED,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "    X_temp_raw,\n",
    "    y_temp,\n",
    "    test_size=0.50,\n",
    "    random_state=SEED,\n",
    "    stratify=y_temp,\n",
    ")\n",
    "\n",
    "# Standardize using train split only\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_val = scaler.transform(X_val_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=X_raw[:, 0],\n",
    "    y=X_raw[:, 1],\n",
    "    color=y.astype(str),\n",
    "    title=\"Raw features (note the scale mismatch)\",\n",
    "    labels={\"x\": \"feature_1\", \"y\": \"feature_2\", \"color\": \"class\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.7))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = scaler.transform(X_raw)\n",
    "fig = px.scatter(\n",
    "    x=X_all[:, 0],\n",
    "    y=X_all[:, 1],\n",
    "    color=y.astype(str),\n",
    "    title=\"Standardized features (zero mean, unit variance)\",\n",
    "    labels={\"x\": \"z(feature_1)\", \"y\": \"z(feature_2)\", \"color\": \"class\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.7))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary_figure(X2d, y, prob_fn, title, grid_n=250, pad=0.6):\n",
    "    X2d = np.asarray(X2d)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    x0_min, x0_max = X2d[:, 0].min() - pad, X2d[:, 0].max() + pad\n",
    "    x1_min, x1_max = X2d[:, 1].min() - pad, X2d[:, 1].max() + pad\n",
    "\n",
    "    xs = np.linspace(x0_min, x0_max, grid_n)\n",
    "    ys = np.linspace(x1_min, x1_max, grid_n)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    probs = prob_fn(grid).reshape(xx.shape)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Probability surface\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=xs,\n",
    "            y=ys,\n",
    "            z=probs,\n",
    "            zmin=0.0,\n",
    "            zmax=1.0,\n",
    "            colorscale=\"RdBu\",\n",
    "            reversescale=True,\n",
    "            opacity=0.75,\n",
    "            colorbar=dict(title=\"P(class=1)\"),\n",
    "            contours=dict(start=0.0, end=1.0, size=0.1),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Decision boundary line at 0.5\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=xs,\n",
    "            y=ys,\n",
    "            z=probs,\n",
    "            contours=dict(start=0.5, end=0.5, size=0.5, coloring=\"lines\"),\n",
    "            line=dict(color=\"black\", width=3),\n",
    "            showscale=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X2d[:, 0],\n",
    "            y=X2d[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=y, colorscale=\"Viridis\", size=5, opacity=0.75),\n",
    "            name=\"data\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"feature_1 (standardized)\",\n",
    "        yaxis_title=\"feature_2 (standardized)\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    )\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579551df",
   "metadata": {},
   "source": [
    "## 3) Baseline: logistic regression (linear boundary)\n",
    "\n",
    "Logistic regression is a **linear** classifier: it can only draw a single straight line in 2D.\n",
    "\n",
    "Our dataset needs a curved boundary, so logistic regression should underfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58034d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=2000, random_state=SEED)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "def eval_sklearn_binary(model, X, y):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    preds = (probs >= 0.5).astype(np.int64)\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y, preds)),\n",
    "        \"logloss\": float(log_loss(y, probs)),\n",
    "    }\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"train\": eval_sklearn_binary(log_reg, X_train, y_train),\n",
    "    \"val\": eval_sklearn_binary(log_reg, X_val, y_val),\n",
    "    \"test\": eval_sklearn_binary(log_reg, X_test, y_test),\n",
    "}\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = decision_boundary_figure(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    prob_fn=lambda X: log_reg.predict_proba(X)[:, 1],\n",
    "    title=\"Logistic regression decision boundary (linear)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932d1ec",
   "metadata": {},
   "source": [
    "## 4) From scratch: a 2-layer MLP in NumPy\n",
    "\n",
    "A 2-layer MLP is:\n",
    "\n",
    "1. a linear layer that mixes the input features\n",
    "2. a nonlinearity (ReLU)\n",
    "3. another linear layer to produce a logit\n",
    "\n",
    "Even this small network can produce a **piecewise-linear** decision boundary that bends around the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a1b1d",
   "metadata": {},
   "source": [
    "### Forward pass (binary classification)\n",
    "\n",
    "Hidden layer:\n",
    "\n",
    "$$\n",
    "z_1 = XW_1 + b_1\\quad\\Rightarrow\\quad a_1 = \\mathrm{ReLU}(z_1)\n",
    "$$\n",
    "\n",
    "Output logit:\n",
    "\n",
    "$$\n",
    "\\ell = a_1 W_2 + b_2\n",
    "$$\n",
    "\n",
    "Probability:\n",
    "\n",
    "$$\n",
    "p = \\sigma(\\ell) = \\frac{1}{1 + e^{-\\ell}}\n",
    "$$\n",
    "\n",
    "Loss (binary cross-entropy, computed stably from logits):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{n}\\sum_i \\left[\\log(1+e^{\\ell_i}) - y_i\\ell_i\\right]\n",
    "$$\n",
    "\n",
    "Key gradient fact:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = \\sigma(\\ell) - y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d66e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def bce_with_logits_loss(logits, y):\n",
    "    \"\"\"Mean binary cross-entropy, but computed stably from logits.\n",
    "\n",
    "    logits: (n, 1)\n",
    "    y:      (n, 1) in {0,1}\n",
    "    \"\"\"\n",
    "    logits = np.asarray(logits)\n",
    "    y = np.asarray(y)\n",
    "    return float((np.logaddexp(0.0, logits) - y * logits).mean())\n",
    "\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    probs = sigmoid(logits)\n",
    "    preds = (probs >= 0.5).astype(np.int64)\n",
    "    return float((preds.ravel() == y.ravel()).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp(in_dim, hidden_dim, rng):\n",
    "    \"\"\"He initialization is a good default for ReLU networks.\"\"\"\n",
    "    W1 = rng.normal(0.0, np.sqrt(2.0 / in_dim), size=(in_dim, hidden_dim))\n",
    "    b1 = np.zeros((hidden_dim,), dtype=np.float64)\n",
    "\n",
    "    W2 = rng.normal(0.0, np.sqrt(2.0 / hidden_dim), size=(hidden_dim, 1))\n",
    "    b2 = np.zeros((1,), dtype=np.float64)\n",
    "\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "\n",
    "def mlp_forward(X, params):\n",
    "    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
    "\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    logits = a1 @ W2 + b2\n",
    "\n",
    "    cache = {\"X\": X, \"z1\": z1, \"a1\": a1}\n",
    "    return logits, cache\n",
    "\n",
    "\n",
    "def mlp_loss_and_grads(X, y, params, weight_decay=0.0):\n",
    "    \"\"\"Return loss and gradients for a 2-layer MLP.\"\"\"\n",
    "    y = y.reshape(-1, 1).astype(np.float64)\n",
    "    logits, cache = mlp_forward(X, params)\n",
    "\n",
    "    loss = bce_with_logits_loss(logits, y)\n",
    "    if weight_decay:\n",
    "        loss += 0.5 * weight_decay * (np.sum(params[\"W1\"] ** 2) + np.sum(params[\"W2\"] ** 2))\n",
    "\n",
    "    probs = sigmoid(logits)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # dL/dlogits = (sigmoid(logits) - y) / n\n",
    "    dlogits = (probs - y) / n\n",
    "\n",
    "    dW2 = cache[\"a1\"].T @ dlogits\n",
    "    db2 = dlogits.sum(axis=0)\n",
    "\n",
    "    da1 = dlogits @ params[\"W2\"].T\n",
    "    dz1 = da1 * (cache[\"z1\"] > 0.0)\n",
    "\n",
    "    dW1 = cache[\"X\"].T @ dz1\n",
    "    db1 = dz1.sum(axis=0)\n",
    "\n",
    "    if weight_decay:\n",
    "        dW1 = dW1 + weight_decay * params[\"W1\"]\n",
    "        dW2 = dW2 + weight_decay * params[\"W2\"]\n",
    "\n",
    "    grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "    return loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_numpy_mlp(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    *,\n",
    "    hidden_dim=32,\n",
    "    lr=0.1,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    weight_decay=1e-4,\n",
    "    seed=SEED,\n",
    "):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    params = init_mlp(in_dim=X_train.shape[1], hidden_dim=hidden_dim, rng=rng_local)\n",
    "\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "    }\n",
    "\n",
    "    y_train_col = y_train.reshape(-1, 1)\n",
    "    y_val_col = y_val.reshape(-1, 1)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        idx = rng_local.permutation(X_train.shape[0])\n",
    "\n",
    "        for start in range(0, X_train.shape[0], batch_size):\n",
    "            batch_idx = idx[start : start + batch_size]\n",
    "            Xb = X_train[batch_idx]\n",
    "            yb = y_train_col[batch_idx]\n",
    "\n",
    "            _, grads = mlp_loss_and_grads(Xb, yb, params, weight_decay=weight_decay)\n",
    "\n",
    "            params[\"W1\"] -= lr * grads[\"W1\"]\n",
    "            params[\"b1\"] -= lr * grads[\"b1\"]\n",
    "            params[\"W2\"] -= lr * grads[\"W2\"]\n",
    "            params[\"b2\"] -= lr * grads[\"b2\"]\n",
    "\n",
    "        train_logits, _ = mlp_forward(X_train, params)\n",
    "        val_logits, _ = mlp_forward(X_val, params)\n",
    "\n",
    "        train_loss = bce_with_logits_loss(train_logits, y_train_col)\n",
    "        val_loss = bce_with_logits_loss(val_logits, y_val_col)\n",
    "\n",
    "        train_acc = accuracy_from_logits(train_logits, y_train_col)\n",
    "        val_acc = accuracy_from_logits(val_logits, y_val_col)\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    return params, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_np, hist_np = train_numpy_mlp(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    hidden_dim=32,\n",
    "    lr=0.1,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "def eval_numpy_mlp(params, X, y):\n",
    "    logits, _ = mlp_forward(X, params)\n",
    "    probs = sigmoid(logits).ravel()\n",
    "    preds = (probs >= 0.5).astype(np.int64)\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y, preds)),\n",
    "        \"logloss\": float(log_loss(y, probs)),\n",
    "    }\n",
    "\n",
    "numpy_metrics = {\n",
    "    \"train\": eval_numpy_mlp(params_np, X_train, y_train),\n",
    "    \"val\": eval_numpy_mlp(params_np, X_val, y_val),\n",
    "    \"test\": eval_numpy_mlp(params_np, X_test, y_test),\n",
    "}\n",
    "numpy_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist_np[\"epoch\"], y=hist_np[\"train_loss\"], name=\"train\"))\n",
    "fig.add_trace(go.Scatter(x=hist_np[\"epoch\"], y=hist_np[\"val_loss\"], name=\"val\"))\n",
    "fig.update_layout(\n",
    "    title=\"NumPy MLP: loss over epochs\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=\"binary cross-entropy\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4db7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist_np[\"epoch\"], y=hist_np[\"train_acc\"], name=\"train\"))\n",
    "fig.add_trace(go.Scatter(x=hist_np[\"epoch\"], y=hist_np[\"val_acc\"], name=\"val\"))\n",
    "fig.update_layout(\n",
    "    title=\"NumPy MLP: accuracy over epochs\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=\"accuracy\",\n",
    "    yaxis=dict(range=[0.0, 1.0]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = decision_boundary_figure(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    prob_fn=lambda X: sigmoid(mlp_forward(X, params_np)[0]).ravel(),\n",
    "    title=\"NumPy MLP decision boundary (nonlinear)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88549fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_np_test = sigmoid(mlp_forward(X_test, params_np)[0]).ravel()\n",
    "preds_np_test = (probs_np_test >= 0.5).astype(np.int64)\n",
    "cm = confusion_matrix(y_test, preds_np_test)\n",
    "\n",
    "fig = px.imshow(\n",
    "    cm,\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    title=\"NumPy MLP: confusion matrix (test)\",\n",
    "    labels=dict(x=\"predicted\", y=\"true\", color=\"count\"),\n",
    ")\n",
    "fig.update_xaxes(tickmode=\"array\", tickvals=[0, 1])\n",
    "fig.update_yaxes(tickmode=\"array\", tickvals=[0, 1])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa9ac2",
   "metadata": {},
   "source": [
    "## 5) Practical: the same model in PyTorch\n",
    "\n",
    "PyTorch gives you:\n",
    "\n",
    "- automatic differentiation (no manual backprop)\n",
    "- battle-tested optimizers (Adam, SGD+momentum)\n",
    "- easy batching with `DataLoader`\n",
    "\n",
    "We’ll build the *same* architecture and train it on the same standardized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=256, shuffle=False)\n",
    "\n",
    "torch_model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), lr=0.03, weight_decay=1e-4)\n",
    "\n",
    "def run_epoch(model, loader, *, train=False):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            total_correct += (preds == yb).float().sum().item()\n",
    "\n",
    "        total_loss += loss.item() * xb.shape[0]\n",
    "        n += xb.shape[0]\n",
    "\n",
    "    return total_loss / n, total_correct / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ede392",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_hist = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "epochs = 120\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_acc = run_epoch(torch_model, train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(torch_model, val_loader, train=False)\n",
    "\n",
    "    torch_hist[\"epoch\"].append(epoch)\n",
    "    torch_hist[\"train_loss\"].append(float(train_loss))\n",
    "    torch_hist[\"val_loss\"].append(float(val_loss))\n",
    "    torch_hist[\"train_acc\"].append(float(train_acc))\n",
    "    torch_hist[\"val_acc\"].append(float(val_acc))\n",
    "\n",
    "@torch.no_grad()\n",
    "def torch_predict_proba(model, X):\n",
    "    model.eval()\n",
    "    Xt = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    probs = torch.sigmoid(model(Xt)).detach().cpu().numpy().ravel()\n",
    "    return probs\n",
    "\n",
    "probs_torch_test = torch_predict_proba(torch_model, X_test)\n",
    "preds_torch_test = (probs_torch_test >= 0.5).astype(np.int64)\n",
    "\n",
    "torch_metrics = {\n",
    "    \"train\": {\n",
    "        \"acc\": float(accuracy_score(y_train, (torch_predict_proba(torch_model, X_train) >= 0.5).astype(np.int64))),\n",
    "        \"logloss\": float(log_loss(y_train, torch_predict_proba(torch_model, X_train))),\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"acc\": float(accuracy_score(y_val, (torch_predict_proba(torch_model, X_val) >= 0.5).astype(np.int64))),\n",
    "        \"logloss\": float(log_loss(y_val, torch_predict_proba(torch_model, X_val))),\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"acc\": float(accuracy_score(y_test, preds_torch_test)),\n",
    "        \"logloss\": float(log_loss(y_test, probs_torch_test)),\n",
    "    },\n",
    "}\n",
    "torch_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d969a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=torch_hist[\"epoch\"], y=torch_hist[\"train_loss\"], name=\"train\"))\n",
    "fig.add_trace(go.Scatter(x=torch_hist[\"epoch\"], y=torch_hist[\"val_loss\"], name=\"val\"))\n",
    "fig.update_layout(\n",
    "    title=\"PyTorch MLP: loss over epochs\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=\"binary cross-entropy\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=torch_hist[\"epoch\"], y=torch_hist[\"train_acc\"], name=\"train\"))\n",
    "fig.add_trace(go.Scatter(x=torch_hist[\"epoch\"], y=torch_hist[\"val_acc\"], name=\"val\"))\n",
    "fig.update_layout(\n",
    "    title=\"PyTorch MLP: accuracy over epochs\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=\"accuracy\",\n",
    "    yaxis=dict(range=[0.0, 1.0]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f46707",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = decision_boundary_figure(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    prob_fn=lambda X: torch_predict_proba(torch_model, X),\n",
    "    title=\"PyTorch MLP decision boundary (nonlinear)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, preds_torch_test)\n",
    "fig = px.imshow(\n",
    "    cm,\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    title=\"PyTorch MLP: confusion matrix (test)\",\n",
    "    labels=dict(x=\"predicted\", y=\"true\", color=\"count\"),\n",
    ")\n",
    "fig.update_xaxes(tickmode=\"array\", tickvals=[0, 1])\n",
    "fig.update_yaxes(tickmode=\"array\", tickvals=[0, 1])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a761d",
   "metadata": {},
   "source": [
    "## 6) Compare models + diagnostics\n",
    "\n",
    "On this toy dataset, both MLPs should learn a nonlinear boundary and outperform logistic regression.\n",
    "\n",
    "We’ll compare test accuracy and log loss (probabilistic quality).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6506cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"log_reg\", \"numpy_mlp\", \"torch_mlp\"]\n",
    "test_acc = [\n",
    "    baseline_metrics[\"test\"][\"acc\"],\n",
    "    numpy_metrics[\"test\"][\"acc\"],\n",
    "    torch_metrics[\"test\"][\"acc\"],\n",
    "]\n",
    "test_logloss = [\n",
    "    baseline_metrics[\"test\"][\"logloss\"],\n",
    "    numpy_metrics[\"test\"][\"logloss\"],\n",
    "    torch_metrics[\"test\"][\"logloss\"],\n",
    "]\n",
    "\n",
    "fig = go.Figure(go.Bar(x=models, y=test_acc))\n",
    "fig.update_layout(title=\"Test accuracy\", xaxis_title=\"model\", yaxis_title=\"accuracy\", yaxis=dict(range=[0.0, 1.0]))\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(go.Bar(x=models, y=test_logloss))\n",
    "fig.update_layout(title=\"Test log loss (lower is better)\", xaxis_title=\"model\", yaxis_title=\"log loss\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7003f",
   "metadata": {},
   "source": [
    "## 7) Practical tips for real tabular data\n",
    "\n",
    "- **Standardize numeric features** (and keep the scaler fitted on train only).\n",
    "- **Categorical features**: try learned embeddings (`nn.Embedding`) instead of one-hot for high-cardinality columns.\n",
    "- **Missing values**: add missingness indicators; don’t just impute and hope.\n",
    "- **Overfitting** is common: use weight decay, dropout, early stopping, and a strong validation protocol.\n",
    "- **Learning rate** matters more than architecture. When in doubt, sweep `lr` and use Adam.\n",
    "- **Baselines first**: compare against logistic regression and strong tree-based models.\n",
    "- **Calibration**: optimize log loss / calibration if your probabilities will drive decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2521d96",
   "metadata": {},
   "source": [
    "## 8) Exercises\n",
    "\n",
    "1. Add another hidden layer (2 hidden layers total). Does it help? Does it overfit?\n",
    "2. Replace ReLU with `tanh`. What changes in training speed / final accuracy?\n",
    "3. Implement **dropout** in the NumPy model.\n",
    "4. Turn this into a **multiclass** problem (softmax + cross-entropy).\n",
    "5. Try a real tabular dataset (e.g., UCI) and compare with a tree baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad2b08",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- PyTorch: https://pytorch.org/docs/stable/index.html\n",
    "- Goodfellow, Bengio, Courville — *Deep Learning* (MLP + backprop chapters)\n",
    "- scikit-learn MLPClassifier docs (for a practical baseline)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}