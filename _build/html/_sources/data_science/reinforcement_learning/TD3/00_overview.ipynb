{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed DDPG (TD3) â€” from scratch in PyTorch\n",
    "\n",
    "TD3 (Fujimoto, van Hoof, Meger, 2018) is a deterministic actor-critic algorithm for **continuous control**.\n",
    "It improves DDPG with three small but crucial modifications:\n",
    "\n",
    "1. **Twin critics**: learn two Q-functions and use the *minimum* in the bootstrap target.\n",
    "2. **Target policy smoothing**: add clipped noise to the target action when computing the target Q.\n",
    "3. **Delayed policy updates**: update the actor (and target networks) less often than the critics.\n",
    "\n",
    "In this notebook we:\n",
    "- write the TD3 update equations precisely (LaTeX)\n",
    "- implement TD3 at a **low level** in PyTorch (no RL libraries)\n",
    "- train on a Gymnasium environment and **plot episodic returns** (Plotly)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Understand why DDPG overestimates and how TD3 fixes it\n",
    "- Implement replay buffer + target networks + twin critics + delayed updates\n",
    "- Train a working agent and visualize learning curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Basic PyTorch (modules, optimizers, autograd)\n",
    "- Q-learning / bootstrapping and the Bellman equation\n",
    "- Actor-critic idea (policy + value function)\n",
    "- Continuous action spaces (e.g., Pendulum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TORCH_AVAILABLE = False\n",
    "    _TORCH_IMPORT_ERROR = e\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "\n",
    "    GYM_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    GYM_AVAILABLE = False\n",
    "    _GYM_IMPORT_ERROR = e\n",
    "\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run configuration ---\n",
    "FAST_RUN = True\n",
    "\n",
    "ENV_ID = 'Pendulum-v1'\n",
    "\n",
    "TOTAL_TIMESTEPS = 10_000 if FAST_RUN else 200_000\n",
    "START_STEPS = 1_000 if FAST_RUN else 10_000\n",
    "UPDATE_AFTER = 1_000 if FAST_RUN else 10_000\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 200_000\n",
    "\n",
    "# TD3 hyperparameters\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "\n",
    "ACTOR_LR = 1e-3\n",
    "CRITIC_LR = 1e-3\n",
    "\n",
    "POLICY_DELAY = 2\n",
    "TARGET_POLICY_NOISE = 0.2\n",
    "TARGET_NOISE_CLIP = 0.5\n",
    "\n",
    "EXPLORATION_NOISE = 0.1\n",
    "\n",
    "HIDDEN_SIZES = (256, 256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) TD3: the exact updates (twin critics + delayed actor)\n",
    "\n",
    "We use:\n",
    "\n",
    "- deterministic policy (actor)  $a = \\pi_\\phi(s)$\n",
    "- **two** critics  $Q_{\\theta_1}(s,a)$ and $Q_{\\theta_2}(s,a)$\n",
    "- target networks  $(\\phi', \\theta_1', \\theta_2')$ updated by Polyak averaging\n",
    "\n",
    "Given a transition $(s,a,r,s',\\text{terminal})$ sampled from the replay buffer, TD3 builds the target in three steps.\n",
    "\n",
    "### 1. Target policy smoothing\n",
    "\n",
    "TD3 does *not* evaluate the target critics at the raw target action $\\pi_{\\phi'}(s')$.\n",
    "Instead it adds clipped Gaussian noise:\n",
    "\n",
    "$$\n",
    "\\tilde a = \\pi_{\\phi'}(s') + \\epsilon,\\qquad\n",
    "\\epsilon \\sim \\mathrm{clip}(\\mathcal N(0, \\sigma^2),\\,-c,\\,+c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde a \\leftarrow \\mathrm{clip}(\\tilde a, a_{\\min}, a_{\\max})\n",
    "$$\n",
    "\n",
    "Intuition: this makes the target Q-value less sensitive to small action errors and prevents the critic from exploiting sharp, unrealistic peaks in $Q$.\n",
    "\n",
    "### 2. Twin critics (min target)\n",
    "\n",
    "Compute both target Q-values and take the **minimum**:\n",
    "\n",
    "$$\n",
    "y = r + \\gamma (1-\\text{terminal})\\,\\min\\Big( Q_{\\theta_1'}(s', \\tilde a),\\; Q_{\\theta_2'}(s', \\tilde a)\\Big)\n",
    "$$\n",
    "\n",
    "Each critic minimizes an MSE to this same target:\n",
    "\n",
    "$$\n",
    "L(\\theta_i) = \\mathbb E\\big[(Q_{\\theta_i}(s,a)-y)^2\\big],\\qquad i\\in\\{1,2\\}\n",
    "$$\n",
    "\n",
    "Taking the minimum is a simple bias-reduction trick: it turns DDPG's optimistic\n",
    "target into a more conservative estimate, reducing overestimation error.\n",
    "\n",
    "### 3. Delayed policy updates\n",
    "\n",
    "The critics are updated **every gradient step**.\n",
    "The actor is updated only every $d$ critic updates (e.g. $d=2$):\n",
    "\n",
    "$$\n",
    "\\max_\\phi\\; J(\\phi) = \\mathbb E\\big[ Q_{\\theta_1}(s, \\pi_\\phi(s)) \\big]\n",
    "$$\n",
    "\n",
    "In code we minimize the negative:\n",
    "\n",
    "$$\n",
    "L_\\pi(\\phi) = -\\mathbb E\\big[ Q_{\\theta_1}(s, \\pi_\\phi(s)) \\big]\n",
    "$$\n",
    "\n",
    "When (and only when) we update the actor, we also update **all** target networks with Polyak averaging:\n",
    "\n",
    "$$\n",
    "\\theta_i' \\leftarrow \\tau \\theta_i + (1-\\tau)\\theta_i',\\qquad\n",
    "\\phi' \\leftarrow \\tau \\phi + (1-\\tau)\\phi'\n",
    "$$\n",
    "\n",
    "Delaying the actor update lets the critics move closer to their fixed point, so the actor sees a less noisy / less biased gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Implementation roadmap\n",
    "\n",
    "We will implement TD3 as a small set of building blocks:\n",
    "\n",
    "1. Gymnasium environment helpers (reset/step API differences)\n",
    "2. Replay buffer (NumPy storage, PyTorch sampling)\n",
    "3. Actor network $\\pi_\\phi(s)$\n",
    "4. Twin critic networks $Q_{\\theta_1}(s,a), Q_{\\theta_2}(s,a)$\n",
    "5. TD3 update step (critic update every step, actor + target update every `POLICY_DELAY` steps)\n",
    "6. Training loop + Plotly learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    if TORCH_AVAILABLE:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def env_reset(env, seed=None):\n",
    "    out = env.reset(seed=seed) if seed is not None else env.reset()\n",
    "    if isinstance(out, tuple) and len(out) == 2:\n",
    "        obs, info = out\n",
    "        return obs, info\n",
    "    return out, {}\n",
    "\n",
    "\n",
    "def env_step(env, action):\n",
    "    out = env.step(action)\n",
    "    if isinstance(out, tuple) and len(out) == 5:\n",
    "        next_obs, reward, terminated, truncated, info = out\n",
    "        done = bool(terminated or truncated)\n",
    "        terminal = bool(terminated)  # time-limit truncation is not a terminal state\n",
    "        return next_obs, float(reward), done, terminal, info\n",
    "    if isinstance(out, tuple) and len(out) == 4:\n",
    "        next_obs, reward, done, info = out\n",
    "        terminal = bool(done)\n",
    "        return next_obs, float(reward), bool(done), terminal, info\n",
    "    raise ValueError('Unexpected env.step(...) output format')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TORCH_AVAILABLE:\n",
    "    raise RuntimeError(f'PyTorch import failed: {_TORCH_IMPORT_ERROR}')\n",
    "\n",
    "if not GYM_AVAILABLE:\n",
    "    raise RuntimeError(f'Gymnasium import failed: {_GYM_IMPORT_ERROR}')\n",
    "\n",
    "\n",
    "set_global_seeds(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "env = gym.make(ENV_ID)\n",
    "obs, _ = env_reset(env, seed=SEED)\n",
    "\n",
    "obs_dim = int(np.prod(env.observation_space.shape))\n",
    "act_dim = int(np.prod(env.action_space.shape))\n",
    "\n",
    "action_low = env.action_space.low.astype(np.float32)\n",
    "action_high = env.action_space.high.astype(np.float32)\n",
    "\n",
    "print('env:', ENV_ID)\n",
    "print('obs_dim:', obs_dim)\n",
    "print('act_dim:', act_dim)\n",
    "print('action_low:', action_low)\n",
    "print('action_high:', action_high)\n",
    "print('device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim: int, act_dim: int, size: int, seed: int, device: torch.device):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "        self.done_buf = np.zeros((size, 1), dtype=np.float32)\n",
    "\n",
    "        self.max_size = int(size)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, obs, act, rew: float, next_obs, terminal: bool) -> None:\n",
    "        self.obs_buf[self.ptr] = np.asarray(obs, dtype=np.float32).reshape(-1)\n",
    "        self.next_obs_buf[self.ptr] = np.asarray(next_obs, dtype=np.float32).reshape(-1)\n",
    "        self.act_buf[self.ptr] = np.asarray(act, dtype=np.float32).reshape(-1)\n",
    "        self.rew_buf[self.ptr] = float(rew)\n",
    "        self.done_buf[self.ptr] = 1.0 if terminal else 0.0\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        if self.size < batch_size:\n",
    "            raise ValueError(f'Not enough samples: size={self.size}, batch_size={batch_size}')\n",
    "        idxs = self.rng.integers(0, self.size, size=batch_size)\n",
    "\n",
    "        obs = torch.as_tensor(self.obs_buf[idxs], device=self.device)\n",
    "        act = torch.as_tensor(self.act_buf[idxs], device=self.device)\n",
    "        rew = torch.as_tensor(self.rew_buf[idxs], device=self.device)\n",
    "        next_obs = torch.as_tensor(self.next_obs_buf[idxs], device=self.device)\n",
    "        done = torch.as_tensor(self.done_buf[idxs], device=self.device)\n",
    "\n",
    "        return obs, act, rew, next_obs, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(layer_sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        act = activation if i < len(layer_sizes) - 2 else output_activation\n",
    "        layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes, action_low, action_high):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim, *hidden_sizes, act_dim], activation=nn.ReLU, output_activation=nn.Identity)\n",
    "\n",
    "        action_low_t = torch.as_tensor(action_low, dtype=torch.float32)\n",
    "        action_high_t = torch.as_tensor(action_high, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('action_scale', (action_high_t - action_low_t) / 2.0)\n",
    "        self.register_buffer('action_bias', (action_high_t + action_low_t) / 2.0)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        a = torch.tanh(self.net(obs))\n",
    "        return a * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim + act_dim, *hidden_sizes, 1], activation=nn.ReLU, output_activation=nn.Identity)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TwinCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.q1 = QNetwork(obs_dim, act_dim, hidden_sizes)\n",
    "        self.q2 = QNetwork(obs_dim, act_dim, hidden_sizes)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: torch.Tensor):\n",
    "        return self.q1(obs, act), self.q2(obs, act)\n",
    "\n",
    "    def q1_only(self, obs: torch.Tensor, act: torch.Tensor) -> torch.Tensor:\n",
    "        return self.q1(obs, act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        act_dim: int,\n",
    "        hidden_sizes,\n",
    "        action_low,\n",
    "        action_high,\n",
    "        device: torch.device,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 0.005,\n",
    "        actor_lr: float = 1e-3,\n",
    "        critic_lr: float = 1e-3,\n",
    "        policy_delay: int = 2,\n",
    "        target_policy_noise: float = 0.2,\n",
    "        target_noise_clip: float = 0.5,\n",
    "    ):\n",
    "        self.device = device\n",
    "\n",
    "        self.actor = Actor(obs_dim, act_dim, hidden_sizes, action_low, action_high).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor).to(device)\n",
    "\n",
    "        self.critic = TwinCritic(obs_dim, act_dim, hidden_sizes).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "\n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.gamma = float(gamma)\n",
    "        self.tau = float(tau)\n",
    "\n",
    "        self.policy_delay = int(policy_delay)\n",
    "        self.target_policy_noise = float(target_policy_noise)\n",
    "        self.target_noise_clip = float(target_noise_clip)\n",
    "\n",
    "        self.action_low_t = torch.as_tensor(action_low, dtype=torch.float32, device=device)\n",
    "        self.action_high_t = torch.as_tensor(action_high, dtype=torch.float32, device=device)\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "        # Targets start identical to online nets\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, obs, noise_scale: float = 0.0):\n",
    "        obs_t = torch.as_tensor(np.asarray(obs, dtype=np.float32).reshape(1, -1), device=self.device)\n",
    "        action = self.actor(obs_t).cpu().numpy().reshape(-1)\n",
    "        if noise_scale and noise_scale > 0:\n",
    "            action = action + np.random.normal(0.0, noise_scale, size=action.shape).astype(np.float32)\n",
    "        action = np.clip(action, self.action_low_t.cpu().numpy(), self.action_high_t.cpu().numpy())\n",
    "        return action\n",
    "\n",
    "    def _soft_update_(self, source: nn.Module, target: nn.Module) -> None:\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(source.parameters(), target.parameters()):\n",
    "                p_targ.data.mul_(1.0 - self.tau)\n",
    "                p_targ.data.add_(self.tau * p.data)\n",
    "\n",
    "    def train_step(self, replay_buffer: ReplayBuffer, batch_size: int):\n",
    "        self.total_it += 1\n",
    "\n",
    "        obs, act, rew, next_obs, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # --- Critic update (every step) ---\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn_like(act) * self.target_policy_noise\n",
    "            noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n",
    "\n",
    "            next_action = self.actor_target(next_obs) + noise\n",
    "            next_action = torch.max(torch.min(next_action, self.action_high_t), self.action_low_t)\n",
    "\n",
    "            target_q1, target_q2 = self.critic_target(next_obs, next_action)\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "\n",
    "            y = rew + (1.0 - done) * self.gamma * target_q\n",
    "\n",
    "        current_q1, current_q2 = self.critic(obs, act)\n",
    "        critic_loss = F.mse_loss(current_q1, y) + F.mse_loss(current_q2, y)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        info = {'critic_loss': float(critic_loss.item())}\n",
    "\n",
    "        # --- Delayed actor + target updates ---\n",
    "        if self.total_it % self.policy_delay == 0:\n",
    "            actor_loss = -self.critic.q1_only(obs, self.actor(obs)).mean()\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optim.step()\n",
    "\n",
    "            info['actor_loss'] = float(actor_loss.item())\n",
    "\n",
    "            self._soft_update_(self.critic, self.critic_target)\n",
    "            self._soft_update_(self.actor, self.actor_target)\n",
    "\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=BUFFER_SIZE, seed=SEED, device=device)\n",
    "agent = TD3Agent(\n",
    "    obs_dim=obs_dim,\n",
    "    act_dim=act_dim,\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    action_low=action_low,\n",
    "    action_high=action_high,\n",
    "    device=device,\n",
    "    gamma=GAMMA,\n",
    "    tau=TAU,\n",
    "    actor_lr=ACTOR_LR,\n",
    "    critic_lr=CRITIC_LR,\n",
    "    policy_delay=POLICY_DELAY,\n",
    "    target_policy_noise=TARGET_POLICY_NOISE,\n",
    "    target_noise_clip=TARGET_NOISE_CLIP,\n",
    ")\n",
    "\n",
    "episode_returns = []\n",
    "episode_lengths = []\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "\n",
    "obs, _ = env_reset(env, seed=SEED)\n",
    "ep_return = 0.0\n",
    "ep_len = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for t in range(TOTAL_TIMESTEPS):\n",
    "    if t < START_STEPS:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = agent.select_action(obs, noise_scale=EXPLORATION_NOISE)\n",
    "\n",
    "    next_obs, reward, done, terminal, _info = env_step(env, action)\n",
    "\n",
    "    replay.add(obs, action, reward, next_obs, terminal)\n",
    "\n",
    "    obs = next_obs\n",
    "    ep_return += reward\n",
    "    ep_len += 1\n",
    "\n",
    "    if t >= UPDATE_AFTER:\n",
    "        train_info = agent.train_step(replay, batch_size=BATCH_SIZE)\n",
    "        critic_losses.append(train_info['critic_loss'])\n",
    "        if 'actor_loss' in train_info:\n",
    "            actor_losses.append(train_info['actor_loss'])\n",
    "\n",
    "    if done:\n",
    "        episode_returns.append(ep_return)\n",
    "        episode_lengths.append(ep_len)\n",
    "\n",
    "        if len(episode_returns) % 5 == 0 or not FAST_RUN:\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"Episode {len(episode_returns):4d} | return {ep_return:9.1f} | len {ep_len:3d} | \"\n",
    "                f\"t {t + 1:6d}/{TOTAL_TIMESTEPS} | elapsed {elapsed:6.1f}s\"\n",
    "            )\n",
    "\n",
    "        obs, _ = env_reset(env)\n",
    "        ep_return = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "env.close()\n",
    "\n",
    "print('episodes:', len(episode_returns))\n",
    "print('last return:', episode_returns[-1] if episode_returns else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episodic returns\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'episode': np.arange(1, len(episode_returns) + 1),\n",
    "        'return': episode_returns,\n",
    "        'length': episode_lengths,\n",
    "    }\n",
    ")\n",
    "\n",
    "window = min(10, max(1, len(df)))\n",
    "df['return_ma'] = df['return'].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['episode'], y=df['return'], mode='lines+markers', name='Return'))\n",
    "fig.add_trace(go.Scatter(x=df['episode'], y=df['return_ma'], mode='lines', name=f'{window}-episode MA'))\n",
    "fig.update_layout(\n",
    "    title=f'TD3 training on {ENV_ID}: episodic return',\n",
    "    xaxis_title='Episode',\n",
    "    yaxis_title='Return',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes, diagnostics, and common pitfalls\n",
    "\n",
    "- **Terminal masking**: for time-limit truncation, you typically still bootstrap, so we mask only `terminated` (Gymnasium) rather than `truncated`.\n",
    "- **Twin critics**: the key is using the *minimum* only in the bootstrap target $y$ (not necessarily everywhere).\n",
    "- **Delayed updates**: do not update the actor every step; it should be updated every `POLICY_DELAY` critic updates.\n",
    "- **Target policy smoothing**: the noise added to *target actions* is separate from exploration noise.\n",
    "- **Exploration**: TD3 is deterministic; you must add noise to actions during data collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable-Baselines TD3 (reference implementation)\n",
    "\n",
    "Stable-Baselines3 (SB3) includes a PyTorch TD3 implementation:\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/td3.html\n",
    "\n",
    "This is useful as a reference and a quick way to validate your intuition against a well-tested baseline.\n",
    "\n",
    "If you want to run it locally:\n",
    "\n",
    "```bash\n",
    "pip install stable-baselines3\n",
    "```\n",
    "\n",
    "If you have SB3 installed, a minimal training script looks like:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = TD3(\n",
    "    policy='MlpPolicy',\n",
    "    env=env,\n",
    "    action_noise=action_noise,\n",
    "    verbose=1,\n",
    ")\n",
    "model.learn(total_timesteps=100_000)\n",
    "```\n",
    "\n",
    "At the end of this notebook we summarize SB3's TD3 hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable-Baselines3 TD3 hyperparameters (glossary + defaults)\n",
    "\n",
    "Web research source: https://stable-baselines3.readthedocs.io/en/master/modules/td3.html\n",
    "\n",
    "Constructor signature (defaults):\n",
    "\n",
    "`TD3(policy, env, learning_rate=0.001, buffer_size=1000000, learning_starts=100, batch_size=256, tau=0.005, gamma=0.99, train_freq=1, gradient_steps=1, action_noise=None, replay_buffer_class=None, replay_buffer_kwargs=None, optimize_memory_usage=False, n_steps=1, policy_delay=2, target_policy_noise=0.2, target_noise_clip=0.5, stats_window_size=100, tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True)`\n",
    "\n",
    "Glossary:\n",
    "\n",
    "- `policy`: policy class/name (e.g., `MlpPolicy`, `CnnPolicy`).\n",
    "- `env`: environment instance or env ID string.\n",
    "- `learning_rate` (default `1e-3`): Adam learning rate (SB3 uses the same LR for actor and critics).\n",
    "- `buffer_size` (default `1_000_000`): replay buffer capacity.\n",
    "- `learning_starts` (default `100`): number of environment steps collected before training begins.\n",
    "- `batch_size` (default `256`): mini-batch size sampled from replay.\n",
    "- `tau` (default `0.005`): Polyak coefficient $\\tau$ for target network updates.\n",
    "- `gamma` (default `0.99`): discount factor $\\gamma$.\n",
    "- `train_freq` (default `1`): how often to train (steps), or a tuple like `(n, 'step')` / `(n, 'episode')`.\n",
    "- `gradient_steps` (default `1`): gradient updates per training iteration.\n",
    "- `action_noise` (default `None`): exploration noise used when collecting data (e.g. Gaussian or OU noise).\n",
    "- `policy_delay` (default `2`): actor/target update period $d$ (critics update every step).\n",
    "- `target_policy_noise` (default `0.2`): $\\sigma$ in target policy smoothing.\n",
    "- `target_noise_clip` (default `0.5`): $c$ in target policy smoothing (clip range).\n",
    "- `replay_buffer_class` (default `None`): custom replay buffer class.\n",
    "- `replay_buffer_kwargs` (default `None`): kwargs passed to the replay buffer.\n",
    "- `optimize_memory_usage` (default `False`): memory-efficient replay buffer variant.\n",
    "- `n_steps` (default `1`): n-step returns (when >1 uses an n-step replay buffer).\n",
    "- `stats_window_size` (default `100`): logging window size (episodes averaged).\n",
    "- `tensorboard_log` (default `None`): TensorBoard log directory.\n",
    "- `policy_kwargs` (default `None`): policy/network architecture options.\n",
    "- `verbose` (default `0`): verbosity (0/1/2).\n",
    "- `seed` (default `None`): RNG seed.\n",
    "- `device` (default `'auto'`): device selection (CPU/GPU).\n",
    "- `_init_setup_model` (default `True`): whether to build networks at init.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Fujimoto, van Hoof, Meger (2018): *Addressing Function Approximation Error in Actor-Critic Methods* (TD3)\n",
    "- Stable-Baselines3 docs / source code (TD3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
