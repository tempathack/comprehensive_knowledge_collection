{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Time Series Classification\n",
    "\n",
    "Ensembles combine **diverse base classifiers** (distance, interval, dictionary, shapelet,\n",
    "deep learning, etc.) and aggregate their predictions. The goal is to reduce variance,\n",
    "stabilize performance across datasets, and benefit from complementary inductive biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core idea (weighted voting)\n",
    "Let base classifiers produce class probabilities $p_k(y \\mid x)$.\n",
    "A weighted ensemble computes:\n",
    "\\[s(y \\mid x) = \\sum_{k=1}^K w_k \\, p_k(y \\mid x), \\quad w_k \\ge 0, \\sum_k w_k = 1\\]\n",
    "Then predict $\\hat{y} = \\arg\\max_y s(y \\mid x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "classes = [\"A\", \"B\", \"C\"]\n",
    "base = pd.DataFrame(\n",
    "    {\n",
    "        \"Distance\": [0.60, 0.30, 0.10],\n",
    "        \"Interval\": [0.20, 0.55, 0.25],\n",
    "        \"Dictionary\": [0.35, 0.25, 0.40],\n",
    "        \"Shapelet\": [0.25, 0.50, 0.25],\n",
    "    },\n",
    "    index=classes,\n",
    ")\n",
    "weights = np.array([0.35, 0.25, 0.20, 0.20])\n",
    "ensemble = (base.values @ weights).round(3)\n",
    "base[\"Ensemble\"] = ensemble\n",
    "\n",
    "fig = px.bar(\n",
    "    base.reset_index().melt(id_vars=\"index\"),\n",
    "    x=\"index\",\n",
    "    y=\"value\",\n",
    "    color=\"variable\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Base probabilities vs ensemble vote\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Class\", yaxis_title=\"Probability\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ensembles help\n",
    "If base learners are accurate **and** make *different mistakes*, the average prediction\n",
    "is more stable. A simple variance model for an average of $K$ estimators with pairwise\n",
    "correlation $\\rho$ is:\n",
    "\\[\\mathrm{Var}(\\bar{f}) = \\frac{1}{K}\\sigma^2 + \\frac{K-1}{K}\\rho\\sigma^2\\]\n",
    "Lower correlation means stronger variance reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "K = 10\n",
    "rho = np.linspace(0, 1, 51)\n",
    "sigma2 = 1.0\n",
    "var_avg = (1 / K) * sigma2 + ((K - 1) / K) * rho * sigma2\n",
    "\n",
    "fig = px.line(x=rho, y=var_avg, title=\"Ensemble variance vs correlation\")\n",
    "fig.update_layout(xaxis_title=\"Correlation between base learners (rho)\", yaxis_title=\"Var(average prediction)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sktime inventory for ensemble classifiers\n",
    "sktime exposes ensembles via the registry. The exact list depends on your installed version\n",
    "and optional dependencies. Use the filter below to surface ensemble-style estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    from sktime.registry import all_estimators\n",
    "\n",
    "    ests = all_estimators(estimator_types=\"classifier\", as_dataframe=True)\n",
    "    mask = (\n",
    "        ests[\"name\"].str.contains(\"Ensemble|HIVE|Proximity|COTE\", case=False, na=False)\n",
    "        | ests[\"module\"].str.contains(\"ensemble|hive|cote|proximity\", case=False, na=False)\n",
    "    )\n",
    "    print(ests.loc[mask, [\"name\", \"module\"]].sort_values(\"name\").to_string(index=False))\n",
    "except Exception as exc:\n",
    "    print(\"sktime is not installed or registry lookup failed:\", exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use\n",
    "- Datasets are heterogeneous and no single model family wins everywhere.\n",
    "- You need robust accuracy and are willing to trade extra compute for stability.\n",
    "- You can afford a validation loop to tune ensemble weights or meta-learners.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}