{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17594ac",
   "metadata": {},
   "source": [
    "# VARMAX (VARMA with eXogenous inputs)\n",
    "\n",
    "## Goals\n",
    "- Precisely define **VARX** and **VARMAX**, and how they extend **VAR/VARMA**.\n",
    "- Build intuition for **exogenous inputs** as *known drivers / control signals*.\n",
    "- Understand complexity and identifiability pitfalls (exogeneity, collinearity, MA issues).\n",
    "- Implement simulation + VARX fitting + forecasting + shock propagation in **NumPy**.\n",
    "- Visualize multivariate forecasts and shock propagation with **Plotly**.\n",
    "\n",
    "## Prerequisites\n",
    "- VAR/VARMA basics (previous folder)\n",
    "- Linear regression / OLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f33e2",
   "metadata": {},
   "source": [
    "## 1) Definitions: VARX vs VARMAX\n",
    "\n",
    "Let \\(\\mathbf{y}_t\\in\\mathbb{R}^k\\) be the target vector and \\(\\mathbf{x}_t\\in\\mathbb{R}^m\\) be a vector of **exogenous** regressors (known inputs).\n",
    "\n",
    "### VARX(p,r)\n",
    "A **VARX** (sometimes written VAR(p)+X) extends VAR with exogenous lags:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} A_i\\,\\mathbf{y}_{t-i}\n",
    "+ \\sum_{\\ell=0}^{r} B_{\\ell}\\,\\mathbf{x}_{t-\\ell} + \\boldsymbol{\\varepsilon}_t.\n",
    "$$\n",
    "\n",
    "### VARMAX(p,q,r)\n",
    "A **VARMAX** is the full ARMA + X model:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} A_i\\,\\mathbf{y}_{t-i}\n",
    "+ \\sum_{\\ell=0}^{r} B_{\\ell}\\,\\mathbf{x}_{t-\\ell}\n",
    "+ \\boldsymbol{\\varepsilon}_t + \\sum_{j=1}^{q} M_j\\,\\boldsymbol{\\varepsilon}_{t-j}.\n",
    "$$\n",
    "\n",
    "### Transfer-function (system) view\n",
    "Using lag polynomials\n",
    "\n",
    "$$\n",
    "A(L)= I - A_1 L - \\cdots - A_p L^p,\\quad\n",
    "B(L)= B_0 + B_1 L + \\cdots + B_r L^r,\\quad\n",
    "M(L)= I + M_1 L + \\cdots + M_q L^q,\n",
    "$$\n",
    "\n",
    "the model becomes\n",
    "\n",
    "$$\n",
    "A(L)\\,\\mathbf{y}_t = \\mathbf{c} + B(L)\\,\\mathbf{x}_t + M(L)\\,\\boldsymbol{\\varepsilon}_t.\n",
    "$$\n",
    "\n",
    "So VARMAX is a **multi-input multi-output (MIMO)** linear system:\n",
    "- \\(A(L)^{-1}B(L)\\): response of outputs to inputs\n",
    "- \\(A(L)^{-1}M(L)\\): response of outputs to shocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7342cd",
   "metadata": {},
   "source": [
    "## 2) What “exogenous” really requires\n",
    "\n",
    "For OLS-style estimation of VARX, a common assumption is **strict exogeneity**:\n",
    "\n",
    "$$\\mathbb{E}[\\boldsymbol{\\varepsilon}_t\\mid \\mathbf{x}_s,\\ s\\le T] = 0.$$\n",
    "\n",
    "If \\(\\mathbf{x}_t\\) is correlated with \\(\\varepsilon_t\\) (simultaneity, omitted variables, feedback), then naive OLS is biased.\n",
    "\n",
    "Practical examples of valid-ish \\(\\mathbf{x}_t\\):\n",
    "- calendar effects / holidays\n",
    "- known interventions (pricing change, policy shock)\n",
    "- lagged covariates measured without error\n",
    "\n",
    "Risky \\(\\mathbf{x}_t\\): contemporaneous variables influenced by \\(\\mathbf{y}_t\\) itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5c8f3",
   "metadata": {},
   "source": [
    "## 3) Complexity + identifiability pitfalls\n",
    "\n",
    "Parameter count (ignoring \\(\\Sigma\\)) for VARMAX(p,q,r):\n",
    "\n",
    "$$k \\; + \\; k^2(p+q) \\; + \\; k m (r+1).$$\n",
    "\n",
    "Pitfalls:\n",
    "- **Collinearity**: \\(\\mathbf{x}_{t-\\ell}\\) can be highly correlated with each other and with \\(\\mathbf{y}_{t-i}\\).\n",
    "- **Exogeneity violations**: if inputs are not exogenous, estimates can be biased.\n",
    "- **MA identifiability**: all VARMA issues still apply (invertibility, cancellation, non-unique parameterizations).\n",
    "\n",
    "Tradeoff intuition:\n",
    "- Adding \\(\\mathbf{x}_t\\) can reduce unexplained variance and improve interpretability.\n",
    "- Adding an MA part can fix autocorrelated residuals, but estimation becomes substantially harder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ab572",
   "metadata": {},
   "source": [
    "## 4) Intuition with abstract systems\n",
    "\n",
    "Think of VARMAX as a linear dynamical system with:\n",
    "\n",
    "- **State feedback**: \\(A_i\\) couples past outputs into the present.\n",
    "- **Control / forcing inputs**: \\(B_\\ell\\) injects known signals \\(\\mathbf{x}\\) (with possible delays).\n",
    "- **Disturbances**: \\(\\varepsilon_t\\) are random shocks.\n",
    "- **Shock filter**: \\(M_j\\) lets shocks persist briefly (colored noise).\n",
    "\n",
    "Two useful “shock propagation” objects:\n",
    "\n",
    "1) **Innovation impulse response** \\(\\Psi_h\\): effect of a one-time shock in \\(\\varepsilon_t\\) on future \\(\\mathbf{y}\\).\n",
    "\n",
    "2) **Dynamic multipliers** \\(\\Theta_h\\): effect of a one-time pulse in \\(\\mathbf{x}_t\\) on future \\(\\mathbf{y}\\).\n",
    "\n",
    "When \\(q=0\\) (VARX), \\(\\Theta_h\\) satisfies a simple recursion (see code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Plotly:\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fcd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_varmax(\n",
    "    x: np.ndarray,\n",
    "    A_list: list[np.ndarray],\n",
    "    B_list: list[np.ndarray],\n",
    "    M_list: list[np.ndarray] | None = None,\n",
    "    c: np.ndarray | None = None,\n",
    "    Sigma: np.ndarray | None = None,\n",
    "    burnin: int = 200,\n",
    "    seed: int = 0,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Simulate VARMAX(p,q,r) with provided exogenous series x.\n",
    "\n",
    "    y_t = c + sum_i A_i y_{t-i} + sum_l B_l x_{t-l} + e_t + sum_j M_j e_{t-j}\n",
    "\n",
    "    Args:\n",
    "      x: (T, m) exogenous inputs. Returned y will have length (T - burnin - max_lag).\n",
    "      B_list: [B0, B1, ..., Br] where B_l is (k,m)\n",
    "      M_list: [M1, ..., Mq] where M_j is (k,k)\n",
    "\n",
    "    Returns:\n",
    "      y: (n, k)\n",
    "      e: (n, k)\n",
    "    \"\"\"\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(\"x must be (T,m)\")\n",
    "    T_x, m = x.shape\n",
    "\n",
    "    p = len(A_list)\n",
    "    r = len(B_list) - 1\n",
    "    q = 0 if (M_list is None) else len(M_list)\n",
    "    M_list = [] if M_list is None else M_list\n",
    "\n",
    "    if p == 0 and r < 0 and q == 0:\n",
    "        raise ValueError(\"Need at least one of p, r, q\")\n",
    "\n",
    "    k = (A_list[0].shape[0] if p > 0 else B_list[0].shape[0])\n",
    "\n",
    "    for A in A_list:\n",
    "        if A.shape != (k, k):\n",
    "            raise ValueError(\"All A_i must be (k,k)\")\n",
    "    for B in B_list:\n",
    "        if B.shape != (k, m):\n",
    "            raise ValueError(\"All B_l must be (k,m)\")\n",
    "    for M in M_list:\n",
    "        if M.shape != (k, k):\n",
    "            raise ValueError(\"All M_j must be (k,k)\")\n",
    "\n",
    "    if c is None:\n",
    "        c = np.zeros(k)\n",
    "    c = np.asarray(c, dtype=float)\n",
    "    if c.shape != (k,):\n",
    "        raise ValueError(\"c must be (k,)\")\n",
    "\n",
    "    if Sigma is None:\n",
    "        Sigma = np.eye(k)\n",
    "    Sigma = np.asarray(Sigma, dtype=float)\n",
    "    if Sigma.shape != (k, k):\n",
    "        raise ValueError(\"Sigma must be (k,k)\")\n",
    "\n",
    "    max_lag = max(p, r, q)\n",
    "    if T_x <= burnin + max_lag:\n",
    "        raise ValueError(\"x is too short for chosen lags + burnin\")\n",
    "\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    e = rng_local.standard_normal((T_x, k)) @ L.T\n",
    "    y = np.zeros((T_x, k), dtype=float)\n",
    "\n",
    "    for t in range(max_lag, T_x):\n",
    "        yt = c.copy()\n",
    "\n",
    "        for i, A in enumerate(A_list, start=1):\n",
    "            yt += A @ y[t - i]\n",
    "\n",
    "        for ell, B in enumerate(B_list):\n",
    "            yt += B @ x[t - ell]\n",
    "\n",
    "        yt += e[t]\n",
    "\n",
    "        for j, M in enumerate(M_list, start=1):\n",
    "            yt += M @ e[t - j]\n",
    "\n",
    "        y[t] = yt\n",
    "\n",
    "    sl = slice(burnin + max_lag, T_x)\n",
    "    return y[sl], e[sl]\n",
    "\n",
    "\n",
    "def varx_ols_fit(\n",
    "    y: np.ndarray,\n",
    "    x: np.ndarray,\n",
    "    p: int,\n",
    "    r: int,\n",
    "    add_intercept: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"Fit VARX(p,r) by stacked OLS.\n",
    "\n",
    "    y_t = c + sum_i A_i y_{t-i} + sum_{ell=0}^r B_ell x_{t-ell} + e_t\n",
    "\n",
    "    Returns c, A_list, B_list, residuals, Sigma_hat.\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if y.ndim != 2 or x.ndim != 2:\n",
    "        raise ValueError(\"y and x must be 2D\")\n",
    "\n",
    "    T, k = y.shape\n",
    "    Tx, m = x.shape\n",
    "    if Tx != T:\n",
    "        raise ValueError(\"x and y must have the same length\")\n",
    "\n",
    "    max_lag = max(p, r)\n",
    "    if T <= max_lag:\n",
    "        raise ValueError(\"Need T > max(p,r)\")\n",
    "\n",
    "    Y = y[max_lag:]\n",
    "\n",
    "    X_blocks = []\n",
    "    if add_intercept:\n",
    "        X_blocks.append(np.ones((T - max_lag, 1)))\n",
    "\n",
    "    for lag in range(1, p + 1):\n",
    "        X_blocks.append(y[max_lag - lag : T - lag])\n",
    "\n",
    "    for lag in range(0, r + 1):\n",
    "        X_blocks.append(x[max_lag - lag : T - lag])\n",
    "\n",
    "    X = np.concatenate(X_blocks, axis=1)\n",
    "    B_hat, *_ = np.linalg.lstsq(X, Y, rcond=None)\n",
    "\n",
    "    Y_hat = X @ B_hat\n",
    "    E = Y - Y_hat\n",
    "\n",
    "    offset = 0\n",
    "    c_hat = np.zeros(k)\n",
    "    if add_intercept:\n",
    "        c_hat = B_hat[0]\n",
    "        offset = 1\n",
    "\n",
    "    A_hat = []\n",
    "    for i in range(p):\n",
    "        A_hat.append(B_hat[offset + i * k : offset + (i + 1) * k].T)\n",
    "\n",
    "    offset2 = offset + p * k\n",
    "    B_list = []\n",
    "    for ell in range(r + 1):\n",
    "        block = B_hat[offset2 + ell * m : offset2 + (ell + 1) * m].T\n",
    "        B_list.append(block)\n",
    "\n",
    "    return {\n",
    "        'c': c_hat,\n",
    "        'A': A_hat,\n",
    "        'B': B_list,\n",
    "        'resid': E,\n",
    "        'Sigma': (E.T @ E) / (T - max_lag),\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "    }\n",
    "\n",
    "\n",
    "def varx_forecast_paths(\n",
    "    y_hist: np.ndarray,\n",
    "    x_hist: np.ndarray,\n",
    "    x_future: np.ndarray,\n",
    "    c: np.ndarray,\n",
    "    A_list: list[np.ndarray],\n",
    "    B_list: list[np.ndarray],\n",
    "    Sigma: np.ndarray,\n",
    "    n_paths: int = 500,\n",
    "    seed: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Monte Carlo forecasts for VARX with known future x.\n",
    "\n",
    "    Returns paths: (n_paths, H, k)\n",
    "    \"\"\"\n",
    "\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "\n",
    "    y_hist = np.asarray(y_hist, dtype=float)\n",
    "    x_hist = np.asarray(x_hist, dtype=float)\n",
    "    x_future = np.asarray(x_future, dtype=float)\n",
    "\n",
    "    T, k = y_hist.shape\n",
    "    Tx, m = x_hist.shape\n",
    "    if Tx != T:\n",
    "        raise ValueError(\"x_hist and y_hist must match\")\n",
    "\n",
    "    H, m2 = x_future.shape\n",
    "    if m2 != m:\n",
    "        raise ValueError(\"x_future must have same m as x_hist\")\n",
    "\n",
    "    p = len(A_list)\n",
    "    r = len(B_list) - 1\n",
    "    max_lag = max(p, r)\n",
    "    if T < max_lag:\n",
    "        raise ValueError(\"Need enough history\")\n",
    "\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "\n",
    "    # stitch x into one array for lag lookup\n",
    "    x_ext = np.vstack([x_hist, x_future])\n",
    "\n",
    "    paths = np.zeros((n_paths, H, k), dtype=float)\n",
    "    for s in range(n_paths):\n",
    "        y_ext = np.zeros((T + H, k), dtype=float)\n",
    "        y_ext[:T] = y_hist\n",
    "\n",
    "        e_fut = rng_local.standard_normal((H, k)) @ L.T\n",
    "\n",
    "        for t in range(T, T + H):\n",
    "            yt = c.copy()\n",
    "\n",
    "            for i, A in enumerate(A_list, start=1):\n",
    "                yt += A @ y_ext[t - i]\n",
    "\n",
    "            for ell, B in enumerate(B_list):\n",
    "                yt += B @ x_ext[t - ell]\n",
    "\n",
    "            yt += e_fut[t - T]\n",
    "\n",
    "            y_ext[t] = yt\n",
    "\n",
    "        paths[s] = y_ext[T:]\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def varx_dynamic_multiplier(A_list: list[np.ndarray], B_list: list[np.ndarray], steps: int) -> np.ndarray:\n",
    "    \"\"\"Theta_0..Theta_steps where y_{t+h} response to a unit pulse in x_t.\n",
    "\n",
    "    For VARX: y_t = sum_i A_i y_{t-i} + sum_ell B_ell x_{t-ell}.\n",
    "\n",
    "    Recursion:\n",
    "      Theta_h = B_h + sum_{i=1}^{min(p,h)} A_i Theta_{h-i}\n",
    "      with B_h = 0 for h>r.\n",
    "\n",
    "    Returns: (steps+1, k, m)\n",
    "    \"\"\"\n",
    "\n",
    "    p = len(A_list)\n",
    "    r = len(B_list) - 1\n",
    "    k, m = B_list[0].shape\n",
    "\n",
    "    Theta = np.zeros((steps + 1, k, m))\n",
    "\n",
    "    for h in range(0, steps + 1):\n",
    "        Bh = B_list[h] if h <= r else np.zeros((k, m))\n",
    "        acc = Bh.copy()\n",
    "        for i in range(1, min(p, h) + 1):\n",
    "            acc += A_list[i - 1] @ Theta[h - i]\n",
    "        Theta[h] = acc\n",
    "\n",
    "    return Theta\n",
    "\n",
    "\n",
    "\n",
    "def varma_irf(A_list: list[np.ndarray], M_list: list[np.ndarray], steps: int) -> np.ndarray:\n",
    "    \"\"\"Impulse response matrices Psi_0..Psi_steps for VARMA(p,q).\"\"\"\n",
    "\n",
    "    p = len(A_list)\n",
    "    q = len(M_list)\n",
    "    k = (A_list[0].shape[0] if p > 0 else M_list[0].shape[0])\n",
    "\n",
    "    Psi = np.zeros((steps + 1, k, k))\n",
    "    Psi[0] = np.eye(k)\n",
    "\n",
    "    for h in range(1, steps + 1):\n",
    "        Mh = M_list[h - 1] if (1 <= h <= q) else np.zeros((k, k))\n",
    "        acc = Mh.copy()\n",
    "        for i in range(1, min(p, h) + 1):\n",
    "            acc += A_list[i - 1] @ Psi[h - i]\n",
    "        Psi[h] = acc\n",
    "\n",
    "    return Psi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba11a7f",
   "metadata": {},
   "source": [
    "## 5) Synthetic example: VARMAX dynamics (simulate) + VARX estimation baseline\n",
    "\n",
    "We simulate a 2D output \\(\\mathbf{y}_t\\) driven by:\n",
    "- its own lags (VAR part),\n",
    "- a 1D exogenous signal \\(x_t\\) (input),\n",
    "- and a short MA(1) disturbance term.\n",
    "\n",
    "Then we fit a **VARX** (no MA) by OLS as a baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35315024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exogenous input: a smooth driver + an intervention step\n",
    "T = 900\n",
    "\n",
    "t = np.arange(T)\n",
    "\n",
    "x = np.zeros((T, 1))\n",
    "x[:, 0] = 0.6 * np.sin(2 * np.pi * t / 60) + 0.25 * np.sin(2 * np.pi * t / 15)\n",
    "\n",
    "# intervention at t=600\n",
    "x[t >= 600, 0] += 1.0\n",
    "\n",
    "# VAR part (stable)\n",
    "A1 = np.array([[0.55, 0.10],\n",
    "               [-0.05, 0.35]])\n",
    "A2 = np.array([[-0.20, 0.03],\n",
    "               [0.01, -0.12]])\n",
    "A = [A1, A2]\n",
    "\n",
    "# Exogenous lags: B0 (instant), B1 (delayed)\n",
    "B0 = np.array([[0.50],\n",
    "               [0.10]])\n",
    "B1 = np.array([[0.15],\n",
    "               [0.25]])\n",
    "B = [B0, B1]\n",
    "\n",
    "# MA(1) disturbance\n",
    "M1 = np.array([[0.45, 0.00],\n",
    "               [0.20, 0.20]])\n",
    "M = [M1]\n",
    "\n",
    "c = np.array([0.0, 0.0])\n",
    "Sigma = np.array([[0.8, 0.25],\n",
    "                  [0.25, 0.6]])\n",
    "\n",
    "# Simulate\n",
    "Y, E = simulate_varmax(x=x, A_list=A, B_list=B, M_list=M, c=c, Sigma=Sigma, burnin=250, seed=SEED)\n",
    "X = x[250 + max(len(A), len(B)-1, len(M)) :]\n",
    "\n",
    "print(\"Y:\", Y.shape, \"X:\", X.shape)\n",
    "\n",
    "# Plot y and x\n",
    "T_eff = Y.shape[0]\n",
    "t_eff = np.arange(T_eff)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    subplot_titles=[\"y[0]\", \"y[1]\", \"x (exogenous)\"]\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=t_eff, y=Y[:, 0], mode=\"lines\", name=\"y0\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=t_eff, y=Y[:, 1], mode=\"lines\", name=\"y1\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=t_eff, y=X[:, 0], mode=\"lines\", name=\"x\"), row=3, col=1)\n",
    "\n",
    "fig.update_layout(height=620, title=\"Simulated VARMAX outputs driven by exogenous input\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747918a",
   "metadata": {},
   "source": [
    "### Fit a VARX baseline (ignoring MA)\n",
    "\n",
    "When \\(q>0\\) the residuals from a VARX fit can remain autocorrelated (because the true disturbance is MA). Still, VARX is often a useful baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444009c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = varx_ols_fit(y=Y, x=X, p=2, r=1, add_intercept=True)\n",
    "print(\"c_hat:\", fit[\"c\"])\n",
    "print(\"B0_hat:\")\n",
    "print(fit[\"B\"][0])\n",
    "print(\"B1_hat:\")\n",
    "print(fit[\"B\"][1])\n",
    "print(\"Sigma_hat:\")\n",
    "print(fit[\"Sigma\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723029a",
   "metadata": {},
   "source": [
    "## 6) Multivariate forecasts with known future inputs\n",
    "\n",
    "Forecasting with exogenous inputs is typically **scenario-based**:\n",
    "- decide or forecast \\(x_{T+1:T+H}\\),\n",
    "- propagate the model forward.\n",
    "\n",
    "Below we forecast \\(H\\) steps ahead using the fitted VARX parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = 520\n",
    "H = 80\n",
    "\n",
    "y_hist = Y[:T0]\n",
    "x_hist = X[:T0]\n",
    "\n",
    "y_true = Y[T0:T0+H]\n",
    "x_future = X[T0:T0+H]\n",
    "\n",
    "paths = varx_forecast_paths(\n",
    "    y_hist=y_hist,\n",
    "    x_hist=x_hist,\n",
    "    x_future=x_future,\n",
    "    c=fit['c'],\n",
    "    A_list=fit['A'],\n",
    "    B_list=fit['B'],\n",
    "    Sigma=fit['Sigma'],\n",
    "    n_paths=800,\n",
    "    seed=SEED + 100,\n",
    ")\n",
    "\n",
    "q_lo, q_med, q_hi = np.quantile(paths, [0.05, 0.5, 0.95], axis=0)\n",
    "\n",
    "x_past = np.arange(T0)\n",
    "x_fut = np.arange(T0, T0 + H)\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.05,\n",
    "                    subplot_titles=[\"Forecast y[0]\", \"Forecast y[1]\", \"x future (given)\"])\n",
    "\n",
    "for i in range(2):\n",
    "    fig.add_trace(go.Scatter(x=x_past, y=y_hist[:, i], mode=\"lines\", name=f\"y{i} history\",\n",
    "                             line=dict(color=\"rgba(0,0,0,0.55)\")), row=i+1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=y_true[:, i], mode=\"lines\", name=f\"y{i} truth\",\n",
    "                             line=dict(color=\"rgba(0,0,0,1.0)\", width=2)), row=i+1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=q_hi[:, i], mode=\"lines\", line=dict(width=0),\n",
    "                             showlegend=False, hoverinfo=\"skip\"), row=i+1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=q_lo[:, i], mode=\"lines\", line=dict(width=0),\n",
    "                             fill=\"tonexty\", fillcolor=\"rgba(0,160,120,0.18)\",\n",
    "                             name=f\"90% PI y{i}\", hoverinfo=\"skip\"), row=i+1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=x_fut, y=q_med[:, i], mode=\"lines\", name=f\"median y{i}\",\n",
    "                             line=dict(color=\"rgba(0,160,120,1.0)\", dash=\"dash\")), row=i+1, col=1)\n",
    "\n",
    "    fig.add_vline(x=T0, line_width=1, line_dash=\"dot\", line_color=\"gray\")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_fut, y=x_future[:, 0], mode=\"lines\", name=\"x given\",\n",
    "                         line=dict(color=\"rgba(120,120,120,1.0)\")), row=3, col=1)\n",
    "\n",
    "fig.update_layout(height=700, title=\"VARX forecasts conditional on exogenous path\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3bb06",
   "metadata": {},
   "source": [
    "## 7) Shock propagation\n\nTwo kinds:\n\n- **Innovation shocks** \\(\\varepsilon_t\\): use impulse responses \\(\\Psi_h\\) (same recursion as VARMA).\n- **Input shocks** \\(x_t\\): use dynamic multipliers \\(\\Theta_h\\) for VARX.\n\nBelow we show:\n1) response to a **unit pulse** in the input at time 0 (dynamic multipliers, using the fitted VARX),\n2) response to a **1-s.d. innovation shock** (impulse responses, using the simulated VARMA disturbance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Dynamic multipliers: response to a unit pulse in x\n",
    "H_mul = 25\n",
    "Theta = varx_dynamic_multiplier(A_list=fit['A'], B_list=fit['B'], steps=H_mul)  # (H+1,k,m)\n",
    "\n",
    "h = np.arange(H_mul + 1)\n",
    "resp = Theta[:, :, 0]  # m=1\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.08,\n",
    "                    subplot_titles=[\"y[0] response to x pulse\", \"y[1] response to x pulse\"])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=h, y=resp[:, 0], mode=\"lines\", name=\"y0\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=h, y=resp[:, 1], mode=\"lines\", name=\"y1\"), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=520, title=\"VARX shock propagation from exogenous input (dynamic multipliers)\")\n",
    "fig.update_xaxes(title_text=\"horizon\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# (2) Innovation impulse responses: response to 1-s.d. shocks in eps\n",
    "H_irf = 25\n",
    "Psi = varma_irf(A_list=A, M_list=M, steps=H_irf)  # uses the simulated VARMAX disturbance\n",
    "shock_std = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.10,\n",
    "    subplot_titles=[\n",
    "        \"response y0 to shock e0\",\n",
    "        \"response y0 to shock e1\",\n",
    "        \"response y1 to shock e0\",\n",
    "        \"response y1 to shock e1\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "h = np.arange(H_irf + 1)\n",
    "for shock_j in range(2):\n",
    "    u = np.zeros(2)\n",
    "    u[shock_j] = shock_std[shock_j]\n",
    "    resp = Psi @ u  # (H+1, k)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=h, y=resp[:, 0], mode=\"lines\", name=f\"shock {shock_j}\"), row=1, col=shock_j+1)\n",
    "    fig.add_trace(go.Scatter(x=h, y=resp[:, 1], mode=\"lines\", showlegend=False), row=2, col=shock_j+1)\n",
    "\n",
    "fig.update_layout(height=520, title=\"VARMAX impulse responses (1-s.d. innovations)\")\n",
    "fig.update_xaxes(title_text=\"horizon\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bd724",
   "metadata": {},
   "source": [
    "## 8) Exercises\n",
    "\n",
    "1) Change \\(B_0\\) and \\(B_1\\) signs and interpret the dynamic multiplier shapes.\n",
    "2) Add more exogenous lags (increase \\(r\\)) and see how \\(\\Theta_h\\) changes.\n",
    "3) Simulate with stronger MA(1) and compare VARX residual behavior.\n",
    "\n",
    "## References\n",
    "- Lütkepohl, *New Introduction to Multiple Time Series Analysis*\n",
    "- Hamilton, *Time Series Analysis*\n",
    "- Tsay, *Multivariate Time Series Analysis*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
