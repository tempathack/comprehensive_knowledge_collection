{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explained Variance Score (`explained_variance_score`)\n",
    "\n",
    "`explained_variance_score` is a regression metric that asks a very specific question:\n",
    "\n",
    "> **How much of the variability (spread) in $y$ remains unexplained after using predictions $\\hat{y}$?**\n",
    "\n",
    "It is *similar* to $R^2$, but with an important twist: it uses the **variance of the residuals** and is therefore **insensitive to constant (additive) bias**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- derive and interpret the metric (including edge cases)\n",
    "- understand its relationship to MSE and $R^2$\n",
    "- implement it from scratch in NumPy (sample weights + multi-output)\n",
    "- visualize how it reacts to noise vs bias\n",
    "- see what happens if you try to **optimize** it directly (gradient descent)\n",
    "\n",
    "## Notation\n",
    "\n",
    "- True targets: $y \\in \\mathbb{R}^n$\n",
    "- Predictions: $\\hat{y} \\in \\mathbb{R}^n$\n",
    "- Residuals: $r = y - \\hat{y}$\n",
    "- Mean: $\\bar{z} = \\frac{1}{n}\\sum_{i=1}^n z_i$\n",
    "- Variance (population / ddof=0): $\\mathrm{Var}(z) = \\frac{1}{n}\\sum_{i=1}^n (z_i - \\bar{z})^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Definition and intuition\n",
    "\n",
    "For a single regression target, explained variance is defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{EVS}(y, \\hat{y}) = 1 - \\frac{\\mathrm{Var}(y - \\hat{y})}{\\mathrm{Var}(y)}.\n",
    "$$\n",
    "\n",
    "- Numerator: variance of the residuals $r = y - \\hat{y}$ (after removing the residual mean).\n",
    "- Denominator: variance of the target $y$.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- `1.0`: residuals have **zero variance** (they are all equal) → the model captures the *shape* of $y$ perfectly.\n",
    "- `0.0`: residual variance equals target variance → predictions do not reduce the spread.\n",
    "- `< 0`: residual variance is larger than target variance → worse than a naive baseline.\n",
    "\n",
    "### Key property: shift invariance (bias blindness)\n",
    "\n",
    "For any constant $c$,\n",
    "\n",
    "$$\n",
    "\\mathrm{EVS}(y, \\hat{y}+c) = \\mathrm{EVS}(y, \\hat{y}),\n",
    "$$\n",
    "\n",
    "because adding a constant shifts residuals by $-c$ but does not change their variance.\n",
    "\n",
    "### Edge case: constant targets and `force_finite`\n",
    "\n",
    "If $\\mathrm{Var}(y)=0$ (all targets identical), the fraction above is ill-defined.\n",
    "`scikit-learn` uses `force_finite=True` by default and returns:\n",
    "\n",
    "- `1.0` if $\\mathrm{Var}(y-\\hat{y})=0$ (e.g., predictions differ from $y$ by a constant offset)\n",
    "- `0.0` otherwise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Relationship to MSE and $R^2$\n",
    "\n",
    "Let residuals be $r = y - \\hat{y}$. The mean squared error decomposes as:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^n r_i^2\n",
    "= \\underbrace{\\mathrm{Var}(r)}_{\\text{spread}} + \\underbrace{\\bar{r}^2}_{\\text{(additive) bias}^2}.\n",
    "$$\n",
    "\n",
    "This yields:\n",
    "\n",
    "$$\n",
    "\\mathrm{EVS} = 1 - \\frac{\\mathrm{Var}(r)}{\\mathrm{Var}(y)},\n",
    "\\qquad\n",
    "R^2 = 1 - \\frac{\\mathrm{MSE}}{\\mathrm{Var}(y)} = \\mathrm{EVS} - \\frac{\\bar{r}^2}{\\mathrm{Var}(y)}.\n",
    "$$\n",
    "\n",
    "So $R^2 \\le \\mathrm{EVS}$, with equality when the mean residual is zero (no additive bias).\n",
    "\n",
    "**Practical takeaway:** explained variance measures how well you match the *fluctuations* of $y$; it can look great even when you are systematically too high/low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic signal + noise\n",
    "n = 400\n",
    "x = np.linspace(0, 2 * np.pi, n)\n",
    "y_signal = np.sin(x)\n",
    "y_true = y_signal + rng.normal(0.0, 0.25, size=n)\n",
    "\n",
    "# A few prediction candidates\n",
    "preds = {\n",
    "    'signal (denoised)': y_signal,\n",
    "    'signal + constant bias': y_signal + 1.0,\n",
    "    'mean baseline': np.full_like(y_true, y_true.mean()),\n",
    "    'signal + extra noise': y_signal + rng.normal(0.0, 0.9, size=n),\n",
    "    'wrong phase': np.sin(x + 0.7),\n",
    "}\n",
    "\n",
    "\n",
    "def summarize(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    return {\n",
    "        'EVS': explained_variance_score(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'residual_mean': float(residuals.mean()),\n",
    "        'residual_var': float(residuals.var()),\n",
    "    }\n",
    "\n",
    "\n",
    "summaries = {name: summarize(y_true, y_pred) for name, y_pred in preds.items()}\n",
    "summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y_true,\n",
    "        mode='markers',\n",
    "        name='y_true',\n",
    "        marker=dict(size=4, opacity=0.5),\n",
    "    )\n",
    ")\n",
    "\n",
    "for name, y_pred in preds.items():\n",
    "    evs = summaries[name]['EVS']\n",
    "    r2 = summaries[name]['R2']\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y_pred,\n",
    "            mode='lines',\n",
    "            name=f'{name} (EVS={evs:.3f}, R2={r2:.3f})',\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Same residual variance ⇒ same EVS (bias does not matter)',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    "    legend_title='Predictor',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "bins = dict(start=-3, end=3, size=0.1)\n",
    "for name, y_pred in preds.items():\n",
    "    residuals = y_true - y_pred\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=residuals,\n",
    "            name=f\"{name} (mean={residuals.mean():.2f}, std={residuals.std():.2f})\",\n",
    "            opacity=0.55,\n",
    "            xbins=bins,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Residual distributions (EVS uses variance, not the mean)',\n",
    "    xaxis_title='residual (y_true - y_pred)',\n",
    "    yaxis_title='count',\n",
    "    barmode='overlay',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = np.linspace(-2.5, 2.5, 101)\n",
    "evs_vals = []\n",
    "r2_vals = []\n",
    "\n",
    "base = preds['signal (denoised)']\n",
    "for c in biases:\n",
    "    y_pred = base + c\n",
    "    evs_vals.append(explained_variance_score(y_true, y_pred))\n",
    "    r2_vals.append(r2_score(y_true, y_pred))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=biases, y=evs_vals, mode='lines', name='EVS'))\n",
    "fig.add_trace(go.Scatter(x=biases, y=r2_vals, mode='lines', name='R2'))\n",
    "fig.update_layout(\n",
    "    title='Adding a constant offset changes R2 but not explained variance',\n",
    "    xaxis_title='constant offset c added to predictions',\n",
    "    yaxis_title='score',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.linspace(0.0, 2.0, 31)\n",
    "var_y = float(np.var(y_true))\n",
    "evs_theory = 1.0 - (sigmas**2) / var_y\n",
    "\n",
    "n_trials = 300\n",
    "rng_sim = np.random.default_rng(123)\n",
    "evs_sim = []\n",
    "for sigma in sigmas:\n",
    "    scores = []\n",
    "    for _ in range(n_trials):\n",
    "        y_pred = y_true + rng_sim.normal(0.0, sigma, size=n)\n",
    "        scores.append(explained_variance_score(y_true, y_pred))\n",
    "    evs_sim.append(float(np.mean(scores)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=sigmas, y=evs_theory, mode='lines', name='theory: 1 - σ²/Var(y)'))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sigmas,\n",
    "        y=evs_sim,\n",
    "        mode='markers+lines',\n",
    "        name=f'simulation mean ({n_trials} trials)',\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title='Independent noise reduces EVS proportionally to its variance',\n",
    "    xaxis_title='noise σ (y_pred = y_true + ε, ε ~ N(0, σ²))',\n",
    "    yaxis_title='explained variance score',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) NumPy implementation (from scratch)\n",
    "\n",
    "`scikit-learn` supports:\n",
    "\n",
    "- optional `sample_weight` (per-sample weights)\n",
    "- multi-output regression (shape `(n_samples, n_outputs)`)\n",
    "- output aggregation via `multioutput`:\n",
    "  - `raw_values`: one score per output\n",
    "  - `uniform_average`: mean across outputs\n",
    "  - `variance_weighted`: mean weighted by each output's target variance\n",
    "\n",
    "Below is a compact NumPy implementation that matches `sklearn.metrics.explained_variance_score` (including `force_finite`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_2d(a):\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim == 1:\n",
    "        return a.reshape(-1, 1)\n",
    "    if a.ndim == 2:\n",
    "        return a\n",
    "    raise ValueError(f'Expected 1D or 2D array, got shape {a.shape}')\n",
    "\n",
    "\n",
    "def _check_sample_weight(sample_weight, n_samples):\n",
    "    if sample_weight is None:\n",
    "        return None\n",
    "    w = np.asarray(sample_weight, dtype=float)\n",
    "    if w.ndim != 1:\n",
    "        raise ValueError(f'sample_weight must be 1D, got shape {w.shape}')\n",
    "    if w.shape[0] != n_samples:\n",
    "        raise ValueError(f'sample_weight length {w.shape[0]} != n_samples {n_samples}')\n",
    "    return w\n",
    "\n",
    "\n",
    "def _weighted_mean(a, sample_weight):\n",
    "    return np.average(a, weights=sample_weight, axis=0)\n",
    "\n",
    "\n",
    "def _weighted_variance(a, sample_weight):\n",
    "    mean = _weighted_mean(a, sample_weight)\n",
    "    return np.average((a - mean) ** 2, weights=sample_weight, axis=0)\n",
    "\n",
    "\n",
    "def explained_variance_score_np(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput='uniform_average',\n",
    "    force_finite=True,\n",
    "):\n",
    "    y_true = _to_2d(y_true)\n",
    "    y_pred = _to_2d(y_pred)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f'y_true shape {y_true.shape} != y_pred shape {y_pred.shape}')\n",
    "\n",
    "    n_samples, n_outputs = y_true.shape\n",
    "    w = _check_sample_weight(sample_weight, n_samples)\n",
    "\n",
    "    residual = y_true - y_pred\n",
    "    numerator = _weighted_variance(residual, w)\n",
    "    denominator = _weighted_variance(y_true, w)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        output_scores = 1.0 - (numerator / denominator)\n",
    "\n",
    "    if force_finite:\n",
    "        zero_den = denominator == 0\n",
    "        output_scores = output_scores.copy()\n",
    "        output_scores[zero_den & (numerator == 0)] = 1.0\n",
    "        output_scores[zero_den & (numerator != 0)] = 0.0\n",
    "\n",
    "    if multioutput == 'raw_values':\n",
    "        return output_scores if n_outputs > 1 else float(output_scores[0])\n",
    "\n",
    "    if multioutput == 'uniform_average':\n",
    "        return float(np.mean(output_scores))\n",
    "\n",
    "    if multioutput == 'variance_weighted':\n",
    "        if np.all(denominator == 0):\n",
    "            return float(np.mean(output_scores))\n",
    "        return float(np.average(output_scores, weights=denominator))\n",
    "\n",
    "    # array-like weights\n",
    "    mo = np.asarray(multioutput, dtype=float)\n",
    "    if mo.shape != (n_outputs,):\n",
    "        raise ValueError(f'multioutput weights must have shape ({n_outputs},), got {mo.shape}')\n",
    "    return float(np.average(output_scores, weights=mo))\n",
    "\n",
    "\n",
    "def mean_squared_error_np(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average'):\n",
    "    y_true = _to_2d(y_true)\n",
    "    y_pred = _to_2d(y_pred)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f'y_true shape {y_true.shape} != y_pred shape {y_pred.shape}')\n",
    "\n",
    "    n_samples, n_outputs = y_true.shape\n",
    "    w = _check_sample_weight(sample_weight, n_samples)\n",
    "    raw = np.average((y_true - y_pred) ** 2, weights=w, axis=0)\n",
    "\n",
    "    if multioutput == 'raw_values':\n",
    "        return raw if n_outputs > 1 else float(raw[0])\n",
    "    if multioutput == 'uniform_average':\n",
    "        return float(np.mean(raw))\n",
    "\n",
    "    mo = np.asarray(multioutput, dtype=float)\n",
    "    if mo.shape != (n_outputs,):\n",
    "        raise ValueError(f'multioutput weights must have shape ({n_outputs},), got {mo.shape}')\n",
    "    return float(np.average(raw, weights=mo))\n",
    "\n",
    "\n",
    "def r2_score_np(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput='uniform_average',\n",
    "    force_finite=True,\n",
    "):\n",
    "    y_true = _to_2d(y_true)\n",
    "    y_pred = _to_2d(y_pred)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f'y_true shape {y_true.shape} != y_pred shape {y_pred.shape}')\n",
    "\n",
    "    n_samples, n_outputs = y_true.shape\n",
    "    w = _check_sample_weight(sample_weight, n_samples)\n",
    "\n",
    "    numerator = np.average((y_true - y_pred) ** 2, weights=w, axis=0)\n",
    "    denominator = _weighted_variance(y_true, w)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        output_scores = 1.0 - (numerator / denominator)\n",
    "\n",
    "    if force_finite:\n",
    "        zero_den = denominator == 0\n",
    "        output_scores = output_scores.copy()\n",
    "        output_scores[zero_den & (numerator == 0)] = 1.0\n",
    "        output_scores[zero_den & (numerator != 0)] = 0.0\n",
    "\n",
    "    if multioutput == 'raw_values':\n",
    "        return output_scores if n_outputs > 1 else float(output_scores[0])\n",
    "    if multioutput == 'uniform_average':\n",
    "        return float(np.mean(output_scores))\n",
    "    if multioutput == 'variance_weighted':\n",
    "        if np.all(denominator == 0):\n",
    "            return float(np.mean(output_scores))\n",
    "        return float(np.average(output_scores, weights=denominator))\n",
    "\n",
    "    mo = np.asarray(multioutput, dtype=float)\n",
    "    if mo.shape != (n_outputs,):\n",
    "        raise ValueError(f'multioutput weights must have shape ({n_outputs},), got {mo.shape}')\n",
    "    return float(np.average(output_scores, weights=mo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick parity checks vs scikit-learn (random data)\n",
    "for n_outputs in [1, 3]:\n",
    "    y_true = rng.normal(size=(200, n_outputs))\n",
    "    y_pred = y_true + rng.normal(scale=0.8, size=(200, n_outputs))\n",
    "    w = rng.uniform(0.1, 2.0, size=200)\n",
    "\n",
    "    for mo in ['raw_values', 'uniform_average', 'variance_weighted']:\n",
    "        sk = explained_variance_score(y_true, y_pred, sample_weight=w, multioutput=mo)\n",
    "        ours = explained_variance_score_np(y_true, y_pred, sample_weight=w, multioutput=mo)\n",
    "        np.testing.assert_allclose(ours, sk, rtol=1e-12, atol=1e-12)\n",
    "\n",
    "# Degenerate denominator behavior (constant y)\n",
    "y_const = np.ones(10)\n",
    "y_const_pred_const = np.zeros_like(y_const)\n",
    "y_const_pred_vary = np.arange(y_const.size)\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    explained_variance_score_np(y_const, y_const_pred_const),\n",
    "    explained_variance_score(y_const, y_const_pred_const),\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    explained_variance_score_np(y_const, y_const_pred_vary),\n",
    "    explained_variance_score(y_const, y_const_pred_vary),\n",
    ")\n",
    "\n",
    "# `force_finite=False` matches NaN/-inf conventions\n",
    "np.testing.assert_allclose(\n",
    "    explained_variance_score_np(y_const, y_const_pred_const, force_finite=False),\n",
    "    explained_variance_score(y_const, y_const_pred_const, force_finite=False),\n",
    "    equal_nan=True,\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    explained_variance_score_np(y_const, y_const_pred_vary, force_finite=False),\n",
    "    explained_variance_score(y_const, y_const_pred_vary, force_finite=False),\n",
    "    equal_nan=True,\n",
    ")\n",
    "\n",
    "'All checks passed.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Multi-output aggregation (`multioutput`)\n",
    "\n",
    "For multi-output regression (shape `(n_samples, n_outputs)`), EVS is computed **per output** first.\n",
    "\n",
    "Then `multioutput` decides how to combine them:\n",
    "\n",
    "- `raw_values`: return one score per output\n",
    "- `uniform_average`: mean of the per-output scores\n",
    "- `variance_weighted`: weighted mean using each output's target variance (outputs with more variance count more)\n",
    "\n",
    "This matters when different targets live on very different scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "y1 = 10 * rng.normal(size=n)  # high-variance target\n",
    "y2 = 0.2 * rng.normal(size=n)  # low-variance target\n",
    "y_true_mo = np.column_stack([y1, y2])\n",
    "\n",
    "# Predictions: do well on y2, poorly on y1\n",
    "y_pred_mo = np.column_stack(\n",
    "    [\n",
    "        y1 + rng.normal(0, 15, size=n),\n",
    "        y2 + rng.normal(0, 0.05, size=n),\n",
    "    ]\n",
    ")\n",
    "\n",
    "raw = explained_variance_score(y_true_mo, y_pred_mo, multioutput='raw_values')\n",
    "uniform = explained_variance_score(y_true_mo, y_pred_mo, multioutput='uniform_average')\n",
    "var_w = explained_variance_score(y_true_mo, y_pred_mo, multioutput='variance_weighted')\n",
    "\n",
    "raw, uniform, var_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['output 1 (high var)', 'output 2 (low var)'],\n",
    "        y=raw,\n",
    "        name='raw EVS',\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=f'Aggregation: uniform={uniform:.3f}, variance_weighted={var_w:.3f}',\n",
    "    yaxis_title='explained variance score',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Sample weights (`sample_weight`)\n",
    "\n",
    "With per-sample weights $w_i \\ge 0$, sklearn uses **weighted** means and variances:\n",
    "\n",
    "$$\n",
    "\\bar{z}_w = \\frac{\\sum_i w_i z_i}{\\sum_i w_i},\n",
    "\\qquad\n",
    "\\mathrm{Var}_w(z) = \\frac{\\sum_i w_i (z_i - \\bar{z}_w)^2}{\\sum_i w_i}.\n",
    "$$\n",
    "\n",
    "So samples with larger weight influence both the numerator and denominator more.\n",
    "A common use case is **heteroscedastic noise** (some samples are much noisier / less reliable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 240\n",
    "x = rng.normal(size=n)\n",
    "y_clean = 2.0 * x\n",
    "\n",
    "# First third is much noisier\n",
    "mask_noisy = np.arange(n) < (n // 3)\n",
    "noise = rng.normal(0.0, 0.2, size=n)\n",
    "noise[mask_noisy] = rng.normal(0.0, 1.5, size=mask_noisy.sum())\n",
    "\n",
    "y_true_w = y_clean + noise\n",
    "y_pred_w = y_clean  # model predicts the underlying signal\n",
    "\n",
    "weights = np.ones(n)\n",
    "weights[mask_noisy] = 0.2  # downweight noisy samples\n",
    "\n",
    "score_unweighted = explained_variance_score(y_true_w, y_pred_w)\n",
    "score_weighted = explained_variance_score(y_true_w, y_pred_w, sample_weight=weights)\n",
    "score_unweighted, score_weighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(x)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y_true_w,\n",
    "        mode='markers',\n",
    "        name='y_true',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=weights,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title='weight'),\n",
    "            opacity=0.85,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x[order],\n",
    "        y=y_pred_w[order],\n",
    "        mode='lines',\n",
    "        name='y_pred (signal)',\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=(\n",
    "        'Downweighting noisy samples increases EVS: '\n",
    "        f'unweighted={score_unweighted:.3f}, weighted={score_weighted:.3f}'\n",
    "    ),\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Optimizing EVS directly (linear regression)\n",
    "\n",
    "`explained_variance_score` is primarily an **evaluation** metric. But it's instructive to see what happens if you try to optimize it.\n",
    "\n",
    "Maximizing EVS is equivalent to minimizing the residual variance:\n",
    "\n",
    "$$\n",
    "\\max \\mathrm{EVS} \\quad \\Longleftrightarrow \\quad \\min \\mathrm{Var}(r), \\; r = y - \\hat{y}.\n",
    "$$\n",
    "\n",
    "For a linear model $\\hat{y} = Xw + b$, define centered residuals $r_c = r - \\bar{r}$ and the loss\n",
    "$L = \\frac{1}{n}\\sum_{i=1}^n r_{c,i}^2$.\n",
    "\n",
    "The gradients are\n",
    "\n",
    "$$\n",
    "\\nabla_w L = -\\frac{2}{n} X^\\top r_c,\n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial b} = -\\frac{2}{n}\\sum_i r_{c,i} = 0.\n",
    "$$\n",
    "\n",
    "So the intercept is **not identifiable** under this objective: any constant shift in $\\hat{y}$ leaves EVS unchanged.\n",
    "We'll see that in gradient descent: $b$ barely moves, EVS can look good, but $R^2$ / MSE reveal the bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_gd(\n",
    "    X,\n",
    "    y,\n",
    "    *,\n",
    "    loss,\n",
    "    lr=0.05,\n",
    "    n_steps=300,\n",
    "    w0=None,\n",
    "    b0=0.0,\n",
    "):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    w = np.zeros(n_features) if w0 is None else np.asarray(w0, dtype=float).copy()\n",
    "    b = float(b0)\n",
    "\n",
    "    history = {'step': [], 'mse': [], 'r2': [], 'evs': [], 'b': []}\n",
    "    for step in range(n_steps):\n",
    "        y_pred = X @ w + b\n",
    "        r = y - y_pred\n",
    "\n",
    "        if loss == 'mse':\n",
    "            grad_w = -(2.0 / n_samples) * (X.T @ r)\n",
    "            grad_b = -(2.0 / n_samples) * float(np.sum(r))\n",
    "        elif loss == 'resid_var':\n",
    "            r_c = r - r.mean()\n",
    "            grad_w = -(2.0 / n_samples) * (X.T @ r_c)\n",
    "            grad_b = -(2.0 / n_samples) * float(np.sum(r_c))  # always ~0\n",
    "        else:\n",
    "            raise ValueError(\"loss must be 'mse' or 'resid_var'\")\n",
    "\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "        y_pred = X @ w + b\n",
    "        history['step'].append(step)\n",
    "        history['mse'].append(mean_squared_error_np(y, y_pred))\n",
    "        history['r2'].append(r2_score_np(y, y_pred))\n",
    "        history['evs'].append(explained_variance_score_np(y, y_pred))\n",
    "        history['b'].append(b)\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "# Synthetic linear regression problem\n",
    "n = 500\n",
    "X = rng.normal(size=(n, 1))\n",
    "w_true = np.array([2.5])\n",
    "b_true = 1.7\n",
    "y = b_true + X @ w_true + rng.normal(0.0, 0.5, size=n)\n",
    "y = y.reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "w_mse, b_mse, hist_mse = fit_linear_gd(X_train, y_train, loss='mse', lr=0.05, n_steps=300)\n",
    "w_evs, b_evs, hist_evs = fit_linear_gd(X_train, y_train, loss='resid_var', lr=0.05, n_steps=300)\n",
    "\n",
    "\n",
    "def evaluate(X_, y_, w, b):\n",
    "    y_pred = X_ @ w + b\n",
    "    return {\n",
    "        'MSE': mean_squared_error_np(y_, y_pred),\n",
    "        'R2': r2_score_np(y_, y_pred),\n",
    "        'EVS': explained_variance_score_np(y_, y_pred),\n",
    "        'residual_mean': float(np.mean(y_ - y_pred)),\n",
    "    }\n",
    "\n",
    "\n",
    "b_evs_post = float(y_train.mean() - (X_train.mean(axis=0) @ w_evs))\n",
    "\n",
    "results = {\n",
    "    'train (MSE loss)': evaluate(X_train, y_train, w_mse, b_mse),\n",
    "    'test (MSE loss)': evaluate(X_test, y_test, w_mse, b_mse),\n",
    "    'train (EVS loss)': evaluate(X_train, y_train, w_evs, b_evs),\n",
    "    'test (EVS loss)': evaluate(X_test, y_test, w_evs, b_evs),\n",
    "    'test (EVS loss + post-hoc intercept)': evaluate(X_test, y_test, w_evs, b_evs_post),\n",
    "}\n",
    "\n",
    "{\n",
    "    'true': {'w': w_true, 'b': b_true},\n",
    "    'trained_on_mse': {'w': w_mse, 'b': b_mse},\n",
    "    'trained_on_evs': {'w': w_evs, 'b': b_evs},\n",
    "    'evs_posthoc_intercept': b_evs_post,\n",
    "    'metrics': results,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist_mse['step'],\n",
    "        y=hist_mse['evs'],\n",
    "        mode='lines',\n",
    "        name='EVS (trained on MSE)',\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist_evs['step'],\n",
    "        y=hist_evs['evs'],\n",
    "        mode='lines',\n",
    "        name='EVS (trained on Var(resid))',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist_mse['step'],\n",
    "        y=hist_mse['r2'],\n",
    "        mode='lines',\n",
    "        name='R2 (trained on MSE)',\n",
    "        line=dict(dash='dash'),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hist_evs['step'],\n",
    "        y=hist_evs['r2'],\n",
    "        mode='lines',\n",
    "        name='R2 (trained on Var(resid))',\n",
    "        line=dict(dash='dash'),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training curves: EVS can look good even with a biased intercept',\n",
    "    xaxis_title='gradient descent step',\n",
    "    yaxis_title='score',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist_mse['step'], y=hist_mse['b'], mode='lines', name='b (MSE loss)'))\n",
    "fig.add_trace(go.Scatter(x=hist_evs['step'], y=hist_evs['b'], mode='lines', name='b (EVS loss)'))\n",
    "fig.update_layout(\n",
    "    title='Intercept during training (EVS-loss has ~zero gradient)',\n",
    "    xaxis_title='step',\n",
    "    yaxis_title='b',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_mse = X_test @ w_mse + b_mse\n",
    "y_pred_test_evs = X_test @ w_evs + b_evs\n",
    "y_pred_test_evs_post = X_test @ w_evs + b_evs_post\n",
    "\n",
    "low = float(\n",
    "    np.min(\n",
    "        [y_test.min(), y_pred_test_mse.min(), y_pred_test_evs.min(), y_pred_test_evs_post.min()]\n",
    "    )\n",
    ")\n",
    "high = float(\n",
    "    np.max(\n",
    "        [y_test.max(), y_pred_test_mse.max(), y_pred_test_evs.max(), y_pred_test_evs_post.max()]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y_test, y=y_pred_test_mse, mode='markers', name='trained on MSE', opacity=0.7))\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=y_test, y=y_pred_test_evs, mode='markers', name='trained on Var(resid)', opacity=0.7)\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test,\n",
    "        y=y_pred_test_evs_post,\n",
    "        mode='markers',\n",
    "        name='Var(resid) + post-hoc intercept',\n",
    "        opacity=0.7,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[low, high],\n",
    "        y=[low, high],\n",
    "        mode='lines',\n",
    "        name='ideal',\n",
    "        line=dict(color='black', dash='dot'),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Predicted vs true (test): EVS-loss can be systematically shifted',\n",
    "    xaxis_title='y_true',\n",
    "    yaxis_title='y_pred',\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Practical usage (`scikit-learn`)\n",
    "\n",
    "In practice you almost never **train** by maximizing EVS; you train with a proper loss (e.g., MSE/MAE) and **evaluate** with EVS.\n",
    "Here's the standard sklearn pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = lin.predict(X_test)\n",
    "\n",
    "{\n",
    "    'coef': lin.coef_,\n",
    "    'intercept': lin.intercept_,\n",
    "    'EVS': explained_variance_score(y_test, y_pred),\n",
    "    'R2': r2_score(y_test, y_pred),\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Pros, cons, and when to use it\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Unitless** and easy to compare across scales (like $R^2$).\n",
    "- Directly measures how much **residual spread** remains relative to the target spread.\n",
    "- Supports **sample weighting** and **multi-output aggregation**.\n",
    "\n",
    "### Cons / pitfalls\n",
    "\n",
    "- **Ignores additive bias:** a constant offset does not change the score (can be very misleading).\n",
    "- Not a great training objective: intercept is not identifiable (zero gradient under the EVS-equivalent loss).\n",
    "- For nearly-constant targets, the metric can be unstable/ill-defined; `force_finite=True` can hide that.\n",
    "\n",
    "### Good use cases\n",
    "\n",
    "- You care about matching **fluctuations** and will correct offsets separately (post-hoc calibration / de-meaning).\n",
    "- Targets are naturally **centered/detrended**, so additive bias is not meaningful.\n",
    "- Multi-output settings where you want to quantify variance capture per output and then aggregate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Diagnostics: what to check alongside EVS\n",
    "\n",
    "Because EVS can be blind to bias, pair it with:\n",
    "\n",
    "- **Residual mean** (bias): $\\bar{r}$\n",
    "- **MAE / MSE / RMSE** (accuracy)\n",
    "- **$R^2$** (captures both variance + bias effects)\n",
    "- plots: $y$ vs $\\hat{y}$, residual histogram, residuals vs features/time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Exercises\n",
    "\n",
    "1) Construct two predictors with the **same** EVS but very different $R^2$. Explain using the MSE decomposition.\n",
    "2) Implement `multioutput` weighting by hand for 3 outputs and verify against sklearn.\n",
    "3) In the gradient descent section, initialize $b$ to a large value and show that the EVS-loss optimizer does not fix it.\n",
    "4) (Time series) Detrend a signal, fit a model, and compare EVS before/after detrending.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html\n",
    "- scikit-learn regression metrics overview: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
