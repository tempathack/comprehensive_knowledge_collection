{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Score (Jaccard Similarity / Intersection-over-Union)\n",
    "\n",
    "The **Jaccard score** measures similarity between two sets:\n",
    "\n",
    "$$\n",
    "J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "In ML, you'll often see the same idea as **Intersection-over-Union (IoU)** for binary masks.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Build intuition for **intersection vs union** (and why true negatives don't matter).\n",
    "- Derive the classification form: $\\displaystyle \\frac{TP}{TP+FP+FN}$.\n",
    "- Implement Jaccard from scratch in NumPy (binary, multiclass, multilabel).\n",
    "- Use Plotly to visualize how thresholds and errors change the score.\n",
    "- Optimize a tiny logistic regression model with a differentiable **soft Jaccard** loss.\n",
    "\n",
    "## Quick import (scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import jaccard_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "versions = {\n",
    "    'numpy': np.__version__,\n",
    "    'plotly': __import__('plotly').__version__,\n",
    "}\n",
    "try:\n",
    "    import sklearn\n",
    "\n",
    "    versions['sklearn'] = sklearn.__version__\n",
    "except Exception:\n",
    "    versions['sklearn'] = None\n",
    "\n",
    "versions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites & notation\n",
    "\n",
    "- Binary labels: $y \\in \\{0,1\\}^n$\n",
    "- Predicted labels: $\\hat{y} \\in \\{0,1\\}^n$\n",
    "- Predicted probabilities: $p \\in [0,1]^n$\n",
    "- Confusion counts: $TP$, $FP$, $FN$, $TN$\n",
    "\n",
    "We'll interpret the \"positive set\" as the indices where a vector equals 1:\n",
    "$A = \\{ i : y_i = 1 \\}$ and $B = \\{ i : \\hat{y}_i = 1 \\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Set intuition\n",
    "\n",
    "Think of two sets:\n",
    "\n",
    "- $A$: the \"true\" items\n",
    "- $B$: the \"predicted\" items\n",
    "\n",
    "The Jaccard score is:\n",
    "\n",
    "$$\n",
    "J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "- Numerator: what both agree on (**overlap**)\n",
    "- Denominator: everything that appears in either (**coverage**)\n",
    "\n",
    "So Jaccard is high only when the overlap is large *and* the union isn't bloated by extras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {1, 2, 3, 5, 8}\n",
    "B = {2, 3, 4, 8, 9}\n",
    "\n",
    "intersection = A & B\n",
    "union = A | B\n",
    "\n",
    "jaccard = len(intersection) / len(union)\n",
    "\n",
    "A, B, intersection, union, jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = np.arange(0, 10)\n",
    "\n",
    "A_mask = np.isin(universe, sorted(A))\n",
    "B_mask = np.isin(universe, sorted(B))\n",
    "\n",
    "# 0: neither, 1: A only, 2: B only, 3: both\n",
    "cat = A_mask.astype(int) + 2 * B_mask.astype(int)\n",
    "\n",
    "colorscale = [\n",
    "    [0.00, '#ffffff'],\n",
    "    [0.249999, '#ffffff'],  # neither\n",
    "    [0.25, '#ff7f0e'],\n",
    "    [0.499999, '#ff7f0e'],  # A only\n",
    "    [0.50, '#1f77b4'],\n",
    "    [0.749999, '#1f77b4'],  # B only\n",
    "    [0.75, '#2ca02c'],\n",
    "    [1.00, '#2ca02c'],  # both (intersection)\n",
    "]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cat[np.newaxis, :],\n",
    "        x=universe,\n",
    "        y=['elements'],\n",
    "        colorscale=colorscale,\n",
    "        zmin=-0.5,\n",
    "        zmax=3.5,\n",
    "        colorbar=dict(\n",
    "            title='category',\n",
    "            tickmode='array',\n",
    "            tickvals=[0, 1, 2, 3],\n",
    "            ticktext=['neither', 'A only', 'B only', 'A ∩ B'],\n",
    "        ),\n",
    "        hovertemplate='element=%{x}<br>category=%{z}<extra></extra>',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Jaccard = |A ∩ B| / |A ∪ B| = {len(intersection)}/{len(union)} = {jaccard:.3f}',\n",
    "    height=220,\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Binary classification view (TP / FP / FN)\n",
    "\n",
    "For binary classification, focus on the **positive class**:\n",
    "\n",
    "- $A = \\{ i : y_i = 1 \\}$ (true positives set)\n",
    "- $B = \\{ i : \\hat{y}_i = 1 \\}$ (predicted positives set)\n",
    "\n",
    "Then:\n",
    "\n",
    "- $|A \\cap B| = TP$\n",
    "- $|A \\cup B| = TP + FP + FN$\n",
    "\n",
    "So the Jaccard score becomes:\n",
    "\n",
    "$$\n",
    "J = \\frac{TP}{TP + FP + FN}\n",
    "$$\n",
    "\n",
    "Notice what's missing: **true negatives** $TN$.\n",
    "If your dataset has tons of negatives, accuracy can look great while Jaccard stays low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_counts_binary(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).astype(bool)\n",
    "    y_pred = np.asarray(y_pred).astype(bool)\n",
    "\n",
    "    tp = np.logical_and(y_true, y_pred).sum()\n",
    "    fp = np.logical_and(~y_true, y_pred).sum()\n",
    "    fn = np.logical_and(y_true, ~y_pred).sum()\n",
    "    tn = np.logical_and(~y_true, ~y_pred).sum()\n",
    "\n",
    "    return int(tp), int(fp), int(fn), int(tn)\n",
    "\n",
    "\n",
    "def jaccard_score_binary(y_true, y_pred, *, zero_division=0.0):\n",
    "    tp, fp, fn, _ = confusion_counts_binary(y_true, y_pred)\n",
    "    denom = tp + fp + fn\n",
    "    if denom == 0:\n",
    "        return float(zero_division)\n",
    "    return tp / denom\n",
    "\n",
    "\n",
    "def accuracy_score_binary(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    return (y_true == y_pred).mean()\n",
    "\n",
    "\n",
    "# quick sanity check\n",
    "y_true = np.array([1, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1])\n",
    "\n",
    "tp, fp, fn, tn = confusion_counts_binary(y_true, y_pred)\n",
    "(tp, fp, fn, tn), jaccard_score_binary(y_true, y_pred), accuracy_score_binary(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 IoU for segmentation (same formula)\n",
    "\n",
    "If $y$ and $\\hat{y}$ are **binary masks** (pixels in/out of an object), then:\n",
    "\n",
    "- intersection = pixels correctly predicted as object\n",
    "- union = pixels that are object in either mask\n",
    "\n",
    "So IoU = Jaccard on the set of \"object pixels\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 40, 40\n",
    "yy, xx = np.mgrid[0:h, 0:w]\n",
    "\n",
    "\n",
    "def circle_mask(*, cx, cy, r):\n",
    "    return (xx - cx) ** 2 + (yy - cy) ** 2 <= r**2\n",
    "\n",
    "\n",
    "true_mask = circle_mask(cx=14, cy=20, r=10)\n",
    "pred_mask = circle_mask(cx=18, cy=20, r=10)\n",
    "\n",
    "# 0: background, 1: true-only (FN), 2: pred-only (FP), 3: overlap (TP)\n",
    "cat = true_mask.astype(int) + 2 * pred_mask.astype(int)\n",
    "iou = jaccard_score_binary(true_mask.ravel(), pred_mask.ravel(), zero_division=1.0)\n",
    "\n",
    "colorscale = [\n",
    "    [0.00, '#ffffff'],\n",
    "    [0.249999, '#ffffff'],\n",
    "    [0.25, '#d62728'],\n",
    "    [0.499999, '#d62728'],  # true-only (red)\n",
    "    [0.50, '#1f77b4'],\n",
    "    [0.749999, '#1f77b4'],  # pred-only (blue)\n",
    "    [0.75, '#2ca02c'],\n",
    "    [1.00, '#2ca02c'],  # overlap (green)\n",
    "]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cat,\n",
    "        colorscale=colorscale,\n",
    "        zmin=-0.5,\n",
    "        zmax=3.5,\n",
    "        showscale=True,\n",
    "        colorbar=dict(\n",
    "            title='pixel',\n",
    "            tickmode='array',\n",
    "            tickvals=[0, 1, 2, 3],\n",
    "            ticktext=['background', 'true only (FN)', 'pred only (FP)', 'overlap (TP)'],\n",
    "        ),\n",
    "        hovertemplate='x=%{x}<br>y=%{y}<br>category=%{z}<extra></extra>',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'IoU (Jaccard) on a toy mask: {iou:.3f}',\n",
    "    width=520,\n",
    "    height=520,\n",
    "    yaxis=dict(scaleanchor='x', autorange='reversed'),\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Why true negatives don't matter\n",
    "\n",
    "Hold $TP$, $FP$, $FN$ fixed and add more and more true negatives.\n",
    "\n",
    "- Accuracy goes up (because it counts $TN$).\n",
    "- Jaccard stays exactly the same (because it ignores $TN$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = 10, 5, 5\n",
    "y_true_core = np.array([1] * tp + [1] * fn + [0] * fp, dtype=int)\n",
    "y_pred_core = np.array([1] * tp + [0] * fn + [1] * fp, dtype=int)\n",
    "\n",
    "tn_sizes = np.arange(0, 2001, 100)\n",
    "accs = []\n",
    "jaccs = []\n",
    "\n",
    "for tn in tn_sizes:\n",
    "    y_true_full = np.concatenate([y_true_core, np.zeros(tn, dtype=int)])\n",
    "    y_pred_full = np.concatenate([y_pred_core, np.zeros(tn, dtype=int)])\n",
    "\n",
    "    accs.append(accuracy_score_binary(y_true_full, y_pred_full))\n",
    "    jaccs.append(jaccard_score_binary(y_true_full, y_pred_full))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tn_sizes, y=accs, mode='lines+markers', name='accuracy'))\n",
    "fig.add_trace(go.Scatter(x=tn_sizes, y=jaccs, mode='lines+markers', name='jaccard'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Add more TN with TP={tp}, FP={fp}, FN={fn}: Jaccard stays constant',\n",
    "    xaxis_title='number of added true negatives (TN)',\n",
    "    yaxis_title='score',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) How FP and FN move Jaccard\n",
    "\n",
    "For fixed $TP$, Jaccard shrinks as you add either false positives or false negatives:\n",
    "\n",
    "$$\n",
    "J = \\frac{TP}{TP + FP + FN}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 10\n",
    "FP_vals = np.arange(0, 31)\n",
    "FN_vals = np.arange(0, 31)\n",
    "\n",
    "Z = np.zeros((len(FN_vals), len(FP_vals)), dtype=float)\n",
    "for i, fn in enumerate(FN_vals):\n",
    "    for j, fp in enumerate(FP_vals):\n",
    "        Z[i, j] = TP / (TP + fp + fn)\n",
    "\n",
    "fig = px.imshow(\n",
    "    Z,\n",
    "    x=FP_vals,\n",
    "    y=FN_vals,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    labels={'x': 'FP', 'y': 'FN', 'color': 'Jaccard'},\n",
    "    title=f'Jaccard for fixed TP={TP}',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Relationship to precision/recall/F1\n",
    "\n",
    "- Precision: $\\displaystyle P = \\frac{TP}{TP+FP}$\n",
    "- Recall: $\\displaystyle R = \\frac{TP}{TP+FN}$\n",
    "- F1: $\\displaystyle F_1 = \\frac{2TP}{2TP+FP+FN}$\n",
    "\n",
    "Jaccard uses the same ingredients but with a different denominator:\n",
    "\n",
    "$$\n",
    "J = \\frac{TP}{TP+FP+FN}\n",
    "$$\n",
    "\n",
    "A useful identity links Jaccard and F1:\n",
    "\n",
    "$$\n",
    "J = \\frac{F_1}{2 - F_1}\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "F_1 = \\frac{2J}{1 + J}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = np.linspace(0, 1, 501)\n",
    "j_from_f1 = f1 / (2 - f1)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=f1, y=j_from_f1, mode='lines', name='J = F1/(2-F1)'))\n",
    "fig.update_layout(\n",
    "    title='Mapping between F1 and Jaccard',\n",
    "    xaxis_title='F1',\n",
    "    yaxis_title='Jaccard',\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Multilabel and multiclass\n",
    "\n",
    "### Multilabel\n",
    "Each sample can have **multiple** positive labels.\n",
    "If $y, \\hat{y} \\in \\{0,1\\}^{n\\times L}$, you can compute Jaccard:\n",
    "\n",
    "- per-sample and average (**`samples`**)\n",
    "- per-label and average (**`macro`**)\n",
    "- globally over all entries (**`micro`**)\n",
    "\n",
    "### Multiclass\n",
    "With mutually-exclusive classes, a common definition is **one-vs-rest** per class and then average.\n",
    "This matches the way `sklearn.metrics.jaccard_score` generalizes Jaccard when `average != 'binary'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_divide(num, den, *, zero_division=0.0):\n",
    "    num = np.asarray(num, dtype=float)\n",
    "    den = np.asarray(den, dtype=float)\n",
    "\n",
    "    out = np.full_like(num, float(zero_division), dtype=float)\n",
    "    mask = den != 0\n",
    "    out[mask] = num[mask] / den[mask]\n",
    "    return out\n",
    "\n",
    "\n",
    "def jaccard_score_multilabel(y_true, y_pred, *, average='samples', zero_division=0.0):\n",
    "    y_true = np.asarray(y_true).astype(bool)\n",
    "    y_pred = np.asarray(y_pred).astype(bool)\n",
    "\n",
    "    if y_true.ndim != 2:\n",
    "        raise ValueError('Expected y_true with shape (n_samples, n_labels)')\n",
    "    if y_pred.shape != y_true.shape:\n",
    "        raise ValueError('y_pred must have the same shape as y_true')\n",
    "\n",
    "    if average == 'micro':\n",
    "        inter = np.logical_and(y_true, y_pred).sum()\n",
    "        uni = np.logical_or(y_true, y_pred).sum()\n",
    "        return float(_safe_divide(inter, uni, zero_division=zero_division))\n",
    "\n",
    "    if average in (None, 'none'):\n",
    "        inter_l = np.logical_and(y_true, y_pred).sum(axis=0)\n",
    "        uni_l = np.logical_or(y_true, y_pred).sum(axis=0)\n",
    "        return _safe_divide(inter_l, uni_l, zero_division=zero_division)\n",
    "\n",
    "    if average in ('macro', 'weighted'):\n",
    "        inter_l = np.logical_and(y_true, y_pred).sum(axis=0)\n",
    "        uni_l = np.logical_or(y_true, y_pred).sum(axis=0)\n",
    "        label_scores = _safe_divide(inter_l, uni_l, zero_division=zero_division)\n",
    "        if average == 'macro':\n",
    "            return float(label_scores.mean())\n",
    "\n",
    "        supports = y_true.sum(axis=0)\n",
    "        if supports.sum() == 0:\n",
    "            return float(zero_division)\n",
    "        return float(np.average(label_scores, weights=supports))\n",
    "\n",
    "    if average == 'samples':\n",
    "        inter_s = np.logical_and(y_true, y_pred).sum(axis=1)\n",
    "        uni_s = np.logical_or(y_true, y_pred).sum(axis=1)\n",
    "        sample_scores = _safe_divide(inter_s, uni_s, zero_division=zero_division)\n",
    "        return float(sample_scores.mean())\n",
    "\n",
    "    raise ValueError(\"average must be one of {'samples','micro','macro','weighted',None}\")\n",
    "\n",
    "\n",
    "def jaccard_score_multiclass(y_true, y_pred, *, average='macro', labels=None, zero_division=0.0):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.ndim != 1 or y_pred.ndim != 1:\n",
    "        raise ValueError('Expected 1D label arrays')\n",
    "    if y_pred.shape != y_true.shape:\n",
    "        raise ValueError('y_pred must have the same shape as y_true')\n",
    "    if len(y_true) == 0:\n",
    "        return float(zero_division)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "\n",
    "    scores = []\n",
    "    supports = []\n",
    "    for lab in labels:\n",
    "        t = y_true == lab\n",
    "        p = y_pred == lab\n",
    "        tp = np.logical_and(t, p).sum()\n",
    "        fp = np.logical_and(~t, p).sum()\n",
    "        fn = np.logical_and(t, ~p).sum()\n",
    "        denom = tp + fp + fn\n",
    "\n",
    "        score = float(zero_division) if denom == 0 else float(tp / denom)\n",
    "        scores.append(score)\n",
    "        supports.append(t.sum())\n",
    "\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    supports = np.asarray(supports, dtype=float)\n",
    "\n",
    "    if average == 'macro':\n",
    "        return float(scores.mean())\n",
    "    if average == 'weighted':\n",
    "        if supports.sum() == 0:\n",
    "            return float(zero_division)\n",
    "        return float(np.average(scores, weights=supports))\n",
    "    if average == 'micro':\n",
    "        correct = (y_true == y_pred).sum()\n",
    "        union = 2 * len(y_true) - correct\n",
    "        return float(zero_division) if union == 0 else float(correct / union)\n",
    "    if average in (None, 'none'):\n",
    "        return scores\n",
    "\n",
    "    raise ValueError(\"average must be one of {'micro','macro','weighted',None}\")\n",
    "\n",
    "\n",
    "# examples\n",
    "y_true_ml = np.array(\n",
    "    [\n",
    "        [1, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ],\n",
    "    dtype=int,\n",
    ")\n",
    "y_pred_ml = np.array(\n",
    "    [\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ],\n",
    "    dtype=int,\n",
    ")\n",
    "\n",
    "scores = {\n",
    "    'samples': jaccard_score_multilabel(y_true_ml, y_pred_ml, average='samples', zero_division=1.0),\n",
    "    'micro': jaccard_score_multilabel(y_true_ml, y_pred_ml, average='micro', zero_division=1.0),\n",
    "    'macro': jaccard_score_multilabel(y_true_ml, y_pred_ml, average='macro', zero_division=1.0),\n",
    "    'weighted': jaccard_score_multilabel(y_true_ml, y_pred_ml, average='weighted', zero_division=1.0),\n",
    "    'per-label': jaccard_score_multilabel(y_true_ml, y_pred_ml, average=None, zero_division=1.0),\n",
    "}\n",
    "\n",
    "y_true_mc = np.array([0, 1, 2, 2, 1, 0])\n",
    "y_pred_mc = np.array([0, 2, 2, 1, 1, 0])\n",
    "\n",
    "scores, {\n",
    "    'multiclass_macro': jaccard_score_multiclass(y_true_mc, y_pred_mc, average='macro'),\n",
    "    'multiclass_micro': jaccard_score_multiclass(y_true_mc, y_pred_mc, average='micro'),\n",
    "    'multiclass_per_class': jaccard_score_multiclass(y_true_mc, y_pred_mc, average=None),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import jaccard_score as sk_jaccard_score\n",
    "\n",
    "    print('Binary (sklearn):', sk_jaccard_score(y_true, y_pred))\n",
    "    print('Binary (ours):   ', jaccard_score_binary(y_true, y_pred))\n",
    "\n",
    "    print('Multilabel macro (sklearn):', sk_jaccard_score(y_true_ml, y_pred_ml, average='macro', zero_division=1.0))\n",
    "    print('Multilabel macro (ours):   ', jaccard_score_multilabel(y_true_ml, y_pred_ml, average='macro', zero_division=1.0))\n",
    "\n",
    "    print('Multiclass macro (sklearn):', sk_jaccard_score(y_true_mc, y_pred_mc, average='macro'))\n",
    "    print('Multiclass macro (ours):   ', jaccard_score_multiclass(y_true_mc, y_pred_mc, average='macro'))\n",
    "except Exception as e:\n",
    "    print('sklearn not available:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Thresholding probabilities\n",
    "\n",
    "Jaccard is defined on *sets* / *hard labels*.\n",
    "If your model outputs probabilities $p$, you typically choose a threshold $t$ and set:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\mathbf{1}[p_i \\ge t]\n",
    "$$\n",
    "\n",
    "Different thresholds trade off $FP$ vs $FN$, so they can change Jaccard a lot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "y_true_thr = rng.binomial(1, 0.15, size=n)\n",
    "\n",
    "# simulate a \"model score\": positives tend to have higher logits\n",
    "logits = rng.normal(loc=0.0, scale=1.0, size=n) + 1.5 * y_true_thr\n",
    "p_thr = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "thresholds = np.linspace(0.0, 1.0, 201)\n",
    "j_scores = np.array(\n",
    "    [jaccard_score_binary(y_true_thr, (p_thr >= t).astype(int), zero_division=0.0) for t in thresholds]\n",
    ")\n",
    "best_idx = int(j_scores.argmax())\n",
    "best_t = float(thresholds[best_idx])\n",
    "best_j = float(j_scores[best_idx])\n",
    "\n",
    "fig = px.line(\n",
    "    x=thresholds,\n",
    "    y=j_scores,\n",
    "    labels={'x': 'threshold', 'y': 'Jaccard'},\n",
    "    title=f'Jaccard vs threshold (best t≈{best_t:.2f}, J≈{best_j:.3f})',\n",
    ")\n",
    "fig.add_vline(x=best_t, line_dash='dash', line_color='black')\n",
    "fig.update_layout(yaxis=dict(range=[0, 1]))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Using Jaccard in optimization: a soft Jaccard loss\n",
    "\n",
    "The \"hard\" Jaccard score uses discrete predictions, so it's **not differentiable** w.r.t. model parameters.\n",
    "\n",
    "A common trick (especially in segmentation) is to replace hard predictions with probabilities $p$:\n",
    "\n",
    "- Soft intersection: $I = \\sum_i y_i p_i$\n",
    "- Soft union: $U = \\sum_i y_i + \\sum_i p_i - \\sum_i y_i p_i$\n",
    "\n",
    "Soft Jaccard:\n",
    "\n",
    "$$\n",
    "J_{soft}(y,p) = \\frac{I + \\varepsilon}{U + \\varepsilon}\n",
    "$$\n",
    "\n",
    "Soft Jaccard loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{IoU}(y,p) = 1 - J_{soft}(y,p)\n",
    "$$\n",
    "\n",
    "Gradient w.r.t. a probability $p_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{soft}}{\\partial p_i}\n",
    "= \\frac{y_i (U+\\varepsilon) - (I+\\varepsilon)(1-y_i)}{(U+\\varepsilon)^2}\n",
    "$$\n",
    "\n",
    "Then use the chain rule for logistic regression, where $p_i = \\sigma(x_i^\\top w)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic 2D binary classification (imbalanced)\n",
    "n0, n1 = 900, 100\n",
    "X0 = rng.normal(loc=[0.0, 0.0], scale=[1.0, 1.0], size=(n0, 2))\n",
    "X1 = rng.normal(loc=[2.0, 2.0], scale=[1.0, 1.0], size=(n1, 2))\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * n0 + [1] * n1, dtype=int)\n",
    "\n",
    "# shuffle\n",
    "perm = rng.permutation(len(y))\n",
    "X = X[perm]\n",
    "y = y[perm]\n",
    "\n",
    "# train/test split (pure NumPy)\n",
    "test_size = 0.30\n",
    "n_test = int(len(y) * test_size)\n",
    "\n",
    "X_test = X[:n_test]\n",
    "y_test = y[:n_test]\n",
    "X_train = X[n_test:]\n",
    "y_train = y[n_test:]\n",
    "\n",
    "# standardize (fit on train)\n",
    "mu = X_train.mean(axis=0)\n",
    "sigma = X_train.std(axis=0) + 1e-12\n",
    "X_train_s = (X_train - mu) / sigma\n",
    "X_test_s = (X_test - mu) / sigma\n",
    "\n",
    "# add bias column\n",
    "Xb_train = np.c_[np.ones(len(y_train)), X_train_s]\n",
    "Xb_test = np.c_[np.ones(len(y_test)), X_test_s]\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_train_s[:, 0],\n",
    "    y=X_train_s[:, 1],\n",
    "    color=y_train.astype(str),\n",
    "    title='Training data (standardized)',\n",
    "    labels={'color': 'class'},\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "Xb_train.shape, Xb_test.shape, float(y_train.mean()), float(y_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip(z, -60, 60)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def log_loss(y, p, *, eps=1e-12):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return float(-np.mean(y * np.log(p) + (1 - y) * np.log(1 - p)))\n",
    "\n",
    "\n",
    "def soft_jaccard_loss(y, p, *, eps=1e-12):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    I = np.sum(y * p)\n",
    "    U = np.sum(y) + np.sum(p) - I\n",
    "    return float(1.0 - (I + eps) / (U + eps))\n",
    "\n",
    "\n",
    "def soft_jaccard_grad_p(y, p, *, eps=1e-12):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    I = np.sum(y * p)\n",
    "    U = np.sum(y) + np.sum(p) - I\n",
    "    Ieps = I + eps\n",
    "    Ueps = U + eps\n",
    "    dJdp = (y * Ueps - Ieps * (1 - y)) / (Ueps**2)\n",
    "    return -dJdp\n",
    "\n",
    "\n",
    "def fit_logreg_gd(Xb, y, *, loss='log', lr=0.1, n_iter=400, l2=0.0, record_every=5):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    w = np.zeros(Xb.shape[1], dtype=float)\n",
    "\n",
    "    history = {'iter': [], 'loss': [], 'jaccard@0.5': []}\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        z = Xb @ w\n",
    "        p = sigmoid(z)\n",
    "\n",
    "        if loss == 'log':\n",
    "            L = log_loss(y, p) + 0.5 * l2 * np.sum(w[1:] ** 2)\n",
    "            grad = Xb.T @ (p - y) / len(y)\n",
    "            grad[1:] += l2 * w[1:]\n",
    "        elif loss == 'soft_jaccard':\n",
    "            L = soft_jaccard_loss(y, p) + 0.5 * l2 * np.sum(w[1:] ** 2)\n",
    "            dLdp = soft_jaccard_grad_p(y, p)\n",
    "            dLdz = dLdp * p * (1 - p)\n",
    "            grad = Xb.T @ dLdz / len(y)\n",
    "            grad[1:] += l2 * w[1:]\n",
    "        else:\n",
    "            raise ValueError(\"loss must be 'log' or 'soft_jaccard'\")\n",
    "\n",
    "        w -= lr * grad\n",
    "\n",
    "        if (t % record_every) == 0 or t == (n_iter - 1):\n",
    "            y_hat = (p >= 0.5).astype(int)\n",
    "            j = jaccard_score_binary(y.astype(int), y_hat, zero_division=0.0)\n",
    "            history['iter'].append(t)\n",
    "            history['loss'].append(float(L))\n",
    "            history['jaccard@0.5'].append(float(j))\n",
    "\n",
    "    return w, history\n",
    "\n",
    "\n",
    "def best_threshold_for_jaccard(y_true, p, thresholds):\n",
    "    scores = np.array(\n",
    "        [jaccard_score_binary(y_true, (p >= t).astype(int), zero_division=0.0) for t in thresholds], dtype=float\n",
    "    )\n",
    "    best_idx = int(scores.argmax())\n",
    "    return float(thresholds[best_idx]), float(scores[best_idx]), scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train two models:\n",
    "# - standard logistic regression (log-loss)\n",
    "# - logistic regression with a soft Jaccard loss\n",
    "\n",
    "w_log, hist_log = fit_logreg_gd(\n",
    "    Xb_train,\n",
    "    y_train,\n",
    "    loss='log',\n",
    "    lr=0.2,\n",
    "    n_iter=400,\n",
    "    l2=0.01,\n",
    "    record_every=5,\n",
    ")\n",
    "w_iou, hist_iou = fit_logreg_gd(\n",
    "    Xb_train,\n",
    "    y_train,\n",
    "    loss='soft_jaccard',\n",
    "    lr=1.0,\n",
    "    n_iter=400,\n",
    "    l2=0.01,\n",
    "    record_every=5,\n",
    ")\n",
    "\n",
    "# Evaluate on test\n",
    "p_test_log = sigmoid(Xb_test @ w_log)\n",
    "p_test_iou = sigmoid(Xb_test @ w_iou)\n",
    "\n",
    "j05_log = jaccard_score_binary(y_test, (p_test_log >= 0.5).astype(int), zero_division=0.0)\n",
    "j05_iou = jaccard_score_binary(y_test, (p_test_iou >= 0.5).astype(int), zero_division=0.0)\n",
    "\n",
    "ths = np.linspace(0.01, 0.99, 99)\n",
    "best_t_log, best_j_log, curve_log = best_threshold_for_jaccard(y_test, p_test_log, ths)\n",
    "best_t_iou, best_j_iou, curve_iou = best_threshold_for_jaccard(y_test, p_test_iou, ths)\n",
    "\n",
    "summary = {\n",
    "    'log_loss': {'J@0.5': j05_log, 'best_t': best_t_log, 'best_J': best_j_log},\n",
    "    'soft_jaccard': {'J@0.5': j05_iou, 'best_t': best_t_iou, 'best_J': best_j_iou},\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves (loss)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hist_log['iter'], y=hist_log['loss'], mode='lines', name='log-loss (train)'))\n",
    "fig.add_trace(go.Scatter(x=hist_iou['iter'], y=hist_iou['loss'], mode='lines', name='soft Jaccard loss (train)'))\n",
    "fig.update_layout(title='Training loss curves (different scales)', xaxis_title='iteration', yaxis_title='loss')\n",
    "fig.show()\n",
    "\n",
    "# Training curves (Jaccard at threshold 0.5)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hist_log['iter'], y=hist_log['jaccard@0.5'], mode='lines', name='log-loss model')\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hist_iou['iter'], y=hist_iou['jaccard@0.5'], mode='lines', name='soft Jaccard model')\n",
    ")\n",
    "fig.update_layout(\n",
    "    title='Training: Jaccard@0.5 over iterations',\n",
    "    xaxis_title='iteration',\n",
    "    yaxis_title='Jaccard@0.5',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Threshold tuning on test: maximize Jaccard\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ths, y=curve_log, mode='lines', name='log-loss model'))\n",
    "fig.add_trace(go.Scatter(x=ths, y=curve_iou, mode='lines', name='soft Jaccard model'))\n",
    "fig.add_vline(x=best_t_log, line_dash='dash', line_color='black')\n",
    "fig.add_vline(x=best_t_iou, line_dash='dash', line_color='gray')\n",
    "fig.update_layout(\n",
    "    title='Test: Jaccard vs threshold (vertical lines = best thresholds)',\n",
    "    xaxis_title='threshold',\n",
    "    yaxis_title='Jaccard',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Pros, cons, and where Jaccard shines\n",
    "\n",
    "### Pros\n",
    "- **Interpretable** overlap measure in $[0,1]$.\n",
    "- **Ignores true negatives**, which is great when negatives are overwhelming (e.g. segmentation background).\n",
    "- Natural fit for **sets**, **sparse binary features**, **multi-label** problems.\n",
    "- Symmetric: $J(A,B)=J(B,A)$.\n",
    "\n",
    "### Cons\n",
    "- Because it ignores $TN$, it can be misleading when correct negatives matter.\n",
    "- The \"hard\" metric is **non-differentiable**, so you usually optimize a surrogate.\n",
    "- For **small objects** in segmentation, a small boundary shift can drop IoU a lot.\n",
    "- For multiclass/multilabel, results depend heavily on the averaging choice (`micro` vs `macro` vs `samples`).\n",
    "\n",
    "### Good use cases\n",
    "- Image segmentation / detection masks (IoU)\n",
    "- Multi-label classification (tags)\n",
    "- Information retrieval and matching (set overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Pitfalls & diagnostics\n",
    "\n",
    "- **Union = 0 edge case**: if both sets are empty, Jaccard is undefined ($0/0$). Decide a convention (`zero_division`).\n",
    "- **Threshold choice**: Jaccard can change a lot with the threshold; tune it on a validation set.\n",
    "- **Averaging**:\n",
    "  - `micro` favors frequent labels/classes\n",
    "  - `macro` treats each label/class equally (often better for rare labels)\n",
    "  - `samples` answers: \"how good are we per example?\" (multilabel)\n",
    "- **Compare with precision/recall** to see whether low IoU comes from extra positives (FP) or missed positives (FN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Exercises\n",
    "\n",
    "1. Create two predictions with the same accuracy but very different Jaccard. Explain the difference using $TP/FP/FN/TN$.\n",
    "2. For multilabel data, build a case where `micro` is high but `macro` is low. What does that imply?\n",
    "3. Implement a **soft Dice** (F1) loss and compare its behavior to soft Jaccard on the same toy dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn `jaccard_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\n",
    "- IoU/Jaccard loss in segmentation (overview): https://en.wikipedia.org/wiki/Jaccard_index\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
