{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohen's Kappa (`cohen_kappa_score`)\n",
    "\n",
    "Cohen’s kappa ($\\kappa$) measures **agreement beyond chance** between two labelers.\n",
    "In ML, it’s commonly used to evaluate a classifier against ground truth, especially under **class imbalance**.\n",
    "\n",
    "$$\n",
    "\\kappa = \\frac{p_o - p_e}{1 - p_e}\n",
    "$$\n",
    "\n",
    "- $p_o$ = observed agreement (diagonal of the confusion matrix; for classifier-vs-truth it equals **accuracy**)\n",
    "- $p_e$ = expected agreement under independent marginals (chance agreement given the label prevalences)\n",
    "\n",
    "Range: $\\kappa\\in[-1, 1]$ (1 = perfect agreement, 0 = chance-level, <0 = worse than chance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- Build intuition for why $\\kappa$ differs from accuracy\n",
    "- Derive $\\kappa$ from the confusion matrix (binary + multiclass)\n",
    "- Implement $\\kappa$ from scratch in NumPy (including **weighted $\\kappa$** for ordinal labels)\n",
    "- Use $\\kappa$ in a simple optimization loop (threshold selection for a logistic regression model)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Confusion matrix basics\n",
    "- Basic probability (marginals, independence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Why $\\kappa$ (and not just accuracy)?\n",
    "\n",
    "Accuracy counts raw agreement.\n",
    "If the data are imbalanced, a trivial classifier can look “good” by mostly predicting the majority class.\n",
    "\n",
    "$\\kappa$ adjusts for the agreement you would expect **by chance**, given the label frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trivial “majority class” classifier\n",
    "y_true = np.r_[np.zeros(950, dtype=int), np.ones(50, dtype=int)]\n",
    "y_pred = np.zeros_like(y_true)\n",
    "\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f\"accuracy = {accuracy:.3f}\")\n",
    "print(f\"kappa    = {kappa:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) The math (confusion matrix → $\\kappa$)\n",
    "\n",
    "Let there be $K$ classes and a confusion matrix $C\\in\\mathbb{N}^{K\\times K}$ where\n",
    "$C_{ij}$ counts samples with **true** class $i$ predicted as class $j$.\n",
    "Let $N = \\sum_{i,j} C_{ij}$.\n",
    "\n",
    "**Observed agreement** (diagonal mass):\n",
    "$$\n",
    "p_o = \\frac{1}{N}\\sum_{i=1}^K C_{ii}\n",
    "$$\n",
    "\n",
    "Define the marginal class frequencies (as proportions):\n",
    "$$\n",
    "r_i = \\frac{1}{N}\\sum_{j=1}^K C_{ij} \\quad (\\text{true prevalence}), \\qquad\n",
    "c_i = \\frac{1}{N}\\sum_{j=1}^K C_{ji} \\quad (\\text{predicted prevalence}).\n",
    "$$\n",
    "\n",
    "If true and predicted labels were **independent** but kept the same marginals, the expected agreement is:\n",
    "$$\n",
    "p_e = \\sum_{i=1}^K r_i\\,c_i\n",
    "$$\n",
    "\n",
    "Finally:\n",
    "$$\n",
    "\\kappa = \\frac{p_o - p_e}{1 - p_e}\n",
    "$$\n",
    "\n",
    "Notes:\n",
    "- $\\kappa=1$ when $p_o=1$.\n",
    "- $\\kappa=0$ when $p_o=p_e$ (no better than chance under the marginals).\n",
    "- $\\kappa<0$ when agreement is worse than chance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_numpy(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    labels: np.ndarray | None = None,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Confusion matrix C where C[i, j] counts true=labels[i], pred=labels[j].\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape.\")\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    label_to_index = {label: i for i, label in enumerate(labels.tolist())}\n",
    "    true_idx = np.array([label_to_index[label] for label in y_true], dtype=int)\n",
    "    pred_idx = np.array([label_to_index[label] for label in y_pred], dtype=int)\n",
    "\n",
    "    k = labels.size\n",
    "    cm = np.zeros((k, k), dtype=int)\n",
    "    np.add.at(cm, (true_idx, pred_idx), 1)\n",
    "    return cm, labels\n",
    "\n",
    "\n",
    "def kappa_components_from_cm(cm: np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"Return (p_o, p_e) computed from a confusion matrix.\"\"\"\n",
    "    cm = np.asarray(cm)\n",
    "    n = cm.sum()\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Confusion matrix has zero total count.\")\n",
    "\n",
    "    p_o = float(np.trace(cm) / n)\n",
    "    row = cm.sum(axis=1) / n\n",
    "    col = cm.sum(axis=0) / n\n",
    "    p_e = float(row @ col)\n",
    "    return p_o, p_e\n",
    "\n",
    "\n",
    "def kappa_weight_matrix(n_classes: int, weights: str | np.ndarray | None) -> np.ndarray:\n",
    "    \"\"\"Disagreement weights W (0 on diagonal).\"\"\"\n",
    "    if n_classes < 1:\n",
    "        raise ValueError(\"n_classes must be >= 1\")\n",
    "\n",
    "    if weights is None:\n",
    "        w = np.ones((n_classes, n_classes), dtype=float)\n",
    "        np.fill_diagonal(w, 0.0)\n",
    "        return w\n",
    "\n",
    "    if isinstance(weights, str):\n",
    "        if weights not in {\"linear\", \"quadratic\"}:\n",
    "            raise ValueError(\"weights must be None, 'linear', 'quadratic', or a (K,K) array.\")\n",
    "\n",
    "        if n_classes == 1:\n",
    "            return np.zeros((1, 1), dtype=float)\n",
    "\n",
    "        idx = np.arange(n_classes)\n",
    "        i, j = np.meshgrid(idx, idx, indexing=\"ij\")\n",
    "        if weights == \"linear\":\n",
    "            return (np.abs(i - j) / (n_classes - 1)).astype(float)\n",
    "        return (((i - j) ** 2) / ((n_classes - 1) ** 2)).astype(float)\n",
    "\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    if w.shape != (n_classes, n_classes):\n",
    "        raise ValueError(f\"custom weights must have shape ({n_classes},{n_classes})\")\n",
    "    return w\n",
    "\n",
    "\n",
    "def cohen_kappa_score_numpy(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    labels: np.ndarray | None = None,\n",
    "    weights: str | np.ndarray | None = None,\n",
    ") -> float:\n",
    "    \"\"\"Cohen's kappa from scratch (unweighted or weighted).\"\"\"\n",
    "    cm, labels = confusion_matrix_numpy(y_true, y_pred, labels=labels)\n",
    "    n = cm.sum()\n",
    "    if n == 0:\n",
    "        raise ValueError(\"No samples.\")\n",
    "\n",
    "    o = cm / n\n",
    "    row = o.sum(axis=1)\n",
    "    col = o.sum(axis=0)\n",
    "    e = np.outer(row, col)\n",
    "\n",
    "    w = kappa_weight_matrix(labels.size, weights)\n",
    "    denom = float((w * e).sum())\n",
    "    num = float((w * o).sum())\n",
    "\n",
    "    # Degenerate case: expected disagreement is zero (perfectly concentrated marginals)\n",
    "    if np.isclose(denom, 0.0):\n",
    "        return 1.0\n",
    "\n",
    "    return 1.0 - (num / denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: match scikit-learn\n",
    "y_true = rng.integers(0, 4, size=1_000)\n",
    "y_pred = rng.integers(0, 4, size=1_000)\n",
    "\n",
    "k_np = cohen_kappa_score_numpy(y_true, y_pred)\n",
    "k_sk = cohen_kappa_score(y_true, y_pred)\n",
    "print(\"unweighted\", k_np, k_sk)\n",
    "assert np.isclose(k_np, k_sk)\n",
    "\n",
    "k_np_lin = cohen_kappa_score_numpy(y_true, y_pred, weights=\"linear\")\n",
    "k_sk_lin = cohen_kappa_score(y_true, y_pred, weights=\"linear\")\n",
    "print(\"linear   \", k_np_lin, k_sk_lin)\n",
    "assert np.isclose(k_np_lin, k_sk_lin)\n",
    "\n",
    "k_np_quad = cohen_kappa_score_numpy(y_true, y_pred, weights=\"quadratic\")\n",
    "k_sk_quad = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "print(\"quadratic\", k_np_quad, k_sk_quad)\n",
    "assert np.isclose(k_np_quad, k_sk_quad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Visual intuition: observed vs expected agreement\n",
    "\n",
    "$\\kappa$ depends on both:\n",
    "- how much mass is on the diagonal (observed agreement)\n",
    "- how concentrated the **marginals** are (chance agreement)\n",
    "\n",
    "Let’s look at a small multiclass example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.ndarray, labels: np.ndarray, title: str) -> go.Figure:\n",
    "    fig = px.imshow(\n",
    "        cm,\n",
    "        x=[str(l) for l in labels],\n",
    "        y=[str(l) for l in labels],\n",
    "        text_auto=True,\n",
    "        color_continuous_scale=\"Blues\",\n",
    "        title=title,\n",
    "        labels={\"x\": \"pred\", \"y\": \"true\", \"color\": \"count\"},\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.update_layout(coloraxis_showscale=False)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_true_vs_pred_marginals(cm: np.ndarray, labels: np.ndarray, title: str) -> go.Figure:\n",
    "    n = cm.sum()\n",
    "    true_marg = cm.sum(axis=1) / n\n",
    "    pred_marg = cm.sum(axis=0) / n\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=[str(l) for l in labels], y=true_marg, name=\"true\", opacity=0.8))\n",
    "    fig.add_trace(go.Bar(x=[str(l) for l in labels], y=pred_marg, name=\"pred\", opacity=0.8))\n",
    "    fig.update_layout(\n",
    "        barmode=\"group\",\n",
    "        title=title,\n",
    "        xaxis_title=\"class\",\n",
    "        yaxis_title=\"proportion\",\n",
    "    )\n",
    "    fig.update_yaxes(range=[0, 1])\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([0, 1, 2])\n",
    "y_true = np.r_[np.zeros(200, dtype=int), np.ones(150, dtype=int), np.full(50, 2, dtype=int)]\n",
    "\n",
    "y_pred = y_true.copy()\n",
    "flip_idx = rng.choice(y_true.size, size=80, replace=False)\n",
    "y_pred[flip_idx] = (y_true[flip_idx] + rng.integers(1, 3, size=flip_idx.size)) % 3\n",
    "\n",
    "cm, labels_used = confusion_matrix_numpy(y_true, y_pred, labels=labels)\n",
    "p_o, p_e = kappa_components_from_cm(cm)\n",
    "kappa = cohen_kappa_score_numpy(y_true, y_pred, labels=labels)\n",
    "\n",
    "print(f\"p_o (observed) = {p_o:.3f}\")\n",
    "print(f\"p_e (expected) = {p_e:.3f}\")\n",
    "print(f\"kappa          = {kappa:.3f}\")\n",
    "\n",
    "plot_confusion_matrix(cm, labels_used, title=\"Confusion matrix (example)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_vs_pred_marginals(cm, labels_used, title=\"True vs predicted marginals\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) How prevalence changes $\\kappa$\n",
    "\n",
    "Even if two classifiers have the same accuracy, $\\kappa$ can differ depending on class prevalence because $p_e$ changes with the marginals.\n",
    "\n",
    "Below we simulate a binary classifier with a fixed error rate: we flip the true label with probability $q$.\n",
    "Accuracy stays at $1-q$, but $\\kappa$ changes as the positive class prevalence moves toward 0 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_symmetric_noise(prevalences: np.ndarray, q: float, n: int = 50_000) -> tuple[np.ndarray, np.ndarray]:\n",
    "    accs: list[float] = []\n",
    "    kappas: list[float] = []\n",
    "    for pi in prevalences:\n",
    "        y = (rng.random(n) < pi).astype(int)\n",
    "        flip = rng.random(n) < q\n",
    "        y_hat = y.copy()\n",
    "        y_hat[flip] = 1 - y_hat[flip]\n",
    "        accs.append(float((y == y_hat).mean()))\n",
    "        kappas.append(cohen_kappa_score_numpy(y, y_hat))\n",
    "    return np.array(accs), np.array(kappas)\n",
    "\n",
    "\n",
    "prevalences = np.linspace(0.02, 0.98, 60)\n",
    "q = 0.10\n",
    "accs, kappas = simulate_symmetric_noise(prevalences, q=q)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=prevalences, y=accs, mode=\"lines\", name=\"accuracy\"))\n",
    "fig.add_trace(go.Scatter(x=prevalences, y=kappas, mode=\"lines\", name=\"kappa\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Fixed error rate q={q:.2f}: accuracy vs kappa across prevalence\",\n",
    "    xaxis_title=\"positive class prevalence π = P(y=1)\",\n",
    "    yaxis_title=\"metric value\",\n",
    ")\n",
    "fig.update_yaxes(range=[-0.1, 1.0])\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority-class predictor: accuracy can be high, but kappa stays ~0\n",
    "prevalences = np.linspace(0.02, 0.98, 60)\n",
    "n = 50_000\n",
    "\n",
    "accs: list[float] = []\n",
    "kappas: list[float] = []\n",
    "for pi in prevalences:\n",
    "    y = (rng.random(n) < pi).astype(int)\n",
    "    y_hat = np.zeros_like(y)\n",
    "    accs.append(float((y == y_hat).mean()))\n",
    "    kappas.append(cohen_kappa_score_numpy(y, y_hat))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=prevalences, y=accs, mode=\"lines\", name=\"accuracy\"))\n",
    "fig.add_trace(go.Scatter(x=prevalences, y=kappas, mode=\"lines\", name=\"kappa\"))\n",
    "fig.update_layout(\n",
    "    title=\"Always-predict-0 baseline across prevalence\",\n",
    "    xaxis_title=\"positive class prevalence π = P(y=1)\",\n",
    "    yaxis_title=\"metric value\",\n",
    ")\n",
    "fig.update_yaxes(range=[-0.1, 1.0])\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Weighted $\\kappa$ (ordinal labels)\n",
    "\n",
    "If classes are **ordered** (e.g., 1–5 star ratings), “off by 1” is usually less severe than “off by 4”.\n",
    "Weighted kappa replaces exact agreement with a **weighted disagreement**.\n",
    "\n",
    "Let $O_{ij}=C_{ij}/N$ be the observed joint distribution and $E_{ij}=r_i c_j$ the expected joint distribution.\n",
    "Choose disagreement weights $w_{ij}$ with $w_{ii}=0$.\n",
    "\n",
    "$$\n",
    "\\kappa_w = 1 - \\frac{\\sum_{i,j} w_{ij} O_{ij}}{\\sum_{i,j} w_{ij} E_{ij}}\n",
    "$$\n",
    "\n",
    "Common choices:\n",
    "- linear: $w_{ij}=\\frac{|i-j|}{K-1}$\n",
    "- quadratic: $w_{ij}=\\frac{(i-j)^2}{(K-1)^2}$\n",
    "\n",
    "These only make sense when the class order is meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_matrix(w: np.ndarray, labels: np.ndarray, title: str) -> go.Figure:\n",
    "    fig = px.imshow(\n",
    "        w,\n",
    "        x=[str(l) for l in labels],\n",
    "        y=[str(l) for l in labels],\n",
    "        text_auto=\".2f\",\n",
    "        color_continuous_scale=\"Reds\",\n",
    "        title=title,\n",
    "        labels={\"x\": \"pred\", \"y\": \"true\", \"color\": \"disagreement\"},\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "k = 5\n",
    "labels = np.arange(k)\n",
    "y_true = rng.integers(0, k, size=2_000)\n",
    "noise = rng.choice([-2, -1, 0, 1, 2], size=y_true.size, p=[0.05, 0.15, 0.60, 0.15, 0.05])\n",
    "y_pred = np.clip(y_true + noise, 0, k - 1)\n",
    "\n",
    "k_unw = cohen_kappa_score_numpy(y_true, y_pred)\n",
    "k_lin = cohen_kappa_score_numpy(y_true, y_pred, weights=\"linear\")\n",
    "k_quad = cohen_kappa_score_numpy(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "print(f\"unweighted kappa = {k_unw:.3f}\")\n",
    "print(f\"linear kappa     = {k_lin:.3f}\")\n",
    "print(f\"quadratic kappa  = {k_quad:.3f}\")\n",
    "\n",
    "cm, labels_used = confusion_matrix_numpy(y_true, y_pred, labels=labels)\n",
    "plot_confusion_matrix(cm, labels_used, title=\"Ordinal example: confusion matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lin = kappa_weight_matrix(k, \"linear\")\n",
    "w_quad = kappa_weight_matrix(k, \"quadratic\")\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Linear weights\", \"Quadratic weights\"])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=w_lin, x=labels.astype(str), y=labels.astype(str), colorscale=\"Reds\", showscale=False),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=w_quad, x=labels.astype(str), y=labels.astype(str), colorscale=\"Reds\", showscale=False),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(title=\"Disagreement weights (larger = more severe)\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Using $\\kappa$ for optimization (threshold tuning)\n",
    "\n",
    "$\\kappa$ is computed from **discrete** predictions, so it is not differentiable with respect to model parameters.\n",
    "That means you usually *don’t* train a model by directly gradient-descenting on $\\kappa$.\n",
    "\n",
    "A common pattern is:\n",
    "1. Train a probabilistic model with a differentiable objective (e.g., log loss)\n",
    "2. Pick a decision threshold on a validation set to maximize $\\kappa$\n",
    "\n",
    "Below: logistic regression trained from scratch with gradient descent, then a threshold sweep to maximize $\\kappa$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    z = np.asarray(z)\n",
    "    out = np.empty_like(z, dtype=float)\n",
    "    pos = z >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    exp_z = np.exp(z[~pos])\n",
    "    out[~pos] = exp_z / (1.0 + exp_z)\n",
    "    return out\n",
    "\n",
    "\n",
    "def standardize_fit(X: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    std = np.where(std == 0.0, 1.0, std)\n",
    "    return (X - mean) / std, mean, std\n",
    "\n",
    "\n",
    "def standardize_transform(X: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "\n",
    "def log_loss_from_logits(y: np.ndarray, logits: np.ndarray) -> float:\n",
    "    # Stable binary cross-entropy: mean(log(1+exp(z)) - y*z)\n",
    "    return float(np.mean(np.logaddexp(0.0, logits) - y * logits))\n",
    "\n",
    "\n",
    "def fit_logistic_regression_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    lr: float = 0.2,\n",
    "    n_iters: int = 2_000,\n",
    "    l2: float = 0.0,\n",
    "    record_every: int = 20,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (weights, steps, losses). L2 excludes the intercept term.\"\"\"\n",
    "    X_i = add_intercept(X)\n",
    "    y = y.astype(float)\n",
    "    w = np.zeros(X_i.shape[1], dtype=float)\n",
    "\n",
    "    steps: list[int] = []\n",
    "    losses: list[float] = []\n",
    "\n",
    "    for step in range(1, n_iters + 1):\n",
    "        logits = X_i @ w\n",
    "        p = sigmoid(logits)\n",
    "\n",
    "        grad = (X_i.T @ (p - y)) / y.size\n",
    "        grad[1:] += l2 * w[1:]\n",
    "        w -= lr * grad\n",
    "\n",
    "        if step % record_every == 0 or step == 1:\n",
    "            steps.append(step)\n",
    "            losses.append(log_loss_from_logits(y, logits) + 0.5 * l2 * float(w[1:] @ w[1:]))\n",
    "\n",
    "    return w, np.array(steps), np.array(losses)\n",
    "\n",
    "\n",
    "def predict_proba_logreg(X: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    X_i = add_intercept(X)\n",
    "    return sigmoid(X_i @ w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic, slightly imbalanced binary classification problem\n",
    "n0, n1 = 2_000, 500\n",
    "mean0 = np.array([-1.2, -0.2])\n",
    "mean1 = np.array([1.0, 0.9])\n",
    "cov = np.array([[1.0, 0.45], [0.45, 1.0]])\n",
    "\n",
    "X0 = rng.multivariate_normal(mean0, cov, size=n0)\n",
    "X1 = rng.multivariate_normal(mean1, cov, size=n1)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.r_[np.zeros(n0, dtype=int), np.ones(n1, dtype=int)]\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize based on train only\n",
    "X_train_s, mean, std = standardize_fit(X_train)\n",
    "X_val_s = standardize_transform(X_val, mean, std)\n",
    "X_test_s = standardize_transform(X_test, mean, std)\n",
    "\n",
    "w, steps, losses = fit_logistic_regression_gd(X_train_s, y_train, lr=0.2, n_iters=2_000, l2=0.1)\n",
    "\n",
    "fig = px.line(x=steps, y=losses, title=\"Logistic regression (GD): training objective\")\n",
    "fig.update_layout(xaxis_title=\"iteration\", yaxis_title=\"log loss + L2\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_thresholds(y_true: np.ndarray, proba: np.ndarray, thresholds: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    kappas = np.empty_like(thresholds, dtype=float)\n",
    "    accs = np.empty_like(thresholds, dtype=float)\n",
    "    for i, t in enumerate(thresholds):\n",
    "        y_hat = (proba >= t).astype(int)\n",
    "        kappas[i] = cohen_kappa_score_numpy(y_true, y_hat)\n",
    "        accs[i] = float((y_true == y_hat).mean())\n",
    "    return kappas, accs\n",
    "\n",
    "\n",
    "proba_val = predict_proba_logreg(X_val_s, w)\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "kappas, accs = sweep_thresholds(y_val, proba_val, thresholds)\n",
    "\n",
    "best_idx = int(np.argmax(kappas))\n",
    "best_t = float(thresholds[best_idx])\n",
    "\n",
    "print(f\"best threshold (val) = {best_t:.2f}\")\n",
    "print(f\"best kappa (val)     = {kappas[best_idx]:.3f}\")\n",
    "print(f\"accuracy @ best_t    = {accs[best_idx]:.3f}\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=kappas, mode=\"lines\", name=\"kappa\"))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=accs, mode=\"lines\", name=\"accuracy\", opacity=0.7))\n",
    "fig.add_vline(x=best_t, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(\n",
    "    title=\"Validation sweep: kappa and accuracy vs threshold\",\n",
    "    xaxis_title=\"threshold\",\n",
    "    yaxis_title=\"metric value\",\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_at_threshold(y_true: np.ndarray, proba: np.ndarray, t: float) -> dict[str, float]:\n",
    "    y_hat = (proba >= t).astype(int)\n",
    "    cm, _ = confusion_matrix_numpy(y_true, y_hat, labels=np.array([0, 1]))\n",
    "    return {\n",
    "        \"threshold\": float(t),\n",
    "        \"accuracy\": float((y_true == y_hat).mean()),\n",
    "        \"kappa\": cohen_kappa_score_numpy(y_true, y_hat),\n",
    "        \"tn\": float(cm[0, 0]),\n",
    "        \"fp\": float(cm[0, 1]),\n",
    "        \"fn\": float(cm[1, 0]),\n",
    "        \"tp\": float(cm[1, 1]),\n",
    "    }\n",
    "\n",
    "\n",
    "proba_test = predict_proba_logreg(X_test_s, w)\n",
    "\n",
    "m_default = eval_at_threshold(y_test, proba_test, t=0.5)\n",
    "m_best = eval_at_threshold(y_test, proba_test, t=best_t)\n",
    "\n",
    "print(\"test @ t=0.50:\", m_default)\n",
    "print(\"test @ best_t:\", m_best)\n",
    "\n",
    "cm_default, _ = confusion_matrix_numpy(y_test, (proba_test >= 0.5).astype(int), labels=np.array([0, 1]))\n",
    "cm_best, _ = confusion_matrix_numpy(y_test, (proba_test >= best_t).astype(int), labels=np.array([0, 1]))\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"t = 0.50\", f\"t = {best_t:.2f}\"])\n",
    "fig.add_trace(go.Heatmap(z=cm_default, x=[\"0\", \"1\"], y=[\"0\", \"1\"], colorscale=\"Blues\", showscale=False), row=1, col=1)\n",
    "fig.add_trace(go.Heatmap(z=cm_best, x=[\"0\", \"1\"], y=[\"0\", \"1\"], colorscale=\"Blues\", showscale=False), row=1, col=2)\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(title=\"Test confusion matrices: default vs kappa-optimized threshold\", xaxis_title=\"pred\", yaxis_title=\"true\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Pros, cons, and good use cases\n",
    "\n",
    "**Pros**\n",
    "- Chance-corrected agreement: unlike accuracy, it explicitly subtracts expected agreement from the marginals\n",
    "- Works for binary and multiclass classification\n",
    "- Weighted variants handle **ordinal** labels naturally\n",
    "\n",
    "**Cons / caveats**\n",
    "- Sensitive to class prevalence and prediction bias (the “prevalence paradox”): the same accuracy can yield different $\\kappa$\n",
    "- Not defined for probabilistic outputs; you must pick a threshold (or argmax) first\n",
    "- Not differentiable → usually not a direct training loss (use it for model selection / threshold tuning)\n",
    "- Can be unstable on small datasets (marginals estimated with high variance)\n",
    "\n",
    "**Good use cases**\n",
    "- Inter-annotator reliability (two humans labeling the same items)\n",
    "- Evaluating classifiers when you care about going beyond “matching prevalence” baselines\n",
    "- Ordinal classification (use weighted $\\kappa$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Construct a dataset where accuracy increases but $\\kappa$ decreases (hint: shift the marginals).\n",
    "2. For the threshold-sweep demo, optimize for accuracy instead of $\\kappa$. How do the chosen thresholds differ?\n",
    "3. Create your own custom weight matrix for ordinal labels (e.g., asymmetric penalties) and compute $\\kappa_w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- scikit-learn API: `sklearn.metrics.cohen_kappa_score`\n",
    "- Cohen, J. (1960). *A coefficient of agreement for nominal scales*.\n",
    "- Weighted kappa: commonly used for ordinal agreement (e.g., clinical ratings).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
