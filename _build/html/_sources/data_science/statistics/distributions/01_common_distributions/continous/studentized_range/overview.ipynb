{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9e5c2a",
   "metadata": {},
   "source": [
    "# Studentized Range Distribution (`studentized_range`)\n",
    "\n",
    "The **studentized range** distribution is the null distribution of a key statistic in **multiple comparisons**: the (scaled) range of \\(k\\) normal means when the variance is **estimated from data**.\n",
    "\n",
    "It is the mathematical backbone behind **Tukey’s HSD** procedure (and related simultaneous confidence intervals) after one-way ANOVA.\n",
    "\n",
    "## What you’ll learn\n",
    "- what the studentized range statistic measures (and why it’s “studentized”)\n",
    "- the parameter roles: number of groups \\(k\\) and degrees of freedom \\(\\nu\\)\n",
    "- an integral-form PDF/CDF (and why there is no simple closed form)\n",
    "- when moments exist (and how to compute them numerically / by Monte Carlo)\n",
    "- a NumPy-only simulator from the defining construction\n",
    "- practical SciPy usage: `scipy.stats.studentized_range` (`pdf`, `cdf`, `rvs`, `fit`)\n",
    "- how it appears in Tukey-style hypothesis tests and modeling workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e4ba8",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & classification\n",
    "2) Intuition & motivation\n",
    "3) Formal definition (PDF/CDF)\n",
    "4) Moments & properties\n",
    "5) Parameter interpretation\n",
    "6) Derivations (expectation, variance, likelihood)\n",
    "7) Sampling & simulation (NumPy-only)\n",
    "8) Visualization (PDF, CDF, Monte Carlo)\n",
    "9) SciPy integration (`scipy.stats.studentized_range`)\n",
    "10) Statistical use cases\n",
    "11) Pitfalls\n",
    "12) Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import gammaln\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9ca8b",
   "metadata": {},
   "source": [
    "## Prerequisites & notation\n",
    "\n",
    "**Prerequisites**\n",
    "- comfort with continuous distributions (PDF/CDF)\n",
    "- basic calculus and change-of-variables\n",
    "- familiarity with the \\(\\chi^2\\) distribution and the Gamma function \\(\\Gamma(\\cdot)\\)\n",
    "- basic ideas from order statistics (max/min/range)\n",
    "\n",
    "**Notation**\n",
    "- \\(k\\): number of groups / number of normal means being compared (typically an integer \\(k\\ge 2\\)).\n",
    "- \\(\\nu\\): degrees of freedom for the variance estimate (\\(\\nu>0\\)).\n",
    "- \\(\\phi\\) and \\(\\Phi\\): standard normal PDF and CDF.\n",
    "- \\(\\chi^2_\\nu\\): chi-square distribution with \\(\\nu\\) degrees of freedom.\n",
    "\n",
    "In SciPy, the distribution is exposed as `scipy.stats.studentized_range` with shape parameters `(k, df)` (where `df` corresponds to \\(\\nu\\)), plus the usual `loc` and `scale`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a85f9f",
   "metadata": {},
   "source": [
    "## 1) Title & classification\n",
    "\n",
    "- **Name**: `studentized_range` (studentized range distribution)\n",
    "- **Type**: **continuous**\n",
    "- **Support**: \\(q \\in [0, \\infty)\\)\n",
    "- **Parameter space**:\n",
    "  - \\(k \\ge 2\\) (typically an integer)\n",
    "  - \\(\\nu > 0\\) degrees of freedom\n",
    "\n",
    "A location-scale family also exists: if \\(Q\\sim \\mathrm{studentized\\_range}(k,\\nu)\\), then \\(X = \\mathrm{loc} + \\mathrm{scale}\\,Q\\) is supported on \\([\\mathrm{loc}, \\infty)\\). SciPy provides this via the standard `loc` and `scale` arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a39d6b",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What this distribution models\n",
    "The studentized range distribution describes the random variable\n",
    "\n",
    "\\[\n",
    "Q \\,=\\, \\frac{\\max_i \\bar{Y}_i - \\min_i \\bar{Y}_i}{\\hat\\sigma\\,/\\,\\sqrt{n}},\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(\\bar{Y}_1,\\dots,\\bar{Y}_k\\) are \\(k\\) sample means,\n",
    "- each mean is based on \\(n\\) observations (often equal group sizes), and\n",
    "- \\(\\hat\\sigma\\) is an estimate of the common standard deviation built from an independent \\(\\chi^2\\)-type sum of squares.\n",
    "\n",
    "In words: **how far apart are the largest and smallest means, measured in estimated standard error units?**\n",
    "\n",
    "The “studentized” part means we divide by an **estimated** standard deviation rather than the true \\(\\sigma\\).\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Tukey’s HSD / multiple comparisons after ANOVA**: control family-wise error across all pairwise mean comparisons.\n",
    "- **Simultaneous confidence intervals**: for differences between many means in a balanced one-way ANOVA.\n",
    "- **Quality / A/B testing / experiments**: quantifying the largest observed separation among several treatments while accounting for variance estimation.\n",
    "\n",
    "### Relations to other distributions\n",
    "- **\\(k=2\\)**: the range of two normals is an absolute difference; one can show\n",
    "  \\[\n",
    "  Q \\;\\overset{d}{=}\\; \\sqrt{2}\\,|T_\\nu|,\n",
    "  \\]\n",
    "  where \\(T_\\nu\\) is Student’s \\(t\\) with \\(\\nu\\) degrees of freedom.\n",
    "- **\\(\\nu\\to\\infty\\)**: the variance estimate becomes exact and \\(Q\\) approaches the **range of \\(k\\) standard normals**.\n",
    "- **Large \\(k\\)**: behavior is driven by extremes (max/min), so after centering/scaling it connects to extreme-value ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85798f6e",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "A clean way to define the studentized range is through a *construction*.\n",
    "\n",
    "Let\n",
    "- \\(Z_1,\\dots,Z_k \\overset{iid}{\\sim} \\mathcal{N}(0,1)\\)\n",
    "- \\(V \\sim \\chi^2_\\nu\\), independent of the \\(Z_i\\)\n",
    "\n",
    "Define the normal range\n",
    "\\[\n",
    "R = \\max_i Z_i - \\min_i Z_i \\;\\;\\; (R\\ge 0)\n",
    "\\]\n",
    "and the studentizing scale\n",
    "\\[\n",
    "S = \\sqrt{V/\\nu} \\;\\;\\; (S>0).\n",
    "\\]\n",
    "\n",
    "Then the **studentized range** random variable is\n",
    "\\[\n",
    "Q = \\frac{R}{S} = \\frac{\\max_i Z_i - \\min_i Z_i}{\\sqrt{V/\\nu}} \\;\\;\\; (Q\\ge 0).\n",
    "\\]\n",
    "\n",
    "### PDF / CDF (integral form)\n",
    "There is no simple closed-form PDF/CDF. One standard representation mixes the **range distribution of normals** over the distribution of \\(S\\).\n",
    "\n",
    "For a continuous parent distribution with PDF \\(f\\) and CDF \\(F\\), the range \\(R\\) of \\(k\\) i.i.d. samples has:\n",
    "\n",
    "\\[\n",
    "F_R(r) = k\\int_{-\\infty}^{\\infty} f(x)\\,[F(x+r)-F(x)]^{k-1}\\,dx, \\quad r\\ge 0\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "f_R(r) = k(k-1)\\int_{-\\infty}^{\\infty} f(x)\\,f(x+r)\\,[F(x+r)-F(x)]^{k-2}\\,dx, \\quad r\\ge 0.\n",
    "\\]\n",
    "\n",
    "For the standard normal, \\(f=\\phi\\) and \\(F=\\Phi\\).\n",
    "\n",
    "Because \\(Q = R/S\\) with \\(R\\perp S\\), we can write:\n",
    "\n",
    "\\[\n",
    "F_Q(q\\,;k,\\nu) = \\int_0^\\infty F_R(qs\\,;k)\\,f_S(s\\,;\\nu)\\,ds, \\quad q\\ge 0\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "f_Q(q\\,;k,\\nu) = \\int_0^\\infty s\\,f_R(qs\\,;k)\\,f_S(s\\,;\\nu)\\,ds, \\quad q\\ge 0,\n",
    "\\]\n",
    "\n",
    "where \\(S=\\sqrt{V/\\nu}\\) and its PDF is\n",
    "\n",
    "\\[\n",
    "f_S(s\\,;\\nu) = \\frac{2\\,\\nu^{\\nu/2}}{2^{\\nu/2}\\,\\Gamma(\\nu/2)}\\,s^{\\nu-1}\\,\\exp\\left(-\\frac{\\nu s^2}{2}\\right),\\quad s>0.\n",
    "\\]\n",
    "\n",
    "In practice, libraries (including SciPy) evaluate the distribution via specialized numerical integration routines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f8635",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "### Moment existence\n",
    "Write\n",
    "\\[\n",
    "Q = R\\,\\sqrt{\\nu/V},\\quad R\\perp V.\n",
    "\\]\n",
    "\n",
    "Since \\(R\\) (a range of Gaussians) has finite moments of all orders, the existence of \\(\\mathbb{E}[Q^m]\\) is controlled by the inverse-\\(\\chi^2\\) term.\n",
    "\n",
    "Using \\(V\\sim\\chi^2_\\nu\\), one can show:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[Q^m] < \\infty \\quad \\Longleftrightarrow \\quad \\nu > m.\n",
    "\\]\n",
    "\n",
    "In particular:\n",
    "- **Mean** exists if \\(\\nu>1\\)\n",
    "- **Variance** exists if \\(\\nu>2\\)\n",
    "- **Skewness** exists if \\(\\nu>3\\)\n",
    "- **(Excess) kurtosis** exists if \\(\\nu>4\\)\n",
    "\n",
    "### General moment formula (factorization)\n",
    "For \\(m<\\nu\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[Q^m] = \\mathbb{E}[R^m]\\,\\mathbb{E}\\!\\left[(\\nu/V)^{m/2}\\right].\n",
    "\\]\n",
    "\n",
    "The inverse-\\(\\chi^2\\) factor has a closed form:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}\\!\\left[(\\nu/V)^{m/2}\\right]\n",
    "= \\left(\\frac{\\nu}{2}\\right)^{m/2}\\,\\frac{\\Gamma\\left((\\nu-m)/2\\right)}{\\Gamma(\\nu/2)},\\quad \\nu>m.\n",
    "\\]\n",
    "\n",
    "What remains is \\(\\mathbb{E}[R^m]\\), the \\(m\\)-th moment of the range of \\(k\\) standard normals, which typically has to be computed numerically (or by Monte Carlo).\n",
    "\n",
    "### MGF / characteristic function\n",
    "- The **MGF** \\(M_Q(t)=\\mathbb{E}[e^{tQ}]\\) does **not** exist (it diverges for any \\(t>0\\)) because \\(Q\\) has polynomially decaying tails (similar to the \\(t\\) distribution).\n",
    "- The **characteristic function** \\(\\varphi_Q(t)=\\mathbb{E}[e^{itQ}]\\) exists for all real \\(t\\) but has no simple closed form; it can be approximated numerically.\n",
    "\n",
    "### Entropy\n",
    "The differential entropy\n",
    "\n",
    "\\[\n",
    "h(Q) = -\\int_0^\\infty f_Q(q)\\,\\log f_Q(q)\\,dq\n",
    "\\]\n",
    "\n",
    "generally has no simple closed form and is typically computed numerically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moments in SciPy (computed numerically)\n",
    "\n",
    "k, df = 5, 10\n",
    "\n",
    "mean, var, skew, ex_kurt = stats.studentized_range.stats(k, df, moments=\"mvsk\")\n",
    "print(f\"k={k}, df={df}\")\n",
    "print(\"mean              \", float(mean))\n",
    "print(\"variance          \", float(var))\n",
    "print(\"skewness          \", float(skew))\n",
    "print(\"excess kurtosis   \", float(ex_kurt))\n",
    "\n",
    "# Monte Carlo check (SciPy sampling)\n",
    "n = 80_000\n",
    "samples = stats.studentized_range.rvs(k, df, size=n, random_state=rng)\n",
    "print(\"\\nMonte Carlo (SciPy rvs)\")\n",
    "print(\"mean    \", float(samples.mean()))\n",
    "print(\"var     \", float(samples.var()))\n",
    "\n",
    "# Moment existence demo: when df <= m, E[Q^m] diverges.\n",
    "print(\"\\nMoment existence (SciPy may return inf/nan when moments diverge):\")\n",
    "for df_test in [0.8, 1.0, 1.5, 2.0, 3.0, 5.0]:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        m, v, s, krt = stats.studentized_range.stats(3, df_test, moments=\"mvsk\")\n",
    "    print(f\"df={df_test:>4}: mean={m}, var={v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e49dda",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "The parameters control two intuitive “sources of size” in the statistic:\n",
    "\n",
    "- **\\(k\\)** (number of groups / means): increasing \\(k\\) increases the typical separation between the largest and smallest of \\(k\\) values. The distribution shifts to the right and becomes more spread out.\n",
    "\n",
    "- **\\(\\nu\\)** (degrees of freedom): controls how noisy the variance estimate is.\n",
    "  - small \\(\\nu\\) means a noisy denominator \\(\\sqrt{V/\\nu}\\), producing heavier right tails and larger quantiles.\n",
    "  - as \\(\\nu\\to\\infty\\), the denominator concentrates near 1 and the distribution approaches the normal-range distribution.\n",
    "\n",
    "Below we visualize these effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 8, 500)\n",
    "\n",
    "# Effect of k (fix df)\n",
    "df_fixed = 10\n",
    "ks = [2, 3, 5, 10]\n",
    "\n",
    "fig = go.Figure()\n",
    "for k_ in ks:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=stats.studentized_range.pdf(x, k_, df_fixed),\n",
    "            mode=\"lines\",\n",
    "            name=f\"k={k_}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"studentized_range PDF: varying k (df={df_fixed})\",\n",
    "    xaxis_title=\"q\",\n",
    "    yaxis_title=\"pdf\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Effect of df (fix k)\n",
    "k_fixed = 5\n",
    "dfs = [3, 5, 10, 30]\n",
    "\n",
    "fig = go.Figure()\n",
    "for df_ in dfs:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=stats.studentized_range.pdf(x, k_fixed, df_),\n",
    "            mode=\"lines\",\n",
    "            name=f\"df={df_}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"studentized_range PDF: varying df (k={k_fixed})\",\n",
    "    xaxis_title=\"q\",\n",
    "    yaxis_title=\"pdf\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38f7ed",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "The studentized range is defined as \\(Q = R/S\\) with:\n",
    "- \\(R = \\max_i Z_i - \\min_i Z_i\\) (range of \\(k\\) standard normals)\n",
    "- \\(S = \\sqrt{V/\\nu}\\) with \\(V\\sim\\chi^2_\\nu\\)\n",
    "- \\(R\\perp V\\)\n",
    "\n",
    "This independence is what makes several useful derivations short.\n",
    "\n",
    "### Expectation\n",
    "For \\(\\nu>1\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[Q] = \\mathbb{E}[R]\\,\\mathbb{E}[1/S] = \\mathbb{E}[R]\\,\\mathbb{E}[\\sqrt{\\nu/V}].\n",
    "\\]\n",
    "\n",
    "Using the Gamma-function moment of \\(\\chi^2\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[\\sqrt{\\nu/V}] = \\sqrt{\\frac{\\nu}{2}}\\,\\frac{\\Gamma\\left((\\nu-1)/2\\right)}{\\Gamma(\\nu/2)}.\n",
    "\\]\n",
    "\n",
    "The remaining factor \\(\\mathbb{E}[R]\\) depends on \\(k\\) and usually must be computed numerically.\n",
    "\n",
    "### Variance\n",
    "For \\(\\nu>2\\), using \\(\\mathbb{E}[Q^2] = \\mathbb{E}[R^2]\\,\\mathbb{E}[\\nu/V]\\) and \\(\\mathbb{E}[\\nu/V] = \\nu/(\\nu-2)\\):\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(Q) = \\mathbb{E}[R^2]\\,\\frac{\\nu}{\\nu-2} - \\left(\\mathbb{E}[R]\\,\\sqrt{\\frac{\\nu}{2}}\\,\\frac{\\Gamma\\left((\\nu-1)/2\\right)}{\\Gamma(\\nu/2)}\\right)^2.\n",
    "\\]\n",
    "\n",
    "### Likelihood\n",
    "If you model observations \\(q_1,\\dots,q_n\\) as i.i.d. from \\(\\mathrm{studentized\\_range}(k,\\nu)\\), the likelihood is\n",
    "\n",
    "\\[\n",
    "L(k,\\nu\\mid q_{1:n}) = \\prod_{i=1}^n f_Q(q_i\\,;k,\\nu),\n",
    "\\]\n",
    "\n",
    "and the log-likelihood is \\(\\ell = \\sum_i \\log f_Q(q_i\\,;k,\\nu)\\).\n",
    "\n",
    "In most classical applications \\(k\\) and \\(\\nu\\) are determined by the experimental design (numbers of groups and error degrees of freedom), so MLE is less common. When fitting is needed, numerical optimization relies on evaluating `logpdf` via numerical integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the moment factorization idea numerically\n",
    "\n",
    "def inv_chi2_scaled_moment(df: float, m: float) -> float:\n",
    "    # E[(df / V)^(m/2)] where V ~ chi2(df). Requires df > m.\n",
    "\n",
    "    if not (np.isfinite(df) and df > m):\n",
    "        return math.inf\n",
    "    # (df/2)^(m/2) * Gamma((df-m)/2) / Gamma(df/2)\n",
    "    return (df / 2) ** (m / 2) * math.exp(gammaln((df - m) / 2) - gammaln(df / 2))\n",
    "\n",
    "\n",
    "def normal_range_moments(k: int, n: int = 200_000) -> tuple[float, float]:\n",
    "    # Monte Carlo E[R] and E[R^2] for the range of k standard normals.\n",
    "\n",
    "    z = rng.standard_normal((n, k))\n",
    "    r = z.max(axis=1) - z.min(axis=1)\n",
    "    return float(r.mean()), float((r**2).mean())\n",
    "\n",
    "\n",
    "k, df = 5, 10\n",
    "ER, ER2 = normal_range_moments(k)\n",
    "\n",
    "a1 = inv_chi2_scaled_moment(df, m=1.0)  # E[(df/V)^(1/2)]\n",
    "a2 = inv_chi2_scaled_moment(df, m=2.0)  # E[(df/V)^(1)]\n",
    "\n",
    "mean_pred = ER * a1\n",
    "var_pred = ER2 * a2 - mean_pred**2\n",
    "\n",
    "mean_scipy = float(stats.studentized_range.mean(k, df))\n",
    "var_scipy = float(stats.studentized_range.var(k, df))\n",
    "\n",
    "print(f\"k={k}, df={df}\")\n",
    "print(\"E[R]   (MC) \", ER)\n",
    "print(\"E[R^2] (MC) \", ER2)\n",
    "print(\"\\nPredicted from factorization\")\n",
    "print(\"mean   \", mean_pred)\n",
    "print(\"var    \", var_pred)\n",
    "print(\"\\nSciPy (numerical)\")\n",
    "print(\"mean   \", mean_scipy)\n",
    "print(\"var    \", var_scipy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1f28d",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation (NumPy-only)\n",
    "\n",
    "The defining construction suggests a direct simulator:\n",
    "\n",
    "1. Draw \\(Z_1,\\dots,Z_k\\overset{iid}{\\sim}\\mathcal{N}(0,1)\\) and compute the range \\(R=\\max Z_i - \\min Z_i\\).\n",
    "2. Draw \\(V\\sim\\chi^2_\\nu\\) independently (equivalently, \\(V\\sim\\mathrm{Gamma}(\\nu/2,\\;\\text{scale}=2)\\)).\n",
    "3. Return \\(Q = R / \\sqrt{V/\\nu}\\).\n",
    "\n",
    "This is **vectorizable**: we can draw a `(size, k)` array of normals and a `(size,)` array of chi-square variables, then compute everything in NumPy without Python loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a992a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def studentized_range_rvs_numpy(k: int, df: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    # NumPy-only sampling via the (range of normals) / sqrt(chi2/df) construction.\n",
    "\n",
    "    if int(k) != k or k < 2:\n",
    "        raise ValueError(\"k must be an integer >= 2\")\n",
    "    if not (np.isfinite(df) and df > 0):\n",
    "        raise ValueError(\"df must be finite and > 0\")\n",
    "    if size < 1:\n",
    "        raise ValueError(\"size must be >= 1\")\n",
    "\n",
    "    z = rng.standard_normal((size, k))\n",
    "    r = z.max(axis=1) - z.min(axis=1)\n",
    "\n",
    "    # Chi-square via Gamma(df/2, scale=2)\n",
    "    v = rng.gamma(shape=df / 2, scale=2.0, size=size)\n",
    "    q = r / np.sqrt(v / df)\n",
    "    return q\n",
    "\n",
    "\n",
    "k, df = 5, 10\n",
    "n = 80_000\n",
    "\n",
    "q_numpy = studentized_range_rvs_numpy(k, df, size=n, rng=rng)\n",
    "q_scipy = stats.studentized_range.rvs(k, df, size=n, random_state=rng)\n",
    "\n",
    "qs = [0.5, 0.9, 0.95, 0.99]\n",
    "\n",
    "print(f\"k={k}, df={df}\")\n",
    "print(\"quantiles\\n  p     numpy      scipy      theory\")\n",
    "q_theory = stats.studentized_range.ppf(qs, k, df)\n",
    "for p, a, b, t in zip(qs, np.quantile(q_numpy, qs), np.quantile(q_scipy, qs), q_theory):\n",
    "    print(f\"{p:>4.2f}  {a:>8.4f}  {b:>8.4f}  {t:>8.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb19cfb",
   "metadata": {},
   "source": [
    "## 8) Visualization (PDF, CDF, Monte Carlo)\n",
    "\n",
    "We’ll compare:\n",
    "- SciPy’s numerical `pdf`/`cdf`\n",
    "- Monte Carlo samples from the NumPy-only sampler\n",
    "\n",
    "This is a good sanity check because the PDF/CDF are computed via numerical integration and the distribution has heavy tails when \\(\\nu\\) is small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(x: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    x = np.sort(np.asarray(x))\n",
    "    y = np.arange(1, x.size + 1) / x.size\n",
    "    return x, y\n",
    "\n",
    "\n",
    "k, df = 5, 10\n",
    "n = 80_000\n",
    "\n",
    "samples = studentized_range_rvs_numpy(k, df, size=n, rng=rng)\n",
    "\n",
    "x = np.linspace(0, 8, 500)\n",
    "pdf = stats.studentized_range.pdf(x, k, df)\n",
    "cdf = stats.studentized_range.cdf(x, k, df)\n",
    "\n",
    "# PDF vs histogram\n",
    "fig = px.histogram(\n",
    "    samples,\n",
    "    nbins=80,\n",
    "    histnorm=\"probability density\",\n",
    "    title=f\"studentized_range: Monte Carlo vs PDF (k={k}, df={df})\",\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=pdf, mode=\"lines\", name=\"SciPy pdf\"))\n",
    "fig.update_layout(xaxis_title=\"q\", yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "# CDF vs ECDF\n",
    "xs, ys = ecdf(samples)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=cdf, mode=\"lines\", name=\"SciPy cdf\"))\n",
    "fig.add_trace(go.Scatter(x=xs[::200], y=ys[::200], mode=\"markers\", name=\"ECDF (subsampled)\"))\n",
    "fig.update_layout(\n",
    "    title=f\"studentized_range: ECDF vs CDF (k={k}, df={df})\",\n",
    "    xaxis_title=\"q\",\n",
    "    yaxis_title=\"probability\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9d4a2",
   "metadata": {},
   "source": [
    "## 9) SciPy integration (`scipy.stats.studentized_range`)\n",
    "\n",
    "SciPy exposes the distribution as a *continuous* distribution object:\n",
    "\n",
    "- `stats.studentized_range.pdf(x, k, df)`\n",
    "- `stats.studentized_range.cdf(x, k, df)`\n",
    "- `stats.studentized_range.ppf(p, k, df)` (quantiles / critical values)\n",
    "- `stats.studentized_range.rvs(k, df, size=..., random_state=...)`\n",
    "- `stats.studentized_range.fit(data, ...)` (MLE; can be slow)\n",
    "\n",
    "Like most SciPy continuous distributions, it also supports `loc` and `scale`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, df = 5, 10\n",
    "x = np.linspace(0, 8, 5)\n",
    "\n",
    "print(\"x         \", x)\n",
    "print(\"pdf(x)    \", stats.studentized_range.pdf(x, k, df))\n",
    "print(\"cdf(x)    \", stats.studentized_range.cdf(x, k, df))\n",
    "print(\"sf(x)     \", stats.studentized_range.sf(x, k, df))\n",
    "\n",
    "# Sampling\n",
    "samp = stats.studentized_range.rvs(k, df, size=5, random_state=rng)\n",
    "print(\"\\nsample   \", samp)\n",
    "\n",
    "# Critical value / quantile (common in Tukey-style tests)\n",
    "alpha = 0.05\n",
    "qcrit = stats.studentized_range.ppf(1 - alpha, k, df)\n",
    "print(f\"\\nqcrit (1-alpha={1-alpha:.2f}) = {qcrit:.4f}\")\n",
    "\n",
    "# Fitting (MLE) can be slow because logpdf involves numerical integration.\n",
    "# We'll fit df on a tiny synthetic sample and keep k fixed.\n",
    "k_true, df_true = 5, 12\n",
    "data = stats.studentized_range.rvs(k_true, df_true, size=10, random_state=rng)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    k_hat, df_hat, loc_hat, scale_hat = stats.studentized_range.fit(\n",
    "        data,\n",
    "        f0=k_true,  # fix k\n",
    "        floc=0,\n",
    "        fscale=1,\n",
    "    )\n",
    "\n",
    "print(\"\\nFit (k fixed)\")\n",
    "print(\"k_hat    \", k_hat)\n",
    "print(\"df_hat   \", df_hat)\n",
    "print(\"loc_hat  \", loc_hat)\n",
    "print(\"scale_hat\", scale_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8e748",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### Hypothesis testing (Tukey’s HSD intuition)\n",
    "In a balanced one-way ANOVA with \\(k\\) groups of size \\(n\\), define:\n",
    "\n",
    "- group means \\(\\bar{Y}_1,\\dots,\\bar{Y}_k\\)\n",
    "- pooled error standard deviation \\(s_p\\)\n",
    "- error degrees of freedom \\(\\nu\\)\n",
    "\n",
    "Tukey’s HSD compares pairwise mean differences using a critical value from the studentized range distribution:\n",
    "\n",
    "\\[\n",
    "\\text{HSD} = q_{1-\\alpha}(k,\\nu)\\,\\frac{s_p}{\\sqrt{n}},\n",
    "\\]\n",
    "\n",
    "where \\(q_{1-\\alpha}(k,\\nu)\\) is the \\((1-\\alpha)\\)-quantile of `studentized_range(k, nu)`.\n",
    "\n",
    "A pair \\((i,j)\\) is flagged when\n",
    "\n",
    "\\[\n",
    "|\\bar{Y}_i - \\bar{Y}_j| > \\text{HSD}.\n",
    "\\]\n",
    "\n",
    "### Bayesian modeling\n",
    "In Bayesian ANOVA / hierarchical normal models, practitioners often compute the *posterior* of a studentized range-like statistic by **sampling from the posterior** of group means and \\(\\sigma\\), then computing the range/scale ratio on each draw. The classical studentized range distribution is useful as a **reference null** and for posterior predictive checks.\n",
    "\n",
    "### Generative modeling\n",
    "The studentized range is a distribution over a *summary statistic* (a standardized range). It can be used as:\n",
    "- a synthetic-data generator for stress-testing multiple-comparison pipelines,\n",
    "- a component in simulation-based calibration (e.g., generate \\(Q\\) under the null),\n",
    "- a target distribution in approximate Bayesian computation (ABC) when ranges are part of the chosen summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ed85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini demo: Tukey-style thresholding in a balanced one-way layout\n",
    "\n",
    "alpha = 0.05\n",
    "k = 4\n",
    "n = 12\n",
    "\n",
    "# Simulate k groups under the null (all means equal)\n",
    "y = rng.normal(loc=0.0, scale=1.0, size=(k, n))\n",
    "\n",
    "group_means = y.mean(axis=1)\n",
    "df_error = k * (n - 1)\n",
    "ss_within = ((y - group_means[:, None]) ** 2).sum()\n",
    "sp = math.sqrt(ss_within / df_error)\n",
    "\n",
    "qcrit = stats.studentized_range.ppf(1 - alpha, k, df_error)\n",
    "hsd = qcrit * sp / math.sqrt(n)\n",
    "\n",
    "print(f\"k={k}, n={n}, df_error={df_error}\")\n",
    "print(\"group means:\", group_means)\n",
    "print(f\"sp (pooled sd) = {sp:.4f}\")\n",
    "print(f\"qcrit          = {qcrit:.4f}\")\n",
    "print(f\"HSD threshold  = {hsd:.4f}\\n\")\n",
    "\n",
    "# Pairwise comparisons\n",
    "pairs = []\n",
    "for i in range(k):\n",
    "    for j in range(i + 1, k):\n",
    "        diff = abs(group_means[i] - group_means[j])\n",
    "        q = diff / (sp / math.sqrt(n))\n",
    "        p_adj = stats.studentized_range.sf(q, k, df_error)\n",
    "        reject = diff > hsd\n",
    "        pairs.append((i, j, diff, q, p_adj, reject))\n",
    "\n",
    "print(\"i  j    |diff|      q     p_adj   reject\")\n",
    "for i, j, diff, q, p_adj, reject in pairs:\n",
    "    print(f\"{i}  {j}  {diff:8.4f}  {q:7.3f}  {p_adj:7.4f}   {reject}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9a010",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter validity**: interpret \\(k\\) as an integer \\(\\ge 2\\); require \\(\\nu>0\\). (SciPy allows non-integer `k` because it treats it as a continuous shape parameter, but the classical meaning is “number of groups”.)\n",
    "- **Moment existence**: mean/variance/skew/kurtosis only exist when \\(\\nu\\) exceeds 1/2/3/4 respectively.\n",
    "- **Numerical integration**: `pdf`, `cdf`, and especially `fit` may be slower than for simpler distributions and can emit integration warnings for some parameter/data combinations.\n",
    "- **Tail behavior**: small \\(\\nu\\) gives heavy right tails; Monte Carlo estimates of extreme quantiles need large sample sizes.\n",
    "- **Modeling assumption**: the distribution is rooted in normality + common variance assumptions; violations (heteroskedasticity, non-normal errors, unbalanced designs) can make Tukey-style procedures inaccurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f59ee",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `studentized_range(k, df)` is the distribution of the **range of \\(k\\)** standard normals divided by an independent **estimated scale** \\(\\sqrt{\\chi^2_{df}/df}\\).\n",
    "- It underpins **Tukey’s HSD** and related multiple-comparison procedures.\n",
    "- The PDF/CDF are typically defined and evaluated via **numerical integration**.\n",
    "- Moments exist only up to order \\(m<df\\); the inverse-\\(\\chi^2\\) part yields a clean Gamma-function factor.\n",
    "- Sampling is straightforward from the defining construction and can be implemented in **NumPy only**.\n",
    "\n",
    "**References (starting points)**\n",
    "- SciPy documentation: `scipy.stats.studentized_range`\n",
    "- Standard treatments in multiple comparisons / ANOVA texts (Tukey HSD and the studentized range statistic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
