{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d460f5bf",
   "metadata": {},
   "source": [
    "# Reciprocal Inverse Gaussian Distribution (`recipinvgauss`)\n",
    "\n",
    "The **reciprocal inverse Gaussian** distribution is the distribution of a **positive random variable whose reciprocal is inverse Gaussian**.\n",
    "It naturally appears when you model *times* with an inverse Gaussian and then switch to modeling the corresponding *rates* (reciprocals of times).\n",
    "\n",
    "## What you’ll learn\n",
    "- what `recipinvgauss` models and when it’s appropriate\n",
    "- the PDF/CDF (with LaTeX) and its relationship to `invgauss`\n",
    "- key moments (mean/variance/skew/kurtosis), MGF/CF, and entropy\n",
    "- how the parameter `mu` changes the shape\n",
    "- core derivations: PDF via change-of-variables, moments via the MGF, likelihood + MLE\n",
    "- NumPy-only sampling (via an inverse-Gaussian sampler) + Monte Carlo validation\n",
    "- SciPy usage: `scipy.stats.recipinvgauss` (`pdf`, `cdf`, `rvs`, `fit`)\n",
    "- hypothesis testing, Bayesian modeling patterns, and generative modeling ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86099c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from scipy import special, stats\n",
    "\n",
    "# Plotly rendering (CKC convention)\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb1a52",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `recipinvgauss` (reciprocal inverse Gaussian)\n",
    "- **Type**: **Continuous**\n",
    "- **Support** (standard form): \\(x > 0\\)\n",
    "- **Parameter space** (SciPy):\n",
    "  - shape \\(\\mu > 0\\)\n",
    "  - `loc` \\(\\in \\mathbb{R}\\)\n",
    "  - `scale` \\(> 0\\)\n",
    "\n",
    "We write (standardized form):\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathrm{RIG}(\\mu)\\qquad (\\mu>0),\n",
    "\\]\n",
    "\n",
    "and SciPy’s location–scale version:\n",
    "\n",
    "\\[\n",
    "X = \\mathrm{loc} + \\mathrm{scale}\\,Y,\\quad Y \\sim \\mathrm{RIG}(\\mu).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877dbd1",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### 2.1 What it models\n",
    "A clean way to understand `recipinvgauss` is through a transformation:\n",
    "\n",
    "- Let \\(Y\\) be **inverse Gaussian**.\n",
    "- Define \\(X = 1/Y\\).\n",
    "\n",
    "Then \\(X\\) has the **reciprocal inverse Gaussian** distribution.\n",
    "\n",
    "This matters because many problems naturally flip between a quantity and its reciprocal:\n",
    "\n",
    "- **time** \\(\\leftrightarrow\\) **rate**\n",
    "- **variance** \\(\\leftrightarrow\\) **precision**\n",
    "- **speed** \\(\\leftrightarrow\\) **travel time per unit distance**\n",
    "\n",
    "If inverse Gaussian is a good model for a positive *time* variable, `recipinvgauss` becomes a principled model for the corresponding *rate*.\n",
    "\n",
    "### 2.2 Real-world use cases (examples)\n",
    "- **Reliability / survival / first-passage times**: inverse Gaussian is a classic first-passage-time model; reciprocals correspond to modeling *rates* or *intensities* derived from those times.\n",
    "- **Hierarchical Bayesian models**: if you model a positive scale parameter with an inverse Gaussian, then the corresponding precision parameter follows a reciprocal inverse Gaussian.\n",
    "- **Variance–mean mixtures**: `recipinvgauss` is a special case of the generalized inverse Gaussian family, which is widely used as a mixing distribution to build heavy-tailed marginals.\n",
    "\n",
    "### 2.3 Relations to other distributions\n",
    "- **Inverse Gaussian**: if \\(X \\sim \\mathrm{RIG}(\\mu)\\), then \\(1/X\\) is inverse Gaussian.\n",
    "- **Generalized inverse Gaussian (GIG)**: `recipinvgauss` is a \\(\\mathrm{GIG}(p=1/2,\\,a=1,\\,b=1/\\mu^2)\\) distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a3564",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "SciPy’s standardized `recipinvgauss(mu)` has PDF:\n",
    "\n",
    "\\[\n",
    " f(x\\mid\\mu) = \\frac{1}{\\sqrt{2\\pi x}}\\exp\\left(-\\frac{(1-\\mu x)^2}{2\\mu^2 x}\\right),\\qquad x>0,\\;\\mu>0.\n",
    "\\]\n",
    "\n",
    "A convenient CDF expression uses the standard normal CDF \\(\\Phi\\):\n",
    "\n",
    "\\[\n",
    "F(x\\mid\\mu)\n",
    "= \\Phi\\!\\left(\\frac{\\mu x - 1}{\\mu\\sqrt{x}}\\right)\n",
    "\\; -\\; \\exp\\!\\left(\\frac{2}{\\mu}\\right)\n",
    "\\,\\Phi\\!\\left(-\\frac{\\mu x + 1}{\\mu\\sqrt{x}}\\right),\\qquad x>0.\n",
    "\\]\n",
    "\n",
    "### Relationship to `invgauss`\n",
    "If \\(Y \\sim \\mathrm{InvGauss}(\\mu)\\) in SciPy’s standardized form and \\(X = 1/Y\\), then:\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathrm{RIG}(\\mu),\n",
    "\\qquad\n",
    "F_X(x) = \\mathbb{P}(X\\le x) = \\mathbb{P}(Y\\ge 1/x) = 1 - F_Y(1/x).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipinvgauss_logpdf(x: np.ndarray, mu: float) -> np.ndarray:\n",
    "    '''Log-PDF of the standardized reciprocal inverse Gaussian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        points where to evaluate (array-like)\n",
    "    mu:\n",
    "        shape parameter, mu > 0\n",
    "    '''\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu = float(mu)\n",
    "    if mu <= 0:\n",
    "        raise ValueError('mu must be > 0')\n",
    "\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "\n",
    "    xm = x[mask]\n",
    "    out[mask] = (\n",
    "        -0.5 * np.log(2.0 * np.pi)\n",
    "        - 0.5 * np.log(xm)\n",
    "        - ((1.0 - mu * xm) ** 2) / (2.0 * mu**2 * xm)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def recipinvgauss_pdf(x: np.ndarray, mu: float) -> np.ndarray:\n",
    "    return np.exp(recipinvgauss_logpdf(x, mu))\n",
    "\n",
    "\n",
    "def recipinvgauss_cdf(x: np.ndarray, mu: float) -> np.ndarray:\n",
    "    '''CDF via the closed-form normal-CDF expression.\n",
    "\n",
    "    Uses log-space for the exp(2/mu)*Phi(.) term for stability.\n",
    "    '''\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu = float(mu)\n",
    "    if mu <= 0:\n",
    "        raise ValueError('mu must be > 0')\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    a = (mu * xm - 1.0) / (mu * np.sqrt(xm))\n",
    "    b = -(mu * xm + 1.0) / (mu * np.sqrt(xm))\n",
    "\n",
    "    term1 = special.ndtr(a)\n",
    "    term2 = np.exp(2.0 / mu + special.log_ndtr(b))\n",
    "\n",
    "    out[mask] = term1 - term2\n",
    "    return np.clip(out, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks vs SciPy\n",
    "mu0 = 2.5\n",
    "x_grid = np.logspace(-3, 2, 200)\n",
    "\n",
    "pdf_max_err = np.max(\n",
    "    np.abs(recipinvgauss_pdf(x_grid, mu0) - stats.recipinvgauss(mu0).pdf(x_grid))\n",
    ")\n",
    "\n",
    "cdf_max_err = np.max(\n",
    "    np.abs(recipinvgauss_cdf(x_grid, mu0) - stats.recipinvgauss(mu0).cdf(x_grid))\n",
    ")\n",
    "\n",
    "pdf_max_err, cdf_max_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436082f",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### 4.1 Mean, variance, skewness, kurtosis\n",
    "For \\(X \\sim \\mathrm{RIG}(\\mu)\\):\n",
    "\n",
    "- **Mean**\n",
    "\\[\n",
    "\\mathbb{E}[X] = 1 + \\frac{1}{\\mu}.\n",
    "\\]\n",
    "\n",
    "- **Variance**\n",
    "\\[\n",
    "\\mathrm{Var}(X) = 2 + \\frac{1}{\\mu}.\n",
    "\\]\n",
    "\n",
    "- **Skewness**\n",
    "\\[\n",
    "\\gamma_1 = \\frac{8 + 3/\\mu}{\\left(2 + 1/\\mu\\right)^{3/2}}.\n",
    "\\]\n",
    "\n",
    "- **Excess kurtosis** (kurtosis minus 3)\n",
    "\\[\n",
    "\\gamma_2 = \\frac{3\\left(16 + 5/\\mu\\right)}{\\left(2 + 1/\\mu\\right)^2}.\n",
    "\\]\n",
    "\n",
    "A useful extra property is the **mode** (maximizer of the PDF):\n",
    "\n",
    "\\[\n",
    "\\mathrm{mode}(X) = \\frac{\\sqrt{1+4/\\mu^2}-1}{2}.\n",
    "\\]\n",
    "\n",
    "### 4.2 MGF and characteristic function\n",
    "The MGF exists for \\(t < 1/2\\) and has a simple closed form:\n",
    "\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = (1-2t)^{-1/2}\\,\\exp\\left(\\frac{1-\\sqrt{1-2t}}{\\mu}\\right),\\qquad t < 1/2.\n",
    "\\]\n",
    "\n",
    "The characteristic function follows by substituting \\(t \\mapsto i t\\):\n",
    "\n",
    "\\[\n",
    "\\varphi_X(t) = (1-2 i t)^{-1/2}\\,\\exp\\left(\\frac{1-\\sqrt{1-2 i t}}{\\mu}\\right).\n",
    "\\]\n",
    "\n",
    "### 4.3 Raw moments via Bessel functions (GIG view)\n",
    "Because `recipinvgauss` is a special case of the **GIG** family, its raw moments can be written with the modified Bessel \\(K\\) function:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X^r] = \\mu^{-r}\\,\\frac{K_{r+1/2}(1/\\mu)}{K_{1/2}(1/\\mu)}.\n",
    "\\]\n",
    "\n",
    "For half-integer orders, \\(K_{n+1/2}\\) reduces to \\(e^{-z}\\) times a polynomial in \\(1/z\\), which is why the low-order moments above become simple rational expressions in \\(\\mu\\).\n",
    "\n",
    "### 4.4 Entropy\n",
    "The **differential entropy** is\n",
    "\n",
    "\\[\n",
    "H(X) = -\\int_0^\\infty f(x)\\,\\log f(x)\\,dx.\n",
    "\\]\n",
    "\n",
    "For this distribution SciPy computes entropy by numerical integration (`.entropy()`), and that’s typically the most practical approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipinvgauss_moments(mu: float) -> dict:\n",
    "    mu = float(mu)\n",
    "    if mu <= 0:\n",
    "        raise ValueError('mu must be > 0')\n",
    "\n",
    "    mean = 1.0 + 1.0 / mu\n",
    "    var = 2.0 + 1.0 / mu\n",
    "    skew = (8.0 + 3.0 / mu) / (var ** 1.5)\n",
    "    excess_kurt = 3.0 * (16.0 + 5.0 / mu) / (var**2)\n",
    "    mode = 0.5 * (np.sqrt(1.0 + 4.0 / mu**2) - 1.0)\n",
    "\n",
    "    def mgf(t: float) -> float:\n",
    "        t = float(t)\n",
    "        if t >= 0.5:\n",
    "            raise ValueError('MGF exists only for t < 1/2')\n",
    "        return (1.0 - 2.0 * t) ** (-0.5) * np.exp((1.0 - np.sqrt(1.0 - 2.0 * t)) / mu)\n",
    "\n",
    "    def cf(t: float) -> complex:\n",
    "        t = float(t)\n",
    "        z = 1.0 - 2.0j * t\n",
    "        return z ** (-0.5) * np.exp((1.0 - np.sqrt(z)) / mu)\n",
    "\n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'var': var,\n",
    "        'skew': skew,\n",
    "        'excess_kurt': excess_kurt,\n",
    "        'mode': mode,\n",
    "        'mgf': mgf,\n",
    "        'cf': cf,\n",
    "    }\n",
    "\n",
    "\n",
    "m = recipinvgauss_moments(mu0)\n",
    "scipy_stats = stats.recipinvgauss.stats(mu0, moments='mvsk')\n",
    "\n",
    "m['mean'], scipy_stats[0], m['var'], scipy_stats[1], m['skew'], scipy_stats[2], m['excess_kurt'], scipy_stats[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check (mean/var + MGF at a few t)\n",
    "mu0 = 2.5\n",
    "n = 300_000\n",
    "samples = stats.recipinvgauss(mu0).rvs(size=n, random_state=rng)\n",
    "\n",
    "mc_mean = samples.mean()\n",
    "mc_var = samples.var(ddof=0)\n",
    "\n",
    "for t in [0.1, 0.2, -0.2]:\n",
    "    mc = np.mean(np.exp(t * samples))\n",
    "    th = recipinvgauss_moments(mu0)['mgf'](t)\n",
    "    print(f't={t:+.2f}  MC={mc:.6f}  theory={th:.6f}')\n",
    "\n",
    "mc_mean, recipinvgauss_moments(mu0)['mean'], mc_var, recipinvgauss_moments(mu0)['var']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f57d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy (SciPy computes it numerically)\n",
    "for mu in [0.5, 1.0, 2.5]:\n",
    "    h = stats.recipinvgauss(mu).entropy()\n",
    "    print(f'mu={mu:g}  entropy={h:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b3255",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "A handy interpretation comes from the reciprocal relationship:\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathrm{RIG}(\\mu) \\quad\\Longleftrightarrow\\quad \\frac{1}{X} \\sim \\mathrm{InvGauss}(\\mu).\n",
    "\\]\n",
    "\n",
    "So **\\(\\mu\\) is the mean of the reciprocal**:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}\\left[\\frac{1}{X}\\right] = \\mu.\n",
    "\\]\n",
    "\n",
    "### 5.1 Shape changes\n",
    "- Smaller \\(\\mu\\) pushes mass to **larger** values (heavier right tail), because the reciprocal has smaller mean.\n",
    "- Larger \\(\\mu\\) shifts the distribution toward smaller typical values (mode shrinks like \\(\\approx 1/\\mu^2\\)), while the mean approaches 1:\n",
    "  \\(\\mathbb{E}[X] \\to 1\\) and \\(\\mathrm{Var}(X) \\to 2\\) as \\(\\mu\\to\\infty\\).\n",
    "\n",
    "### 5.2 Location–scale parameters (SciPy)\n",
    "SciPy’s `loc` and `scale` transform the variable:\n",
    "\n",
    "\\[\n",
    "X = \\mathrm{loc} + \\mathrm{scale}\\,Y,\\quad Y \\sim \\mathrm{RIG}(\\mu).\n",
    "\\]\n",
    "\n",
    "- `scale` stretches/shrinks the distribution (mean and std scale accordingly)\n",
    "- `loc` shifts the support to \\(x > \\mathrm{loc}\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb934c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF shape for different mu\n",
    "mus = [0.3, 0.7, 1.5, 4.0]\n",
    "\n",
    "x = np.linspace(1e-4, 12, 700)\n",
    "fig = go.Figure()\n",
    "\n",
    "for mu in mus:\n",
    "    fig.add_trace(go.Scatter(x=x, y=recipinvgauss_pdf(x, mu), mode='lines', name=f'mu={mu:g}'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='recipinvgauss PDF for various μ',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='f(x)',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18950fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and mode as functions of mu\n",
    "mu_grid = np.linspace(0.2, 6.0, 200)\n",
    "means = np.array([recipinvgauss_moments(mu)['mean'] for mu in mu_grid])\n",
    "modes = np.array([recipinvgauss_moments(mu)['mode'] for mu in mu_grid])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_grid, y=means, mode='lines', name='mean'))\n",
    "fig.add_trace(go.Scatter(x=mu_grid, y=modes, mode='lines', name='mode'))\n",
    "fig.update_layout(\n",
    "    title='Mean and mode vs μ',\n",
    "    xaxis_title='μ',\n",
    "    yaxis_title='value',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8059c3",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "From the MGF,\n",
    "\n",
    "\\[\n",
    "M(t) = (1-2t)^{-1/2}\\exp\\left(\\frac{1-\\sqrt{1-2t}}{\\mu}\\right),\\qquad t<1/2,\n",
    "\\]\n",
    "\n",
    "take logs:\n",
    "\n",
    "\\[\n",
    "\\log M(t) = -\\frac{1}{2}\\log(1-2t) + \\frac{1-\\sqrt{1-2t}}{\\mu}.\n",
    "\\]\n",
    "\n",
    "Differentiate:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dt}\\log M(t) = \\frac{1}{1-2t} + \\frac{1}{\\mu\\sqrt{1-2t}}.\n",
    "\\]\n",
    "\n",
    "Evaluating at \\(t=0\\) yields:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = M'(0) = \\left.\\frac{d}{dt}\\log M(t)\\right|_{t=0} = 1 + \\frac{1}{\\mu}.\n",
    "\\]\n",
    "\n",
    "### 6.2 Variance\n",
    "Differentiate again:\n",
    "\n",
    "\\[\n",
    "\\frac{d^2}{dt^2}\\log M(t) = \\frac{2}{(1-2t)^2} + \\frac{1}{\\mu}(1-2t)^{-3/2}.\n",
    "\\]\n",
    "\n",
    "At \\(t=0\\) this is the second cumulant (the variance):\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = 2 + \\frac{1}{\\mu}.\n",
    "\\]\n",
    "\n",
    "### 6.3 Likelihood and MLE (standardized form)\n",
    "For i.i.d. data \\(x_1,\\dots,x_n\\) with \\(x_i>0\\), the log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell(\\mu) = \\sum_{i=1}^n \\log f(x_i\\mid\\mu)\n",
    "= -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_i\\log x_i - \\sum_i\\frac{(1-\\mu x_i)^2}{2\\mu^2 x_i}.\n",
    "\\]\n",
    "\n",
    "Expand the quadratic term:\n",
    "\n",
    "\\[\n",
    "\\frac{(1-\\mu x)^2}{2\\mu^2 x} = \\frac{1}{2\\mu^2 x} - \\frac{1}{\\mu} + \\frac{x}{2}.\n",
    "\\]\n",
    "\n",
    "So (dropping \\(\\mu\\)-free constants) the objective becomes:\n",
    "\n",
    "\\[\n",
    "\\ell(\\mu) = \\frac{n}{\\mu} - \\frac{1}{2\\mu^2}\\sum_{i=1}^n\\frac{1}{x_i} + \\text{const}.\n",
    "\\]\n",
    "\n",
    "Setting \\(\\ell'(\\mu)=0\\) yields a closed-form MLE:\n",
    "\n",
    "\\[\n",
    "\\hat{\\mu}_{\\mathrm{MLE}} = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{x_i}.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipinvgauss_loglikelihood(x: np.ndarray, mu: float) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu = float(mu)\n",
    "    if mu <= 0 or np.any(x <= 0):\n",
    "        return -np.inf\n",
    "    return float(np.sum(recipinvgauss_logpdf(x, mu)))\n",
    "\n",
    "\n",
    "def recipinvgauss_mle_mu(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if np.any(x <= 0):\n",
    "        raise ValueError('all observations must be > 0')\n",
    "    return float(np.mean(1.0 / x))\n",
    "\n",
    "\n",
    "# Demonstrate MLE on synthetic data\n",
    "mu_true = 1.7\n",
    "x = stats.recipinvgauss(mu_true).rvs(size=5000, random_state=rng)\n",
    "mu_hat = recipinvgauss_mle_mu(x)\n",
    "\n",
    "mu_true, mu_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0c388",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "A practical sampler uses the relationship:\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathrm{RIG}(\\mu) \\iff \\frac{1}{X} \\sim \\mathrm{InvGauss}(\\mu).\n",
    "\\]\n",
    "\n",
    "So the plan is:\n",
    "\n",
    "1. sample \\(Y \\sim \\mathrm{InvGauss}(\\mu)\\) using a NumPy-only algorithm\n",
    "2. return \\(X = 1/Y\\)\n",
    "\n",
    "### 7.1 Inverse Gaussian sampler (Michael–Schucany–Haas)\n",
    "A widely used method for \\(Y\\sim\\mathrm{IG}(\\mu,\\lambda)\\) is:\n",
    "\n",
    "1) draw \\(V \\sim \\mathcal{N}(0,1)\\) and set \\(W = V^2\\)\n",
    "\n",
    "2) compute\n",
    "\n",
    "\\[\n",
    "Y^\\* = \\mu + \\frac{\\mu^2 W}{2\\lambda} - \\frac{\\mu}{2\\lambda}\\sqrt{4\\mu\\lambda W + \\mu^2 W^2}\n",
    "\\]\n",
    "\n",
    "3) draw \\(U\\sim\\mathrm{Uniform}(0,1)\\) and set\n",
    "\n",
    "\\[\n",
    "Y = \\begin{cases}\n",
    "Y^\\*, & U \\le \\frac{\\mu}{\\mu+Y^\\*}\\\\\n",
    "\\frac{\\mu^2}{Y^\\*}, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "SciPy’s standardized `invgauss(mu)` corresponds to \\(\\lambda=1\\) in this notation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invgauss_rvs_numpy(mu: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''NumPy-only sampler for SciPy's standardized invgauss(mu).\n",
    "\n",
    "    Uses the Michael–Schucany–Haas method with lambda=1.\n",
    "    '''\n",
    "\n",
    "    mu = float(mu)\n",
    "    if mu <= 0:\n",
    "        raise ValueError('mu must be > 0')\n",
    "    if size <= 0:\n",
    "        raise ValueError('size must be >= 1')\n",
    "\n",
    "    v = rng.normal(size=size)\n",
    "    w = v * v\n",
    "\n",
    "    # lambda = 1\n",
    "    y_star = mu + 0.5 * mu**2 * w - 0.5 * mu * np.sqrt(4.0 * mu * w + mu**2 * w**2)\n",
    "\n",
    "    u = rng.random(size)\n",
    "    y = np.where(u <= mu / (mu + y_star), y_star, mu**2 / y_star)\n",
    "    return y\n",
    "\n",
    "\n",
    "def recipinvgauss_rvs_numpy(mu: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    '''NumPy-only sampler for recipinvgauss(mu) via inversion of invgauss samples.'''\n",
    "\n",
    "    y = invgauss_rvs_numpy(mu=mu, size=size, rng=rng)\n",
    "    return 1.0 / y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate sampler vs SciPy (quick KS test + moment check)\n",
    "mu0 = 1.3\n",
    "n = 50_000\n",
    "\n",
    "samples_numpy = recipinvgauss_rvs_numpy(mu0, n, rng)\n",
    "samples_scipy = stats.recipinvgauss(mu0).rvs(size=n, random_state=rng)\n",
    "\n",
    "ks = stats.ks_2samp(samples_numpy, samples_scipy)\n",
    "\n",
    "mc_mean = samples_numpy.mean()\n",
    "mc_var = samples_numpy.var(ddof=0)\n",
    "\n",
    "ks, (mc_mean, recipinvgauss_moments(mu0)['mean'], mc_var, recipinvgauss_moments(mu0)['var'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698fa73",
   "metadata": {},
   "source": [
    "## 8) Visualization (PDF, CDF, Monte Carlo)\n",
    "\n",
    "We’ll visualize:\n",
    "- the PDF for multiple \\(\\mu\\)\n",
    "- the CDF for multiple \\(\\mu\\)\n",
    "- Monte Carlo samples vs the theoretical PDF and CDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF and CDF curves\n",
    "mus = [0.5, 1.0, 2.5]\n",
    "x = np.linspace(1e-4, 12, 700)\n",
    "\n",
    "fig_pdf = go.Figure()\n",
    "fig_cdf = go.Figure()\n",
    "\n",
    "for mu in mus:\n",
    "    fig_pdf.add_trace(go.Scatter(x=x, y=recipinvgauss_pdf(x, mu), mode='lines', name=f'μ={mu:g}'))\n",
    "    fig_cdf.add_trace(go.Scatter(x=x, y=recipinvgauss_cdf(x, mu), mode='lines', name=f'μ={mu:g}'))\n",
    "\n",
    "fig_pdf.update_layout(title='recipinvgauss PDF', xaxis_title='x', yaxis_title='f(x)')\n",
    "fig_cdf.update_layout(title='recipinvgauss CDF', xaxis_title='x', yaxis_title='F(x)')\n",
    "\n",
    "fig_pdf.show()\n",
    "fig_cdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7620e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo histogram vs PDF\n",
    "mu0 = 1.3\n",
    "n = 80_000\n",
    "samples = recipinvgauss_rvs_numpy(mu0, n, rng)\n",
    "\n",
    "x_grid = np.linspace(1e-4, np.quantile(samples, 0.995), 500)\n",
    "\n",
    "fig = px.histogram(\n",
    "    samples,\n",
    "    nbins=70,\n",
    "    histnorm='probability density',\n",
    "    title=f'Monte Carlo samples vs PDF (n={n}, μ={mu0:g})',\n",
    "    labels={'value': 'x'},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=recipinvgauss_pdf(x_grid, mu0), mode='lines', name='true pdf'))\n",
    "fig.update_layout(yaxis_title='density')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical CDF vs theoretical CDF\n",
    "mu0 = 1.3\n",
    "n = 30_000\n",
    "samples = recipinvgauss_rvs_numpy(mu0, n, rng)\n",
    "\n",
    "xs = np.sort(samples)\n",
    "ys = np.arange(1, n + 1) / n\n",
    "\n",
    "x_grid = np.linspace(1e-4, np.quantile(xs, 0.995), 500)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=ys, mode='lines', name='empirical CDF'))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=recipinvgauss_cdf(x_grid, mu0), mode='lines', name='true CDF'))\n",
    "fig.update_layout(\n",
    "    title=f'Empirical CDF vs true CDF (n={n}, μ={mu0:g})',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='CDF',\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ff213",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.recipinvgauss`)\n",
    "\n",
    "SciPy exposes the distribution as `scipy.stats.recipinvgauss` with signature:\n",
    "\n",
    "- `recipinvgauss(mu, loc=0, scale=1)`\n",
    "\n",
    "Recall the location–scale transform:\n",
    "\n",
    "\\[\n",
    "\\texttt{pdf}(x;\\mu,\\mathrm{loc},\\mathrm{scale})\n",
    "= \\frac{1}{\\mathrm{scale}}\\,f\\!\\left(\\frac{x-\\mathrm{loc}}{\\mathrm{scale}}\\middle|\\mu\\right).\n",
    "\\]\n",
    "\n",
    "So:\n",
    "- changing `scale` multiplies the mean by `scale` and the variance by `scale**2`\n",
    "- changing `loc` shifts the support to \\(x > \\mathrm{loc}\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f38953",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = 1.7\n",
    "rv = stats.recipinvgauss(mu0)\n",
    "\n",
    "x = np.linspace(1e-4, 6, 400)\n",
    "\n",
    "pdf_vals = rv.pdf(x)\n",
    "cdf_vals = rv.cdf(x)\n",
    "samples = rv.rvs(size=5, random_state=rng)\n",
    "\n",
    "pdf_vals[:3], cdf_vals[:3], samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting: compare SciPy's MLE (with fixed loc/scale) to the closed-form MLE\n",
    "mu_true = 2.2\n",
    "x = stats.recipinvgauss(mu_true).rvs(size=4000, random_state=rng)\n",
    "\n",
    "mu_hat_closed = recipinvgauss_mle_mu(x)\n",
    "mu_hat_fit, loc_hat, scale_hat = stats.recipinvgauss.fit(x, floc=0, fscale=1)\n",
    "\n",
    "(mu_true, mu_hat_closed, mu_hat_fit, loc_hat, scale_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26db0e",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing (example)\n",
    "If you want to test a specific value \\(\\mu=\\mu_0\\), one option is a **likelihood ratio (LR) test**:\n",
    "\n",
    "\\[\n",
    "\\Lambda = 2\\left(\\ell(\\hat\\mu) - \\ell(\\mu_0)\\right).\n",
    "\\]\n",
    "\n",
    "Under regularity conditions, \\(\\Lambda\\) is approximately \\(\\chi^2_1\\) under the null.\n",
    "\n",
    "### 10.2 Bayesian modeling (pattern)\n",
    "Because the log-likelihood simplifies to\n",
    "\n",
    "\\[\n",
    "\\ell(\\mu) = \\frac{n}{\\mu} - \\frac{1}{2\\mu^2}\\sum_i\\frac{1}{x_i} + \\text{const},\n",
    "\\]\n",
    "\n",
    "it is often convenient to reparameterize with \\(\\theta = 1/\\mu\\):\n",
    "\n",
    "\\[\n",
    "\\ell(\\theta) = n\\theta - \\frac{S}{2}\\theta^2 + \\text{const},\\qquad S=\\sum_i\\frac{1}{x_i}.\n",
    "\\]\n",
    "\n",
    "As a function of \\(\\theta\\), this is (up to constants) the log-density of a **Gaussian** (restricted to \\(\\theta>0\\)).\n",
    "So a (truncated) normal prior on \\(\\theta\\) yields a tractable posterior.\n",
    "\n",
    "### 10.3 Generative modeling (idea)\n",
    "`recipinvgauss` is a positive distribution that can serve as a **latent scale/precision**.\n",
    "For example, sampling a latent \\(\\tau>0\\) and then generating\n",
    "\n",
    "\\[\n",
    "Y\\mid\\tau \\sim \\mathcal{N}(0,\\tau)\n",
    "\\]\n",
    "\n",
    "creates heavier-tailed marginals than a fixed-variance Gaussian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Likelihood ratio test example\n",
    "mu0 = 1.5\n",
    "n = 800\n",
    "x = stats.recipinvgauss(mu0).rvs(size=n, random_state=rng)\n",
    "\n",
    "mu_hat = recipinvgauss_mle_mu(x)\n",
    "ll_hat = recipinvgauss_loglikelihood(x, mu_hat)\n",
    "ll_null = recipinvgauss_loglikelihood(x, mu0)\n",
    "\n",
    "lr_stat = 2.0 * (ll_hat - ll_null)\n",
    "# asymptotic p-value\n",
    "p_value = stats.chi2(df=1).sf(lr_stat)\n",
    "\n",
    "(mu0, mu_hat, lr_stat, p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian example on theta = 1/mu with a (truncated) Normal prior\n",
    "# Prior: theta ~ Normal(m0, s0^2), restricted to theta>0.\n",
    "# Likelihood in theta is proportional to exp(n*theta - (S/2)*theta^2).\n",
    "\n",
    "x = stats.recipinvgauss(1.7).rvs(size=400, random_state=rng)\n",
    "\n",
    "n = x.size\n",
    "S = np.sum(1.0 / x)\n",
    "\n",
    "# Likelihood corresponds to theta ~ Normal(mean=n/S, var=1/S) up to constants.\n",
    "like_mean = n / S\n",
    "like_var = 1.0 / S\n",
    "\n",
    "m0, s0 = 0.6, 0.5  # prior mean/std on theta\n",
    "prior_prec = 1.0 / (s0**2)\n",
    "like_prec = 1.0 / like_var\n",
    "\n",
    "post_prec = prior_prec + like_prec\n",
    "post_mean = (prior_prec * m0 + like_prec * like_mean) / post_prec\n",
    "post_std = np.sqrt(1.0 / post_prec)\n",
    "\n",
    "# Sample theta from the (untruncated) Normal posterior and keep positive draws\n",
    "n_draws = 50_000\n",
    "theta_draws = rng.normal(loc=post_mean, scale=post_std, size=n_draws)\n",
    "theta_draws = theta_draws[theta_draws > 0]\n",
    "mu_draws = 1.0 / theta_draws\n",
    "\n",
    "(mu_draws.mean(), np.quantile(mu_draws, [0.05, 0.5, 0.95]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23037e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Generative modeling: a simple scale-mixture example\n",
    "# Compare Y ~ Normal(0, 1) vs Y = sqrt(tau) * Z with tau ~ recipinvgauss(mu)\n",
    "\n",
    "mu_tau = 2.0\n",
    "n = 200_000\n",
    "\n",
    "z = rng.normal(size=n)\n",
    "\n",
    "# fixed-variance baseline\n",
    "y_gauss = z\n",
    "\n",
    "# scale-mixture\n",
    "tau = stats.recipinvgauss(mu_tau).rvs(size=n, random_state=rng)\n",
    "y_mix = np.sqrt(tau) * z\n",
    "\n",
    "# compare tail quantiles\n",
    "qs = [0.9, 0.95, 0.99, 0.995]\n",
    "summary = {\n",
    "    'q': qs,\n",
    "    '|Y| Gaussian': [np.quantile(np.abs(y_gauss), q) for q in qs],\n",
    "    '|Y| mixture': [np.quantile(np.abs(y_mix), q) for q in qs],\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cff9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_gauss,\n",
    "        nbinsx=200,\n",
    "        histnorm='probability density',\n",
    "        name='Normal(0,1)',\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_mix,\n",
    "        nbinsx=200,\n",
    "        histnorm='probability density',\n",
    "        name=f'sqrt(tau)*Z, tau~RIG(mu={mu_tau:g})',\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Scale mixture increases tail mass',\n",
    "    barmode='overlay',\n",
    "    xaxis_title='y',\n",
    "    yaxis_title='density',\n",
    ")\n",
    "fig.update_xaxes(range=[-6, 6])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79651039",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: `mu` must be \\(>0\\); `scale` must be \\(>0\\). Data must lie in the support.\n",
    "- **Near-zero behavior**: the PDF contains \\(\\log x\\) and a \\(1/x\\) term in the exponent; evaluate via `logpdf` when possible.\n",
    "- **CDF stability**: the closed form uses differences of terms involving \\(\\exp(2/\\mu)\\); for extreme parameters use SciPy’s `cdf`, `sf`, `logcdf`, `logsf` for better numerical behavior.\n",
    "- **`loc`/`scale` ambiguity**: if you fit all parameters freely, location/scale can absorb structure; fix `loc`/`scale` when you know the data are standardized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46247f40",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `recipinvgauss` is a **continuous** distribution on \\(x>0\\) whose reciprocal is inverse Gaussian.\n",
    "- PDF: \\(f(x\\mid\\mu)=\\frac{1}{\\sqrt{2\\pi x}}\\exp\\left(-\\frac{(1-\\mu x)^2}{2\\mu^2 x}\\right)\\).\n",
    "- Mean/variance: \\(\\mathbb{E}[X]=1+1/\\mu\\), \\(\\mathrm{Var}(X)=2+1/\\mu\\).\n",
    "- MGF exists for \\(t<1/2\\): \\((1-2t)^{-1/2}\\exp\\left(\\frac{1-\\sqrt{1-2t}}{\\mu}\\right)\\).\n",
    "- Standardized MLE: \\(\\hat\\mu = \\frac{1}{n}\\sum_i 1/x_i\\).\n",
    "- Sampling is easy via NumPy-only inverse Gaussian sampling + reciprocal transform.\n",
    "- In practice, use SciPy’s `scipy.stats.recipinvgauss` for robust `cdf`/`sf`/`fit` and numerical entropy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
