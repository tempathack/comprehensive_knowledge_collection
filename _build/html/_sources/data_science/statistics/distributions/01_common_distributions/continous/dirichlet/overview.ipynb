{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dirichlet Distribution (`dirichlet`) \u2014 Modeling Random Probability Vectors\n",
        "\n",
        "The **Dirichlet distribution** is the canonical distribution over **probability vectors**:\n",
        "random vectors \\(X = (X_1,\\dots,X_K)\\) with \\(X_i \\ge 0\\) and \\(\\sum_i X_i = 1\\).\n",
        "\n",
        "It appears everywhere you want to express uncertainty about categorical probabilities:\n",
        "Bayesian smoothing of histograms, mixture-model weights, topic proportions, and more.\n",
        "\n",
        "**Goals**\n",
        "- Understand what the Dirichlet models (the probability simplex).\n",
        "- Work with the PDF, moments, and key properties.\n",
        "- Interpret parameters and see how the shape changes.\n",
        "- Sample from a Dirichlet using a **NumPy-only** algorithm.\n",
        "- Use `scipy.stats.dirichlet` for evaluation/simulation and fit parameters via MLE.\n",
        "\n",
        "**Prerequisites**\n",
        "- Gamma/Beta functions and basic multivariate calculus.\n",
        "- Familiarity with Bayesian updating for categorical/multinomial data.\n",
        "- NumPy + basic plotting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook roadmap\n",
        "\n",
        "1. Title & classification\n",
        "2. Intuition & motivation\n",
        "3. Formal definition (PDF + CDF notes)\n",
        "4. Moments & properties\n",
        "5. Parameter interpretation + shape changes\n",
        "6. Derivations (expectation, variance, likelihood)\n",
        "7. Sampling & simulation (NumPy-only)\n",
        "8. Visualization (PDF, CDF, Monte Carlo)\n",
        "9. SciPy integration\n",
        "10. Statistical use cases\n",
        "11. Pitfalls\n",
        "12. Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.tri as mtri\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.special import gammaln, psi\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "\n",
        "# Reproducibility\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# Quick/slow toggle (mirrors patterns used elsewhere in this repo)\n",
        "FAST_RUN = True\n",
        "N_SAMPLES = 30_000 if FAST_RUN else 300_000\n",
        "GRID_N = 35 if FAST_RUN else 90\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (9, 4.5),\n",
        "    \"axes.grid\": True,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_alpha(alpha: np.ndarray) -> np.ndarray:\n",
        "    '''Validate Dirichlet parameters (all strictly positive).'''\n",
        "    alpha = np.asarray(alpha, dtype=float)\n",
        "    if alpha.ndim != 1:\n",
        "        raise ValueError(f\"alpha must be 1D, got shape={alpha.shape}\")\n",
        "    if alpha.size < 2:\n",
        "        raise ValueError(\"Dirichlet requires K>=2 parameters\")\n",
        "    if np.any(alpha <= 0):\n",
        "        raise ValueError(\"All alpha_i must be > 0\")\n",
        "    return alpha\n",
        "\n",
        "\n",
        "def validate_simplex(X: np.ndarray, *, atol: float = 1e-10, allow_zeros: bool = True) -> np.ndarray:\n",
        "    '''Validate points on the probability simplex (rows sum to 1).'''\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    if X.ndim == 1:\n",
        "        X = X[None, :]\n",
        "    if X.ndim != 2:\n",
        "        raise ValueError(f\"X must be 1D or 2D, got shape={X.shape}\")\n",
        "    if np.any(X < 0):\n",
        "        raise ValueError(\"Simplex components must be >= 0\")\n",
        "    if not allow_zeros and np.any(X <= 0):\n",
        "        raise ValueError(\"All simplex components must be > 0 for log-likelihood computations\")\n",
        "    row_sums = X.sum(axis=1)\n",
        "    if not np.allclose(row_sums, 1.0, atol=atol):\n",
        "        raise ValueError(\"Each row of X must sum to 1\")\n",
        "    return X\n",
        "\n",
        "\n",
        "def dirichlet_logpdf_numpy(X: np.ndarray, alpha: np.ndarray) -> np.ndarray:\n",
        "    '''Dirichlet log-PDF implemented with NumPy + SciPy special functions.\n",
        "\n",
        "    Notes:\n",
        "    - Supports X as shape (K,) or (N, K).\n",
        "    - Works on the boundary too (x_i=0), returning +/-inf where appropriate.\n",
        "    '''\n",
        "    alpha = validate_alpha(alpha)\n",
        "    X = validate_simplex(X, allow_zeros=True)\n",
        "\n",
        "    log_norm = gammaln(alpha.sum()) - gammaln(alpha).sum()\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        log_x = np.log(X)\n",
        "    return log_norm + ((alpha - 1.0) * log_x).sum(axis=1)\n",
        "\n",
        "\n",
        "def sample_dirichlet_numpy(alpha: np.ndarray, *, size: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    '''NumPy-only sampler via Gamma normalization.'''\n",
        "    alpha = validate_alpha(alpha)\n",
        "    y = rng.gamma(shape=alpha, scale=1.0, size=(size, alpha.size))\n",
        "    return y / y.sum(axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def dirichlet_mean(alpha: np.ndarray) -> np.ndarray:\n",
        "    alpha = validate_alpha(alpha)\n",
        "    return alpha / alpha.sum()\n",
        "\n",
        "\n",
        "def dirichlet_cov(alpha: np.ndarray) -> np.ndarray:\n",
        "    alpha = validate_alpha(alpha)\n",
        "    a0 = alpha.sum()\n",
        "    cov = -np.outer(alpha, alpha) / (a0**2 * (a0 + 1.0))\n",
        "    np.fill_diagonal(cov, alpha * (a0 - alpha) / (a0**2 * (a0 + 1.0)))\n",
        "    return cov\n",
        "\n",
        "\n",
        "def beta_skewness(a: float, b: float) -> float:\n",
        "    # Standardized third central moment of Beta(a, b)\n",
        "    return 2 * (b - a) * np.sqrt(a + b + 1) / ((a + b + 2) * np.sqrt(a * b))\n",
        "\n",
        "\n",
        "def beta_excess_kurtosis(a: float, b: float) -> float:\n",
        "    # Excess kurtosis (kurtosis minus 3) of Beta(a, b)\n",
        "    num = 6 * ((a - b) ** 2 * (a + b + 1) - a * b * (a + b + 2))\n",
        "    den = a * b * (a + b + 2) * (a + b + 3)\n",
        "    return num / den\n",
        "\n",
        "\n",
        "def dirichlet_entropy(alpha: np.ndarray) -> float:\n",
        "    '''Differential entropy H(X) in nats.'''\n",
        "    alpha = validate_alpha(alpha)\n",
        "    a0 = alpha.sum()\n",
        "    k = alpha.size\n",
        "    log_B = gammaln(alpha).sum() - gammaln(a0)\n",
        "    return log_B + (a0 - k) * psi(a0) - ((alpha - 1.0) * psi(alpha)).sum()\n",
        "\n",
        "\n",
        "SQRT3 = float(np.sqrt(3))\n",
        "\n",
        "\n",
        "def simplex3_grid(n: int, *, min_component: float = 0.0) -> np.ndarray:\n",
        "    '''Grid of points on the 2-simplex for K=3.\n",
        "\n",
        "    Returns an array of shape (M, 3) with entries in [0,1] summing to 1.\n",
        "    '''\n",
        "    pts = []\n",
        "    for i in range(n + 1):\n",
        "        for j in range(n + 1 - i):\n",
        "            k = n - i - j\n",
        "            x = np.array([i, j, k], dtype=float) / n\n",
        "            if x.min() < min_component:\n",
        "                continue\n",
        "            pts.append(x)\n",
        "    return np.vstack(pts)\n",
        "\n",
        "\n",
        "def simplex3_to_xy(X: np.ndarray) -> np.ndarray:\n",
        "    '''Map (x1,x2,x3) on the simplex to 2D coordinates inside an equilateral triangle.'''\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    if X.ndim == 1:\n",
        "        X = X[None, :]\n",
        "    # vertices: e1=(0,0), e2=(1,0), e3=(1/2, sqrt(3)/2)\n",
        "    x = X[:, 1] + 0.5 * X[:, 2]\n",
        "    y = (SQRT3 / 2.0) * X[:, 2]\n",
        "    return np.column_stack([x, y])\n",
        "\n",
        "\n",
        "def plot_simplex3_outline(ax: plt.Axes) -> None:\n",
        "    tri = np.array([[0.0, 0.0], [1.0, 0.0], [0.5, SQRT3 / 2.0], [0.0, 0.0]])\n",
        "    ax.plot(tri[:, 0], tri[:, 1], color=\"black\", lw=1.2)\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "\n",
        "\n",
        "def plot_dirichlet_simplex3(alpha: np.ndarray, *, ax: plt.Axes, grid_n: int, min_component: float) -> plt.cm.ScalarMappable:\n",
        "    pts = simplex3_grid(grid_n, min_component=min_component)\n",
        "    xy = simplex3_to_xy(pts)\n",
        "    tri = mtri.Triangulation(xy[:, 0], xy[:, 1])\n",
        "    logpdf = dirichlet_logpdf_numpy(pts, alpha)\n",
        "    contour = ax.tricontourf(tri, logpdf, levels=35, cmap=\"viridis\")\n",
        "    plot_simplex3_outline(ax)\n",
        "    return contour\n",
        "\n",
        "\n",
        "def dirichlet_fit_mle(X: np.ndarray, *, alpha_init: np.ndarray | None = None) -> tuple[np.ndarray, object]:\n",
        "    '''Fit alpha by maximum likelihood.\n",
        "\n",
        "    SciPy's `scipy.stats.dirichlet` does not provide `.fit()` (as of SciPy 1.15),\n",
        "    so we optimize the log-likelihood ourselves.\n",
        "\n",
        "    We optimize over theta = log(alpha) to enforce alpha_i > 0.\n",
        "    '''\n",
        "    X = validate_simplex(X, allow_zeros=False)\n",
        "    n, k = X.shape\n",
        "    sum_log_x = np.log(X).sum(axis=0)\n",
        "\n",
        "    if alpha_init is None:\n",
        "        # Method-of-moments-inspired initialization\n",
        "        mean = X.mean(axis=0)\n",
        "        var = X.var(axis=0, ddof=1)\n",
        "        eps = 1e-8\n",
        "        a0_est = np.mean(mean * (1.0 - mean) / np.maximum(var, eps) - 1.0)\n",
        "        a0_est = float(np.clip(a0_est, 1e-2, 1e6))\n",
        "        alpha_init = np.clip(mean * a0_est, 1e-2, None)\n",
        "\n",
        "    def nll(theta: np.ndarray) -> float:\n",
        "        alpha = np.exp(theta)\n",
        "        a0 = alpha.sum()\n",
        "        ll = n * (gammaln(a0) - gammaln(alpha).sum()) + ((alpha - 1.0) * sum_log_x).sum()\n",
        "        return -float(ll)\n",
        "\n",
        "    def grad(theta: np.ndarray) -> np.ndarray:\n",
        "        alpha = np.exp(theta)\n",
        "        a0 = alpha.sum()\n",
        "        grad_alpha = -(n * (psi(a0) - psi(alpha)) + sum_log_x)\n",
        "        return grad_alpha * alpha\n",
        "\n",
        "    res = minimize(nll, x0=np.log(alpha_init), jac=grad, method=\"L-BFGS-B\")\n",
        "    alpha_hat = np.exp(res.x)\n",
        "    return alpha_hat, res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Title & Classification\n",
        "\n",
        "- **Name**: `dirichlet`\n",
        "- **Type**: **continuous** (a \\((K-1)\\)-dimensional distribution embedded in \\(\\mathbb{R}^K\\))\n",
        "- **Support** (the probability simplex):\n",
        "\n",
        "\\[\n",
        "\\Delta^{K-1} = \\left\\{x \\in \\mathbb{R}^K : x_i \\ge 0, \\; \\sum_{i=1}^K x_i = 1\\right\\}\n",
        "\\]\n",
        "\n",
        "- **Parameter space**:\n",
        "\n",
        "\\[\n",
        "\\alpha = (\\alpha_1,\\dots,\\alpha_K), \\qquad \\alpha_i > 0\n",
        "\\]\n",
        "\n",
        "Useful shorthand:\n",
        "\n",
        "\\[\n",
        "\\alpha_0 = \\sum_{i=1}^K \\alpha_i \\quad \\text{(total concentration)}\n",
        "\\]\n",
        "\n",
        "Interpretation preview:\n",
        "- The mean is \\(\\mathbb{E}[X_i] = \\alpha_i / \\alpha_0\\).\n",
        "- \\(\\alpha_0\\) controls how *concentrated* draws are around the mean.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Intuition & Motivation\n",
        "\n",
        "### What it models\n",
        "A Dirichlet random vector is a **random categorical probability vector**.\n",
        "For \\(K\\) categories, a draw \\(X\\) can be used as parameters of a categorical/multinomial distribution:\n",
        "\n",
        "\\[\n",
        "X \\sim \\text{Dirichlet}(\\alpha)\\quad \\Rightarrow \\quad Y \\mid X \\sim \\text{Multinomial}(n, X)\n",
        "\\]\n",
        "\n",
        "### Typical real-world use cases\n",
        "- **Bayesian smoothing** of empirical category frequencies (avoids zero probabilities).\n",
        "- **Mixture models**: prior over mixture weights (e.g., Gaussian mixture weights).\n",
        "- **Topic models** (LDA): prior over per-document topic proportions.\n",
        "- **Compositional data** (parts of a whole): proportions of time, budget, species, etc.\n",
        "\n",
        "### Relations to other distributions\n",
        "- **Beta distribution**: \\(K=2\\) gives \\(X_1 \\sim \\text{Beta}(\\alpha_1, \\alpha_2)\\) and \\(X_2 = 1 - X_1\\).\n",
        "- **Gamma normalization**: if \\(Y_i \\sim \\text{Gamma}(\\alpha_i, 1)\\) i.i.d., then\n",
        "  \\(X_i = Y_i / \\sum_j Y_j\\) is Dirichlet.\n",
        "- **Conjugacy**: Dirichlet is conjugate to categorical/multinomial likelihoods.\n",
        "- **Dirichlet\u2013multinomial**: integrating out \\(X\\) yields an overdispersed count model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Formal Definition\n",
        "\n",
        "### PDF\n",
        "For \\(x \\in \\Delta^{K-1}\\) and \\(\\alpha_i > 0\\):\n",
        "\n",
        "\\[\n",
        " f(x;\\alpha) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n",
        "\\]\n",
        "\n",
        "where the multivariate Beta function \\(B(\\alpha)\\) is\n",
        "\n",
        "\\[\n",
        "B(\\alpha) = \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)}{\\Gamma(\\alpha_0)},\n",
        "\\qquad \\alpha_0 = \\sum_{i=1}^K \\alpha_i.\n",
        "\\]\n",
        "\n",
        "A numerically stable log-form is\n",
        "\n",
        "\\[\n",
        "\\log f(x;\\alpha) = \\log \\Gamma(\\alpha_0) - \\sum_i \\log \\Gamma(\\alpha_i) + \\sum_i (\\alpha_i - 1)\\log x_i.\n",
        "\\]\n",
        "\n",
        "### CDF\n",
        "A multivariate CDF can be defined in \\((K-1)\\) free coordinates, e.g. \\(x_K = 1 - \\sum_{i=1}^{K-1} x_i\\):\n",
        "\n",
        "\\[\n",
        "F(x_1,\\dots,x_{K-1}) = \\mathbb{P}(X_1 \\le x_1, \\dots, X_{K-1} \\le x_{K-1}).\n",
        "\\]\n",
        "\n",
        "For \\(K>2\\), there is **no simple closed form in general**.\n",
        "A key special case is \\(K=2\\), where Dirichlet reduces to **Beta**, and the CDF is the regularized incomplete beta function.\n",
        "\n",
        "In practice, common workarounds are:\n",
        "- use **marginal CDFs** (each \\(X_i\\) marginal is Beta), or\n",
        "- estimate multivariate probabilities by **Monte Carlo**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick consistency check: our logpdf vs SciPy\n",
        "alpha = np.array([2.5, 1.2, 3.0])\n",
        "x = np.array([0.2, 0.5, 0.3])\n",
        "\n",
        "our = dirichlet_logpdf_numpy(x, alpha)[0]\n",
        "scipy = stats.dirichlet(alpha).logpdf(x)\n",
        "\n",
        "our, scipy, float(our - scipy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Moments & Properties\n",
        "\n",
        "Let \\(X \\sim \\text{Dirichlet}(\\alpha)\\) with \\(\\alpha_0 = \\sum_i \\alpha_i\\).\n",
        "\n",
        "### Mean\n",
        "\\[\n",
        "\\mathbb{E}[X_i] = \\frac{\\alpha_i}{\\alpha_0}.\n",
        "\\]\n",
        "\n",
        "### Variance and covariance\n",
        "\\[\n",
        "\\mathrm{Var}(X_i) = \\frac{\\alpha_i(\\alpha_0 - \\alpha_i)}{\\alpha_0^2(\\alpha_0+1)}\n",
        "= \\frac{m_i(1-m_i)}{\\alpha_0+1},\\quad m_i=\\alpha_i/\\alpha_0\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\mathrm{Cov}(X_i, X_j) = -\\frac{\\alpha_i\\alpha_j}{\\alpha_0^2(\\alpha_0+1)}\\quad (i\\ne j).\n",
        "\\]\n",
        "\n",
        "### Marginals\n",
        "Each component has a Beta marginal:\n",
        "\n",
        "\\[\n",
        "X_i \\sim \\mathrm{Beta}(\\alpha_i,\\; \\alpha_0 - \\alpha_i).\n",
        "\\]\n",
        "\n",
        "This is very useful: you can get **univariate PDFs/CDFs** and skewness/kurtosis per component.\n",
        "\n",
        "### Skewness and kurtosis (component-wise)\n",
        "For \\(X_i\\), with \\(a=\\alpha_i\\) and \\(b=\\alpha_0-\\alpha_i\\):\n",
        "\n",
        "\\[\n",
        "\\mathrm{skew}(X_i) = \\frac{2(b-a)\\sqrt{a+b+1}}{(a+b+2)\\sqrt{ab}}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\mathrm{excess\\ kurt}(X_i) = \\frac{6\\big((a-b)^2(a+b+1)-ab(a+b+2)\\big)}{ab(a+b+2)(a+b+3)}.\n",
        "\\]\n",
        "\n",
        "### Other useful properties\n",
        "- **Mode** (if all \\(\\alpha_i>1\\)):\n",
        "  \\(\\;\\mathrm{mode}_i = \\frac{\\alpha_i-1}{\\alpha_0-K}\\).\n",
        "- **Conjugacy** (multinomial counts \\(c_i\\)):\n",
        "  \\(\\alpha \\mapsto \\alpha + c\\).\n",
        "- **Additivity**: merging categories keeps Dirichlet form; e.g. \\((X_1+X_2, X_3,\\dots)\\) is Dirichlet with \\((\\alpha_1+\\alpha_2, \\alpha_3,\\dots)\\).\n",
        "\n",
        "### MGF / characteristic function\n",
        "They exist (support is bounded), but are not usually expressed in elementary functions for general \\(K\\).\n",
        "They can be written using multivariate hypergeometric functions (e.g. Lauricella functions).\n",
        "\n",
        "### Entropy\n",
        "Differential entropy (nats):\n",
        "\n",
        "\\[\n",
        "H(X) = \\log B(\\alpha) + (\\alpha_0-K)\\,\\psi(\\alpha_0) - \\sum_i (\\alpha_i-1)\\,\\psi(\\alpha_i)\n",
        "\\]\n",
        "\n",
        "where \\(\\psi\\) is the digamma function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = np.array([2.0, 3.0, 5.0])\n",
        "rv = stats.dirichlet(alpha)\n",
        "\n",
        "samples = sample_dirichlet_numpy(alpha, size=N_SAMPLES, rng=rng)\n",
        "\n",
        "mean_theory = dirichlet_mean(alpha)\n",
        "mean_mc = samples.mean(axis=0)\n",
        "\n",
        "cov_theory = dirichlet_cov(alpha)\n",
        "cov_mc = np.cov(samples, rowvar=False)\n",
        "\n",
        "# Component-wise skewness/kurtosis via Beta marginals\n",
        "alpha0 = alpha.sum()\n",
        "skew_theory = np.array([beta_skewness(a, alpha0 - a) for a in alpha])\n",
        "exkurt_theory = np.array([beta_excess_kurtosis(a, alpha0 - a) for a in alpha])\n",
        "\n",
        "skew_mc = stats.skew(samples, axis=0, bias=False)\n",
        "exkurt_mc = stats.kurtosis(samples, axis=0, fisher=True, bias=False)\n",
        "\n",
        "print('Mean (theory):', mean_theory)\n",
        "print('Mean (MC):    ', mean_mc)\n",
        "\n",
        "print()\n",
        "print('Cov (theory):')\n",
        "print(cov_theory)\n",
        "\n",
        "print()\n",
        "print('Cov (MC):')\n",
        "print(cov_mc)\n",
        "\n",
        "print()\n",
        "print('Skewness (theory, per component):', skew_theory)\n",
        "print('Skewness (MC):                   ', skew_mc)\n",
        "\n",
        "print()\n",
        "print('Excess kurtosis (theory, per component):', exkurt_theory)\n",
        "print('Excess kurtosis (MC):                   ', exkurt_mc)\n",
        "\n",
        "print()\n",
        "print('Entropy (theory):', dirichlet_entropy(alpha))\n",
        "print('Entropy (SciPy): ', rv.entropy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Parameter Interpretation\n",
        "\n",
        "A very useful reparameterization is:\n",
        "\n",
        "\\[\n",
        "\\alpha = \\alpha_0\\, m, \\qquad m \\in \\Delta^{K-1}, \\quad \\alpha_0 > 0.\n",
        "\\]\n",
        "\n",
        "- \\(m\\) is the **mean direction** (since \\(\\mathbb{E}[X]=m\\)).\n",
        "- \\(\\alpha_0\\) is the **concentration** (larger means less variance).\n",
        "\n",
        "Heuristics (for symmetric \\(\\alpha_1=\\cdots=\\alpha_K=a\\)):\n",
        "- \\(a=1\\): uniform over the simplex.\n",
        "- \\(a>1\\): mass near the center (balanced proportions).\n",
        "- \\(0<a<1\\): mass near corners/edges (sparse proportions).\n",
        "\n",
        "For asymmetric \\(\\alpha\\), the mean shifts toward larger \\(\\alpha_i\\) components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shape changes on the 2-simplex (K=3)\n",
        "\n",
        "alphas = [\n",
        "    np.array([1.0, 1.0, 1.0]),     # uniform\n",
        "    np.array([5.0, 5.0, 5.0]),     # concentrated around center\n",
        "    np.array([0.35, 0.35, 0.35]),  # corners/edges (divergent at boundaries)\n",
        "    np.array([2.0, 6.0, 1.2]),     # asymmetric\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\n",
        "axes = axes.ravel()\n",
        "\n",
        "mappables = []\n",
        "for ax, a in zip(axes, alphas):\n",
        "    min_comp = 1.0 / GRID_N\n",
        "    m = plot_dirichlet_simplex3(a, ax=ax, grid_n=GRID_N, min_component=min_comp)\n",
        "    ax.set_title(f\"alpha={a}\")\n",
        "    mappables.append(m)\n",
        "\n",
        "# One shared colorbar (log-density scale)\n",
        "cb = fig.colorbar(mappables[0], ax=axes, shrink=0.85)\n",
        "cb.set_label(\"log PDF\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Derivations\n",
        "\n",
        "### Expectation (sketch)\n",
        "Start from the definition:\n",
        "\n",
        "\\[\n",
        "\\mathbb{E}[X_i] = \\int_{\\Delta^{K-1}} x_i\\, f(x;\\alpha)\\, dx.\n",
        "\\]\n",
        "\n",
        "Using the normalization constant, notice that multiplying by \\(x_i\\) increases the exponent of \\(x_i\\) by 1:\n",
        "\n",
        "\\[\n",
        "\\mathbb{E}[X_i] = \\frac{B(\\alpha + e_i)}{B(\\alpha)}\n",
        "\\]\n",
        "\n",
        "where \\(e_i\\) is the unit vector.\n",
        "Since\n",
        "\n",
        "\\[\n",
        "\\frac{B(\\alpha + e_i)}{B(\\alpha)} = \\frac{\\alpha_i}{\\alpha_0},\n",
        "\\]\n",
        "\n",
        "we get \\(\\mathbb{E}[X_i]=\\alpha_i/\\alpha_0\\).\n",
        "\n",
        "### Variance / covariance (sketch)\n",
        "Similarly, for second moments:\n",
        "\n",
        "\\[\n",
        "\\mathbb{E}[X_i^2] = \\frac{B(\\alpha + 2e_i)}{B(\\alpha)} = \\frac{\\alpha_i(\\alpha_i+1)}{\\alpha_0(\\alpha_0+1)}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\mathbb{E}[X_iX_j] = \\frac{B(\\alpha + e_i + e_j)}{B(\\alpha)} = \\frac{\\alpha_i\\alpha_j}{\\alpha_0(\\alpha_0+1)}\\quad (i\\ne j)\n",
        "\\]\n",
        "\n",
        "Combine these with \\(\\mathrm{Var}(X_i)=\\mathbb{E}[X_i^2]-\\mathbb{E}[X_i]^2\\) to get the usual formulas.\n",
        "\n",
        "### Likelihood (iid observations)\n",
        "Given samples \\(x^{(1)},\\dots,x^{(N)}\\) on the simplex:\n",
        "\n",
        "\\[\n",
        "\\ell(\\alpha) = \\sum_{n=1}^N \\log f(x^{(n)};\\alpha)\n",
        "= N\\Big(\\log\\Gamma(\\alpha_0) - \\sum_i \\log\\Gamma(\\alpha_i)\\Big) + \\sum_i (\\alpha_i-1)\\sum_{n=1}^N \\log x_i^{(n)}.\n",
        "\\]\n",
        "\n",
        "The gradient is\n",
        "\n",
        "\\[\n",
        "\\frac{\\partial \\ell}{\\partial \\alpha_i} = N\\big(\\psi(\\alpha_0) - \\psi(\\alpha_i)\\big) + \\sum_{n=1}^N \\log x_i^{(n)}.\n",
        "\\]\n",
        "\n",
        "This is the basis for MLE algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLE demo on synthetic data\n",
        "alpha_true = np.array([1.8, 3.2, 5.0])\n",
        "X = sample_dirichlet_numpy(alpha_true, size=8_000 if FAST_RUN else 40_000, rng=rng)\n",
        "\n",
        "alpha_hat, opt = dirichlet_fit_mle(X)\n",
        "\n",
        "print('alpha_true:', alpha_true)\n",
        "print('alpha_hat: ', np.round(alpha_hat, 4))\n",
        "print('optimizer success:', opt.success)\n",
        "print('final nll:', opt.fun)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Sampling & Simulation (NumPy-only)\n",
        "\n",
        "A standard and efficient sampling method uses the **Gamma normalization** representation.\n",
        "\n",
        "**Algorithm** for \\(X \\sim \\mathrm{Dirichlet}(\\alpha)\\):\n",
        "1. For each component \\(i\\), draw \\(Y_i \\sim \\mathrm{Gamma}(\\alpha_i, 1)\\) independently.\n",
        "2. Normalize: \\(X_i = \\frac{Y_i}{\\sum_j Y_j}\\).\n",
        "\n",
        "Why it works (high-level idea):\n",
        "- The joint density of independent Gammas factorizes nicely.\n",
        "- The change of variables from \\((Y_1,\\dots,Y_K)\\) to \\((X_1,\\dots,X_{K-1}, S)\\) with \\(S=\\sum_i Y_i\\)\n",
        "  yields a Jacobian of \\(S^{K-1}\\).\n",
        "- After integrating out \\(S\\), the remaining density over \\(X\\) matches the Dirichlet PDF.\n",
        "\n",
        "This approach is the practical default in most libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = np.array([0.7, 1.5, 2.2])\n",
        "X = sample_dirichlet_numpy(alpha, size=5, rng=rng)\n",
        "\n",
        "print('samples:')\n",
        "print(X)\n",
        "print()\n",
        "print('row sums:', X.sum(axis=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Visualization\n",
        "\n",
        "Because the Dirichlet is multivariate, the cleanest visualization is for \\(K=3\\):\n",
        "a density over a **triangle** (the 2-simplex).\n",
        "\n",
        "We also visualize a **univariate CDF** via a marginal \\(X_i\\), since each component is Beta-distributed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = np.array([2.0, 3.0, 5.0])\n",
        "rv = stats.dirichlet(alpha)\n",
        "\n",
        "# 1) PDF over the 2-simplex (K=3)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4), constrained_layout=True)\n",
        "\n",
        "m = plot_dirichlet_simplex3(alpha, ax=axes[0], grid_n=GRID_N, min_component=1.0 / GRID_N)\n",
        "axes[0].set_title('Dirichlet log PDF on simplex (K=3)')\n",
        "cb = fig.colorbar(m, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "cb.set_label('log PDF')\n",
        "\n",
        "# 2) Monte Carlo samples\n",
        "samps = rv.rvs(size=4_000 if FAST_RUN else 20_000, random_state=rng)\n",
        "xy = simplex3_to_xy(samps)\n",
        "axes[1].scatter(xy[:, 0], xy[:, 1], s=6, alpha=0.35)\n",
        "plot_simplex3_outline(axes[1])\n",
        "axes[1].set_title('Monte Carlo samples')\n",
        "\n",
        "# 3) Marginal PDF and CDF (component 1)\n",
        "a = alpha[0]\n",
        "b = alpha.sum() - alpha[0]\n",
        "xs = np.linspace(0, 1, 400)\n",
        "\n",
        "axes[2].plot(xs, stats.beta(a, b).pdf(xs), label='marginal PDF (Beta)')\n",
        "axes[2].set_xlabel('x')\n",
        "axes[2].set_ylabel('pdf')\n",
        "\n",
        "ax2 = axes[2].twinx()\n",
        "ax2.plot(xs, stats.beta(a, b).cdf(xs), color='tab:orange', label='marginal CDF (Beta)')\n",
        "ax2.set_ylabel('cdf')\n",
        "\n",
        "lines1, labels1 = axes[2].get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "axes[2].legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
        "axes[2].set_title('Univariate marginal (X1)')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) SciPy Integration (`scipy.stats.dirichlet`)\n",
        "\n",
        "SciPy provides a frozen Dirichlet distribution via:\n",
        "\n",
        "```python\n",
        "rv = scipy.stats.dirichlet(alpha)\n",
        "```\n",
        "\n",
        "Available methods (SciPy 1.15):\n",
        "- `pdf`, `logpdf`\n",
        "- `rvs`\n",
        "- `mean`, `var`, `entropy`\n",
        "\n",
        "**Notably missing**:\n",
        "- `cdf` (multivariate CDF is nontrivial)\n",
        "- `fit` (no built-in MLE)\n",
        "\n",
        "Workarounds:\n",
        "- For CDF-like quantities, use **marginal Beta CDFs** or Monte Carlo.\n",
        "- For fitting, optimize the log-likelihood (we implemented `dirichlet_fit_mle`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = np.array([1.4, 2.1, 3.7])\n",
        "rv = stats.dirichlet(alpha)\n",
        "\n",
        "x = np.array([0.25, 0.5, 0.25])\n",
        "\n",
        "print('pdf:', rv.pdf(x))\n",
        "print('logpdf:', rv.logpdf(x))\n",
        "print('mean:', rv.mean())\n",
        "print('var:', rv.var())\n",
        "print('entropy:', rv.entropy())\n",
        "\n",
        "# Fit alpha on synthetic samples\n",
        "X = rv.rvs(size=6_000 if FAST_RUN else 30_000, random_state=rng)\n",
        "alpha_hat, opt = dirichlet_fit_mle(X)\n",
        "print()\n",
        "print('alpha true:', alpha)\n",
        "print('alpha hat :', np.round(alpha_hat, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Statistical Use Cases\n",
        "\n",
        "### A) Hypothesis testing (Bayesian style)\n",
        "With a Dirichlet posterior over probabilities \\(p\\), you can ask questions like:\n",
        "\n",
        "\\[\n",
        "\\mathbb{P}(p_1 > p_2 \\mid \\text{data})\n",
        "\\]\n",
        "\n",
        "and decide using a threshold (e.g. > 0.95).\n",
        "\n",
        "### B) Bayesian modeling (conjugate updating)\n",
        "For multinomial counts \\(c\\) and prior \\(\\alpha\\):\n",
        "\n",
        "\\[\n",
        "\\alpha_{\\text{post}} = \\alpha + c.\n",
        "\\]\n",
        "\n",
        "This gives closed-form posterior mean/variance and easy simulation.\n",
        "\n",
        "### C) Generative modeling\n",
        "Dirichlet priors are commonly used for:\n",
        "- **mixture weights** (e.g. Dirichlet prior on component proportions),\n",
        "- **topic proportions** (LDA),\n",
        "- any model where a latent probability simplex vector is required.\n",
        "\n",
        "A simple generative story:\n",
        "1. Sample probabilities \\(p \\sim \\text{Dirichlet}(\\alpha)\\)\n",
        "2. Sample data \\(y \\mid p \\sim \\text{Multinomial}(n, p)\\)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Bayesian updating + a posterior probability \"test\"\n",
        "\n",
        "prior_alpha = np.array([1.0, 1.0, 1.0])  # uniform prior on simplex\n",
        "counts = np.array([12, 5, 3])\n",
        "posterior_alpha = prior_alpha + counts\n",
        "\n",
        "rv_post = stats.dirichlet(posterior_alpha)\n",
        "\n",
        "print('prior alpha:    ', prior_alpha)\n",
        "print('observed counts:', counts)\n",
        "print('posterior alpha:', posterior_alpha)\n",
        "\n",
        "print()\n",
        "print('posterior mean:', np.round(rv_post.mean(), 4))\n",
        "print('posterior var :', np.round(rv_post.var(), 6))\n",
        "\n",
        "# Posterior probability that category 1 is more likely than category 2\n",
        "post_samples = rv_post.rvs(size=50_000 if FAST_RUN else 250_000, random_state=rng)\n",
        "p_gt = (post_samples[:, 0] > post_samples[:, 1]).mean()\n",
        "print()\n",
        "print('P(p1 > p2 | data) \u2248', round(float(p_gt), 4))\n",
        "\n",
        "# Posterior predictive simulation for 20 future trials\n",
        "n_future = 20\n",
        "p_draw = rv_post.rvs(size=1, random_state=rng)[0]\n",
        "future_counts = rng.multinomial(n_future, p_draw)\n",
        "print()\n",
        "print('p_draw (one posterior draw):', np.round(p_draw, 4))\n",
        "print('one predictive sample (n=20):', future_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Pitfalls\n",
        "\n",
        "- **Invalid parameters**: all \\(\\alpha_i\\) must be strictly positive.\n",
        "- **Zeros in data**: real datasets often contain exact zeros in proportions; Dirichlet assigns probability 0 to exact zeros (continuous support), and the MLE log-likelihood uses \\(\\log x_i\\).\n",
        "  Common fixes include adding small pseudocounts / smoothing, or using models designed for zero inflation.\n",
        "- **Numerical issues near boundaries**:\n",
        "  - when \\(\\alpha_i < 1\\), the density diverges as \\(x_i \\to 0\\);\n",
        "  - evaluate in **log space** (`logpdf`) and avoid plotting exactly at \\(x_i=0\\) for such parameters.\n",
        "- **Interpreting scale**: scaling all \\(\\alpha\\) by a constant keeps the mean the same but changes concentration.\n",
        "- **Fitting can be tricky**: MLE is well-defined but may require good initialization and care with constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Summary\n",
        "\n",
        "- Dirichlet is a **continuous distribution on the probability simplex**, modeling random categorical probabilities.\n",
        "- Parameters \\(\u0007lpha\\) encode both a **mean direction** \\(\u0007lpha/\u0007lpha_0\\) and a **concentration** \\(\u0007lpha_0\\).\n",
        "- Each component has a **Beta marginal**, which provides easy univariate PDFs/CDFs and skewness/kurtosis.\n",
        "- Sampling is simple and efficient via **Gamma normalization**.\n",
        "- `scipy.stats.dirichlet` supports `pdf/logpdf/rvs/mean/var/entropy`; multivariate `cdf` and `fit` are not provided, but MLE fitting is straightforward via likelihood optimization.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
