{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F distribution (`f`)\n",
    "\n",
    "The **F (Fisher–Snedecor)** distribution in SciPy (`scipy.stats.f`) is a *continuous* distribution on $(0,\\infty)$ that arises as a **ratio of two independent scaled chi-square variables**.\n",
    "\n",
    "It is the workhorse behind **ANOVA**, **overall regression significance tests**, and classic tests comparing **variances**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Understand the chi-square ratio generative story and why the distribution is heavy-tailed.\n",
    "- Write down the PDF/CDF (and the Beta-function connection).\n",
    "- Derive the mean/variance and understand when moments **do not exist**.\n",
    "- Sample from $\\mathrm{F}(\\mathrm{dfn},\\mathrm{dfd})$ using a **NumPy-only** algorithm.\n",
    "- Use `scipy.stats.f` for evaluation, simulation, and fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "from scipy.special import beta as beta_func\n",
    "from scipy.special import betainc, betaln, digamma\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Record versions for reproducibility (useful when numerical details matter).\n",
    "VERSIONS = {\"numpy\": np.__version__, \"scipy\": scipy.__version__, \"plotly\": plotly.__version__}\n",
    "VERSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `f` (F distribution; Fisher–Snedecor; SciPy: `scipy.stats.f`)\n",
    "- **Type**: Continuous\n",
    "- **Support (standard form)**: $x \\in (0,\\infty)$\n",
    "- **Parameter space (standard form)**: numerator df $\\mathrm{dfn} > 0$, denominator df $\\mathrm{dfd} > 0$\n",
    "- **SciPy location/scale**: `loc \\in \\mathbb{R}`, `scale > 0` with\n",
    "  $$X = \\text{loc} + \\text{scale}\\,Y, \\qquad Y \\sim \\mathrm{F}(\\mathrm{dfn},\\mathrm{dfd}).$$\n",
    "\n",
    "Unless stated otherwise, this notebook works with the **standard form** (`loc=0`, `scale=1`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "The F distribution models a **ratio of (noise) energy levels**.\n",
    "\n",
    "A standard generative story is:\n",
    "\n",
    "1. Draw $U \\sim \\chi^2_{\\mathrm{dfn}}$ and $V \\sim \\chi^2_{\\mathrm{dfd}}$ independently.\n",
    "2. Normalize each by its degrees of freedom.\n",
    "3. Take their ratio:\n",
    "\n",
    "$$\n",
    "X = \\frac{U/\\mathrm{dfn}}{V/\\mathrm{dfd}} \\;\\sim\\; \\mathrm{F}(\\mathrm{dfn},\\mathrm{dfd}).\n",
    "$$\n",
    "\n",
    "If $U/\\mathrm{dfn}$ and $V/\\mathrm{dfd}$ are thought of as **variance-like** quantities (they are averages of squared standard normals), then $X$ is a variance ratio.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "- **ANOVA**: compares explained variance to unexplained variance.\n",
    "- **Regression F-test**: tests whether a set of predictors improves fit beyond a baseline model.\n",
    "- **Comparing variances**: under Gaussian assumptions, $s_1^2/s_2^2$ follows an F law under the null $\\sigma_1^2=\\sigma_2^2$.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **t distribution**: if $T \\sim t_{\\nu}$ then $T^2 \\sim \\mathrm{F}(1,\\nu)$.\n",
    "- **Reciprocal symmetry**: if $X \\sim \\mathrm{F}(d_1,d_2)$ then $1/X \\sim \\mathrm{F}(d_2,d_1)$.\n",
    "- **Beta prime / Beta connection**: if $B \\sim \\mathrm{Beta}(a,b)$ with $a=d_1/2$, $b=d_2/2$, then\n",
    "  $$X = \\frac{d_2}{d_1}\\,\\frac{B}{1-B} \\sim \\mathrm{F}(d_1,d_2).$$\n",
    "  This identity explains why the CDF involves the **regularized incomplete beta function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $d_1=\\mathrm{dfn}>0$ and $d_2=\\mathrm{dfd}>0$.\n",
    "\n",
    "### PDF\n",
    "\n",
    "For $x>0$ the F distribution has density\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(x; d_1,d_2)\n",
    " &= \\frac{\\left(\\frac{d_1}{d_2}\\right)^{d_1/2}}{\\mathrm{B}(d_1/2,\\,d_2/2)}\\;\\frac{x^{\\,d_1/2-1}}{\\left(1+\\frac{d_1}{d_2}x\\right)^{(d_1+d_2)/2}}, \\qquad x>0, \\\\\n",
    " f(x; d_1,d_2) &= 0, \\qquad x\\le 0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{B}(\\cdot,\\cdot)$ is the Beta function.\n",
    "\n",
    "### CDF\n",
    "\n",
    "Define the transformation\n",
    "\n",
    "$$\n",
    " u(x) = \\frac{d_1 x}{d_1 x + d_2} \\in (0,1).\n",
    "$$\n",
    "\n",
    "Then the CDF is\n",
    "\n",
    "$$\n",
    "F(x; d_1,d_2) = \\mathrm{I}_{u(x)}\\!\\left(\\frac{d_1}{2},\\frac{d_2}{2}\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathrm{I}_z(a,b)$ is the **regularized incomplete beta** function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_logpdf(x: np.ndarray, dfn: float, dfd: float) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of the standard F(dfn, dfd) distribution (loc=0, scale=1).\"\"\"\n",
    "\n",
    "    dfn = float(dfn)\n",
    "    dfd = float(dfd)\n",
    "    if dfn <= 0 or dfd <= 0:\n",
    "        raise ValueError(\"dfn and dfd must be > 0\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    a = 0.5 * dfn\n",
    "    b = 0.5 * dfd\n",
    "    ratio = dfn / dfd\n",
    "\n",
    "    # log f(x) = a log(dfn/dfd) - log B(a,b) + (a-1) log x - (a+b) log(1 + (dfn/dfd) x)\n",
    "    out[mask] = (\n",
    "        a * np.log(ratio)\n",
    "        - betaln(a, b)\n",
    "        + (a - 1.0) * np.log(xm)\n",
    "        - (a + b) * np.log1p(ratio * xm)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def f_pdf(x: np.ndarray, dfn: float, dfd: float) -> np.ndarray:\n",
    "    \"\"\"PDF of the standard F(dfn, dfd) distribution (loc=0, scale=1).\"\"\"\n",
    "    return np.exp(f_logpdf(x, dfn, dfd))\n",
    "\n",
    "\n",
    "def f_cdf(x: np.ndarray, dfn: float, dfd: float) -> np.ndarray:\n",
    "    \"\"\"CDF of the standard F(dfn, dfd) distribution (loc=0, scale=1).\"\"\"\n",
    "\n",
    "    dfn = float(dfn)\n",
    "    dfd = float(dfd)\n",
    "    if dfn <= 0 or dfd <= 0:\n",
    "        raise ValueError(\"dfn and dfd must be > 0\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "\n",
    "    a = 0.5 * dfn\n",
    "    b = 0.5 * dfd\n",
    "\n",
    "    # Stable transform u = dfn*x/(dfn*x+dfd) = 1 / (1 + dfd/(dfn*x))\n",
    "    u = 1.0 / (1.0 + (dfd / (dfn * xm)))\n",
    "    out[mask] = betainc(a, b, u)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: our formulas match SciPy.\n",
    "dfn, dfd = 5.0, 10.0\n",
    "x = np.logspace(-3, 2, 25)\n",
    "\n",
    "dist = stats.f(dfn, dfd)\n",
    "\n",
    "assert np.allclose(f_pdf(x, dfn, dfd), dist.pdf(x))\n",
    "assert np.allclose(f_cdf(x, dfn, dfd), dist.cdf(x))\n",
    "assert np.allclose(f_logpdf(x, dfn, dfd), dist.logpdf(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "A key fact is that the F distribution has **polynomial tails**, so not all moments exist.\n",
    "\n",
    "Let $a=d_1/2$ and $b=d_2/2$.\n",
    "\n",
    "### Raw moments\n",
    "\n",
    "For $k < b$ (equivalently $d_2 > 2k$), the $k$-th raw moment exists and is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k] = \\left(\\frac{d_2}{d_1}\\right)^k\\,\\frac{\\mathrm{B}(a+k,\\,b-k)}{\\mathrm{B}(a,b)}.\n",
    "$$\n",
    "\n",
    "This immediately implies:\n",
    "\n",
    "- mean exists iff $d_2>2$\n",
    "- variance exists iff $d_2>4$\n",
    "- skewness exists iff $d_2>6$\n",
    "- (excess) kurtosis exists iff $d_2>8$\n",
    "\n",
    "### Mean / variance / skewness / kurtosis\n",
    "\n",
    "When the corresponding moments exist:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\frac{d_2}{d_2-2}, \\qquad (d_2>2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X) = \\frac{2 d_2^2(d_1+d_2-2)}{d_1(d_2-2)^2(d_2-4)}, \\qquad (d_2>4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma_1 = \\frac{(2d_1 + d_2 - 2)\\sqrt{8(d_2-4)}}{(d_2-6)\\sqrt{d_1(d_1+d_2-2)}}, \\qquad (d_2>6)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma_2 = \\frac{12\\left[d_1(5d_2-22)(d_1+d_2-2) + (d_2-4)(d_2-2)^2\\right]}{d_1(d_2-6)(d_2-8)(d_1+d_2-2)}, \\qquad (d_2>8)\n",
    "$$\n",
    "\n",
    "where $\\gamma_2$ is the **excess kurtosis**.\n",
    "\n",
    "### Mode\n",
    "\n",
    "If $d_1>2$, the mode is\n",
    "\n",
    "$$\n",
    "\\operatorname{mode}(X) = \\frac{d_2(d_1-2)}{d_1(d_2+2)}.\n",
    "$$\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "- The **moment generating function** $M_X(t)=\\mathbb{E}[e^{tX}]$ does **not** exist for any $t>0$ (the tail is too heavy).\n",
    "- The **Laplace transform** $\\mathbb{E}[e^{tX}]$ exists for $t<0$.\n",
    "- The **characteristic function** $\\varphi_X(t)=\\mathbb{E}[e^{itX}]$ exists for all real $t$ because $|e^{itX}|\\le 1$.\n",
    "\n",
    "A generic integral representation is\n",
    "\n",
    "$$\n",
    "\\varphi_X(t) = \\int_0^{\\infty} e^{itx}\\,f(x;d_1,d_2)\\,dx.\n",
    "$$\n",
    "\n",
    "Closed forms involve special functions (hypergeometric functions) and are rarely used directly in practice.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy has a closed form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " h(X)\n",
    " &= -\\int_0^{\\infty} f(x;d_1,d_2)\\,\\log f(x;d_1,d_2)\\,dx \\\\\n",
    " &= \\log \\mathrm{B}(a,b)\n",
    "    - (a-1)\\psi(a)\n",
    "    - (b+1)\\psi(b)\n",
    "    + (a+b)\\psi(a+b)\n",
    "    + \\log\\left(\\frac{d_2}{d_1}\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\psi$ is the digamma function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_moment_raw(dfn: float, dfd: float, k: float) -> float:\n",
    "    \"\"\"Raw moment E[X^k] for X ~ F(dfn, dfd), when it exists (dfd > 2k).\"\"\"\n",
    "\n",
    "    dfn = float(dfn)\n",
    "    dfd = float(dfd)\n",
    "    k = float(k)\n",
    "    if dfn <= 0 or dfd <= 0:\n",
    "        raise ValueError(\"dfn and dfd must be > 0\")\n",
    "    if dfd <= 2 * k:\n",
    "        return np.nan\n",
    "\n",
    "    a = 0.5 * dfn\n",
    "    b = 0.5 * dfd\n",
    "    return (dfd / dfn) ** k * np.exp(betaln(a + k, b - k) - betaln(a, b))\n",
    "\n",
    "\n",
    "def f_mean(dfn: float, dfd: float) -> float:\n",
    "    return np.nan if dfd <= 2 else dfd / (dfd - 2)\n",
    "\n",
    "\n",
    "def f_var(dfn: float, dfd: float) -> float:\n",
    "    if dfd <= 4:\n",
    "        return np.nan\n",
    "    return (2 * dfd**2 * (dfn + dfd - 2)) / (dfn * (dfd - 2) ** 2 * (dfd - 4))\n",
    "\n",
    "\n",
    "def f_skew(dfn: float, dfd: float) -> float:\n",
    "    if dfd <= 6:\n",
    "        return np.nan\n",
    "    return ((2 * dfn + dfd - 2) * np.sqrt(8 * (dfd - 4))) / ((dfd - 6) * np.sqrt(dfn * (dfn + dfd - 2)))\n",
    "\n",
    "\n",
    "def f_kurt_excess(dfn: float, dfd: float) -> float:\n",
    "    if dfd <= 8:\n",
    "        return np.nan\n",
    "\n",
    "    num = 12 * (dfn * (5 * dfd - 22) * (dfn + dfd - 2) + (dfd - 4) * (dfd - 2) ** 2)\n",
    "    den = dfn * (dfd - 6) * (dfd - 8) * (dfn + dfd - 2)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def f_entropy(dfn: float, dfd: float) -> float:\n",
    "    a = 0.5 * float(dfn)\n",
    "    b = 0.5 * float(dfd)\n",
    "    return (\n",
    "        np.log(beta_func(a, b))\n",
    "        - (a - 1) * digamma(a)\n",
    "        - (b + 1) * digamma(b)\n",
    "        + (a + b) * digamma(a + b)\n",
    "        + np.log(dfd / dfn)\n",
    "    )\n",
    "\n",
    "\n",
    "dfn, dfd = 7.0, 25.0\n",
    "scipy_m, scipy_v, scipy_s, scipy_k = stats.f(dfn, dfd).stats(moments=\"mvsk\")\n",
    "\n",
    "out = {\n",
    "    \"mean\": (f_mean(dfn, dfd), scipy_m),\n",
    "    \"var\": (f_var(dfn, dfd), scipy_v),\n",
    "    \"skew\": (f_skew(dfn, dfd), scipy_s),\n",
    "    \"kurt_excess\": (f_kurt_excess(dfn, dfd), scipy_k),\n",
    "    \"entropy\": (f_entropy(dfn, dfd), stats.f(dfn, dfd).entropy()),\n",
    "}\n",
    "\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The parameters are **degrees of freedom**:\n",
    "\n",
    "- $d_1=\\mathrm{dfn}$ controls the numerator chi-square $U \\sim \\chi^2_{d_1}$.\n",
    "- $d_2=\\mathrm{dfd}$ controls the denominator chi-square $V \\sim \\chi^2_{d_2}$.\n",
    "\n",
    "### How shape changes\n",
    "\n",
    "- **Small $d_2$** means a noisy denominator $V/d_2$, which creates a **very heavy right tail**.\n",
    "  - This is why the mean/variance/skewness/kurtosis stop existing as $d_2$ crosses $2,4,6,8$.\n",
    "- **Larger $d_1$** pushes the distribution away from 0 (the density behaves like $x^{d_1/2-1}$ near 0).\n",
    "\n",
    "### Useful limits\n",
    "\n",
    "- As $d_2 \\to \\infty$, the denominator concentrates: $V/d_2 \\to 1$, so\n",
    "\n",
    "  $$\\mathrm{F}(d_1,d_2) \\Rightarrow \\chi^2_{d_1}/d_1.$$\n",
    "\n",
    "- As both $d_1,d_2$ grow, both $U/d_1$ and $V/d_2$ concentrate near 1, and the ratio concentrates near **1**.\n",
    "- Reciprocal symmetry: $X \\sim \\mathrm{F}(d_1,d_2)$ implies $1/X \\sim \\mathrm{F}(d_2,d_1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.001, 6.0, 600)\n",
    "\n",
    "# Vary dfd (tail heaviness)\n",
    "dfn_fixed = 5\n",
    "params_tail = [(dfn_fixed, 5), (dfn_fixed, 10), (dfn_fixed, 30)]\n",
    "\n",
    "fig = go.Figure()\n",
    "for dfn, dfd in params_tail:\n",
    "    fig.add_trace(go.Scatter(x=x, y=f_pdf(x, dfn, dfd), mode=\"lines\", name=f\"dfn={dfn}, dfd={dfd}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F PDF: effect of the denominator df (tail heaviness)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"pdf(x)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Vary dfn (left/peak behavior)\n",
    "dfd_fixed = 10\n",
    "params_peak = [(2.5, dfd_fixed), (5, dfd_fixed), (20, dfd_fixed)]\n",
    "\n",
    "fig = go.Figure()\n",
    "for dfn, dfd in params_peak:\n",
    "    fig.add_trace(go.Scatter(x=x, y=f_pdf(x, dfn, dfd), mode=\"lines\", name=f\"dfn={dfn}, dfd={dfd}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F PDF: effect of the numerator df (behavior near 0 and peak)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"pdf(x)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "This section sketches three useful derivations: the CDF, the mean/variance, and the likelihood.\n",
    "\n",
    "### Deriving the CDF via a Beta transformation\n",
    "\n",
    "Let $X \\sim \\mathrm{F}(d_1,d_2)$ and define\n",
    "\n",
    "$$\n",
    "U = \\frac{d_1 X}{d_1 X + d_2} \\in (0,1).\n",
    "$$\n",
    "\n",
    "Using the Beta-prime identity (or a Jacobian change of variables), one can show\n",
    "\n",
    "$$\n",
    "U \\sim \\mathrm{Beta}\\left(\\frac{d_1}{2},\\frac{d_2}{2}\\right).\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X\\le x)\n",
    "= \\mathbb{P}\\left(\\frac{d_1 X}{d_1 X + d_2} \\le \\frac{d_1 x}{d_1 x + d_2}\\right)\n",
    "= \\mathbb{P}\\left(U \\le u(x)\\right)\n",
    "= \\mathrm{I}_{u(x)}\\!\\left(\\frac{d_1}{2},\\frac{d_2}{2}\\right).\n",
    "$$\n",
    "\n",
    "### Deriving moments\n",
    "\n",
    "From the Beta connection\n",
    "\n",
    "$$\n",
    "X = \\frac{d_2}{d_1}\\,\\frac{B}{1-B},\\qquad B\\sim\\mathrm{Beta}(a,b),\\ a=d_1/2,\\ b=d_2/2.\n",
    "$$\n",
    "\n",
    "Then for $k<b$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k] = \\left(\\frac{d_2}{d_1}\\right)^k\\,\\mathbb{E}\\left[B^k(1-B)^{-k}\\right]\n",
    "= \\left(\\frac{d_2}{d_1}\\right)^k\\,\\frac{\\mathrm{B}(a+k,b-k)}{\\mathrm{B}(a,b)}.\n",
    "$$\n",
    "\n",
    "Setting $k=1$ and $k=2$ yields $\\mathbb{E}[X]$ and $\\mathbb{E}[X^2]$; then\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2.\n",
    "$$\n",
    "\n",
    "### Likelihood (i.i.d. sample)\n",
    "\n",
    "Given data $x_1,\\dots,x_n$ i.i.d. from $\\mathrm{F}(d_1,d_2)$ (standard form), the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell(d_1,d_2)\n",
    "&= \\sum_{i=1}^n \\log f(x_i;d_1,d_2)\\\\\n",
    "&= n\\left[\\tfrac{d_1}{2}\\log\\left(\\tfrac{d_1}{d_2}\\right) - \\log \\mathrm{B}(d_1/2,d_2/2)\\right]\n",
    "  + \\left(\\tfrac{d_1}{2}-1\\right)\\sum_{i=1}^n \\log x_i\n",
    "  - \\tfrac{d_1+d_2}{2}\\sum_{i=1}^n \\log\\left(1+\\tfrac{d_1}{d_2}x_i\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "There is no closed-form MLE for $(d_1,d_2)$; it is typically found by numerical optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_loglik(dfn: float, dfd: float, data: np.ndarray) -> float:\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    if np.any(data <= 0):\n",
    "        return -np.inf\n",
    "    return float(np.sum(f_logpdf(data, dfn, dfd)))\n",
    "\n",
    "\n",
    "# Demonstration: recover parameters from synthetic data using log-likelihood.\n",
    "rng_fit = np.random.default_rng(123)\n",
    "dfn_true, dfd_true = 6.0, 18.0\n",
    "sample = stats.f(dfn_true, dfd_true).rvs(size=2000, random_state=rng_fit)\n",
    "\n",
    "\n",
    "def neg_loglik(theta: np.ndarray) -> float:\n",
    "    # Optimize in log-space to enforce positivity.\n",
    "    dfn = float(np.exp(theta[0]))\n",
    "    dfd = float(np.exp(theta[1]))\n",
    "    return -f_loglik(dfn, dfd, sample)\n",
    "\n",
    "\n",
    "theta0 = np.log([5.0, 10.0])\n",
    "res = optimize.minimize(neg_loglik, theta0, method=\"Nelder-Mead\")\n",
    "\n",
    "dfn_hat, dfd_hat = np.exp(res.x)\n",
    "\n",
    "# Compare to SciPy's built-in MLE (fix loc=0, scale=1).\n",
    "dfn_fit, dfd_fit, loc_fit, scale_fit = stats.f.fit(sample, floc=0, fscale=1)\n",
    "\n",
    "{\n",
    "    \"true\": (dfn_true, dfd_true),\n",
    "    \"mle_manual\": (dfn_hat, dfd_hat),\n",
    "    \"mle_scipy_fit\": (dfn_fit, dfd_fit),\n",
    "    \"success\": res.success,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "The chi-square ratio representation gives a simple sampling recipe:\n",
    "\n",
    "1. Sample $U \\sim \\chi^2_{d_1}$ and $V \\sim \\chi^2_{d_2}$ independently.\n",
    "2. Return $X = (U/d_1)/(V/d_2)$.\n",
    "\n",
    "A chi-square random variable is a Gamma:\n",
    "\n",
    "$$\n",
    "\\chi^2_{\\nu} \\equiv \\mathrm{Gamma}\\left(k=\\nu/2,\\;\\text{scale}=2\\right).\n",
    "$$\n",
    "\n",
    "So sampling from F reduces to sampling from a Gamma distribution. Below we implement a **NumPy-only** Gamma sampler using the Marsaglia–Tsang method.\n",
    "\n",
    "### Marsaglia–Tsang (high level)\n",
    "\n",
    "For $k\\ge 1$ (shape):\n",
    "\n",
    "- Draw $Z\\sim\\mathcal{N}(0,1)$ and set $V=(1+cZ)^3$.\n",
    "- Accept/reject using a cheap test first, then a log test.\n",
    "\n",
    "For $k<1$, boost to $k+1$ and apply a power transform.\n",
    "\n",
    "This is a standard, fast algorithm used in many libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_rvs_numpy(shape: float, size: int, rng: np.random.Generator, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Sample Gamma(shape, scale) using NumPy only (Marsaglia–Tsang).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape:\n",
    "        k > 0\n",
    "    size:\n",
    "        number of samples\n",
    "    rng:\n",
    "        NumPy Generator\n",
    "    scale:\n",
    "        theta > 0 (multiplicative scale)\n",
    "    \"\"\"\n",
    "\n",
    "    k = float(shape)\n",
    "    theta = float(scale)\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"shape must be > 0\")\n",
    "    if theta <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    # k < 1: boost to k+1 and apply power transform\n",
    "    if k < 1:\n",
    "        g = gamma_rvs_numpy(k + 1.0, size, rng, scale=1.0)\n",
    "        u = rng.random(size)\n",
    "        return theta * g * (u ** (1.0 / k))\n",
    "\n",
    "    # k >= 1: Marsaglia–Tsang\n",
    "    d = k - 1.0 / 3.0\n",
    "    c = 1.0 / np.sqrt(9.0 * d)\n",
    "\n",
    "    out = np.empty(size, dtype=float)\n",
    "    filled = 0\n",
    "\n",
    "    while filled < size:\n",
    "        n = size - filled\n",
    "        x = rng.standard_normal(n)\n",
    "        v = 1.0 + c * x\n",
    "        v = v * v * v  # (1 + c x)^3\n",
    "        u = rng.random(n)\n",
    "\n",
    "        positive = v > 0\n",
    "\n",
    "        # First (cheap) acceptance\n",
    "        accept = positive & (u < 1.0 - 0.0331 * (x**4))\n",
    "\n",
    "        # Second acceptance (log test)\n",
    "        log_v = np.zeros_like(v)\n",
    "        log_v[positive] = np.log(v[positive])\n",
    "\n",
    "        accept2 = positive & (~accept) & (np.log(u) < 0.5 * x * x + d * (1.0 - v + log_v))\n",
    "\n",
    "        accept = accept | accept2\n",
    "        accepted = d * v[accept]\n",
    "\n",
    "        take = min(accepted.size, n)\n",
    "        out[filled : filled + take] = accepted[:take]\n",
    "        filled += take\n",
    "\n",
    "    return theta * out\n",
    "\n",
    "\n",
    "def chisquare_rvs_numpy(df: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Sample ChiSquare(df) using Gamma(df/2, scale=2).\"\"\"\n",
    "    df = float(df)\n",
    "    if df <= 0:\n",
    "        raise ValueError(\"df must be > 0\")\n",
    "    return gamma_rvs_numpy(df / 2.0, size, rng, scale=2.0)\n",
    "\n",
    "\n",
    "def f_rvs_numpy(dfn: float, dfd: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Sample F(dfn, dfd) via ratio of independent chi-squares.\"\"\"\n",
    "\n",
    "    u = chisquare_rvs_numpy(dfn, size, rng)\n",
    "    v = chisquare_rvs_numpy(dfd, size, rng)\n",
    "    return (u / dfn) / (v / dfd)\n",
    "\n",
    "\n",
    "# Monte Carlo validation against theory\n",
    "n = 200_000\n",
    "params = (5.0, 10.0)\n",
    "samples_numpy = f_rvs_numpy(*params, n, rng)\n",
    "\n",
    "m_theory, v_theory = stats.f(*params).stats(moments=\"mv\")\n",
    "np.mean(samples_numpy), np.var(samples_numpy), m_theory, v_theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We will visualize:\n",
    "\n",
    "- the **PDF** for several parameter settings\n",
    "- the **CDF** and an empirical CDF from Monte Carlo samples\n",
    "- a **histogram** of simulated samples overlaid with the theoretical PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn, dfd = 5.0, 10.0\n",
    "x = np.linspace(0.001, 6.0, 600)\n",
    "\n",
    "# PDF line + Monte Carlo histogram\n",
    "n_vis = 60_000\n",
    "samples = f_rvs_numpy(dfn, dfd, n_vis, rng)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples,\n",
    "        histnorm=\"probability density\",\n",
    "        nbinsx=120,\n",
    "        name=\"Monte Carlo\",\n",
    "        opacity=0.55,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=f_pdf(x, dfn, dfd), mode=\"lines\", name=\"PDF (theory)\", line=dict(width=3)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"F(dfn={dfn}, dfd={dfd}): histogram vs PDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    "    bargap=0.02,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# CDF vs empirical CDF\n",
    "xs = np.sort(samples)\n",
    "ecdf = np.arange(1, xs.size + 1) / xs.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=f_cdf(x, dfn, dfd), mode=\"lines\", name=\"CDF (theory)\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=xs, y=ecdf, mode=\"lines\", name=\"CDF (empirical)\", line=dict(width=2, dash=\"dot\")))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"F(dfn={dfn}, dfd={dfd}): empirical CDF vs theory\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"CDF\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.f`)\n",
    "\n",
    "SciPy provides a robust implementation with location/scale support.\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "dist = stats.f(dfn, dfd)          # standard form\n",
    "pdf_vals = dist.pdf(x)\n",
    "cdf_vals = dist.cdf(x)\n",
    "rv = dist.rvs(size=1000, random_state=rng)\n",
    "\n",
    "# Fit (MLE). By default SciPy fits dfn, dfd, loc, scale.\n",
    "# If you want the *standard* F family, fix loc=0 and scale=1.\n",
    "dfn_hat, dfd_hat, loc_hat, scale_hat = stats.f.fit(data, floc=0, fscale=1)\n",
    "```\n",
    "\n",
    "Below is a short, end-to-end example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn, dfd = 4.0, 12.0\n",
    "x0 = np.array([0.25, 0.5, 1.0, 2.0, 4.0])\n",
    "\n",
    "dist = stats.f(dfn, dfd)\n",
    "\n",
    "pdf = dist.pdf(x0)\n",
    "cdf = dist.cdf(x0)\n",
    "\n",
    "rv = dist.rvs(size=5, random_state=rng)\n",
    "\n",
    "# Fitting: generate data, then recover parameters.\n",
    "rng_fit = np.random.default_rng(7)\n",
    "data = stats.f(dfn, dfd).rvs(size=4000, random_state=rng_fit)\n",
    "\n",
    "dfn_hat, dfd_hat, loc_hat, scale_hat = stats.f.fit(data, floc=0, fscale=1)\n",
    "\n",
    "{\n",
    "    \"x\": x0,\n",
    "    \"pdf\": pdf,\n",
    "    \"cdf\": cdf,\n",
    "    \"rvs\": rv,\n",
    "    \"fit\": {\"dfn_hat\": dfn_hat, \"dfd_hat\": dfd_hat, \"loc\": loc_hat, \"scale\": scale_hat},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing\n",
    "\n",
    "1) **Variance ratio test (Gaussian assumption)**\n",
    "\n",
    "If $X_1,\\dots,X_{n_1}$ and $Y_1,\\dots,Y_{n_2}$ are independent normal samples with equal variance under $H_0$,\n",
    "then\n",
    "\n",
    "$$\n",
    "\\frac{S_X^2}{S_Y^2} \\sim \\mathrm{F}(n_1-1,\\,n_2-1)\n",
    "$$\n",
    "\n",
    "where $S^2$ denotes the sample variance.\n",
    "\n",
    "2) **ANOVA / regression**\n",
    "\n",
    "F statistics often take the form\n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{signal per parameter}}{\\text{noise per degree of freedom}},\n",
    "$$\n",
    "\n",
    "and compare explained vs unexplained variability.\n",
    "\n",
    "### B) Bayesian modeling (variance ratios)\n",
    "\n",
    "For a normal sample with Jeffreys prior $p(\\mu,\\sigma^2)\\propto 1/\\sigma^2$, the posterior for $\\sigma^2$ is an inverse-chi-square.\n",
    "For two independent groups, the posterior for the **variance ratio** satisfies\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma_1^2}{\\sigma_2^2}\\,\\bigg|\\,\\text{data}\n",
    "\\;\\sim\\;\n",
    "\\frac{s_1^2}{s_2^2}\\;\\mathrm{F}(\\nu_2,\\nu_1),\n",
    "\\qquad \\nu_i=n_i-1.\n",
    "$$\n",
    "\n",
    "This yields a closed-form credible interval for $\\sigma_1^2/\\sigma_2^2$.\n",
    "\n",
    "### C) Generative modeling\n",
    "\n",
    "The F distribution is a flexible **heavy-tailed positive** distribution. One trick is to use it as a random *scale*:\n",
    "\n",
    "$$\n",
    "S \\sim \\mathrm{F}(d_1,d_2),\\qquad Y = \\sqrt{S}\\,\\varepsilon,\\ \\varepsilon\\sim\\mathcal{N}(0,1).\n",
    "$$\n",
    "\n",
    "This produces occasional large scales (outliers) while remaining easy to simulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Variance ratio test example\n",
    "rng_test = np.random.default_rng(0)\n",
    "\n",
    "n1, n2 = 25, 18\n",
    "sigma = 2.0\n",
    "x = rng_test.normal(0.0, sigma, size=n1)\n",
    "y = rng_test.normal(0.0, sigma, size=n2)\n",
    "\n",
    "s1 = np.var(x, ddof=1)\n",
    "s2 = np.var(y, ddof=1)\n",
    "F_stat = s1 / s2\n",
    "\n",
    "# Two-sided p-value\n",
    "p_left = stats.f.cdf(F_stat, n1 - 1, n2 - 1)\n",
    "p_right = stats.f.sf(F_stat, n1 - 1, n2 - 1)\n",
    "p_two_sided = 2 * min(p_left, p_right)\n",
    "\n",
    "# B) Bayesian variance ratio posterior (Jeffreys prior)\n",
    "nu1, nu2 = n1 - 1, n2 - 1\n",
    "scale_ratio = s1 / s2\n",
    "\n",
    "# Posterior: (sigma1^2/sigma2^2) | data ~ scale_ratio * F(nu2, nu1)\n",
    "ci_level = 0.95\n",
    "alpha = 0.5 * (1 - ci_level)\n",
    "q_lo = stats.f.ppf(alpha, nu2, nu1)\n",
    "q_hi = stats.f.ppf(1 - alpha, nu2, nu1)\n",
    "ci_post = (scale_ratio * q_lo, scale_ratio * q_hi)\n",
    "\n",
    "# Monte Carlo check: sample sigma_i^2 ~ Inv-chi-square(nu_i, s_i^2)\n",
    "mc = 80_000\n",
    "sigma1_sq = (nu1 * s1) / chisquare_rvs_numpy(nu1, mc, rng_test)\n",
    "sigma2_sq = (nu2 * s2) / chisquare_rvs_numpy(nu2, mc, rng_test)\n",
    "ratio_mc = sigma1_sq / sigma2_sq\n",
    "ci_mc = (np.quantile(ratio_mc, alpha), np.quantile(ratio_mc, 1 - alpha))\n",
    "\n",
    "# C) Generative modeling: heavy-tailed scale mixture\n",
    "m = 120_000\n",
    "s = f_rvs_numpy(5.0, 5.0, m, rng_test)\n",
    "eps = rng_test.standard_normal(m)\n",
    "y_heavy = np.sqrt(s) * eps\n",
    "y_normal = eps\n",
    "\n",
    "# Compare tail probabilities\n",
    "thr = 3.0\n",
    "p_tail_heavy = np.mean(np.abs(y_heavy) > thr)\n",
    "p_tail_normal = np.mean(np.abs(y_normal) > thr)\n",
    "\n",
    "{\n",
    "    \"variance_ratio_test\": {\"F\": F_stat, \"p_two_sided\": p_two_sided},\n",
    "    \"posterior_CI_closed_form\": ci_post,\n",
    "    \"posterior_CI_monte_carlo\": ci_mc,\n",
    "    \"tail_P(|Y|>3)\": {\"heavy\": p_tail_heavy, \"normal\": p_tail_normal},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: $d_1\\le 0$ or $d_2\\le 0$ is invalid.\n",
    "- **Non-existent moments**: if $d_2\\le 2$, the mean is infinite; if $d_2\\le 4$, the variance is infinite; etc.\n",
    "- **Numerical stability**: for extreme $x$ or degrees of freedom, prefer `logpdf` over `pdf`.\n",
    "- **`loc`/`scale` confusion in SciPy**: `scipy.stats.f` is a *four-parameter* family by default; fix `loc=0, scale=1` if you mean the standard F distribution.\n",
    "- **Fitting**: MLE can be sensitive to outliers (heavy tail) and may converge slowly; check diagnostics and consider robust alternatives when appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- The F distribution models a **ratio of two variance-like quantities**: $(\\chi^2_{d_1}/d_1)/(\\chi^2_{d_2}/d_2)$.\n",
    "- The PDF/CDF have closed forms involving the **Beta function** and the **regularized incomplete beta**.\n",
    "- Moments exist only up to order $k<d_2/2$; in particular, the mean requires $d_2>2$ and the variance requires $d_2>4$.\n",
    "- Sampling is straightforward via **Gamma/chi-square sampling**; a NumPy-only Marsaglia–Tsang Gamma sampler works well.\n",
    "- In practice, `scipy.stats.f` provides evaluation, simulation, and MLE fitting.\n",
    "\n",
    "**References**\n",
    "- Marsaglia, G. & Tsang, W. W. (2000). *A Simple Method for Generating Gamma Variables*.\n",
    "- Johnson, Kotz, and Balakrishnan. *Continuous Univariate Distributions, Volume 2* (2nd ed.), Wiley, 1995.\n",
    "- SciPy documentation: `scipy.stats.f`, `scipy.special.betainc`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
