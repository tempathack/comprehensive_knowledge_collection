{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Normal distribution (`matrix_normal`) — Gaussian random matrices with separable covariance\n",
    "\n",
    "The **matrix normal distribution** is the matrix-valued analogue of the multivariate normal.\n",
    "It models a random matrix \\(X \\in \\mathbb{R}^{n\\times p}\\) whose dependence structure **separates** into:\n",
    "\n",
    "- a **row covariance** matrix \\(U \\in \\mathbb{R}^{n\\times n}\\), and\n",
    "- a **column covariance** matrix \\(V \\in \\mathbb{R}^{p\\times p}\\).\n",
    "\n",
    "Equivalently, if you stack the columns of \\(X\\) into a vector (`vec`), then\n",
    "\\(\\mathrm{vec}(X)\\) is multivariate normal with covariance \\(V \\otimes U\\) (a Kronecker product).\n",
    "\n",
    "**Goals**\n",
    "- Understand what the matrix normal models and why the Kronecker structure is useful.\n",
    "- Work with the PDF, moments, entropy, and key identities.\n",
    "- Interpret parameters \\(M, U, V\\) and see how they shape samples.\n",
    "- Sample from \\(\\mathrm{MN}(M, U, V)\\) with a **NumPy-only** algorithm.\n",
    "- Use `scipy.stats.matrix_normal` for evaluation and simulation, and fit \\(U,V\\) via an alternating MLE.\n",
    "\n",
    "**Prerequisites**\n",
    "- Multivariate normal distribution (quadratic forms, covariance).\n",
    "- Basic linear algebra (trace, determinant, Cholesky).\n",
    "- NumPy + plotting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2, multivariate_normal, norm\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Plotly notebook defaults (mirrors patterns used elsewhere in this repo)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Quick/slow toggle\n",
    "FAST_RUN = True\n",
    "N_SAMPLES = 15_000 if FAST_RUN else 120_000\n",
    "N_EXPERIMENTS = 250 if FAST_RUN else 2_000\n",
    "\n",
    "print(\"Python\", platform.python_version())\n",
    "print(\"NumPy\", np.__version__)\n",
    "print(\"SciPy\", scipy.__version__)\n",
    "print(\"Plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_f(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Column-stacking vectorization (Fortran order).\n",
    "\n",
    "    With this convention the Kronecker covariance is `np.kron(V, U)`.\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.asarray(A)\n",
    "    return A.reshape(-1, order=\"F\")\n",
    "\n",
    "\n",
    "def unvec_f(v: np.ndarray, shape: tuple[int, int]) -> np.ndarray:\n",
    "    v = np.asarray(v)\n",
    "    return v.reshape(shape, order=\"F\")\n",
    "\n",
    "\n",
    "def ar1_cov(n: int, rho: float, sigma2: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"AR(1)-style covariance: sigma2 * rho^{|i-j|}. SPD for |rho| < 1.\"\"\"\n",
    "\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive\")\n",
    "    if not (-1.0 < rho < 1.0):\n",
    "        raise ValueError(\"rho must be in (-1, 1)\")\n",
    "    idx = np.arange(n)\n",
    "    return sigma2 * (rho ** np.abs(idx[:, None] - idx[None, :]))\n",
    "\n",
    "\n",
    "def chol_spd(A: np.ndarray, name: str = \"A\", jitter: float = 1e-12, max_tries: int = 6) -> np.ndarray:\n",
    "    \"\"\"Cholesky factor of an SPD matrix with optional diagonal jitter.\n",
    "\n",
    "    Returns lower-triangular L such that A ≈ L L^T.\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n",
    "        raise ValueError(f\"{name} must be a square matrix\")\n",
    "\n",
    "    A = 0.5 * (A + A.T)  # symmetrize to reduce numerical asymmetry\n",
    "    eye = np.eye(A.shape[0])\n",
    "\n",
    "    for k in range(max_tries):\n",
    "        try:\n",
    "            return np.linalg.cholesky(A + (jitter * (10**k)) * eye)\n",
    "        except np.linalg.LinAlgError:\n",
    "            pass\n",
    "\n",
    "    raise ValueError(f\"{name} must be symmetric positive definite (Cholesky failed)\")\n",
    "\n",
    "\n",
    "def validate_matrix_normal_params(mean: np.ndarray, rowcov: np.ndarray, colcov: np.ndarray) -> tuple[int, int]:\n",
    "    mean = np.asarray(mean, dtype=float)\n",
    "    if mean.ndim != 2:\n",
    "        raise ValueError(\"mean must be a 2D matrix\")\n",
    "\n",
    "    n, p = mean.shape\n",
    "\n",
    "    rowcov = np.asarray(rowcov, dtype=float)\n",
    "    colcov = np.asarray(colcov, dtype=float)\n",
    "\n",
    "    if rowcov.shape != (n, n):\n",
    "        raise ValueError(f\"rowcov must have shape ({n}, {n})\")\n",
    "    if colcov.shape != (p, p):\n",
    "        raise ValueError(f\"colcov must have shape ({p}, {p})\")\n",
    "\n",
    "    _ = chol_spd(rowcov, name=\"rowcov\")\n",
    "    _ = chol_spd(colcov, name=\"colcov\")\n",
    "\n",
    "    return n, p\n",
    "\n",
    "\n",
    "def matrix_normal_rvs_numpy(\n",
    "    mean: np.ndarray,\n",
    "    rowcov: np.ndarray,\n",
    "    colcov: np.ndarray,\n",
    "    size: int = 1,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Sample from MN(mean, rowcov, colcov) using only NumPy.\n",
    "\n",
    "    Algorithm:\n",
    "    1) Factorize rowcov = L_u L_u^T and colcov = L_v L_v^T.\n",
    "    2) Draw Z with i.i.d. N(0,1) entries.\n",
    "    3) Return mean + L_u Z L_v^T.\n",
    "\n",
    "    Returns:\n",
    "    - if size == 1: array (n, p)\n",
    "    - else: array (size, n, p)\n",
    "    \"\"\"\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    mean = np.asarray(mean, dtype=float)\n",
    "    rowcov = np.asarray(rowcov, dtype=float)\n",
    "    colcov = np.asarray(colcov, dtype=float)\n",
    "    n, p = validate_matrix_normal_params(mean, rowcov, colcov)\n",
    "\n",
    "    L_u = chol_spd(rowcov, name=\"rowcov\")\n",
    "    L_v = chol_spd(colcov, name=\"colcov\")\n",
    "\n",
    "    Z = rng.standard_normal(size=(size, n, p))\n",
    "    out = np.empty_like(Z)\n",
    "\n",
    "    for i in range(size):\n",
    "        out[i] = mean + L_u @ Z[i] @ L_v.T\n",
    "\n",
    "    return out[0] if size == 1 else out\n",
    "\n",
    "\n",
    "def matrix_normal_logpdf_numpy(\n",
    "    x: np.ndarray,\n",
    "    mean: np.ndarray,\n",
    "    rowcov: np.ndarray,\n",
    "    colcov: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Log-PDF of X ~ MN(mean, rowcov, colcov) using NumPy linear algebra.\n",
    "\n",
    "    Supports:\n",
    "    - x shape (n, p) -> returns scalar\n",
    "    - x shape (m, n, p) -> returns length-m array\n",
    "    \"\"\"\n",
    "\n",
    "    mean = np.asarray(mean, dtype=float)\n",
    "    rowcov = np.asarray(rowcov, dtype=float)\n",
    "    colcov = np.asarray(colcov, dtype=float)\n",
    "\n",
    "    n, p = validate_matrix_normal_params(mean, rowcov, colcov)\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.shape[-2:] != (n, p):\n",
    "        raise ValueError(f\"x must end with shape ({n}, {p})\")\n",
    "\n",
    "    L_u = chol_spd(rowcov, name=\"rowcov\")\n",
    "    L_v = chol_spd(colcov, name=\"colcov\")\n",
    "\n",
    "    logdet_u = 2.0 * np.sum(np.log(np.diag(L_u)))\n",
    "    logdet_v = 2.0 * np.sum(np.log(np.diag(L_v)))\n",
    "\n",
    "    const = (\n",
    "        -0.5 * n * p * np.log(2.0 * np.pi)\n",
    "        -0.5 * p * logdet_u\n",
    "        -0.5 * n * logdet_v\n",
    "    )\n",
    "\n",
    "    def quad_form(E: np.ndarray) -> float:\n",
    "        # || L_u^{-1} E L_v^{-T} ||_F^2 computed via two triangular solves\n",
    "        A = np.linalg.solve(L_u, E)  # (n, p)\n",
    "        B = np.linalg.solve(L_v, A.T)  # (p, n)\n",
    "        return float(np.sum(B**2))\n",
    "\n",
    "    if x.ndim == 2:\n",
    "        E = x - mean\n",
    "        return const - 0.5 * quad_form(E)\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        out = np.empty(x.shape[0], dtype=float)\n",
    "        for i in range(x.shape[0]):\n",
    "            E = x[i] - mean\n",
    "            out[i] = const - 0.5 * quad_form(E)\n",
    "        return out\n",
    "\n",
    "    raise ValueError(\"x must be 2D or 3D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "| Item | Value |\n",
    "|---|---|\n",
    "| Name | Matrix Normal (`matrix_normal`) |\n",
    "| Type | **Continuous** |\n",
    "| Support | \\(X \\in \\mathbb{R}^{n\\times p}\\) (all real matrices of a fixed shape) |\n",
    "| Parameters | mean matrix \\(M \\in \\mathbb{R}^{n\\times p}\\), row covariance \\(U \\in \\mathbb{S}_{++}^n\\), column covariance \\(V \\in \\mathbb{S}_{++}^p\\) |\n",
    "| Parameter space | \\(\\mathbb{S}_{++}^k\\) denotes the set of \\(k\\times k\\) symmetric positive definite matrices |\n",
    "\n",
    "We write\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathrm{MN}_{n\\times p}(M,\\,U,\\,V).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "Matrix normal is a distribution over **random matrices** that behaves like a Gaussian in every direction.\n",
    "The key modeling assumption is **separable covariance**:\n",
    "\n",
    "- **Rows** are correlated according to \\(U\\).\n",
    "- **Columns** are correlated according to \\(V\\).\n",
    "- The covariance between entries factorizes:\n",
    "\n",
    "  \\[\n",
    "  \\mathrm{Cov}(X_{ij}, X_{k\\ell}) = U_{ik}\\,V_{j\\ell}.\n",
    "  \\]\n",
    "\n",
    "This Kronecker structure is useful when a full \\((np)\\times(np)\\) covariance is too expensive to store or estimate.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Multivariate linear regression / MANOVA**: residuals in \\(Y\\in\\mathbb{R}^{n\\times p}\\) often have correlations across samples (rows) and across responses (columns).\n",
    "- **Spatiotemporal grids**: rows = time, columns = space (or vice versa) with separable dependence.\n",
    "- **Images / patches**: covariance that factors into horizontal/vertical components.\n",
    "- **Gaussian-process models with Kronecker structure**: matrix normal appears when discretizing separable kernels.\n",
    "\n",
    "### Relations to other distributions\n",
    "- If \\(X \\sim \\mathrm{MN}(M,U,V)\\), then\n",
    "\n",
    "  \\[\n",
    "  \\mathrm{vec}(X) \\sim \\mathcal{N}(\\mathrm{vec}(M),\\, V \\otimes U).\n",
    "  \\]\n",
    "\n",
    "- If \\(p=1\\) (a single column), matrix normal reduces to an \\(n\\)-dimensional multivariate normal.\n",
    "- If \\(U = I_n\\) and \\(V = I_p\\), entries of \\(X\\) are i.i.d. \\(\\mathcal{N}(M_{ij}, 1)\\).\n",
    "- In matrix-variate statistics, sums of quadratic forms in matrix-normal samples lead to **Wishart** distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let \\(X \\in \\mathbb{R}^{n\\times p}\\). We write\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathrm{MN}_{n\\times p}(M,\\,U,\\,V)\n",
    "\\]\n",
    "\n",
    "with mean \\(M\\in\\mathbb{R}^{n\\times p}\\), row covariance \\(U\\in\\mathbb{S}_{++}^n\\), and column covariance \\(V\\in\\mathbb{S}_{++}^p\\).\n",
    "\n",
    "### PDF\n",
    "The density with respect to Lebesgue measure on \\(\\mathbb{R}^{np}\\) is\n",
    "\n",
    "\\[\n",
    " f(X\\mid M,U,V)\n",
    "= \\frac{\\exp\\left(-\\tfrac12\\,\\mathrm{tr}\\left(U^{-1}(X-M)V^{-1}(X-M)^\\top\\right)\\right)}\n",
    "       {(2\\pi)^{\\tfrac{np}{2}}\\,|U|^{\\tfrac{p}{2}}\\,|V|^{\\tfrac{n}{2}}}.\n",
    "\\]\n",
    "\n",
    "A numerically stable way to view the quadratic term is\n",
    "\n",
    "\\[\n",
    "\\mathrm{tr}\\left(U^{-1}(X-M)V^{-1}(X-M)^\\top\\right)\n",
    "= \\lVert L_U^{-1}(X-M)L_V^{-\\top} \\rVert_F^2,\n",
    "\\]\n",
    "\n",
    "where \\(U=L_U L_U^\\top\\) and \\(V=L_V L_V^\\top\\) are Cholesky factorizations.\n",
    "\n",
    "### CDF\n",
    "A full multivariate CDF is defined componentwise:\n",
    "\n",
    "\\[\n",
    "F(X) = \\mathbb{P}(X_{11}\\le x_{11},\\,\\dots,\\,X_{np}\\le x_{np}).\n",
    "\\]\n",
    "\n",
    "In terms of vectorization,\n",
    "\n",
    "\\[\n",
    "F(X) = \\Phi_{np}\\big(\\mathrm{vec}(X);\\,\\mathrm{vec}(M),\\,V\\otimes U\\big),\n",
    "\\]\n",
    "\n",
    "where \\(\\Phi_{np}\\) is the \\(np\\)-dimensional multivariate normal CDF. In general there is **no closed-form expression**;\n",
    "numerical evaluation is feasible only for small \\(np\\).\n",
    "\n",
    "### LaTeX notation\n",
    "Common shorthands:\n",
    "\n",
    "\\[\n",
    "X \\sim \\mathcal{MN}(M,U,V)\\quad\\text{or}\\quad X \\sim \\mathrm{MN}_{n\\times p}(M,U,V).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick numerical check of logpdf against SciPy\n",
    "n, p = 3, 2\n",
    "M = rng.normal(size=(n, p))\n",
    "U = ar1_cov(n, rho=0.4, sigma2=1.5)\n",
    "V = ar1_cov(p, rho=-0.2, sigma2=0.7)\n",
    "\n",
    "X = matrix_normal_rvs_numpy(M, U, V, size=1, rng=rng)\n",
    "\n",
    "rv = stats.matrix_normal(mean=M, rowcov=U, colcov=V)\n",
    "logpdf_scipy = rv.logpdf(X)\n",
    "logpdf_numpy = matrix_normal_logpdf_numpy(X, M, U, V)\n",
    "\n",
    "float(logpdf_scipy), float(logpdf_numpy), float(logpdf_numpy - logpdf_scipy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "A convenient starting point is the vectorized representation:\n",
    "\n",
    "\\[\n",
    "\\mathrm{vec}(X) \\sim \\mathcal{N}(\\mathrm{vec}(M),\\, \\Sigma),\\quad \\Sigma = V\\otimes U.\n",
    "\\]\n",
    "\n",
    "### Mean and (co)variance\n",
    "- **Mean**: \\(\\mathbb{E}[X]=M\\).\n",
    "- **Covariance of vectorization**: \\(\\mathrm{Cov}(\\mathrm{vec}(X)) = V\\otimes U\\).\n",
    "- **Entrywise covariance**:\n",
    "\n",
    "  \\[\n",
    "  \\mathrm{Cov}(X_{ij}, X_{k\\ell}) = U_{ik}\\,V_{j\\ell}.\n",
    "  \\]\n",
    "\n",
    "  In particular, \\(\\mathrm{Var}(X_{ij}) = U_{ii}V_{jj}\\).\n",
    "\n",
    "### Skewness and kurtosis\n",
    "Because \\(\\mathrm{vec}(X)\\) is multivariate normal:\n",
    "- every centered **third moment** is 0 (zero skewness for any scalar linear functional),\n",
    "- **fourth moments** follow Isserlis' (Wick's) theorem.\n",
    "\n",
    "For any scalar projection \\(S = \\langle T, X\\rangle = \\mathrm{tr}(T^\\top X)\\), we have\n",
    "\\(S\\sim\\mathcal{N}(\\mu_S, \\sigma_S^2)\\), so\n",
    "\n",
    "- skewness \\(=0\\)\n",
    "- kurtosis \\(=3\\) (excess kurtosis \\(=0\\)).\n",
    "\n",
    "### MGF / characteristic function\n",
    "For \\(S = \\mathrm{tr}(T^\\top X)\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[e^{tS}] = \\exp\\left(t\\,\\mathrm{tr}(T^\\top M) + \\tfrac12 t^2\\,\\mathrm{tr}(U T V T^\\top)\\right).\n",
    "\\]\n",
    "\n",
    "Equivalently,\n",
    "\n",
    "\\[\n",
    "M_X(T) = \\mathbb{E}[\\exp(\\mathrm{tr}(T^\\top X))]\n",
    "       = \\exp\\left(\\mathrm{tr}(T^\\top M) + \\tfrac12\\,\\mathrm{tr}(U T V T^\\top)\\right).\n",
    "\\]\n",
    "\n",
    "The characteristic function replaces \\(T\\) with \\(iT\\).\n",
    "\n",
    "### Entropy\n",
    "The differential entropy is that of an \\(np\\)-dimensional normal with covariance \\(V\\otimes U\\):\n",
    "\n",
    "\\[\n",
    "H(X) = \\tfrac12\\log\\big((2\\pi e)^{np}\\,|V\\otimes U|\\big)\n",
    "     = \\tfrac{np}{2}(1+\\log 2\\pi) + \\tfrac{p}{2}\\log|U| + \\tfrac{n}{2}\\log|V|,\n",
    "\\]\n",
    "\n",
    "using \\(|V\\otimes U| = |V|^n |U|^p\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical check: mean and covariance structure\n",
    "n, p = 4, 3\n",
    "M = np.arange(n * p, dtype=float).reshape(n, p) / 10.0\n",
    "U = ar1_cov(n, rho=0.6, sigma2=2.0)\n",
    "V = ar1_cov(p, rho=-0.3, sigma2=1.2)\n",
    "\n",
    "X_samps = matrix_normal_rvs_numpy(M, U, V, size=N_SAMPLES, rng=rng)  # (N, n, p)\n",
    "\n",
    "# Mean\n",
    "M_hat = X_samps.mean(axis=0)\n",
    "\n",
    "# Covariance of vec (column-stacking)\n",
    "X_vec = np.stack([vec_f(X_samps[i]) for i in range(X_samps.shape[0])], axis=0)\n",
    "Sigma_hat = np.cov(X_vec, rowvar=False)\n",
    "Sigma_theory = np.kron(V, U)  # consistent with vec_f\n",
    "\n",
    "mean_err = np.linalg.norm(M_hat - M) / np.linalg.norm(M)\n",
    "rel_cov_err = np.linalg.norm(Sigma_hat - Sigma_theory) / np.linalg.norm(Sigma_theory)\n",
    "\n",
    "mean_err, rel_cov_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### Mean matrix \\(M\\)\n",
    "- \\(M_{ij}\\) is the average value of entry \\((i,j)\\).\n",
    "- Changing \\(M\\) shifts the distribution without affecting dependence.\n",
    "\n",
    "### Row covariance \\(U\\)\n",
    "- Controls dependence **between rows** (across all columns).\n",
    "- For a fixed column \\(j\\), the column vector \\(X_{:,j}\\) satisfies\n",
    "  \\(X_{:,j} \\sim \\mathcal{N}(M_{:,j},\\, V_{jj} U)\\).\n",
    "\n",
    "### Column covariance \\(V\\)\n",
    "- Controls dependence **between columns** (across all rows).\n",
    "- For a fixed row \\(i\\), the row vector \\(X_{i,:}\\) satisfies\n",
    "  \\(X_{i,:} \\sim \\mathcal{N}(M_{i,:},\\, U_{ii} V)\\).\n",
    "\n",
    "### Shape changes\n",
    "- Increasing variances in \\(U\\) (diagonal entries) increases variability across rows.\n",
    "- Increasing variances in \\(V\\) increases variability across columns.\n",
    "- Increasing off-diagonal correlations in \\(U\\) makes rows “move together”.\n",
    "- Increasing off-diagonal correlations in \\(V\\) makes columns “move together”.\n",
    "\n",
    "**Identifiability note**: \\(U\\) and \\(V\\) are not separately identifiable from \\(V\\otimes U\\). For any \\(c>0\\),\n",
    "\n",
    "\\[\n",
    "\\mathrm{MN}(M,\\,U,\\,V) \\equiv \\mathrm{MN}(M,\\,cU,\\,V/c).\n",
    "\\]\n",
    "\n",
    "Many fitting procedures impose a constraint such as \\(\\mathrm{tr}(V)=p\\) (or \\(|V|=1\\)) to fix this scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual intuition: one draw under different row/column correlations\n",
    "n, p = 8, 8\n",
    "M = np.zeros((n, p))\n",
    "\n",
    "scenarios = [\n",
    "    {\"rho_u\": 0.0, \"rho_v\": 0.0, \"title\": \"independent\"},\n",
    "    {\"rho_u\": 0.6, \"rho_v\": 0.0, \"title\": \"row-correlated\"},\n",
    "    {\"rho_u\": 0.6, \"rho_v\": 0.7, \"title\": \"row+col correlated\"},\n",
    "]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=[s[\"title\"] for s in scenarios])\n",
    "\n",
    "for j, s in enumerate(scenarios, start=1):\n",
    "    U = ar1_cov(n, rho=s[\"rho_u\"], sigma2=1.0)\n",
    "    V = ar1_cov(p, rho=s[\"rho_v\"], sigma2=1.0)\n",
    "    X = matrix_normal_rvs_numpy(M, U, V, size=1, rng=rng)\n",
    "    fig.add_trace(go.Heatmap(z=X, coloraxis=\"coloraxis\"), row=1, col=j)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"One draw from MN(0, U, V) under different correlations\",\n",
    "    coloraxis={\"colorscale\": \"RdBu\", \"cmin\": -3, \"cmax\": 3},\n",
    "    height=350,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "We sketch core derivations using vectorization identities.\n",
    "\n",
    "### Expectation\n",
    "Using \\(\\mathrm{vec}(X) \\sim \\mathcal{N}(\\mathrm{vec}(M), V\\otimes U)\\), the mean of a multivariate normal is its location:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[\\mathrm{vec}(X)] = \\mathrm{vec}(M)\\quad\\Rightarrow\\quad \\mathbb{E}[X]=M.\n",
    "\\]\n",
    "\n",
    "### Variance / covariance\n",
    "From \\(\\Sigma = V\\otimes U\\) and the mapping between \\(\\Sigma\\) entries and matrix indices, we obtain\n",
    "\n",
    "\\[\n",
    "\\mathrm{Cov}(X_{ij}, X_{k\\ell}) = U_{ik} V_{j\\ell}.\n",
    "\\]\n",
    "\n",
    "A useful special case is the scalar projection \\(S=\\mathrm{tr}(T^\\top X)=\\mathrm{vec}(T)^\\top\\mathrm{vec}(X)\\):\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(S)\n",
    "= \\mathrm{vec}(T)^\\top (V\\otimes U)\\,\\mathrm{vec}(T)\n",
    "= \\mathrm{tr}(U T V T^\\top).\n",
    "\\]\n",
    "\n",
    "### Likelihood\n",
    "For one observation \\(X\\), the log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell(M,U,V\\mid X)\n",
    "= -\\tfrac{np}{2}\\log(2\\pi) - \\tfrac{p}{2}\\log|U| - \\tfrac{n}{2}\\log|V|\n",
    "  -\\tfrac12\\,\\mathrm{tr}\\left(U^{-1}(X-M)V^{-1}(X-M)^\\top\\right).\n",
    "\\]\n",
    "\n",
    "**MLE notes**\n",
    "- For fixed \\(U,V\\), the MLE of \\(M\\) is the sample mean.\n",
    "- Jointly maximizing over \\((U,V)\\) has no closed form. A common approach is the **flip-flop** (alternating) algorithm.\n",
    "- Because of the scale non-identifiability \\((cU, V/c)\\), one typically normalizes after each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_flop_mle(\n",
    "    X: np.ndarray,\n",
    "    max_iter: int = 50,\n",
    "    tol: float = 1e-6,\n",
    "    normalize: str | None = \"trace_v\",\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, dict]:\n",
    "    \"\"\"Estimate (M, U, V) for matrix-normal samples using the flip-flop algorithm.\"\"\"\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(\"X must have shape (m, n, p)\")\n",
    "\n",
    "    m, n, p = X.shape\n",
    "\n",
    "    M_hat = X.mean(axis=0)\n",
    "    E = X - M_hat\n",
    "\n",
    "    U = np.eye(n)\n",
    "    V = np.eye(p)\n",
    "\n",
    "    history = {\"loglik\": []}\n",
    "    prev_ll = None\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Update U given V\n",
    "        L_v = chol_spd(V, name=\"V\")\n",
    "        U_new = np.zeros((n, n), dtype=float)\n",
    "        for r in range(m):\n",
    "            B = np.linalg.solve(L_v, E[r].T)  # (p, n)\n",
    "            U_new += B.T @ B\n",
    "        U_new /= (m * p)\n",
    "        U_new = 0.5 * (U_new + U_new.T)\n",
    "\n",
    "        # Update V given U\n",
    "        L_u = chol_spd(U_new, name=\"U\")\n",
    "        V_new = np.zeros((p, p), dtype=float)\n",
    "        for r in range(m):\n",
    "            A = np.linalg.solve(L_u, E[r])\n",
    "            V_new += A.T @ A\n",
    "        V_new /= (m * n)\n",
    "        V_new = 0.5 * (V_new + V_new.T)\n",
    "\n",
    "        if normalize == \"trace_v\":\n",
    "            c = float(np.trace(V_new) / p)\n",
    "            if c <= 0:\n",
    "                raise ValueError(\"normalization failed (non-positive trace)\")\n",
    "            V_new = V_new / c\n",
    "            U_new = U_new * c\n",
    "\n",
    "        ll = float(matrix_normal_logpdf_numpy(X, M_hat, U_new, V_new).sum())\n",
    "        history[\"loglik\"].append(ll)\n",
    "\n",
    "        if prev_ll is not None:\n",
    "            rel = abs(ll - prev_ll) / (abs(prev_ll) + 1.0)\n",
    "            if rel < tol:\n",
    "                U, V = U_new, V_new\n",
    "                break\n",
    "\n",
    "        prev_ll = ll\n",
    "        U, V = U_new, V_new\n",
    "\n",
    "    return M_hat, U, V, history\n",
    "\n",
    "\n",
    "# Fit demo on synthetic data\n",
    "n, p = 4, 3\n",
    "M_true = rng.normal(size=(n, p))\n",
    "U_true = ar1_cov(n, rho=0.5, sigma2=1.8)\n",
    "V_true = ar1_cov(p, rho=-0.4, sigma2=0.9)\n",
    "\n",
    "X = matrix_normal_rvs_numpy(M_true, U_true, V_true, size=800 if FAST_RUN else 4_000, rng=rng)\n",
    "M_hat, U_hat, V_hat, info = flip_flop_mle(X, max_iter=60, tol=1e-7)\n",
    "\n",
    "Sigma_true = np.kron(V_true, U_true)\n",
    "Sigma_hat = np.kron(V_hat, U_hat)\n",
    "\n",
    "mean_err = np.linalg.norm(M_hat - M_true) / np.linalg.norm(M_true)\n",
    "rel_sigma_err = np.linalg.norm(Sigma_hat - Sigma_true) / np.linalg.norm(Sigma_true)\n",
    "\n",
    "mean_err, rel_sigma_err, len(info[\"loglik\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "A clean NumPy-only sampling recipe follows directly from the Kronecker structure.\n",
    "\n",
    "If\n",
    "\\(U = L_U L_U^\\top\\) and \\(V = L_V L_V^\\top\\) are Cholesky factorizations and\n",
    "\\(Z\\in\\mathbb{R}^{n\\times p}\\) has i.i.d. \\(\\mathcal{N}(0,1)\\) entries, then\n",
    "\n",
    "\\[\n",
    "X = M + L_U\\,Z\\,L_V^\\top\n",
    "\\]\n",
    "\n",
    "satisfies\n",
    "\\(X\\sim\\mathrm{MN}(M,U,V)\\).\n",
    "\n",
    "**Why it works (sketch)**\n",
    "Using the identity \\(\\mathrm{vec}(AZB^\\top) = (B\\otimes A)\\mathrm{vec}(Z)\\), we get\n",
    "\n",
    "\\[\n",
    "\\mathrm{vec}(X-M) = (L_V\\otimes L_U)\\,\\mathrm{vec}(Z),\n",
    "\\]\n",
    "\n",
    "so the covariance is\n",
    "\\((L_VL_V^\\top)\\otimes(L_UL_U^\\top) = V\\otimes U\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic sampling sanity check\n",
    "n, p = 5, 4\n",
    "M = np.zeros((n, p))\n",
    "U = ar1_cov(n, rho=0.3, sigma2=1.0)\n",
    "V = ar1_cov(p, rho=0.7, sigma2=0.5)\n",
    "\n",
    "X_samps = matrix_normal_rvs_numpy(M, U, V, size=3, rng=rng)\n",
    "X_samps.shape, X_samps[0].mean(), X_samps[0].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "Matrix-normal densities live on \\(\\mathbb{R}^{np}\\), so direct “PDF over \\(X\\)” plots are rarely helpful.\n",
    "Instead, we visualize:\n",
    "\n",
    "- **Univariate marginals** (e.g. a single entry \\(X_{ij}\\))\n",
    "- **Scalar projections** \\(S=\\mathrm{tr}(T^\\top X)\\)\n",
    "- **Bivariate marginals** of a few entries to show correlation\n",
    "\n",
    "These are all normal distributions because the matrix normal is Gaussian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Univariate marginal PDF + CDF for one entry\n",
    "n, p = 4, 4\n",
    "M = np.zeros((n, p))\n",
    "U = ar1_cov(n, rho=0.65, sigma2=1.2)\n",
    "V = ar1_cov(p, rho=-0.3, sigma2=0.8)\n",
    "\n",
    "i, j = 1, 2\n",
    "mu_ij = M[i, j]\n",
    "var_ij = U[i, i] * V[j, j]\n",
    "\n",
    "xs = np.linspace(mu_ij - 4 * np.sqrt(var_ij), mu_ij + 4 * np.sqrt(var_ij), 600)\n",
    "\n",
    "pdf = norm(loc=mu_ij, scale=np.sqrt(var_ij)).pdf(xs)\n",
    "cdf = norm(loc=mu_ij, scale=np.sqrt(var_ij)).cdf(xs)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Marginal PDF\", \"Marginal CDF\"])\n",
    "fig.add_trace(go.Scatter(x=xs, y=pdf, name=\"theory pdf\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=xs, y=cdf, name=\"theory cdf\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"pdf\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"cdf\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title=f\"Entry X[{i},{j}] ~ N({mu_ij:.2f}, {var_ij:.2f})\", height=320)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Monte Carlo vs theory for a scalar projection S = tr(T^T X)\n",
    "\n",
    "n, p = 6, 5\n",
    "M = rng.normal(size=(n, p)) * 0.2\n",
    "U = ar1_cov(n, rho=0.5, sigma2=1.0)\n",
    "V = ar1_cov(p, rho=0.3, sigma2=0.8)\n",
    "\n",
    "T = rng.normal(size=(n, p))\n",
    "\n",
    "mu_S = float(np.sum(T * M))  # tr(T^T M)\n",
    "var_S = float(np.trace(U @ T @ V @ T.T))\n",
    "\n",
    "X_samps = matrix_normal_rvs_numpy(M, U, V, size=N_SAMPLES, rng=rng)\n",
    "S_samps = np.einsum(\"ij,nij->n\", T, X_samps)\n",
    "\n",
    "xs = np.linspace(mu_S - 4 * np.sqrt(var_S), mu_S + 4 * np.sqrt(var_S), 600)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"PDF (hist + theory)\", \"CDF (empirical + theory)\"])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=S_samps, nbinsx=70, histnorm=\"probability density\", name=\"MC\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=xs, y=norm(mu_S, np.sqrt(var_S)).pdf(xs), name=\"theory\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "s_sorted = np.sort(S_samps)\n",
    "emp_cdf = np.linspace(1.0 / len(s_sorted), 1.0, len(s_sorted))\n",
    "fig.add_trace(go.Scatter(x=s_sorted, y=emp_cdf, mode=\"lines\", name=\"empirical\"), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=xs, y=norm(mu_S, np.sqrt(var_S)).cdf(xs), name=\"theory\"), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"s\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"density\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"s\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"cdf\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"S=⟨T,X⟩ ~ N({mu_S:.3f}, {var_S:.3f}) (verified by Monte Carlo)\",\n",
    "    height=350,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Bivariate marginal of two entries to show correlation\n",
    "\n",
    "n, p = 4, 4\n",
    "M = np.zeros((n, p))\n",
    "U = ar1_cov(n, rho=0.8, sigma2=1.0)\n",
    "V = ar1_cov(p, rho=0.0, sigma2=1.0)\n",
    "\n",
    "(i1, j1), (i2, j2) = (0, 1), (2, 1)\n",
    "\n",
    "mu = np.array([M[i1, j1], M[i2, j2]])\n",
    "Sigma2 = np.array(\n",
    "    [\n",
    "        [U[i1, i1] * V[j1, j1], U[i1, i2] * V[j1, j2]],\n",
    "        [U[i2, i1] * V[j2, j1], U[i2, i2] * V[j2, j2]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_samps = matrix_normal_rvs_numpy(M, U, V, size=N_SAMPLES, rng=rng)\n",
    "Y = np.stack([X_samps[:, i1, j1], X_samps[:, i2, j2]], axis=1)\n",
    "\n",
    "xg = np.linspace(Y[:, 0].min(), Y[:, 0].max(), 120)\n",
    "yg = np.linspace(Y[:, 1].min(), Y[:, 1].max(), 120)\n",
    "XG, YG = np.meshgrid(xg, yg)\n",
    "pts = np.stack([XG.ravel(), YG.ravel()], axis=1)\n",
    "Z = multivariate_normal(mean=mu, cov=Sigma2).pdf(pts).reshape(XG.shape)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram2dContour(\n",
    "        x=Y[:, 0],\n",
    "        y=Y[:, 1],\n",
    "        nbinsx=40,\n",
    "        nbinsy=40,\n",
    "        contours=dict(coloring=\"fill\", showlines=False),\n",
    "        colorscale=\"Blues\",\n",
    "        name=\"MC density\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=xg,\n",
    "        y=yg,\n",
    "        z=Z,\n",
    "        line=dict(color=\"black\"),\n",
    "        contours=dict(showlabels=False),\n",
    "        showscale=False,\n",
    "        name=\"theory pdf\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Bivariate marginal: (X[{i1},{j1}], X[{i2},{j2}])\",\n",
    "    xaxis_title=f\"X[{i1},{j1}]\",\n",
    "    yaxis_title=f\"X[{i2},{j2}]\",\n",
    "    height=380,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration (`scipy.stats.matrix_normal`)\n",
    "\n",
    "SciPy provides a matrix normal distribution:\n",
    "\n",
    "```python\n",
    "rv = scipy.stats.matrix_normal(mean=M, rowcov=U, colcov=V)\n",
    "```\n",
    "\n",
    "Available methods (SciPy 1.15):\n",
    "- `pdf`, `logpdf`\n",
    "- `rvs`\n",
    "- `mean`\n",
    "\n",
    "**Notably missing**\n",
    "- `cdf` (full multivariate CDF is expensive)\n",
    "- `fit` (Kronecker-structured MLE needs an iterative procedure)\n",
    "\n",
    "Workarounds:\n",
    "- For CDF-like quantities, use **univariate/bivariate marginals**, or for very small \\(np\\) use `scipy.stats.multivariate_normal.cdf` on `vec(X)`.\n",
    "- For fitting, use a flip-flop / alternating MLE (implemented above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy usage: pdf / logpdf / rvs\n",
    "n, p = 3, 2\n",
    "M = np.zeros((n, p))\n",
    "U = ar1_cov(n, rho=0.4, sigma2=1.0)\n",
    "V = ar1_cov(p, rho=-0.2, sigma2=0.6)\n",
    "\n",
    "rv = stats.matrix_normal(mean=M, rowcov=U, colcov=V)\n",
    "X = rv.rvs(size=5, random_state=rng)\n",
    "\n",
    "print(\"rvs shape:\", X.shape)\n",
    "print(\"mean shape:\", rv.mean.shape)\n",
    "print(\"logpdf[0] scipy:\", rv.logpdf(X[0]))\n",
    "print(\"logpdf[0] numpy:\", matrix_normal_logpdf_numpy(X[0], M, U, V))\n",
    "\n",
    "# CDF workaround for small np: vectorize and use multivariate_normal.cdf\n",
    "M_small = np.array([[0.0, 0.0]])  # 1x2\n",
    "U_small = np.array([[1.0]])\n",
    "V_small = np.array([[1.0, 0.5], [0.5, 1.0]])\n",
    "\n",
    "x_small = np.array([[0.2, -0.1]])\n",
    "Sigma_small = np.kron(V_small, U_small)\n",
    "\n",
    "cdf_small = multivariate_normal(mean=vec_f(M_small), cov=Sigma_small).cdf(vec_f(x_small))\n",
    "print(\"joint CDF for 1x2 example:\", float(cdf_small))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing\n",
    "With known \\(U,V\\), testing a mean matrix is a multivariate-normal problem.\n",
    "Under \\(H_0: M=M_0\\), for one observation\n",
    "\n",
    "\\[\n",
    "Q = \\mathrm{tr}\\left(U^{-1}(X-M_0)V^{-1}(X-M_0)^\\top\\right)\n",
    "\\]\n",
    "\n",
    "is chi-square: \\(Q \\sim \\chi^2_{np}\\). For \\(m\\) i.i.d. observations,\n",
    "\\(\\sum_{r=1}^m Q_r \\sim \\chi^2_{mnp}\\).\n",
    "\n",
    "### Bayesian modeling\n",
    "Matrix normal is a **conjugate** building block in multivariate regression.\n",
    "A classic model:\n",
    "\n",
    "\\[\n",
    "Y \\mid B,\\Sigma \\sim \\mathrm{MN}(X B,\\, I_n,\\, \\Sigma)\n",
    "\\]\n",
    "\n",
    "with a matrix-normal prior on coefficients \\(B\\mid\\Sigma\\), often paired with an inverse-Wishart prior on \\(\\Sigma\\).\n",
    "\n",
    "### Generative modeling\n",
    "The matrix normal provides a lightweight way to generate correlated random matrices\n",
    "(e.g. noise images or fields) while keeping computations cheap via the Kronecker structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: chi-square test for H0: M = 0 with known (U, V)\n",
    "\n",
    "n, p = 3, 4\n",
    "m = 10\n",
    "M0 = np.zeros((n, p))\n",
    "U = ar1_cov(n, rho=0.4, sigma2=1.0)\n",
    "V = ar1_cov(p, rho=0.2, sigma2=0.7)\n",
    "\n",
    "L_u = chol_spd(U, name=\"U\")\n",
    "L_v = chol_spd(V, name=\"V\")\n",
    "\n",
    "alpha = 0.05\n",
    "crit = chi2.ppf(1 - alpha, df=m * n * p)\n",
    "\n",
    "rejections = 0\n",
    "for _ in range(N_EXPERIMENTS):\n",
    "    X = matrix_normal_rvs_numpy(M0, U, V, size=m, rng=rng)\n",
    "\n",
    "    Q = 0.0\n",
    "    for r in range(m):\n",
    "        A = np.linalg.solve(L_u, X[r] - M0)\n",
    "        B = np.linalg.solve(L_v, A.T)\n",
    "        Q += float(np.sum(B**2))\n",
    "\n",
    "    if Q > crit:\n",
    "        rejections += 1\n",
    "\n",
    "rejections / N_EXPERIMENTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: \\(U\\) and \\(V\\) must be symmetric positive definite.\n",
    "- **Scale non-identifiability**: \\((U,V)\\) and \\((cU, V/c)\\) describe the same covariance \\(V\\otimes U\\).\n",
    "  Impose a constraint (e.g. \\(\\mathrm{tr}(V)=p\\)) when fitting.\n",
    "- **Numerical stability**:\n",
    "  - avoid explicit inverses; use Cholesky solves;\n",
    "  - work with `logpdf` rather than `pdf` when \\(np\\) is moderate/large.\n",
    "- **Kronecker blow-up**: forming \\(V\\otimes U\\) explicitly costs \\(O(n^2 p^2)\\) memory.\n",
    "  Prefer trace / Frobenius formulas and two-sided solves.\n",
    "- **Vectorization convention**: \\(V\\otimes U\\) corresponds to *column-stacking* `vec`.\n",
    "  Mixing row-major flattening with the Kronecker formula is a common source of bugs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- Matrix normal is a **continuous distribution over real matrices** with separable row/column covariance.\n",
    "- \\(X\\sim\\mathrm{MN}(M,U,V)\\) iff \\(\\mathrm{vec}(X)\\sim\\mathcal{N}(\\mathrm{vec}(M), V\\otimes U)\\).\n",
    "- Mean is \\(M\\); entrywise covariance factorizes: \\(\\mathrm{Cov}(X_{ij},X_{k\\ell})=U_{ik}V_{j\\ell}\\).\n",
    "- Sampling is efficient via \\(X=M+L_U Z L_V^\\top\\) with i.i.d. standard-normal \\(Z\\).\n",
    "- SciPy supports `pdf/logpdf/rvs/mean`; full `cdf` and `fit` are not provided, but CDFs for small problems and iterative fitting are straightforward.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
