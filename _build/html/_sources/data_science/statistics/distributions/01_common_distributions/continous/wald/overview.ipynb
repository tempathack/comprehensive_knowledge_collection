{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wald (Inverse Gaussian) distribution (`wald`)\n",
    "\n",
    "The **Wald** distribution is a classic **continuous** model for **positive, right-skewed waiting times**. It is most famous as the **first-passage time** distribution of a **Brownian motion with drift** (a core story in sequential analysis and drift–diffusion models).\n",
    "\n",
    "> In SciPy, `scipy.stats.wald` is the **standardized Wald** (a special case of the inverse Gaussian). In the literature, “Wald distribution” is often used interchangeably with the **inverse Gaussian distribution**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Title & classification\n",
    "\n",
    "| Item | Value |\n",
    "|---|---|\n",
    "| Name | Wald / inverse Gaussian (`wald`) |\n",
    "| Type | Continuous |\n",
    "| Support (standard) | $x \\in (0,\\infty)$ |\n",
    "| Parameter space (inverse Gaussian form) | $(\\mu,\\lambda) \\in (0,\\infty)\\times(0,\\infty)$ |\n",
    "| SciPy `wald` | standardized case $\\mu=1,\\ \\lambda=1$ with optional `loc\\in\\mathbb{R}`, `scale>0` |\n",
    "\n",
    "### What you’ll be able to do after this notebook\n",
    "\n",
    "- recognize when a Wald / inverse Gaussian model makes sense (especially **first-passage times**)\n",
    "- write the PDF/CDF and compute key moments\n",
    "- interpret how $(\\mu,\\lambda)$ change shape (and how SciPy parameterizes them)\n",
    "- derive mean/variance (via the MGF) and write down the log-likelihood\n",
    "- sample from an inverse Gaussian **using NumPy only**\n",
    "- use `scipy.stats.wald` and `scipy.stats.invgauss` for simulation and fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & classification\n",
    "2) Intuition & motivation\n",
    "3) Formal definition (PDF/CDF)\n",
    "4) Moments & properties\n",
    "5) Parameter interpretation\n",
    "6) Derivations ($\\mathbb{E}[X]$, $\\mathrm{Var}(X)$, likelihood)\n",
    "7) Sampling & simulation (NumPy-only)\n",
    "8) Visualization (PDF, CDF, Monte Carlo)\n",
    "9) SciPy integration (`scipy.stats.wald`, `scipy.stats.invgauss`)\n",
    "10) Statistical use cases\n",
    "11) Pitfalls\n",
    "12) Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import log_ndtr\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "SEED = 123\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites & notation\n",
    "\n",
    "**Prerequisites**\n",
    "- comfort with calculus (differentiation, basic integrals)\n",
    "- probability basics (PDF/CDF, expectation, likelihood)\n",
    "- optional: familiarity with Brownian motion / diffusion models\n",
    "\n",
    "**Notation (inverse Gaussian parameterization)**\n",
    "\n",
    "We use parameters $(\\mu,\\lambda)$ with\n",
    "\n",
    "- $\\mu>0$ = **mean** parameter\n",
    "- $\\lambda>0$ = **shape** (sometimes called “scale” in older sources)\n",
    "\n",
    "and write\n",
    "\n",
    "$$X \\sim \\mathrm{IG}(\\mu,\\lambda).$$\n",
    "\n",
    "We also use $\\Phi(\\cdot)$ for the standard normal CDF.\n",
    "\n",
    "**SciPy mapping**\n",
    "- `scipy.stats.wald` is the *standardized* case (no shape parameters):\n",
    "  $$f(x)=\\frac{1}{\\sqrt{2\\pi x^3}}\\exp\\!\\left(-\\frac{(x-1)^2}{2x}\\right),\\quad x>0.$$\n",
    "- For the general inverse Gaussian $\\mathrm{IG}(\\mu,\\lambda)$, SciPy recommends `scipy.stats.invgauss` with\n",
    "  `invgauss(mu=mu/lam, scale=lam, loc=0)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition & motivation\n",
    "\n",
    "### What it models\n",
    "The Wald / inverse Gaussian distribution is a model for a **positive time to reach a threshold** when progress is noisy but has a consistent drift.\n",
    "\n",
    "A canonical story:\n",
    "\n",
    "- Let $B_t$ be standard Brownian motion.\n",
    "- Consider a drift–diffusion process\n",
    "  $$X_t = \\nu t + \\sigma B_t,$$\n",
    "  with drift $\\nu>0$ and diffusion scale $\\sigma>0$.\n",
    "- Define the **first-passage time** to level $a>0$:\n",
    "  $$T = \\inf\\{t\\ge 0 : X_t = a\\}.$$\n",
    "\n",
    "Then $T$ has an inverse Gaussian distribution:\n",
    "\n",
    "$$T \\sim \\mathrm{IG}\\left(\\mu=\\frac{a}{\\nu},\\ \\lambda=\\frac{a^2}{\\sigma^2}\\right).$$\n",
    "\n",
    "So:\n",
    "- larger drift $\\nu$ makes the threshold reached sooner (smaller $\\mu$)\n",
    "- larger noise $\\sigma$ makes the time more variable (smaller $\\lambda$)\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Response times** in cognitive models (drift–diffusion / sequential probability ratio tests)\n",
    "- **Time-to-failure** / fatigue-life models (some engineering contexts)\n",
    "- **Queueing / transport**: positive travel times with asymmetric right tails\n",
    "- **Finance**: first-hitting times of drifted diffusions\n",
    "\n",
    "### Relations to other distributions\n",
    "- The Wald is a special case of the **inverse Gaussian** (`invgauss` in SciPy).\n",
    "- For large $\\lambda$ (small relative variance), $\\mathrm{IG}(\\mu,\\lambda)$ is approximately **Normal**:\n",
    "  $$X \\approx \\mathcal{N}\\!\\left(\\mu,\\ \\frac{\\mu^3}{\\lambda}\\right).$$\n",
    "- The inverse Gaussian is used as a **mixing distribution** in normal variance–mean mixtures, yielding the **normal-inverse-Gaussian** family.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Formal definition\n",
    "\n",
    "### PDF (inverse Gaussian form)\n",
    "For $\\mu>0$, $\\lambda>0$, the inverse Gaussian density is\n",
    "\n",
    "$$f(x;\\mu,\\lambda)\n",
    "=\\sqrt{\\frac{\\lambda}{2\\pi x^3}}\\,\n",
    "\\exp\\!\\left(-\\frac{\\lambda(x-\\mu)^2}{2\\mu^2 x}\\right),\\quad x>0.$$\n",
    "\n",
    "### Wald as a special case\n",
    "The **standardized Wald** distribution corresponds to $\\mu=1$, $\\lambda=1$:\n",
    "\n",
    "$$f(x)=\\frac{1}{\\sqrt{2\\pi x^3}}\\exp\\!\\left(-\\frac{(x-1)^2}{2x}\\right),\\quad x>0.$$\n",
    "\n",
    "### CDF\n",
    "Let $\\Phi$ denote the standard normal CDF. For $x>0$,\n",
    "\n",
    "$$F(x;\\mu,\\lambda)\n",
    "= \\Phi\\!\\left(\\sqrt{\\frac{\\lambda}{x}}\\left(\\frac{x}{\\mu}-1\\right)\\right)\n",
    "+ \\exp\\!\\left(\\frac{2\\lambda}{\\mu}\\right)\n",
    "\\Phi\\!\\left(-\\sqrt{\\frac{\\lambda}{x}}\\left(\\frac{x}{\\mu}+1\\right)\\right).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invgauss_logpdf(x, mu, lam):\n",
    "    \"\"\"Log-PDF of IG(mu, lam) on x>0 (mu>0, lam>0).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu = float(mu)\n",
    "    lam = float(lam)\n",
    "\n",
    "    if mu <= 0 or lam <= 0:\n",
    "        return np.full_like(x, -np.inf, dtype=float)\n",
    "\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "    out[mask] = (\n",
    "        0.5 * (np.log(lam) - np.log(2.0 * np.pi))\n",
    "        - 1.5 * np.log(xm)\n",
    "        - (lam * (xm - mu) ** 2) / (2.0 * mu**2 * xm)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def invgauss_pdf(x, mu, lam):\n",
    "    return np.exp(invgauss_logpdf(x, mu, lam))\n",
    "\n",
    "\n",
    "def invgauss_cdf(x, mu, lam):\n",
    "    \"\"\"CDF of IG(mu, lam) using a numerically-stable log-space form.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu = float(mu)\n",
    "    lam = float(lam)\n",
    "\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    if mu <= 0 or lam <= 0:\n",
    "        out[:] = np.nan\n",
    "        return out\n",
    "\n",
    "    mask = x > 0\n",
    "    xm = x[mask]\n",
    "    z1 = np.sqrt(lam / xm) * (xm / mu - 1.0)\n",
    "    z2 = -np.sqrt(lam / xm) * (xm / mu + 1.0)\n",
    "\n",
    "    # F = Phi(z1) + exp(2*lam/mu) * Phi(z2)\n",
    "    log_term1 = log_ndtr(z1)\n",
    "    log_term2 = (2.0 * lam / mu) + log_ndtr(z2)\n",
    "    out[mask] = np.exp(np.logaddexp(log_term1, log_term2))\n",
    "    return out\n",
    "\n",
    "\n",
    "def wald_pdf(x):\n",
    "    return invgauss_pdf(x, mu=1.0, lam=1.0)\n",
    "\n",
    "\n",
    "def wald_cdf(x):\n",
    "    return invgauss_cdf(x, mu=1.0, lam=1.0)\n",
    "\n",
    "\n",
    "# Quick checks vs SciPy\n",
    "xs = np.linspace(0.05, 6.0, 7)\n",
    "print(\"wald pdf match:\", np.allclose(wald_pdf(xs), stats.wald.pdf(xs)))\n",
    "print(\"wald cdf match:\", np.allclose(wald_cdf(xs), stats.wald.cdf(xs)))\n",
    "\n",
    "# General IG(mu, lam) via scipy.stats.invgauss(mu/lam, scale=lam)\n",
    "mu_test, lam_test = 1.7, 4.2\n",
    "rv_ig = stats.invgauss(mu_test / lam_test, scale=lam_test)\n",
    "print(\"IG pdf match:\", np.allclose(invgauss_pdf(xs, mu_test, lam_test), rv_ig.pdf(xs)))\n",
    "print(\"IG cdf match:\", np.allclose(invgauss_cdf(xs, mu_test, lam_test), rv_ig.cdf(xs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Moments & properties\n",
    "\n",
    "For $X\\sim\\mathrm{IG}(\\mu,\\lambda)$:\n",
    "\n",
    "| Quantity | Value |\n",
    "|---|---|\n",
    "| Mean | $\\mathbb{E}[X] = \\mu$ |\n",
    "| Variance | $\\mathrm{Var}(X)=\\dfrac{\\mu^3}{\\lambda}$ |\n",
    "| Skewness | $\\gamma_1 = 3\\sqrt{\\dfrac{\\mu}{\\lambda}}$ |\n",
    "| (Excess) kurtosis | $\\gamma_2 = 15\\dfrac{\\mu}{\\lambda}$ (so kurtosis $= 3+\\gamma_2$) |\n",
    "\n",
    "A useful scale-free summary is the **coefficient of variation**:\n",
    "\n",
    "$$\\mathrm{CV} = \\frac{\\sqrt{\\mathrm{Var}(X)}}{\\mathbb{E}[X]} = \\sqrt{\\frac{\\mu}{\\lambda}}.$$\n",
    "\n",
    "### MGF and characteristic function\n",
    "For $t < \\lambda/(2\\mu^2)$, the **MGF** is\n",
    "\n",
    "$$M_X(t)=\\mathbb{E}[e^{tX}] = \\exp\\!\\left(\\frac{\\lambda}{\\mu}\\left(1-\\sqrt{1-\\frac{2\\mu^2 t}{\\lambda}}\\right)\\right).$$\n",
    "\n",
    "The **characteristic function** follows by substituting $t\\mapsto it$.\n",
    "\n",
    "### Entropy\n",
    "The differential entropy is\n",
    "\n",
    "$$h(X) = -\\int_0^{\\infty} f(x;\\mu,\\lambda)\\,\\log f(x;\\mu,\\lambda)\\,dx,$$\n",
    "\n",
    "which does not simplify to a short elementary closed form. In practice, compute it numerically (SciPy provides `.entropy()`), or estimate it by Monte Carlo via $-\\mathbb{E}[\\log f(X)]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, lam = 1.5, 3.0\n",
    "\n",
    "# Theory\n",
    "mean_th = mu\n",
    "var_th = mu**3 / lam\n",
    "skew_th = 3.0 * math.sqrt(mu / lam)\n",
    "exkurt_th = 15.0 * (mu / lam)\n",
    "\n",
    "print(\"theory mean,var,skew,exkurt:\", (mean_th, var_th, skew_th, exkurt_th))\n",
    "\n",
    "# SciPy check (invgauss(mu/lam, scale=lam) corresponds to IG(mu, lam))\n",
    "rv = stats.invgauss(mu / lam, scale=lam)\n",
    "mean_sp, var_sp, skew_sp, exkurt_sp = rv.stats(moments=\"mvsk\")\n",
    "print(\"scipy  mean,var,skew,exkurt:\", (float(mean_sp), float(var_sp), float(skew_sp), float(exkurt_sp)))\n",
    "\n",
    "# Entropy: SciPy vs Monte Carlo estimate\n",
    "h_scipy = float(rv.entropy())\n",
    "samples = rv.rvs(size=80_000, random_state=rng)\n",
    "h_mc = float(-np.mean(invgauss_logpdf(samples, mu, lam)))\n",
    "print(\"entropy scipy:\", h_scipy)\n",
    "print(\"entropy MC   :\", h_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Parameter interpretation\n",
    "\n",
    "### Meaning of parameters\n",
    "- $\\mu$ sets the **typical scale**: it is the **mean** and (often) close to the mode when the distribution is concentrated.\n",
    "- $\\lambda$ controls **concentration** around $\\mu$:\n",
    "  $$\\mathrm{Var}(X)=\\frac{\\mu^3}{\\lambda} \\quad\\Rightarrow\\quad \\lambda\\uparrow \\;\\Longrightarrow\\; \\text{smaller variance and less skew.}$$\n",
    "\n",
    "A helpful way to think about shape is via the ratio $\\mu/\\lambda$:\n",
    "- relative spread: $\\mathrm{CV}=\\sqrt{\\mu/\\lambda}$\n",
    "- skewness: $3\\sqrt{\\mu/\\lambda}$\n",
    "\n",
    "### Shape changes\n",
    "- Fix $\\mu$ and increase $\\lambda$: the density becomes sharply peaked near $\\mu$ and looks increasingly Normal.\n",
    "- Fix $\\lambda$ and increase $\\mu$: the mean shifts right and dispersion grows quickly (since variance scales like $\\mu^3$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF: varying mu (keep lambda fixed)\n",
    "lam_fixed = 3.0\n",
    "mus = [0.6, 1.0, 1.8, 3.0]\n",
    "\n",
    "x_max = max(stats.invgauss(mu / lam_fixed, scale=lam_fixed).ppf(0.995) for mu in mus)\n",
    "x = np.linspace(1e-6, float(x_max), 900)\n",
    "\n",
    "fig = go.Figure()\n",
    "for mu_i in mus:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=invgauss_pdf(x, mu_i, lam_fixed),\n",
    "            mode=\"lines\",\n",
    "            name=f\"μ={mu_i}, λ={lam_fixed}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Inverse Gaussian PDF: varying μ (λ fixed)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"pdf\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF: varying lambda (keep mu fixed)\n",
    "mu_fixed = 1.2\n",
    "lams = [0.5, 1.0, 3.0, 10.0]\n",
    "\n",
    "x_max = max(stats.invgauss(mu_fixed / lam, scale=lam).ppf(0.995) for lam in lams)\n",
    "x = np.linspace(1e-6, float(x_max), 900)\n",
    "\n",
    "fig = go.Figure()\n",
    "for lam_i in lams:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=invgauss_pdf(x, mu_fixed, lam_i),\n",
    "            mode=\"lines\",\n",
    "            name=f\"μ={mu_fixed}, λ={lam_i}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Inverse Gaussian PDF: varying λ (μ fixed)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"pdf\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation and variance (via cumulant generating function)\n",
    "Let $K(t)=\\log M_X(t)$ where\n",
    "\n",
    "$$K(t)=\\frac{\\lambda}{\\mu}\\left(1-\\sqrt{1-\\frac{2\\mu^2 t}{\\lambda}}\\right).$$\n",
    "\n",
    "Cumulants are derivatives at $t=0$:\n",
    "- $K'(0)=\\mathbb{E}[X]$\n",
    "- $K''(0)=\\mathrm{Var}(X)$\n",
    "\n",
    "Differentiate:\n",
    "\n",
    "$$K'(t) = \\frac{\\lambda}{\\mu}\\cdot \\frac{\\mu^2/\\lambda}{\\sqrt{1-\\frac{2\\mu^2 t}{\\lambda}}}\n",
    "= \\frac{\\mu}{\\sqrt{1-\\frac{2\\mu^2 t}{\\lambda}}} \\quad\\Rightarrow\\quad K'(0)=\\mu.$$\n",
    "\n",
    "Differentiate again:\n",
    "\n",
    "$$K''(t)=\\mu\\cdot \\frac{\\mu^2/\\lambda}{\\left(1-\\frac{2\\mu^2 t}{\\lambda}\\right)^{3/2}}\n",
    "= \\frac{\\mu^3/\\lambda}{\\left(1-\\frac{2\\mu^2 t}{\\lambda}\\right)^{3/2}} \\quad\\Rightarrow\\quad K''(0)=\\frac{\\mu^3}{\\lambda}.$$\n",
    "\n",
    "### Likelihood\n",
    "For i.i.d. data $x_1,\\dots,x_n$ from $\\mathrm{IG}(\\mu,\\lambda)$, the log-likelihood is\n",
    "\n",
    "$$\\ell(\\mu,\\lambda) = \\sum_{i=1}^n \\log f(x_i;\\mu,\\lambda)$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\\log f(x;\\mu,\\lambda)\n",
    "= \\tfrac12\\log\\lambda - \\tfrac12\\log(2\\pi) - \\tfrac32\\log x\n",
    "- \\frac{\\lambda(x-\\mu)^2}{2\\mu^2 x}.$$\n",
    "\n",
    "A well-known MLE result (for the inverse Gaussian form) is:\n",
    "\n",
    "$$\\hat\\mu = \\bar x,\\qquad \\hat\\lambda = \\frac{n}{\\sum_{i=1}^n \\frac{(x_i-\\bar x)^2}{\\bar x^2 x_i}}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invgauss_loglik(x, mu, lam):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return float(np.sum(invgauss_logpdf(x, mu, lam)))\n",
    "\n",
    "\n",
    "def invgauss_mle(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if np.any(x <= 0):\n",
    "        raise ValueError(\"All observations must be > 0 for IG(mu, lam).\")\n",
    "    n = x.size\n",
    "    mu_hat = float(np.mean(x))\n",
    "    denom = float(np.sum((x - mu_hat) ** 2 / (mu_hat**2 * x)))\n",
    "    lam_hat = float(n / denom)\n",
    "    return mu_hat, lam_hat\n",
    "\n",
    "\n",
    "def invgauss_mle_lam_given_mu(x, mu_fixed):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu_fixed = float(mu_fixed)\n",
    "    if mu_fixed <= 0:\n",
    "        raise ValueError(\"mu_fixed must be > 0\")\n",
    "    if np.any(x <= 0):\n",
    "        raise ValueError(\"All observations must be > 0\")\n",
    "    n = x.size\n",
    "    return float(n * mu_fixed**2 / np.sum((x - mu_fixed) ** 2 / x))\n",
    "\n",
    "\n",
    "# Demonstration on synthetic data\n",
    "mu_true, lam_true = 1.7, 4.0\n",
    "rv_true = stats.invgauss(mu_true / lam_true, scale=lam_true)\n",
    "x = rv_true.rvs(size=3000, random_state=rng)\n",
    "\n",
    "mu_hat, lam_hat = invgauss_mle(x)\n",
    "print(\"true (mu, lam):\", (mu_true, lam_true))\n",
    "print(\"mle  (mu, lam):\", (mu_hat, lam_hat))\n",
    "print(\"loglik at true:\", invgauss_loglik(x, mu_true, lam_true))\n",
    "print(\"loglik at mle :\", invgauss_loglik(x, mu_hat, lam_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Sampling & simulation\n",
    "\n",
    "### NumPy-only sampler (Michael–Schucany–Haas)\n",
    "A standard exact algorithm to sample $X\\sim\\mathrm{IG}(\\mu,\\lambda)$ uses only Normal and Uniform random variables:\n",
    "\n",
    "1) Draw $V\\sim\\mathcal{N}(0,1)$ and set $Y=V^2$.\n",
    "2) Form a candidate\n",
    "   $$X_1 = \\mu + \\frac{\\mu^2 Y}{2\\lambda} - \\frac{\\mu}{2\\lambda}\\sqrt{4\\mu\\lambda Y + \\mu^2 Y^2}.$$\n",
    "3) Draw $U\\sim\\mathrm{Unif}(0,1)$.\n",
    "4) Return $X=X_1$ if $U\\le \\frac{\\mu}{\\mu+X_1}$, else return $X=\\frac{\\mu^2}{X_1}$.\n",
    "\n",
    "This produces exact inverse Gaussian samples and is fast and vectorizable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invgauss_rvs_numpy(mu, lam, size=1, rng=None):\n",
    "    \"\"\"Sample IG(mu, lam) using NumPy only (Michael–Schucany–Haas).\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    mu = float(mu)\n",
    "    lam = float(lam)\n",
    "    if mu <= 0 or lam <= 0:\n",
    "        raise ValueError(\"mu and lam must be > 0\")\n",
    "\n",
    "    v = rng.standard_normal(size)\n",
    "    y = v * v\n",
    "\n",
    "    mu2 = mu * mu\n",
    "    x1 = mu + (mu2 * y) / (2.0 * lam) - (mu / (2.0 * lam)) * np.sqrt(4.0 * mu * lam * y + mu2 * y * y)\n",
    "\n",
    "    u = rng.random(size)\n",
    "    x = np.where(u <= (mu / (mu + x1)), x1, mu2 / x1)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Quick validation of the sampler\n",
    "mu, lam = 1.5, 3.0\n",
    "n = 80_000\n",
    "s = invgauss_rvs_numpy(mu, lam, size=n, rng=rng)\n",
    "\n",
    "print(\"sample mean/var:\", (float(np.mean(s)), float(np.var(s, ddof=0))))\n",
    "print(\"theory  mean/var:\", (mu, mu**3 / lam))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the PDF vs a histogram of Monte Carlo samples\n",
    "- the CDF vs the empirical CDF from samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, lam = 1.3, 2.5\n",
    "rv = stats.invgauss(mu / lam, scale=lam)\n",
    "\n",
    "samples = invgauss_rvs_numpy(mu, lam, size=60_000, rng=rng)\n",
    "x_max = float(rv.ppf(0.995))\n",
    "x = np.linspace(1e-6, x_max, 900)\n",
    "\n",
    "# PDF + histogram\n",
    "fig = px.histogram(\n",
    "    x=samples,\n",
    "    nbins=120,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"IG(μ, λ): histogram vs theoretical PDF\",\n",
    "    labels={\"x\": \"x\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x, y=invgauss_pdf(x, mu, lam), mode=\"lines\", name=\"theory pdf\"))\n",
    "fig.update_layout(bargap=0.02)\n",
    "fig.show()\n",
    "\n",
    "# CDF + empirical CDF\n",
    "xs = np.sort(samples)\n",
    "ecdf = np.arange(1, xs.size + 1) / xs.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=invgauss_cdf(x, mu, lam), mode=\"lines\", name=\"theory cdf\"))\n",
    "fig.add_trace(go.Scatter(x=xs[::50], y=ecdf[::50], mode=\"markers\", name=\"empirical cdf\", opacity=0.6))\n",
    "fig.update_layout(\n",
    "    title=\"IG(μ, λ): empirical CDF vs theoretical CDF\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"cdf\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) SciPy integration\n",
    "\n",
    "### `scipy.stats.wald`\n",
    "- standardized Wald distribution (no shape parameters)\n",
    "- supports generic `loc` and `scale` transforms\n",
    "\n",
    "### `scipy.stats.invgauss`\n",
    "For the full inverse Gaussian $\\mathrm{IG}(\\mu,\\lambda)$, use:\n",
    "\n",
    "```python\n",
    "rv = scipy.stats.invgauss(mu / lam, scale=lam, loc=0)\n",
    "```\n",
    "\n",
    "This matches the $(\\mu,\\lambda)$ parameterization used throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wald: pdf/cdf/rvs\n",
    "x = np.linspace(1e-4, 6, 600)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.wald.pdf(x), mode=\"lines\", name=\"wald pdf\"))\n",
    "fig.update_layout(title=\"scipy.stats.wald PDF (standardized)\", xaxis_title=\"x\", yaxis_title=\"pdf\")\n",
    "fig.show()\n",
    "\n",
    "s_wald = stats.wald.rvs(size=10_000, random_state=rng)\n",
    "print(\"wald sample mean/var:\", (float(np.mean(s_wald)), float(np.var(s_wald))))\n",
    "\n",
    "# wald.fit fits loc/scale (note: loc/scale are generic, not part of the classic (mu, lam) parameterization)\n",
    "loc_true, scale_true = 0.2, 1.5\n",
    "data = stats.wald.rvs(loc=loc_true, scale=scale_true, size=5000, random_state=rng)\n",
    "loc_hat, scale_hat = stats.wald.fit(data)\n",
    "print(\"wald true loc,scale:\", (loc_true, scale_true))\n",
    "print(\"wald fit  loc,scale:\", (float(loc_hat), float(scale_hat)))\n",
    "\n",
    "# Inverse Gaussian (mu, lam) via invgauss\n",
    "mu_true, lam_true = 1.7, 4.0\n",
    "rv_ig = stats.invgauss(mu_true / lam_true, scale=lam_true)\n",
    "data = rv_ig.rvs(size=5000, random_state=rng)\n",
    "\n",
    "# Fit invgauss; fix loc=0 to match the (mu, lam) form.\n",
    "mu_shape_hat, loc_hat, scale_hat = stats.invgauss.fit(data, floc=0)\n",
    "mu_hat = float(mu_shape_hat * scale_hat)\n",
    "lam_hat = float(scale_hat)\n",
    "print(\"IG true (mu, lam):\", (mu_true, lam_true))\n",
    "print(\"IG fit  (mu, lam):\", (mu_hat, lam_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Statistical use cases\n",
    "\n",
    "### A) Hypothesis testing (likelihood ratio test for $\\mu$)\n",
    "A common inferential question in first-passage-time models is whether the **mean time** $\\mu$ equals a value implied by a hypothesized drift.\n",
    "\n",
    "We can test\n",
    "$$H_0: \\mu = \\mu_0 \\quad\\text{vs}\\quad H_1: \\mu \\ne \\mu_0$$\n",
    "using the likelihood ratio statistic\n",
    "$$\\Lambda = 2\\left(\\ell(\\hat\\mu,\\hat\\lambda) - \\ell(\\mu_0, \\hat\\lambda_{\\mu_0})\\right),$$\n",
    "which is approximately $\\chi^2_1$ for large $n$.\n",
    "\n",
    "### B) Bayesian modeling\n",
    "Inverse Gaussian likelihoods are common for **positive time** data. Even without closed-form conjugacy, you can do simple grid-based Bayes or use modern samplers (PyMC/Stan) for hierarchical models.\n",
    "\n",
    "### C) Generative modeling\n",
    "The inverse Gaussian is a standard **mixing distribution**: if $V\\sim\\mathrm{IG}$ and\n",
    "$$Y\\mid V\\sim\\mathcal{N}(\\beta V,\\ V),$$\n",
    "then $Y$ has heavier tails than a Gaussian (this leads to the normal-inverse-Gaussian family).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) LRT example for mu\n",
    "mu_true, lam_true = 1.4, 3.5\n",
    "rv = stats.invgauss(mu_true / lam_true, scale=lam_true)\n",
    "x = rv.rvs(size=3000, random_state=rng)\n",
    "\n",
    "mu0 = 1.2\n",
    "\n",
    "mu_hat, lam_hat = invgauss_mle(x)\n",
    "lam_hat_mu0 = invgauss_mle_lam_given_mu(x, mu0)\n",
    "\n",
    "ll_alt = invgauss_loglik(x, mu_hat, lam_hat)\n",
    "ll_null = invgauss_loglik(x, mu0, lam_hat_mu0)\n",
    "\n",
    "lrt = 2.0 * (ll_alt - ll_null)\n",
    "p_value = float(stats.chi2.sf(lrt, df=1))\n",
    "\n",
    "print(\"true (mu, lam):\", (mu_true, lam_true))\n",
    "print(\"H0 mu0:\", mu0)\n",
    "print(\"MLE (mu, lam):\", (mu_hat, lam_hat))\n",
    "print(\"LRT statistic:\", float(lrt))\n",
    "print(\"approx p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Simple grid Bayesian posterior for mu (treat lambda as known)\n",
    "mu_true, lam_known = 1.6, 4.0\n",
    "x = stats.invgauss(mu_true / lam_known, scale=lam_known).rvs(size=400, random_state=rng)\n",
    "\n",
    "# Prior: log-normal on mu (mean roughly around 1.5)\n",
    "prior = stats.lognorm(s=0.35, scale=np.exp(np.log(1.5)))\n",
    "\n",
    "mu_grid = np.linspace(0.4, 3.2, 800)\n",
    "log_prior = prior.logpdf(mu_grid)\n",
    "log_like = np.array([invgauss_loglik(x, mu, lam_known) for mu in mu_grid])\n",
    "log_post_unnorm = log_prior + log_like\n",
    "\n",
    "# Normalize on the grid (log-sum-exp)\n",
    "log_post = log_post_unnorm - scipy.special.logsumexp(log_post_unnorm)\n",
    "post = np.exp(log_post)\n",
    "\n",
    "post_mean = float(np.sum(mu_grid * post))\n",
    "cdf_post = np.cumsum(post)\n",
    "cdf_post /= cdf_post[-1]\n",
    "ci_low = float(mu_grid[np.searchsorted(cdf_post, 0.025)])\n",
    "ci_high = float(mu_grid[np.searchsorted(cdf_post, 0.975)])\n",
    "\n",
    "print(\"true mu:\", mu_true)\n",
    "print(\"posterior mean:\", post_mean)\n",
    "print(\"95% credible interval:\", (ci_low, ci_high))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mu_grid, y=post, mode=\"lines\", name=\"posterior\"))\n",
    "fig.add_vline(x=mu_true, line_dash=\"dash\", line_color=\"black\")\n",
    "fig.update_layout(title=\"Posterior for μ (λ known)\", xaxis_title=\"μ\", yaxis_title=\"posterior density (grid)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Generative modeling: normal mean–variance mixture with IG mixing\n",
    "n = 80_000\n",
    "mu_v, lam_v = 1.0, 1.5\n",
    "beta = 0.6\n",
    "\n",
    "v = invgauss_rvs_numpy(mu_v, lam_v, size=n, rng=rng)\n",
    "z = rng.standard_normal(n)\n",
    "\n",
    "# Y | V=v ~ Normal(beta*v, v)\n",
    "y = beta * v + np.sqrt(v) * z\n",
    "\n",
    "mean_y = float(np.mean(y))\n",
    "std_y = float(np.std(y))\n",
    "print(\"mixture sample mean/std:\", (mean_y, std_y))\n",
    "\n",
    "# Compare to a Gaussian with the same mean/std\n",
    "x = np.linspace(mean_y - 5 * std_y, mean_y + 5 * std_y, 800)\n",
    "fig = px.histogram(y, nbins=160, histnorm=\"probability density\", title=\"IG mixing yields heavier tails than Gaussian\")\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.norm.pdf(x, loc=mean_y, scale=std_y), mode=\"lines\", name=\"matched Normal\"))\n",
    "fig.update_layout(bargap=0.02)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "1) **Parameterization confusion**\n",
    "- The literature often uses $(\\mu,\\lambda)$ (mean + shape).\n",
    "- SciPy uses `wald` for a standardized special case, and `invgauss(mu, scale)` for the general case.\n",
    "- To represent $\\mathrm{IG}(\\mu,\\lambda)$ in SciPy, use `invgauss(mu/lam, scale=lam, loc=0)`.\n",
    "\n",
    "2) **Invalid parameters / support**\n",
    "- Require $\\mu>0$, $\\lambda>0$, and data $x_i>0$.\n",
    "- Using `loc` shifts the support to $(\\mathrm{loc},\\infty)$; fitting `loc` freely can yield surprising results.\n",
    "\n",
    "3) **Numerical issues**\n",
    "- For small $x$ or large $\\lambda/\\mu$, PDF/CDF computations can underflow/overflow.\n",
    "- Prefer `logpdf`, and for the CDF use stable forms (e.g. via `log_ndtr` as above).\n",
    "\n",
    "4) **Goodness-of-fit p-values after fitting**\n",
    "- If you estimate parameters from the same data, naive KS-test p-values are not calibrated (use bootstrap if you need a valid GOF p-value).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- The Wald / inverse Gaussian is a **continuous** distribution on $(0,\\infty)$ with mean $\\mu$ and shape $\\lambda$.\n",
    "- It naturally models **first-passage times** of drifted Brownian motion: $\\mu=a/\\nu$ and $\\lambda=a^2/\\sigma^2$.\n",
    "- Key moments: $\\mathbb{E}[X]=\\mu$, $\\mathrm{Var}(X)=\\mu^3/\\lambda$, skewness $=3\\sqrt{\\mu/\\lambda}$.\n",
    "- `scipy.stats.wald` is a standardized special case; for general $(\\mu,\\lambda)$ use `scipy.stats.invgauss(mu/lam, scale=lam)`.\n",
    "- Exact sampling is easy with a fast NumPy-only algorithm (Michael–Schucany–Haas).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
