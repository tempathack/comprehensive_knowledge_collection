{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a8a874",
   "metadata": {},
   "source": [
    "# `gausshyper` (Gauss hypergeometric) distribution\n",
    "\n",
    "A flexible four-parameter distribution on the unit interval \\([0,1]\\). In SciPy it lives at `scipy.stats.gausshyper`.\n",
    "\n",
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `gausshyper` (Gauss hypergeometric distribution)\n",
    "- **Type**: **continuous**\n",
    "- **Support**: \\(x \\in (0,1)\\) (endpoints have zero probability mass; the density may diverge at 0 or 1 depending on \\(a,b\\))\n",
    "- **Parameter space (SciPy convention)**: \\(a>0,\\ b>0,\\ c\\in\\mathbb{R},\\ z>-1\\)\n",
    "- **SciPy signature**: `scipy.stats.gausshyper(a, b, c, z, loc=0, scale=1)`\n",
    "\n",
    "### Learning goals\n",
    "- understand how `gausshyper` generalizes a Beta distribution via a *tilt* factor\n",
    "- recognize where the Gauss hypergeometric function \\({}_2F_1\\) enters (normalization + moments)\n",
    "- derive mean/variance from the general raw-moment expression\n",
    "- implement a **NumPy-only** sampler (rejection sampling)\n",
    "- use SciPy for `pdf`, `cdf`, `rvs`, and `fit`\n",
    "\n",
    "### Table of contents\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3244f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy import special\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import gausshyper\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "print(\"python\", __import__(\"sys\").version.split()[0])\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "import plotly\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489da5da",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "A helpful way to view `gausshyper` is as a **Beta(a,b)** density multiplied by a *tilt*:\n",
    "\n",
    "\\[\n",
    " f(x) \\propto \\underbrace{x^{a-1}(1-x)^{b-1}}_{\\text{Beta kernel}}\\;\\underbrace{(1+z x)^{-c}}_{\\text{tilt}},\\qquad x\\in(0,1).\n",
    "\\]\n",
    "\n",
    "- The Beta kernel controls how the density behaves near 0 and 1.\n",
    "- The factor \\((1+z x)^{-c}\\) reweights the interior:\n",
    "  - for \\(z>0\\), \\(c>0\\): \\((1+z x)^{-c}\\) decreases in \\(x\\) → shifts mass toward 0\n",
    "  - for \\(z>0\\), \\(c<0\\): \\((1+z x)^{-c}\\) increases in \\(x\\) → shifts mass toward 1\n",
    "  - for \\(-1<z<0\\), the direction flips because \\(1+z x\\) decreases with \\(x\\).\n",
    "\n",
    "### What this distribution models\n",
    "A random **probability / proportion** in \\([0,1]\\) when a plain Beta distribution is close but not flexible enough—especially if you want an extra “tilt” across the interval *without* changing the endpoint exponents \\(a-1\\) and \\(b-1\\).\n",
    "\n",
    "### Typical real-world use cases\n",
    "- Bayesian priors for probabilities (conversion rates, reliabilities, mixture weights) with extra curvature beyond Beta.\n",
    "- Prior elicitation in Bayesian queueing models (Armero & Bayarri, 1994; cited by SciPy) where the tilt term arises naturally.\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Beta(a,b)** is a special case: if \\(c=0\\) (or \\(z=0\\)), then \\((1+z x)^{-c}=1\\) and `gausshyper` reduces to Beta.\n",
    "- Related in spirit to other *generalized Beta* families that multiply the Beta kernel by an extra factor (e.g., exponential tilts like the Kummer beta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362f82d",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### PDF\n",
    "For \\(x\\in(0,1)\\), the probability density function is\n",
    "\n",
    "$$\n",
    " f(x; a,b,c,z) = \\frac{1}{B(a,b)\\,{}_2F_1(c,a;\\,a+b;\\,-z)}\\;x^{a-1}(1-x)^{b-1}(1+z x)^{-c},\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\) is the Beta function,\n",
    "- \\({}_2F_1\\) is the Gauss hypergeometric function.\n",
    "\n",
    "**Parameter space (SciPy convention)**:\n",
    "\\(a>0\\), \\(b>0\\), \\(c\\in\\mathbb{R}\\), \\(z>-1\\).\n",
    "\n",
    "The constraint \\(z>-1\\) guarantees \\(1+z x>0\\) on \\([0,1]\\).\n",
    "\n",
    "### CDF\n",
    "A standard way to write the CDF is as the defining integral\n",
    "\n",
    "$$\n",
    "F(x) = \\int_0^x f(t; a,b,c,z)\\,dt.\n",
    "$$\n",
    "\n",
    "SciPy evaluates this numerically (`gausshyper.cdf`). For plotting, a fast approximation is to integrate the PDF on a fine grid (trapezoidal rule).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gausshyper_norm(a: float, b: float, c: float, z: float) -> float:\n",
    "    # Normalization integral: ∫_0^1 x^{a-1}(1-x)^{b-1}(1+zx)^{-c} dx\n",
    "    return special.beta(a, b) * special.hyp2f1(c, a, a + b, -z)\n",
    "\n",
    "\n",
    "def gausshyper_pdf_formula(x: np.ndarray, a: float, b: float, c: float, z: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    norm = gausshyper_norm(a, b, c, z)\n",
    "    return (x ** (a - 1.0) * (1.0 - x) ** (b - 1.0) / (1.0 + z * x) ** c) / norm\n",
    "\n",
    "\n",
    "# Sanity check: formula vs SciPy\n",
    "params = dict(a=2.0, b=5.0, c=3.0, z=2.0)\n",
    "xs = np.linspace(1e-6, 1 - 1e-6, 400)\n",
    "err = np.max(np.abs(gausshyper.pdf(xs, **params) - gausshyper_pdf_formula(xs, **params)))\n",
    "err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a23a7",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "A clean way to work with moments is via *raw moments* \\(m_n = \\mathbb{E}[X^n]\\).\n",
    "\n",
    "### Raw moments\n",
    "Using the same integral identity that normalizes the PDF, one can show (for \\(n>-a\\)):\n",
    "\n",
    "$$\n",
    " m_n = \\mathbb{E}[X^n]\n",
    " = \\frac{B(a+n,b)}{B(a,b)}\\;\\frac{{}_2F_1(c, a+n;\\,a+b+n;\\,-z)}{{}_2F_1(c, a;\\,a+b;\\,-z)}.\n",
    "$$\n",
    "\n",
    "In particular:\n",
    "- **Mean**: \\(\\mu = m_1\\)\n",
    "- **Variance**: \\(\\sigma^2 = m_2 - m_1^2\\)\n",
    "\n",
    "### Skewness and kurtosis\n",
    "From raw moments \\(m_1,\\dots,m_4\\):\n",
    "\n",
    "\\[\n",
    "\\mu_2 = m_2 - m_1^2,\n",
    "\\quad \\mu_3 = m_3 - 3 m_1 m_2 + 2 m_1^3,\n",
    "\\quad \\mu_4 = m_4 - 4 m_1 m_3 + 6 m_1^2 m_2 - 3 m_1^4.\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{skewness} = \\frac{\\mu_3}{\\mu_2^{3/2}},\n",
    "\\qquad \\text{kurtosis} = \\frac{\\mu_4}{\\mu_2^{2}},\n",
    "\\qquad \\text{excess kurtosis} = \\frac{\\mu_4}{\\mu_2^{2}} - 3.\n",
    "\\]\n",
    "\n",
    "SciPy computes \\((\\mu,\\sigma^2,\\text{skew},\\text{kurtosis})\\) via `gausshyper.stats(..., moments=\"mvsk\")`.\n",
    "\n",
    "### MGF / characteristic function\n",
    "Because \\(X\\in[0,1]\\), the MGF exists for all real \\(t\\):\n",
    "\n",
    "\\[\n",
    "M_X(t)=\\mathbb{E}[e^{tX}] = \\int_0^1 e^{tx} f(x)\\,dx.\n",
    "\\]\n",
    "\n",
    "A practical representation is the power series in terms of raw moments:\n",
    "\n",
    "\\[\n",
    "M_X(t) = \\sum_{n=0}^{\\infty} m_n\\,\\frac{t^n}{n!}.\n",
    "\\]\n",
    "\n",
    "The characteristic function is \\(\\varphi_X(t)=M_X(it)\\).\n",
    "\n",
    "### Entropy\n",
    "The differential entropy is\n",
    "\n",
    "\\[\n",
    "H(X) = -\\int_0^1 f(x)\\log f(x)\\,dx,\n",
    "\\]\n",
    "\n",
    "and is typically evaluated numerically (quadrature or Monte Carlo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_moment(n: int, a: float, b: float, c: float, z: float) -> float:\n",
    "    num = special.beta(a + n, b) * special.hyp2f1(c, a + n, a + b + n, -z)\n",
    "    den = special.beta(a, b) * special.hyp2f1(c, a, a + b, -z)\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "a, b, c, z = params[\"a\"], params[\"b\"], params[\"c\"], params[\"z\"]\n",
    "\n",
    "m1 = raw_moment(1, a, b, c, z)\n",
    "m2 = raw_moment(2, a, b, c, z)\n",
    "m3 = raw_moment(3, a, b, c, z)\n",
    "m4 = raw_moment(4, a, b, c, z)\n",
    "\n",
    "var = m2 - m1**2\n",
    "mu2 = var\n",
    "mu3 = m3 - 3 * m1 * m2 + 2 * m1**3\n",
    "mu4 = m4 - 4 * m1 * m3 + 6 * (m1**2) * m2 - 3 * m1**4\n",
    "\n",
    "skew = mu3 / mu2**1.5\n",
    "kurt = mu4 / mu2**2\n",
    "excess_kurt = kurt - 3\n",
    "\n",
    "print(\"mean (moment)     \", m1)\n",
    "print(\"var  (moment)     \", var)\n",
    "print(\"skewness (moment) \", skew)\n",
    "print(\"excess kurt (mom) \", excess_kurt)\n",
    "\n",
    "mean_s, var_s, skew_s, kurt_s = gausshyper.stats(a, b, c, z, moments=\"mvsk\")\n",
    "print(\"\\nSciPy stats(mvsk)\")\n",
    "print(\"mean\", float(mean_s))\n",
    "print(\"var \", float(var_s))\n",
    "print(\"skew\", float(skew_s))\n",
    "print(\"kurt\", float(kurt_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c41fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical entropy estimate (trapezoidal integration)\n",
    "xs = np.linspace(1e-6, 1 - 1e-6, 5000)\n",
    "pdf = gausshyper.pdf(xs, a, b, c, z)\n",
    "entropy_trapz = -np.trapz(pdf * np.log(pdf), xs)\n",
    "entropy_trapz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932618af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MGF via Gauss–Legendre quadrature (NumPy nodes/weights)\n",
    "\n",
    "def mgf_legendre(t: float, a: float, b: float, c: float, z: float, n_nodes: int = 200) -> float:\n",
    "    nodes, weights = np.polynomial.legendre.leggauss(n_nodes)\n",
    "    x = 0.5 * (nodes + 1.0)  # [-1,1] → [0,1]\n",
    "    w = 0.5 * weights\n",
    "    return float(np.sum(w * np.exp(t * x) * gausshyper.pdf(x, a, b, c, z)))\n",
    "\n",
    "\n",
    "for t in (-4.0, -1.0, 0.0, 1.0, 4.0):\n",
    "    print(f\"t={t:>4}: M(t)≈{mgf_legendre(t, a, b, c, z):.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0205c",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "You can interpret `gausshyper` as\n",
    "\n",
    "\\[\n",
    " f(x) \\propto \\underbrace{x^{a-1}(1-x)^{b-1}}_{\\text{Beta kernel}}\\,\\underbrace{(1+z x)^{-c}}_{\\text{tilt}}.\n",
    "\\]\n",
    "\n",
    "- \\(a\\): controls behavior near 0.\n",
    "  - smaller \\(a\\) puts more mass near 0; \\(a<1\\) yields an (integrable) singularity at 0.\n",
    "- \\(b\\): controls behavior near 1.\n",
    "  - smaller \\(b\\) puts more mass near 1; \\(b<1\\) yields an (integrable) singularity at 1.\n",
    "- \\(c\\): strength/direction of the tilt.\n",
    "  - for fixed \\(z>0\\): \\(c>0\\) pushes mass toward 0, \\(c<0\\) pushes mass toward 1.\n",
    "- \\(z\\): how quickly the tilt changes with \\(x\\).\n",
    "  - as \\(z\\to 0\\), the tilt disappears and the distribution approaches Beta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape changes: PDFs for several parameter settings\n",
    "\n",
    "param_sets = [\n",
    "    dict(a=2, b=5, c=0, z=2, label=\"Beta special case (c=0)\"),\n",
    "    dict(a=2, b=5, c=+3, z=2, label=\"c=+3, z=2 (tilt toward 0)\"),\n",
    "    dict(a=2, b=5, c=-3, z=2, label=\"c=-3, z=2 (tilt toward 1)\"),\n",
    "    dict(a=2, b=5, c=+3, z=-0.75, label=\"c=+3, z=-0.75\"),\n",
    "]\n",
    "\n",
    "xs = np.linspace(1e-4, 1 - 1e-4, 500)\n",
    "\n",
    "fig = go.Figure()\n",
    "for p in param_sets:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=xs,\n",
    "            y=gausshyper.pdf(xs, p[\"a\"], p[\"b\"], p[\"c\"], p[\"z\"]),\n",
    "            mode=\"lines\",\n",
    "            name=p[\"label\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PDF shapes for gausshyper\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ea922",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation / variance (via raw moments)\n",
    "\n",
    "Start from the PDF:\n",
    "\n",
    "\\[\n",
    " f(x)=\\frac{1}{B(a,b)\\,{}_2F_1(c,a;a+b;-z)}\\;x^{a-1}(1-x)^{b-1}(1+z x)^{-c}.\n",
    "\\]\n",
    "\n",
    "For \\(n\\ge 0\\), the raw moment is\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X^n]\n",
    "=\\frac{1}{B(a,b)\\,{}_2F_1(c,a;a+b;-z)}\\int_0^1 x^{a+n-1}(1-x)^{b-1}(1+z x)^{-c}\\,dx.\n",
    "\\]\n",
    "\n",
    "The integral is the same normalization integral with \\(a\\to a+n\\), so\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X^n]\n",
    "=\\frac{B(a+n,b)}{B(a,b)}\\,\\frac{{}_2F_1(c,a+n;a+b+n;-z)}{{}_2F_1(c,a;a+b;-z)}.\n",
    "\\]\n",
    "\n",
    "Then\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X]=m_1,\n",
    "\\qquad \\mathrm{Var}(X)=m_2-m_1^2.\n",
    "\\]\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Given i.i.d. data \\(x_1,\\dots,x_N\\in(0,1)\\) and `loc=0, scale=1`, the log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell(a,b,c,z)\n",
    "= -N\\log B(a,b) - N\\log {}_2F_1(c,a;a+b;-z)\n",
    "+ (a-1)\\sum_i \\log x_i\n",
    "+ (b-1)\\sum_i \\log(1-x_i)\n",
    "- c\\sum_i \\log(1+z x_i).\n",
    "\\]\n",
    "\n",
    "SciPy’s `fit` performs numerical maximum likelihood estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b18f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gausshyper_logpdf_formula(x: np.ndarray, a: float, b: float, c: float, z: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    norm = gausshyper_norm(a, b, c, z)\n",
    "    return (a - 1.0) * np.log(x) + (b - 1.0) * np.log(1.0 - x) - c * np.log1p(z * x) - np.log(norm)\n",
    "\n",
    "\n",
    "xs_test = np.linspace(1e-6, 1 - 1e-6, 20)\n",
    "max_logpdf_err = np.max(\n",
    "    np.abs(gausshyper.logpdf(xs_test, a, b, c, z) - gausshyper_logpdf_formula(xs_test, a, b, c, z))\n",
    ")\n",
    "max_logpdf_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f5219",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only rejection sampler\n",
    "\n",
    "Because\n",
    "\n",
    "\\[\n",
    " f(x) \\propto \\text{Beta}(a,b)\\times (1+z x)^{-c},\n",
    "\\]\n",
    "\n",
    "we can use **rejection sampling** with a Beta proposal:\n",
    "\n",
    "1. Propose \\(X\\sim\\text{Beta}(a,b)\\).\n",
    "2. Accept with probability \\(\\frac{(1+zX)^{-c}}{M}\\), where\n",
    "\n",
    "\\[\n",
    "M=\\max_{x\\in[0,1]}(1+z x)^{-c} = \\max\\big(1,\\ (1+z)^{-c}\\big).\n",
    "\\]\n",
    "\n",
    "Key point: this does **not** require computing \\({}_2F_1\\) or the Beta function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rvs_gausshyper_numpy(\n",
    "    size: int,\n",
    "    a: float,\n",
    "    b: float,\n",
    "    c: float,\n",
    "    z: float,\n",
    "    rng: np.random.Generator | None = None,\n",
    "    batch: int = 20_000,\n",
    "    return_acceptance: bool = False,\n",
    "):\n",
    "    # Sample from gausshyper(a,b,c,z) on (0,1) using NumPy-only rejection sampling.\n",
    "    # Proposal: X ~ Beta(a,b)\n",
    "    # Accept with prob: (1+zX)^(-c) / M,  where M = max(1, (1+z)^(-c))\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    size = int(size)\n",
    "    if size < 0:\n",
    "        raise ValueError(\"size must be non-negative\")\n",
    "    if not (a > 0 and b > 0 and z > -1 and np.isfinite(c)):\n",
    "        raise ValueError(\"invalid parameters: require a>0, b>0, z>-1, c finite\")\n",
    "\n",
    "    if size == 0:\n",
    "        out = np.array([], dtype=float)\n",
    "        return (out, 1.0) if return_acceptance else out\n",
    "\n",
    "    logM = max(0.0, -c * np.log1p(z))\n",
    "\n",
    "    out = np.empty(size, dtype=float)\n",
    "    filled = 0\n",
    "    proposals = 0\n",
    "\n",
    "    while filled < size:\n",
    "        x = rng.beta(a, b, size=batch)\n",
    "        u = rng.random(batch)\n",
    "\n",
    "        logw = -c * np.log1p(z * x)\n",
    "        accept = np.log(u) < (logw - logM)\n",
    "\n",
    "        accepted = x[accept]\n",
    "        n_take = min(size - filled, accepted.size)\n",
    "        out[filled : filled + n_take] = accepted[:n_take]\n",
    "        filled += n_take\n",
    "\n",
    "        proposals += batch\n",
    "\n",
    "        if proposals > 100_000_000:\n",
    "            raise RuntimeError(\"Too many proposals; acceptance rate is extremely low.\")\n",
    "\n",
    "    accept_rate = size / proposals\n",
    "    return (out, accept_rate) if return_acceptance else out\n",
    "\n",
    "\n",
    "samples_np, acc = rvs_gausshyper_numpy(20_000, a, b, c, z, rng=rng, return_acceptance=True)\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NumPy-only sampling to SciPy moments\n",
    "\n",
    "mean_s, var_s, skew_s, kurt_s = gausshyper.stats(a, b, c, z, moments=\"mvsk\")\n",
    "\n",
    "print(\"NumPy sampler acceptance rate:\", acc)\n",
    "print(\"sample mean:\", float(np.mean(samples_np)))\n",
    "print(\"sample var :\", float(np.var(samples_np)))\n",
    "print(\"theory mean:\", float(mean_s))\n",
    "print(\"theory var :\", float(var_s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a569de2",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- PDF\n",
    "- CDF (numerical integration on a grid)\n",
    "- Monte Carlo samples (histogram + empirical CDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF + Monte Carlo histogram\n",
    "\n",
    "xs = np.linspace(1e-4, 1 - 1e-4, 600)\n",
    "pdf = gausshyper.pdf(xs, a, b, c, z)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=samples_np,\n",
    "        nbinsx=60,\n",
    "        histnorm=\"probability density\",\n",
    "        name=\"NumPy samples (hist)\",\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=xs, y=pdf, mode=\"lines\", name=\"SciPy PDF\", line=dict(width=3)))\n",
    "fig.update_layout(title=\"gausshyper: PDF and Monte Carlo samples\", xaxis_title=\"x\", yaxis_title=\"density\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2388faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF: integrate PDF on a grid (trapezoidal rule)\n",
    "\n",
    "dx = xs[1] - xs[0]\n",
    "# cumulative trapezoid without scipy.integrate\n",
    "cdf = np.concatenate([[0.0], np.cumsum(0.5 * (pdf[:-1] + pdf[1:]) * dx)])\n",
    "cdf = cdf / cdf[-1]  # enforce ending at 1 (small numerical drift)\n",
    "\n",
    "# Empirical CDF from samples\n",
    "x_sorted = np.sort(samples_np)\n",
    "ecdf = np.arange(1, x_sorted.size + 1) / x_sorted.size\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=cdf, mode=\"lines\", name=\"CDF (integrated PDF)\", line=dict(width=3)))\n",
    "fig.add_trace(go.Scatter(x=x_sorted, y=ecdf, mode=\"lines\", name=\"Empirical CDF (samples)\", line=dict(dash=\"dot\")))\n",
    "\n",
    "# A few SciPy CDF evaluations as a spot-check\n",
    "x_pts = np.array([0.05, 0.2, 0.5, 0.8, 0.95])\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_pts,\n",
    "        y=gausshyper.cdf(x_pts, a, b, c, z),\n",
    "        mode=\"markers\",\n",
    "        name=\"SciPy CDF (points)\",\n",
    "        marker=dict(size=9),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(title=\"gausshyper: CDF\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011a765",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "`scipy.stats.gausshyper` provides:\n",
    "- `pdf`, `logpdf`\n",
    "- `cdf`, `ppf` (computed numerically)\n",
    "- `rvs`\n",
    "- `moment`, `stats`\n",
    "- `fit` (MLE)\n",
    "\n",
    "Below is a minimal “cookbook” usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic usage\n",
    "x0 = 0.3\n",
    "print(\"pdf:\", gausshyper.pdf(x0, a, b, c, z))\n",
    "print(\"cdf:\", gausshyper.cdf(x0, a, b, c, z))\n",
    "\n",
    "# Random variates\n",
    "samples_scipy = gausshyper.rvs(a, b, c, z, size=10_000, random_state=rng)\n",
    "\n",
    "# Moments / stats\n",
    "print(\"stats (mvsk):\", gausshyper.stats(a, b, c, z, moments=\"mvsk\"))\n",
    "print(\"moment(3):\", gausshyper.moment(3, a, b, c, z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c814987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE fitting (fix loc/scale to stay on [0,1])\n",
    "\n",
    "data_fit = gausshyper.rvs(a, b, c, z, size=2_000, random_state=rng)\n",
    "\n",
    "(a_hat, b_hat, c_hat, z_hat, loc_hat, scale_hat) = gausshyper.fit(data_fit, floc=0, fscale=1)\n",
    "print(\"fitted shapes:\", (a_hat, b_hat, c_hat, z_hat))\n",
    "print(\"loc/scale:\", (loc_hat, scale_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ff12c",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing\n",
    "\n",
    "A simple nested test fixes \\(z\\) and compares:\n",
    "- **Null**: \\(c=0\\) (reduces to a Beta distribution)\n",
    "- **Alternative**: \\(c\\) free\n",
    "\n",
    "With \\(z\\) fixed, this is a 1-degree-of-freedom likelihood ratio test (as an approximation).\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "\n",
    "Let \\(p\\in(0,1)\\) be a success probability.\n",
    "\n",
    "Prior:\n",
    "\\[\n",
    " p \\sim \\text{gausshyper}(a,b,c,z)\n",
    "\\]\n",
    "\n",
    "Binomial likelihood:\n",
    "\\[\n",
    " y\\mid p \\sim \\text{Binomial}(n,p)\n",
    "\\]\n",
    "\n",
    "Posterior is in the same family:\n",
    "\\[\n",
    " p\\mid y \\sim \\text{gausshyper}(a+y,\\ b+n-y,\\ c,\\ z).\n",
    "\\]\n",
    "\n",
    "This mirrors Beta–Binomial conjugacy; the tilt term \\((1+z p)^{-c}\\) stays unchanged.\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "\n",
    "A simple hierarchical model:\n",
    "\\[\n",
    " p_i \\sim \\text{gausshyper}(a,b,c,z),\\qquad y_i\\mid p_i \\sim \\text{Binomial}(m,p_i).\n",
    "\\]\n",
    "\n",
    "The induced marginal distribution of counts \\(y_i\\) is typically more flexible than a Beta–Binomial because the prior over \\(p_i\\) is more flexible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a91694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Likelihood ratio test: H0 (c=0) vs H1 (c free), with z fixed\n",
    "\n",
    "z_fixed = z\n",
    "x = gausshyper.rvs(a, b, c, z_fixed, size=1_500, random_state=rng)\n",
    "\n",
    "# Null: c=0, z fixed\n",
    "(a0, b0, c0, z0, loc0, scale0) = gausshyper.fit(x, fc=0, fz=z_fixed, floc=0, fscale=1)\n",
    "ll0 = float(np.sum(gausshyper.logpdf(x, a0, b0, c0, z0)))\n",
    "\n",
    "# Alternative: c free, z fixed\n",
    "(a1, b1, c1, z1, loc1, scale1) = gausshyper.fit(x, fz=z_fixed, floc=0, fscale=1)\n",
    "ll1 = float(np.sum(gausshyper.logpdf(x, a1, b1, c1, z1)))\n",
    "\n",
    "lr = 2 * (ll1 - ll0)\n",
    "p_value = 1 - chi2.cdf(lr, df=1)\n",
    "\n",
    "print(\"H0 fit (a,b,c,z):\", (a0, b0, c0, z0))\n",
    "print(\"H1 fit (a,b,c,z):\", (a1, b1, c1, z1))\n",
    "print(\"LL0:\", ll0)\n",
    "print(\"LL1:\", ll1)\n",
    "print(\"LR statistic:\", lr)\n",
    "print(\"approx p-value (chi^2_1):\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Bayesian update for Binomial: prior/posterior densities\n",
    "\n",
    "# Prior parameters\n",
    "prior = dict(a=2.0, b=5.0, c=3.0, z=2.0)\n",
    "\n",
    "# Binomial data\n",
    "n = 40\n",
    "y = 12\n",
    "\n",
    "post = dict(a=prior[\"a\"] + y, b=prior[\"b\"] + (n - y), c=prior[\"c\"], z=prior[\"z\"])\n",
    "\n",
    "xs = np.linspace(1e-4, 1 - 1e-4, 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=gausshyper.pdf(xs, **prior), mode=\"lines\", name=\"prior\"))\n",
    "fig.add_trace(go.Scatter(x=xs, y=gausshyper.pdf(xs, **post), mode=\"lines\", name=\"posterior\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Conjugate update for Binomial: n={n}, y={y}\",\n",
    "    xaxis_title=\"p\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"prior mean     :\", float(gausshyper.mean(**prior)))\n",
    "print(\"posterior mean :\", float(gausshyper.mean(**post)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64066a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Hierarchical simulation: p ~ gausshyper, y|p ~ Binomial(m,p)\n",
    "\n",
    "m = 20\n",
    "N = 10_000\n",
    "\n",
    "p_gh = gausshyper.rvs(prior[\"a\"], prior[\"b\"], prior[\"c\"], prior[\"z\"], size=N, random_state=rng)\n",
    "y_gh = rng.binomial(m, p_gh)\n",
    "\n",
    "# Baseline: Beta prior with same a,b (c=0)\n",
    "p_beta = rng.beta(prior[\"a\"], prior[\"b\"], size=N)\n",
    "y_beta = rng.binomial(m, p_beta)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=y_beta, histnorm=\"probability\", name=\"Beta prior\", opacity=0.55))\n",
    "fig.add_trace(go.Histogram(x=y_gh, histnorm=\"probability\", name=\"gausshyper prior\", opacity=0.55))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"overlay\",\n",
    "    title=f\"Counts y with m={m}: Beta vs gausshyper prior over p\",\n",
    "    xaxis_title=\"y\",\n",
    "    yaxis_title=\"probability\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"Var(y) with Beta prior      :\", float(np.var(y_beta)))\n",
    "print(\"Var(y) with gausshyper prior:\", float(np.var(y_gh)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360ca9e",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**:\n",
    "  - must have \\(a>0\\), \\(b>0\\), \\(z>-1\\); otherwise \\(1+z x\\) can become non-positive on \\([0,1]\\).\n",
    "- **Boundary behavior**:\n",
    "  - if \\(a<1\\) or \\(b<1\\), the PDF diverges at 0 or 1 (still integrable). Avoid evaluating exactly at 0 or 1 in floating point.\n",
    "- **Numerical stability**:\n",
    "  - the normalization constant uses \\({}_2F_1\\), which can overflow/underflow for extreme parameters. Prefer `logpdf` when fitting.\n",
    "  - `cdf` can be relatively slow because it is computed numerically.\n",
    "- **Rejection sampling efficiency**:\n",
    "  - acceptance can be very low if \\(M=\\max(1,(1+z)^{-c})\\) is huge (e.g., \\(z\\) close to \\(-1\\) and \\(c>0\\)). In that regime you may need a better proposal than Beta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62366066",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `gausshyper` is a continuous distribution on \\((0,1)\\) with density\n",
    "  \\(\\propto x^{a-1}(1-x)^{b-1}(1+z x)^{-c}\\).\n",
    "- It generalizes the Beta distribution; setting \\(c=0\\) (or \\(z=0\\)) recovers Beta.\n",
    "- Normalization and moments involve the Gauss hypergeometric function \\({}_2F_1\\).\n",
    "- Raw moments have a closed form ratio of Beta functions and \\({}_2F_1\\) terms; mean/variance follow directly.\n",
    "- A simple **NumPy-only** sampler uses rejection sampling with a Beta proposal.\n",
    "- SciPy provides `pdf`, `cdf`, `rvs`, and `fit`, but be mindful of numerical and performance pitfalls for extreme parameters.\n",
    "\n",
    "**References**\n",
    "- SciPy `gausshyper` docstring/source\n",
    "- Armero, C., & Bayarri, M. J. (1994). “Prior Assessments for Prediction in Queues.” *Journal of the Royal Statistical Society: Series D (The Statistician)*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
