{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a443d3e",
   "metadata": {},
   "source": [
    "# `exponnorm` (Exponentially Modified Normal / exGaussian)\n",
    "\n",
    "The `exponnorm` distribution (SciPy’s name) is the distribution of a **normal random variable plus an independent exponential delay**.\n",
    "\n",
    "It’s a simple, interpretable model for **right-skewed** continuous data: symmetric measurement noise (Normal) + one-sided waiting time (Exponential).\n",
    "\n",
    "## What you’ll learn\n",
    "- what `exponnorm` models and when it’s appropriate\n",
    "- the PDF/CDF in closed form (and how SciPy parameterizes it)\n",
    "- mean/variance/skewness/kurtosis, plus MGF/characteristic function and entropy notes\n",
    "- how parameters change the shape (skew and tail behavior)\n",
    "- key derivations: expectation, variance, likelihood\n",
    "- NumPy-only sampling via the “Normal + Exponential” construction\n",
    "- SciPy usage: `scipy.stats.exponnorm` (`pdf`, `cdf`, `rvs`, `fit`)\n",
    "- statistical use cases: hypothesis testing, Bayesian modeling patterns, generative modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27d8e0",
   "metadata": {},
   "source": [
    "## Notebook roadmap\n",
    "1) Title & classification\n",
    "2) Intuition & motivation\n",
    "3) Formal definition (PDF/CDF)\n",
    "4) Moments & properties\n",
    "5) Parameter interpretation\n",
    "6) Derivations (\\(\\mathbb{E}[X]\\), \\(\\mathrm{Var}(X)\\), likelihood)\n",
    "7) Sampling & simulation (NumPy-only)\n",
    "8) Visualization (PDF, CDF, Monte Carlo)\n",
    "9) SciPy integration (`scipy.stats.exponnorm`)\n",
    "10) Statistical use cases\n",
    "11) Pitfalls\n",
    "12) Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d022be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special, stats\n",
    "from scipy.optimize import brentq, minimize\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "print(\"numpy \", np.__version__)\n",
    "print(\"scipy \", scipy.__version__)\n",
    "print(\"plotly\", plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73151ef",
   "metadata": {},
   "source": [
    "## Prerequisites & notation\n",
    "\n",
    "**Prerequisites**\n",
    "- basic probability (PDF/CDF, independence, expectation)\n",
    "- comfort with calculus (a convolution integral)\n",
    "\n",
    "**Notation**\n",
    "- standard normal PDF/CDF: \\(\\phi(\\cdot)\\), \\(\\Phi(\\cdot)\\)\n",
    "- complementary error function: \\(\\mathrm{erfc}(\\cdot)\\)\n",
    "\n",
    "**Parameterizations**\n",
    "\n",
    "A common “exGaussian” construction is:\n",
    "\n",
    "\\[\n",
    "X = N + D,\\quad\n",
    "N \\sim \\mathcal{N}(\\mu,\\sigma^2),\\quad\n",
    "D \\sim \\mathrm{Exp}(\\text{rate}=\\lambda),\\quad\n",
    "N \\perp D.\n",
    "\\]\n",
    "\n",
    "SciPy’s `exponnorm` uses a dimensionless shape parameter \\(K\\) instead of \\(\\lambda\\):\n",
    "\n",
    "- `stats.exponnorm(K, loc=μ, scale=σ)`\n",
    "- mapping: \\(K = \\frac{1}{\\sigma\\lambda}\\)  (so \\(\\lambda = \\frac{1}{K\\sigma}\\))\n",
    "- exponential mean (delay scale): \\(\\tau = 1/\\lambda = K\\sigma\\)\n",
    "\n",
    "In the standardized form (`loc=0`, `scale=1`): \\(Y = Z + E\\) with\n",
    "\\(Z\\sim \\mathcal{N}(0,1)\\) and \\(E\\sim \\mathrm{Exp}(\\text{rate}=1/K)\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699eb28",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `exponnorm` (Exponentially modified normal / exGaussian)\n",
    "- **Type**: **continuous**\n",
    "- **Support**: \\(x \\in \\mathbb{R}\\)\n",
    "- **Parameter space** (SciPy):\n",
    "  - \\(K > 0\\) (shape)\n",
    "  - \\(\\text{loc} \\in \\mathbb{R}\\) (location)\n",
    "  - \\(\\text{scale} > 0\\) (scale)\n",
    "\n",
    "A useful generative view (SciPy parameterization):\n",
    "\n",
    "\\[\n",
    "X = \\text{loc} + \\text{scale}\\,(Z + E),\n",
    "\\quad Z \\sim \\mathcal{N}(0,1),\n",
    "\\quad E \\sim \\mathrm{Exp}(\\text{rate}=1/K),\n",
    "\\quad Z \\perp E.\n",
    "\\]\n",
    "\n",
    "Equivalently, \\(X = N + D\\) with \\(N \\sim \\mathcal{N}(\\text{loc},\\,\\text{scale}^2)\\) and\n",
    "\\(D \\sim \\mathrm{Exp}(\\text{rate}=1/(K\\,\\text{scale}))\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a356927",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "`exponnorm` is a **convolution** of a normal and an exponential distribution.\n",
    "\n",
    "Think of an observation as:\n",
    "\n",
    "- a symmetric baseline measurement \\(N\\) (sensor noise, natural variability), plus\n",
    "- a one-sided random delay \\(D \\ge 0\\) (queueing, reaction-time delay, processing latency).\n",
    "\n",
    "The exponential part creates a **long right tail** and **positive skew**, while the normal part keeps the distribution “normal-like” near its center.\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Reaction times** (psychology / neuroscience): decision time + motor delay, often right-skewed.\n",
    "- **Network / system latency**: baseline jitter + occasional queueing delays.\n",
    "- **Service times** in queues: variability around a typical duration plus delay bursts.\n",
    "- **Chromatography / mass spectrometry**: peak shapes with exponential tailing.\n",
    "\n",
    "### Relations to other distributions\n",
    "- \\(K \\to 0\\) (or \\(\\lambda \\to \\infty\\)): the exponential delay vanishes and the distribution approaches a **normal**.\n",
    "- \\(\\sigma \\to 0\\) (keeping \\(\\tau\\) fixed): the normal collapses and you approach a **shifted exponential**.\n",
    "- Compared to **lognormal** or **gamma**, `exponnorm` has an explicit “noise + delay” interpretation and a very simple sampling story.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b363b",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### Hierarchical (convolution) definition\n",
    "\n",
    "Let \\(Z \\sim \\mathcal{N}(0,1)\\) and \\(E \\sim \\mathrm{Exp}(\\text{rate}=1/K)\\) be independent with \\(K>0\\).\n",
    "Define the standardized variable \\(Y = Z + E\\). Then:\n",
    "\n",
    "\\[\n",
    "Y \\sim \\mathrm{exponnorm}(K).\n",
    "\\]\n",
    "\n",
    "The general (shifted/scaled) variable is:\n",
    "\n",
    "\\[\n",
    "X = \\text{loc} + \\text{scale}\\,Y.\n",
    "\\]\n",
    "\n",
    "### PDF\n",
    "\n",
    "For the standardized form (\\(\\text{loc}=0\\), \\(\\text{scale}=1\\)), SciPy uses:\n",
    "\n",
    "\\[\n",
    "f_Y(y;K)\n",
    "= \\frac{1}{2K}\\exp\\!\\left(\\frac{1}{2K^2}-\\frac{y}{K}\\right)\\,\n",
    "\\mathrm{erfc}\\!\\left(\\frac{1/K - y}{\\sqrt{2}}\\right),\n",
    "\\qquad y\\in\\mathbb{R},\\;K>0.\n",
    "\\]\n",
    "\n",
    "The shifted/scaled PDF is obtained by the change of variables \\(y=(x-\\text{loc})/\\text{scale}\\):\n",
    "\n",
    "\\[\n",
    "f_X(x;K,\\text{loc},\\text{scale}) = \\frac{1}{\\text{scale}}\\, f_Y\\!\\left(\\frac{x-\\text{loc}}{\\text{scale}};K\\right).\n",
    "\\]\n",
    "\n",
    "### CDF\n",
    "\n",
    "A convenient closed form in terms of the standard normal CDF \\(\\Phi\\) is:\n",
    "\n",
    "\\[\n",
    "F_Y(y;K)\n",
    "= \\Phi(y)\n",
    "- \\exp\\!\\left(\\frac{1}{2K^2}-\\frac{y}{K}\\right)\\,\n",
    "\\Phi\\!\\left(y-\\frac{1}{K}\\right).\n",
    "\\]\n",
    "\n",
    "Again, \\(F_X(x) = F_Y\\!\\big((x-\\text{loc})/\\text{scale}\\big)\\).\n",
    "\n",
    "We’ll implement these formulas (carefully, in log-space where helpful) and check them against SciPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d592385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exgauss_to_scipy(mu: float, sigma: float, lam: float) -> tuple[float, float, float]:\n",
    "    \"\"\"Map (μ, σ, λ) exGaussian parameters to SciPy's (K, loc, scale).\"\"\"\n",
    "\n",
    "    if sigma <= 0:\n",
    "        raise ValueError(\"sigma must be > 0\")\n",
    "    if lam <= 0:\n",
    "        raise ValueError(\"lam must be > 0\")\n",
    "\n",
    "    K = 1.0 / (sigma * lam)\n",
    "    return float(K), float(mu), float(sigma)\n",
    "\n",
    "\n",
    "def scipy_to_exgauss(K: float, loc: float, scale: float) -> tuple[float, float, float, float]:\n",
    "    \"\"\"Map SciPy's (K, loc, scale) to (μ, σ, λ, τ) where τ=1/λ is the exp mean.\"\"\"\n",
    "\n",
    "    if K <= 0:\n",
    "        raise ValueError(\"K must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    lam = 1.0 / (K * scale)\n",
    "    tau = 1.0 / lam\n",
    "    return float(loc), float(scale), float(lam), float(tau)\n",
    "\n",
    "\n",
    "def exponnorm_logpdf_standard(y: np.ndarray, K: float) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of the standardized exponnorm (loc=0, scale=1).\n",
    "\n",
    "    Formula (SciPy):\n",
    "        f(y) = 1/(2K) * exp(1/(2K^2) - y/K) * erfc((1/K - y)/sqrt(2))\n",
    "\n",
    "    We evaluate it stably via erfcx(z) = exp(z^2) * erfc(z).\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if K <= 0:\n",
    "        raise ValueError(\"K must be > 0\")\n",
    "\n",
    "    z = (1.0 / K - y) / np.sqrt(2.0)\n",
    "\n",
    "    return (\n",
    "        -np.log(2.0 * K)\n",
    "        + (1.0 / (2.0 * K * K))\n",
    "        - y / K\n",
    "        - z * z\n",
    "        + np.log(special.erfcx(z))\n",
    "    )\n",
    "\n",
    "\n",
    "def exponnorm_logpdf(x: np.ndarray, K: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of exponnorm(K, loc, scale) using the standardized log-PDF.\"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    y = (x - loc) / scale\n",
    "    return exponnorm_logpdf_standard(y, K) - np.log(scale)\n",
    "\n",
    "\n",
    "def exponnorm_pdf(x: np.ndarray, K: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    return np.exp(exponnorm_logpdf(x, K, loc=loc, scale=scale))\n",
    "\n",
    "\n",
    "def exponnorm_cdf_standard(y: np.ndarray, K: float) -> np.ndarray:\n",
    "    \"\"\"CDF of the standardized exponnorm.\n",
    "\n",
    "    Closed form:\n",
    "        F(y) = Φ(y) - exp(1/(2K^2) - y/K) * Φ(y - 1/K)\n",
    "\n",
    "    We evaluate the second term using logcdf for stability.\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if K <= 0:\n",
    "        raise ValueError(\"K must be > 0\")\n",
    "\n",
    "    term1 = stats.norm.cdf(y)\n",
    "    log_term2 = (1.0 / (2.0 * K * K)) - y / K + stats.norm.logcdf(y - 1.0 / K)\n",
    "    term2 = np.exp(log_term2)\n",
    "    return term1 - term2\n",
    "\n",
    "\n",
    "def exponnorm_cdf(x: np.ndarray, K: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    y = (x - loc) / scale\n",
    "    return exponnorm_cdf_standard(y, K)\n",
    "\n",
    "\n",
    "def exponnorm_stats_closed_form(\n",
    "    K: float,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Return (mean, var, skew, excess_kurtosis).\"\"\"\n",
    "\n",
    "    if K <= 0:\n",
    "        raise ValueError(\"K must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    mean = loc + scale * K\n",
    "    var = (scale * scale) * (1.0 + K * K)\n",
    "    skew = 2.0 * (K**3) / ((1.0 + K * K) ** 1.5)\n",
    "    excess_kurt = 6.0 * (K**4) / ((1.0 + K * K) ** 2.0)\n",
    "    return float(mean), float(var), float(skew), float(excess_kurt)\n",
    "\n",
    "\n",
    "def exponnorm_mgf(t: np.ndarray, K: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"MGF M_X(t) = E[exp(tX)]. Exists only for t < 1/(K*scale).\"\"\"\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    if K <= 0:\n",
    "        raise ValueError(\"K must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    denom = 1.0 - K * scale * t\n",
    "    if np.any(denom <= 0):\n",
    "        raise ValueError(\"MGF exists only for t < 1/(K*scale).\")\n",
    "\n",
    "    return np.exp(loc * t + 0.5 * (scale * t) ** 2) / denom\n",
    "\n",
    "\n",
    "def exponnorm_cf(t: np.ndarray, K: float, loc: float = 0.0, scale: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Characteristic function φ_X(t) = E[exp(i t X)].\"\"\"\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    if K <= 0:\n",
    "        raise ValueError(\"K must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    denom = 1.0 - 1j * K * scale * t\n",
    "    return np.exp(1j * loc * t - 0.5 * (scale * t) ** 2) / denom\n",
    "\n",
    "\n",
    "def sample_exponnorm_numpy(\n",
    "    n: int,\n",
    "    K: float,\n",
    "    rng: np.random.Generator,\n",
    "    loc: float = 0.0,\n",
    "    scale: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"NumPy-only sampling via X = loc + scale * (Z + E).\n",
    "\n",
    "    Z ~ N(0,1)\n",
    "    E ~ Exp(rate=1/K)  (mean K)\n",
    "    \"\"\"\n",
    "\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive\")\n",
    "    if K <= 0:\n",
    "        raise ValueError(\"K must be > 0\")\n",
    "    if scale <= 0:\n",
    "        raise ValueError(\"scale must be > 0\")\n",
    "\n",
    "    z = rng.normal(size=n)\n",
    "    e = rng.exponential(scale=K, size=n)\n",
    "    return loc + scale * (z + e)\n",
    "\n",
    "\n",
    "def sample_moments(x: np.ndarray) -> tuple[float, float, float, float]:\n",
    "    \"\"\"Return (mean, var, skew, excess_kurtosis) using population moments (ddof=0).\"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    m = float(np.mean(x))\n",
    "    v = float(np.var(x, ddof=0))\n",
    "    s = float(np.sqrt(v))\n",
    "    if s == 0:\n",
    "        return m, v, np.nan, np.nan\n",
    "\n",
    "    z = (x - m) / s\n",
    "    skew = float(np.mean(z**3))\n",
    "    excess_kurt = float(np.mean(z**4) - 3.0)\n",
    "    return m, v, skew, excess_kurt\n",
    "\n",
    "\n",
    "def K_from_skewness(gamma1: float, eps: float = 1e-10) -> float:\n",
    "    \"\"\"Invert gamma1 = 2 K^3 / (1 + K^2)^(3/2) for K>0.\n",
    "\n",
    "    Theoretical range: gamma1 ∈ [0, 2). We clip slightly below 2 for stability.\n",
    "    \"\"\"\n",
    "\n",
    "    gamma1 = float(np.clip(gamma1, 0.0, 2.0 - 1e-12))\n",
    "    if gamma1 < eps:\n",
    "        return eps\n",
    "\n",
    "    def f(K: float) -> float:\n",
    "        return 2.0 * (K**3) / ((1.0 + K * K) ** 1.5) - gamma1\n",
    "\n",
    "    return float(brentq(f, eps, 1e6))\n",
    "\n",
    "\n",
    "def exponnorm_mom_initial_guess(x: np.ndarray) -> tuple[float, float, float]:\n",
    "    \"\"\"Method-of-moments initial guess (K, loc, scale) from mean/var/skew.\"\"\"\n",
    "\n",
    "    m, v, skew, _ = sample_moments(x)\n",
    "    K0 = K_from_skewness(skew if np.isfinite(skew) else 0.0)\n",
    "\n",
    "    scale0 = float(np.sqrt(v / (1.0 + K0 * K0)))\n",
    "    loc0 = float(m - scale0 * K0)\n",
    "    return float(K0), float(loc0), float(scale0)\n",
    "\n",
    "\n",
    "def x_grid_for_plotting(rv, q_low: float = 1e-3, q_high: float = 0.999, n: int = 600) -> np.ndarray:\n",
    "    \"\"\"Choose a reasonable finite x-grid for plotting a continuous distribution.\"\"\"\n",
    "\n",
    "    lo = float(rv.ppf(q_low))\n",
    "    hi = float(rv.ppf(q_high))\n",
    "\n",
    "    if not (np.isfinite(lo) and np.isfinite(hi) and hi > lo):\n",
    "        mean, var = rv.stats(moments=\"mv\")\n",
    "        mean = float(mean)\n",
    "        var = float(var)\n",
    "        if np.isfinite(mean) and np.isfinite(var) and var > 0:\n",
    "            std = float(np.sqrt(var))\n",
    "            lo = mean - 6 * std\n",
    "            hi = mean + 10 * std  # skewed right: give more room on the right\n",
    "        else:\n",
    "            lo, hi = -5.0, 15.0\n",
    "\n",
    "    return np.linspace(lo, hi, n)\n",
    "\n",
    "\n",
    "def empirical_cdf(samples: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    samples = np.asarray(samples, dtype=float)\n",
    "    xs = np.sort(samples)\n",
    "    n = xs.size\n",
    "    ys = np.arange(1, n + 1) / n\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe188ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick correctness check vs SciPy (PDF/CDF + moments)\n",
    "\n",
    "K, loc, scale = 1.5, -0.2, 0.8\n",
    "rv = stats.exponnorm(K, loc=loc, scale=scale)\n",
    "\n",
    "x = x_grid_for_plotting(rv, q_low=1e-4, q_high=0.9999, n=500)\n",
    "\n",
    "pdf_err = float(np.max(np.abs(exponnorm_pdf(x, K, loc=loc, scale=scale) - rv.pdf(x))))\n",
    "cdf_err = float(np.max(np.abs(exponnorm_cdf(x, K, loc=loc, scale=scale) - rv.cdf(x))))\n",
    "\n",
    "mean_cf, var_cf, skew_cf, kurt_cf = exponnorm_stats_closed_form(K, loc=loc, scale=scale)\n",
    "mean_sp, var_sp, skew_sp, kurt_sp = rv.stats(moments=\"mvsk\")\n",
    "\n",
    "{\n",
    "    \"max|pdf - SciPy|\": pdf_err,\n",
    "    \"max|cdf - SciPy|\": cdf_err,\n",
    "    \"mean (closed, SciPy)\": (mean_cf, float(mean_sp)),\n",
    "    \"var  (closed, SciPy)\": (var_cf, float(var_sp)),\n",
    "    \"skew (closed, SciPy)\": (skew_cf, float(skew_sp)),\n",
    "    \"kurt_excess (closed, SciPy)\": (kurt_cf, float(kurt_sp)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24653a1d",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Mean, variance, skewness, kurtosis\n",
    "\n",
    "Using the additive construction \\(X = N + D\\) (normal + exponential), moments follow cleanly.\n",
    "\n",
    "For SciPy’s parameters \\((K,\\text{loc},\\text{scale})\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\text{loc} + K\\,\\text{scale}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\text{scale}^2\\,(1+K^2)\n",
    "\\]\n",
    "\n",
    "Skewness and **excess** kurtosis depend only on \\(K\\) (location/scale don’t change these shape measures):\n",
    "\n",
    "\\[\n",
    "\\gamma_1 = \\frac{2K^3}{(1+K^2)^{3/2}},\n",
    "\\qquad\n",
    "\\gamma_2 = \\frac{6K^4}{(1+K^2)^2}.\n",
    "\\]\n",
    "\n",
    "As \\(K\\to 0\\), \\(\\gamma_1\\to 0\\) and you approach a normal; as \\(K\\to \\infty\\), \\(\\gamma_1\\to 2\\) and you approach an exponential-like skew.\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "Because \\(X\\) is a sum of independent variables, MGFs multiply.\n",
    "\n",
    "For \\(X\\sim\\mathrm{exponnorm}(K,\\text{loc},\\text{scale})\\):\n",
    "\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = \\frac{\\exp\\!\\left(\\text{loc}\\,t + \\tfrac{1}{2}(\\text{scale}\\,t)^2\\right)}{1 - K\\,\\text{scale}\\,t},\n",
    "\\qquad t < \\frac{1}{K\\,\\text{scale}}.\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\frac{\\exp\\!\\left(i\\,\\text{loc}\\,t - \\tfrac{1}{2}(\\text{scale}\\,t)^2\\right)}{1 - iK\\,\\text{scale}\\,t}.\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The differential entropy \\(h(X) = -\\mathbb{E}[\\log f_X(X)]\\) does not have a simple elementary closed form.\n",
    "\n",
    "In practice you compute it numerically (SciPy provides `entropy`), and scaling behaves as usual:\n",
    "\\(h(\\text{loc}+\\text{scale}\\,Y)=h(Y)+\\log(\\text{scale})\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffaabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moment checks + MGF/CF + entropy sanity checks\n",
    "\n",
    "K, loc, scale = 1.2, -0.5, 0.8\n",
    "rv = stats.exponnorm(K, loc=loc, scale=scale)\n",
    "\n",
    "# Closed form vs SciPy\n",
    "closed = exponnorm_stats_closed_form(K, loc=loc, scale=scale)\n",
    "scipy_stats = tuple(float(v) for v in rv.stats(moments=\"mvsk\"))\n",
    "\n",
    "print(\"(mean, var, skew, kurt_excess)\")\n",
    "print(\"closed:\", closed)\n",
    "print(\"scipy :\", scipy_stats)\n",
    "\n",
    "# Monte Carlo checks\n",
    "n = 200_000\n",
    "samples = rv.rvs(size=n, random_state=rng)\n",
    "mc = sample_moments(samples)\n",
    "\n",
    "print(\"mc   :\", mc)\n",
    "\n",
    "# MGF at a safe t\n",
    "# Domain: t < 1/(K*scale)\n",
    "t = 0.4\n",
    "mgf_theory = float(exponnorm_mgf(t, K, loc=loc, scale=scale))\n",
    "mgf_mc = float(np.mean(np.exp(t * samples)))\n",
    "\n",
    "# Characteristic function at a frequency\n",
    "w = 1.0\n",
    "cf_theory = exponnorm_cf(w, K, loc=loc, scale=scale)\n",
    "cf_mc = np.mean(np.exp(1j * w * samples))\n",
    "\n",
    "print(\"mgf theory vs mc:\", mgf_theory, mgf_mc)\n",
    "print(\"cf  theory vs mc:\", cf_theory, cf_mc)\n",
    "\n",
    "# Entropy: SciPy vs Monte Carlo estimate -E[log f(X)]\n",
    "h_scipy = float(rv.entropy())\n",
    "h_mc = float(-np.mean(rv.logpdf(samples)))\n",
    "\n",
    "print(\"entropy scipy vs mc:\", h_scipy, h_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be59ec",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### Shape parameter \\(K\\)\n",
    "\n",
    "- \\(K\\) is **dimensionless**: it’s the exponential mean measured in units of the normal standard deviation.\n",
    "- In the \\((\\mu,\\sigma,\\lambda)\\) parameterization: \\(K = 1/(\\sigma\\lambda) = \\tau/\\sigma\\).\n",
    "\n",
    "Intuition:\n",
    "- small \\(K\\): the exponential delay is tiny relative to the normal noise \\(\\Rightarrow\\) nearly symmetric (almost normal)\n",
    "- large \\(K\\): the delay dominates \\(\\Rightarrow\\) strong right tail and skew\n",
    "\n",
    "### `loc` and `scale`\n",
    "\n",
    "- `loc` shifts the distribution left/right.\n",
    "- `scale` stretches the distribution and also scales the exponential delay (since the exponential component is multiplied by `scale`).\n",
    "\n",
    "Mean/variance reminders:\n",
    "\\(\\mathbb{E}[X]=\\text{loc}+K\\,\\text{scale}\\),\n",
    "\\(\\mathrm{Var}(X)=\\text{scale}^2(1+K^2)\\).\n",
    "\n",
    "### Shape changes\n",
    "\n",
    "Below we vary \\(K\\) (keeping the mean fixed) and vary `scale` to see how the PDF/CDF morph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape changes as K varies (keep mean fixed at 0 by setting loc = -K when scale=1)\n",
    "\n",
    "Ks = [0.1, 0.5, 1.0, 2.0]\n",
    "rvs = [stats.exponnorm(K, loc=-K, scale=1.0) for K in Ks]\n",
    "\n",
    "lo = min(float(rv.ppf(0.001)) for rv in rvs)\n",
    "hi = max(float(rv.ppf(0.999)) for rv in rvs)\n",
    "x = np.linspace(lo, hi, 700)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"PDF\", \"CDF\"))\n",
    "\n",
    "for K, rv in zip(Ks, rvs):\n",
    "    fig.add_trace(go.Scatter(x=x, y=rv.pdf(x), mode=\"lines\", name=f\"K={K:g}\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=rv.cdf(x), mode=\"lines\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.add_vline(x=0.0, line_dash=\"dot\", opacity=0.35)\n",
    "fig.update_layout(title=\"exponnorm: varying K (mean aligned to 0)\")\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"density\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"cdf\", row=1, col=2)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Scale changes (keep mean fixed at 0 by setting loc = -scale*K)\n",
    "K = 1.0\n",
    "scales = [0.5, 1.0, 2.0]\n",
    "rvs = [stats.exponnorm(K, loc=-(s * K), scale=s) for s in scales]\n",
    "\n",
    "lo = min(float(rv.ppf(0.001)) for rv in rvs)\n",
    "hi = max(float(rv.ppf(0.999)) for rv in rvs)\n",
    "x = np.linspace(lo, hi, 700)\n",
    "\n",
    "fig = go.Figure()\n",
    "for s, rv in zip(scales, rvs):\n",
    "    fig.add_trace(go.Scatter(x=x, y=rv.pdf(x), mode=\"lines\", name=f\"scale={s:g}\"))\n",
    "\n",
    "fig.add_vline(x=0.0, line_dash=\"dot\", opacity=0.35)\n",
    "fig.update_layout(\n",
    "    title=\"exponnorm: varying scale (mean aligned to 0, K fixed)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed480f0",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Expectation\n",
    "\n",
    "Using the exGaussian sum representation:\n",
    "\n",
    "\\[\n",
    "X = N + D,\n",
    "\\quad N\\sim\\mathcal{N}(\\mu,\\sigma^2),\n",
    "\\quad D\\sim\\mathrm{Exp}(\\text{rate}=\\lambda),\n",
    "\\quad N\\perp D.\n",
    "\\]\n",
    "\n",
    "Linearity of expectation gives:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\mathbb{E}[N] + \\mathbb{E}[D] = \\mu + \\frac{1}{\\lambda}.\n",
    "\\]\n",
    "\n",
    "Mapping to SciPy, \\(\\mu=\\text{loc}\\) and \\(1/\\lambda=\\tau = K\\,\\text{scale}\\), so\n",
    "\\(\\mathbb{E}[X]=\\text{loc}+K\\,\\text{scale}\\).\n",
    "\n",
    "### 6.2 Variance\n",
    "\n",
    "Independence implies variances add:\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\mathrm{Var}(N) + \\mathrm{Var}(D) = \\sigma^2 + \\frac{1}{\\lambda^2}.\n",
    "\\]\n",
    "\n",
    "Under SciPy’s parameterization, \\(\\sigma=\\text{scale}\\) and \\(1/\\lambda=K\\,\\text{scale}\\), hence\n",
    "\\(\\mathrm{Var}(X)=\\text{scale}^2(1+K^2)\\).\n",
    "\n",
    "### 6.3 Likelihood (for data)\n",
    "\n",
    "Given i.i.d. observations \\(x_1,\\dots,x_n\\), the likelihood is\n",
    "\n",
    "\\[\n",
    "L(K,\\text{loc},\\text{scale}) = \\prod_{i=1}^n f_X(x_i;K,\\text{loc},\\text{scale}).\n",
    "\\]\n",
    "\n",
    "The log-likelihood is\n",
    "\n",
    "\\[\n",
    "\\ell = \\sum_{i=1}^n \\log f_X(x_i;K,\\text{loc},\\text{scale})\n",
    "= -n\\log(\\text{scale}) + \\sum_{i=1}^n \\log f_Y\\!\\left(\\frac{x_i-\\text{loc}}{\\text{scale}};K\\right),\n",
    "\\]\n",
    "\n",
    "where \\(f_Y\\) is the standardized density.\n",
    "\n",
    "There is no simple closed-form MLE; numerical optimization (or `scipy.stats.exponnorm.fit`) is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood + MLE demo (SciPy fit vs a simple custom optimizer)\n",
    "\n",
    "# Simulate data from a known parameter set\n",
    "K_true, loc_true, scale_true = 1.3, -0.5, 0.8\n",
    "n = 3_000\n",
    "x = sample_exponnorm_numpy(n, K=K_true, loc=loc_true, scale=scale_true, rng=rng)\n",
    "\n",
    "# SciPy MLE\n",
    "K_hat_scipy, loc_hat_scipy, scale_hat_scipy = stats.exponnorm.fit(x)\n",
    "\n",
    "\n",
    "def loglik(x: np.ndarray, K: float, loc: float, scale: float) -> float:\n",
    "    return float(np.sum(exponnorm_logpdf(x, K, loc=loc, scale=scale)))\n",
    "\n",
    "\n",
    "def fit_exponnorm_mle(x: np.ndarray) -> tuple[float, float, float]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    K0, loc0, scale0 = exponnorm_mom_initial_guess(x)\n",
    "    theta0 = np.array([math.log(K0), loc0, math.log(scale0)], dtype=float)\n",
    "\n",
    "    def nll(theta: np.ndarray) -> float:\n",
    "        logK, loc, logscale = float(theta[0]), float(theta[1]), float(theta[2])\n",
    "        K = float(np.exp(logK))\n",
    "        scale = float(np.exp(logscale))\n",
    "        if not (np.isfinite(K) and np.isfinite(loc) and np.isfinite(scale) and scale > 0):\n",
    "            return np.inf\n",
    "        return -loglik(x, K, loc=loc, scale=scale)\n",
    "\n",
    "    res = minimize(nll, x0=theta0, method=\"Nelder-Mead\")\n",
    "    logK_hat, loc_hat, logscale_hat = (float(v) for v in res.x)\n",
    "    return float(np.exp(logK_hat)), float(loc_hat), float(np.exp(logscale_hat))\n",
    "\n",
    "\n",
    "K_hat_opt, loc_hat_opt, scale_hat_opt = fit_exponnorm_mle(x)\n",
    "\n",
    "print(\"true :\", (K_true, loc_true, scale_true))\n",
    "print(\"scipy:\", (float(K_hat_scipy), float(loc_hat_scipy), float(scale_hat_scipy)))\n",
    "print(\"opt  :\", (K_hat_opt, loc_hat_opt, scale_hat_opt))\n",
    "\n",
    "print(\"loglik true :\", loglik(x, K_true, loc_true, scale_true))\n",
    "print(\"loglik scipy:\", loglik(x, float(K_hat_scipy), float(loc_hat_scipy), float(scale_hat_scipy)))\n",
    "print(\"loglik opt  :\", loglik(x, K_hat_opt, loc_hat_opt, scale_hat_opt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98079c1c",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation (NumPy-only)\n",
    "\n",
    "### Algorithm (Normal + Exponential)\n",
    "\n",
    "Use the defining construction:\n",
    "\n",
    "1) Sample \\(Z \\sim \\mathcal{N}(0,1)\\).\n",
    "2) Sample \\(E \\sim \\mathrm{Exp}(\\text{rate}=1/K)\\) (mean \\(K\\)).\n",
    "3) Set \\(Y = Z + E\\).\n",
    "4) Return \\(X = \\text{loc} + \\text{scale}\\,Y\\).\n",
    "\n",
    "This is efficient and numerically stable because it uses only well-behaved base RNGs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo sanity check of the NumPy-only sampler\n",
    "\n",
    "K, loc, scale = 0.9, -0.3, 1.1\n",
    "n = 200_000\n",
    "samples = sample_exponnorm_numpy(n, K=K, loc=loc, scale=scale, rng=rng)\n",
    "\n",
    "mc = sample_moments(samples)\n",
    "closed = exponnorm_stats_closed_form(K, loc=loc, scale=scale)\n",
    "\n",
    "print(\"(mean, var, skew, kurt_excess)\")\n",
    "print(\"mc   :\", mc)\n",
    "print(\"closed:\", closed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e3eca1",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the PDF and CDF\n",
    "- Monte Carlo samples (histogram)\n",
    "- empirical CDF vs the theoretical CDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF/CDF and Monte Carlo comparison\n",
    "\n",
    "K, loc, scale = 0.9, -0.3, 1.1\n",
    "rv = stats.exponnorm(K, loc=loc, scale=scale)\n",
    "\n",
    "n = 80_000\n",
    "samples = sample_exponnorm_numpy(n, K=K, loc=loc, scale=scale, rng=rng)\n",
    "\n",
    "x_grid = x_grid_for_plotting(rv, q_low=0.001, q_high=0.999, n=600)\n",
    "\n",
    "# Histogram + PDF\n",
    "fig = px.histogram(\n",
    "    samples,\n",
    "    nbins=80,\n",
    "    histnorm=\"probability density\",\n",
    "    title=f\"Monte Carlo samples vs PDF (n={n:,}, K={K:g}, loc={loc:g}, scale={scale:g})\",\n",
    "    labels={\"value\": \"x\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=rv.pdf(x_grid), mode=\"lines\", name=\"SciPy PDF\"))\n",
    "fig.update_layout(yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Empirical CDF vs theoretical CDF\n",
    "xs, ys = empirical_cdf(samples)\n",
    "\n",
    "x_grid = np.linspace(float(np.quantile(xs, 0.001)), float(np.quantile(xs, 0.999)), 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", name=\"empirical CDF\"))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=rv.cdf(x_grid), mode=\"lines\", name=\"theoretical CDF\"))\n",
    "fig.update_layout(\n",
    "    title=f\"Empirical CDF vs theoretical CDF (n={n:,})\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdf9fe",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy implements this distribution as `scipy.stats.exponnorm`.\n",
    "\n",
    "Useful methods:\n",
    "- `pdf`, `logpdf`\n",
    "- `cdf`, `logcdf`\n",
    "- `sf`, `logsf` (often more accurate in the tail)\n",
    "- `rvs` for sampling\n",
    "- `fit` for MLE parameter estimation\n",
    "\n",
    "**About `fit`**\n",
    "\n",
    "- `exponnorm.fit(data)` estimates \\((K,\\text{loc},\\text{scale})\\) via maximum likelihood.\n",
    "- As \\(K\\to 0\\), the distribution approaches a normal and the MLE can become numerically delicate (boundary effects).\n",
    "- For more control, you can fix parameters (e.g., `floc=...`) or fit via a custom optimizer on `logpdf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SciPy usage + fit demo\n",
    "\n",
    "K, loc, scale = 1.1, -0.4, 0.9\n",
    "rv = stats.exponnorm(K, loc=loc, scale=scale)\n",
    "\n",
    "xs = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"pdf:\", rv.pdf(xs))\n",
    "print(\"cdf:\", rv.cdf(xs))\n",
    "\n",
    "# Sampling via SciPy\n",
    "samples_scipy = rv.rvs(size=5_000, random_state=rng)\n",
    "\n",
    "# Fit parameters back\n",
    "K_hat, loc_hat, scale_hat = stats.exponnorm.fit(samples_scipy)\n",
    "print(\"fit (K, loc, scale):\", (float(K_hat), float(loc_hat), float(scale_hat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95cff23",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### 10.1 Hypothesis testing / anomaly detection\n",
    "\n",
    "If you have a baseline model \\(X\\sim\\mathrm{exponnorm}(K,\\text{loc},\\text{scale})\\), then an unusually large observation \\(x_\\text{obs}\\) can be scored by a right-tail probability:\n",
    "\n",
    "\\[\n",
    "\\text{p-value} = \\Pr(X \\ge x_\\text{obs}) = \\mathrm{sf}(x_\\text{obs}).\n",
    "\\]\n",
    "\n",
    "For model comparison (e.g., “is a normal enough?”), likelihood-ratio statistics are natural — but because \\(K\\ge 0\\) and the normal corresponds to the boundary case \\(K=0\\), a **parametric bootstrap** is a safer way to calibrate the test than relying on a \\(\\chi^2\\) reference.\n",
    "\n",
    "### 10.2 Bayesian modeling\n",
    "\n",
    "A Bayesian model uses the same latent decomposition:\n",
    "\n",
    "\\[\n",
    "x_i = \\mu + \\sigma z_i + d_i,\\quad z_i\\sim\\mathcal{N}(0,1),\\quad d_i\\sim\\mathrm{Exp}(\\lambda).\n",
    "\\]\n",
    "\n",
    "Choose priors like:\n",
    "- \\(\\mu\\sim\\mathcal{N}(m_0,s_0^2)\\)\n",
    "- \\(\\sigma\\sim\\mathrm{HalfNormal}(s)\\)\n",
    "- \\(\\lambda\\sim\\mathrm{Gamma}(\\alpha,\\beta)\\)\n",
    "\n",
    "This representation is convenient in probabilistic programming systems (Stan, PyMC, Turing).\n",
    "\n",
    "### 10.3 Generative modeling\n",
    "\n",
    "`exponnorm` is a useful **component distribution** for skewed data:\n",
    "- mixture models (mixture of exGaussians)\n",
    "- emission models in HMMs for duration-like observations\n",
    "- synthetic data generation for latency / response-time benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Tail probability (anomaly scoring) + bootstrap LRT vs Normal\n",
    "\n",
    "# Tail probability under a baseline model\n",
    "K0, loc0, scale0 = 0.9, -0.3, 1.1\n",
    "rv0 = stats.exponnorm(K0, loc=loc0, scale=scale0)\n",
    "\n",
    "x_obs = float(rv0.ppf(0.995))\n",
    "p_value = float(rv0.sf(x_obs))  # P(X >= x_obs)\n",
    "\n",
    "print(\"x_obs:\", x_obs)\n",
    "print(\"p_value (right tail):\", p_value)\n",
    "\n",
    "\n",
    "# Model comparison: ExponNorm vs Normal via parametric bootstrap\n",
    "n = 400\n",
    "x = rv0.rvs(size=n, random_state=rng)\n",
    "\n",
    "# Fit Normal (2 params)\n",
    "mu_hat, sigma_hat = stats.norm.fit(x)\n",
    "ll_norm = float(np.sum(stats.norm.logpdf(x, loc=mu_hat, scale=sigma_hat)))\n",
    "\n",
    "# Fit ExponNorm (3 params)\n",
    "K_hat, loc_hat, scale_hat = stats.exponnorm.fit(x)\n",
    "ll_expn = float(np.sum(stats.exponnorm.logpdf(x, K_hat, loc=loc_hat, scale=scale_hat)))\n",
    "\n",
    "T_obs = 2.0 * (ll_expn - ll_norm)\n",
    "\n",
    "B = 30  # increase for a more stable p-value\n",
    "T_boot = []\n",
    "for _ in range(B):\n",
    "    xb = stats.norm.rvs(loc=mu_hat, scale=sigma_hat, size=n, random_state=rng)\n",
    "\n",
    "    mu_b, sigma_b = stats.norm.fit(xb)\n",
    "    ll_n_b = float(np.sum(stats.norm.logpdf(xb, loc=mu_b, scale=sigma_b)))\n",
    "\n",
    "    K_b, loc_b, scale_b = stats.exponnorm.fit(xb)\n",
    "    ll_e_b = float(np.sum(stats.exponnorm.logpdf(xb, K_b, loc=loc_b, scale=scale_b)))\n",
    "\n",
    "    T_boot.append(2.0 * (ll_e_b - ll_n_b))\n",
    "\n",
    "p_boot = float(np.mean(np.asarray(T_boot) >= T_obs))\n",
    "\n",
    "print(\"T_obs:\", T_obs)\n",
    "print(\"bootstrap p-value (Normal is enough?):\", p_boot)\n",
    "\n",
    "\n",
    "# 10.3 Simple mixture of two exponnorm components (generative modeling)\n",
    "\n",
    "w = 0.6\n",
    "n = 60_000\n",
    "n1 = int(w * n)\n",
    "n2 = n - n1\n",
    "\n",
    "x1 = sample_exponnorm_numpy(n1, K=0.4, loc=-1.0, scale=0.7, rng=rng)\n",
    "x2 = sample_exponnorm_numpy(n2, K=1.5, loc=1.0, scale=0.9, rng=rng)\n",
    "\n",
    "x_mix = np.concatenate([x1, x2])\n",
    "rng.shuffle(x_mix)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x_mix,\n",
    "    nbins=90,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Mixture of two `exponnorm` components\",\n",
    "    labels={\"value\": \"x\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d03ac",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: SciPy requires `K > 0` and `scale > 0`.\n",
    "- **Parameterization confusion**: many references use \\((\\mu,\\sigma,\\lambda)\\); SciPy uses \\(K=1/(\\sigma\\lambda)\\). Track whether you mean **rate** \\(\\lambda\\) or **mean delay** \\(\\tau=1/\\lambda\\).\n",
    "- **Right-skew only**: `exponnorm` produces positive skew. If your data is left-skewed, consider reflecting the data or using a different family.\n",
    "- **Fitting near \\(K\\approx 0\\)**: the normal is a boundary case; optimization can be sensitive and standard LRT asymptotics may fail.\n",
    "- **Tail numerics**: for extreme \\(x\\), `1 - cdf(x)` can suffer catastrophic cancellation. Prefer `sf` / `logsf`, and prefer `logpdf` over `pdf` in the far tail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical pitfall demo: 1 - cdf vs sf in the far right tail\n",
    "\n",
    "K, loc, scale = 1.2, 0.0, 1.0\n",
    "rv = stats.exponnorm(K, loc=loc, scale=scale)\n",
    "\n",
    "x = 50.0\n",
    "sf_naive = 1.0 - rv.cdf(x)\n",
    "sf_stable = rv.sf(x)\n",
    "logsf = rv.logsf(x)\n",
    "\n",
    "{\"1-cdf\": sf_naive, \"sf\": float(sf_stable), \"logsf\": float(logsf)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b5ae1",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `exponnorm` is a **continuous** distribution on \\(\\mathbb{R}\\) modeling **Normal + Exponential** (noise + delay).\n",
    "- SciPy parameters: \\(K>0\\), `loc`, `scale`; mean \\(= \\text{loc} + K\\,\\text{scale}\\), variance \\(= \\text{scale}^2(1+K^2)\\).\n",
    "- Shape depends only on \\(K\\): skewness \\(\\gamma_1 = 2K^3/(1+K^2)^{3/2}\\), excess kurtosis \\(\\gamma_2 = 6K^4/(1+K^2)^2\\).\n",
    "- Sampling is simple with NumPy: `normal + exponential`, then an affine transform.\n",
    "- In practice, prefer `logpdf`, `logcdf`, and `sf`/`logsf` for tail computations, and treat `fit` near \\(K\\approx 0\\) with care.\n",
    "\n",
    "### References\n",
    "- SciPy docstring: `scipy.stats.exponnorm`\n",
    "- Exponentially modified Gaussian distribution (Wikipedia)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
