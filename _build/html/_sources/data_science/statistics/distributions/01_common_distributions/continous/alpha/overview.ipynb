{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha distribution (`alpha`)\n",
    "\n",
    "The **Alpha distribution** in SciPy (`scipy.stats.alpha`) is a *continuous* distribution on $(0,\\infty)$ built from a simple transformation of a **truncated standard normal**.\n",
    "\n",
    "Despite the generic name, this is not “the alpha parameter” from other contexts (e.g., Dirichlet/Beta). It’s its own distribution with a distinctive **$1/x^2$ tail**, which implies that the mean and variance are infinite.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Write down the PDF/CDF and understand where they come from.\n",
    "- Build intuition via the truncated-normal generative story.\n",
    "- Understand which moments exist (and which diverge).\n",
    "- Sample from the distribution with a NumPy-only algorithm.\n",
    "- Use `scipy.stats.alpha` for evaluation and fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.stats import alpha as alpha_dist\n",
    "from scipy.stats import norm\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Record versions for reproducibility (useful when numerical details matter).\n",
    "VERSIONS = {\"numpy\": np.__version__, \"scipy\": scipy.__version__, \"plotly\": plotly.__version__}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "- **Name**: `alpha` (Alpha distribution; SciPy: `scipy.stats.alpha`)\n",
    "- **Type**: Continuous\n",
    "- **Support (standard form)**: $x \\in (0,\\infty)$\n",
    "- **Parameter space (standard form)**: shape $a > 0$\n",
    "- **SciPy location/scale**: `loc \\in \\mathbb{R}`, `scale > 0` with\n",
    "  $$X = \\text{loc} + \\text{scale}\\,Y, \\qquad Y \\sim \\mathrm{Alpha}(a).$$\n",
    "\n",
    "Unless stated otherwise, this notebook works with the **standard form** (`loc=0`, `scale=1`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What it models\n",
    "\n",
    "A helpful way to think about the Alpha distribution is through its **generative story**:\n",
    "\n",
    "1. Draw a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$.\n",
    "2. Condition it to be below a threshold: $Z \\mid (Z \\le a)$.\n",
    "3. Convert “distance to the threshold” into a positive quantity and take a reciprocal:\n",
    "   $$X = \\frac{1}{a - Z}.$$\n",
    "\n",
    "If $Z$ lands **very close to $a$ from below**, then $(a-Z)$ is tiny and $X$ becomes huge. That rare-but-possible event produces a **heavy right tail**.\n",
    "\n",
    "### Typical real-world use cases\n",
    "\n",
    "The Alpha distribution appears in the reliability literature as a model for **positive quantities with occasional extreme values** (e.g., lifetimes, repair times, stress/strength-like ratios after a transformation). It is most appropriate when:\n",
    "\n",
    "- values are strictly positive;\n",
    "- the right tail can be very long (outliers are not just noise);\n",
    "- you want a model with a clear “threshold proximity” interpretation via the truncated normal story.\n",
    "\n",
    "### Relations to other distributions\n",
    "\n",
    "- **Truncated normal**: $Z \\mid (Z \\le a)$ is the core latent variable.\n",
    "- **Reciprocal transform**: $X$ is a reciprocal of $(a-Z)$.\n",
    "- **Pareto-like tail**: the Alpha PDF behaves like $\\text{const} \\cdot x^{-2}$ for large $x$, which implies $\\mathbb{P}(X>x) \\approx \\text{const} \\cdot x^{-1}$.\n",
    "  This is heavy enough that $\\mathbb{E}[X]$ diverges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "Let $\\phi$ and $\\Phi$ denote the standard normal PDF and CDF:\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\tfrac{1}{2}z^2\\right),\n",
    "\\qquad\n",
    "\\Phi(z) = \\int_{-\\infty}^z \\phi(t)\\,dt.\n",
    "$$\n",
    "\n",
    "### PDF\n",
    "\n",
    "For $x>0$ and $a>0$, the Alpha distribution has PDF\n",
    "\n",
    "$$\n",
    "f(x; a) = \\frac{1}{x^2\\,\\Phi(a)}\\,\\phi\\!\\left(a - \\frac{1}{x}\\right).\n",
    "$$\n",
    "\n",
    "### CDF\n",
    "\n",
    "For $x>0$,\n",
    "\n",
    "$$\n",
    "F(x; a) = \\frac{\\Phi\\!\\left(a - \\frac{1}{x}\\right)}{\\Phi(a)},\n",
    "$$\n",
    "\n",
    "and $F(x;a)=0$ for $x\\le 0$.\n",
    "\n",
    "### Quantile function (PPF)\n",
    "\n",
    "Solving $q = F(x;a)$ for $x$ gives\n",
    "\n",
    "$$\n",
    "F^{-1}(q;a) = \\frac{1}{a - \\Phi^{-1}\\!\\bigl(q\\,\\Phi(a)\\bigr)}\n",
    "\\qquad (0<q<1).\n",
    "$$\n",
    "\n",
    "This matches SciPy’s internal implementation of `alpha.ppf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_pdf(x: np.ndarray, a: float) -> np.ndarray:\n",
    "    \"\"\"PDF of the standard Alpha(a) distribution (loc=0, scale=1).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x > 0\n",
    "    xa = x[mask]\n",
    "    out[mask] = norm.pdf(a - 1.0 / xa) / (xa**2 * norm.cdf(a))\n",
    "    return out\n",
    "\n",
    "\n",
    "def alpha_logpdf(x: np.ndarray, a: float) -> np.ndarray:\n",
    "    \"\"\"Log-PDF of the standard Alpha(a) distribution (more stable in the tail).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.full_like(x, -np.inf, dtype=float)\n",
    "    mask = x > 0\n",
    "    xa = x[mask]\n",
    "    out[mask] = norm.logpdf(a - 1.0 / xa) - 2.0 * np.log(xa) - np.log(norm.cdf(a))\n",
    "    return out\n",
    "\n",
    "\n",
    "def alpha_cdf(x: np.ndarray, a: float) -> np.ndarray:\n",
    "    \"\"\"CDF of the standard Alpha(a) distribution.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    out = np.zeros_like(x, dtype=float)\n",
    "    mask = x > 0\n",
    "    xa = x[mask]\n",
    "    out[mask] = norm.cdf(a - 1.0 / xa) / norm.cdf(a)\n",
    "    return out\n",
    "\n",
    "\n",
    "def alpha_ppf(q: np.ndarray, a: float) -> np.ndarray:\n",
    "    \"\"\"Quantile function (inverse CDF) of the standard Alpha(a) distribution.\"\"\"\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    return 1.0 / (a - norm.ppf(q * norm.cdf(a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: our formulas match SciPy.\n",
    "a = 1.7\n",
    "x = np.logspace(-3, 2, 25)\n",
    "\n",
    "assert np.allclose(alpha_pdf(x, a), alpha_dist.pdf(x, a))\n",
    "assert np.allclose(alpha_cdf(x, a), alpha_dist.cdf(x, a))\n",
    "assert np.allclose(alpha_ppf(np.linspace(0.01, 0.99, 9), a), alpha_dist.ppf(np.linspace(0.01, 0.99, 9), a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Tail behavior\n",
    "\n",
    "As $x\\to\\infty$, we have $a - 1/x \\to a$, so\n",
    "\n",
    "$$\n",
    "f(x;a)\n",
    "= \\frac{1}{x^2\\,\\Phi(a)}\\,\\phi\\!\\left(a - \\frac{1}{x}\\right)\n",
    "\\sim \\frac{\\phi(a)}{\\Phi(a)}\\,\\frac{1}{x^2}.\n",
    "$$\n",
    "\n",
    "Consequently,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X>x) = \\int_x^{\\infty} f(t;a)\\,dt \\sim \\frac{\\phi(a)}{\\Phi(a)}\\,\\frac{1}{x}.\n",
    "$$\n",
    "\n",
    "This is a **power-law tail** with exponent 1 for the survival function.\n",
    "\n",
    "### Mean / variance / skewness / kurtosis\n",
    "\n",
    "Because the tail is so heavy:\n",
    "\n",
    "- $\\mathbb{E}[X] = \\infty$ (diverges logarithmically)\n",
    "- $\\mathrm{Var}(X) = \\infty$ (since $\\mathbb{E}[X^2] = \\infty$)\n",
    "- skewness and kurtosis are **undefined** (they require finite third/fourth central moments)\n",
    "\n",
    "More generally, the power-law tail implies:\n",
    "\n",
    "- $\\mathbb{E}[X^p]$ is finite for $p < 1$\n",
    "- $\\mathbb{E}[X^p]$ diverges for $p \\ge 1$\n",
    "\n",
    "### MGF / characteristic function\n",
    "\n",
    "- The **moment generating function** $M_X(t)=\\mathbb{E}[e^{tX}]$ does **not** exist for any $t>0$.\n",
    "- The **Laplace transform** $\\mathbb{E}[e^{tX}]$ *does* exist for $t<0$ (and can be computed numerically).\n",
    "- The **characteristic function** $\\varphi_X(t)=\\mathbb{E}[e^{itX}]$ exists for all real $t$ because $|e^{itX}|\\le 1$.\n",
    "\n",
    "A useful integral representation comes from the truncated-normal story. If $Z\\sim\\mathcal{N}(0,1)$ conditioned on $Z\\le a$ and $X=1/(a-Z)$, then\n",
    "\n",
    "$$\n",
    "\\varphi_X(t)\n",
    "= \\mathbb{E}[e^{itX}]\n",
    "= \\frac{1}{\\Phi(a)}\\int_{-\\infty}^{a} \\exp\\!\\left(\\frac{it}{a-z}\\right)\\,\\phi(z)\\,dz.\n",
    "$$\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The (differential) entropy is\n",
    "\n",
    "$$\n",
    "h(X) = -\\int_0^{\\infty} f(x;a)\\,\\log f(x;a)\\,dx,\n",
    "$$\n",
    "\n",
    "which is finite and available via `scipy.stats.alpha.entropy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.0\n",
    "\n",
    "mean, var, skew, kurt = alpha_dist.stats(a, moments=\"mvsk\")\n",
    "entropy = alpha_dist.entropy(a)\n",
    "\n",
    "print(\"SciPy stats (a=1.0):\")\n",
    "print(\"  mean   =\", mean)\n",
    "print(\"  var    =\", var)\n",
    "print(\"  skew   =\", skew)\n",
    "print(\"  kurt   =\", kurt)\n",
    "print(\"  entropy=\", entropy)\n",
    "\n",
    "qs = [0.5, 0.9, 0.99]\n",
    "print(\"\\nSelected quantiles:\")\n",
    "for q in qs:\n",
    "    print(f\"  q={q:>4}: {alpha_dist.ppf(q, a):.5f}\")\n",
    "\n",
    "# Empirical mean is unstable (finite for finite samples, but does not converge).\n",
    "print(\"\\nEmpirical summaries (same a, increasing n):\")\n",
    "for n in [200, 2_000, 20_000]:\n",
    "    x = alpha_dist.rvs(a, size=n, random_state=rng)\n",
    "    print(\n",
    "        f\"  n={n:>6}: mean={x.mean():.3f}, median={np.median(x):.3f}, 95%={np.quantile(x, 0.95):.3f}, max={x.max():.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The single shape parameter $a>0$ plays two linked roles:\n",
    "\n",
    "1. It sets the **truncation point** for the latent normal: $Z\\mid(Z\\le a)$.\n",
    "2. It appears in the reciprocal transform $X = 1/(a-Z)$.\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "- Larger $a$ increases the typical size of $(a-Z)$, so $X$ tends to be **smaller**.\n",
    "- Smaller $a$ makes it easier for $Z$ to land near $a$ (from below), producing **more extreme** $X$ values.\n",
    "\n",
    "The tail constant is proportional to $\\phi(a)/\\Phi(a)$; this ratio decreases rapidly as $a$ grows, so the far tail becomes less prominent for large $a$ (even though the asymptotic power $x^{-2}$ remains).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.logspace(-3, 2, 600)\n",
    "a_values = [0.5, 1.0, 2.0, 4.0]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in a_values:\n",
    "    fig.add_trace(go.Scatter(x=x, y=alpha_pdf(x, a), mode=\"lines\", name=f\"a={a}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Alpha PDF for different a (log x-axis)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x; a)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# Tail view: survival function on log-log axes (highlights the ~1/x behavior).\n",
    "x_tail = np.logspace(-1, 3, 600)\n",
    "fig = go.Figure()\n",
    "for a in a_values:\n",
    "    sf = 1.0 - alpha_cdf(x_tail, a)\n",
    "    fig.add_trace(go.Scatter(x=x_tail, y=sf, mode=\"lines\", name=f\"a={a}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Alpha survival function on log-log axes\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X > x)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_yaxes(type=\"log\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.logspace(-3, 2, 600)\n",
    "a_values = [0.5, 1.0, 2.0, 4.0]\n",
    "\n",
    "fig = go.Figure()\n",
    "for a in a_values:\n",
    "    fig.add_trace(go.Scatter(x=x, y=alpha_cdf(x, a), mode=\"lines\", name=f\"a={a}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Alpha CDF for different a (log x-axis)\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"F(x; a)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### 6.1 Deriving the CDF and PDF\n",
    "\n",
    "Start with a truncated standard normal:\n",
    "\n",
    "$$\n",
    "Z \\sim \\mathcal{N}(0,1) \\ \\text{conditioned on}\\ \\{Z\\le a\\}.\n",
    "$$\n",
    "\n",
    "Its density is\n",
    "\n",
    "$$\n",
    "f_Z(z) = \\frac{\\phi(z)}{\\Phi(a)}\\,\\mathbf{1}\\{z\\le a\\}.\n",
    "$$\n",
    "\n",
    "Define $X = 1/(a-Z)$. For $x>0$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F(x;a)\n",
    "  &= \\mathbb{P}\\left(\\frac{1}{a-Z} \\le x\\ \\middle|\\ Z\\le a\\right) \\\\\n",
    "  &= \\mathbb{P}\\left(a-Z \\ge \\frac{1}{x}\\ \\middle|\\ Z\\le a\\right) \\\\\n",
    "  &= \\mathbb{P}\\left(Z \\le a - \\frac{1}{x}\\ \\middle|\\ Z\\le a\\right) \\\\\n",
    "  &= \\frac{\\Phi\\!\\left(a - \\frac{1}{x}\\right)}{\\Phi(a)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Differentiating with respect to $x$ gives\n",
    "\n",
    "$$\n",
    "f(x;a) = \\frac{1}{\\Phi(a)}\\,\\phi\\!\\left(a - \\frac{1}{x}\\right)\\,\\frac{1}{x^2}.\n",
    "$$\n",
    "\n",
    "### 6.2 Why the mean and variance diverge\n",
    "\n",
    "Use the tail asymptotic $f(x;a) \\sim C/x^2$ with $C=\\phi(a)/\\Phi(a)$.\n",
    "\n",
    "- For the mean:\n",
    "  $$\\mathbb{E}[X] = \\int_0^{\\infty} x\\,f(x;a)\\,dx \\ \\text{has integrand}\\ \\sim C/x,$$\n",
    "  and $\\int^\\infty (1/x)\\,dx$ diverges (logarithmically).\n",
    "\n",
    "- For the second moment:\n",
    "  $$\\mathbb{E}[X^2] = \\int_0^{\\infty} x^2\\,f(x;a)\\,dx \\ \\text{has integrand}\\ \\sim C,$$\n",
    "  and $\\int^\\infty 1\\,dx$ diverges.\n",
    "\n",
    "So $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$ are infinite.\n",
    "\n",
    "### 6.3 Likelihood (i.i.d. sample)\n",
    "\n",
    "Given data $x_1,\\dots,x_n$ with $x_i>0$, the likelihood for $a$ (standard form) is\n",
    "\n",
    "$$\n",
    "L(a; x_{1:n}) = \\prod_{i=1}^n \\frac{1}{x_i^2\\,\\Phi(a)}\\,\\phi\\!\\left(a - \\frac{1}{x_i}\\right).\n",
    "$$\n",
    "\n",
    "The log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(a)\n",
    "  = -2\\sum_{i=1}^n \\log x_i\\; -\\; n\\log\\Phi(a)\\; +\\; \\sum_{i=1}^n \\log\\phi\\!\\left(a - \\frac{1}{x_i}\\right).\n",
    "$$\n",
    "\n",
    "Differentiating (using $\\tfrac{d}{da}\\log\\Phi(a)=\\phi(a)/\\Phi(a)$ and $\\tfrac{d}{da}\\log\\phi(u)= -u$) gives the score:\n",
    "\n",
    "$$\n",
    "\\ell'(a)\n",
    "  = -n\\,\\frac{\\phi(a)}{\\Phi(a)}\\; -\\; \\sum_{i=1}^n\\left(a - \\frac{1}{x_i}\\right).\n",
    "$$\n",
    "\n",
    "Setting $\\ell'(a)=0$ yields a nonlinear equation in $a$ (because of $\\phi(a)/\\Phi(a)$), so maximum likelihood estimation typically uses numerical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_loglik(a: float, x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if a <= 0 or np.any(x <= 0):\n",
    "        return -np.inf\n",
    "    # Uses SciPy's stable norm.logpdf implementation.\n",
    "    return float(np.sum(norm.logpdf(a - 1.0 / x) - 2.0 * np.log(x)) - x.size * np.log(norm.cdf(a)))\n",
    "\n",
    "\n",
    "def alpha_score(a: float, x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if a <= 0 or np.any(x <= 0):\n",
    "        return np.nan\n",
    "    return float(-x.size * (norm.pdf(a) / norm.cdf(a)) - np.sum(a - 1.0 / x))\n",
    "\n",
    "\n",
    "# MLE demo in the standard form (loc=0, scale=1) via a grid.\n",
    "a_true = 1.8\n",
    "x = alpha_dist.rvs(a_true, size=3_000, random_state=rng)\n",
    "\n",
    "a_grid = np.linspace(0.05, 6.0, 500)\n",
    "ll = np.array([alpha_loglik(a, x) for a in a_grid])\n",
    "a_hat_grid = a_grid[np.argmax(ll)]\n",
    "\n",
    "fig = go.Figure(go.Scatter(x=a_grid, y=ll - ll.max(), mode=\"lines\"))\n",
    "fig.add_vline(x=a_true, line_dash=\"dash\", line_color=\"green\", annotation_text=\"true a\")\n",
    "fig.add_vline(x=a_hat_grid, line_dash=\"dash\", line_color=\"red\", annotation_text=\"grid MLE\")\n",
    "fig.update_layout(\n",
    "    title=\"Log-likelihood (centered) for a (standard form)\",\n",
    "    xaxis_title=\"a\",\n",
    "    yaxis_title=\"log L(a) - max_a log L(a)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Compare to SciPy's fit when loc/scale are fixed.\n",
    "a_hat_scipy, loc_hat, scale_hat = alpha_dist.fit(x, floc=0, fscale=1)\n",
    "print(\"True a     =\", a_true)\n",
    "print(\"Grid MLE   =\", a_hat_grid)\n",
    "print(\"SciPy fit  =\", a_hat_scipy)\n",
    "print(\"(loc,scale)=\", (loc_hat, scale_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only algorithm\n",
    "\n",
    "Use the truncated-normal representation:\n",
    "\n",
    "1. Sample $Z \\sim \\mathcal{N}(0,1)$ until $Z\\le a$ (rejection sampling for a one-sided truncation).\n",
    "2. Transform $X = 1/(a-Z)$.\n",
    "\n",
    "Because $a>0$, the acceptance probability is $\\mathbb{P}(Z\\le a)=\\Phi(a)\\ge 1/2$, so this rejection sampler is typically efficient.\n",
    "\n",
    "We implement it below using **only NumPy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_rvs_numpy(a: float, size=1, *, rng: np.random.Generator | None = None) -> np.ndarray:\n",
    "    \"\"\"Draw samples from Alpha(a) using only NumPy.\n",
    "\n",
    "    Algorithm:\n",
    "      - sample Z ~ N(0,1) until Z <= a (one-sided truncation)\n",
    "      - return X = 1/(a - Z)\n",
    "    \"\"\"\n",
    "    if a <= 0:\n",
    "        raise ValueError(\"a must be > 0\")\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    size_tuple = (size,) if np.isscalar(size) else tuple(size)\n",
    "    n = int(np.prod(size_tuple))\n",
    "    out = np.empty(n, dtype=float)\n",
    "\n",
    "    filled = 0\n",
    "    while filled < n:\n",
    "        # Oversample to reduce loop overhead.\n",
    "        m = max(256, 2 * (n - filled))\n",
    "        z = rng.normal(size=m)\n",
    "        z = z[z <= a]\n",
    "        if z.size == 0:\n",
    "            continue\n",
    "        take = min(z.size, n - filled)\n",
    "        out[filled : filled + take] = 1.0 / (a - z[:take])\n",
    "        filled += take\n",
    "\n",
    "    return out.reshape(size_tuple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll compare:\n",
    "\n",
    "- the theoretical PDF and CDF\n",
    "- Monte Carlo samples (NumPy-only sampler)\n",
    "- SciPy’s implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.0\n",
    "n = 50_000\n",
    "\n",
    "x_np = alpha_rvs_numpy(a, size=n, rng=rng)\n",
    "x_sp = alpha_dist.rvs(a, size=n, random_state=rng)\n",
    "\n",
    "# Histogram vs PDF\n",
    "x_grid = np.logspace(-3, 2, 500)\n",
    "pdf_grid = alpha_pdf(x_grid, a)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=x_np,\n",
    "    nbins=120,\n",
    "    histnorm=\"probability density\",\n",
    "    title=\"Monte Carlo histogram (NumPy-only) vs theoretical PDF\",\n",
    "    labels={\"x\": \"x\"},\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=pdf_grid, mode=\"lines\", name=\"theoretical PDF\"))\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# Empirical CDF vs theoretical CDF\n",
    "x_sorted = np.sort(x_np)\n",
    "ecdf = np.arange(1, n + 1) / n\n",
    "cdf_grid = alpha_cdf(x_grid, a)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_sorted, y=ecdf, mode=\"lines\", name=\"empirical CDF (NumPy-only)\"))\n",
    "fig.add_trace(go.Scatter(x=x_grid, y=cdf_grid, mode=\"lines\", name=\"theoretical CDF\"))\n",
    "fig.update_layout(title=\"CDF: empirical vs theoretical\", xaxis_title=\"x\", yaxis_title=\"F(x)\")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# Quick check that NumPy-only samples and SciPy samples look similar (KS statistic).\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "ks = ks_2samp(x_np, x_sp)\n",
    "print(\"KS two-sample test (NumPy vs SciPy samples):\")\n",
    "print(ks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "`scipy.stats.alpha` provides the usual distribution API:\n",
    "\n",
    "- `alpha.pdf(x, a, loc=0, scale=1)`\n",
    "- `alpha.cdf(x, a, loc=0, scale=1)`\n",
    "- `alpha.rvs(a, loc=0, scale=1, size=..., random_state=...)`\n",
    "- `alpha.fit(data, ...)` (MLE)\n",
    "\n",
    "A common pattern is to **freeze** the distribution: `rv = alpha(a, loc=..., scale=...)`, then call `rv.pdf`, `rv.cdf`, `rv.rvs`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "rv = alpha_dist(a)  # frozen, standard form\n",
    "\n",
    "x = np.array([0.1, 0.5, 1.0, 5.0])\n",
    "print(\"pdf:\", rv.pdf(x))\n",
    "print(\"cdf:\", rv.cdf(x))\n",
    "\n",
    "samples = rv.rvs(size=5, random_state=rng)\n",
    "print(\"rvs:\", samples)\n",
    "\n",
    "# Fitting (standard form): fix loc=0, scale=1 and estimate only a.\n",
    "a_true = 1.5\n",
    "data = alpha_dist.rvs(a_true, size=5_000, random_state=rng)\n",
    "a_hat, loc_hat, scale_hat = alpha_dist.fit(data, floc=0, fscale=1)\n",
    "print(\"\\nFit (fixed loc/scale):\")\n",
    "print(\"  true a:\", a_true)\n",
    "print(\"  est  a:\", a_hat)\n",
    "print(\"  (loc, scale):\", (loc_hat, scale_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### Hypothesis testing (goodness-of-fit)\n",
    "\n",
    "If you have a **specified** parameter $a$ (not estimated from the same sample), you can test whether data plausibly comes from an Alpha distribution using a goodness-of-fit test such as Kolmogorov–Smirnov (KS).\n",
    "\n",
    "Caveat: if you estimate $a$ from the data and then run KS on the same data, the usual p-values are no longer exact (you need a corrected procedure or a bootstrap).\n",
    "\n",
    "### Bayesian modeling\n",
    "\n",
    "Because the likelihood $p(x\\mid a)$ is available in closed form, you can put a prior on $a>0$ (e.g., Gamma) and perform Bayesian inference with generic tools:\n",
    "\n",
    "$$\n",
    "p(a\\mid x_{1:n}) \\propto p(x_{1:n}\\mid a)\\,p(a).\n",
    "$$\n",
    "\n",
    "Below we show a simple grid-based posterior computation.\n",
    "\n",
    "### Generative modeling\n",
    "\n",
    "The Alpha distribution can be used as a **heavy-tailed positive noise model** or as a component in a mixture model when you want strictly-positive data with occasional extremes.\n",
    "\n",
    "Because the mean is infinite, summary statistics and loss functions that rely on the mean (e.g., squared error to the mean) can behave poorly; robust summaries (median/quantiles) are often more appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing example: KS test when a is known.\n",
    "from scipy.stats import kstest\n",
    "\n",
    "a = 1.2\n",
    "x = alpha_dist.rvs(a, size=2_000, random_state=rng)\n",
    "\n",
    "D, p_value = kstest(x, alpha_dist(a).cdf)\n",
    "print(\"KS test against Alpha(a=1.2):\")\n",
    "print(\"  D      =\", D)\n",
    "print(\"  p-value=\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian modeling example: grid posterior for a with a Gamma prior.\n",
    "\n",
    "from scipy.stats import gamma as gamma_dist\n",
    "\n",
    "rng_local = np.random.default_rng(123)\n",
    "a_true = 1.8\n",
    "x = alpha_dist.rvs(a_true, size=800, random_state=rng_local)\n",
    "\n",
    "# Prior: a ~ Gamma(k, theta) with support (0, inf)\n",
    "k, theta = 2.0, 1.0\n",
    "\n",
    "a_grid = np.linspace(0.05, 6.0, 800)\n",
    "log_prior = gamma_dist(a=k, scale=theta).logpdf(a_grid)\n",
    "log_like = np.array([alpha_loglik(a, x) for a in a_grid])\n",
    "\n",
    "log_post_unnorm = log_like + log_prior\n",
    "log_post = log_post_unnorm - np.max(log_post_unnorm)\n",
    "post_unnorm = np.exp(log_post)\n",
    "post = post_unnorm / np.trapz(post_unnorm, a_grid)\n",
    "\n",
    "a_map = a_grid[np.argmax(post)]\n",
    "\n",
    "fig = go.Figure(go.Scatter(x=a_grid, y=post, mode=\"lines\"))\n",
    "fig.add_vline(x=a_true, line_dash=\"dash\", line_color=\"green\", annotation_text=\"true a\")\n",
    "fig.add_vline(x=a_map, line_dash=\"dash\", line_color=\"red\", annotation_text=\"MAP\")\n",
    "fig.update_layout(\n",
    "    title=\"Posterior over a (Gamma prior + Alpha likelihood)\",\n",
    "    xaxis_title=\"a\",\n",
    "    yaxis_title=\"posterior density\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"True a =\", a_true)\n",
    "print(\"MAP    =\", a_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative modeling example: heavy-tailed positive \"durations\".\n",
    "# In practice, prefer robust summaries (quantiles) over the mean.\n",
    "\n",
    "a = 1.0\n",
    "durations = alpha_rvs_numpy(a, size=80_000, rng=rng).ravel()\n",
    "\n",
    "print(\"Summaries (a=1.0):\")\n",
    "print(\"  median =\", float(np.median(durations)))\n",
    "print(\"  mean   =\", float(durations.mean()))\n",
    "print(\"  99%    =\", float(np.quantile(durations, 0.99)))\n",
    "print(\"  max    =\", float(durations.max()))\n",
    "\n",
    "# Empirical CCDF on log-log axes; the far tail is close to ~const/x.\n",
    "x_sorted = np.sort(durations)\n",
    "n = x_sorted.size\n",
    "ccdf = 1.0 - np.arange(1, n + 1) / n\n",
    "\n",
    "x0 = float(np.quantile(x_sorted, 0.9))\n",
    "mask = x_sorted >= x0\n",
    "x_tail = x_sorted[mask]\n",
    "ccdf_tail = ccdf[mask]\n",
    "\n",
    "# Reference ~c/x line anchored at x0 using the empirical CCDF.\n",
    "c0 = float(ccdf_tail[0] * x_tail[0])\n",
    "ref = c0 / x_tail\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_tail, y=ccdf_tail, mode=\"lines\", name=\"empirical CCDF (tail)\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_tail,\n",
    "        y=ref,\n",
    "        mode=\"lines\",\n",
    "        name=\"~c/x reference\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Generative example: empirical tail on log-log axes\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"P(X > x)\",\n",
    ")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_yaxes(type=\"log\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Invalid parameters**: $a\\le 0$ is not allowed; the support is $x>0$ in the standard form.\n",
    "- **Infinite mean/variance**: sample means can look “reasonable” for small $n$ but are not stable estimators; prefer medians/quantiles.\n",
    "- **Numerical issues in the tail**:\n",
    "  - use `logpdf` when multiplying many densities or when $x$ is extreme;\n",
    "  - `ppf(q)` for $q$ extremely close to 1 can overflow because the true quantile is enormous.\n",
    "- **Fitting sensitivity**: because extreme values occur, MLE can be sensitive to outliers and optimizer settings; consider robust diagnostics (QQ plots, tail checks) and bootstrap uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- The Alpha distribution (`scipy.stats.alpha`) is a continuous distribution on $(0,\\infty)$ with shape parameter $a>0$.\n",
    "- It can be generated by sampling a truncated normal $Z\\mid(Z\\le a)$ and transforming via $X=1/(a-Z)$.\n",
    "- Its right tail behaves like $\\text{const} \\cdot x^{-2}$, implying $\\mathbb{E}[X]=\\infty$ and $\\mathrm{Var}(X)=\\infty$.\n",
    "- PDF/CDF/PPF have clean expressions in terms of the standard normal $\\phi$ and $\\Phi$.\n",
    "- Sampling is straightforward with a NumPy-only rejection sampler for the one-sided truncated normal.\n",
    "\n",
    "### References\n",
    "\n",
    "- Johnson, Kotz, and Balakrishnan. *Continuous Univariate Distributions, Volume 1* (2nd ed.), Wiley, 1994.\n",
    "- Salvia, A. A. “Reliability applications of the Alpha Distribution.” *IEEE Transactions on Reliability*, 1985.\n",
    "- SciPy documentation: `scipy.stats.alpha`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
