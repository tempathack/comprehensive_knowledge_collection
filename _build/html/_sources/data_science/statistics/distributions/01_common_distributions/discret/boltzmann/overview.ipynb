{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d12cc5",
   "metadata": {},
   "source": [
    "# Boltzmann distribution (`scipy.stats.boltzmann`)\n",
    "\n",
    "In SciPy, the **Boltzmann distribution** is a **truncated discrete exponential** distribution on the integers\n",
    "\n",
    "$$\n",
    "X \\in \\{0,1,\\dots,N-1\\}.\n",
    "$$\n",
    "\n",
    "It shows up whenever probabilities are proportional to an **exponentiated negative “energy”**:\n",
    "\\(\\;p(k)\\propto e^{-\\lambda k}\\;\\) with a hard cutoff at \\(N\\).\n",
    "\n",
    "## What you’ll learn\n",
    "- how the PMF/CDF come from finite geometric series\n",
    "- closed-form mean/variance (and how to get higher moments)\n",
    "- a NumPy-only inverse-CDF sampler\n",
    "- practical usage with `scipy.stats.boltzmann` (and fitting via `scipy.stats.fit`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "np.set_printoptions(precision=6, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064ad88",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name:** `boltzmann`  \n",
    "**Type:** Discrete distribution (truncated discrete exponential)\n",
    "\n",
    "### Support\n",
    "With SciPy’s parameterization (and default `loc=0`):\n",
    "\n",
    "$$\n",
    "\\mathcal{X} = \\{0,1,\\dots,N-1\\}.\n",
    "$$\n",
    "\n",
    "More generally, with a location shift `loc`:\n",
    "\n",
    "$$\n",
    "\\mathcal{X} = \\{\\mathrm{loc},\\, \\mathrm{loc}+1,\\,\\dots,\\,\\mathrm{loc}+N-1\\}.\n",
    "$$\n",
    "\n",
    "### Parameter space\n",
    "SciPy uses two **shape parameters**:\n",
    "\n",
    "- \\(\\lambda > 0\\) (often written \\(\\beta\\) in physics; acts like an *inverse temperature*)\n",
    "- \\(N \\in \\{1,2,3,\\dots\\}\\)\n",
    "\n",
    "So the parameter space is \\((0,\\infty)\\times\\mathbb{N}_{>0}\\) (plus an optional integer `loc`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14e6b9",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "The (Gibbs-)Boltzmann idea is:\n",
    "\n",
    "> **Lower energy states are exponentially more likely.**\n",
    "\n",
    "If state \\(k\\) has “energy” proportional to \\(k\\), then\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X=k) \\propto e^{-\\lambda k}.\n",
    "$$\n",
    "\n",
    "In statistical mechanics, \\(\\lambda\\) is often \\(\\beta = 1/(k_B T)\\), where \\(T\\) is temperature. Higher temperature (smaller \\(\\lambda\\)) flattens the distribution; lower temperature (larger \\(\\lambda\\)) concentrates mass near the minimum-energy state.\n",
    "\n",
    "SciPy’s `boltzmann` is the special case where energies are equally spaced and truncated to \\(k\\in\\{0,\\dots,N-1\\}\\).\n",
    "\n",
    "### Typical real-world use cases\n",
    "- **Thermal equilibrium over discretized energy levels** (toy models, truncated state spaces).\n",
    "- **Softmax / temperature sampling** when scores are linear in an integer index.\n",
    "- **Truncated exponential decay** over discrete ranks: higher rank \\(\\Rightarrow\\) exponentially less likely.\n",
    "- **Discrete maximum-entropy models** under an expected “energy” constraint (finite support).\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Geometric distribution**: if \\(r=e^{-\\lambda}\\), then \\(p(k)\\propto r^k\\). SciPy’s Boltzmann is essentially a *geometric distribution truncated to \\(0,\\dots,N-1\\)*.\n",
    "- **Exponential distribution (continuous analog)**: \\(f(x)\\propto e^{-\\lambda x}\\) on \\([0,\\infty)\\).\n",
    "- **Categorical Gibbs distribution**: for arbitrary energies \\(E_i\\), \\(p(i) \\propto e^{-\\beta E_i}\\) (a softmax with temperature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede60026",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### PMF\n",
    "Let \\(\\lambda>0\\) and integer \\(N\\ge 1\\). Define the partition function\n",
    "\n",
    "$$\n",
    "Z(\\lambda,N) = \\sum_{j=0}^{N-1} e^{-\\lambda j}.\n",
    "$$\n",
    "\n",
    "Then the **probability mass function** is\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X=k\\mid\\lambda,N)\n",
    "= \\frac{e^{-\\lambda k}}{Z(\\lambda,N)}\n",
    "= \\frac{(1-e^{-\\lambda})\\,e^{-\\lambda k}}{1-e^{-\\lambda N}},\n",
    "\\qquad k\\in\\{0,1,\\dots,N-1\\}.\n",
    "$$\n",
    "\n",
    "Outside the support, the PMF is 0.\n",
    "\n",
    "### CDF\n",
    "For real \\(x\\), the CDF can be written using \\(m=\\lfloor x \\rfloor\\):\n",
    "\n",
    "$$\n",
    "F(x) = \\mathbb{P}(X\\le x) =\n",
    "\\begin{cases}\n",
    "0, & x < 0\\\\\n",
    "\\dfrac{1-e^{-\\lambda(m+1)}}{1-e^{-\\lambda N}}, & 0 \\le m < N-1\\\\\n",
    "1, & x \\ge N-1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Location shift (`loc`)\n",
    "SciPy also supports an integer shift `loc`. If \\(Y=X+\\mathrm{loc}\\), then\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y=y) = \\mathbb{P}(X=y-\\mathrm{loc}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9305984",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Boltzmann:\n",
    "    '''Boltzmann (truncated discrete exponential) distribution.\n",
    "\n",
    "    This matches `scipy.stats.boltzmann` with `loc=0`:\n",
    "    - support: {0, 1, ..., N-1}\n",
    "    - pmf is proportional to exp(-lambda_ * k)\n",
    "    '''\n",
    "\n",
    "    lambda_: float\n",
    "    N: int\n",
    "\n",
    "\n",
    "def _validate_params(lambda_: float, N: int) -> tuple[float, int]:\n",
    "    lambda_ = float(lambda_)\n",
    "    if not np.isfinite(lambda_) or lambda_ <= 0:\n",
    "        raise ValueError(f\"lambda_ must be positive and finite; got {lambda_!r}.\")\n",
    "\n",
    "    N_int = int(N)\n",
    "    if N_int != N or N_int <= 0:\n",
    "        raise ValueError(f\"N must be a positive integer; got {N!r}.\")\n",
    "\n",
    "    return lambda_, N_int\n",
    "\n",
    "\n",
    "def boltzmann_logZ(lambda_: float, N: int) -> float:\n",
    "    \"\"\"log Z(lambda_, N) where Z = sum_{k=0}^{N-1} exp(-lambda_ k).\"\"\"\n",
    "    lambda_, N = _validate_params(lambda_, N)\n",
    "    # Z = (1 - exp(-lambda_ N)) / (1 - exp(-lambda_))\n",
    "    num = -np.expm1(-lambda_ * N)  # 1 - exp(-lambda_ N)\n",
    "    den = -np.expm1(-lambda_)  # 1 - exp(-lambda_)\n",
    "    return float(np.log(num) - np.log(den))\n",
    "\n",
    "\n",
    "def boltzmann_logpmf(k, lambda_: float, N: int):\n",
    "    \"\"\"Log-PMF evaluated at k (supports scalar or array).\"\"\"\n",
    "    lambda_, N = _validate_params(lambda_, N)\n",
    "\n",
    "    k = np.asarray(k)\n",
    "    k_int = k.astype(int)\n",
    "    is_int = k_int == k\n",
    "    valid = is_int & (k_int >= 0) & (k_int < N)\n",
    "\n",
    "    out = np.full(k.shape, -np.inf, dtype=float)\n",
    "\n",
    "    log_norm = np.log(-np.expm1(-lambda_)) - np.log(-np.expm1(-lambda_ * N))\n",
    "    out[valid] = log_norm - lambda_ * k_int[valid]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def boltzmann_pmf(k, lambda_: float, N: int):\n",
    "    \"\"\"PMF evaluated at k (supports scalar or array).\"\"\"\n",
    "    return np.exp(boltzmann_logpmf(k, lambda_, N))\n",
    "\n",
    "\n",
    "def boltzmann_cdf(x, lambda_: float, N: int):\n",
    "    \"\"\"CDF evaluated at x (supports scalar or array).\"\"\"\n",
    "    lambda_, N = _validate_params(lambda_, N)\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    m = np.floor(x).astype(int)\n",
    "\n",
    "    out = np.zeros(m.shape, dtype=float)\n",
    "    out[m >= N - 1] = 1.0\n",
    "\n",
    "    valid = (m >= 0) & (m < N)\n",
    "    num = -np.expm1(-lambda_ * (m[valid] + 1))\n",
    "    den = -np.expm1(-lambda_ * N)\n",
    "    out[valid] = num / den\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def boltzmann_mean_var(lambda_: float, N: int) -> tuple[float, float]:\n",
    "    \"\"\"Closed-form mean and variance.\n",
    "\n",
    "    Uses stable expm1-based expressions and switches to the uniform limit\n",
    "    when lambda_ is extremely small.\n",
    "    \"\"\"\n",
    "    lambda_, N = _validate_params(lambda_, N)\n",
    "\n",
    "    if lambda_ < 1e-8:\n",
    "        mean = 0.5 * (N - 1)\n",
    "        var = (N**2 - 1) / 12.0\n",
    "        return float(mean), float(var)\n",
    "\n",
    "    r = np.exp(-lambda_)\n",
    "    rN = np.exp(-lambda_ * N)\n",
    "\n",
    "    one_minus_r = -np.expm1(-lambda_)  # 1 - r\n",
    "    one_minus_rN = -np.expm1(-lambda_ * N)  # 1 - rN\n",
    "\n",
    "    mean = r / one_minus_r - N * rN / one_minus_rN\n",
    "    var = r / (one_minus_r**2) - (N**2) * rN / (one_minus_rN**2)\n",
    "\n",
    "    return float(mean), float(var)\n",
    "\n",
    "\n",
    "def boltzmann_entropy(lambda_: float, N: int) -> float:\n",
    "    \"\"\"Shannon entropy H(X) = -E[log p(X)].\"\"\"\n",
    "    mean, _ = boltzmann_mean_var(lambda_, N)\n",
    "    return float(lambda_ * mean + boltzmann_logZ(lambda_, N))\n",
    "\n",
    "\n",
    "def boltzmann_mgf(t, lambda_: float, N: int):\n",
    "    \"\"\"Moment-generating function M(t) = E[exp(t X)].\n",
    "\n",
    "    For finite N this exists for all real t.\n",
    "    \"\"\"\n",
    "    lambda_, N = _validate_params(lambda_, N)\n",
    "\n",
    "    t = np.asarray(t)\n",
    "    dtype = np.complex128 if np.iscomplexobj(t) else float\n",
    "    t = t.astype(dtype)\n",
    "\n",
    "    def geom_sum(a):\n",
    "        # sum_{k=0}^{N-1} exp(-a k) = (1-exp(-a N))/(1-exp(-a)) with a~0 -> N\n",
    "        num = -np.expm1(-a * N)\n",
    "        den = -np.expm1(-a)\n",
    "        out = num / den\n",
    "        return np.where(np.isclose(a, 0), N, out)\n",
    "\n",
    "    return geom_sum(lambda_ - t) / geom_sum(lambda_)\n",
    "\n",
    "\n",
    "def boltzmann_stats(lambda_: float, N: int) -> dict:\n",
    "    \"\"\"Return common moments/properties as a dict.\"\"\"\n",
    "    ks = np.arange(int(N))\n",
    "    pmf = boltzmann_pmf(ks, lambda_, N)\n",
    "\n",
    "    mean, var = boltzmann_mean_var(lambda_, N)\n",
    "    centered = ks - mean\n",
    "\n",
    "    mu3 = float(np.sum((centered**3) * pmf))\n",
    "    mu4 = float(np.sum((centered**4) * pmf))\n",
    "\n",
    "    skew = mu3 / (var ** 1.5)\n",
    "    excess_kurt = mu4 / (var**2) - 3.0\n",
    "\n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'var': var,\n",
    "        'skew': float(skew),\n",
    "        'excess_kurtosis': float(excess_kurt),\n",
    "        'entropy': boltzmann_entropy(lambda_, N),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efae5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks\n",
    "\n",
    "lambda_, N = 1.4, 19\n",
    "k = np.arange(N)\n",
    "\n",
    "pmf_np = boltzmann_pmf(k, lambda_, N)\n",
    "print('PMF sums to:', pmf_np.sum())\n",
    "\n",
    "pmf_sp = stats.boltzmann.pmf(k, lambda_, N)\n",
    "print('Max |numpy - scipy|:', np.max(np.abs(pmf_np - pmf_sp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5d113",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Partition function\n",
    "\n",
    "$$\n",
    "Z(\\lambda,N) = \\sum_{k=0}^{N-1} e^{-\\lambda k}\n",
    "= \\frac{1-e^{-\\lambda N}}{1-e^{-\\lambda}}.\n",
    "$$\n",
    "\n",
    "### Mean and variance (closed form)\n",
    "A convenient trick is to use **log-partition derivatives**.\n",
    "With \\(Z(\\lambda,N)=\\sum_k e^{-\\lambda k}\\):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = -\\frac{\\partial}{\\partial\\lambda}\\log Z(\\lambda,N),\n",
    "\\qquad\n",
    "\\mathrm{Var}(X) = \\frac{\\partial^2}{\\partial\\lambda^2}\\log Z(\\lambda,N).\n",
    "$$\n",
    "\n",
    "Carrying out the derivatives gives\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\frac{e^{-\\lambda}}{1-e^{-\\lambda}} - \\frac{N e^{-\\lambda N}}{1-e^{-\\lambda N}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X) = \\frac{e^{-\\lambda}}{(1-e^{-\\lambda})^2}\n",
    "- \\frac{N^2 e^{-\\lambda N}}{(1-e^{-\\lambda N})^2}.\n",
    "$$\n",
    "\n",
    "### Skewness and kurtosis\n",
    "Because the support is finite, *all moments exist*. Higher moments can be computed either by\n",
    "\n",
    "- summing \\(\\sum_k k^m\\,p(k)\\) exactly (finite sum), or\n",
    "- differentiating \\(\\log Z\\) further (cumulants).\n",
    "\n",
    "We report **skewness** \\(\\gamma_1 = \\mu_3/\\sigma^3\\) and **excess kurtosis** \\(\\gamma_2 = \\mu_4/\\sigma^4 - 3\\).\n",
    "\n",
    "### MGF / characteristic function\n",
    "Let \\(M(t)=\\mathbb{E}[e^{tX}]\\). Using the same geometric-series idea:\n",
    "\n",
    "$$\n",
    "M(t) = \\frac{\\sum_{k=0}^{N-1} e^{(t-\\lambda)k}}{\\sum_{k=0}^{N-1} e^{-\\lambda k}}\n",
    "= \\frac{Z(\\lambda-t, N)}{Z(\\lambda, N)}.\n",
    "$$\n",
    "\n",
    "The **characteristic function** is \\(\\varphi(\\omega)=M(i\\omega)\\).\n",
    "\n",
    "### Entropy\n",
    "Using \\(\\log p(X)=-\\lambda X - \\log Z\\),\n",
    "\n",
    "$$\n",
    "H(X) = -\\mathbb{E}[\\log p(X)] = \\lambda\\,\\mathbb{E}[X] + \\log Z(\\lambda,N).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50702f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moments: NumPy formulas vs SciPy\n",
    "\n",
    "lambda_, N = 1.4, 19\n",
    "\n",
    "mom_np = boltzmann_stats(lambda_, N)\n",
    "mean_sp, var_sp, skew_sp, kurt_sp = stats.boltzmann.stats(lambda_, N, moments='mvsk')\n",
    "ent_sp = stats.boltzmann.entropy(lambda_, N)\n",
    "\n",
    "print('NumPy moments:', mom_np)\n",
    "print('SciPy mean/var/skew/kurt:', float(mean_sp), float(var_sp), float(skew_sp), float(kurt_sp))\n",
    "print('SciPy entropy:', float(ent_sp))\n",
    "\n",
    "# MGF spot-check: compare closed form M(t) to finite sum\n",
    "k = np.arange(N)\n",
    "pmf = boltzmann_pmf(k, lambda_, N)\n",
    "\n",
    "t = 0.3\n",
    "mgf_sum = float(np.sum(np.exp(t * k) * pmf))\n",
    "mgf_closed = float(np.real(boltzmann_mgf(t, lambda_, N)))\n",
    "print('MGF sum vs closed:', mgf_sum, mgf_closed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb81f37",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "### Meaning of the parameters\n",
    "- \\(\\lambda\\) controls **how fast probability decays** with increasing \\(k\\).\n",
    "  - larger \\(\\lambda\\) \\(\\Rightarrow\\) much more mass near 0 (low “energy”)\n",
    "  - smaller \\(\\lambda\\) \\(\\Rightarrow\\) flatter distribution; as \\(\\lambda\\to 0\\), it approaches a **discrete uniform** on \\(\\{0,\\dots,N-1\\}\\)\n",
    "- \\(N\\) sets the **maximum state** (a hard cutoff). Increasing \\(N\\) extends the right tail and increases the mean/variance.\n",
    "\n",
    "### Shape changes\n",
    "Below, we hold \\(N\\) fixed and vary \\(\\lambda\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PMF shapes for different lambda_\n",
    "\n",
    "N = 25\n",
    "lambdas = [0.1, 0.3, 0.8, 1.5]\n",
    "ks = np.arange(N)\n",
    "\n",
    "rows = []\n",
    "for lam in lambdas:\n",
    "    pmf = boltzmann_pmf(ks, lam, N)\n",
    "    rows.append(\n",
    "        {\n",
    "            'k': ks,\n",
    "            'pmf': pmf,\n",
    "            'lambda_': np.full_like(ks, lam, dtype=float),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = {\n",
    "    'k': np.concatenate([r['k'] for r in rows]),\n",
    "    'pmf': np.concatenate([r['pmf'] for r in rows]),\n",
    "    'lambda_': np.concatenate([r['lambda_'] for r in rows]),\n",
    "}\n",
    "\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x='k',\n",
    "    y='pmf',\n",
    "    color='lambda_',\n",
    "    markers=True,\n",
    "    title=f\"Boltzmann PMF for N={N} (varying lambda_)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title='k', yaxis_title='P(X = k)')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d8883",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation via the log-partition function\n",
    "Start with\n",
    "\n",
    "$$\n",
    "Z(\\lambda,N) = \\sum_{k=0}^{N-1} e^{-\\lambda k}.\n",
    "$$\n",
    "\n",
    "Differentiate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\lambda} Z(\\lambda,N)\n",
    "= \\sum_{k=0}^{N-1} (-k) e^{-\\lambda k}.\n",
    "$$\n",
    "\n",
    "Divide by \\(Z\\) to get a derivative of \\(\\log Z\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\lambda} \\log Z\n",
    "= \\frac{1}{Z}\\,\\frac{\\partial Z}{\\partial\\lambda}\n",
    "= -\\sum_{k=0}^{N-1} k\\,\\frac{e^{-\\lambda k}}{Z}\n",
    "= -\\mathbb{E}[X].\n",
    "$$\n",
    "\n",
    "So \\(\\mathbb{E}[X] = -\\partial_\\lambda \\log Z\\). A second derivative yields\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X) = \\partial_{\\lambda}^2 \\log Z.\n",
    "$$\n",
    "\n",
    "### Likelihood (iid sample)\n",
    "Given observations \\(x_1,\\dots,x_n\\) with known \\(N\\), the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(\\lambda) = \\sum_{i=1}^n \\log p(x_i\\mid\\lambda,N)\n",
    "= -\\lambda \\sum_i x_i - n\\log Z(\\lambda,N).\n",
    "$$\n",
    "\n",
    "The score equation is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial\\lambda} = -\\sum_i x_i - n\\frac{\\partial}{\\partial\\lambda}\\log Z(\\lambda,N)\n",
    "= -n\\bar{x} + n\\,\\mathbb{E}_{\\lambda}[X].\n",
    "$$\n",
    "\n",
    "Setting this to 0 yields a classic exponential-family moment match:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\hat\\lambda}[X] = \\bar{x}.\n",
    "$$\n",
    "\n",
    "Because \\(\\mathbb{E}_{\\lambda}[X]\\) decreases monotonically in \\(\\lambda\\) for \\(\\lambda>0\\), this equation has a unique solution when \\(\\bar{x}\\in(0,(N-1)/2]\\). If \\(\\bar{x}\\) is larger than \\((N-1)/2\\), the best fit under the \\(\\lambda>0\\) constraint occurs near the boundary \\(\\lambda\\to 0\\) (almost-uniform).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmann_loglik(lambda_: float, data: np.ndarray, N: int) -> float:\n",
    "    lambda_, N = _validate_params(lambda_, N)\n",
    "    x = np.asarray(data, dtype=float)\n",
    "    if np.any((x < 0) | (x >= N) | (x != np.floor(x))):\n",
    "        raise ValueError('All observations must be integers in {0,...,N-1}.')\n",
    "\n",
    "    x_sum = float(x.sum())\n",
    "\n",
    "    # log p(x) = log(1-exp(-lambda_)) - lambda_ x - log(1-exp(-lambda_ N))\n",
    "    log_norm = np.log(-np.expm1(-lambda_)) - np.log(-np.expm1(-lambda_ * N))\n",
    "    return float(len(x) * log_norm - lambda_ * x_sum)\n",
    "\n",
    "\n",
    "def boltzmann_mle_lambda(data: np.ndarray, N: int, *, tol: float = 1e-10, max_iter: int = 200) -> float:\n",
    "    \"\"\"MLE for lambda_ with known N under lambda_ > 0.\n",
    "\n",
    "    Solves E_lambda[X] = x̄ with a monotone bisection search.\n",
    "    \"\"\"\n",
    "    N = int(N)\n",
    "    x = np.asarray(data)\n",
    "    xbar = float(np.mean(x))\n",
    "\n",
    "    # boundary cases\n",
    "    mean_uniform = 0.5 * (N - 1)\n",
    "    if xbar >= mean_uniform:\n",
    "        return 0.0  # corresponds to the lambda_ -> 0 limit (uniform)\n",
    "    if xbar <= 0:\n",
    "        return float('inf')\n",
    "\n",
    "    # Find an upper bracket where mean <= xbar\n",
    "    lo = 1e-12\n",
    "    hi = 1.0\n",
    "    while boltzmann_mean_var(hi, N)[0] > xbar:\n",
    "        hi *= 2.0\n",
    "        if hi > 1e6:\n",
    "            break\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        m_mid = boltzmann_mean_var(mid, N)[0]\n",
    "        if m_mid > xbar:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "        if hi - lo < tol * max(1.0, hi):\n",
    "            break\n",
    "\n",
    "    return hi\n",
    "\n",
    "\n",
    "# Demo: recover lambda_ from simulated data\n",
    "true_lambda, N = 1.2, 15\n",
    "x = stats.boltzmann.rvs(true_lambda, N, size=4000, random_state=rng)\n",
    "\n",
    "lambda_hat = boltzmann_mle_lambda(x, N)\n",
    "\n",
    "print('true lambda_ :', true_lambda)\n",
    "print('mle  lambda_ :', lambda_hat)\n",
    "print('sample mean  :', float(np.mean(x)))\n",
    "print('theory mean@true:', boltzmann_mean_var(true_lambda, N)[0])\n",
    "print('theory mean@hat :', boltzmann_mean_var(lambda_hat, N)[0])\n",
    "\n",
    "print('loglik(true):', boltzmann_loglik(true_lambda, x, N))\n",
    "print('loglik(hat) :', boltzmann_loglik(lambda_hat, x, N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34c15c",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "### NumPy-only inverse CDF sampling\n",
    "Because the support is finite, a simple and robust approach is **inverse transform sampling**:\n",
    "\n",
    "1) Compute \\(p(k)\\) for \\(k=0,\\dots,N-1\\)\n",
    "2) Form the CDF: \\(F(k)=\\sum_{j\\le k} p(j)\\)\n",
    "3) Draw \\(U\\sim\\mathrm{Unif}(0,1)\\)\n",
    "4) Return the smallest \\(k\\) such that \\(F(k)\\ge U\\)\n",
    "\n",
    "This is \\(\\mathcal{O}(N)\\) preprocessing and then \\(\\mathcal{O}(\\log N)\\) per sample via binary search (`np.searchsorted`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380553cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp_np(a: np.ndarray) -> float:\n",
    "    '''NumPy-only log-sum-exp for 1D arrays.'''\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    a_max = float(np.max(a))\n",
    "    return float(a_max + np.log(np.sum(np.exp(a - a_max))))\n",
    "\n",
    "\n",
    "def boltzmann_rvs_numpy(lambda_: float, N: int, size: int, *, rng: np.random.Generator) -> np.ndarray:\n",
    "    lambda_, N = _validate_params(lambda_, N)\n",
    "\n",
    "    ks = np.arange(N)\n",
    "    logw = -lambda_ * ks\n",
    "    logw -= logsumexp_np(logw)\n",
    "    p = np.exp(logw)\n",
    "\n",
    "    cdf = np.cumsum(p)\n",
    "    cdf[-1] = 1.0  # protect against roundoff\n",
    "\n",
    "    u = rng.random(size)\n",
    "    return np.searchsorted(cdf, u, side='right')\n",
    "\n",
    "\n",
    "# Monte Carlo check\n",
    "lambda_, N = 0.7, 30\n",
    "samples = boltzmann_rvs_numpy(lambda_, N, size=200_000, rng=rng)\n",
    "\n",
    "mean_theory, var_theory = boltzmann_mean_var(lambda_, N)\n",
    "mean_mc = float(np.mean(samples))\n",
    "var_mc = float(np.var(samples))\n",
    "\n",
    "print('theory mean/var:', mean_theory, var_theory)\n",
    "print('MC     mean/var:', mean_mc, var_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8857a0",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the PMF (bars)\n",
    "- the CDF (step-like curve)\n",
    "- Monte Carlo samples compared to the theoretical PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_, N = 0.6, 25\n",
    "ks = np.arange(N)\n",
    "\n",
    "pmf = boltzmann_pmf(ks, lambda_, N)\n",
    "cdf = boltzmann_cdf(ks, lambda_, N)\n",
    "\n",
    "fig_pmf = go.Figure(\n",
    "    data=[go.Bar(x=ks, y=pmf, name='PMF')],\n",
    "    layout=go.Layout(\n",
    "        title=f\"Boltzmann PMF (lambda_={lambda_}, N={N})\",\n",
    "        xaxis_title='k',\n",
    "        yaxis_title='P(X = k)',\n",
    "    ),\n",
    ")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure(\n",
    "    data=[go.Scatter(x=ks, y=cdf, mode='lines+markers', name='CDF')],\n",
    "    layout=go.Layout(\n",
    "        title=f\"Boltzmann CDF (lambda_={lambda_}, N={N})\",\n",
    "        xaxis_title='k',\n",
    "        yaxis_title='P(X ≤ k)',\n",
    "        yaxis=dict(range=[0, 1.05]),\n",
    "    ),\n",
    ")\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo samples\n",
    "samples = boltzmann_rvs_numpy(lambda_, N, size=50_000, rng=rng)\n",
    "counts = np.bincount(samples, minlength=N)\n",
    "emp_p = counts / counts.sum()\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=ks, y=emp_p, name='Empirical (MC)', opacity=0.7))\n",
    "fig_mc.add_trace(go.Scatter(x=ks, y=pmf, name='Theoretical PMF', mode='lines+markers'))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo vs theory (n={len(samples):,})\",\n",
    "    xaxis_title='k',\n",
    "    yaxis_title='Probability',\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0ce0c",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides `scipy.stats.boltzmann` as an `rv_discrete` distribution.\n",
    "\n",
    "Common methods:\n",
    "- `pmf`, `logpmf`\n",
    "- `cdf`, `sf`\n",
    "- `rvs`\n",
    "- `stats` (mean/var/skew/kurtosis)\n",
    "- `entropy`\n",
    "\n",
    "### Fitting\n",
    "Unlike many continuous distributions, `rv_discrete` distributions don’t expose a `.fit(...)` method.\n",
    "In modern SciPy, you can fit *discrete or continuous* distributions with `scipy.stats.fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fe9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SciPy usage\n",
    "lambda_, N = 1.0, 20\n",
    "rv = stats.boltzmann(lambda_, N)  # frozen distribution\n",
    "\n",
    "ks = np.arange(N)\n",
    "print('pmf[:5]:', rv.pmf(ks[:5]))\n",
    "print('cdf[:5]:', rv.cdf(ks[:5]))\n",
    "\n",
    "s = rv.rvs(size=10, random_state=rng)\n",
    "print('rvs:', s)\n",
    "\n",
    "mean_sp, var_sp = rv.stats(moments='mv')\n",
    "print('mean/var:', float(mean_sp), float(var_sp))\n",
    "\n",
    "# Fit using scipy.stats.fit (fix N and loc, estimate lambda_)\n",
    "true_lambda, true_N = 1.2, 15\n",
    "x = stats.boltzmann.rvs(true_lambda, true_N, size=2500, random_state=rng)\n",
    "\n",
    "fit_res = stats.fit(\n",
    "    stats.boltzmann,\n",
    "    x,\n",
    "    bounds={\n",
    "        'lambda_': (1e-6, 10.0),\n",
    "        'N': (true_N, true_N),  # keep fixed (common in practice)\n",
    "        'loc': (0, 0),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(fit_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723600ac",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### (A) Hypothesis testing: likelihood ratio test (LRT)\n",
    "Suppose \\(N\\) is known and you want to test\n",
    "\n",
    "- \\(H_0: \\lambda = \\lambda_0\\)\n",
    "- \\(H_1: \\lambda\\) free\n",
    "\n",
    "The LRT statistic is\n",
    "\n",
    "$$\n",
    "\\Lambda = 2\\big(\\ell(\\hat\\lambda) - \\ell(\\lambda_0)\\big)\n",
    "\\overset{\\cdot}{\\sim} \\chi^2_1\n",
    "$$\n",
    "\n",
    "for large samples (Wilks’ theorem).\n",
    "\n",
    "### (B) Bayesian modeling: grid posterior over \\(\\lambda\\)\n",
    "Because \\(\\lambda\\) is 1D, you can do simple grid Bayes:\n",
    "\n",
    "$$\n",
    "\\log p(\\lambda\\mid x) = \\log p(\\lambda) + \\ell(\\lambda) + C.\n",
    "$$\n",
    "\n",
    "### (C) Generative modeling: Gibbs / softmax over energies\n",
    "More generally, for a *finite set of energies* \\(E_i\\):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(i) = \\frac{e^{-\\beta E_i}}{\\sum_j e^{-\\beta E_j}}.\n",
    "$$\n",
    "\n",
    "This is exactly a **softmax with temperature** \\(T=1/\\beta\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Likelihood ratio test example\n",
    "\n",
    "N = 20\n",
    "lambda0 = 0.8\n",
    "\n",
    "# Simulate under an alternative\n",
    "true_lambda = 1.1\n",
    "x = stats.boltzmann.rvs(true_lambda, N, size=4000, random_state=rng)\n",
    "\n",
    "lambda_hat = boltzmann_mle_lambda(x, N)\n",
    "\n",
    "lrt = 2.0 * (boltzmann_loglik(lambda_hat, x, N) - boltzmann_loglik(lambda0, x, N))\n",
    "p_value = stats.chi2.sf(lrt, df=1)\n",
    "\n",
    "print('lambda0:', lambda0)\n",
    "print('lambda_hat:', lambda_hat)\n",
    "print('LRT statistic:', lrt)\n",
    "print('p-value:', float(p_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B) Bayesian grid posterior for lambda_\n",
    "\n",
    "N = 20\n",
    "x = stats.boltzmann.rvs(1.0, N, size=200, random_state=rng)\n",
    "\n",
    "lam_grid = np.linspace(1e-3, 4.0, 600)\n",
    "\n",
    "# Example prior: Gamma(shape=2, scale=1) on lambda_ (mean=2)\n",
    "prior = stats.gamma(a=2.0, scale=1.0)\n",
    "log_prior = prior.logpdf(lam_grid)\n",
    "\n",
    "log_like = np.array([boltzmann_loglik(lam, x, N) for lam in lam_grid])\n",
    "log_post_unnorm = log_prior + log_like\n",
    "log_post_unnorm -= np.max(log_post_unnorm)\n",
    "post = np.exp(log_post_unnorm)\n",
    "post /= np.trapz(post, lam_grid)\n",
    "\n",
    "# Posterior summaries\n",
    "post_mean = float(np.trapz(lam_grid * post, lam_grid))\n",
    "cdf = np.cumsum(post)\n",
    "cdf /= cdf[-1]\n",
    "ci_low = float(np.interp(0.025, cdf, lam_grid))\n",
    "ci_high = float(np.interp(0.975, cdf, lam_grid))\n",
    "\n",
    "print('posterior mean:', post_mean)\n",
    "print('95% credible interval:', (ci_low, ci_high))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=lam_grid, y=post, mode='lines', name='posterior'))\n",
    "fig.add_vline(x=post_mean, line_dash='dash', line_color='black', annotation_text='posterior mean')\n",
    "fig.update_layout(title='Posterior over lambda_ (grid approximation)', xaxis_title='lambda_', yaxis_title='density')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Gibbs / softmax sampling over arbitrary energies\n",
    "\n",
    "energies = np.array([0.0, 0.5, 1.0, 2.0, 3.0])\n",
    "labels = [f'state {i}' for i in range(len(energies))]\n",
    "\n",
    "betas = [0.5, 1.0, 2.0]  # inverse temperatures\n",
    "\n",
    "fig = go.Figure()\n",
    "for beta in betas:\n",
    "    w = np.exp(-beta * energies)\n",
    "    p = w / w.sum()\n",
    "    fig.add_trace(go.Scatter(x=labels, y=p, mode='lines+markers', name=f'beta={beta}'))\n",
    "\n",
    "fig.update_layout(title='Gibbs probabilities vs inverse temperature', xaxis_title='state', yaxis_title='probability')\n",
    "fig.show()\n",
    "\n",
    "# Sample from one setting\n",
    "beta = 1.5\n",
    "p = np.exp(-beta * energies)\n",
    "p = p / p.sum()\n",
    "\n",
    "draws = rng.choice(len(energies), size=20, p=p)\n",
    "print('draws:', draws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94775604",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "- **Parameter constraints (SciPy):** `lambda_` must be \\(>0\\); `N` must be a positive integer.\n",
    "- **Support mismatch:** data must be integers in \\(\\{0,\\dots,N-1\\}\\) (or shifted by `loc`).\n",
    "- **Fitting `N`:** if `N` is unknown, estimating it from finite data can be unstable; using `max(data)+1` can underestimate the true cutoff.\n",
    "- **Small \\(\\lambda\\) numerical cancellation:** expressions like \\(1-e^{-\\lambda}\\) lose precision when \\(\\lambda\\) is tiny; use `expm1` (`-np.expm1(-x)`) or switch to the uniform limit.\n",
    "- **CDF rounding:** ensure the last CDF value is exactly 1.0 before `searchsorted` sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny-lambda numerical pitfall demo\n",
    "\n",
    "lam = 1e-12\n",
    "naive = 1 - np.exp(-lam)\n",
    "stable = -np.expm1(-lam)\n",
    "print('1 - exp(-lam) naive :', naive)\n",
    "print('1 - exp(-lam) stable:', stable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c86724",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `boltzmann` is a **discrete**, **finite-support** distribution with \\(p(k)\\propto e^{-\\lambda k}\\) on \\(\\{0,\\dots,N-1\\}\\).\n",
    "- The PMF/CDF come directly from **finite geometric series**.\n",
    "- Mean/variance have clean closed forms via **derivatives of** \\(\\log Z\\).\n",
    "- Sampling is straightforward with **inverse CDF** on the finite support.\n",
    "- SciPy provides `pmf/cdf/rvs/stats/entropy`; for fitting, use `scipy.stats.fit` or solve the MLE moment equation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
