{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4f1ed4",
   "metadata": {},
   "source": [
    "# `poisson` (Poisson distribution)\n",
    "\n",
    "The **Poisson** distribution models the number of events that occur in a fixed *exposure* (time, area, volume, etc.) when events happen independently at a constant average rate.\n",
    "\n",
    "This notebook uses the same parameterization as `scipy.stats.poisson`:\n",
    "- `mu` (often written \\(\\lambda\\)) = expected count in the exposure, \\(\\mu \\ge 0\\)\n",
    "\n",
    "## Learning goals\n",
    "By the end you should be able to:\n",
    "- recognize when a Poisson model is appropriate (and when it isn’t)\n",
    "- write down the PMF/CDF and key properties\n",
    "- derive the mean, variance, and likelihood / MLE\n",
    "- implement Poisson sampling using **NumPy only**\n",
    "- visualize PMF/CDF and validate with Monte Carlo simulation\n",
    "- use `scipy.stats.poisson` for computation and basic estimation workflows\n",
    "\n",
    "## Table of contents\n",
    "1. Title & Classification\n",
    "2. Intuition & Motivation\n",
    "3. Formal Definition\n",
    "4. Moments & Properties\n",
    "5. Parameter Interpretation\n",
    "6. Derivations\n",
    "7. Sampling & Simulation\n",
    "8. Visualization\n",
    "9. SciPy Integration\n",
    "10. Statistical Use Cases\n",
    "11. Pitfalls\n",
    "12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1874e",
   "metadata": {},
   "source": [
    "## 1) Title & Classification\n",
    "\n",
    "**Name**: `poisson` (Poisson distribution)  \n",
    "**Type**: **Discrete**  \n",
    "**Support**: \\(k \\in \\{0, 1, 2, \\dots\\}\\)  \n",
    "**Parameter space**: \\(\\lambda \\in [0, \\infty)\\) (called `mu` in SciPy)\n",
    "\n",
    "A useful interpretation is \\(\\lambda = r \\times \\text{exposure}\\), where \\(r\\) is an event rate per unit exposure (e.g., “calls per hour”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68621ab",
   "metadata": {},
   "source": [
    "## 2) Intuition & Motivation\n",
    "\n",
    "### What this distribution models\n",
    "A Poisson random variable \\(X\\) counts how many events occur in a fixed exposure when:\n",
    "- events occur one-at-a-time,\n",
    "- the event rate is approximately constant over the exposure,\n",
    "- counts in disjoint sub-intervals are independent (**independent increments**).\n",
    "\n",
    "A canonical construction is the **Poisson process**: if events arrive at rate \\(r\\) per unit time, then the number of arrivals in an interval of length \\(T\\) is\n",
    "\\[\n",
    "X \\sim \\text{Poisson}(\\lambda), \\qquad \\lambda = rT.\n",
    "\\]\n",
    "\n",
    "### Typical real-world use cases\n",
    "- call arrivals to a helpdesk per minute\n",
    "- defects per meter of manufactured material\n",
    "- photons counted by a sensor in a fixed time window\n",
    "- insurance claims per policy-year\n",
    "- mutations in a stretch of DNA\n",
    "- web requests to an endpoint per second\n",
    "\n",
    "In applied modeling, \\(\\lambda\\) often depends on covariates (Poisson regression / GLMs) and on exposure (an *offset*).\n",
    "\n",
    "### Relations to other distributions\n",
    "- **Binomial limit (rare events)**: if \\(X_n \\sim \\text{Bin}(n, p)\\) with large \\(n\\), small \\(p\\), and \\(np \\to \\lambda\\), then \\(X_n \\Rightarrow \\text{Poisson}(\\lambda)\\).\n",
    "- **Exponential / Gamma waiting times**: in a Poisson process with rate \\(r\\), inter-arrival times are \\(\\text{Exp}(r)\\), and the waiting time to the \\(k\\)-th event is \\(\\text{Gamma}(k, r)\\).\n",
    "- **Additivity**: if \\(X_i \\sim \\text{Poisson}(\\lambda_i)\\) are independent, then \\(\\sum_i X_i \\sim \\text{Poisson}(\\sum_i \\lambda_i)\\).\n",
    "- **Thinning**: if you keep each event independently with probability \\(p\\), the kept count is \\(\\text{Poisson}(p\\lambda)\\).\n",
    "- **Gamma–Poisson mixture**: if \\(\\lambda\\) is random with a Gamma distribution, the marginal count is **Negative Binomial** (useful for over-dispersion).\n",
    "- **Normal approximation**: for large \\(\\lambda\\), \\(\\text{Poisson}(\\lambda)\\) is close to \\(\\mathcal{N}(\\lambda, \\lambda)\\) with a continuity correction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488c3db",
   "metadata": {},
   "source": [
    "## 3) Formal Definition\n",
    "\n",
    "### PMF\n",
    "For \\(\\lambda \\ge 0\\) and \\(k \\in \\{0,1,2,\\dots\\}\\), the probability mass function (PMF) is\n",
    "\\[\n",
    "\\Pr(X = k \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^k}{k!}.\n",
    "\\]\n",
    "\n",
    "### CDF\n",
    "The cumulative distribution function (CDF) is\n",
    "\\[\n",
    "F(k;\\lambda) = \\Pr(X \\le k) = e^{-\\lambda} \\sum_{j=0}^{k} \\frac{\\lambda^j}{j!}.\n",
    "\\]\n",
    "\n",
    "Using the **upper incomplete gamma function** \\(\\Gamma(s, x)\\), we can write\n",
    "\\[\n",
    "F(k;\\lambda) = \\frac{\\Gamma(k+1, \\lambda)}{\\Gamma(k+1)} = Q(k+1, \\lambda),\n",
    "\\]\n",
    "where \\(Q\\) is the regularized upper incomplete gamma function (implemented in SciPy as `scipy.special.gammaincc`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad128158",
   "metadata": {},
   "source": [
    "## 4) Moments & Properties\n",
    "\n",
    "### Mean and variance\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\lambda, \\qquad \\mathrm{Var}(X) = \\lambda.\n",
    "\\]\n",
    "A key modeling implication is **equidispersion**: mean equals variance.\n",
    "\n",
    "### Skewness and kurtosis\n",
    "For \\(\\lambda > 0\\):\n",
    "\\[\n",
    "\\text{skewness} = \\frac{1}{\\sqrt{\\lambda}},\n",
    "\\qquad\n",
    "\\text{excess kurtosis} = \\frac{1}{\\lambda}\n",
    "\\quad(\\text{kurtosis} = 3 + \\tfrac{1}{\\lambda}).\n",
    "\\]\n",
    "\n",
    "### MGF and characteristic function\n",
    "The moment generating function (MGF) and characteristic function are\n",
    "\\[\n",
    "M_X(t) = \\mathbb{E}[e^{tX}] = \\exp\\big(\\lambda(e^t - 1)\\big),\n",
    "\\qquad\n",
    "\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\exp\\big(\\lambda(e^{it} - 1)\\big).\n",
    "\\]\n",
    "\n",
    "### Entropy\n",
    "There is no simple closed form for the Shannon entropy. One convenient expression (in **nats**) is\n",
    "\\[\n",
    "H(X) = -\\sum_{k=0}^{\\infty} p(k)\\,\\log p(k),\n",
    "\\qquad\n",
    "p(k)=\\frac{e^{-\\lambda}\\lambda^k}{k!}.\n",
    "\\]\n",
    "For large \\(\\lambda\\), a useful approximation is\n",
    "\\[\n",
    "H(X) \\approx \\tfrac{1}{2}\\log(2\\pi e\\lambda) - \\frac{1}{12\\lambda} + \\mathcal{O}(\\lambda^{-2}).\n",
    "\\]\n",
    "\n",
    "### Other useful properties\n",
    "- **Mode**: \\(\\lfloor\\lambda\\rfloor\\) is a mode; if \\(\\lambda\\) is an integer, both \\(\\lambda-1\\) and \\(\\lambda\\) are modes.\n",
    "- **Factorial moments**: \\(\\mathbb{E}[X(X-1)\\cdots(X-m+1)] = \\lambda^m\\).\n",
    "- **Closure under sums**: sums of independent Poisson variables remain Poisson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_mu(mu):\n",
    "    if isinstance(mu, bool):\n",
    "        raise TypeError(\"mu must be a real number, not bool\")\n",
    "    mu = float(mu)\n",
    "    if mu < 0:\n",
    "        raise ValueError(\"mu must be >= 0\")\n",
    "    return mu\n",
    "\n",
    "\n",
    "def poisson_pmf_array(mu, *, tail=1e-12, max_k=None):\n",
    "    '''Return (ks, pmf) over k=0..K where CDF is ~1-tail.\n",
    "\n",
    "    Notes:\n",
    "    - This is a *truncated* representation of an infinite-support distribution.\n",
    "    - It is accurate when the remaining tail mass beyond K is negligible.\n",
    "    '''\n",
    "    mu = _validate_mu(mu)\n",
    "\n",
    "    if mu == 0.0:\n",
    "        return np.array([0], dtype=int), np.array([1.0], dtype=float)\n",
    "\n",
    "    if max_k is None:\n",
    "        # Heuristic upper bound: mean + several std devs.\n",
    "        max_k = int(np.ceil(mu + 12.0 * np.sqrt(mu + 1.0) + 10.0))\n",
    "\n",
    "    pmf = []\n",
    "    p0 = math.exp(-mu)  # underflows for extremely large mu\n",
    "    pmf.append(p0)\n",
    "\n",
    "    cdf = p0\n",
    "    k = 0\n",
    "    while cdf < 1.0 - tail and k < max_k:\n",
    "        k += 1\n",
    "        pmf.append(pmf[-1] * mu / k)\n",
    "        cdf += pmf[-1]\n",
    "\n",
    "    ks = np.arange(len(pmf), dtype=int)\n",
    "    pmf = np.asarray(pmf, dtype=float)\n",
    "    return ks, pmf\n",
    "\n",
    "\n",
    "def poisson_logpmf(k, mu):\n",
    "    '''Log PMF for Poisson(mu). Returns -inf outside support.'''\n",
    "    mu = _validate_mu(mu)\n",
    "\n",
    "    k_arr = np.asarray(k)\n",
    "    out = np.full(k_arr.shape, -np.inf, dtype=float)\n",
    "\n",
    "    k_int = k_arr.astype(int)\n",
    "    valid = (k_arr == k_int) & (k_int >= 0)\n",
    "    if not np.any(valid):\n",
    "        return out\n",
    "\n",
    "    if mu == 0.0:\n",
    "        out[valid & (k_int == 0)] = 0.0\n",
    "        return out\n",
    "\n",
    "    kv = k_int[valid]\n",
    "\n",
    "    # log(k!) via log-gamma: log(k!) = lgamma(k+1)\n",
    "    log_fact = np.vectorize(lambda x: math.lgamma(x + 1.0), otypes=[float])(kv)\n",
    "\n",
    "    out[valid] = -mu + kv * np.log(mu) - log_fact\n",
    "    return out\n",
    "\n",
    "\n",
    "def poisson_pmf(k, mu):\n",
    "    return np.exp(poisson_logpmf(k, mu))\n",
    "\n",
    "\n",
    "def poisson_moments(mu):\n",
    "    mu = _validate_mu(mu)\n",
    "    if mu == 0.0:\n",
    "        return {\n",
    "            \"mean\": 0.0,\n",
    "            \"var\": 0.0,\n",
    "            \"skewness\": float(\"nan\"),\n",
    "            \"excess_kurtosis\": float(\"nan\"),\n",
    "            \"kurtosis\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"mean\": mu,\n",
    "        \"var\": mu,\n",
    "        \"skewness\": 1.0 / math.sqrt(mu),\n",
    "        \"excess_kurtosis\": 1.0 / mu,\n",
    "        \"kurtosis\": 3.0 + 1.0 / mu,\n",
    "    }\n",
    "\n",
    "\n",
    "def poisson_entropy_trunc(mu, *, tail=1e-12):\n",
    "    '''Approximate entropy (nats) by truncating the PMF.'''\n",
    "    ks, pmf = poisson_pmf_array(mu, tail=tail)\n",
    "    pmf = pmf[pmf > 0]\n",
    "    return float(-(pmf * np.log(pmf)).sum())\n",
    "\n",
    "\n",
    "def poisson_entropy_asymptotic(mu):\n",
    "    '''Large-mu approximation for entropy (nats).'''\n",
    "    mu = _validate_mu(mu)\n",
    "    if mu == 0.0:\n",
    "        return 0.0\n",
    "    return float(0.5 * np.log(2.0 * np.pi * np.e * mu) - 1.0 / (12.0 * mu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18719288",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 6.0\n",
    "moments = poisson_moments(mu)\n",
    "{\n",
    "    **moments,\n",
    "    \"entropy_trunc_nats\": poisson_entropy_trunc(mu),\n",
    "    \"entropy_asymptotic_nats\": poisson_entropy_asymptotic(mu),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo check (matches formulas up to sampling error)\n",
    "mu = 8.0\n",
    "samples = rng.poisson(lam=mu, size=200_000)\n",
    "\n",
    "est_mean = samples.mean()\n",
    "est_var = samples.var(ddof=0)\n",
    "\n",
    "{\n",
    "    \"formula_mean\": mu,\n",
    "    \"mc_mean\": float(est_mean),\n",
    "    \"formula_var\": mu,\n",
    "    \"mc_var\": float(est_var),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c3f88",
   "metadata": {},
   "source": [
    "## 5) Parameter Interpretation\n",
    "\n",
    "The single parameter \\(\\lambda\\) (SciPy: `mu`) is both:\n",
    "- the **mean** number of events in the exposure\n",
    "- the **variance** of the count\n",
    "\n",
    "If \\(\\lambda = rT\\) comes from a Poisson process, then:\n",
    "- \\(r\\) is a rate (“events per unit exposure”)\n",
    "- \\(T\\) is the exposure (“how long / how big the window is”)\n",
    "\n",
    "### Shape changes\n",
    "- Small \\(\\lambda\\): most mass is at 0 and 1, strongly right-skewed.\n",
    "- Moderate \\(\\lambda\\): the distribution spreads out; the mode moves right.\n",
    "- Large \\(\\lambda\\): the distribution becomes approximately symmetric and close to \\(\\mathcal{N}(\\lambda,\\lambda)\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = [0.5, 1.5, 4.0, 10.0]\n",
    "mu_max = max(mu_values)\n",
    "\n",
    "ks, _ = poisson_pmf_array(mu_max, tail=1e-12)\n",
    "\n",
    "fig = go.Figure()\n",
    "for mu in mu_values:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=ks,\n",
    "            y=poisson_pmf(ks, mu),\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"mu={mu}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Poisson PMF for different mu\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(X=k)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abffca2",
   "metadata": {},
   "source": [
    "## 6) Derivations\n",
    "\n",
    "### Expectation\n",
    "Starting from the PMF,\n",
    "\\[\n",
    "\\mathbb{E}[X] = \\sum_{k=0}^\\infty k\\,\\frac{e^{-\\lambda}\\lambda^k}{k!}.\n",
    "\\]\n",
    "Use the identity \\(k\\lambda^k/k! = \\lambda\\,\\lambda^{k-1}/(k-1)!\\) to shift the sum:\n",
    "\\[\n",
    "\\mathbb{E}[X]\n",
    "= e^{-\\lambda}\\sum_{k=1}^\\infty k\\frac{\\lambda^k}{k!}\n",
    "= \\lambda e^{-\\lambda}\\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k-1)!}\n",
    "= \\lambda e^{-\\lambda}\\sum_{j=0}^\\infty \\frac{\\lambda^{j}}{j!}\n",
    "= \\lambda.\n",
    "\\]\n",
    "\n",
    "### Variance\n",
    "A standard route is via factorial moments:\n",
    "\\[\n",
    "\\mathbb{E}[X(X-1)]\n",
    "= \\sum_{k=0}^\\infty k(k-1)\\,\\frac{e^{-\\lambda}\\lambda^k}{k!}.\n",
    "\\]\n",
    "Since \\(k(k-1)\\lambda^k/k! = \\lambda^2\\,\\lambda^{k-2}/(k-2)!\\), the same shift gives\n",
    "\\(\\mathbb{E}[X(X-1)] = \\lambda^2\\).\n",
    "Then\n",
    "\\[\n",
    "\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n",
    "= \\big(\\mathbb{E}[X(X-1)] + \\mathbb{E}[X]\\big) - \\lambda^2\n",
    "= (\\lambda^2 + \\lambda) - \\lambda^2\n",
    "= \\lambda.\n",
    "\\]\n",
    "\n",
    "### Likelihood (i.i.d. sample)\n",
    "If \\(x_1,\\dots,x_n\\) are i.i.d. \\(\\text{Poisson}(\\lambda)\\), the likelihood is\n",
    "\\[\n",
    "L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!}.\n",
    "\\]\n",
    "The log-likelihood is\n",
    "\\[\n",
    "\\ell(\\lambda) = \\sum_{i=1}^n \\big(x_i\\log\\lambda - \\lambda - \\log(x_i!)\\big).\n",
    "\\]\n",
    "Differentiate and set to zero:\n",
    "\\[\n",
    "\\ell'(\\lambda) = \\frac{\\sum_i x_i}{\\lambda} - n = 0\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\hat\\lambda_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n x_i.\n",
    "\\]\n",
    "So the MLE is the **sample mean**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff762f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the log-likelihood for lambda (single observation)\n",
    "k_obs = 7\n",
    "lam_grid = np.linspace(1e-6, 20.0, 600)\n",
    "\n",
    "logL = k_obs * np.log(lam_grid) - lam_grid - math.lgamma(k_obs + 1)\n",
    "lam_hat = k_obs\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=lam_grid, y=logL, mode=\"lines\", name=\"log-likelihood\"))\n",
    "fig.add_vline(\n",
    "    x=lam_hat,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"black\",\n",
    "    annotation_text=f\"MLE λ̂={lam_hat}\",\n",
    ")\n",
    "fig.update_layout(title=f\"Poisson log-likelihood (k={k_obs})\", xaxis_title=\"λ\", yaxis_title=\"ℓ(λ)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66d587",
   "metadata": {},
   "source": [
    "## 7) Sampling & Simulation\n",
    "\n",
    "Below are two **NumPy-only** samplers that illustrate common ideas.\n",
    "\n",
    "### A) Knuth’s algorithm (product of uniforms)\n",
    "A Poisson process perspective: the number of events in time \\(T\\) with rate \\(r\\) is Poisson with \\(\\lambda=rT\\). Knuth’s algorithm draws uniforms and multiplies them until the product drops below \\(e^{-\\lambda}\\). The loop runs about \\(\\lambda\\) iterations on average, so it is excellent for small \\(\\lambda\\) and slow for large \\(\\lambda\\).\n",
    "\n",
    "### B) Inverse CDF sampling (with truncated tail)\n",
    "If we can compute the CDF \\(F(k)\\) on \\(k=0,1,2,\\dots,K\\) such that \\(F(K)\\approx 1\\), then sampling is:\n",
    "1) draw \\(U\\sim\\text{Uniform}(0,1)\\)\n",
    "2) return the smallest \\(k\\) such that \\(F(k)\\ge U\\) (`searchsorted`)\n",
    "\n",
    "This is exact up to the (tiny) omitted tail mass beyond \\(K\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c540bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_poisson_knuth(mu, size=1, *, rng: np.random.Generator):\n",
    "    mu = _validate_mu(mu)\n",
    "\n",
    "    size = (size,) if isinstance(size, int) else tuple(size)\n",
    "    out = np.empty(size, dtype=int)\n",
    "\n",
    "    if mu == 0.0:\n",
    "        out.fill(0)\n",
    "        return out\n",
    "\n",
    "    L = math.exp(-mu)\n",
    "\n",
    "    for idx in np.ndindex(out.shape):\n",
    "        k = 0\n",
    "        p = 1.0\n",
    "        while p > L:\n",
    "            k += 1\n",
    "            p *= rng.random()\n",
    "        out[idx] = k - 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_poisson_inverse_cdf(mu, size=1, *, rng: np.random.Generator, tail=1e-12):\n",
    "    mu = _validate_mu(mu)\n",
    "\n",
    "    size = (size,) if isinstance(size, int) else tuple(size)\n",
    "    if mu == 0.0:\n",
    "        return np.zeros(size, dtype=int)\n",
    "\n",
    "    ks, pmf = poisson_pmf_array(mu, tail=tail)\n",
    "    cdf = np.cumsum(pmf)\n",
    "    cdf[-1] = 1.0  # absorb the omitted tail mass\n",
    "\n",
    "    u = rng.random(size)\n",
    "    idx = np.searchsorted(cdf, u, side=\"left\")\n",
    "    return ks[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 5.0\n",
    "size = 50_000\n",
    "\n",
    "x_knuth = sample_poisson_knuth(mu, size=size, rng=rng)\n",
    "x_inv = sample_poisson_inverse_cdf(mu, size=size, rng=rng)\n",
    "\n",
    "{\n",
    "    \"knuth_mean\": float(x_knuth.mean()),\n",
    "    \"inv_cdf_mean\": float(x_inv.mean()),\n",
    "    \"theory_mean\": mu,\n",
    "    \"knuth_var\": float(x_knuth.var(ddof=0)),\n",
    "    \"inv_cdf_var\": float(x_inv.var(ddof=0)),\n",
    "    \"theory_var\": mu,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a121e",
   "metadata": {},
   "source": [
    "## 8) Visualization\n",
    "\n",
    "We’ll visualize:\n",
    "- the **PMF** \\(k \\mapsto \\Pr(X=k)\\)\n",
    "- the **CDF** \\(k \\mapsto \\Pr(X\\le k)\\)\n",
    "- a **Monte Carlo** histogram compared to the theoretical PMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 7.0\n",
    "ks, pmf = poisson_pmf_array(mu, tail=1e-12)\n",
    "cdf = np.cumsum(pmf)\n",
    "\n",
    "fig_pmf = go.Figure()\n",
    "fig_pmf.add_trace(go.Bar(x=ks, y=pmf, name=\"PMF\"))\n",
    "fig_pmf.update_layout(title=f\"Poisson PMF (mu={mu})\", xaxis_title=\"k\", yaxis_title=\"P(X=k)\")\n",
    "fig_pmf.show()\n",
    "\n",
    "fig_cdf = go.Figure()\n",
    "fig_cdf.add_trace(go.Scatter(x=ks, y=cdf, mode=\"lines\", line_shape=\"hv\", name=\"CDF\"))\n",
    "fig_cdf.update_layout(title=f\"Poisson CDF (mu={mu})\", xaxis_title=\"k\", yaxis_title=\"P(X≤k)\")\n",
    "fig_cdf.show()\n",
    "\n",
    "# Monte Carlo vs PMF\n",
    "mc = sample_poisson_inverse_cdf(mu, size=80_000, rng=rng)\n",
    "\n",
    "hist = np.bincount(mc, minlength=len(ks)) / len(mc)\n",
    "\n",
    "fig_mc = go.Figure()\n",
    "fig_mc.add_trace(go.Bar(x=ks, y=hist[: len(ks)], name=\"MC histogram\", opacity=0.6))\n",
    "fig_mc.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers+lines\", name=\"PMF\"))\n",
    "fig_mc.update_layout(\n",
    "    title=f\"Monte Carlo vs PMF (mu={mu})\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"probability\",\n",
    ")\n",
    "fig_mc.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc11ed",
   "metadata": {},
   "source": [
    "### Approximations in action\n",
    "\n",
    "Two classic approximations are worth seeing:\n",
    "- **Binomial \\(\\to\\) Poisson** when events are rare (large \\(n\\), small \\(p\\), \\(np\\approx\\lambda\\))\n",
    "- **Normal \\(\\approx\\) Poisson** when \\(\\lambda\\) is large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binom_pmf(k, n, p):\n",
    "    k_arr = np.asarray(k)\n",
    "    k_int = k_arr.astype(int)\n",
    "\n",
    "    out = np.zeros_like(k_arr, dtype=float)\n",
    "\n",
    "    valid = (k_int == k_arr) & (k_int >= 0) & (k_int <= n)\n",
    "    if not np.any(valid):\n",
    "        return out\n",
    "\n",
    "    kv = k_int[valid]\n",
    "\n",
    "    log_coeff = (\n",
    "        math.lgamma(n + 1)\n",
    "        - np.vectorize(math.lgamma)(kv + 1)\n",
    "        - np.vectorize(math.lgamma)(n - kv + 1)\n",
    "    )\n",
    "    log_pmf = log_coeff + kv * math.log(p) + (n - kv) * math.log1p(-p)\n",
    "\n",
    "    out[valid] = np.exp(log_pmf)\n",
    "    return out\n",
    "\n",
    "\n",
    "# A) Binomial -> Poisson\n",
    "n = 300\n",
    "p = 0.02\n",
    "lam = n * p\n",
    "\n",
    "ks = np.arange(0, 25)\n",
    "\n",
    "pmf_binom = binom_pmf(ks, n, p)\n",
    "pmf_pois = poisson_pmf(ks, lam)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ks, y=pmf_binom, mode=\"markers+lines\", name=f\"Bin(n={n}, p={p})\"))\n",
    "fig.add_trace(go.Scatter(x=ks, y=pmf_pois, mode=\"markers+lines\", name=f\"Poisson(mu=np={lam:.1f})\"))\n",
    "fig.update_layout(title=\"Rare-event limit: Binomial vs Poisson\", xaxis_title=\"k\", yaxis_title=\"PMF\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# B) Normal approximation (continuity-corrected)\n",
    "lam = 40.0\n",
    "ks, pmf = poisson_pmf_array(lam, tail=1e-12)\n",
    "\n",
    "# Approximate P(X=k) ≈ Φ((k+0.5-λ)/sqrt(λ)) - Φ((k-0.5-λ)/sqrt(λ))\n",
    "from math import erf, sqrt\n",
    "\n",
    "def std_norm_cdf(z):\n",
    "    return 0.5 * (1.0 + erf(z / sqrt(2.0)))\n",
    "\n",
    "sigma = math.sqrt(lam)\n",
    "normal_approx = np.array(\n",
    "    [\n",
    "        std_norm_cdf((k + 0.5 - lam) / sigma) - std_norm_cdf((k - 0.5 - lam) / sigma)\n",
    "        for k in ks\n",
    "    ],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ks, y=pmf, mode=\"markers\", name=\"Poisson PMF\"))\n",
    "fig.add_trace(go.Scatter(x=ks, y=normal_approx, mode=\"lines\", name=\"Normal approx\"))\n",
    "fig.update_layout(title=f\"Normal approximation (mu={lam})\", xaxis_title=\"k\", yaxis_title=\"probability\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a004a87",
   "metadata": {},
   "source": [
    "## 9) SciPy Integration\n",
    "\n",
    "SciPy provides a full-featured implementation via `scipy.stats.poisson`.\n",
    "\n",
    "Common methods:\n",
    "- `poisson.pmf(k, mu)` / `poisson.logpmf(k, mu)`\n",
    "- `poisson.cdf(k, mu)` / `poisson.sf(k, mu)`\n",
    "- `poisson.rvs(mu, size=..., random_state=...)`\n",
    "- `scipy.stats.fit(poisson, data, bounds=..., method=\"mle\")` (generic fitting API for discrete/continuous distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b05f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from scipy.stats import poisson\n",
    "\n",
    "mu = 6.5\n",
    "ks = np.arange(0, 25)\n",
    "\n",
    "pmf_scipy = poisson.pmf(ks, mu)\n",
    "cdf_scipy = poisson.cdf(ks, mu)\n",
    "samples_scipy = poisson.rvs(mu, size=20_000, random_state=rng)\n",
    "\n",
    "# For the canonical Poisson (loc=0), the MLE for mu is just the sample mean.\n",
    "mu_mle = float(samples_scipy.mean())\n",
    "\n",
    "# SciPy also provides a general-purpose fitter for (discrete or continuous) distributions.\n",
    "# Because mu has domain [0, ∞), we supply a finite upper bound for numerical optimization.\n",
    "fit_res = st.fit(\n",
    "    poisson,\n",
    "    samples_scipy,\n",
    "    bounds={\"mu\": (0.0, max(1.0, mu_mle * 10.0)), \"loc\": (0.0, 0.0)},\n",
    "    guess={\"mu\": mu_mle, \"loc\": 0.0},\n",
    "    method=\"mle\",\n",
    ")\n",
    "\n",
    "{\n",
    "    \"pmf_sum_over_range\": float(pmf_scipy.sum()),\n",
    "    \"cdf_last\": float(cdf_scipy[-1]),\n",
    "    \"sample_mean\": float(samples_scipy.mean()),\n",
    "    \"theory_mean\": mu,\n",
    "    \"mle_mu_hat_closed_form\": mu_mle,\n",
    "    \"fit_mu_hat_scipy\": float(fit_res.params.mu),\n",
    "    \"fit_loc_hat_scipy\": float(fit_res.params.loc),\n",
    "    \"fit_success\": bool(fit_res.success),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad63916",
   "metadata": {},
   "source": [
    "## 10) Statistical Use Cases\n",
    "\n",
    "### A) Hypothesis testing (rate / count)\n",
    "A common task: test whether an observed count is unusually high or low compared to a baseline rate.\n",
    "\n",
    "If \\(X\\sim\\text{Poisson}(\\lambda_0)\\) under the null, then an upper-tail p-value is\n",
    "\\[\n",
    "\\text{p-value} = \\Pr(X \\ge k_\\text{obs} \\mid \\lambda_0) = 1 - F(k_\\text{obs}-1; \\lambda_0).\n",
    "\\]\n",
    "\n",
    "### B) Bayesian modeling (Gamma–Poisson conjugacy)\n",
    "With a Gamma prior on \\(\\lambda\\) and Poisson likelihood, the posterior is also Gamma.\n",
    "This is a workhorse model for counts.\n",
    "\n",
    "### C) Generative modeling\n",
    "Poisson counts appear in simulation pipelines (arrivals, defects, clicks). In conditional models, \\(\\lambda\\) is linked to features via \\(\\lambda_i = \\exp(x_i^\\top\\beta)\\times \\text{exposure}_i\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Hypothesis testing example: \"are we seeing more events than usual?\"\n",
    "from scipy.stats import chi2\n",
    "\n",
    "k_obs = 12\n",
    "lambda0 = 5.0\n",
    "\n",
    "p_upper = poisson.sf(k_obs - 1, lambda0)  # P(X >= k_obs)\n",
    "\n",
    "# Exact (central) confidence interval for lambda using chi-square quantiles\n",
    "alpha = 0.05\n",
    "\n",
    "if k_obs == 0:\n",
    "    ci_low = 0.0\n",
    "else:\n",
    "    ci_low = 0.5 * chi2.ppf(alpha / 2, 2 * k_obs)\n",
    "ci_high = 0.5 * chi2.ppf(1 - alpha / 2, 2 * (k_obs + 1))\n",
    "\n",
    "{\n",
    "    \"k_obs\": k_obs,\n",
    "    \"lambda0\": lambda0,\n",
    "    \"upper_tail_p_value\": float(p_upper),\n",
    "    \"95%_CI_for_lambda\": (float(ci_low), float(ci_high)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19259466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Bayesian modeling: Gamma prior + Poisson likelihood\n",
    "from scipy.stats import gamma, nbinom\n",
    "\n",
    "# Prior: lambda ~ Gamma(alpha0, rate=beta0)\n",
    "alpha0, beta0 = 2.0, 1.0\n",
    "\n",
    "# Data: n independent Poisson draws (unit exposure)\n",
    "data = rng.poisson(lam=4.5, size=40)\n",
    "\n",
    "alpha_post = alpha0 + data.sum()\n",
    "beta_post = beta0 + len(data)\n",
    "\n",
    "posterior_mean = alpha_post / beta_post\n",
    "posterior_ci = gamma.ppf([0.025, 0.975], a=alpha_post, scale=1.0 / beta_post)\n",
    "\n",
    "# Plot prior vs posterior\n",
    "lam_grid = np.linspace(0, 12, 600)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lam_grid,\n",
    "        y=gamma.pdf(lam_grid, a=alpha0, scale=1.0 / beta0),\n",
    "        mode=\"lines\",\n",
    "        name=f\"prior Gamma({alpha0},{beta0}) (rate)\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lam_grid,\n",
    "        y=gamma.pdf(lam_grid, a=alpha_post, scale=1.0 / beta_post),\n",
    "        mode=\"lines\",\n",
    "        name=f\"posterior Gamma({alpha_post:.1f},{beta_post:.1f}) (rate)\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(title=\"Gamma–Poisson conjugacy\", xaxis_title=\"lambda\", yaxis_title=\"density\")\n",
    "fig.show()\n",
    "\n",
    "# Posterior predictive for a future count X_new (unit exposure): Negative Binomial\n",
    "# If lambda ~ Gamma(alpha_post, rate=beta_post) and X|lambda ~ Poisson(lambda), then\n",
    "# X_new ~ NegBin(n=alpha_post, p=beta_post/(beta_post+1)) in SciPy's parameterization.\n",
    "\n",
    "n = alpha_post\n",
    "p = beta_post / (beta_post + 1.0)\n",
    "ks = np.arange(0, 30)\n",
    "pred_pmf = nbinom.pmf(ks, n, p)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ks, y=pred_pmf, name=\"posterior predictive\"))\n",
    "fig.update_layout(title=\"Posterior predictive (Negative Binomial)\", xaxis_title=\"k\", yaxis_title=\"P(X_new=k)\")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"prior_mean\": alpha0 / beta0,\n",
    "    \"posterior_mean\": float(posterior_mean),\n",
    "    \"posterior_95%_CI\": (float(posterior_ci[0]), float(posterior_ci[1])),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Generative modeling example: Poisson regression-style simulation\n",
    "\n",
    "n = 300\n",
    "x = rng.normal(size=n)\n",
    "exposure = rng.uniform(0.5, 2.0, size=n)  # e.g. time at risk\n",
    "\n",
    "beta0, beta1 = 1.0, 0.6\n",
    "\n",
    "# log-link: lambda_i = exp(beta0 + beta1 * x_i) * exposure_i\n",
    "lam = np.exp(beta0 + beta1 * x) * exposure\n",
    "\n",
    "y = rng.poisson(lam=lam)\n",
    "\n",
    "df = {\n",
    "    \"x\": x,\n",
    "    \"exposure\": exposure,\n",
    "    \"lambda\": lam,\n",
    "    \"y\": y,\n",
    "}\n",
    "\n",
    "fig = px.scatter(df, x=\"lambda\", y=\"y\", opacity=0.6)\n",
    "fig.update_layout(title=\"Simulated counts vs true mean (lambda)\", xaxis_title=\"lambda\", yaxis_title=\"count y\")\n",
    "fig.show()\n",
    "\n",
    "{\n",
    "    \"mean_lambda\": float(lam.mean()),\n",
    "    \"mean_y\": float(y.mean()),\n",
    "    \"var_y\": float(y.var(ddof=0)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a5060",
   "metadata": {},
   "source": [
    "## 11) Pitfalls\n",
    "\n",
    "### Invalid parameters\n",
    "- \\(\\lambda < 0\\) is invalid.\n",
    "- \\(\\lambda = 0\\) is valid but degenerate: \\(\\Pr(X=0)=1\\).\n",
    "\n",
    "### Numerical issues\n",
    "- Direct PMF computation can underflow/overflow for large \\(k\\) or \\(\\lambda\\). Prefer `logpmf` and stable special functions.\n",
    "- Summing the PMF naively to get the CDF may lose precision in the tails. Prefer `scipy.stats.poisson.cdf/sf`.\n",
    "\n",
    "### Modeling issues (the big ones)\n",
    "- **Over-dispersion**: real count data often has variance \\(>\\) mean (heterogeneity, clustering). Consider Negative Binomial, quasi-Poisson, or hierarchical models.\n",
    "- **Zero inflation**: too many zeros vs Poisson; consider zero-inflated models.\n",
    "- **Non-constant rate / dependence**: if the rate changes over time or events cluster, the Poisson process assumptions fail.\n",
    "- **Exposure matters**: comparing counts without normalizing by exposure can be misleading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad93df4",
   "metadata": {},
   "source": [
    "## 12) Summary\n",
    "\n",
    "- `poisson(mu)` is a **discrete** distribution on \\(\\{0,1,2,\\dots\\}\\) with parameter \\(\\mu=\\lambda\\ge 0\\).\n",
    "- PMF: \\(\\Pr(X=k)=e^{-\\lambda}\\lambda^k/k!\\); CDF can be written via incomplete gamma functions.\n",
    "- Mean = variance = \\(\\lambda\\); skewness \\(=1/\\sqrt{\\lambda}\\); excess kurtosis \\(=1/\\lambda\\).\n",
    "- The likelihood yields \\(\\hat\\lambda=\\bar x\\) for i.i.d. samples.\n",
    "- Useful in hypothesis tests for rates, in Bayesian models via Gamma–Poisson conjugacy, and in generative simulations for count data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
