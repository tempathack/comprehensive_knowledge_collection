{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friedman Test (Nonparametric Repeated-Measures ANOVA)\n",
    "\n",
    "The **Friedman test** answers a very specific question:\n",
    "\n",
    "> When I measure the *same blocks/subjects* under **k \\u2265 2 conditions** (treatments, models, UI variants, \\u2026), do the conditions differ **systematically**, without assuming normality?\n",
    "\n",
    "It\\u2019s the rank-based analogue of **repeated-measures ANOVA**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end you should be able to:\n",
    "\n",
    "- decide when Friedman is the right test (and when it isn\\u2019t)\n",
    "- map your data into the required **(n_blocks \\u00d7 k_treatments)** matrix\n",
    "- compute the Friedman statistic step-by-step from **within-block ranks**\n",
    "- interpret the p-value and report an effect size (**Kendall\\u2019s W**)\n",
    "- run a **NumPy-only** Monte Carlo / permutation view of the null distribution\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Hypothesis testing basics (null, p-value)\n",
    "- NumPy arrays\n",
    "- Plotly for visualization (this notebook uses `plotly_white`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "# Optional: SciPy cross-checks (the core implementation below is NumPy-only)\n",
    "try:\n",
    "    from scipy import stats\n",
    "except Exception:\n",
    "    stats = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) When to use the Friedman test\n",
    "\n",
    "Use Friedman when you have:\n",
    "\n",
    "- **Paired / repeated** measurements: each block has one observation for every treatment.\n",
    "  - blocks: people, datasets, days, machines, \\u2026\n",
    "  - treatments: algorithms, drugs, UI variants, \\u2026\n",
    "- **k \\u2265 2** treatments and **n \\u2265 2** blocks\n",
    "- a measurement scale that is at least **ordinal** (so ranking makes sense)\n",
    "- you don\\u2019t want to assume normality (and you want a robust omnibus test)\n",
    "\n",
    "### Data layout\n",
    "\n",
    "Put your data in a matrix `X` of shape `(n_blocks, k_treatments)`:\n",
    "\n",
    "- row `i` = block `i` (one paired set)\n",
    "- column `j` = treatment `j`\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "- **H0**: all treatments have the same distribution (no systematic treatment effect)\n",
    "- **H1**: at least one treatment differs\n",
    "\n",
    "The test is **omnibus**: if you reject H0, you learned \\u201cnot all treatments are equivalent\\u201d, but not *which* treatments differ.\n",
    "\n",
    "### Key assumptions (often overlooked)\n",
    "\n",
    "- Blocks are independent of each other.\n",
    "- Within a block, treatment labels are comparable (same scale/units).\n",
    "- No missing values in the standard formulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Intuition: ranks within each block\n",
    "\n",
    "For each block (row), replace raw values by ranks `1..k`.\n",
    "\n",
    "- If **higher is better** (e.g., accuracy), the largest value gets rank 1.\n",
    "- If **lower is better** (e.g., error), the smallest value gets rank 1.\n",
    "\n",
    "Under **H0**, each treatment should be \\u201crandomly\\u201d spread across the ranks across many blocks, so the **rank sums** per treatment should be similar.\n",
    "\n",
    "If one treatment is systematically better, it gets smaller ranks more often \\u2192 its rank sum becomes noticeably smaller than the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) The statistic (what is actually computed)\n",
    "\n",
    "Let:\n",
    "\n",
    "- `n` = number of blocks\n",
    "- `k` = number of treatments\n",
    "- `r_ij` = rank of treatment `j` within block `i` (rank 1 = best)\n",
    "- `R_j = \\u2211_i r_ij` = sum of ranks for treatment `j`\n",
    "\n",
    "The Friedman statistic is:\n",
    "\n",
    "$$\n",
    "Q = \\frac{12}{n k (k+1)} \\sum_{j=1}^k R_j^2 - 3n(k+1).\n",
    "$$\n",
    "\n",
    "### Tie correction (important in discrete data)\n",
    "\n",
    "If there are ties within blocks, ranks are averaged (e.g. two tied values both get rank 1.5). A standard tie correction is:\n",
    "\n",
    "$$\n",
    "C = 1 - \\frac{\\sum_{i=1}^n \\sum_{g \\in \\text{ties in block } i} (t_g^3 - t_g)}{n k (k^2 - 1)},\n",
    "\\qquad\n",
    "Q_{\\text{corr}} = \\frac{Q}{C}.\n",
    "$$\n",
    "\n",
    "Here, each tie group `g` has size `t_g`.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- Large `Q` means rank sums are more spread out than expected under H0 \\u2192 evidence that treatments differ.\n",
    "- The p-value is an **upper-tail** probability: `p = P(Q \\u2265 Q_obs | H0)`.\n",
    "\n",
    "### Effect size: Kendall\\u2019s W\n",
    "\n",
    "A common effect size is **Kendall\\u2019s W** (coefficient of concordance):\n",
    "\n",
    "$$\n",
    "W = \\frac{Q}{n (k-1)} \\in [0,1].\n",
    "$$\n",
    "\n",
    "- `W \\u2248 0` \\u2192 little/no systematic ranking difference across treatments\n",
    "- `W \\u2248 1` \\u2192 very strong, consistent ordering across blocks\n",
    "\n",
    "(With ties, `W` is often computed from the tie-corrected `Q`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankdata_average_ties_1d(x: np.ndarray, *, descending: bool = False) -> np.ndarray:\n",
    "    \"\"\"Rank a 1D array with average ranks for ties.\n",
    "\n",
    "    Returns ranks in {1,...,len(x)} as float.\n",
    "    If descending=True, larger values get smaller (better) ranks.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"x must be 1D\")\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"x must be non-empty\")\n",
    "    if not np.all(np.isfinite(x)):\n",
    "        raise ValueError(\"x contains non-finite values\")\n",
    "\n",
    "    x_work = -x if descending else x\n",
    "    order = np.argsort(x_work, kind=\"mergesort\")\n",
    "    x_sorted = x_work[order]\n",
    "\n",
    "    ranks_sorted = np.empty_like(x_sorted, dtype=float)\n",
    "    n = x_sorted.size\n",
    "\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        j = i + 1\n",
    "        while j < n and x_sorted[j] == x_sorted[i]:\n",
    "            j += 1\n",
    "\n",
    "        # Items i..(j-1) are tied and would have ranks (i+1)..j.\n",
    "        rank_avg = (i + 1 + j) / 2.0\n",
    "        ranks_sorted[i:j] = rank_avg\n",
    "        i = j\n",
    "\n",
    "    ranks = np.empty_like(ranks_sorted)\n",
    "    ranks[order] = ranks_sorted\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def rank_rows_average_ties(X: np.ndarray, *, descending: bool = False) -> np.ndarray:\n",
    "    \"\"\"Rank each row of X independently (average ranks for ties).\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be 2D with shape (n_blocks, k_treatments)\")\n",
    "    if not np.all(np.isfinite(X)):\n",
    "        raise ValueError(\"X contains non-finite values\")\n",
    "\n",
    "    return np.vstack(\n",
    "        [rankdata_average_ties_1d(row, descending=descending) for row in X]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friedman_statistic_from_ranks(\n",
    "    ranks: np.ndarray, *, tie_correction: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"Compute Friedman Q (and Kendall's W) from a rank matrix.\n",
    "\n",
    "    ranks has shape (n_blocks, k_treatments) and contains within-block ranks.\n",
    "    \"\"\"\n",
    "    ranks = np.asarray(ranks, dtype=float)\n",
    "    if ranks.ndim != 2:\n",
    "        raise ValueError(\"ranks must be 2D\")\n",
    "\n",
    "    n, k = ranks.shape\n",
    "    if n < 2:\n",
    "        raise ValueError(\"Need at least n>=2 blocks\")\n",
    "    if k < 2:\n",
    "        raise ValueError(\"Need at least k>=2 treatments\")\n",
    "\n",
    "    rank_sums = ranks.sum(axis=0)\n",
    "\n",
    "    Q = (\n",
    "        12.0 / (n * k * (k + 1.0)) * np.sum(rank_sums**2)\n",
    "        - 3.0 * n * (k + 1.0)\n",
    "    )\n",
    "\n",
    "    correction = 1.0\n",
    "    if tie_correction:\n",
    "        tie_sum = 0.0\n",
    "        for i in range(n):\n",
    "            _, counts = np.unique(ranks[i], return_counts=True)\n",
    "            counts = counts[counts > 1]\n",
    "            if counts.size:\n",
    "                tie_sum += np.sum(counts**3 - counts)\n",
    "\n",
    "        correction = 1.0 - tie_sum / (n * k * (k**2 - 1.0))\n",
    "        if correction <= 0:\n",
    "            raise ValueError(\"Non-positive tie correction factor; check ranks\")\n",
    "        Q = Q / correction\n",
    "\n",
    "    W = Q / (n * (k - 1.0))\n",
    "\n",
    "    expected_rank_sum = n * (k + 1.0) / 2.0\n",
    "    return {\n",
    "        \"n\": int(n),\n",
    "        \"k\": int(k),\n",
    "        \"Q\": float(Q),\n",
    "        \"W\": float(W),\n",
    "        \"rank_sums\": rank_sums,\n",
    "        \"expected_rank_sum\": float(expected_rank_sum),\n",
    "        \"tie_correction_factor\": float(correction),\n",
    "    }\n",
    "\n",
    "\n",
    "def friedman_Q_from_rank_sums(rank_sums: np.ndarray, *, n: int, k: int) -> np.ndarray:\n",
    "    \"\"\"Compute Friedman Q from rank sums R_j (vectorized).\"\"\"\n",
    "    rank_sums = np.asarray(rank_sums, dtype=float)\n",
    "    return 12.0 / (n * k * (k + 1.0)) * np.sum(rank_sums**2, axis=-1) - 3.0 * n * (\n",
    "        k + 1.0\n",
    "    )\n",
    "\n",
    "\n",
    "def friedman_null_Q_from_ranks(\n",
    "    ranks: np.ndarray,\n",
    "    *,\n",
    "    tie_correction_factor: float = 1.0,\n",
    "    n_resamples: int = 20000,\n",
    "    seed: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Permutation null distribution of Q by shuffling ranks within each block.\n",
    "\n",
    "    This is equivalent to permuting treatment labels within each block under H0.\n",
    "    It also preserves tie patterns (because the multiset of ranks per block is fixed).\n",
    "    \"\"\"\n",
    "    ranks = np.asarray(ranks, dtype=float)\n",
    "    if ranks.ndim != 2:\n",
    "        raise ValueError(\"ranks must be 2D\")\n",
    "\n",
    "    n, k = ranks.shape\n",
    "    if n < 2 or k < 2:\n",
    "        raise ValueError(\"Need n>=2 and k>=2\")\n",
    "    if n_resamples < 1:\n",
    "        raise ValueError(\"Need n_resamples>=1\")\n",
    "    if tie_correction_factor <= 0:\n",
    "        raise ValueError(\"tie_correction_factor must be positive\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Random permutations per (resample, block)\n",
    "    u = rng.random((n_resamples, n, k))\n",
    "    perm = np.argsort(u, axis=2)\n",
    "\n",
    "    ranks_perm = np.take_along_axis(\n",
    "        np.broadcast_to(ranks, (n_resamples, n, k)), perm, axis=2\n",
    "    )\n",
    "    rank_sums = ranks_perm.sum(axis=1)\n",
    "    Q = friedman_Q_from_rank_sums(rank_sums, n=n, k=k)\n",
    "    return Q / tie_correction_factor\n",
    "\n",
    "\n",
    "def friedman_test_numpy(\n",
    "    X: np.ndarray,\n",
    "    *,\n",
    "    higher_is_better: bool = True,\n",
    "    tie_correction: bool = True,\n",
    "    n_resamples: int = 20000,\n",
    "    seed: int = 0,\n",
    ") -> dict:\n",
    "    \"\"\"Friedman test computed from scratch (NumPy-only), plus a Monte Carlo p-value.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be 2D with shape (n_blocks, k_treatments)\")\n",
    "    if not np.all(np.isfinite(X)):\n",
    "        raise ValueError(\"X contains non-finite values\")\n",
    "\n",
    "    ranks = rank_rows_average_ties(X, descending=higher_is_better)\n",
    "    core = friedman_statistic_from_ranks(ranks, tie_correction=tie_correction)\n",
    "\n",
    "    Q_null = friedman_null_Q_from_ranks(\n",
    "        ranks,\n",
    "        tie_correction_factor=core[\"tie_correction_factor\"],\n",
    "        n_resamples=n_resamples,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Upper-tail p-value (add-one smoothing avoids returning exactly 0.0).\n",
    "    p_value = (1.0 + np.sum(Q_null >= core[\"Q\"])) / (n_resamples + 1.0)\n",
    "\n",
    "    return {\n",
    "        **core,\n",
    "        \"ranks\": ranks,\n",
    "        \"Q_null\": Q_null,\n",
    "        \"p_value_mc\": float(p_value),\n",
    "        \"higher_is_better\": bool(higher_is_better),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny example with ties (two equal best values)\n",
    "toy = np.array([[10.0, 10.0, 7.0, 3.0]])\n",
    "ranks_toy = rank_rows_average_ties(toy, descending=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"value\": toy[0],\n",
    "        \"rank (1=best)\": ranks_toy[0],\n",
    "    },\n",
    "    index=[\"A\", \"B\", \"C\", \"D\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Worked example (algorithms across datasets)\n",
    "\n",
    "A classic use case is comparing several ML algorithms across multiple datasets (each dataset is a *block*).\n",
    "\n",
    "We\\u2019ll simulate `k=4` algorithms evaluated on `n=24` datasets with **paired** accuracies (higher is better).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_blocks = 24\n",
    "algorithms = np.array([\"Algo A\", \"Algo B\", \"Algo C\", \"Algo D\"])\n",
    "k = algorithms.size\n",
    "\n",
    "# Dataset difficulty / baseline accuracy\n",
    "baseline = rng.uniform(0.65, 0.80, size=n_blocks)\n",
    "\n",
    "# Systematic treatment effects (A < B < C < D in accuracy)\n",
    "effects = np.array([0.00, 0.02, 0.04, 0.06])\n",
    "\n",
    "noise = rng.normal(0, 0.02, size=(n_blocks, k))\n",
    "scores = np.clip(baseline[:, None] + effects[None, :] + noise, 0.0, 1.0)\n",
    "\n",
    "df = pd.DataFrame(scores, columns=algorithms)\n",
    "df.insert(0, \"block\", np.arange(1, n_blocks + 1))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired nature: each line is one block (dataset)\n",
    "fig = go.Figure()\n",
    "for i in range(n_blocks):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=algorithms,\n",
    "            y=scores[i],\n",
    "            mode=\"lines+markers\",\n",
    "            line=dict(width=1),\n",
    "            opacity=0.55,\n",
    "            showlegend=False,\n",
    "            hovertemplate=f\"block={i+1}<br>%{{x}}=%{{y:.3f}}<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Paired accuracies (each line = one block)\",\n",
    "    xaxis_title=\"Algorithm\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution per algorithm (still paired, but shows marginal spread)\n",
    "df_long = df.melt(id_vars=\"block\", var_name=\"algorithm\", value_name=\"accuracy\")\n",
    "fig = px.box(\n",
    "    df_long,\n",
    "    x=\"algorithm\",\n",
    "    y=\"accuracy\",\n",
    "    points=\"all\",\n",
    "    title=\"Accuracy by algorithm (paired blocks)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Algorithm\", yaxis_title=\"Accuracy\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = friedman_test_numpy(\n",
    "    scores,\n",
    "    higher_is_better=True,\n",
    "    tie_correction=True,\n",
    "    n_resamples=20000,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "result[\"Q\"], result[\"p_value_mc\"], result[\"W\"], result[\"tie_correction_factor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize rank sums / mean ranks (lower is better)\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"algorithm\": algorithms,\n",
    "        \"rank_sum\": result[\"rank_sums\"],\n",
    "        \"mean_rank\": result[\"rank_sums\"] / result[\"n\"],\n",
    "        \"expected_rank_sum\": result[\"expected_rank_sum\"],\n",
    "    }\n",
    ").sort_values(\"mean_rank\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of within-block ranks (1=best)\n",
    "fig = px.imshow(\n",
    "    result[\"ranks\"],\n",
    "    x=algorithms,\n",
    "    y=[f\"Block {i}\" for i in range(1, n_blocks + 1)],\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"Viridis_r\",\n",
    "    title=\"Within-block ranks (1=best)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Algorithm\", yaxis_title=\"Block\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean rank plot (a common way to report Friedman results)\n",
    "fig = px.bar(\n",
    "    summary,\n",
    "    x=\"algorithm\",\n",
    "    y=\"mean_rank\",\n",
    "    title=\"Mean rank per algorithm (lower is better)\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Algorithm\", yaxis_title=\"Mean rank\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null distribution of Q (Monte Carlo) + observed statistic\n",
    "Q_null = result[\"Q_null\"]\n",
    "Q_obs = result[\"Q\"]\n",
    "p_mc = result[\"p_value_mc\"]\n",
    "\n",
    "fig = px.histogram(\n",
    "    Q_null,\n",
    "    nbins=60,\n",
    "    title=\"Friedman Q under H0 (permutation of within-block ranks)\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=Q_obs,\n",
    "    line_color=\"crimson\",\n",
    "    line_width=3,\n",
    "    annotation_text=f\"Observed Q={Q_obs:.2f}<br>p\\u2248{p_mc:.4f}\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Q\", yaxis_title=\"count\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) How to interpret the result (what it *means*)\n",
    "\n",
    "### If `p` is small (e.g. `< 0.05`)\n",
    "\n",
    "- You reject **H0**: it\\u2019s unlikely that all treatments are equivalent.\n",
    "- Concretely: the observed **spread of rank sums** is too large to plausibly come from random rank assignment.\n",
    "- You still don\\u2019t know *which* treatments differ \\u2192 you need **post-hoc** comparisons.\n",
    "\n",
    "### If `p` is not small\n",
    "\n",
    "- You do *not* reject H0: you don\\u2019t have evidence of systematic differences.\n",
    "- This is not proof of equality; you might be underpowered (small `n`), or differences may be tiny.\n",
    "\n",
    "### Reporting tips\n",
    "\n",
    "- Report `Q`, df=`k-1`, `p`, and an effect size like `W`.\n",
    "- Also report **mean ranks** (often more interpretable than raw `Q`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: compare against SciPy's friedmanchisquare (asymptotic chi-square p-value)\n",
    "if stats is None:\n",
    "    print(\"SciPy not available; skipping cross-check.\")\n",
    "else:\n",
    "    Q_scipy, p_scipy = stats.friedmanchisquare(*[scores[:, j] for j in range(k)])\n",
    "    print(f\"SciPy: Q={Q_scipy:.6f}, p={p_scipy:.6g}\")\n",
    "    print(f\"Ours : Q={result['Q']:.6f}, p_mc\\u2248{result['p_value_mc']:.6g}\")\n",
    "\n",
    "    # Iman-Davenport correction (often used in ML-algorithm comparison)\n",
    "    Q = result[\"Q\"]\n",
    "    F = (n_blocks - 1.0) * Q / (n_blocks * (k - 1.0) - Q)\n",
    "    p_F = stats.f.sf(F, k - 1, (k - 1) * (n_blocks - 1))\n",
    "    print(f\"Iman-Davenport: F={F:.6f}, p={p_F:.6g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Practical notes, pitfalls, and next steps\n",
    "\n",
    "- **Direction matters**: decide whether higher or lower values should get rank 1.\n",
    "- **Don\\u2019t ignore pairing**: if blocks are not the same units across treatments, Friedman is the wrong tool.\n",
    "- **Significant Friedman does not localize** differences.\n",
    "  - Common post-hoc options: Nemenyi (all-pairs on mean ranks) or pairwise Wilcoxon signed-rank with multiplicity correction.\n",
    "- **Ties** are common in discrete scores (e.g., integer ratings). Use average ranks and a tie correction.\n",
    "\n",
    "If your data are independent groups (not repeated measures), look at **Kruskal\\u2013Wallis** instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: when there is no systematic treatment effect, p-values should look non-significant on average.\n",
    "\n",
    "n_blocks_0 = 24\n",
    "k_0 = 4\n",
    "\n",
    "baseline0 = rng.uniform(0.65, 0.80, size=n_blocks_0)\n",
    "effects0 = np.zeros(k_0)\n",
    "scores0 = np.clip(\n",
    "    baseline0[:, None] + effects0[None, :] + rng.normal(0, 0.02, size=(n_blocks_0, k_0)),\n",
    "    0.0,\n",
    "    1.0,\n",
    ")\n",
    "\n",
    "res0 = friedman_test_numpy(scores0, higher_is_better=True, n_resamples=20000, seed=999)\n",
    "res0[\"Q\"], res0[\"p_value_mc\"], res0[\"W\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Change `effects` to make the algorithms closer together. How do `Q`, `p`, and `W` change?\n",
    "2. Increase/decrease `n_blocks` and see how the test\\u2019s sensitivity changes.\n",
    "3. Create a dataset with deliberate ties (rounded scores) and see how the tie correction factor behaves.\n",
    "\n",
    "## References\n",
    "\n",
    "- Friedman (1937): *The use of ranks to avoid the assumption of normality implicit in the analysis of variance*\n",
    "- Dem\\u0161ar (2006): *Statistical Comparisons of Classifiers over Multiple Data Sets*\n",
    "- `scipy.stats.friedmanchisquare`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
