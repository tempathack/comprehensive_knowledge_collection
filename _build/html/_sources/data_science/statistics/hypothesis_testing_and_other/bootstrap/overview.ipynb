{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Hypothesis Testing (from scratch)\n",
    "\n",
    "A **bootstrap hypothesis test** is a *resampling-based* way to get a p-value when the sampling distribution of a statistic is hard (or undesirable) to derive analytically.\n",
    "\n",
    "This notebook focuses on:\n",
    "- what bootstrap tests are used for (and what they are *not*)\n",
    "- how to build a **null distribution** with bootstrap\n",
    "- how to compute and interpret a bootstrap **p-value**\n",
    "- a low-level implementation using only **NumPy**, plus **Plotly** visuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = os.environ.get(\"PLOTLY_RENDERER\", \"notebook\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "SEED_DATA = 42\n",
    "SEED_BOOTSTRAP = 123\n",
    "\n",
    "rng_data = np.random.default_rng(SEED_DATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- You know what a **null hypothesis** $H_0$ and **alternative** $H_1$ are.\n",
    "- You know what a **test statistic** is (a number computed from data).\n",
    "- You can interpret a **p-value** at a basic level.\n",
    "\n",
    "If any of that is fuzzy, still continue: this notebook keeps the math light and emphasizes *mechanics and interpretation*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) What problem does a bootstrap test solve?\n",
    "\n",
    "In many tests (e.g. z-test, t-test), we can compute a p-value because we know (or approximate) the distribution of a statistic under $H_0$.\n",
    "\n",
    "But sometimes:\n",
    "- the statistic is complicated (median, trimmed mean, ratio, custom metric)\n",
    "- the usual distributional assumptions are uncomfortable (strong skew, heavy tails)\n",
    "- you want a method that matches the **sampling process** more directly\n",
    "\n",
    "Bootstrap methods approximate sampling distributions by **resampling the observed data**.\n",
    "\n",
    "Key idea:\n",
    "- Treat the observed sample as an estimate of the population.\n",
    "- Resample from it (with replacement) many times.\n",
    "- Recompute the statistic each time to get an empirical distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick visual: bootstrap distribution of the sample mean\n",
    "\n",
    "Below, we draw one skewed sample and then bootstrap the mean many times to visualize the **sampling distribution** estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = rng_data.exponential(scale=1.0, size=40)\n",
    "observed_mean = float(np.mean(sample))\n",
    "\n",
    "n_boot_demo = 5_000\n",
    "rng_demo = np.random.default_rng(SEED_BOOTSTRAP)\n",
    "\n",
    "idx = rng_demo.integers(0, sample.size, size=(n_boot_demo, sample.size))\n",
    "bootstrap_means = sample[idx].mean(axis=1)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Observed sample\", \"Bootstrap distribution of the mean\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Histogram(x=sample, nbinsx=30, name=\"sample\"), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=bootstrap_means, nbinsx=40, name=\"boot means\"), row=1, col=2)\n",
    "\n",
    "fig.add_vline(\n",
    "    x=observed_mean,\n",
    "    line_width=3,\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"observed mean = {observed_mean:.3f}\",\n",
    "    annotation_position=\"top\",\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=380,\n",
    "    width=950,\n",
    "    showlegend=False,\n",
    "    title_text=\"Bootstrap intuition: resample → recompute → distribution\",\n",
    ")\n",
    "fig.update_xaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"bootstrap mean\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Turning bootstrap into a hypothesis test\n",
    "\n",
    "A hypothesis test needs the distribution of a statistic **under the null hypothesis**.\n",
    "\n",
    "Let:\n",
    "- $T(\\text{data})$ be your test statistic (e.g., difference in means)\n",
    "- $t_{\\text{obs}} = T(\\text{observed data})$\n",
    "\n",
    "A (two-sided) bootstrap p-value is estimated as:\n",
    "\n",
    "$$\\hat{p} = \\frac{1 + \\sum_{b=1}^{B} \\mathbb{1}\\left(|T_b^* - T_0| \\ge |t_{\\text{obs}} - T_0|\\right)}{B + 1}$$\n",
    "\n",
    "Where:\n",
    "- $T_b^*$ are bootstrap statistics computed from **null-consistent** resamples\n",
    "- $T_0$ is the null value of the statistic (often 0)\n",
    "- the `+1` correction avoids reporting exactly 0 when $B$ is finite\n",
    "\n",
    "Important subtlety:\n",
    "- If you bootstrap directly from the observed data, the bootstrap distribution is typically centered around **the observed effect**, not around $H_0$.\n",
    "- For a *test*, we need to generate resamples from a **null world** where $H_0$ is true.\n",
    "\n",
    "How to build the null world depends on $H_0$.\n",
    "A common (and intuitive) approach for mean-based tests is **centering / shifting** the samples so the null is satisfied, then bootstrapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null-world construction for a two-sample mean test\n",
    "\n",
    "Suppose we have two independent groups:\n",
    "- Control sample: $X_1, \\dots, X_n$\n",
    "- Treatment sample: $Y_1, \\dots, Y_m$\n",
    "\n",
    "We want to test:\n",
    "$$H_0: \\mu_Y - \\mu_X = \\Delta_0$$\n",
    "\n",
    "We can enforce $H_0$ by shifting both samples so their means match what they *should* be under the null, while keeping their shapes:\n",
    "\n",
    "1. Compute the pooled mean $\\bar{Z}$ of all observations.\n",
    "2. Choose null means $(\\mu_{X,0}, \\mu_{Y,0})$ such that:\n",
    "   - $\\mu_{Y,0} - \\mu_{X,0} = \\Delta_0$\n",
    "   - the pooled mean is preserved.\n",
    "3. Shift each group to those null means.\n",
    "4. Bootstrap-resample **within each group** and recompute the difference in means.\n",
    "\n",
    "This yields an empirical null distribution for the statistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_diff_in_means(sample_a, sample_b, *, n_boot=20_000, rng):\n",
    "    \"\"\"Bootstrap distribution of mean(sample_b) - mean(sample_a).\"\"\"\n",
    "    sample_a = np.asarray(sample_a, dtype=float)\n",
    "    sample_b = np.asarray(sample_b, dtype=float)\n",
    "\n",
    "    n_a = sample_a.size\n",
    "    n_b = sample_b.size\n",
    "\n",
    "    idx_a = rng.integers(0, n_a, size=(n_boot, n_a))\n",
    "    idx_b = rng.integers(0, n_b, size=(n_boot, n_b))\n",
    "\n",
    "    boot_a = sample_a[idx_a].mean(axis=1)\n",
    "    boot_b = sample_b[idx_b].mean(axis=1)\n",
    "\n",
    "    return boot_b - boot_a\n",
    "\n",
    "\n",
    "def bootstrap_null_diff_in_means(sample_a, sample_b, *, delta0=0.0, n_boot=20_000, rng):\n",
    "    \"\"\"Null bootstrap distribution for H0: mean(B) - mean(A) = delta0.\n",
    "\n",
    "    Strategy: shift both groups so their means satisfy H0, then resample.\n",
    "    \"\"\"\n",
    "    sample_a = np.asarray(sample_a, dtype=float)\n",
    "    sample_b = np.asarray(sample_b, dtype=float)\n",
    "\n",
    "    n_a = sample_a.size\n",
    "    n_b = sample_b.size\n",
    "\n",
    "    pooled = np.concatenate([sample_a, sample_b])\n",
    "    pooled_mean = float(np.mean(pooled))\n",
    "\n",
    "    # Pick (mu_a0, mu_b0) that preserve pooled mean and enforce mu_b0 - mu_a0 = delta0.\n",
    "    total = n_a + n_b\n",
    "    mu_a0 = pooled_mean - (n_b / total) * delta0\n",
    "    mu_b0 = pooled_mean + (n_a / total) * delta0\n",
    "\n",
    "    sample_a_null = sample_a - np.mean(sample_a) + mu_a0\n",
    "    sample_b_null = sample_b - np.mean(sample_b) + mu_b0\n",
    "\n",
    "    idx_a = rng.integers(0, n_a, size=(n_boot, n_a))\n",
    "    idx_b = rng.integers(0, n_b, size=(n_boot, n_b))\n",
    "\n",
    "    boot_a = sample_a_null[idx_a].mean(axis=1)\n",
    "    boot_b = sample_b_null[idx_b].mean(axis=1)\n",
    "\n",
    "    return boot_b - boot_a\n",
    "\n",
    "\n",
    "def bootstrap_p_value(null_stats, observed_stat, *, delta0=0.0, alternative=\"two-sided\"):\n",
    "    \"\"\"Compute a bootstrap p-value given a null distribution of the statistic.\"\"\"\n",
    "    null_stats = np.asarray(null_stats, dtype=float)\n",
    "    observed_stat = float(observed_stat)\n",
    "\n",
    "    if alternative not in {\"two-sided\", \"greater\", \"less\"}:\n",
    "        raise ValueError(\"alternative must be one of: 'two-sided', 'greater', 'less'\")\n",
    "\n",
    "    if alternative == \"two-sided\":\n",
    "        extreme = np.abs(null_stats - delta0) >= abs(observed_stat - delta0)\n",
    "    elif alternative == \"greater\":\n",
    "        extreme = null_stats >= observed_stat\n",
    "    else:  # \"less\"\n",
    "        extreme = null_stats <= observed_stat\n",
    "\n",
    "    # +1 correction: avoids returning exactly 0 for finite B.\n",
    "    return (int(np.sum(extreme)) + 1) / (null_stats.size + 1)\n",
    "\n",
    "\n",
    "def bootstrap_ci_percentile(bootstrap_stats, *, conf_level=0.95):\n",
    "    \"\"\"Percentile bootstrap confidence interval.\"\"\"\n",
    "    bootstrap_stats = np.asarray(bootstrap_stats, dtype=float)\n",
    "    alpha = 1.0 - float(conf_level)\n",
    "    low, high = np.quantile(bootstrap_stats, [alpha / 2, 1 - alpha / 2])\n",
    "    return float(low), float(high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Worked example: two independent samples\n",
    "\n",
    "Imagine an experiment where we measure a **time-to-event** metric (often skewed), like \"time on page\" or \"time until first click\".\n",
    "\n",
    "We'll simulate:\n",
    "- a *control* group\n",
    "- a *treatment* group with a positive location shift\n",
    "\n",
    "Then we test:\n",
    "$$H_0: \\mu_{\\text{treatment}} - \\mu_{\\text{control}} = 0$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = rng_data.exponential(scale=1.0, size=50)\n",
    "treatment = rng_data.exponential(scale=1.0, size=45) + 0.35\n",
    "\n",
    "observed_diff = float(np.mean(treatment) - np.mean(control))\n",
    "observed_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        y=control,\n",
    "        name=\"Control\",\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        points=\"all\",\n",
    "        jitter=0.25,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        y=treatment,\n",
    "        name=\"Treatment\",\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        points=\"all\",\n",
    "        jitter=0.25,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Raw data (skewed): control vs treatment\",\n",
    "    yaxis_title=\"metric value\",\n",
    "    height=420,\n",
    "    width=750,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boot = 30_000\n",
    "\n",
    "null_stats = bootstrap_null_diff_in_means(\n",
    "    control,\n",
    "    treatment,\n",
    "    delta0=0.0,\n",
    "    n_boot=n_boot,\n",
    "    rng=np.random.default_rng(SEED_BOOTSTRAP),\n",
    ")\n",
    "\n",
    "p_two_sided = bootstrap_p_value(null_stats, observed_diff, alternative=\"two-sided\")\n",
    "p_greater = bootstrap_p_value(null_stats, observed_diff, alternative=\"greater\")\n",
    "\n",
    "print(f\"Observed mean difference (treatment - control): {observed_diff:.4f}\")\n",
    "print(f\"Bootstrap p-value (two-sided): {p_two_sided:.4f}\")\n",
    "print(f\"Bootstrap p-value (greater, i.e. treatment > control): {p_greater:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme = np.abs(null_stats) >= abs(observed_diff)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=null_stats[~extreme],\n",
    "        nbinsx=70,\n",
    "        name=\"null distribution\",\n",
    "        marker_color=\"#4C78A8\",\n",
    "        opacity=0.85,\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=null_stats[extreme],\n",
    "        nbinsx=70,\n",
    "        name=\"as/extreme as observed\",\n",
    "        marker_color=\"#E45756\",\n",
    "        opacity=0.95,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=observed_diff,\n",
    "    line_width=3,\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"observed = {observed_diff:.3f}\",\n",
    "    annotation_position=\"top\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"overlay\",\n",
    "    title=f\"Null distribution via bootstrap (two-sided p ≈ {p_two_sided:.4f})\",\n",
    "    xaxis_title=\"mean difference under H0\",\n",
    "    yaxis_title=\"count\",\n",
    "    height=430,\n",
    "    width=900,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret the p-value (what it *exactly* means)\n",
    "\n",
    "A p-value answers this question:\n",
    "\n",
    "> If the null hypothesis were true, how often would we see a statistic at least as extreme as the one we observed?\n",
    "\n",
    "In this notebook, that probability is approximated by the fraction of **null bootstrap resamples** whose mean difference is as (or more) extreme than the observed mean difference.\n",
    "\n",
    "Crucial interpretation points:\n",
    "- The p-value is **not** $P(H_0 \\mid \\text{data})$ (it is not “the probability the null is true”).\n",
    "- A small p-value means the observed result would be **rare under $H_0$**, so it is evidence against $H_0$.\n",
    "- A large p-value means the result is **not surprising under $H_0$**; it does *not* prove $H_0$.\n",
    "- The p-value does not tell you whether the effect is *important*; it is influenced by sample size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Effect size + uncertainty (bootstrap confidence interval)\n",
    "\n",
    "Hypothesis tests answer “is this surprising under $H_0$?”.\n",
    "Often you also want:\n",
    "- the estimated effect size (here: mean difference)\n",
    "- uncertainty around that effect (e.g. a 95% CI)\n",
    "\n",
    "A common companion is a **percentile bootstrap confidence interval** from the *standard* bootstrap distribution (bootstrapping directly from the observed groups).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_stats = bootstrap_diff_in_means(\n",
    "    control,\n",
    "    treatment,\n",
    "    n_boot=n_boot,\n",
    "    rng=np.random.default_rng(SEED_BOOTSTRAP),\n",
    ")\n",
    "\n",
    "ci_low, ci_high = bootstrap_ci_percentile(boot_stats, conf_level=0.95)\n",
    "boot_se = float(np.std(boot_stats, ddof=1))\n",
    "\n",
    "print(f\"Observed mean difference: {observed_diff:.4f}\")\n",
    "print(f\"Bootstrap SE (approx): {boot_se:.4f}\")\n",
    "print(f\"95% percentile bootstrap CI: [{ci_low:.4f}, {ci_high:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=boot_stats,\n",
    "        nbinsx=70,\n",
    "        name=\"bootstrap effect distribution\",\n",
    "        marker_color=\"#4C78A8\",\n",
    "        opacity=0.9,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=0.0,\n",
    "    line_width=2,\n",
    "    line_color=\"black\",\n",
    "    line_dash=\"dash\",\n",
    "    annotation_text=\"null (0)\",\n",
    "    annotation_position=\"top\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=observed_diff,\n",
    "    line_width=3,\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"observed = {observed_diff:.3f}\",\n",
    "    annotation_position=\"top\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=ci_low,\n",
    "    line_width=3,\n",
    "    line_color=\"#72B7B2\",\n",
    "    annotation_text=f\"2.5% = {ci_low:.3f}\",\n",
    "    annotation_position=\"bottom\",\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=ci_high,\n",
    "    line_width=3,\n",
    "    line_color=\"#72B7B2\",\n",
    "    annotation_text=f\"97.5% = {ci_high:.3f}\",\n",
    "    annotation_position=\"bottom\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Standard bootstrap distribution for the effect + 95% CI\",\n",
    "    xaxis_title=\"mean difference (treatment - control)\",\n",
    "    yaxis_title=\"count\",\n",
    "    height=430,\n",
    "    width=900,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the CI\n",
    "\n",
    "A 95% confidence interval can be read as:\n",
    "\n",
    "> If we repeated the entire data-collection process many times and built the interval each time, about 95% of those intervals would contain the true effect.\n",
    "\n",
    "For the common “test via CI inversion” rule:\n",
    "- For a two-sided test at $\\alpha=0.05$, you reject $H_0$ if **0 is not inside the 95% CI**.\n",
    "\n",
    "Compared to a p-value, the CI forces you to look at **magnitude** and **uncertainty**, not just a threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) When (and how) to use bootstrap tests in practice\n",
    "\n",
    "Bootstrap tests are most useful when you:\n",
    "- care about a statistic without a clean analytic null distribution\n",
    "- want fewer distributional assumptions than classic parametric tests\n",
    "- are willing to pay compute to get a flexible, data-driven approximation\n",
    "\n",
    "But you must match the resampling scheme to the data structure:\n",
    "\n",
    "- **One-sample location test** ($H_0: \\mu = \\mu_0$): shift the sample so its mean equals $\\mu_0$, then bootstrap.\n",
    "- **Two-sample independent groups**: resample within each group (as done here).\n",
    "- **Paired / matched data**: resample the *pairs* (or resample the paired differences).\n",
    "- **Regression**: consider a *residual bootstrap* or *pairs bootstrap*.\n",
    "- **Time series / dependence**: use *block bootstrap* variants (simple i.i.d. resampling breaks dependence).\n",
    "\n",
    "Also note:\n",
    "- For “are two groups different?” with exchangeable observations, a **permutation test** is often preferred for hypothesis testing (exact under $H_0$), while the bootstrap shines for **confidence intervals**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Pitfalls\n",
    "\n",
    "- **Garbage in, garbage out**: bootstrap assumes your sample represents the population well.\n",
    "- **Dependence matters**: naive bootstrap on correlated data can be very misleading.\n",
    "- **Small samples**: bootstrap approximations can be noisy or biased.\n",
    "- **Null-world choice matters**: different ways of enforcing $H_0$ can lead to different results.\n",
    "- **Report more than p-values**: always include effect size + uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Change the statistic from mean to median and repeat the workflow.\n",
    "2. Increase/decrease the sample size and see how the p-value and CI change.\n",
    "3. Implement a paired bootstrap test by simulating paired differences.\n",
    "4. Compare a bootstrap p-value to a permutation-test p-value for the same data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Efron, B. (1979). *Bootstrap methods: Another look at the jackknife*.\n",
    "- Davison, A. C., & Hinkley, D. V. (1997). *Bootstrap Methods and Their Application*.\n",
    "- Good, P. (2005). *Permutation, Parametric and Bootstrap Tests of Hypotheses*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
