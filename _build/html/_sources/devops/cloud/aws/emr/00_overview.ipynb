{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amazon EMR (Elastic MapReduce)\n",
        "\n",
        "<img src=\"../_assets/aws_service_icons/emr.svg\" width=\"80\" alt=\"Amazon EMR\">\n",
        "\n",
        "## Goals\n",
        "- Understand what **Amazon EMR** is (and what it is not).\n",
        "- Know common practical use-cases for EMR in data/ML workflows.\n",
        "- See a minimal **AWS SDK** pseudo-code workflow (no execution).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "- Basic AWS concepts (regions, IAM roles, VPC/subnets).\n",
        "- Familiarity with batch processing and distributed compute (e.g., Spark concepts) helps.\n",
        "\n",
        "> This notebook includes **pseudo-code only**. It does not run any AWS SDK calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What EMR is\n",
        "**Amazon EMR** is AWS\u2019s managed service for running common **open-source distributed data processing frameworks**.\n",
        "\n",
        "Think of EMR as \u201cmanaged clusters + managed integrations\u201d for frameworks like:\n",
        "- **Apache Spark** (ETL, feature engineering, ML pipelines)\n",
        "- **Apache Hadoop** (HDFS/MapReduce ecosystem)\n",
        "- **Hive/Presto/Trino** (SQL-on-data-lake style querying)\n",
        "- (Depending on the release) other ecosystem tools\n",
        "\n",
        "EMR can be used in different deployment modes:\n",
        "- **EMR on EC2**: EMR provisions and manages an EC2 cluster for you.\n",
        "- **EMR on EKS**: run EMR workloads on Kubernetes (EKS) using EMR\u2019s runtime.\n",
        "- **EMR Serverless**: run supported workloads without managing clusters.\n",
        "\n",
        "Key concepts (EMR on EC2 terminology):\n",
        "- **Cluster / Job flow**: a set of compute instances configured for your frameworks.\n",
        "- **Steps**: ordered units of work (e.g., a Spark submit, a Hive query).\n",
        "- **Release label**: the EMR platform version (pins framework versions).\n",
        "- **Logs**: typically shipped to S3 for debugging and auditing.\n",
        "\n",
        "### What it is not\n",
        "- Not a general-purpose orchestrator (use Airflow/Step Functions/Prefect for multi-system workflows).\n",
        "- Not primarily a storage layer (most modern EMR setups use **S3** as the data lake; HDFS is optional/temporary).\n",
        "- Not the same as AWS Glue: Glue is more \u201cserverless ETL\u201d, while EMR is for when you want **more control** over the runtime, libraries, tuning, and cluster shape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What EMR is practically used for\n",
        "EMR is commonly used when a workload benefits from **distributed compute** and you want a managed way to run/tune the underlying frameworks.\n",
        "\n",
        "Typical use-cases:\n",
        "- **Batch ETL at scale**: transform raw data in S3 into curated tables/files.\n",
        "- **Feature engineering**: build training datasets and feature tables using Spark.\n",
        "- **Large joins/aggregations**: computations that are too slow/expensive on a single machine.\n",
        "- **SQL over a data lake**: interactive or scheduled queries via Hive/Trino/Presto.\n",
        "- **Migration from on-prem Hadoop**: lift-and-shift (then modernize) existing Spark/Hive workloads.\n",
        "\n",
        "Common operational patterns:\n",
        "- **Ephemeral clusters**: create a cluster, run steps, and auto-terminate to control cost.\n",
        "- **Separation of storage and compute**: keep data in S3; treat the cluster as disposable.\n",
        "- **Cost optimization**: mix On-Demand and Spot instances; right-size instance groups.\n",
        "- **Reproducibility**: pin the EMR release label and package dependencies carefully.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using EMR with the AWS SDK (pseudo-code)\n",
        "Below is a minimal, **non-executable** sketch of creating an EMR cluster (EMR on EC2), submitting a Spark step, monitoring state, and cleaning up.\n",
        "\n",
        "Notes:\n",
        "- Prefer **IAM roles** for permissions; never hardcode AWS keys in notebooks.\n",
        "- Creating EMR resources can incur cost; use auto-termination and clean up.\n",
        "- Network settings (subnets, security groups) and IAM roles vary by organization; treat them as placeholders.\n",
        "\n",
        "```python\n",
        "# PSEUDO-CODE (do not run)\n",
        "\n",
        "import time\n",
        "import boto3\n",
        "\n",
        "region = \"us-east-1\"\n",
        "emr = boto3.client(\"emr\", region_name=region)\n",
        "\n",
        "# Inputs/outputs live in S3 (common EMR pattern)\n",
        "s3_code_uri = \"s3://<bucket>/jobs/etl.py\"\n",
        "s3_input_uri = \"s3://<bucket>/data/raw/\"\n",
        "s3_output_uri = \"s3://<bucket>/data/curated/\"\n",
        "s3_log_uri = \"s3://<bucket>/emr-logs/\"\n",
        "\n",
        "# IAM roles (use org-approved roles; names below are common defaults)\n",
        "service_role = \"EMR_DefaultRole\"           # cluster-level permissions\n",
        "job_flow_role = \"EMR_EC2_DefaultRole\"      # permissions for EC2 instances in the cluster\n",
        "\n",
        "# Minimal cluster + step definition\n",
        "resp = emr.run_job_flow(\n",
        "    Name=\"demo-emr-spark-etl\",\n",
        "    ReleaseLabel=\"emr-6.15.0\",\n",
        "    Applications=[{\"Name\": \"Spark\"}],\n",
        "    LogUri=s3_log_uri,\n",
        "    Instances={\n",
        "        \"Ec2SubnetId\": \"subnet-xxxxxxxx\",\n",
        "        \"KeepJobFlowAliveWhenNoSteps\": False,\n",
        "        \"TerminationProtected\": False,\n",
        "        \"InstanceGroups\": [\n",
        "            {\n",
        "                \"Name\": \"Primary\",\n",
        "                \"InstanceRole\": \"MASTER\",\n",
        "                \"InstanceType\": \"m5.xlarge\",\n",
        "                \"InstanceCount\": 1,\n",
        "                \"Market\": \"ON_DEMAND\",\n",
        "            },\n",
        "            {\n",
        "                \"Name\": \"Core\",\n",
        "                \"InstanceRole\": \"CORE\",\n",
        "                \"InstanceType\": \"m5.xlarge\",\n",
        "                \"InstanceCount\": 2,\n",
        "                \"Market\": \"ON_DEMAND\",\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    Steps=[\n",
        "        {\n",
        "            \"Name\": \"spark-etl\",\n",
        "            \"ActionOnFailure\": \"TERMINATE_CLUSTER\",\n",
        "            \"HadoopJarStep\": {\n",
        "                \"Jar\": \"command-runner.jar\",\n",
        "                \"Args\": [\n",
        "                    \"spark-submit\",\n",
        "                    \"--deploy-mode\",\n",
        "                    \"cluster\",\n",
        "                    s3_code_uri,\n",
        "                    \"--input\",\n",
        "                    s3_input_uri,\n",
        "                    \"--output\",\n",
        "                    s3_output_uri,\n",
        "                ],\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "    ServiceRole=service_role,\n",
        "    JobFlowRole=job_flow_role,\n",
        "    VisibleToAllUsers=True,\n",
        ")\n",
        "\n",
        "cluster_id = resp[\"JobFlowId\"]\n",
        "print(\"Started cluster:\", cluster_id)\n",
        "\n",
        "# Poll cluster state until it ends (or use an org-specific orchestration tool)\n",
        "terminal_states = {\"TERMINATED\", \"TERMINATED_WITH_ERRORS\"}\n",
        "while True:\n",
        "    cluster = emr.describe_cluster(ClusterId=cluster_id)[\"Cluster\"]\n",
        "    state = cluster[\"Status\"][\"State\"]\n",
        "    print(\"Cluster state:\", state)\n",
        "    if state in terminal_states:\n",
        "        break\n",
        "    time.sleep(30)\n",
        "\n",
        "# If you kept the cluster alive (KeepJobFlowAliveWhenNoSteps=True), terminate explicitly:\n",
        "# emr.terminate_job_flows(JobFlowIds=[cluster_id])\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
