{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWS Glue\n",
        "\n",
        "<img src=\"../_assets/aws_service_icons/glue.svg\" width=\"80\" alt=\"AWS Glue\">\n",
        "\n",
        "## Goals\n",
        "- Understand what **AWS Glue** is.\n",
        "- Know what it is used for in real data platforms.\n",
        "- See a minimal **AWS SDK** pseudo-code workflow (no execution).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "- Familiarity with **S3** and basic **IAM** concepts helps.\n",
        "- High-level idea of a **data lake** (raw → curated) is useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Glue is\n",
        "**AWS Glue** is a managed, serverless **data integration** service. In practice, it gives you:\n",
        "\n",
        "- A central **metadata catalog** (the *Glue Data Catalog*) for datasets and schemas.\n",
        "- **Crawlers** to discover data (often in S3) and populate/update that catalog.\n",
        "- Managed **ETL / ELT jobs** (commonly Apache Spark) to transform and move data.\n",
        "\n",
        "Glue often sits in the middle of an AWS analytics stack, connecting storage (S3), query engines (Athena), warehouses (Redshift), and processing frameworks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Glue is practically used for\n",
        "Common real-world uses include:\n",
        "\n",
        "- **Cataloging** a data lake: keep an inventory of datasets, schemas, partitions, and locations.\n",
        "- **Schema discovery** with crawlers: infer tables/columns from files (CSV/JSON/Parquet) and update metadata over time.\n",
        "- **Batch ETL**: read raw data, clean/normalize it, write curated outputs (often partitioned Parquet) back to S3.\n",
        "- **Interoperability**: make datasets queryable via **Athena** and usable by tools that speak to the Glue Catalog.\n",
        "- **Orchestrated pipelines**: run jobs on schedules/events via Glue triggers/workflows (or external orchestrators).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core concepts (minimum you should know)\n",
        "- **Data Catalog**: databases + tables + partitions (metadata pointing to data in S3, JDBC sources, etc.).\n",
        "- **Crawler**: scans a target (often S3) and creates/updates catalog tables.\n",
        "- **Job**: an ETL program (often Spark) Glue runs using a configured IAM role and arguments.\n",
        "- **IAM Role**: permissions for Glue to read sources (e.g., S3), write targets, and log to CloudWatch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Glue with the AWS SDK (pseudo-code)\n",
        "Below is a minimal, **non-executable** sketch using an AWS SDK (shown as `boto3`, the AWS SDK for Python).\n",
        "\n",
        "Notes:\n",
        "- Avoid hardcoding credentials; prefer **IAM roles** (for AWS compute) and SSO/role-based credentials locally.\n",
        "- `create_*` APIs are not idempotent by default; production code typically handles \"already exists\" errors.\n",
        "\n",
        "```python\n",
        "# PSEUDO-CODE (do not run)\n",
        "\n",
        "import boto3\n",
        "import time\n",
        "\n",
        "region = \"us-east-1\"\n",
        "glue = boto3.client(\"glue\", region_name=region)\n",
        "\n",
        "# Placeholders — replace with your environment\n",
        "database_name = \"my_data_lake_raw\"\n",
        "crawler_name = \"raw_s3_crawler\"\n",
        "table_prefix = \"raw_\"\n",
        "s3_target_path = \"s3://my-bucket/raw/events/\"\n",
        "\n",
        "job_name = \"etl_raw_to_curated\"\n",
        "iam_role_arn = \"arn:aws:iam::123456789012:role/AWSGlueServiceRole-MyRole\"\n",
        "script_location = \"s3://my-bucket/glue-scripts/etl_raw_to_curated.py\"\n",
        "\n",
        "# 1) Create (or ensure) a Glue Data Catalog database\n",
        "glue.create_database(DatabaseInput={\"Name\": database_name})\n",
        "\n",
        "# 2) Create a crawler to infer schema from S3 and populate the catalog\n",
        "glue.create_crawler(\n",
        "    Name=crawler_name,\n",
        "    Role=iam_role_arn,\n",
        "    DatabaseName=database_name,\n",
        "    Targets={\"S3Targets\": [{\"Path\": s3_target_path}]},\n",
        "    TablePrefix=table_prefix,\n",
        ")\n",
        "\n",
        "glue.start_crawler(Name=crawler_name)\n",
        "\n",
        "# 3) (Optional) Poll until the crawler finishes\n",
        "while True:\n",
        "    state = glue.get_crawler(Name=crawler_name)[\"Crawler\"][\"State\"]  # READY / RUNNING\n",
        "    if state == \"READY\":\n",
        "        break\n",
        "    time.sleep(15)\n",
        "\n",
        "# 4) Create a Glue ETL job (Spark) and run it\n",
        "glue.create_job(\n",
        "    Name=job_name,\n",
        "    Role=iam_role_arn,\n",
        "    Command={\n",
        "        \"Name\": \"glueetl\",\n",
        "        \"ScriptLocation\": script_location,\n",
        "        \"PythonVersion\": \"3\",\n",
        "    },\n",
        "    GlueVersion=\"4.0\",\n",
        "    DefaultArguments={\n",
        "        \"--job-language\": \"python\",\n",
        "        \"--TempDir\": \"s3://my-bucket/glue-temp/\",\n",
        "        \"--SOURCE_DB\": database_name,\n",
        "        \"--SOURCE_TABLE\": f\"{table_prefix}events\",\n",
        "        \"--TARGET_S3\": \"s3://my-bucket/curated/events/\",\n",
        "    },\n",
        ")\n",
        "\n",
        "run = glue.start_job_run(JobName=job_name, Arguments={\"--RUN_ID\": \"2026-01-06\"})\n",
        "run_id = run[\"JobRunId\"]\n",
        "\n",
        "# 5) (Optional) Poll job status\n",
        "while True:\n",
        "    jr = glue.get_job_run(\n",
        "        JobName=job_name,\n",
        "        RunId=run_id,\n",
        "        PredecessorsIncluded=False,\n",
        "    )[\"JobRun\"]\n",
        "    status = jr[\"JobRunState\"]  # STARTING / RUNNING / SUCCEEDED / FAILED / ...\n",
        "    if status in {\"SUCCEEDED\", \"FAILED\", \"STOPPED\", \"TIMEOUT\"}:\n",
        "        break\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"final_status:\", status)\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
