{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a311527f",
   "metadata": {},
   "source": [
    "# Econometrics — Overview\n",
    "\n",
    "## Purpose\n",
    "Econometrics is the application of statistical and mathematical methods to economic and financial data. It bridges economic theory with real-world observations, enabling us to:\n",
    "\n",
    "- **Test hypotheses** — validate or reject theoretical predictions with data\n",
    "- **Estimate relationships** — quantify how variables affect each other\n",
    "- **Forecast future values** — predict asset prices, economic indicators, risk metrics\n",
    "- **Identify causal effects** — distinguish correlation from causation\n",
    "\n",
    "## Key Questions This Section Answers\n",
    "1. How do we estimate the relationship between financial variables? (OLS Regression)\n",
    "2. How do we handle time-dependent data? (Time Series Econometrics)\n",
    "3. How do we analyze data with both cross-sectional and time dimensions? (Panel Data)\n",
    "4. How do we address endogeneity problems? (Instrumental Variables)\n",
    "5. How do we model changing volatility? (GARCH Models)\n",
    "6. How do we model multiple interrelated time series? (VAR Models)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd963e3",
   "metadata": {},
   "source": [
    "## 1. Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "OLS is the foundational econometric technique for estimating linear relationships.\n",
    "\n",
    "### The Linear Regression Model\n",
    "$$Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_k X_{ki} + \\varepsilon_i$$\n",
    "\n",
    "Or in matrix form:\n",
    "$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$$\n",
    "\n",
    "### OLS Estimator\n",
    "The OLS estimator minimizes the sum of squared residuals:\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{Y}$$\n",
    "\n",
    "### Gauss-Markov Assumptions (BLUE)\n",
    "For OLS to be the **Best Linear Unbiased Estimator (BLUE)**, these assumptions must hold:\n",
    "\n",
    "1. **Linearity**: The relationship between $Y$ and $X$ is linear in parameters\n",
    "2. **Random Sampling**: Observations are independently drawn\n",
    "3. **No Perfect Multicollinearity**: No exact linear relationships among regressors\n",
    "4. **Zero Conditional Mean**: $E[\\varepsilon | X] = 0$ (exogeneity)\n",
    "5. **Homoskedasticity**: $Var(\\varepsilon | X) = \\sigma^2$ (constant variance)\n",
    "6. **No Autocorrelation**: $Cov(\\varepsilon_i, \\varepsilon_j) = 0$ for $i \\neq j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, acorr_breusch_godfrey\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7111fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Example: Simulating a Factor Model (CAPM-style)\n",
    "# Y (excess return) = alpha + beta * market_return + epsilon\n",
    "\n",
    "n = 500  # Number of observations\n",
    "market_return = np.random.normal(0.08, 0.18, n)  # Market excess return\n",
    "true_alpha = 0.02  # True alpha (manager skill)\n",
    "true_beta = 1.2    # True beta (market sensitivity)\n",
    "epsilon = np.random.normal(0, 0.10, n)  # Idiosyncratic risk\n",
    "\n",
    "# Generate stock excess returns\n",
    "stock_return = true_alpha + true_beta * market_return + epsilon\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'stock_return': stock_return,\n",
    "    'market_return': market_return\n",
    "})\n",
    "\n",
    "# Fit OLS model\n",
    "X = sm.add_constant(df['market_return'])\n",
    "model = sm.OLS(df['stock_return'], X).fit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OLS REGRESSION: CAPM Factor Model\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTrue Alpha: {true_alpha:.4f}, Estimated Alpha: {model.params['const']:.4f}\")\n",
    "print(f\"True Beta:  {true_beta:.4f}, Estimated Beta:  {model.params['market_return']:.4f}\")\n",
    "print(f\"\\nR-squared: {model.rsquared:.4f}\")\n",
    "print(f\"Adjusted R-squared: {model.rsquared_adj:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OLS Regression Line\n",
    "fig = go.Figure()\n",
    "\n",
    "# Scatter plot of actual data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['market_return'],\n",
    "    y=df['stock_return'],\n",
    "    mode='markers',\n",
    "    marker=dict(color='steelblue', size=6, opacity=0.6),\n",
    "    name='Observations'\n",
    "))\n",
    "\n",
    "# Regression line\n",
    "x_line = np.linspace(df['market_return'].min(), df['market_return'].max(), 100)\n",
    "y_line = model.params['const'] + model.params['market_return'] * x_line\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_line,\n",
    "    y=y_line,\n",
    "    mode='lines',\n",
    "    line=dict(color='red', width=3),\n",
    "    name=f'OLS Fit: α={model.params[\"const\"]:.3f}, β={model.params[\"market_return\"]:.3f}'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='CAPM Regression: Stock Returns vs Market Returns',\n",
    "    xaxis_title='Market Excess Return',\n",
    "    yaxis_title='Stock Excess Return',\n",
    "    template='plotly_white',\n",
    "    legend=dict(x=0.02, y=0.98),\n",
    "    hovermode='closest'\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6898a23",
   "metadata": {},
   "source": [
    "## 2. OLS Diagnostics\n",
    "\n",
    "### Key Diagnostic Tests\n",
    "\n",
    "| Issue | Test | Null Hypothesis | Solution |\n",
    "|-------|------|-----------------|----------|\n",
    "| Heteroskedasticity | Breusch-Pagan, White | Constant variance | Robust SEs, WLS |\n",
    "| Autocorrelation | Durbin-Watson, Breusch-Godfrey | No serial correlation | HAC SEs, GLS |\n",
    "| Non-Normality | Jarque-Bera | Residuals are normal | Large samples (CLT) |\n",
    "| Multicollinearity | VIF | Low multicollinearity | Remove/combine vars |\n",
    "\n",
    "### Heteroskedasticity\n",
    "When $Var(\\varepsilon_i | X_i) \\neq \\sigma^2$, the OLS standard errors are biased.\n",
    "\n",
    "### Autocorrelation\n",
    "When $Cov(\\varepsilon_t, \\varepsilon_{t-k}) \\neq 0$, common in time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9105e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Diagnostics\n",
    "residuals = model.resid\n",
    "fitted = model.fittedvalues\n",
    "\n",
    "# 1. Breusch-Pagan Test for Heteroskedasticity\n",
    "bp_test = het_breuschpagan(residuals, X)\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGNOSTIC TESTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. Breusch-Pagan Test (Heteroskedasticity):\")\n",
    "print(f\"   LM Statistic: {bp_test[0]:.4f}\")\n",
    "print(f\"   p-value: {bp_test[1]:.4f}\")\n",
    "print(f\"   Result: {'Reject H0 (Heteroskedasticity detected)' if bp_test[1] < 0.05 else 'Fail to reject H0 (Homoskedasticity)'}\")\n",
    "\n",
    "# 2. Durbin-Watson Test for Autocorrelation\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"\\n2. Durbin-Watson Test (Autocorrelation):\")\n",
    "print(f\"   DW Statistic: {dw_stat:.4f}\")\n",
    "print(f\"   Result: {'No autocorrelation' if 1.5 < dw_stat < 2.5 else 'Possible autocorrelation'}\")\n",
    "\n",
    "# 3. Jarque-Bera Test for Normality\n",
    "jb_stat, jb_pval = stats.jarque_bera(residuals)\n",
    "print(f\"\\n3. Jarque-Bera Test (Normality):\")\n",
    "print(f\"   JB Statistic: {jb_stat:.4f}\")\n",
    "print(f\"   p-value: {jb_pval:.4f}\")\n",
    "print(f\"   Result: {'Reject H0 (Non-normal)' if jb_pval < 0.05 else 'Fail to reject H0 (Normal)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66508557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic Plots\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=('Residuals vs Fitted', 'Q-Q Plot',\n",
    "                                   'Scale-Location', 'Residual Distribution'))\n",
    "\n",
    "# 1. Residuals vs Fitted\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fitted, y=residuals, mode='markers',\n",
    "               marker=dict(color='steelblue', opacity=0.6),\n",
    "               showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=0, line_dash='dash', line_color='red', row=1, col=1)\n",
    "\n",
    "# 2. Q-Q Plot\n",
    "theoretical_q = stats.norm.ppf(np.linspace(0.01, 0.99, len(residuals)))\n",
    "sample_q = np.sort(residuals)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=theoretical_q, y=sample_q, mode='markers',\n",
    "               marker=dict(color='steelblue', opacity=0.6),\n",
    "               showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "# 45-degree line\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[-3, 3], y=[-3 * residuals.std(), 3 * residuals.std()],\n",
    "               mode='lines', line=dict(color='red', dash='dash'),\n",
    "               showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Scale-Location (sqrt of standardized residuals)\n",
    "std_resid = np.sqrt(np.abs(residuals / residuals.std()))\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fitted, y=std_resid, mode='markers',\n",
    "               marker=dict(color='steelblue', opacity=0.6),\n",
    "               showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Histogram of Residuals\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=residuals, nbinsx=30,\n",
    "                 marker_color='steelblue', opacity=0.7,\n",
    "                 showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, title_text='OLS Diagnostic Plots', template='plotly_white')\n",
    "fig.update_xaxes(title_text='Fitted Values', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Residuals', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Theoretical Quantiles', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Sample Quantiles', row=1, col=2)\n",
    "fig.update_xaxes(title_text='Fitted Values', row=2, col=1)\n",
    "fig.update_yaxes(title_text='√|Standardized Residuals|', row=2, col=1)\n",
    "fig.update_xaxes(title_text='Residuals', row=2, col=2)\n",
    "fig.update_yaxes(title_text='Frequency', row=2, col=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bfd6f",
   "metadata": {},
   "source": [
    "## 3. Time Series Econometrics\n",
    "\n",
    "### Stationarity\n",
    "A time series is **stationary** if its statistical properties don't change over time:\n",
    "\n",
    "1. **Constant mean**: $E[Y_t] = \\mu$ for all $t$\n",
    "2. **Constant variance**: $Var(Y_t) = \\sigma^2$ for all $t$\n",
    "3. **Autocovariance depends only on lag**: $Cov(Y_t, Y_{t-k}) = \\gamma_k$\n",
    "\n",
    "### Why Stationarity Matters\n",
    "- Non-stationary series can lead to **spurious regressions** (high $R^2$ without real relationship)\n",
    "- Standard inference (t-tests, F-tests) may be invalid\n",
    "- Many econometric models require stationary data\n",
    "\n",
    "### Unit Root\n",
    "A series $Y_t$ has a **unit root** if:\n",
    "$$Y_t = \\rho Y_{t-1} + \\varepsilon_t \\quad \\text{with } \\rho = 1$$\n",
    "\n",
    "This is a **random walk** — the series is non-stationary.\n",
    "\n",
    "### Augmented Dickey-Fuller (ADF) Test\n",
    "Tests for unit root:\n",
    "$$\\Delta Y_t = \\alpha + \\beta t + \\gamma Y_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta Y_{t-i} + \\varepsilon_t$$\n",
    "\n",
    "- **H₀**: $\\gamma = 0$ (unit root exists, non-stationary)\n",
    "- **H₁**: $\\gamma < 0$ (no unit root, stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss, coint\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Generate stationary vs non-stationary series\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "# Stationary: AR(1) with |phi| < 1\n",
    "phi = 0.7\n",
    "stationary_series = np.zeros(n)\n",
    "for t in range(1, n):\n",
    "    stationary_series[t] = phi * stationary_series[t-1] + np.random.normal(0, 1)\n",
    "\n",
    "# Non-stationary: Random Walk (unit root)\n",
    "random_walk = np.cumsum(np.random.normal(0, 1, n))\n",
    "\n",
    "# Visualize both series\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=('Stationary Series (AR(1), φ=0.7)',\n",
    "                                   'Non-Stationary Series (Random Walk)'))\n",
    "\n",
    "fig.add_trace(go.Scatter(y=stationary_series, mode='lines',\n",
    "                         line=dict(color='green', width=1), showlegend=False),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=random_walk, mode='lines',\n",
    "                         line=dict(color='red', width=1), showlegend=False),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, title_text='Stationary vs Non-Stationary Time Series',\n",
    "                  template='plotly_white')\n",
    "fig.update_xaxes(title_text='Time', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Time', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Value', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Value', row=1, col=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a55e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dickey-Fuller Test\n",
    "def run_adf_test(series, name):\n",
    "    result = adfuller(series, autolag='AIC')\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"  p-value: {result[1]:.4f}\")\n",
    "    print(f\"  Lags Used: {result[2]}\")\n",
    "    print(f\"  Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"    {key}: {value:.4f}\")\n",
    "    print(f\"  Result: {'Stationary (Reject H0)' if result[1] < 0.05 else 'Non-Stationary (Fail to reject H0)'}\")\n",
    "    return result\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AUGMENTED DICKEY-FULLER TEST FOR UNIT ROOT\")\n",
    "print(\"H0: Unit root exists (non-stationary)\")\n",
    "print(\"H1: No unit root (stationary)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "adf_stationary = run_adf_test(stationary_series, \"Stationary AR(1) Series\")\n",
    "adf_rw = run_adf_test(random_walk, \"Random Walk (Non-Stationary)\")\n",
    "adf_diff = run_adf_test(np.diff(random_walk), \"First Difference of Random Walk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d48add",
   "metadata": {},
   "source": [
    "### Cointegration\n",
    "\n",
    "Two non-stationary series $X_t$ and $Y_t$ are **cointegrated** if there exists a linear combination that is stationary:\n",
    "\n",
    "$$Y_t - \\beta X_t = \\varepsilon_t \\quad \\text{where } \\varepsilon_t \\sim I(0)$$\n",
    "\n",
    "**Interpretation**: The series move together in the long run despite short-term deviations.\n",
    "\n",
    "**Finance Applications**:\n",
    "- Pairs trading strategies\n",
    "- Long-run relationships between asset prices\n",
    "- Error correction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cointegration Example: Two cointegrated stock prices\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "# Common stochastic trend\n",
    "common_trend = np.cumsum(np.random.normal(0, 1, n))\n",
    "\n",
    "# Two cointegrated series\n",
    "stock_A = 50 + common_trend + np.random.normal(0, 2, n)  # Stock A\n",
    "stock_B = 30 + 0.6 * common_trend + np.random.normal(0, 1.5, n)  # Stock B (cointegrated with A)\n",
    "\n",
    "# Non-cointegrated series (independent random walk)\n",
    "stock_C = 40 + np.cumsum(np.random.normal(0, 1, n))  # Independent\n",
    "\n",
    "# Test for cointegration using Engle-Granger method\n",
    "print(\"=\" * 60)\n",
    "print(\"COINTEGRATION TEST (Engle-Granger Method)\")\n",
    "print(\"H0: No cointegration\")\n",
    "print(\"H1: Cointegration exists\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test Stock A and Stock B (should be cointegrated)\n",
    "coint_result_AB = coint(stock_A, stock_B)\n",
    "print(f\"\\nStock A & Stock B (Cointegrated):\")\n",
    "print(f\"  Test Statistic: {coint_result_AB[0]:.4f}\")\n",
    "print(f\"  p-value: {coint_result_AB[1]:.4f}\")\n",
    "print(f\"  Result: {'Cointegrated' if coint_result_AB[1] < 0.05 else 'Not Cointegrated'}\")\n",
    "\n",
    "# Test Stock A and Stock C (should NOT be cointegrated)\n",
    "coint_result_AC = coint(stock_A, stock_C)\n",
    "print(f\"\\nStock A & Stock C (Not Cointegrated):\")\n",
    "print(f\"  Test Statistic: {coint_result_AC[0]:.4f}\")\n",
    "print(f\"  p-value: {coint_result_AC[1]:.4f}\")\n",
    "print(f\"  Result: {'Cointegrated' if coint_result_AC[1] < 0.05 else 'Not Cointegrated'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36181cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Cointegrated vs Non-Cointegrated Series\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=('Cointegrated Stocks (A & B)',\n",
    "                                   'Spread: Stock A - 1.67*Stock B',\n",
    "                                   'Non-Cointegrated Stocks (A & C)',\n",
    "                                   'Spread: Stock A - Stock C'))\n",
    "\n",
    "# Cointegrated series\n",
    "fig.add_trace(go.Scatter(y=stock_A, name='Stock A', line=dict(color='blue')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=stock_B, name='Stock B', line=dict(color='green')), row=1, col=1)\n",
    "\n",
    "# Spread of cointegrated series (stationary)\n",
    "spread_AB = stock_A - 1.67 * stock_B\n",
    "fig.add_trace(go.Scatter(y=spread_AB, name='Spread A-B',\n",
    "                         line=dict(color='purple'), showlegend=False), row=1, col=2)\n",
    "fig.add_hline(y=spread_AB.mean(), line_dash='dash', line_color='red', row=1, col=2)\n",
    "\n",
    "# Non-cointegrated series\n",
    "fig.add_trace(go.Scatter(y=stock_A, name='Stock A', line=dict(color='blue'),\n",
    "                         showlegend=False), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=stock_C, name='Stock C', line=dict(color='orange'),\n",
    "                         showlegend=False), row=2, col=1)\n",
    "\n",
    "# Spread of non-cointegrated series (non-stationary)\n",
    "spread_AC = stock_A - stock_C\n",
    "fig.add_trace(go.Scatter(y=spread_AC, name='Spread A-C',\n",
    "                         line=dict(color='red'), showlegend=False), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=600, title_text='Cointegration: Mean-Reverting Spread',\n",
    "                  template='plotly_white')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eeedaa",
   "metadata": {},
   "source": [
    "## 4. Panel Data Models\n",
    "\n",
    "Panel data has **two dimensions**: cross-sectional (individuals) and time series.\n",
    "\n",
    "### General Panel Data Model\n",
    "$$Y_{it} = \\alpha + \\mathbf{X}_{it}'\\boldsymbol{\\beta} + u_{it}$$\n",
    "\n",
    "where $i = 1, ..., N$ (individuals) and $t = 1, ..., T$ (time periods).\n",
    "\n",
    "### Fixed Effects (FE) Model\n",
    "Controls for unobserved, time-invariant heterogeneity:\n",
    "$$Y_{it} = \\alpha_i + \\mathbf{X}_{it}'\\boldsymbol{\\beta} + \\varepsilon_{it}$$\n",
    "\n",
    "- Each individual has its own intercept $\\alpha_i$\n",
    "- Uses **within-group variation** (demeaning)\n",
    "- Allows $\\alpha_i$ to be correlated with $\\mathbf{X}_{it}$\n",
    "\n",
    "### Random Effects (RE) Model\n",
    "Treats individual effects as random:\n",
    "$$Y_{it} = \\alpha + \\mathbf{X}_{it}'\\boldsymbol{\\beta} + u_i + \\varepsilon_{it}$$\n",
    "\n",
    "- $u_i$ is a random individual effect\n",
    "- Uses **GLS estimation** (both within and between variation)\n",
    "- Assumes $u_i$ is uncorrelated with $\\mathbf{X}_{it}$\n",
    "\n",
    "### Hausman Test\n",
    "Tests whether to use FE or RE:\n",
    "- **H₀**: RE is consistent (effects uncorrelated with regressors)\n",
    "- **H₁**: FE is needed (effects correlated with regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearmodels.panel import PanelOLS, RandomEffects, compare\n",
    "\n",
    "# Simulate Panel Data: Returns for multiple firms over time\n",
    "np.random.seed(42)\n",
    "n_firms = 50\n",
    "n_years = 10\n",
    "\n",
    "# Create panel structure\n",
    "firms = np.repeat(np.arange(n_firms), n_years)\n",
    "years = np.tile(np.arange(n_years), n_firms)\n",
    "\n",
    "# Firm-specific fixed effects (unobserved heterogeneity)\n",
    "firm_effects = np.random.normal(0, 0.02, n_firms)\n",
    "firm_effects_panel = np.repeat(firm_effects, n_years)\n",
    "\n",
    "# Generate independent variables\n",
    "market_beta = 0.8 + 0.4 * np.random.rand(n_firms * n_years)  # Market beta\n",
    "size = np.random.normal(0, 1, n_firms * n_years)  # Size factor\n",
    "value = np.random.normal(0, 1, n_firms * n_years)  # Value factor\n",
    "\n",
    "# Generate returns\n",
    "true_beta_mkt = 0.06\n",
    "true_beta_size = 0.02\n",
    "true_beta_value = 0.03\n",
    "\n",
    "returns = (0.05 + firm_effects_panel +\n",
    "           true_beta_mkt * market_beta +\n",
    "           true_beta_size * size +\n",
    "           true_beta_value * value +\n",
    "           np.random.normal(0, 0.05, n_firms * n_years))\n",
    "\n",
    "# Create Panel DataFrame\n",
    "panel_df = pd.DataFrame({\n",
    "    'firm': firms,\n",
    "    'year': years,\n",
    "    'returns': returns,\n",
    "    'market_beta': market_beta,\n",
    "    'size': size,\n",
    "    'value': value\n",
    "})\n",
    "panel_df = panel_df.set_index(['firm', 'year'])\n",
    "\n",
    "print(\"Panel Data Structure:\")\n",
    "print(f\"  Number of firms (N): {n_firms}\")\n",
    "print(f\"  Number of years (T): {n_years}\")\n",
    "print(f\"  Total observations: {len(panel_df)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(panel_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe29f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Effects Model\n",
    "fe_model = PanelOLS(panel_df['returns'],\n",
    "                    panel_df[['market_beta', 'size', 'value']],\n",
    "                    entity_effects=True)\n",
    "fe_results = fe_model.fit()\n",
    "\n",
    "# Random Effects Model\n",
    "re_model = RandomEffects(panel_df['returns'],\n",
    "                         sm.add_constant(panel_df[['market_beta', 'size', 'value']]))\n",
    "re_results = re_model.fit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PANEL DATA MODELS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTrue Coefficients:\")\n",
    "print(f\"  Market Beta: {true_beta_mkt:.4f}\")\n",
    "print(f\"  Size: {true_beta_size:.4f}\")\n",
    "print(f\"  Value: {true_beta_value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FIXED EFFECTS MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(fe_results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RANDOM EFFECTS MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(re_results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hausman Test: FE vs RE\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Extract coefficients (excluding constant for RE)\n",
    "b_fe = fe_results.params\n",
    "b_re = re_results.params[['market_beta', 'size', 'value']]\n",
    "\n",
    "# Covariance matrices\n",
    "var_fe = fe_results.cov\n",
    "var_re = re_results.cov.loc[['market_beta', 'size', 'value'], ['market_beta', 'size', 'value']]\n",
    "\n",
    "# Hausman statistic\n",
    "diff = b_fe - b_re\n",
    "var_diff = var_fe - var_re\n",
    "hausman_stat = diff.T @ np.linalg.inv(var_diff) @ diff\n",
    "hausman_pval = 1 - chi2.cdf(hausman_stat, df=len(b_fe))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HAUSMAN TEST: Fixed Effects vs Random Effects\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nH0: Random Effects is consistent (use RE)\")\n",
    "print(f\"H1: Fixed Effects is needed (use FE)\")\n",
    "print(f\"\\nHausman Statistic: {hausman_stat:.4f}\")\n",
    "print(f\"p-value: {hausman_pval:.4f}\")\n",
    "print(f\"Degrees of freedom: {len(b_fe)}\")\n",
    "print(f\"\\nConclusion: {'Use Fixed Effects (Reject H0)' if hausman_pval < 0.05 else 'Use Random Effects (Fail to reject H0)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75a42c",
   "metadata": {},
   "source": [
    "## 5. Instrumental Variables and 2SLS\n",
    "\n",
    "### The Endogeneity Problem\n",
    "Endogeneity occurs when $Cov(X, \\varepsilon) \\neq 0$, causing OLS estimates to be **biased** and **inconsistent**.\n",
    "\n",
    "**Sources of Endogeneity**:\n",
    "1. **Omitted Variable Bias**: Relevant variable correlated with both $X$ and $Y$\n",
    "2. **Measurement Error**: Noise in measuring $X$\n",
    "3. **Simultaneity**: $X$ affects $Y$ and $Y$ affects $X$\n",
    "\n",
    "### Instrumental Variables (IV)\n",
    "An instrument $Z$ must satisfy:\n",
    "1. **Relevance**: $Cov(Z, X) \\neq 0$ — instrument is correlated with endogenous variable\n",
    "2. **Exogeneity**: $Cov(Z, \\varepsilon) = 0$ — instrument is uncorrelated with error\n",
    "\n",
    "### Two-Stage Least Squares (2SLS)\n",
    "\n",
    "**Stage 1**: Regress endogenous $X$ on instrument $Z$:\n",
    "$$X = \\pi_0 + \\pi_1 Z + v$$\n",
    "Get fitted values $\\hat{X}$\n",
    "\n",
    "**Stage 2**: Regress $Y$ on $\\hat{X}$:\n",
    "$$Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon$$\n",
    "\n",
    "The IV estimator:\n",
    "$$\\hat{\\beta}_{IV} = \\frac{Cov(Z, Y)}{Cov(Z, X)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "# Simulate Endogeneity Problem\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Unobserved confounder (affects both X and Y)\n",
    "confounder = np.random.normal(0, 1, n)\n",
    "\n",
    "# Instrument (affects X but NOT Y directly)\n",
    "instrument = np.random.normal(0, 1, n)\n",
    "\n",
    "# Endogenous variable X (affected by instrument and confounder)\n",
    "true_pi = 0.5  # Effect of instrument on X\n",
    "X = 2 + true_pi * instrument + 0.7 * confounder + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Outcome Y (affected by X and confounder, but NOT directly by instrument)\n",
    "true_beta = 1.5  # True causal effect of X on Y\n",
    "Y = 1 + true_beta * X + 0.8 * confounder + np.random.normal(0, 1, n)\n",
    "\n",
    "# Create DataFrame\n",
    "iv_df = pd.DataFrame({'Y': Y, 'X': X, 'Z': instrument})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSTRUMENTAL VARIABLES EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTrue causal effect (β): {true_beta}\")\n",
    "print(\"\\nProblem: X is endogenous (correlated with unobserved confounder)\")\n",
    "print(\"Solution: Use instrument Z that affects Y only through X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS vs 2SLS\n",
    "\n",
    "# OLS (biased due to endogeneity)\n",
    "ols_model = sm.OLS(iv_df['Y'], sm.add_constant(iv_df['X'])).fit()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OLS ESTIMATION (BIASED)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"OLS estimate of β: {ols_model.params['X']:.4f}\")\n",
    "print(f\"True β: {true_beta}\")\n",
    "print(f\"Bias: {ols_model.params['X'] - true_beta:.4f}\")\n",
    "\n",
    "# 2SLS (consistent)\n",
    "iv_model = IV2SLS(iv_df['Y'], exog=None, endog=iv_df['X'], instruments=iv_df['Z']).fit()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2SLS ESTIMATION (CONSISTENT)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"2SLS estimate of β: {iv_model.params['X']:.4f}\")\n",
    "print(f\"True β: {true_beta}\")\n",
    "print(f\"Difference: {iv_model.params['X'] - true_beta:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FIRST STAGE REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "first_stage = sm.OLS(iv_df['X'], sm.add_constant(iv_df['Z'])).fit()\n",
    "print(f\"Instrument coefficient: {first_stage.params['Z']:.4f}\")\n",
    "print(f\"F-statistic: {first_stage.fvalue:.2f}\")\n",
    "print(f\"Rule of thumb: F > 10 indicates strong instrument\")\n",
    "print(f\"Result: {'Strong instrument' if first_stage.fvalue > 10 else 'Weak instrument warning'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OLS vs IV estimates\n",
    "fig = go.Figure()\n",
    "\n",
    "# Data points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=iv_df['X'], y=iv_df['Y'],\n",
    "    mode='markers', marker=dict(color='steelblue', size=5, opacity=0.4),\n",
    "    name='Data'\n",
    "))\n",
    "\n",
    "# OLS line (biased)\n",
    "x_range = np.linspace(iv_df['X'].min(), iv_df['X'].max(), 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_range, y=ols_model.params['const'] + ols_model.params['X'] * x_range,\n",
    "    mode='lines', line=dict(color='red', width=3),\n",
    "    name=f'OLS (β={ols_model.params[\"X\"]:.3f}) - BIASED'\n",
    "))\n",
    "\n",
    "# IV line (consistent)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_range, y=iv_model.params['X'] * x_range,\n",
    "    mode='lines', line=dict(color='green', width=3),\n",
    "    name=f'2SLS (β={iv_model.params[\"X\"]:.3f}) - CONSISTENT'\n",
    "))\n",
    "\n",
    "# True relationship\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_range, y=1 + true_beta * x_range,\n",
    "    mode='lines', line=dict(color='black', width=2, dash='dash'),\n",
    "    name=f'True (β={true_beta})'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='OLS vs 2SLS: Correcting for Endogeneity',\n",
    "    xaxis_title='X (Endogenous Variable)',\n",
    "    yaxis_title='Y',\n",
    "    template='plotly_white',\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806607ca",
   "metadata": {},
   "source": [
    "## 6. GARCH Models for Volatility\n",
    "\n",
    "Financial returns exhibit **volatility clustering** — periods of high volatility tend to be followed by high volatility.\n",
    "\n",
    "### ARCH (Autoregressive Conditional Heteroskedasticity)\n",
    "$$\\sigma_t^2 = \\omega + \\alpha_1 \\varepsilon_{t-1}^2$$\n",
    "\n",
    "Volatility depends on past squared shocks.\n",
    "\n",
    "### GARCH(1,1) (Generalized ARCH)\n",
    "$$\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "Where:\n",
    "- $\\omega$ = baseline volatility (constant)\n",
    "- $\\alpha$ = reaction to recent shocks\n",
    "- $\\beta$ = persistence of volatility\n",
    "- $\\alpha + \\beta$ = persistence coefficient (should be < 1 for stationarity)\n",
    "\n",
    "### Key Properties\n",
    "- **Volatility Clustering**: Captured by $\\beta$\n",
    "- **Fat Tails**: GARCH generates leptokurtic distributions\n",
    "- **Mean Reversion**: Long-run variance = $\\frac{\\omega}{1 - \\alpha - \\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8db837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "# Simulate GARCH(1,1) process\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "# GARCH parameters\n",
    "omega = 0.00001  # Baseline variance\n",
    "alpha = 0.1      # Shock impact\n",
    "beta = 0.85      # Persistence\n",
    "\n",
    "# Initialize\n",
    "returns = np.zeros(n)\n",
    "sigma2 = np.zeros(n)\n",
    "sigma2[0] = omega / (1 - alpha - beta)  # Unconditional variance\n",
    "\n",
    "# Simulate\n",
    "for t in range(1, n):\n",
    "    sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "    returns[t] = np.sqrt(sigma2[t]) * np.random.normal()\n",
    "\n",
    "# Create time series\n",
    "returns_series = pd.Series(returns * 100, name='returns')  # Convert to percentage\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GARCH(1,1) SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTrue Parameters:\")\n",
    "print(f\"  ω (omega): {omega:.6f}\")\n",
    "print(f\"  α (alpha): {alpha:.4f}\")\n",
    "print(f\"  β (beta):  {beta:.4f}\")\n",
    "print(f\"  α + β:     {alpha + beta:.4f}\")\n",
    "print(f\"  Long-run variance: {omega/(1-alpha-beta):.6f}\")\n",
    "print(f\"  Long-run volatility (daily %): {np.sqrt(omega/(1-alpha-beta))*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Volatility Clustering\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    subplot_titles=('Returns (%)', 'Conditional Volatility'),\n",
    "                    row_heights=[0.5, 0.5])\n",
    "\n",
    "# Returns\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=returns_series, mode='lines',\n",
    "               line=dict(color='steelblue', width=0.8),\n",
    "               showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Volatility\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=np.sqrt(sigma2) * 100, mode='lines',\n",
    "               line=dict(color='red', width=1),\n",
    "               showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, title_text='GARCH(1,1): Volatility Clustering',\n",
    "                  template='plotly_white')\n",
    "fig.update_xaxes(title_text='Time', row=2, col=1)\n",
    "fig.update_yaxes(title_text='Return (%)', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Volatility (%)', row=2, col=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GARCH(1,1) Model to Simulated Data\n",
    "garch_model = arch_model(returns_series, vol='Garch', p=1, q=1, rescale=False)\n",
    "garch_fit = garch_model.fit(disp='off')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GARCH(1,1) ESTIMATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(garch_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f179650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare True vs Estimated Volatility\n",
    "estimated_vol = garch_fit.conditional_volatility\n",
    "true_vol = np.sqrt(sigma2) * 100\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=true_vol[100:], name='True Volatility',\n",
    "    line=dict(color='blue', width=1.5)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=estimated_vol[100:], name='Estimated Volatility',\n",
    "    line=dict(color='red', width=1.5, dash='dot')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='GARCH(1,1): True vs Estimated Conditional Volatility',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Volatility (%)',\n",
    "    template='plotly_white',\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Correlation between true and estimated\n",
    "corr = np.corrcoef(true_vol[100:], estimated_vol[100:])[0, 1]\n",
    "print(f\"\\nCorrelation between true and estimated volatility: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a604aa4",
   "metadata": {},
   "source": [
    "## 7. Vector Autoregression (VAR)\n",
    "\n",
    "VAR models capture **dynamic relationships** among multiple time series.\n",
    "\n",
    "### VAR(p) Model\n",
    "$$\\mathbf{Y}_t = \\mathbf{c} + \\mathbf{A}_1 \\mathbf{Y}_{t-1} + \\mathbf{A}_2 \\mathbf{Y}_{t-2} + ... + \\mathbf{A}_p \\mathbf{Y}_{t-p} + \\boldsymbol{\\varepsilon}_t$$\n",
    "\n",
    "For two variables:\n",
    "$$\\begin{bmatrix} Y_{1t} \\\\ Y_{2t} \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} + \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} Y_{1,t-1} \\\\ Y_{2,t-1} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1t} \\\\ \\varepsilon_{2t} \\end{bmatrix}$$\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Granger Causality**: $X$ Granger-causes $Y$ if past values of $X$ help predict $Y$.\n",
    "\n",
    "**Impulse Response Functions (IRF)**: Track how a shock to one variable propagates through the system.\n",
    "\n",
    "**Forecast Error Variance Decomposition (FEVD)**: Shows what fraction of forecast error variance is due to each shock.\n",
    "\n",
    "### Finance Applications\n",
    "- Stock-bond return relationships\n",
    "- Macroeconomic forecasting\n",
    "- Risk spillover analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e21c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Simulate VAR(1) process: Stock Returns and Bond Returns\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "# VAR coefficient matrix\n",
    "A = np.array([[0.5, -0.2],   # Stock: AR(1) + negative bond effect\n",
    "              [0.1, 0.6]])   # Bond: slight stock spillover + AR(1)\n",
    "\n",
    "# Simulate\n",
    "stock_returns = np.zeros(n)\n",
    "bond_returns = np.zeros(n)\n",
    "\n",
    "for t in range(1, n):\n",
    "    stock_returns[t] = A[0,0] * stock_returns[t-1] + A[0,1] * bond_returns[t-1] + np.random.normal(0, 1)\n",
    "    bond_returns[t] = A[1,0] * stock_returns[t-1] + A[1,1] * bond_returns[t-1] + np.random.normal(0, 0.5)\n",
    "\n",
    "# Create DataFrame\n",
    "var_df = pd.DataFrame({\n",
    "    'Stock_Returns': stock_returns,\n",
    "    'Bond_Returns': bond_returns\n",
    "})\n",
    "\n",
    "# Visualize the series\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=('Stock Returns', 'Bond Returns'))\n",
    "\n",
    "fig.add_trace(go.Scatter(y=stock_returns, mode='lines',\n",
    "                         line=dict(color='blue', width=1), showlegend=False),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=bond_returns, mode='lines',\n",
    "                         line=dict(color='green', width=1), showlegend=False),\n",
    "              row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=400, title_text='Simulated Stock and Bond Returns',\n",
    "                  template='plotly_white')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit VAR Model\n",
    "var_model = VAR(var_df)\n",
    "\n",
    "# Select optimal lag using information criteria\n",
    "lag_order = var_model.select_order(maxlags=10)\n",
    "print(\"=\" * 60)\n",
    "print(\"VAR LAG ORDER SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(lag_order.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit VAR(1)\n",
    "var_results = var_model.fit(1)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VAR(1) ESTIMATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(var_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d72d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Granger Causality Tests\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRANGER CAUSALITY TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Does Bond Granger-cause Stock? ---\")\n",
    "gc_bond_stock = grangercausalitytests(var_df[['Stock_Returns', 'Bond_Returns']], maxlag=2, verbose=True)\n",
    "\n",
    "print(\"\\n--- Does Stock Granger-cause Bond? ---\")\n",
    "gc_stock_bond = grangercausalitytests(var_df[['Bond_Returns', 'Stock_Returns']], maxlag=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e148b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impulse Response Functions\n",
    "irf = var_results.irf(periods=20)\n",
    "\n",
    "# Plot IRFs\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=('Stock → Stock', 'Bond → Stock',\n",
    "                                   'Stock → Bond', 'Bond → Bond'))\n",
    "\n",
    "periods = np.arange(21)\n",
    "\n",
    "# Stock shock on Stock\n",
    "fig.add_trace(go.Scatter(x=periods, y=irf.irfs[:, 0, 0],\n",
    "                         line=dict(color='blue', width=2), showlegend=False),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Bond shock on Stock\n",
    "fig.add_trace(go.Scatter(x=periods, y=irf.irfs[:, 0, 1],\n",
    "                         line=dict(color='green', width=2), showlegend=False),\n",
    "              row=1, col=2)\n",
    "\n",
    "# Stock shock on Bond\n",
    "fig.add_trace(go.Scatter(x=periods, y=irf.irfs[:, 1, 0],\n",
    "                         line=dict(color='blue', width=2), showlegend=False),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Bond shock on Bond\n",
    "fig.add_trace(go.Scatter(x=periods, y=irf.irfs[:, 1, 1],\n",
    "                         line=dict(color='green', width=2), showlegend=False),\n",
    "              row=2, col=2)\n",
    "\n",
    "# Add zero lines\n",
    "for row in [1, 2]:\n",
    "    for col in [1, 2]:\n",
    "        fig.add_hline(y=0, line_dash='dash', line_color='gray', row=row, col=col)\n",
    "\n",
    "fig.update_layout(height=500, title_text='Impulse Response Functions',\n",
    "                  template='plotly_white')\n",
    "fig.update_xaxes(title_text='Periods', row=2, col=1)\n",
    "fig.update_xaxes(title_text='Periods', row=2, col=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e850d",
   "metadata": {},
   "source": [
    "## 8. Practical Finance Applications\n",
    "\n",
    "### Fama-French Factor Model\n",
    "The classic multi-factor model:\n",
    "$$R_i - R_f = \\alpha_i + \\beta_{i,mkt}(R_m - R_f) + \\beta_{i,smb} SMB + \\beta_{i,hml} HML + \\varepsilon_i$$\n",
    "\n",
    "Where:\n",
    "- $R_m - R_f$ = Market excess return\n",
    "- $SMB$ = Small Minus Big (size factor)\n",
    "- $HML$ = High Minus Low (value factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ee87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Fama-French 3-Factor Model\n",
    "np.random.seed(42)\n",
    "n = 252 * 10  # 10 years of daily data\n",
    "\n",
    "# Factor returns (simulated)\n",
    "mkt_rf = np.random.normal(0.0003, 0.01, n)  # Market excess return\n",
    "smb = np.random.normal(0.0001, 0.005, n)    # Size factor\n",
    "hml = np.random.normal(0.0001, 0.006, n)    # Value factor\n",
    "\n",
    "# True factor loadings for a hypothetical stock\n",
    "true_alpha = 0.0002  # Daily alpha (~5% annual)\n",
    "true_beta_mkt = 1.2\n",
    "true_beta_smb = 0.5   # Small-cap tilt\n",
    "true_beta_hml = -0.3  # Growth tilt (negative HML)\n",
    "\n",
    "# Generate stock excess returns\n",
    "stock_excess = (true_alpha + \n",
    "                true_beta_mkt * mkt_rf +\n",
    "                true_beta_smb * smb +\n",
    "                true_beta_hml * hml +\n",
    "                np.random.normal(0, 0.008, n))  # Idiosyncratic risk\n",
    "\n",
    "# Create DataFrame\n",
    "ff_df = pd.DataFrame({\n",
    "    'excess_return': stock_excess,\n",
    "    'mkt_rf': mkt_rf,\n",
    "    'smb': smb,\n",
    "    'hml': hml\n",
    "})\n",
    "\n",
    "# Fit Fama-French model\n",
    "X_ff = sm.add_constant(ff_df[['mkt_rf', 'smb', 'hml']])\n",
    "ff_model = sm.OLS(ff_df['excess_return'], X_ff).fit(cov_type='HC1')  # Robust SEs\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FAMA-FRENCH 3-FACTOR MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTrue Factor Loadings:\")\n",
    "print(f\"  Alpha (daily): {true_alpha:.6f} (~{true_alpha*252*100:.2f}% annual)\")\n",
    "print(f\"  Market Beta:   {true_beta_mkt:.4f}\")\n",
    "print(f\"  SMB Beta:      {true_beta_smb:.4f}\")\n",
    "print(f\"  HML Beta:      {true_beta_hml:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(ff_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Factor Exposures\n",
    "factor_names = ['Alpha', 'Market', 'SMB', 'HML']\n",
    "true_values = [true_alpha * 252, true_beta_mkt, true_beta_smb, true_beta_hml]  # Annualize alpha\n",
    "estimated = [ff_model.params['const'] * 252, \n",
    "             ff_model.params['mkt_rf'],\n",
    "             ff_model.params['smb'],\n",
    "             ff_model.params['hml']]\n",
    "conf_int = ff_model.conf_int()\n",
    "errors = [1.96 * ff_model.bse['const'] * 252,\n",
    "          1.96 * ff_model.bse['mkt_rf'],\n",
    "          1.96 * ff_model.bse['smb'],\n",
    "          1.96 * ff_model.bse['hml']]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Estimated',\n",
    "    x=factor_names,\n",
    "    y=estimated,\n",
    "    error_y=dict(type='data', array=errors),\n",
    "    marker_color='steelblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    name='True Value',\n",
    "    x=factor_names,\n",
    "    y=true_values,\n",
    "    mode='markers',\n",
    "    marker=dict(color='red', size=12, symbol='diamond')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Fama-French Factor Exposures: Estimated vs True',\n",
    "    yaxis_title='Factor Loading',\n",
    "    template='plotly_white',\n",
    "    barmode='group',\n",
    "    legend=dict(x=0.75, y=0.95)\n",
    ")\n",
    "fig.add_hline(y=0, line_dash='dash', line_color='gray')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e401202",
   "metadata": {},
   "source": [
    "### Risk Model: Value at Risk with GARCH\n",
    "\n",
    "GARCH-based VaR accounts for time-varying volatility:\n",
    "$$VaR_{t+1}^{\\alpha} = -\\mu_{t+1} + \\sigma_{t+1} \\cdot z_{\\alpha}$$\n",
    "\n",
    "Where $\\sigma_{t+1}$ is the GARCH-forecasted volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH-based VaR\n",
    "\n",
    "# Use our previously simulated returns\n",
    "# Forecast volatility\n",
    "forecast = garch_fit.forecast(horizon=1)\n",
    "forecasted_vol = np.sqrt(forecast.variance.values[-1, 0])\n",
    "\n",
    "# VaR at different confidence levels\n",
    "confidence_levels = [0.90, 0.95, 0.99]\n",
    "z_scores = [stats.norm.ppf(1 - cl) for cl in confidence_levels]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALUE AT RISK (VaR) - GARCH APPROACH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nForecasted 1-day volatility: {forecasted_vol:.4f}%\")\n",
    "print(f\"\\nFor $1,000,000 portfolio:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "portfolio_value = 1_000_000\n",
    "\n",
    "for cl, z in zip(confidence_levels, z_scores):\n",
    "    var = forecasted_vol * abs(z) / 100 * portfolio_value\n",
    "    print(f\"  {int(cl*100)}% VaR: ${var:,.0f}\")\n",
    "    print(f\"    Interpretation: {int(cl*100)}% confident loss won't exceed ${var:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling VaR Visualization\n",
    "window = 252  # 1 year rolling window\n",
    "\n",
    "# Calculate rolling VaR using historical simulation and GARCH\n",
    "rolling_var_95 = []\n",
    "garch_var_95 = []\n",
    "\n",
    "for i in range(window, len(returns_series)):\n",
    "    # Historical simulation VaR\n",
    "    hist_var = -np.percentile(returns_series[i-window:i], 5)\n",
    "    rolling_var_95.append(hist_var)\n",
    "    \n",
    "    # GARCH VaR\n",
    "    garch_vol = estimated_vol[i]\n",
    "    garch_var_95.append(garch_vol * 1.645)  # 95% z-score\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "                    subplot_titles=('Returns with VaR Breaches', '95% VaR: Historical vs GARCH'),\n",
    "                    row_heights=[0.5, 0.5])\n",
    "\n",
    "# Returns\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=returns_series[window:], mode='lines',\n",
    "               line=dict(color='steelblue', width=0.8), name='Returns'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Negative VaR line\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=[-v for v in garch_var_95], mode='lines',\n",
    "               line=dict(color='red', width=1.5), name='-VaR (95%)'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# VaR comparison\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=rolling_var_95, mode='lines',\n",
    "               line=dict(color='blue', width=1.5), name='Historical VaR'),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=garch_var_95, mode='lines',\n",
    "               line=dict(color='red', width=1.5), name='GARCH VaR'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, title_text='Value at Risk: Historical vs GARCH',\n",
    "                  template='plotly_white')\n",
    "fig.update_xaxes(title_text='Time', row=2, col=1)\n",
    "fig.update_yaxes(title_text='Return (%)', row=1, col=1)\n",
    "fig.update_yaxes(title_text='VaR (%)', row=2, col=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ef588",
   "metadata": {},
   "source": [
    "## 9. Summary: Econometric Toolkit for Finance\n",
    "\n",
    "| Method | Use Case | Key Assumptions | Python Package |\n",
    "|--------|----------|-----------------|----------------|\n",
    "| **OLS** | Cross-sectional relationships, factor models | Exogeneity, homoskedasticity | `statsmodels` |\n",
    "| **ADF/Unit Root** | Stationarity testing | - | `statsmodels.tsa` |\n",
    "| **Cointegration** | Long-run relationships, pairs trading | Non-stationary series | `statsmodels.tsa` |\n",
    "| **Panel FE/RE** | Cross-sectional + time series | Entity effects | `linearmodels` |\n",
    "| **2SLS/IV** | Endogeneity correction | Valid instruments | `linearmodels` |\n",
    "| **GARCH** | Volatility modeling, VaR | Volatility clustering | `arch` |\n",
    "| **VAR** | Multivariate dynamics, forecasting | Stationarity | `statsmodels.tsa` |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always check assumptions**: Run diagnostic tests before interpreting results\n",
    "2. **Use robust standard errors**: When heteroskedasticity or autocorrelation suspected\n",
    "3. **Test for stationarity**: Before running time series regressions\n",
    "4. **Be cautious with causality**: Econometric relationships are correlational by default\n",
    "5. **Out-of-sample testing**: Validate models on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e4c10",
   "metadata": {},
   "source": [
    "## 10. Key Formulas Reference\n",
    "\n",
    "### OLS Estimator\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{Y}$$\n",
    "\n",
    "### R-squared\n",
    "$$R^2 = 1 - \\frac{SSR}{SST} = 1 - \\frac{\\sum(Y_i - \\hat{Y}_i)^2}{\\sum(Y_i - \\bar{Y})^2}$$\n",
    "\n",
    "### GARCH(1,1)\n",
    "$$\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "### Instrumental Variables\n",
    "$$\\hat{\\beta}_{IV} = \\frac{Cov(Z, Y)}{Cov(Z, X)}$$\n",
    "\n",
    "### Hausman Statistic\n",
    "$$H = (\\hat{\\beta}_{FE} - \\hat{\\beta}_{RE})'[Var(\\hat{\\beta}_{FE}) - Var(\\hat{\\beta}_{RE})]^{-1}(\\hat{\\beta}_{FE} - \\hat{\\beta}_{RE}) \\sim \\chi^2_k$$\n",
    "\n",
    "### Durbin-Watson\n",
    "$$DW = \\frac{\\sum_{t=2}^{n}(e_t - e_{t-1})^2}{\\sum_{t=1}^{n}e_t^2}$$\n",
    "\n",
    "- DW ≈ 2: No autocorrelation\n",
    "- DW < 2: Positive autocorrelation\n",
    "- DW > 2: Negative autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages summary\n",
    "print(\"=\" * 60)\n",
    "print(\"REQUIRED PACKAGES FOR ECONOMETRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Core packages\n",
    "pip install numpy pandas scipy\n",
    "\n",
    "# Econometrics\n",
    "pip install statsmodels\n",
    "pip install linearmodels\n",
    "pip install arch\n",
    "\n",
    "# Visualization\n",
    "pip install plotly\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}